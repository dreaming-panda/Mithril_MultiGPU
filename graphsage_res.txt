gnerv1
Thu Aug  3 20:38:09 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   30C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   29C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   28C    P8    23W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 11%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 91%] Built target OSDI2023_MULTI_NODES_graphsage
[ 91%] Built target OSDI2023_MULTI_NODES_gcnii
[ 91%] Built target OSDI2023_MULTI_NODES_gcn
[ 91%] Built target estimate_comm_volume
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.848 seconds.
Building the CSC structure...
        It takes 1.902 seconds.
Building the CSC structure...
        It takes 2.022 seconds.
Building the CSC structure...
        It takes 2.304 seconds.
Building the CSC structure...
        It takes 2.420 seconds.
Building the CSC structure...
        It takes 2.497 seconds.
Building the CSC structure...
        It takes 2.532 seconds.
Building the CSC structure...
        It takes 2.600 seconds.
Building the CSC structure...
        It takes 1.796 seconds.
        It takes 1.853 seconds.
        It takes 1.846 seconds.
        It takes 2.282 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.339 seconds.
        It takes 2.291 seconds.
        It takes 2.358 seconds.
        It takes 0.250 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
        It takes 2.367 seconds.
Building the Feature Vector...
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.250 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
        It takes 0.039 seconds.
Building the Feature Vector...
GPU 0, layer [0, 32)
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.275 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.258 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
        It takes 0.035 seconds.
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 32)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
Chunks (number of global chunks: 8): 0-[0, 31876) 1-[31876, 56041) 2-[56041, 86963) 3-[86963, 111539) 4-[111539, 150873) 5-[150873, 175023) 6-[175023, 206335) 7-[206335, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
GPU 0, layer [0, 32)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.431 Gbps (per GPU), 451.450 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.172 Gbps (per GPU), 449.374 Gbps (aggregated)
The layer-level communication performance: 56.179 Gbps (per GPU), 449.431 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.939 Gbps (per GPU), 447.510 Gbps (aggregated)
The layer-level communication performance: 55.894 Gbps (per GPU), 447.151 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.702 Gbps (per GPU), 445.617 Gbps (aggregated)
The layer-level communication performance: 55.667 Gbps (per GPU), 445.333 Gbps (aggregated)
The layer-level communication performance: 55.627 Gbps (per GPU), 445.015 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.264 Gbps (per GPU), 1266.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.264 Gbps (per GPU), 1266.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.273 Gbps (per GPU), 1266.181 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.270 Gbps (per GPU), 1266.157 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.258 Gbps (per GPU), 1266.062 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.270 Gbps (per GPU), 1266.157 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.264 Gbps (per GPU), 1266.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.192 Gbps (per GPU), 1265.536 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.496 Gbps (per GPU), 811.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.496 Gbps (per GPU), 811.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.491 Gbps (per GPU), 811.932 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.496 Gbps (per GPU), 811.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.491 Gbps (per GPU), 811.932 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.496 Gbps (per GPU), 811.971 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.494 Gbps (per GPU), 811.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.496 Gbps (per GPU), 811.964 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.911 Gbps (per GPU), 303.287 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.911 Gbps (per GPU), 303.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.910 Gbps (per GPU), 303.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.909 Gbps (per GPU), 303.275 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.911 Gbps (per GPU), 303.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.911 Gbps (per GPU), 303.286 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.911 Gbps (per GPU), 303.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.910 Gbps (per GPU), 303.281 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.47ms 10.02ms  9.18ms  1.36 31.88K 14.26M
 chk_1 11.87ms 10.12ms  9.46ms  1.25 24.16K 14.58M
 chk_2 12.89ms 10.53ms  9.68ms  1.33 30.92K 14.38M
 chk_3 12.25ms 10.44ms  9.70ms  1.26 24.58K 14.59M
 chk_4 13.39ms 10.36ms  9.37ms  1.43 39.33K 13.77M
 chk_5 12.16ms 10.34ms  9.65ms  1.26 24.15K 14.35M
 chk_6 13.98ms 11.29ms 10.47ms  1.34 31.31K 14.22M
 chk_7 12.74ms 10.78ms  9.77ms  1.30 26.63K 14.46M
   Avg 12.72 10.48  9.66
   Max 13.98 11.29 10.47
   Min 11.87 10.02  9.18
 Ratio  1.18  1.13  1.14
   Var  0.42  0.14  0.13
Profiling takes 3.004 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 353.380 ms
Partition 0 [0, 4) has cost: 353.380 ms
Partition 1 [4, 8) has cost: 335.484 ms
Partition 2 [8, 12) has cost: 335.484 ms
Partition 3 [12, 16) has cost: 335.484 ms
Partition 4 [16, 20) has cost: 335.484 ms
Partition 5 [20, 24) has cost: 335.484 ms
Partition 6 [24, 28) has cost: 335.484 ms
Partition 7 [28, 32) has cost: 328.886 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 242.863 ms
GPU 0, Compute+Comm Time: 127.338 ms, Bubble Time: 114.531 ms, Imbalance Overhead: 0.994 ms
GPU 1, Compute+Comm Time: 122.778 ms, Bubble Time: 113.305 ms, Imbalance Overhead: 6.780 ms
GPU 2, Compute+Comm Time: 122.778 ms, Bubble Time: 111.789 ms, Imbalance Overhead: 8.296 ms
GPU 3, Compute+Comm Time: 122.778 ms, Bubble Time: 111.262 ms, Imbalance Overhead: 8.823 ms
GPU 4, Compute+Comm Time: 122.778 ms, Bubble Time: 110.162 ms, Imbalance Overhead: 9.923 ms
GPU 5, Compute+Comm Time: 122.778 ms, Bubble Time: 110.133 ms, Imbalance Overhead: 9.952 ms
GPU 6, Compute+Comm Time: 122.778 ms, Bubble Time: 109.603 ms, Imbalance Overhead: 10.482 ms
GPU 7, Compute+Comm Time: 120.784 ms, Bubble Time: 111.735 ms, Imbalance Overhead: 10.344 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 474.553 ms
GPU 0, Compute+Comm Time: 234.523 ms, Bubble Time: 220.427 ms, Imbalance Overhead: 19.602 ms
GPU 1, Compute+Comm Time: 239.128 ms, Bubble Time: 216.273 ms, Imbalance Overhead: 19.152 ms
GPU 2, Compute+Comm Time: 239.128 ms, Bubble Time: 217.241 ms, Imbalance Overhead: 18.184 ms
GPU 3, Compute+Comm Time: 239.128 ms, Bubble Time: 216.985 ms, Imbalance Overhead: 18.439 ms
GPU 4, Compute+Comm Time: 239.128 ms, Bubble Time: 218.239 ms, Imbalance Overhead: 17.186 ms
GPU 5, Compute+Comm Time: 239.128 ms, Bubble Time: 218.451 ms, Imbalance Overhead: 16.974 ms
GPU 6, Compute+Comm Time: 239.128 ms, Bubble Time: 220.677 ms, Imbalance Overhead: 14.747 ms
GPU 7, Compute+Comm Time: 252.463 ms, Bubble Time: 222.014 ms, Imbalance Overhead: 0.076 ms
The estimated cost of the whole pipeline: 753.286 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 688.865 ms
Partition 0 [0, 8) has cost: 688.865 ms
Partition 1 [8, 16) has cost: 670.969 ms
Partition 2 [16, 24) has cost: 670.969 ms
Partition 3 [24, 32) has cost: 664.371 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 245.559 ms
GPU 0, Compute+Comm Time: 137.422 ms, Bubble Time: 103.884 ms, Imbalance Overhead: 4.253 ms
GPU 1, Compute+Comm Time: 134.946 ms, Bubble Time: 103.322 ms, Imbalance Overhead: 7.291 ms
GPU 2, Compute+Comm Time: 134.946 ms, Bubble Time: 102.259 ms, Imbalance Overhead: 8.355 ms
GPU 3, Compute+Comm Time: 133.891 ms, Bubble Time: 105.971 ms, Imbalance Overhead: 5.698 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 459.046 ms
GPU 0, Compute+Comm Time: 250.815 ms, Bubble Time: 196.832 ms, Imbalance Overhead: 11.399 ms
GPU 1, Compute+Comm Time: 253.273 ms, Bubble Time: 190.726 ms, Imbalance Overhead: 15.048 ms
GPU 2, Compute+Comm Time: 253.273 ms, Bubble Time: 193.958 ms, Imbalance Overhead: 11.815 ms
GPU 3, Compute+Comm Time: 260.312 ms, Bubble Time: 195.481 ms, Imbalance Overhead: 3.254 ms
    The estimated cost with 2 DP ways is 739.836 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1359.833 ms
Partition 0 [0, 16) has cost: 1359.833 ms
Partition 1 [16, 32) has cost: 1335.340 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 275.139 ms
GPU 0, Compute+Comm Time: 179.628 ms, Bubble Time: 81.422 ms, Imbalance Overhead: 14.088 ms
GPU 1, Compute+Comm Time: 177.788 ms, Bubble Time: 97.351 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 460.304 ms
GPU 0, Compute+Comm Time: 299.670 ms, Bubble Time: 160.634 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 304.321 ms, Bubble Time: 141.568 ms, Imbalance Overhead: 14.415 ms
    The estimated cost with 4 DP ways is 772.215 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2695.173 ms
Partition 0 [0, 32) has cost: 2695.173 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 490.908 ms
GPU 0, Compute+Comm Time: 490.908 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 615.928 ms
GPU 0, Compute+Comm Time: 615.928 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1162.177 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 287)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 111539, Num Local Vertices: 39334
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 287)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 31876
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 287)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 150873, Num Local Vertices: 24150
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 287)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 31876, Num Local Vertices: 24165
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 287)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 175023, Num Local Vertices: 31312
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 287)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 56041, Num Local Vertices: 30922
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 206335, Num Local Vertices: 26630
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 287)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 86963, Num Local Vertices: 24576
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 287)...
+++++++++ Node 0 initializing the weights for op[0, 287)...
+++++++++ Node 6 initializing the weights for op[0, 287)...
+++++++++ Node 1 initializing the weights for op[0, 287)...
+++++++++ Node 7 initializing the weights for op[0, 287)...
+++++++++ Node 5 initializing the weights for op[0, 287)...
+++++++++ Node 2 initializing the weights for op[0, 287)...
+++++++++ Node 3 initializing the weights for op[0, 287)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 1094887
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 1.3459	TrainAcc 0.7036	ValidAcc 0.7300	TestAcc 0.7245	BestValid 0.7300
	Epoch 50:	Loss 0.5402	TrainAcc 0.8953	ValidAcc 0.9047	TestAcc 0.9026	BestValid 0.9047
	Epoch 75:	Loss 0.3817	TrainAcc 0.9315	ValidAcc 0.9384	TestAcc 0.9366	BestValid 0.9384
	Epoch 100:	Loss 0.3254	TrainAcc 0.9398	ValidAcc 0.9455	TestAcc 0.9441	BestValid 0.9455
	Epoch 125:	Loss 0.2969	TrainAcc 0.9433	ValidAcc 0.9485	TestAcc 0.9470	BestValid 0.9485
	Epoch 150:	Loss 0.2796	TrainAcc 0.9462	ValidAcc 0.9507	TestAcc 0.9497	BestValid 0.9507
	Epoch 175:	Loss 0.2642	TrainAcc 0.9483	ValidAcc 0.9517	TestAcc 0.9519	BestValid 0.9517
	Epoch 200:	Loss 0.2555	TrainAcc 0.9500	ValidAcc 0.9530	TestAcc 0.9529	BestValid 0.9530
	Epoch 225:	Loss 0.2471	TrainAcc 0.9514	ValidAcc 0.9542	TestAcc 0.9538	BestValid 0.9542
	Epoch 250:	Loss 0.2389	TrainAcc 0.9523	ValidAcc 0.9545	TestAcc 0.9541	BestValid 0.9545
	Epoch 275:	Loss 0.2327	TrainAcc 0.9533	ValidAcc 0.9552	TestAcc 0.9548	BestValid 0.9552
	Epoch 300:	Loss 0.2277	TrainAcc 0.9539	ValidAcc 0.9559	TestAcc 0.9554	BestValid 0.9559
	Epoch 325:	Loss 0.2234	TrainAcc 0.9549	ValidAcc 0.9564	TestAcc 0.9562	BestValid 0.9564
	Epoch 350:	Loss 0.2190	TrainAcc 0.9553	ValidAcc 0.9569	TestAcc 0.9564	BestValid 0.9569
	Epoch 375:	Loss 0.2171	TrainAcc 0.9560	ValidAcc 0.9569	TestAcc 0.9570	BestValid 0.9569
	Epoch 400:	Loss 0.2135	TrainAcc 0.9568	ValidAcc 0.9574	TestAcc 0.9574	BestValid 0.9574
	Epoch 425:	Loss 0.2121	TrainAcc 0.9572	ValidAcc 0.9578	TestAcc 0.9575	BestValid 0.9578
	Epoch 450:	Loss 0.2100	TrainAcc 0.9579	ValidAcc 0.9584	TestAcc 0.9579	BestValid 0.9584
	Epoch 475:	Loss 0.2058	TrainAcc 0.9581	ValidAcc 0.9587	TestAcc 0.9580	BestValid 0.9587
	Epoch 500:	Loss 0.2025	TrainAcc 0.9587	ValidAcc 0.9587	TestAcc 0.9585	BestValid 0.9587
	Epoch 525:	Loss 0.2013	TrainAcc 0.9589	ValidAcc 0.9593	TestAcc 0.9584	BestValid 0.9593
	Epoch 550:	Loss 0.1999	TrainAcc 0.9595	ValidAcc 0.9594	TestAcc 0.9588	BestValid 0.9594
	Epoch 575:	Loss 0.1976	TrainAcc 0.9596	ValidAcc 0.9595	TestAcc 0.9588	BestValid 0.9595
	Epoch 600:	Loss 0.1947	TrainAcc 0.9599	ValidAcc 0.9600	TestAcc 0.9590	BestValid 0.9600
	Epoch 625:	Loss 0.1941	TrainAcc 0.9601	ValidAcc 0.9599	TestAcc 0.9591	BestValid 0.9600
	Epoch 650:	Loss 0.1907	TrainAcc 0.9604	ValidAcc 0.9602	TestAcc 0.9594	BestValid 0.9602
	Epoch 675:	Loss 0.1899	TrainAcc 0.9608	ValidAcc 0.9603	TestAcc 0.9597	BestValid 0.9603
	Epoch 700:	Loss 0.1900	TrainAcc 0.9611	ValidAcc 0.9611	TestAcc 0.9599	BestValid 0.9611
	Epoch 725:	Loss 0.1880	TrainAcc 0.9614	ValidAcc 0.9609	TestAcc 0.9599	BestValid 0.9611
	Epoch 750:	Loss 0.1863	TrainAcc 0.9617	ValidAcc 0.9610	TestAcc 0.9601	BestValid 0.9611
	Epoch 775:	Loss 0.1839	TrainAcc 0.9619	ValidAcc 0.9609	TestAcc 0.9600	BestValid 0.9611
	Epoch 800:	Loss 0.1850	TrainAcc 0.9622	ValidAcc 0.9609	TestAcc 0.9602	BestValid 0.9611
	Epoch 825:	Loss 0.1823	TrainAcc 0.9625	ValidAcc 0.9612	TestAcc 0.9604	BestValid 0.9612
	Epoch 850:	Loss 0.1809	TrainAcc 0.9625	ValidAcc 0.9611	TestAcc 0.9604	BestValid 0.9612
	Epoch 875:	Loss 0.1794	TrainAcc 0.9629	ValidAcc 0.9616	TestAcc 0.9606	BestValid 0.9616
	Epoch 900:	Loss 0.1795	TrainAcc 0.9631	ValidAcc 0.9611	TestAcc 0.9607	BestValid 0.9616
	Epoch 925:	Loss 0.1795	TrainAcc 0.9635	ValidAcc 0.9617	TestAcc 0.9609	BestValid 0.9617
	Epoch 950:	Loss 0.1765	TrainAcc 0.9634	ValidAcc 0.9619	TestAcc 0.9609	BestValid 0.9619
	Epoch 975:	Loss 0.1766	TrainAcc 0.9640	ValidAcc 0.9616	TestAcc 0.9610	BestValid 0.9619
	Epoch 1000:	Loss 0.1733	TrainAcc 0.9642	ValidAcc 0.9619	TestAcc 0.9612	BestValid 0.9619
	Epoch 1025:	Loss 0.1733	TrainAcc 0.9642	ValidAcc 0.9616	TestAcc 0.9613	BestValid 0.9619
	Epoch 1050:	Loss 0.1719	TrainAcc 0.9643	ValidAcc 0.9617	TestAcc 0.9612	BestValid 0.9619
	Epoch 1075:	Loss 0.1724	TrainAcc 0.9647	ValidAcc 0.9621	TestAcc 0.9615	BestValid 0.9621
	Epoch 1100:	Loss 0.1712	TrainAcc 0.9648	ValidAcc 0.9618	TestAcc 0.9615	BestValid 0.9621
	Epoch 1125:	Loss 0.1703	TrainAcc 0.9650	ValidAcc 0.9622	TestAcc 0.9616	BestValid 0.9622
	Epoch 1150:	Loss 0.1687	TrainAcc 0.9653	ValidAcc 0.9622	TestAcc 0.9619	BestValid 0.9622
	Epoch 1175:	Loss 0.1685	TrainAcc 0.9652	ValidAcc 0.9624	TestAcc 0.9618	BestValid 0.9624
	Epoch 1200:	Loss 0.1686	TrainAcc 0.9658	ValidAcc 0.9626	TestAcc 0.9620	BestValid 0.9626
	Epoch 1225:	Loss 0.1655	TrainAcc 0.9656	ValidAcc 0.9625	TestAcc 0.9620	BestValid 0.9626
	Epoch 1250:	Loss 0.1644	TrainAcc 0.9659	ValidAcc 0.9619	TestAcc 0.9621	BestValid 0.9626
	Epoch 1275:	Loss 0.1637	TrainAcc 0.9661	ValidAcc 0.9623	TestAcc 0.9621	BestValid 0.9626
	Epoch 1300:	Loss 0.1656	TrainAcc 0.9662	ValidAcc 0.9624	TestAcc 0.9621	BestValid 0.9626
	Epoch 1325:	Loss 0.1618	TrainAcc 0.9663	ValidAcc 0.9630	TestAcc 0.9623	BestValid 0.9630
	Epoch 1350:	Loss 0.1639	TrainAcc 0.9664	ValidAcc 0.9626	TestAcc 0.9624	BestValid 0.9630
	Epoch 1375:	Loss 0.1630	TrainAcc 0.9666	ValidAcc 0.9628	TestAcc 0.9622	BestValid 0.9630
	Epoch 1400:	Loss 0.1617	TrainAcc 0.9670	ValidAcc 0.9628	TestAcc 0.9626	BestValid 0.9630
	Epoch 1425:	Loss 0.1605	TrainAcc 0.9671	ValidAcc 0.9629	TestAcc 0.9625	BestValid 0.9630
	Epoch 1450:	Loss 0.1594	TrainAcc 0.9670	ValidAcc 0.9628	TestAcc 0.9625	BestValid 0.9630
	Epoch 1475:	Loss 0.1589	TrainAcc 0.9672	ValidAcc 0.9629	TestAcc 0.9627	BestValid 0.9630
	Epoch 1500:	Loss 0.1594	TrainAcc 0.9673	ValidAcc 0.9632	TestAcc 0.9625	BestValid 0.9632
	Epoch 1525:	Loss 0.1591	TrainAcc 0.9677	ValidAcc 0.9631	TestAcc 0.9629	BestValid 0.9632
	Epoch 1550:	Loss 0.1576	TrainAcc 0.9676	ValidAcc 0.9631	TestAcc 0.9629	BestValid 0.9632
	Epoch 1575:	Loss 0.1582	TrainAcc 0.9680	ValidAcc 0.9633	TestAcc 0.9630	BestValid 0.9633
	Epoch 1600:	Loss 0.1579	TrainAcc 0.9680	ValidAcc 0.9633	TestAcc 0.9630	BestValid 0.9633
	Epoch 1625:	Loss 0.1548	TrainAcc 0.9682	ValidAcc 0.9635	TestAcc 0.9631	BestValid 0.9635
	Epoch 1650:	Loss 0.1558	TrainAcc 0.9681	ValidAcc 0.9632	TestAcc 0.9630	BestValid 0.9635
	Epoch 1675:	Loss 0.1548	TrainAcc 0.9686	ValidAcc 0.9637	TestAcc 0.9633	BestValid 0.9637
	Epoch 1700:	Loss 0.1552	TrainAcc 0.9684	ValidAcc 0.9633	TestAcc 0.9629	BestValid 0.9637
	Epoch 1725:	Loss 0.1534	TrainAcc 0.9689	ValidAcc 0.9639	TestAcc 0.9632	BestValid 0.9639
	Epoch 1750:	Loss 0.1522	TrainAcc 0.9689	ValidAcc 0.9634	TestAcc 0.9634	BestValid 0.9639
	Epoch 1775:	Loss 0.1521	TrainAcc 0.9690	ValidAcc 0.9637	TestAcc 0.9634	BestValid 0.9639
	Epoch 1800:	Loss 0.1515	TrainAcc 0.9689	ValidAcc 0.9635	TestAcc 0.9633	BestValid 0.9639
	Epoch 1825:	Loss 0.1526	TrainAcc 0.9692	ValidAcc 0.9636	TestAcc 0.9633	BestValid 0.9639
	Epoch 1850:	Loss 0.1513	TrainAcc 0.9694	ValidAcc 0.9642	TestAcc 0.9635	BestValid 0.9642
	Epoch 1875:	Loss 0.1491	TrainAcc 0.9694	ValidAcc 0.9640	TestAcc 0.9639	BestValid 0.9642
	Epoch 1900:	Loss 0.1490	TrainAcc 0.9695	ValidAcc 0.9637	TestAcc 0.9636	BestValid 0.9642
	Epoch 1925:	Loss 0.1503	TrainAcc 0.9697	ValidAcc 0.9641	TestAcc 0.9638	BestValid 0.9642
	Epoch 1950:	Loss 0.1467	TrainAcc 0.9697	ValidAcc 0.9640	TestAcc 0.9636	BestValid 0.9642
	Epoch 1975:	Loss 0.1490	TrainAcc 0.9700	ValidAcc 0.9644	TestAcc 0.9640	BestValid 0.9644
	Epoch 2000:	Loss 0.1470	TrainAcc 0.9699	ValidAcc 0.9642	TestAcc 0.9639	BestValid 0.9644
	Epoch 2025:	Loss 0.1475	TrainAcc 0.9700	ValidAcc 0.9639	TestAcc 0.9641	BestValid 0.9644
	Epoch 2050:	Loss 0.1466	TrainAcc 0.9702	ValidAcc 0.9640	TestAcc 0.9642	BestValid 0.9644
	Epoch 2075:	Loss 0.1471	TrainAcc 0.9703	ValidAcc 0.9641	TestAcc 0.9639	BestValid 0.9644
	Epoch 2100:	Loss 0.1454	TrainAcc 0.9703	ValidAcc 0.9639	TestAcc 0.9639	BestValid 0.9644
	Epoch 2125:	Loss 0.1453	TrainAcc 0.9706	ValidAcc 0.9642	TestAcc 0.9642	BestValid 0.9644
	Epoch 2150:	Loss 0.1449	TrainAcc 0.9705	ValidAcc 0.9642	TestAcc 0.9643	BestValid 0.9644
	Epoch 2175:	Loss 0.1440	TrainAcc 0.9708	ValidAcc 0.9647	TestAcc 0.9643	BestValid 0.9647
	Epoch 2200:	Loss 0.1440	TrainAcc 0.9708	ValidAcc 0.9645	TestAcc 0.9642	BestValid 0.9647
	Epoch 2225:	Loss 0.1438	TrainAcc 0.9710	ValidAcc 0.9645	TestAcc 0.9643	BestValid 0.9647
	Epoch 2250:	Loss 0.1431	TrainAcc 0.9711	ValidAcc 0.9645	TestAcc 0.9642	BestValid 0.9647
	Epoch 2275:	Loss 0.1426	TrainAcc 0.9713	ValidAcc 0.9646	TestAcc 0.9645	BestValid 0.9647
	Epoch 2300:	Loss 0.1421	TrainAcc 0.9712	ValidAcc 0.9643	TestAcc 0.9642	BestValid 0.9647
	Epoch 2325:	Loss 0.1418	TrainAcc 0.9713	ValidAcc 0.9645	TestAcc 0.9644	BestValid 0.9647
	Epoch 2350:	Loss 0.1410	TrainAcc 0.9714	ValidAcc 0.9645	TestAcc 0.9643	BestValid 0.9647
	Epoch 2375:	Loss 0.1400	TrainAcc 0.9714	ValidAcc 0.9650	TestAcc 0.9643	BestValid 0.9650
	Epoch 2400:	Loss 0.1408	TrainAcc 0.9717	ValidAcc 0.9647	TestAcc 0.9645	BestValid 0.9650
	Epoch 2425:	Loss 0.1388	TrainAcc 0.9715	ValidAcc 0.9645	TestAcc 0.9644	BestValid 0.9650
	Epoch 2450:	Loss 0.1404	TrainAcc 0.9718	ValidAcc 0.9651	TestAcc 0.9644	BestValid 0.9651
	Epoch 2475:	Loss 0.1389	TrainAcc 0.9719	ValidAcc 0.9651	TestAcc 0.9646	BestValid 0.9651
	Epoch 2500:	Loss 0.1383	TrainAcc 0.9721	ValidAcc 0.9650	TestAcc 0.9647	BestValid 0.9651
	Epoch 2525:	Loss 0.1383	TrainAcc 0.9721	ValidAcc 0.9648	TestAcc 0.9645	BestValid 0.9651
	Epoch 2550:	Loss 0.1386	TrainAcc 0.9722	ValidAcc 0.9648	TestAcc 0.9646	BestValid 0.9651
	Epoch 2575:	Loss 0.1380	TrainAcc 0.9724	ValidAcc 0.9652	TestAcc 0.9648	BestValid 0.9652
	Epoch 2600:	Loss 0.1371	TrainAcc 0.9724	ValidAcc 0.9653	TestAcc 0.9647	BestValid 0.9653
	Epoch 2625:	Loss 0.1364	TrainAcc 0.9726	ValidAcc 0.9654	TestAcc 0.9650	BestValid 0.9654
	Epoch 2650:	Loss 0.1361	TrainAcc 0.9726	ValidAcc 0.9650	TestAcc 0.9647	BestValid 0.9654
	Epoch 2675:	Loss 0.1375	TrainAcc 0.9727	ValidAcc 0.9654	TestAcc 0.9648	BestValid 0.9654
	Epoch 2700:	Loss 0.1365	TrainAcc 0.9728	ValidAcc 0.9657	TestAcc 0.9649	BestValid 0.9657
	Epoch 2725:	Loss 0.1349	TrainAcc 0.9726	ValidAcc 0.9650	TestAcc 0.9648	BestValid 0.9657
	Epoch 2750:	Loss 0.1338	TrainAcc 0.9730	ValidAcc 0.9652	TestAcc 0.9647	BestValid 0.9657
	Epoch 2775:	Loss 0.1347	TrainAcc 0.9731	ValidAcc 0.9653	TestAcc 0.9649	BestValid 0.9657
	Epoch 2800:	Loss 0.1356	TrainAcc 0.9733	ValidAcc 0.9658	TestAcc 0.9650	BestValid 0.9658
	Epoch 2825:	Loss 0.1337	TrainAcc 0.9734	ValidAcc 0.9656	TestAcc 0.9651	BestValid 0.9658
	Epoch 2850:	Loss 0.1326	TrainAcc 0.9733	ValidAcc 0.9657	TestAcc 0.9649	BestValid 0.9658
	Epoch 2875:	Loss 0.1322	TrainAcc 0.9736	ValidAcc 0.9658	TestAcc 0.9654	BestValid 0.9658
	Epoch 2900:	Loss 0.1332	TrainAcc 0.9735	ValidAcc 0.9659	TestAcc 0.9650	BestValid 0.9659
	Epoch 2925:	Loss 0.1312	TrainAcc 0.9737	ValidAcc 0.9657	TestAcc 0.9651	BestValid 0.9659
	Epoch 2950:	Loss 0.1335	TrainAcc 0.9738	ValidAcc 0.9658	TestAcc 0.9653	BestValid 0.9659
	Epoch 2975:	Loss 0.1339	TrainAcc 0.9738	ValidAcc 0.9656	TestAcc 0.9652	BestValid 0.9659
	Epoch 3000:	Loss 0.1311	TrainAcc 0.9740	ValidAcc 0.9658	TestAcc 0.9654	BestValid 0.9659
	Epoch 3025:	Loss 0.1330	TrainAcc 0.9737	ValidAcc 0.9654	TestAcc 0.9653	BestValid 0.9659
	Epoch 3050:	Loss 0.1305	TrainAcc 0.9740	ValidAcc 0.9658	TestAcc 0.9654	BestValid 0.9659
	Epoch 3075:	Loss 0.1305	TrainAcc 0.9741	ValidAcc 0.9659	TestAcc 0.9654	BestValid 0.9659
	Epoch 3100:	Loss 0.1311	TrainAcc 0.9742	ValidAcc 0.9659	TestAcc 0.9655	BestValid 0.9659
	Epoch 3125:	Loss 0.1313	TrainAcc 0.9743	ValidAcc 0.9659	TestAcc 0.9654	BestValid 0.9659
	Epoch 3150:	Loss 0.1281	TrainAcc 0.9744	ValidAcc 0.9659	TestAcc 0.9654	BestValid 0.9659
	Epoch 3175:	Loss 0.1293	TrainAcc 0.9745	ValidAcc 0.9658	TestAcc 0.9655	BestValid 0.9659
	Epoch 3200:	Loss 0.1299	TrainAcc 0.9746	ValidAcc 0.9659	TestAcc 0.9656	BestValid 0.9659
	Epoch 3225:	Loss 0.1294	TrainAcc 0.9745	ValidAcc 0.9665	TestAcc 0.9656	BestValid 0.9665
	Epoch 3250:	Loss 0.1287	TrainAcc 0.9745	ValidAcc 0.9661	TestAcc 0.9653	BestValid 0.9665
	Epoch 3275:	Loss 0.1280	TrainAcc 0.9748	ValidAcc 0.9660	TestAcc 0.9655	BestValid 0.9665
	Epoch 3300:	Loss 0.1276	TrainAcc 0.9747	ValidAcc 0.9661	TestAcc 0.9656	BestValid 0.9665
	Epoch 3325:	Loss 0.1287	TrainAcc 0.9750	ValidAcc 0.9663	TestAcc 0.9657	BestValid 0.9665
	Epoch 3350:	Loss 0.1289	TrainAcc 0.9748	ValidAcc 0.9663	TestAcc 0.9655	BestValid 0.9665
	Epoch 3375:	Loss 0.1262	TrainAcc 0.9750	ValidAcc 0.9660	TestAcc 0.9658	BestValid 0.9665
	Epoch 3400:	Loss 0.1264	TrainAcc 0.9748	ValidAcc 0.9658	TestAcc 0.9655	BestValid 0.9665
	Epoch 3425:	Loss 0.1267	TrainAcc 0.9753	ValidAcc 0.9660	TestAcc 0.9656	BestValid 0.9665
	Epoch 3450:	Loss 0.1266	TrainAcc 0.9752	ValidAcc 0.9661	TestAcc 0.9657	BestValid 0.9665
	Epoch 3475:	Loss 0.1266	TrainAcc 0.9753	ValidAcc 0.9666	TestAcc 0.9656	BestValid 0.9666
	Epoch 3500:	Loss 0.1264	TrainAcc 0.9754	ValidAcc 0.9662	TestAcc 0.9656	BestValid 0.9666
	Epoch 3525:	Loss 0.1251	TrainAcc 0.9756	ValidAcc 0.9659	TestAcc 0.9657	BestValid 0.9666
	Epoch 3550:	Loss 0.1259	TrainAcc 0.9756	ValidAcc 0.9662	TestAcc 0.9657	BestValid 0.9666
	Epoch 3575:	Loss 0.1258	TrainAcc 0.9755	ValidAcc 0.9664	TestAcc 0.9657	BestValid 0.9666
	Epoch 3600:	Loss 0.1253	TrainAcc 0.9756	ValidAcc 0.9661	TestAcc 0.9658	BestValid 0.9666
	Epoch 3625:	Loss 0.1248	TrainAcc 0.9759	ValidAcc 0.9661	TestAcc 0.9658	BestValid 0.9666
	Epoch 3650:	Loss 0.1260	TrainAcc 0.9759	ValidAcc 0.9663	TestAcc 0.9658	BestValid 0.9666
	Epoch 3675:	Loss 0.1248	TrainAcc 0.9759	ValidAcc 0.9661	TestAcc 0.9656	BestValid 0.9666
	Epoch 3700:	Loss 0.1235	TrainAcc 0.9764	ValidAcc 0.9663	TestAcc 0.9658	BestValid 0.9666
	Epoch 3725:	Loss 0.1228	TrainAcc 0.9762	ValidAcc 0.9665	TestAcc 0.9659	BestValid 0.9666
	Epoch 3750:	Loss 0.1238	TrainAcc 0.9759	ValidAcc 0.9660	TestAcc 0.9657	BestValid 0.9666
	Epoch 3775:	Loss 0.1235	TrainAcc 0.9759	ValidAcc 0.9662	TestAcc 0.9656	BestValid 0.9666
	Epoch 3800:	Loss 0.1237	TrainAcc 0.9761	ValidAcc 0.9661	TestAcc 0.9656	BestValid 0.9666
	Epoch 3825:	Loss 0.1233	TrainAcc 0.9766	ValidAcc 0.9663	TestAcc 0.9660	BestValid 0.9666
	Epoch 3850:	Loss 0.1212	TrainAcc 0.9766	ValidAcc 0.9666	TestAcc 0.9659	BestValid 0.9666
	Epoch 3875:	Loss 0.1214	TrainAcc 0.9765	ValidAcc 0.9661	TestAcc 0.9660	BestValid 0.9666
	Epoch 3900:	Loss 0.1222	TrainAcc 0.9765	ValidAcc 0.9663	TestAcc 0.9658	BestValid 0.9666
	Epoch 3925:	Loss 0.1219	TrainAcc 0.9766	ValidAcc 0.9664	TestAcc 0.9657	BestValid 0.9666
	Epoch 3950:	Loss 0.1219	TrainAcc 0.9767	ValidAcc 0.9664	TestAcc 0.9661	BestValid 0.9666
	Epoch 3975:	Loss 0.1218	TrainAcc 0.9766	ValidAcc 0.9661	TestAcc 0.9658	BestValid 0.9666
	Epoch 4000:	Loss 0.1201	TrainAcc 0.9769	ValidAcc 0.9667	TestAcc 0.9659	BestValid 0.9667
	Epoch 4025:	Loss 0.1194	TrainAcc 0.9770	ValidAcc 0.9666	TestAcc 0.9660	BestValid 0.9667
	Epoch 4050:	Loss 0.1205	TrainAcc 0.9767	ValidAcc 0.9662	TestAcc 0.9659	BestValid 0.9667
	Epoch 4075:	Loss 0.1216	TrainAcc 0.9771	ValidAcc 0.9664	TestAcc 0.9658	BestValid 0.9667
	Epoch 4100:	Loss 0.1206	TrainAcc 0.9771	ValidAcc 0.9667	TestAcc 0.9660	BestValid 0.9667
	Epoch 4125:	Loss 0.1197	TrainAcc 0.9773	ValidAcc 0.9665	TestAcc 0.9658	BestValid 0.9667
	Epoch 4150:	Loss 0.1191	TrainAcc 0.9771	ValidAcc 0.9664	TestAcc 0.9661	BestValid 0.9667
	Epoch 4175:	Loss 0.1199	TrainAcc 0.9775	ValidAcc 0.9671	TestAcc 0.9662	BestValid 0.9671
	Epoch 4200:	Loss 0.1198	TrainAcc 0.9774	ValidAcc 0.9668	TestAcc 0.9663	BestValid 0.9671
	Epoch 4225:	Loss 0.1189	TrainAcc 0.9775	ValidAcc 0.9668	TestAcc 0.9659	BestValid 0.9671
	Epoch 4250:	Loss 0.1180	TrainAcc 0.9777	ValidAcc 0.9669	TestAcc 0.9660	BestValid 0.9671
	Epoch 4275:	Loss 0.1180	TrainAcc 0.9774	ValidAcc 0.9669	TestAcc 0.9661	BestValid 0.9671
	Epoch 4300:	Loss 0.1186	TrainAcc 0.9778	ValidAcc 0.9669	TestAcc 0.9664	BestValid 0.9671
	Epoch 4325:	Loss 0.1180	TrainAcc 0.9779	ValidAcc 0.9673	TestAcc 0.9664	BestValid 0.9673
	Epoch 4350:	Loss 0.1178	TrainAcc 0.9774	ValidAcc 0.9668	TestAcc 0.9662	BestValid 0.9673
	Epoch 4375:	Loss 0.1178	TrainAcc 0.9778	ValidAcc 0.9668	TestAcc 0.9663	BestValid 0.9673
	Epoch 4400:	Loss 0.1179	TrainAcc 0.9778	ValidAcc 0.9669	TestAcc 0.9662	BestValid 0.9673
	Epoch 4425:	Loss 0.1164	TrainAcc 0.9779	ValidAcc 0.9669	TestAcc 0.9664	BestValid 0.9673
	Epoch 4450:	Loss 0.1173	TrainAcc 0.9779	ValidAcc 0.9670	TestAcc 0.9664	BestValid 0.9673
	Epoch 4475:	Loss 0.1187	TrainAcc 0.9782	ValidAcc 0.9671	TestAcc 0.9666	BestValid 0.9673
	Epoch 4500:	Loss 0.1157	TrainAcc 0.9780	ValidAcc 0.9667	TestAcc 0.9663	BestValid 0.9673
	Epoch 4525:	Loss 0.1168	TrainAcc 0.9780	ValidAcc 0.9668	TestAcc 0.9665	BestValid 0.9673
	Epoch 4550:	Loss 0.1174	TrainAcc 0.9782	ValidAcc 0.9671	TestAcc 0.9668	BestValid 0.9673
	Epoch 4575:	Loss 0.1178	TrainAcc 0.9780	ValidAcc 0.9669	TestAcc 0.9663	BestValid 0.9673
	Epoch 4600:	Loss 0.1154	TrainAcc 0.9782	ValidAcc 0.9670	TestAcc 0.9664	BestValid 0.9673
	Epoch 4625:	Loss 0.1163	TrainAcc 0.9784	ValidAcc 0.9668	TestAcc 0.9665	BestValid 0.9673
	Epoch 4650:	Loss 0.1139	TrainAcc 0.9782	ValidAcc 0.9669	TestAcc 0.9664	BestValid 0.9673
	Epoch 4675:	Loss 0.1150	TrainAcc 0.9784	ValidAcc 0.9669	TestAcc 0.9664	BestValid 0.9673
	Epoch 4700:	Loss 0.1159	TrainAcc 0.9785	ValidAcc 0.9669	TestAcc 0.9664	BestValid 0.9673
	Epoch 4725:	Loss 0.1152	TrainAcc 0.9784	ValidAcc 0.9668	TestAcc 0.9666	BestValid 0.9673
	Epoch 4750:	Loss 0.1139	TrainAcc 0.9786	ValidAcc 0.9671	TestAcc 0.9667	BestValid 0.9673
	Epoch 4775:	Loss 0.1141	TrainAcc 0.9785	ValidAcc 0.9671	TestAcc 0.9666	BestValid 0.9673
	Epoch 4800:	Loss 0.1139	TrainAcc 0.9788	ValidAcc 0.9674	TestAcc 0.9666	BestValid 0.9674
	Epoch 4825:	Loss 0.1144	TrainAcc 0.9790	ValidAcc 0.9673	TestAcc 0.9668	BestValid 0.9674
	Epoch 4850:	Loss 0.1134	TrainAcc 0.9787	ValidAcc 0.9667	TestAcc 0.9667	BestValid 0.9674
	Epoch 4875:	Loss 0.1125	TrainAcc 0.9791	ValidAcc 0.9670	TestAcc 0.9666	BestValid 0.9674
	Epoch 4900:	Loss 0.1126	TrainAcc 0.9791	ValidAcc 0.9673	TestAcc 0.9668	BestValid 0.9674
	Epoch 4925:	Loss 0.1145	TrainAcc 0.9790	ValidAcc 0.9671	TestAcc 0.9668	BestValid 0.9674
	Epoch 4950:	Loss 0.1139	TrainAcc 0.9792	ValidAcc 0.9674	TestAcc 0.9666	BestValid 0.9674
	Epoch 4975:	Loss 0.1132	TrainAcc 0.9794	ValidAcc 0.9672	TestAcc 0.9667	BestValid 0.9674
Node 4, Pre/Post-Pipelining: 8.708 / 22.793 ms, Bubble: 0.403 ms, Compute: 1074.689 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
	Epoch 5000:	Loss 0.1132	TrainAcc 0.9787	ValidAcc 0.9667	TestAcc 0.9667	BestValid 0.9674
Node 5, Pre/Post-Pipelining: 8.707 / 22.756 ms, Bubble: 0.472 ms, Compute: 1074.662 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 0, Pre/Post-Pipelining: 8.702 / 22.699 ms, Bubble: 0.489 ms, Compute: 1074.704 ms, Comm: 0.008 ms, Imbalance: 0.018 ms
Node 6, Pre/Post-Pipelining: 8.712 / 22.841 ms, Bubble: 0.155 ms, Compute: 1074.888 ms, Comm: 0.011 ms, Imbalance: 0.019 ms
Node 1, Pre/Post-Pipelining: 8.703 / 22.745 ms, Bubble: 0.518 ms, Compute: 1074.629 ms, Comm: 0.009 ms, Imbalance: 0.018 ms
Node 7, Pre/Post-Pipelining: 8.700 / 22.742 ms, Bubble: 0.314 ms, Compute: 1074.835 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 2, Pre/Post-Pipelining: 8.710 / 22.755 ms, Bubble: 0.067 ms, Compute: 1075.069 ms, Comm: 0.008 ms, Imbalance: 0.017 ms
Node 3, Pre/Post-Pipelining: 8.703 / 23.152 ms, Bubble: 0.068 ms, Compute: 1074.674 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.702 ms
Cluster-Wide Average, Post-Pipelining Overhead: 22.699 ms
Cluster-Wide Average, Bubble: 0.489 ms
Cluster-Wide Average, Compute: 1074.704 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.018 ms
Node 4, GPU memory consumption: 18.491 GB
Node 0, GPU memory consumption: 19.233 GB
Node 5, GPU memory consumption: 18.467 GB
Node 2, GPU memory consumption: 18.489 GB
Node 6, GPU memory consumption: 18.491 GB
Node 1, GPU memory consumption: 18.467 GB
Node 7, GPU memory consumption: 18.454 GB
Node 3, GPU memory consumption: 18.446 GB
Node 4, Graph-Level Communication Throughput: 37.591 Gbps, Time: 681.234 ms
Node 0, Graph-Level Communication Throughput: 46.440 Gbps, Time: 696.184 ms
Node 5, Graph-Level Communication Throughput: 36.999 Gbps, Time: 686.869 ms
Node 1, Graph-Level Communication Throughput: 31.548 Gbps, Time: 692.507 ms
Node 6, Graph-Level Communication Throughput: 48.621 Gbps, Time: 643.521 ms
Node 2, Graph-Level Communication Throughput: 49.500 Gbps, Time: 670.755 ms
Node 7, Graph-Level Communication Throughput: 40.755 Gbps, Time: 670.894 ms
Node 3, Graph-Level Communication Throughput: 40.338 Gbps, Time: 674.312 ms
------------------------node id 4,  per-epoch time: 1.633865 s---------------
------------------------node id 0,  per-epoch time: 1.633864 s---------------
------------------------node id 5,  per-epoch time: 1.633865 s---------------
------------------------node id 1,  per-epoch time: 1.633864 s---------------
------------------------node id 6,  per-epoch time: 1.633865 s---------------
------------------------node id 2,  per-epoch time: 1.633864 s---------------
------------------------node id 7,  per-epoch time: 1.633865 s---------------
------------------------node id 3,  per-epoch time: 1.633864 s---------------
************ Profiling Results ************
	Bubble: 558.848242 (ms) (34.21 percentage)
	Compute: 347.785880 (ms) (21.29 percentage)
	GraphCommComputeOverhead: 32.044916 (ms) (1.96 percentage)
	GraphCommNetwork: 677.032868 (ms) (41.45 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 17.643222 (ms) (1.08 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 26.104 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.022 GB
	Total communication (cluster-wide, per-epoch): 26.126 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.9674
Target test_acc: 0.9666
Epoch to reach the target acc: 4949
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
