g000.anvil.rcac.purdue.edu
Thu Jan 26 02:23:02 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |
| N/A   33C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 18%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 43%] Built target test_cuda_data_compression
[ 43%] Built target test_mpi_gpu_hybrid
[ 53%] Built target test_cuda_pipeline_parallel
[ 50%] Built target test_hello_world
[ 57%] Built target test_cuda_graph
[ 57%] Built target test_mpi_gpu_pipelined_model_parallel
[ 57%] Built target test_cuda
[ 58%] Built target test_mpi_gpu_model_parallel
[ 60%] Built target test_cuda_model_parallel
[ 60%] Built target test_mpi_combined
[ 64%] Built target test_nccl_thread
[ 66%] Built target test_trivial
[ 67%] Built target test_graph
[ 68%] Built target test_mpi_structual_graph
[ 77%] Built target test_mpi_non_structual_graph
[ 77%] Built target test_mpi_loader
[ 78%] Built target test_single_node_fullgpu_training
[ 80%] Built target test_full_non_structual_graph
[ 81%] Built target test_full_structual_graph
[ 86%] Built target test_single_node_training
[ 86%] Built target test_mpi_pipelined_model_parallel
[ 89%] Built target test_single_node_gpu_training
[ 89%] Built target test_nccl_mpi
[ 90%] Built target estimate_comm_volume
[ 91%] Built target test_mpi_model_parallel
[100%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target test_two_layer_hybrid_parallelism_designer
[100%] Built target OSDI2023_SINGLE_NODE_gcn
[100%] Built target OSDI2023_SINGLE_NODE_gcn_inference
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
Initialized node 0 on machine g000.anvil.rcac.purdue.edu
Initialized node 3 on machine g008.anvil.rcac.purdue.edu
Initialized node 2 on machine g007.anvil.rcac.purdue.edu
Initialized node 1 on machine g002.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.893 seconds.
Building the CSC structure...
        It takes 1.954 seconds.
Building the CSC structure...
        It takes 1.959 seconds.
Building the CSC structure...
        It takes 1.987 seconds.
Building the CSC structure...
        It takes 1.825 seconds.
        It takes 1.886 seconds.
        It takes 1.888 seconds.
        It takes 1.881 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.648 seconds.
Building the Label Vector...
        It takes 0.669 seconds.
Building the Label Vector...
        It takes 0.680 seconds.
Building the Label Vector...
        It takes 0.600 seconds.
Building the Label Vector...
        It takes 0.408 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.408 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.393 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.328 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 2449029)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
Boundaries: 0 0 0 0 2449029 2449029 2449029 2449029
Fragments: [0, 2449029)
Chunks (number of global chunks: 16): 0-[0, 153065) 1-[153065, 306130) 2-[306130, 459195) 3-[459195, 612260) 4-[612260, 765325) 5-[765325, 918390) 6-[918390, 1071455) 7-[1071455, 1224520) 8-[1224520, 1377585) ... 15-[2295975, 2449029)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [11, 16) x [0, 2449029)
*** Node 2, constructing the helper classes...
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 3, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [16, 20) x [0, 2449029)
*** Node 3, constructing the helper classes...
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 15)
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 15)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes:
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 1, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 2449029)
*** Node 1, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
2449029, 126167053, 126167053
2449029, 126167053, 126167053
2449029, 126167053, 126167053
2449029, 126167053, 126167053
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 3, starting the helper threads...
*** Node 2, starting the helper threads...
*** Node 1, starting the helper threads...
+++++++++ Node 2 initializing the weights for op[11, 16)...
+++++++++ Node 2, mapping weight op 11
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 6
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 3 initializing the weights for op[16, 20)...
+++++++++ Node 3, mapping weight op 16
RANDOMLY DISPATCH THE CHUNKS...
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 9:	Loss 3.39152	TrainAcc 0.3084	ValidAcc 0.3083	TestAcc 0.2687
    Epoch 19:	Loss 2.21481	TrainAcc 0.6089	ValidAcc 0.6173	TestAcc 0.4682
    Epoch 29:	Loss 1.49675	TrainAcc 0.7465	ValidAcc 0.7441	TestAcc 0.5619
    Epoch 39:	Loss 1.08627	TrainAcc 0.7903	ValidAcc 0.7869	TestAcc 0.6135
    Epoch 49:	Loss 0.85447	TrainAcc 0.8288	ValidAcc 0.8255	TestAcc 0.6492
    Epoch 59:	Loss 0.73506	TrainAcc 0.8485	ValidAcc 0.8430	TestAcc 0.6720
    Epoch 69:	Loss 0.66786	TrainAcc 0.8546	ValidAcc 0.8506	TestAcc 0.6793
    Epoch 79:	Loss 0.62253	TrainAcc 0.8605	ValidAcc 0.8572	TestAcc 0.6837
    Epoch 89:	Loss 0.59040	TrainAcc 0.8687	ValidAcc 0.8642	TestAcc 0.6897
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.928 GBps
Node 2, Layer-level comm throughput (act): 10.299 GBps
Node 3, Layer-level comm throughput (act): 10.478 GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 2, Layer-level comm throughput (grad): 10.605 GBps
Node 1, Layer-level comm throughput (grad): 10.497 GBps
Node 0, Layer-level comm throughput (grad): 10.925 GBps
    Epoch 99:	Loss 0.56580	TrainAcc 0.8711	ValidAcc 0.8665	TestAcc 0.6923
Node 0, compression time: 0.395s, compression size: 116.779GB, throughput: 295.638GBps
Node 0, decompression time: 1.355s, compression size: 116.779GB, throughput: 86.177GBps
Node 0, pure compute time: 7.270 s, total compute time: 9.020 s
Node 0, wait_for_task_time: 7.807 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 0,  per-epoch time: 0.173901 s---------------
Node 2, compression time: 1.069s, compression size: 233.558GB, throughput: 218.423GBps
Node 2, decompression time: 4.784s, compression size: 233.558GB, throughput: 48.821GBps
Node 2, pure compute time: 6.468 s, total compute time: 12.321 s
Node 1, compression time: 1.061s, compression size: 233.558GB, throughput: 220.062GBps
Node 1, decompression time: 4.543s, compression size: 233.558GB, throughput: 51.408GBps
Node 1, pure compute time: 6.472 s, total compute time: 12.076 s
Node 1, wait_for_task_time: 4.082 s, wait_for_other_gpus_time: 0.001 s
Node 3, compression time: 0.674s, compression size: 116.779GB, throughput: 173.288GBps
Node 3, decompression time: 1.782s, compression size: 116.779GB, throughput: 65.533GBps
Node 3, pure compute time: 6.561 s, total compute time: 9.016 s
Node 3, wait_for_task_time: 2.642 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 3,  per-epoch time: 0.173901 s---------------
Node 2, wait_for_task_time: 2.928 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 2,  per-epoch time: 0.173901 s---------------
------------------------node id 1,  per-epoch time: 0.173901 s---------------
ERROR: undercount the overhead: breakdown sum / all time: 0.636
************ Profiling Results ************
	Bubble: 1.679133 (s) (14.91 percentage)
	Compute: 6.707004 (s) (59.56 percentage)
	GradSync: 0.043121 (s) (0.38 percentage)
	GraphComm: 0.003437 (s) (0.03 percentage)
	Imbalance: 1.700121 (s) (15.10 percentage)
	LayerComm: 1.128033 (s) (10.02 percentage)
ERROR: undercount the overhead: breakdown sum / all time: 0.636
ERROR: undercount the overhead: breakdown sum / all time: 0.636
	Layer-level communication (cluster-wide, per epoch): 2.229 GB
Highest valid_acc: 0.8665
Target test_acc: 0.6923
Epoch to reach the target acc: 100
ERROR: undercount the overhead: breakdown sum / all time: 0.636
[MPI Rank 3] Success 
[MPI Rank 0] Success 
[MPI Rank 2] Success 
[MPI Rank 1] Success 
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 0
Learning rate: 0.000000
Initialized node g000.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 1.936 seconds.
Building the CSC structure...
        It takes 1.880 seconds.
Building the Feature Vector...
        It takes 0.581 seconds.
Building the Label Vector...
        It takes 0.313 seconds.
Number of classes: 47
Number of feature dimensions: 100
Dropout: 0.000 
train nodes 196615, valid nodes 39323, test nodes 2213091
*** Allocating resources for all tensors...
    OP_TYPE: OPERATOR_INPUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_SOFTMAX
*** Done allocating resource.
*** Preparing the input tensor...
*** Done preparing the input tensor.
*** Preparing the STD tensor...
    Number of labels: 47
    Number of vertices: 2449029
*** Done preparing the STD tensor.
Version 0	TrainAcc 0.3084	ValidAcc 0.3083	TestAcc 0.2692
Version 1	TrainAcc 0.6202	ValidAcc 0.6293	TestAcc 0.5019
Version 2	TrainAcc 0.7695	ValidAcc 0.7696	TestAcc 0.6012
Version 3	TrainAcc 0.8064	ValidAcc 0.8036	TestAcc 0.6402
Version 4	TrainAcc 0.8356	ValidAcc 0.8321	TestAcc 0.6706
Version 5	TrainAcc 0.8518	ValidAcc 0.8480	TestAcc 0.6908
Version 6	TrainAcc 0.8587	ValidAcc 0.8550	TestAcc 0.6968
Version 7	TrainAcc 0.8616	ValidAcc 0.8601	TestAcc 0.7007
Version 8	TrainAcc 0.8695	ValidAcc 0.8648	TestAcc 0.7057
Version 9	TrainAcc 0.8747	ValidAcc 0.8702	TestAcc 0.7090
Version 9 achieved the highest validation accuracy 0.8702 (test accuracy: 0.7090)
