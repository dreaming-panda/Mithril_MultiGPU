Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2

Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
DONE MPI INIT
Initialized node 7 on machine gnerv3
DONE MPI INIT
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.999 seconds.
Building the CSC structure...
        It takes 2.124 seconds.
Building the CSC structure...
        It takes 2.315 seconds.
Building the CSC structure...
        It takes 2.401 seconds.
Building the CSC structure...
        It takes 2.410 seconds.
Building the CSC structure...
        It takes 2.432 seconds.
Building the CSC structure...
        It takes 2.460 seconds.
Building the CSC structure...
        It takes 2.464 seconds.
Building the CSC structure...
        It takes 1.861 seconds.
        It takes 1.940 seconds.
        It takes 2.276 seconds.
        It takes 2.326 seconds.
        It takes 2.359 seconds.
        It takes 2.394 seconds.
        It takes 2.404 seconds.
        It takes 2.528 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.261 seconds.
Building the Label Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
        It takes 0.273 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
        It takes 0.328 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.292 seconds.
Building the Label Vector...
        It takes 0.304 seconds.
Building the Label Vector...
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.037 seconds.
Building the Feature Vector...
        It takes 0.037 seconds.
Building the Feature Vector...
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.382 Gbps (per GPU), 443.058 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.146 Gbps (per GPU), 441.165 Gbps (aggregated)
The layer-level communication performance: 55.130 Gbps (per GPU), 441.041 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.925 Gbps (per GPU), 439.398 Gbps (aggregated)
The layer-level communication performance: 54.893 Gbps (per GPU), 439.142 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.708 Gbps (per GPU), 437.663 Gbps (aggregated)
The layer-level communication performance: 54.672 Gbps (per GPU), 437.375 Gbps (aggregated)
The layer-level communication performance: 54.645 Gbps (per GPU), 437.162 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.601 Gbps (per GPU), 1260.805 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.595 Gbps (per GPU), 1260.758 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.586 Gbps (per GPU), 1260.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.607 Gbps (per GPU), 1260.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.586 Gbps (per GPU), 1260.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.601 Gbps (per GPU), 1260.805 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.583 Gbps (per GPU), 1260.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.584 Gbps (per GPU), 1260.669 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.933 Gbps (per GPU), 815.464 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.932 Gbps (per GPU), 815.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.936 Gbps (per GPU), 815.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.933 Gbps (per GPU), 815.464 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.934 Gbps (per GPU), 815.470 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.934 Gbps (per GPU), 815.470 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.930 Gbps (per GPU), 815.444 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.933 Gbps (per GPU), 815.464 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.690 Gbps (per GPU), 269.520 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.690 Gbps (per GPU), 269.520 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.690 Gbps (per GPU), 269.520 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.690 Gbps (per GPU), 269.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.690 Gbps (per GPU), 269.519 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.691 Gbps (per GPU), 269.526 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.691 Gbps (per GPU), 269.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.690 Gbps (per GPU), 269.522 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.42ms  2.35ms  2.42ms  1.03  8.38K  3.53M
 chk_1  2.79ms  2.62ms  2.50ms  1.12  6.74K  3.60M
 chk_2  2.69ms  2.48ms  2.41ms  1.12  7.27K  3.53M
 chk_3  2.70ms  2.52ms  2.42ms  1.11  7.92K  3.61M
 chk_4  2.61ms  2.48ms  2.62ms  1.06  5.33K  3.68M
 chk_5  2.60ms  2.38ms  2.25ms  1.16 10.07K  3.45M
 chk_6  2.75ms  2.55ms  2.43ms  1.13  9.41K  3.48M
 chk_7  2.59ms  2.43ms  2.35ms  1.10  8.12K  3.60M
 chk_8  2.69ms  2.58ms  2.68ms  1.04  6.09K  3.64M
 chk_9  2.51ms  2.26ms  2.13ms  1.18 11.10K  3.38M
chk_10  2.74ms  2.63ms  2.55ms  1.07  5.67K  3.63M
chk_11  2.61ms  2.44ms  2.35ms  1.11  8.16K  3.54M
chk_12  2.81ms  2.66ms  2.71ms  1.06  7.24K  3.55M
chk_13  2.62ms  2.52ms  2.45ms  1.07  5.41K  3.68M
chk_14  2.87ms  2.73ms  2.64ms  1.09  7.14K  3.53M
chk_15  2.73ms  2.53ms  2.42ms  1.13  9.25K  3.49M
chk_16  2.55ms  2.45ms  2.40ms  1.06  4.78K  3.77M
chk_17  2.69ms  2.80ms  2.47ms  1.14  6.85K  3.60M
chk_18  2.52ms  2.34ms  2.26ms  1.11  7.47K  3.57M
chk_19  2.56ms  2.47ms  2.39ms  1.07  4.88K  3.75M
chk_20  2.57ms  2.43ms  2.35ms  1.09  7.00K  3.63M
chk_21  2.54ms  2.71ms  2.37ms  1.14  5.41K  3.68M
chk_22  2.76ms  2.51ms  2.39ms  1.16 11.07K  3.39M
chk_23  2.67ms  2.50ms  2.42ms  1.10  7.23K  3.64M
chk_24  2.72ms  2.49ms  2.36ms  1.16 10.13K  3.43M
chk_25  2.53ms  2.65ms  2.30ms  1.15  6.40K  3.57M
chk_26  2.74ms  2.57ms  2.53ms  1.08  5.78K  3.55M
chk_27  2.59ms  2.39ms  2.30ms  1.13  9.34K  3.48M
chk_28  2.94ms  2.75ms  2.68ms  1.10  6.37K  3.57M
chk_29  2.73ms  2.59ms  2.53ms  1.08  5.16K  3.78M
chk_30  2.61ms  2.52ms  2.41ms  1.08  5.44K  3.67M
chk_31  2.76ms  2.65ms  2.53ms  1.09  6.33K  3.63M
   Avg  2.66  2.53  2.44
   Max  2.94  2.80  2.71
   Min  2.42  2.26  2.13
 Ratio  1.22  1.24  1.27
   Var  0.01  0.02  0.02
Profiling takes 2.835 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[81, 101)...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
+++++++++ Node 7 initializing the weights for op[141, 160)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
+++++++++ Node 0 initializing the weights for op[0, 21)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ************ Start Scheduling the Tasks in a Pipelined Fashion ******

****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7136	TrainAcc 0.0398	ValidAcc 0.0348	TestAcc 0.0370	BestValid 0.0348
	Epoch 50:	Loss 3.3681	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
****** Epoch Time (Excluding Evaluation Cost): 0.352 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 69.186 ms (Max: 72.034, Min: 54.962, Sum: 553.491)
Cluster-Wide Average, Compute: 234.493 ms (Max: 247.732, Min: 227.508, Sum: 1875.943)
Cluster-Wide Average, Communication-Layer: 25.070 ms (Max: 30.313, Min: 16.741, Sum: 200.563)
Cluster-Wide Average, Bubble-Imbalance: 19.412 ms (Max: 23.881, Min: 14.851, Sum: 155.299)
Cluster-Wide Average, Communication-Graph: 0.474 ms (Max: 0.519, Min: 0.422, Sum: 3.793)
Cluster-Wide Average, Optimization: 0.095 ms (Max: 0.100, Min: 0.090, Sum: 0.757)
Cluster-Wide Average, Others: 3.740 ms (Max: 17.564, Min: 1.756, Sum: 29.920)
****** Breakdown Sum: 352.471 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.852 GB (Max: 6.698, Min: 5.534, Sum: 46.817)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 51.456 Gbps (Max: 58.989, Min: 44.210, Sum: 411.649)
Layer-level communication (cluster-wide, per-epoch): 1.215 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 1.215 GB
****** Accuracy Results ******
Highest valid_acc: 0.0584
Target test_acc: 0.0574
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
