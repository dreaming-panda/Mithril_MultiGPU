Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2

Initialized node 0 on machine gnerv2
DONE MPI INIT
DONE MPI INIT
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.922 seconds.
Building the CSC structure...
        It takes 2.448 seconds.
Building the CSC structure...
        It takes 2.455 seconds.
Building the CSC structure...
        It takes 2.466 seconds.
Building the CSC structure...
        It takes 2.553 seconds.
Building the CSC structure...
        It takes 2.587 seconds.
Building the CSC structure...
        It takes 2.625 seconds.
Building the CSC structure...
        It takes 2.660 seconds.
Building the CSC structure...
        It takes 1.850 seconds.
Building the Feature Vector...
        It takes 2.330 seconds.
        It takes 2.354 seconds.
        It takes 2.418 seconds.
        It takes 2.357 seconds.
        It takes 2.332 seconds.
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 2.393 seconds.
        It takes 0.034 seconds.
        It takes 3.121 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.275 Gbps (per GPU), 458.203 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.000 Gbps (per GPU), 455.997 Gbps (aggregated)
The layer-level communication performance: 56.995 Gbps (per GPU), 455.962 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.767 Gbps (per GPU), 454.133 Gbps (aggregated)
The layer-level communication performance: 56.728 Gbps (per GPU), 453.825 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.556 Gbps (per GPU), 452.449 Gbps (aggregated)
The layer-level communication performance: 56.516 Gbps (per GPU), 452.125 Gbps (aggregated)
The layer-level communication performance: 56.480 Gbps (per GPU), 451.836 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.943 Gbps (per GPU), 1255.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.940 Gbps (per GPU), 1255.521 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.946 Gbps (per GPU), 1255.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.931 Gbps (per GPU), 1255.451 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.917 Gbps (per GPU), 1255.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.934 Gbps (per GPU), 1255.474 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.931 Gbps (per GPU), 1255.451 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.726 Gbps (per GPU), 1253.811 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.299 Gbps (per GPU), 810.389 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.297 Gbps (per GPU), 810.377 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.299 Gbps (per GPU), 810.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.299 Gbps (per GPU), 810.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.299 Gbps (per GPU), 810.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.299 Gbps (per GPU), 810.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.297 Gbps (per GPU), 810.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.299 Gbps (per GPU), 810.389 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.100 Gbps (per GPU), 304.796 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.099 Gbps (per GPU), 304.795 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.099 Gbps (per GPU), 304.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.098 Gbps (per GPU), 304.782 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.099 Gbps (per GPU), 304.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.099 Gbps (per GPU), 304.795 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.099 Gbps (per GPU), 304.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.097 Gbps (per GPU), 304.775 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.67ms  3.25ms  4.58ms  1.71  8.38K  3.53M
 chk_1  3.01ms  3.40ms  4.25ms  1.41  6.74K  3.60M
 chk_2  2.86ms  3.30ms  4.16ms  1.45  7.27K  3.53M
 chk_3  2.89ms  3.38ms  4.29ms  1.49  7.92K  3.61M
 chk_4  2.77ms  3.13ms  3.85ms  1.39  5.33K  3.68M
 chk_5  2.86ms  3.43ms  4.50ms  1.57 10.07K  3.45M
 chk_6  3.04ms  3.57ms  4.60ms  1.51  9.41K  3.48M
 chk_7  2.85ms  3.31ms  4.23ms  1.48  8.12K  3.60M
 chk_8  2.92ms  3.30ms  4.07ms  1.40  6.09K  3.64M
 chk_9  2.81ms  3.40ms  4.78ms  1.70 11.10K  3.38M
chk_10  2.95ms  3.30ms  4.04ms  1.37  5.67K  3.63M
chk_11  2.85ms  3.34ms  4.26ms  1.50  8.16K  3.54M
chk_12  3.04ms  3.47ms  4.33ms  1.42  7.24K  3.55M
chk_13  2.82ms  3.19ms  4.27ms  1.51  5.41K  3.68M
chk_14  3.10ms  3.55ms  4.40ms  1.42  7.14K  3.53M
chk_15  3.00ms  3.53ms  4.54ms  1.51  9.25K  3.49M
chk_16  2.73ms  3.05ms  3.76ms  1.38  4.78K  3.77M
chk_17  2.92ms  3.33ms  4.64ms  1.59  6.85K  3.60M
chk_18  2.74ms  3.17ms  4.02ms  1.47  7.47K  3.57M
chk_19  2.75ms  3.09ms  3.80ms  1.38  4.88K  3.75M
chk_20  2.80ms  3.22ms  4.06ms  1.45  7.00K  3.63M
chk_21  2.75ms  3.10ms  3.86ms  1.41  5.41K  3.68M
chk_22  3.05ms  4.05ms  5.05ms  1.65 11.07K  3.39M
chk_23  2.90ms  3.30ms  4.16ms  1.43  7.23K  3.64M
chk_24  2.98ms  3.56ms  4.65ms  1.56 10.13K  3.43M
chk_25  2.75ms  3.12ms  3.93ms  1.43  6.40K  3.57M
chk_26  2.93ms  3.30ms  4.05ms  1.38  5.78K  3.55M
chk_27  2.92ms  3.67ms  4.40ms  1.51  9.34K  3.48M
chk_28  3.14ms  3.51ms  4.34ms  1.38  6.37K  3.57M
chk_29  2.91ms  3.24ms  3.97ms  1.36  5.16K  3.78M
chk_30  2.80ms  3.15ms  3.92ms  1.40  5.44K  3.67M
chk_31  2.98ms  3.37ms  4.18ms  1.40  6.33K  3.63M
   Avg  2.89  3.35  4.25
   Max  3.14  4.05  5.05
   Min  2.67  3.05  3.76
 Ratio  1.18  1.33  1.34
   Var  0.01  0.04  0.09
Profiling takes 3.871 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 48)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [48, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 152)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 256)
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [256, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [308, 360)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [360, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [152, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 48)...
+++++++++ Node 5 initializing the weights for op[256, 308)...
+++++++++ Node 1 initializing the weights for op[48, 100)...
+++++++++ Node 6 initializing the weights for op[308, 360)...
+++++++++ Node 2 initializing the weights for op[100, 152)...
+++++++++ Node 7 initializing the weights for op[360, 421)...
+++++++++ Node 3 initializing the weights for op[152, 204)...
+++++++++ Node 4 initializing the weights for op[204, 256)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.1985	TrainAcc 0.1450	ValidAcc 0.1458	TestAcc 0.1439	BestValid 0.1458
	Epoch 50:	Loss 0.8667	TrainAcc 0.8657	ValidAcc 0.8781	TestAcc 0.8749	BestValid 0.8781
****** Epoch Time (Excluding Evaluation Cost): 0.500 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 91.769 ms (Max: 98.104, Min: 75.541, Sum: 734.148)
Cluster-Wide Average, Compute: 336.881 ms (Max: 377.198, Min: 321.081, Sum: 2695.046)
Cluster-Wide Average, Communication-Layer: 25.874 ms (Max: 31.081, Min: 17.194, Sum: 206.995)
Cluster-Wide Average, Bubble-Imbalance: 41.467 ms (Max: 60.716, Min: 12.889, Sum: 331.734)
Cluster-Wide Average, Communication-Graph: 0.554 ms (Max: 0.687, Min: 0.445, Sum: 4.430)
Cluster-Wide Average, Optimization: 0.267 ms (Max: 0.289, Min: 0.252, Sum: 2.133)
Cluster-Wide Average, Others: 3.975 ms (Max: 17.099, Min: 2.044, Sum: 31.802)
****** Breakdown Sum: 500.786 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.303 GB (Max: 8.260, Min: 7.065, Sum: 58.423)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 49.794 Gbps (Max: 55.339, Min: 42.559, Sum: 398.355)
Layer-level communication (cluster-wide, per-epoch): 1.215 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 1.215 GB
****** Accuracy Results ******
Highest valid_acc: 0.8781
Target test_acc: 0.8749
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
