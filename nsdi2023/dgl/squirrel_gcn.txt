gnerv2
gnerv4
launching jobs on gnerv4, in total 2 nodes
launching jobs on gnerv2, in total 2 nodes
Executing:  'python ./main.py 1 2 gnerv2 4 8' on gnerv4
Executing:  'python ./main.py 1 2 gnerv2 5 8' on gnerv4
Executing:  'python ./main.py 1 2 gnerv2 6 8' on gnerv4
Executing:  'python ./main.py 1 2 gnerv2 7 8' on gnerv4
Executing:  'python ./main.py 0 2 gnerv2 0 8' on gnerv2
Executing:  'python ./main.py 0 2 gnerv2 1 8' on gnerv2
Executing:  'python ./main.py 0 2 gnerv2 2 8' on gnerv2
Executing:  'python ./main.py 0 2 gnerv2 3 8' on gnerv2
['./main.py', '0', '2', 'gnerv2', '0', '8']
Hello World From Process 0, the World Size is 8
Done loading data from cached files.
Downloaded the graph dataset
['./main.py', '1', '2', 'gnerv2', '6', '8']
Hello World From Process 6, the World Size is 8
Done loading data from cached files.
Downloaded the graph dataset
['./main.py', '0', '2', 'gnerv2', '3', '8']
Hello World From Process 3, the World Size is 8
Done loading data from cached files.
Downloaded the graph dataset
['./main.py', '1', '2', 'gnerv2', '4', '8']
Hello World From Process 4, the World Size is 8
Done loading data from cached files.
Downloaded the graph dataset
['./main.py', '1', '2', 'gnerv2', '7', '8']
Hello World From Process 7, the World Size is 8
Done loading data from cached files.
Downloaded the graph dataset
['./main.py', '0', '2', 'gnerv2', '2', '8']
Hello World From Process 2, the World Size is 8
Done loading data from cached files.
Downloaded the graph dataset
['./main.py', '1', '2', 'gnerv2', '5', '8']
Hello World From Process 5, the World Size is 8
Done loading data from cached files.
Downloaded the graph dataset
['./main.py', '0', '2', 'gnerv2', '1', '8']
Hello World From Process 1, the World Size is 8
Done loading data from cached files.
Downloaded the graph dataset
StochasticGCN(
  (convs): ModuleList(
    (0): GraphConv(in=2089, out=1000, normalization=both, activation=None)
    (1-30): 30 x GraphConv(in=1000, out=1000, normalization=both, activation=None)
    (31): GraphConv(in=1000, out=5, normalization=both, activation=None)
  )
)
StochasticGCN(
  (convs): ModuleList(
    (0): GraphConv(in=2089, out=1000, normalization=both, activation=None)
    (1-30): 30 x GraphConv(in=1000, out=1000, normalization=both, activation=None)
    (31): GraphConv(in=1000, out=5, normalization=both, activation=None)
  )
)
StochasticGCN(
  (convs): ModuleList(
    (0): GraphConv(in=2089, out=1000, normalization=both, activation=None)
    (1-30): 30 x GraphConv(in=1000, out=1000, normalization=both, activation=None)
    (31): GraphConv(in=1000, out=5, normalization=both, activation=None)
  )
)
StochasticGCN(
  (convs): ModuleList(
    (0): GraphConv(in=2089, out=1000, normalization=both, activation=None)
    (1-30): 30 x GraphConv(in=1000, out=1000, normalization=both, activation=None)
    (31): GraphConv(in=1000, out=5, normalization=both, activation=None)
  )
)
StochasticGCN(
  (convs): ModuleList(
    (0): GraphConv(in=2089, out=1000, normalization=both, activation=None)
    (1-30): 30 x GraphConv(in=1000, out=1000, normalization=both, activation=None)
    (31): GraphConv(in=1000, out=5, normalization=both, activation=None)
  )
)
StochasticGCN(
  (convs): ModuleList(
    (0): GraphConv(in=2089, out=1000, normalization=both, activation=None)
    (1-30): 30 x GraphConv(in=1000, out=1000, normalization=both, activation=None)
    (31): GraphConv(in=1000, out=5, normalization=both, activation=None)
  )
)
StochasticGCN(
  (convs): ModuleList(
    (0): GraphConv(in=2089, out=1000, normalization=both, activation=None)
    (1-30): 30 x GraphConv(in=1000, out=1000, normalization=both, activation=None)
    (31): GraphConv(in=1000, out=5, normalization=both, activation=None)
  )
)
StochasticGCN(
  (convs): ModuleList(
    (0): GraphConv(in=2089, out=1000, normalization=both, activation=None)
    (1-30): 30 x GraphConv(in=1000, out=1000, normalization=both, activation=None)
    (31): GraphConv(in=1000, out=5, normalization=both, activation=None)
  )
)
Start Distributed GNN Training
Start Distributed GNN Training
Start Distributed GNN Training
Start Distributed GNN Training
Start Distributed GNN Training
Start Distributed GNN Training
Start Distributed GNN Training
Start Distributed GNN Training
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.6094
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6003
Epoch 1, Loss 1.6086, Train Accuracy 0.2055, Valid Accuracy 0.1953, Test Accuracy 0.1921
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.6036
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6315
Epoch 2, Loss 1.6021, Train Accuracy 0.2003, Valid Accuracy 0.2007, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5982
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5710
Epoch 3, Loss 1.5966, Train Accuracy 0.2003, Valid Accuracy 0.2007, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5951
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5569
Epoch 4, Loss 1.5978, Train Accuracy 0.2003, Valid Accuracy 0.2007, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5900
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6176
Epoch 5, Loss 1.5983, Train Accuracy 0.2003, Valid Accuracy 0.2007, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5958
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5723
Epoch 6, Loss 1.5978, Train Accuracy 0.2023, Valid Accuracy 0.2007, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5896
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6248
Epoch 7, Loss 1.5974, Train Accuracy 0.2031, Valid Accuracy 0.2013, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5958
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5800
Epoch 8, Loss 1.5960, Train Accuracy 0.2023, Valid Accuracy 0.2013, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5973
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5630
Epoch 9, Loss 1.5961, Train Accuracy 0.2019, Valid Accuracy 0.2013, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5927
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6080
Epoch 10, Loss 1.5953, Train Accuracy 0.2023, Valid Accuracy 0.2013, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5933
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5852
Epoch 11, Loss 1.5943, Train Accuracy 0.2015, Valid Accuracy 0.2013, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5931
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5601
Epoch 12, Loss 1.5929, Train Accuracy 0.2023, Valid Accuracy 0.2013, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5873
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6019
Epoch 13, Loss 1.5917, Train Accuracy 0.2091, Valid Accuracy 0.2007, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5900
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6345
Epoch 14, Loss 1.5938, Train Accuracy 0.2039, Valid Accuracy 0.2013, Test Accuracy 0.1988
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5971
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5465
Epoch 15, Loss 1.5935, Train Accuracy 0.2039, Valid Accuracy 0.2007, Test Accuracy 0.1950
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5857
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6224
Epoch 16, Loss 1.5916, Train Accuracy 0.2051, Valid Accuracy 0.2043, Test Accuracy 0.1940
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5915
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5937
Epoch 17, Loss 1.5924, Train Accuracy 0.2079, Valid Accuracy 0.2055, Test Accuracy 0.1960
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5922
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5766
Epoch 18, Loss 1.5920, Train Accuracy 0.2067, Valid Accuracy 0.2067, Test Accuracy 0.2017
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5895
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5752
Epoch 19, Loss 1.5926, Train Accuracy 0.2091, Valid Accuracy 0.2079, Test Accuracy 0.2008
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5880
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6059
Epoch 20, Loss 1.5918, Train Accuracy 0.2111, Valid Accuracy 0.2085, Test Accuracy 0.2008
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5906
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5737
Epoch 21, Loss 1.5909, Train Accuracy 0.2115, Valid Accuracy 0.2097, Test Accuracy 0.2017
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5887
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5810
Epoch 22, Loss 1.5908, Train Accuracy 0.2139, Valid Accuracy 0.2097, Test Accuracy 0.2017
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5831
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6157
Epoch 23, Loss 1.5926, Train Accuracy 0.2111, Valid Accuracy 0.2091, Test Accuracy 0.1979
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5846
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6052
Epoch 24, Loss 1.5904, Train Accuracy 0.2135, Valid Accuracy 0.2091, Test Accuracy 0.1940
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5970
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5368
Epoch 25, Loss 1.5903, Train Accuracy 0.2224, Valid Accuracy 0.2181, Test Accuracy 0.1969
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5944
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5706
Epoch 26, Loss 1.5904, Train Accuracy 0.2083, Valid Accuracy 0.2157, Test Accuracy 0.1931
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5900
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5926
Epoch 27, Loss 1.5886, Train Accuracy 0.2059, Valid Accuracy 0.2049, Test Accuracy 0.1931
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5889
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5915
Epoch 28, Loss 1.5890, Train Accuracy 0.2055, Valid Accuracy 0.2067, Test Accuracy 0.2008
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5868
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5747
Epoch 29, Loss 1.5873, Train Accuracy 0.2107, Valid Accuracy 0.2079, Test Accuracy 0.1998
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5913
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5198
Epoch 30, Loss 1.5889, Train Accuracy 0.2111, Valid Accuracy 0.2061, Test Accuracy 0.2017
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5866
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5731
Epoch 31, Loss 1.5901, Train Accuracy 0.2123, Valid Accuracy 0.2127, Test Accuracy 0.2027
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5934
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5402
Epoch 32, Loss 1.5876, Train Accuracy 0.2103, Valid Accuracy 0.2169, Test Accuracy 0.1969
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5900
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5742
Epoch 33, Loss 1.5882, Train Accuracy 0.2163, Valid Accuracy 0.2115, Test Accuracy 0.1940
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5824
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6389
Epoch 34, Loss 1.5891, Train Accuracy 0.2175, Valid Accuracy 0.2181, Test Accuracy 0.1950
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5835
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6081
Epoch 35, Loss 1.5883, Train Accuracy 0.2171, Valid Accuracy 0.2194, Test Accuracy 0.1988
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5866
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5828
Epoch 36, Loss 1.5898, Train Accuracy 0.2163, Valid Accuracy 0.2236, Test Accuracy 0.2085
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5826
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6222
Epoch 37, Loss 1.5882, Train Accuracy 0.2276, Valid Accuracy 0.2254, Test Accuracy 0.2152
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5902
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5488
Epoch 38, Loss 1.5884, Train Accuracy 0.2264, Valid Accuracy 0.2230, Test Accuracy 0.2104
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5850
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6071
Epoch 39, Loss 1.5901, Train Accuracy 0.2183, Valid Accuracy 0.2236, Test Accuracy 0.2152
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5913
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5510
Epoch 40, Loss 1.5863, Train Accuracy 0.2236, Valid Accuracy 0.2200, Test Accuracy 0.2104
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5858
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5864
Epoch 41, Loss 1.5874, Train Accuracy 0.2196, Valid Accuracy 0.2212, Test Accuracy 0.2104
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5858
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5748
Epoch 42, Loss 1.5860, Train Accuracy 0.2167, Valid Accuracy 0.2218, Test Accuracy 0.2056
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5844
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5606
Epoch 43, Loss 1.5862, Train Accuracy 0.2147, Valid Accuracy 0.2230, Test Accuracy 0.2075
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5792
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6035
Epoch 44, Loss 1.5863, Train Accuracy 0.2188, Valid Accuracy 0.2254, Test Accuracy 0.2075
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5862
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5485
Epoch 45, Loss 1.5848, Train Accuracy 0.2204, Valid Accuracy 0.2224, Test Accuracy 0.2017
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5801
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6399
Epoch 46, Loss 1.5871, Train Accuracy 0.2264, Valid Accuracy 0.2266, Test Accuracy 0.1950
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5863
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5795
Epoch 47, Loss 1.5868, Train Accuracy 0.2348, Valid Accuracy 0.2290, Test Accuracy 0.2161
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5886
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5809
Epoch 48, Loss 1.5872, Train Accuracy 0.2376, Valid Accuracy 0.2368, Test Accuracy 0.2229
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5835
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.6252
Epoch 49, Loss 1.5883, Train Accuracy 0.2308, Valid Accuracy 0.2392, Test Accuracy 0.2238
	Rank 0, True Batch Size 256, Iteration 0, Loss 1.5848
	Rank 0, True Batch Size 56, Iteration 1, Loss 1.5847
Epoch 50, Loss 1.5858, Train Accuracy 0.2360, Valid Accuracy 0.2374, Test Accuracy 0.2142
Per-epoch Time: 1.5182 s
Highest Validation Accuracy: 0.2392
Target Test Accuracy: 0.2238
