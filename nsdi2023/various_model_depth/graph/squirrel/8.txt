Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
DONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv4
Initialized node 6 on machine gnerv4
Initialized node 4 on machine gnerv4
Initialized node 7 on machine gnerv4
DONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv2
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
        It takes 0.010 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.009 seconds.
        It takes 0.010 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.028 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.029 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCNII layers: 8
The number of hidden units: 1000
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.030 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.045 seconds.
        It takes 0.043 seconds.
        It takes 0.044 seconds.
        It takes 0.048 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.000 seconds.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
5201, 401907, 401907
Number of vertices per chunk: 651
Number of vertices per chunk: 651
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 49.380 Gbps (per GPU), 395.040 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 49.188 Gbps (per GPU), 393.502 Gbps (aggregated)
The layer-level communication performance: 49.175 Gbps (per GPU), 393.402 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 49.014 Gbps (per GPU), 392.109 Gbps (aggregated)
The layer-level communication performance: 48.990 Gbps (per GPU), 391.923 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 48.841 Gbps (per GPU), 390.727 Gbps (aggregated)
The layer-level communication performance: 48.815 Gbps (per GPU), 390.517 Gbps (aggregated)
The layer-level communication performance: 48.790 Gbps (per GPU), 390.318 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.747 Gbps (per GPU), 1253.973 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.723 Gbps (per GPU), 1253.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.729 Gbps (per GPU), 1253.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.720 Gbps (per GPU), 1253.762 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.729 Gbps (per GPU), 1253.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.735 Gbps (per GPU), 1253.879 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.738 Gbps (per GPU), 1253.903 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.735 Gbps (per GPU), 1253.879 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.338 Gbps (per GPU), 810.703 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.334 Gbps (per GPU), 810.670 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.333 Gbps (per GPU), 810.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.348 Gbps (per GPU), 810.780 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.335 Gbps (per GPU), 810.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.337 Gbps (per GPU), 810.696 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.317 Gbps (per GPU), 810.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.345 Gbps (per GPU), 810.761 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.966 Gbps (per GPU), 255.731 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.967 Gbps (per GPU), 255.737 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.964 Gbps (per GPU), 255.714 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.967 Gbps (per GPU), 255.733 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.964 Gbps (per GPU), 255.715 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.966 Gbps (per GPU), 255.727 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.960 Gbps (per GPU), 255.680 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.960 Gbps (per GPU), 255.677 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.02ms  1.63ms  1.77ms  1.74  0.65K  0.22M
 chk_1  1.02ms  0.96ms  1.11ms  1.16  0.65K  0.05M
 chk_2  1.02ms  0.83ms  0.97ms  1.22  0.65K  0.02M
 chk_3  1.02ms  0.85ms  0.99ms  1.21  0.65K  0.02M
 chk_4  1.03ms  0.79ms  0.93ms  1.29  0.65K  0.01M
 chk_5  1.03ms  0.79ms  0.93ms  1.30  0.65K  0.01M
 chk_6  1.03ms  0.98ms  1.12ms  1.14  0.65K  0.06M
 chk_7  1.03ms  0.81ms  0.95ms  1.27  0.65K  0.01M
   Avg  1.02  0.95  1.10
   Max  1.03  1.63  1.77
   Min  1.02  0.79  0.93
 Ratio  1.01  2.07  1.91
   Var  0.00  0.07  0.07
Profiling takes 0.359 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 65)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 65)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 65)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 65)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 65)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
Num Stages: 1 / 1
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 65)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 65)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 65)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 7 initializing the weights for op[0, 65)...
+++++++++ Node 6 initializing the weights for op[0, 65)...
+++++++++ Node 4 initializing the weights for op[0, 65)...
+++++++++ Node 5 initializing the weights for op[0, 65)...
+++++++++ Node 0 initializing the weights for op[0, 65)...
+++++++++ Node 1 initializing the weights for op[0, 65)...
+++++++++ Node 2 initializing the weights for op[0, 65)...
+++++++++ Node 3 initializing the weights for op[0, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6119	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 1.0135	TrainAcc 0.6907	ValidAcc 0.3377	TestAcc 0.3295	BestValid 0.3377
****** Epoch Time (Excluding Evaluation Cost): 0.052 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.401 ms (Max: 0.687, Min: 0.049, Sum: 3.207)
Cluster-Wide Average, Compute: 8.286 ms (Max: 11.992, Min: 7.372, Sum: 66.290)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.123)
Cluster-Wide Average, Communication-Graph: 28.287 ms (Max: 29.159, Min: 24.930, Sum: 226.295)
Cluster-Wide Average, Optimization: 14.331 ms (Max: 14.375, Min: 14.273, Sum: 114.647)
Cluster-Wide Average, Others: 0.336 ms (Max: 0.343, Min: 0.327, Sum: 2.685)
****** Breakdown Sum: 51.664 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.916 GB (Max: 2.174, Min: 1.866, Sum: 15.329)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.901 Gbps (Max: 43.644, Min: 9.753, Sum: 215.208)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.688 GB
Weight-sync communication (cluster-wide, per-epoch): 0.526 GB
Total communication (cluster-wide, per-epoch): 1.215 GB
****** Accuracy Results ******
Highest valid_acc: 0.3377
Target test_acc: 0.3295
Epoch to reach the target acc: 49
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
