Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
Initialized node 6 on machine gnerv8

Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
        It takes 0.012 seconds.
        It takes 0.012 seconds.
Building the Feature Vector...
        It takes 0.012 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.011 seconds.
Building the Feature Vector...
        It takes 0.015 seconds.
        It takes 0.015 seconds.
        It takes 0.014 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.470 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.469 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.502 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.505 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.570 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.600 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.611 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/physics/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 8
        It takes 0.612 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 4311) 1-[4311, 8623) 2-[8623, 12935) 3-[12935, 17247) 4-[17247, 21558) 5-[21558, 25870) 6-[25870, 30181) 7-[30181, 34493)
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.231 Gbps (per GPU), 481.850 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.939 Gbps (per GPU), 479.511 Gbps (aggregated)
The layer-level communication performance: 59.930 Gbps (per GPU), 479.442 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.695 Gbps (per GPU), 477.564 Gbps (aggregated)
The layer-level communication performance: 59.662 Gbps (per GPU), 477.295 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.424 Gbps (per GPU), 475.390 Gbps (aggregated)
The layer-level communication performance: 59.374 Gbps (per GPU), 474.992 Gbps (aggregated)
The layer-level communication performance: 59.345 Gbps (per GPU), 474.758 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.680 Gbps (per GPU), 1269.441 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.683 Gbps (per GPU), 1269.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.683 Gbps (per GPU), 1269.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.668 Gbps (per GPU), 1269.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.683 Gbps (per GPU), 1269.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.680 Gbps (per GPU), 1269.439 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.680 Gbps (per GPU), 1269.439 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.683 Gbps (per GPU), 1269.462 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.984 Gbps (per GPU), 831.873 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.984 Gbps (per GPU), 831.873 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.986 Gbps (per GPU), 831.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.987 Gbps (per GPU), 831.894 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.942 Gbps (per GPU), 831.536 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.903 Gbps (per GPU), 831.220 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.901 Gbps (per GPU), 831.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.859 Gbps (per GPU), 830.871 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.305 Gbps (per GPU), 290.440 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.306 Gbps (per GPU), 290.448 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.304 Gbps (per GPU), 290.429 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.306 Gbps (per GPU), 290.446 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.305 Gbps (per GPU), 290.438 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.301 Gbps (per GPU), 290.410 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.303 Gbps (per GPU), 290.422 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.298 Gbps (per GPU), 290.382 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  5.09ms  0.44ms  0.57ms 11.53  4.31K  0.10M
 chk_1  4.22ms  0.44ms  0.57ms  9.66  4.31K  0.10M
 chk_2  4.26ms  0.43ms  0.56ms 10.00  4.31K  0.06M
 chk_3  4.26ms  0.43ms  0.56ms  9.99  4.31K  0.06M
 chk_4  4.26ms  0.41ms  0.54ms 10.38  4.31K  0.04M
 chk_5  5.16ms  0.40ms  1.10ms 12.76  4.31K  0.04M
 chk_6  4.26ms  0.41ms  0.54ms 10.43  4.31K  0.04M
 chk_7  4.26ms  0.41ms  0.54ms 10.42  4.31K  0.06M
   Avg  4.47  0.42  0.62
   Max  5.16  0.44  1.10
   Min  4.22  0.40  0.54
 Ratio  1.22  1.09  2.04
   Var  0.14  0.00  0.03
Profiling takes 0.584 s
*** Node 0, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 4311
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 12935, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 17247, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 8623, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 21558, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 4311, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 25870, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 30181, Num Local Vertices: 4312
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 34236
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.5971	TrainAcc 0.8400	ValidAcc 0.7880	TestAcc 0.8010	BestValid 0.7880
	Epoch 50:	Loss 0.1246	TrainAcc 0.9900	ValidAcc 0.9480	TestAcc 0.9420	BestValid 0.9480
****** Epoch Time (Excluding Evaluation Cost): 0.060 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.092 ms (Max: 0.157, Min: 0.040, Sum: 0.739)
Cluster-Wide Average, Compute: 18.987 ms (Max: 19.637, Min: 18.448, Sum: 151.898)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.008, Sum: 0.067)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.013, Sum: 0.117)
Cluster-Wide Average, Communication-Graph: 36.283 ms (Max: 36.797, Min: 35.673, Sum: 290.268)
Cluster-Wide Average, Optimization: 3.792 ms (Max: 3.812, Min: 3.784, Sum: 30.339)
Cluster-Wide Average, Others: 0.387 ms (Max: 0.412, Min: 0.359, Sum: 3.097)
****** Breakdown Sum: 59.566 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.796 GB (Max: 4.296, Min: 3.712, Sum: 30.370)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.625 Gbps (Max: 41.290, Min: 8.494, Sum: 212.998)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.816 GB
Weight-sync communication (cluster-wide, per-epoch): 0.061 GB
Total communication (cluster-wide, per-epoch): 0.877 GB
****** Accuracy Results ******
Highest valid_acc: 0.9480
Target test_acc: 0.9420
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
