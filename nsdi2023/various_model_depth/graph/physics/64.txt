Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT

Initialized node 5 on machine gnerv8
Initialized node 4 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
        It takes 0.012 seconds.
        It takes 0.012 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.013 seconds.
        It takes 0.019 seconds.
        It takes 0.012 seconds.
        It takes 0.013 seconds.
        It takes 0.013 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.482 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.497 seconds.
        It takes 0.497 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.003 seconds.
        It takes 0.508 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.598 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/physics/8_parts
The number of GCNII layers: 64
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 8
        It takes 0.603 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.609 seconds.
Building the Label Vector...
        It takes 0.609 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 4311) 1-[4311, 8623) 2-[8623, 12935) 3-[12935, 17247) 4-[17247, 21558) 5-[21558, 25870) 6-[25870, 30181) 7-[30181, 34493)
34493, 530417, 530417
Number of vertices per chunk: 4312
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.622 Gbps (per GPU), 484.973 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.315 Gbps (per GPU), 482.517 Gbps (aggregated)
The layer-level communication performance: 60.304 Gbps (per GPU), 482.431 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.063 Gbps (per GPU), 480.502 Gbps (aggregated)
The layer-level communication performance: 60.037 Gbps (per GPU), 480.297 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.810 Gbps (per GPU), 478.480 Gbps (aggregated)
The layer-level communication performance: 59.758 Gbps (per GPU), 478.066 Gbps (aggregated)
The layer-level communication performance: 59.725 Gbps (per GPU), 477.798 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.470 Gbps (per GPU), 1267.760 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.473 Gbps (per GPU), 1267.784 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.467 Gbps (per GPU), 1267.736 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.473 Gbps (per GPU), 1267.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.467 Gbps (per GPU), 1267.740 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.474 Gbps (per GPU), 1267.788 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.470 Gbps (per GPU), 1267.760 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.476 Gbps (per GPU), 1267.808 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.629 Gbps (per GPU), 837.033 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.634 Gbps (per GPU), 837.068 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.629 Gbps (per GPU), 837.033 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.636 Gbps (per GPU), 837.089 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.613 Gbps (per GPU), 836.901 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.635 Gbps (per GPU), 837.082 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.587 Gbps (per GPU), 836.699 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.635 Gbps (per GPU), 837.081 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.058 Gbps (per GPU), 304.463 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.058 Gbps (per GPU), 304.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.056 Gbps (per GPU), 304.447 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.058 Gbps (per GPU), 304.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.058 Gbps (per GPU), 304.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.051 Gbps (per GPU), 304.412 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.046 Gbps (per GPU), 304.371 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.027 Gbps (per GPU), 304.214 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  4.23ms  0.44ms  0.57ms  9.51  4.31K  0.10M
 chk_1  4.25ms  0.44ms  0.57ms  9.71  4.31K  0.10M
 chk_2  4.27ms  0.43ms  1.14ms  9.99  4.31K  0.06M
 chk_3  4.82ms  0.43ms  0.56ms 11.31  4.31K  0.06M
 chk_4  4.27ms  0.41ms  0.54ms 10.45  4.31K  0.04M
 chk_5  4.27ms  0.40ms  0.54ms 10.59  4.31K  0.04M
 chk_6  4.27ms  0.41ms  0.54ms 10.49  4.31K  0.04M
 chk_7  4.26ms  0.41ms  0.54ms 10.46  4.31K  0.06M
   Avg  4.33  0.42  0.62
   Max  4.82  0.44  1.14
   Min  4.23  0.40  0.54
 Ratio  1.14  1.10  2.13
   Var  0.03  0.00  0.04
Profiling takes 0.568 s
*** Node 0, starting model training...
*** Node 2, starting model training...
*** Node 1, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 4311
*** Node 5, starting model training...
*** Node 7, starting model training...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 30181, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 8623, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 21558, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 12935, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 4311, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 25870, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 17247, Num Local Vertices: 4311
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 457)...
+++++++++ Node 5 initializing the weights for op[0, 457)...
+++++++++ Node 1 initializing the weights for op[0, 457)...
+++++++++ Node 3 initializing the weights for op[0, 457)...
+++++++++ Node 2 initializing the weights for op[0, 457)...
+++++++++ Node 4 initializing the weights for op[0, 457)...
+++++++++ Node 7 initializing the weights for op[0, 457)...
+++++++++ Node 6 initializing the weights for op[0, 457)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 34236
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6062	TrainAcc 0.4600	ValidAcc 0.5200	TestAcc 0.5390	BestValid 0.5200
	Epoch 50:	Loss 0.1534	TrainAcc 0.9900	ValidAcc 0.9460	TestAcc 0.9430	BestValid 0.9460
****** Epoch Time (Excluding Evaluation Cost): 0.114 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.111 ms (Max: 0.145, Min: 0.055, Sum: 0.886)
Cluster-Wide Average, Compute: 33.279 ms (Max: 34.672, Min: 32.380, Sum: 266.232)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.011, Min: 0.007, Sum: 0.067)
Cluster-Wide Average, Bubble-Imbalance: 0.014 ms (Max: 0.016, Min: 0.011, Sum: 0.114)
Cluster-Wide Average, Communication-Graph: 73.397 ms (Max: 74.351, Min: 71.919, Sum: 587.175)
Cluster-Wide Average, Optimization: 6.315 ms (Max: 6.348, Min: 6.281, Sum: 50.518)
Cluster-Wide Average, Others: 0.728 ms (Max: 0.840, Min: 0.642, Sum: 5.827)
****** Breakdown Sum: 113.852 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.830 GB (Max: 5.567, Min: 4.712, Sum: 38.642)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.546 Gbps (Max: 41.241, Min: 8.427, Sum: 212.367)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 1.632 GB
Weight-sync communication (cluster-wide, per-epoch): 0.077 GB
Total communication (cluster-wide, per-epoch): 1.710 GB
****** Accuracy Results ******
Highest valid_acc: 0.9460
Target test_acc: 0.9430
Epoch to reach the target acc: 49
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
