Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
DONE MPI INIT
Initialized node 4 on machine gnerv8
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
        It takes 0.012 seconds.
Building the Feature Vector...
        It takes 0.013 seconds.
        It takes 0.013 seconds.
Building the Feature Vector...
        It takes 0.014 seconds.
        It takes 0.015 seconds.
        It takes 0.015 seconds.
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.472 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.471 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.502 seconds.
Building the Label Vector...
        It takes 0.505 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.007 seconds.
        It takes 0.589 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/physics/8_parts
The number of GCNII layers: 128
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 8
        It takes 0.596 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.602 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.609 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 4311) 1-[4311, 8623) 2-[8623, 12935) 3-[12935, 17247) 4-[17247, 21558) 5-[21558, 25870) 6-[25870, 30181) 7-[30181, 34493)
34493, 530417, 530417
Number of vertices per chunk: 4312
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.701 Gbps (per GPU), 485.606 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.403 Gbps (per GPU), 483.226 Gbps (aggregated)
The layer-level communication performance: 60.381 Gbps (per GPU), 483.049 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.138 Gbps (per GPU), 481.106 Gbps (aggregated)
The layer-level communication performance: 60.112 Gbps (per GPU), 480.893 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.891 Gbps (per GPU), 479.129 Gbps (aggregated)
The layer-level communication performance: 59.844 Gbps (per GPU), 478.751 Gbps (aggregated)
The layer-level communication performance: 59.814 Gbps (per GPU), 478.510 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 161.902 Gbps (per GPU), 1295.213 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.898 Gbps (per GPU), 1295.188 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.902 Gbps (per GPU), 1295.213 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.895 Gbps (per GPU), 1295.163 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.905 Gbps (per GPU), 1295.238 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.892 Gbps (per GPU), 1295.138 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.902 Gbps (per GPU), 1295.214 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.902 Gbps (per GPU), 1295.213 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.670 Gbps (per GPU), 837.361 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.670 Gbps (per GPU), 837.361 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.672 Gbps (per GPU), 837.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.667 Gbps (per GPU), 837.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.669 Gbps (per GPU), 837.354 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.667 Gbps (per GPU), 837.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.667 Gbps (per GPU), 837.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.666 Gbps (per GPU), 837.326 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.940 Gbps (per GPU), 311.521 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.941 Gbps (per GPU), 311.526 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.941 Gbps (per GPU), 311.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.938 Gbps (per GPU), 311.504 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.936 Gbps (per GPU), 311.485 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.938 Gbps (per GPU), 311.507 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.931 Gbps (per GPU), 311.446 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.930 Gbps (per GPU), 311.441 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  4.23ms  0.44ms  0.55ms  9.69  4.31K  0.10M
 chk_1  4.25ms  0.43ms  0.55ms  9.84  4.31K  0.10M
 chk_2  4.26ms  0.41ms  0.54ms 10.30  4.31K  0.06M
 chk_3  4.78ms  0.41ms  1.11ms 11.56  4.31K  0.06M
 chk_4  4.25ms  0.40ms  0.52ms 10.67  4.31K  0.04M
 chk_5  4.37ms  0.39ms  0.52ms 11.10  4.31K  0.04M
 chk_6  4.26ms  0.40ms  0.52ms 10.76  4.31K  0.04M
 chk_7  4.24ms  0.40ms  0.52ms 10.67  4.31K  0.06M
   Avg  4.33  0.41  0.60
   Max  4.78  0.44  1.11
   Min  4.23  0.39  0.52
 Ratio  1.13  1.11  2.16
   Var  0.03  0.00  0.04
Profiling takes 0.566 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 905)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 4311
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 905)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 21558, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 905)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 30181, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 905)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 8623, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 905)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 4311, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 905)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 12935, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 905)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 25870, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 905)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 17247, Num Local Vertices: 4311
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[0, 905)...
+++++++++ Node 3 initializing the weights for op[0, 905)...
+++++++++ Node 5 initializing the weights for op[0, 905)...
+++++++++ Node 1 initializing the weights for op[0, 905)...
+++++++++ Node 0 initializing the weights for op[0, 905)...
+++++++++ Node 2 initializing the weights for op[0, 905)...
+++++++++ Node 4 initializing the weights for op[0, 905)...
+++++++++ Node 7 initializing the weights for op[0, 905)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 34236
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6096	TrainAcc 0.6900	ValidAcc 0.7900	TestAcc 0.7930	BestValid 0.7900
	Epoch 50:	Loss 0.1713	TrainAcc 0.9900	ValidAcc 0.9400	TestAcc 0.9450	BestValid 0.9400
****** Epoch Time (Excluding Evaluation Cost): 0.220 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.104 ms (Max: 0.134, Min: 0.038, Sum: 0.834)
Cluster-Wide Average, Compute: 62.453 ms (Max: 63.482, Min: 61.560, Sum: 499.620)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.011, Min: 0.007, Sum: 0.069)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.013, Sum: 0.117)
Cluster-Wide Average, Communication-Graph: 144.984 ms (Max: 146.077, Min: 144.038, Sum: 1159.874)
Cluster-Wide Average, Optimization: 11.485 ms (Max: 11.575, Min: 11.405, Sum: 91.881)
Cluster-Wide Average, Others: 1.377 ms (Max: 1.544, Min: 1.253, Sum: 11.019)
****** Breakdown Sum: 220.427 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.898 GB (Max: 8.108, Min: 6.712, Sum: 55.183)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.890 Gbps (Max: 41.471, Min: 8.876, Sum: 215.117)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 3.265 GB
Weight-sync communication (cluster-wide, per-epoch): 0.111 GB
Total communication (cluster-wide, per-epoch): 3.376 GB
****** Accuracy Results ******
Highest valid_acc: 0.9400
Target test_acc: 0.9450
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
