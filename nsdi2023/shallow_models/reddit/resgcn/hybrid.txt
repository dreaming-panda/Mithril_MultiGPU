Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
DONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INITInitialized node 0 on machine gnerv2

Initialized node 3 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.450 seconds.
Building the CSC structure...
        It takes 2.455 seconds.
Building the CSC structure...
        It takes 2.457 seconds.
Building the CSC structure...
        It takes 2.727 seconds.
Building the CSC structure...
        It takes 2.750 seconds.
Building the CSC structure...
        It takes 2.751 seconds.
Building the CSC structure...
        It takes 2.772 seconds.
Building the CSC structure...
        It takes 2.790 seconds.
Building the CSC structure...
        It takes 2.159 seconds.
        It takes 2.358 seconds.
        It takes 2.416 seconds.
        It takes 2.523 seconds.
        It takes 2.902 seconds.
        It takes 2.901 seconds.
        It takes 2.862 seconds.
        It takes 2.943 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.470 seconds.
        It takes 0.839 seconds.
        It takes 0.893 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Feature Vector...
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
        It takes 0.066 seconds.
        It takes 0.067 seconds.
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.318 seconds.
        It takes 0.717 seconds.
        It takes 0.388 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.082 seconds.
        It takes 0.082 seconds.
        It takes 0.082 seconds.
        It takes 0.277 seconds.
Building the Label Vector...
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.049 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/32_parts
The number of GCNII layers: 4
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 7497) 1-[7497, 14564) 2-[14564, 21631) 3-[21631, 28706) 4-[28706, 35773) 5-[35773, 43271) 6-[43271, 50769) 7-[50769, 57836) 8-[57836, 65334) ... 31-[225467, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.827 Gbps (per GPU), 446.615 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.583 Gbps (per GPU), 444.663 Gbps (aggregated)
The layer-level communication performance: 55.579 Gbps (per GPU), 444.634 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.355 Gbps (per GPU), 442.839 Gbps (aggregated)
The layer-level communication performance: 55.336 Gbps (per GPU), 442.690 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.167 Gbps (per GPU), 441.334 Gbps (aggregated)
The layer-level communication performance: 55.116 Gbps (per GPU), 440.931 Gbps (aggregated)
The layer-level communication performance: 55.099 Gbps (per GPU), 440.791 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.393 Gbps (per GPU), 1251.144 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.378 Gbps (per GPU), 1251.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.361 Gbps (per GPU), 1250.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.396 Gbps (per GPU), 1251.166 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.358 Gbps (per GPU), 1250.864 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.376 Gbps (per GPU), 1251.004 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.364 Gbps (per GPU), 1250.911 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.358 Gbps (per GPU), 1250.864 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.582 Gbps (per GPU), 804.656 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.576 Gbps (per GPU), 804.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.582 Gbps (per GPU), 804.656 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.583 Gbps (per GPU), 804.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.583 Gbps (per GPU), 804.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.582 Gbps (per GPU), 804.656 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.583 Gbps (per GPU), 804.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.582 Gbps (per GPU), 804.657 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.166 Gbps (per GPU), 281.330 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.165 Gbps (per GPU), 281.321 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.166 Gbps (per GPU), 281.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.166 Gbps (per GPU), 281.329 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.165 Gbps (per GPU), 281.321 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.166 Gbps (per GPU), 281.330 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.165 Gbps (per GPU), 281.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.166 Gbps (per GPU), 281.329 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.64ms  4.15ms  5.04ms  1.38  7.50K  5.22M
 chk_1  3.69ms  4.09ms  4.90ms  1.33  7.07K  5.00M
 chk_2  2.82ms  3.25ms  4.09ms  1.45  7.07K  3.66M
 chk_3  2.42ms  2.84ms  3.68ms  1.52  7.08K  3.05M
 chk_4  3.81ms  4.21ms  5.05ms  1.32  7.07K  5.22M
 chk_5  6.48ms  6.90ms  7.79ms  1.20  7.50K  9.29M
 chk_6  4.35ms  4.78ms  5.65ms  1.30  7.50K  6.01M
 chk_7  3.61ms  4.01ms  4.84ms  1.34  7.07K  4.64M
 chk_8  2.27ms  2.72ms  3.57ms  1.57  7.50K  2.67M
 chk_9  2.54ms  2.97ms  3.79ms  1.49  7.07K  3.20M
chk_10  2.24ms  2.68ms  3.50ms  1.57  7.12K  2.85M
chk_11  1.99ms  2.45ms  3.32ms  1.67  7.50K  2.35M
chk_12  5.01ms  5.49ms  6.39ms  1.28  7.50K  5.46M
chk_13  2.57ms  3.12ms  3.83ms  1.49  7.07K  3.20M
chk_14  2.52ms  2.98ms  3.85ms  1.53  7.50K  2.88M
chk_15  2.44ms  2.86ms  3.72ms  1.53  7.50K  3.06M
chk_16  2.36ms  2.77ms  3.64ms  1.54  7.50K  2.97M
chk_17  1.85ms  2.30ms  3.17ms  1.71  7.50K  1.98M
chk_18  1.29ms  1.76ms  2.63ms  2.04  7.50K  1.01M
chk_19  1.36ms  1.82ms  2.70ms  1.98  7.50K  1.17M
chk_20  2.10ms  2.52ms  3.38ms  1.61  7.06K  2.46M
chk_21  2.32ms  2.71ms  3.58ms  1.54  7.07K  2.80M
chk_22  2.02ms  2.45ms  3.30ms  1.63  7.06K  2.32M
chk_23  3.56ms  3.99ms  4.88ms  1.37  7.50K  4.36M
chk_24  1.91ms  2.34ms  3.20ms  1.67  7.07K  2.03M
chk_25  2.07ms  2.51ms  3.39ms  1.64  7.45K  2.34M
chk_26  3.54ms  3.91ms  4.94ms  1.40  7.00K  5.06M
chk_27  3.72ms  4.05ms  4.87ms  1.31  7.07K  4.98M
chk_28  2.62ms  3.00ms  3.86ms  1.47  7.07K  3.20M
chk_29  3.77ms  4.13ms  4.99ms  1.32  7.50K  5.03M
chk_30  2.03ms  2.43ms  3.28ms  1.62  7.07K  2.21M
chk_31  2.30ms  2.70ms  3.56ms  1.55  7.50K  2.90M
   Avg  2.85  3.28  4.14
   Max  6.48  6.90  7.79
   Min  1.29  1.76  2.63
 Ratio  5.03  3.92  2.96
   Var  1.17  1.16  1.18
Profiling takes 3.788 s
*** Node 0, starting model training...
Num Stages: 2 / 2
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 22)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 57836
*** Node 1, starting model training...
Num Stages: 2 / 2
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 22)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 57836, Num Local Vertices: 58746
*** Node 2, starting model training...
Num Stages: 2 / 2
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [0, 22)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 116582, Num Local Vertices: 58676
*** Node 3, starting model training...
Num Stages: 2 / 2
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [0, 22)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 175258, Num Local Vertices: 57707
*** Node 4, starting model training...
Num Stages: 2 / 2
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [22, 57)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 57836
*** Node 5, starting model training...
Num Stages: 2 / 2
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [22, 57)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 57836, Num Local Vertices: 58746
*** Node 6, starting model training...
Num Stages: 2 / 2
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [22, 57)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 116582, Num Local Vertices: 58676
*** Node 7, starting model training...
Num Stages: 2 / 2
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [22, 57)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 175258, Num Local Vertices: 57707
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 22)...
+++++++++ Node 2 initializing the weights for op[0, 22)...
+++++++++ Node 3 initializing the weights for op[0, 22)...
+++++++++ Node 0 initializing the weights for op[0, 22)...
+++++++++ Node 6 initializing the weights for op[22, 57)...
+++++++++ Node 4 initializing the weights for op[22, 57)...
+++++++++ Node 5 initializing the weights for op[22, 57)...
+++++++++ Node 7 initializing the weights for op[22, 57)...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 909120
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.3818	TrainAcc 0.0340	ValidAcc 0.0327	TestAcc 0.0283	BestValid 0.0327
	Epoch 50:	Loss 0.8473	TrainAcc 0.8644	ValidAcc 0.8731	TestAcc 0.8724	BestValid 0.8731
****** Epoch Time (Excluding Evaluation Cost): 0.104 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 8.740 ms (Max: 12.364, Min: 5.164, Sum: 69.918)
Cluster-Wide Average, Compute: 48.434 ms (Max: 62.359, Min: 36.953, Sum: 387.469)
Cluster-Wide Average, Communication-Layer: 5.936 ms (Max: 7.400, Min: 4.471, Sum: 47.485)
Cluster-Wide Average, Bubble-Imbalance: 6.511 ms (Max: 11.247, Min: 1.723, Sum: 52.088)
Cluster-Wide Average, Communication-Graph: 30.086 ms (Max: 36.401, Min: 21.991, Sum: 240.687)
Cluster-Wide Average, Optimization: 1.092 ms (Max: 1.307, Min: 0.879, Sum: 8.739)
Cluster-Wide Average, Others: 2.886 ms (Max: 4.677, Min: 1.130, Sum: 23.085)
****** Breakdown Sum: 103.684 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.256 GB (Max: 5.282, Min: 4.001, Sum: 34.048)
Cluster-Wide Average, Graph-Level Communication Throughput: 53.782 Gbps (Max: 72.394, Min: 41.081, Sum: 430.256)
Cluster-Wide Average, Layer-Level Communication Throughput: 33.123 Gbps (Max: 41.303, Min: 25.009, Sum: 264.986)
Layer-level communication (cluster-wide, per-epoch): 0.174 GB
Graph-level communication (cluster-wide, per-epoch): 1.355 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 1.532 GB
****** Accuracy Results ******
Highest valid_acc: 0.8731
Target test_acc: 0.8724
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
