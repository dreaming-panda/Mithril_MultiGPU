Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.906 seconds.
Building the CSC structure...
        It takes 2.305 seconds.
Building the CSC structure...
        It takes 2.431 seconds.
Building the CSC structure...
        It takes 2.440 seconds.
Building the CSC structure...
        It takes 2.477 seconds.
Building the CSC structure...
        It takes 2.573 seconds.
Building the CSC structure...
        It takes 2.693 seconds.
Building the CSC structure...
        It takes 2.696 seconds.
Building the CSC structure...
        It takes 1.848 seconds.
        It takes 2.293 seconds.
Building the Feature Vector...
        It takes 2.354 seconds.
        It takes 2.337 seconds.
        It takes 2.448 seconds.
        It takes 2.283 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 2.384 seconds.
        It takes 2.999 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.249 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
GPU 0, layer [0, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 0.039 seconds.
Building the Feature Vector...
        It takes 0.303 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 4
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.278 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
GPU 0, layer [0, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.261 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.037 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.170 Gbps (per GPU), 441.356 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.935 Gbps (per GPU), 439.480 Gbps (aggregated)
The layer-level communication performance: 54.932 Gbps (per GPU), 439.456 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.731 Gbps (per GPU), 437.851 Gbps (aggregated)
The layer-level communication performance: 54.700 Gbps (per GPU), 437.596 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.507 Gbps (per GPU), 436.053 Gbps (aggregated)
The layer-level communication performance: 54.474 Gbps (per GPU), 435.791 Gbps (aggregated)
The layer-level communication performance: 54.440 Gbps (per GPU), 435.518 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.833 Gbps (per GPU), 1270.664 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.833 Gbps (per GPU), 1270.664 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.821 Gbps (per GPU), 1270.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.827 Gbps (per GPU), 1270.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.821 Gbps (per GPU), 1270.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.824 Gbps (per GPU), 1270.594 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.830 Gbps (per GPU), 1270.640 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.734 Gbps (per GPU), 1269.871 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.363 Gbps (per GPU), 818.907 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.364 Gbps (per GPU), 818.913 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.363 Gbps (per GPU), 818.907 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.363 Gbps (per GPU), 818.907 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.363 Gbps (per GPU), 818.900 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.363 Gbps (per GPU), 818.907 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.363 Gbps (per GPU), 818.907 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.363 Gbps (per GPU), 818.900 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.440 Gbps (per GPU), 283.520 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.440 Gbps (per GPU), 283.517 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.440 Gbps (per GPU), 283.523 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.439 Gbps (per GPU), 283.512 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.437 Gbps (per GPU), 283.497 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.439 Gbps (per GPU), 283.515 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.438 Gbps (per GPU), 283.508 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.436 Gbps (per GPU), 283.491 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.51ms  9.50ms  9.82ms  3.92 29.12K 14.23M
 chk_1  2.51ms  5.13ms  5.50ms  2.19 29.12K  6.56M
 chk_2  2.53ms 16.48ms 16.88ms  6.67 29.12K 24.68M
 chk_3  2.53ms 16.64ms 16.91ms  6.69 29.12K 22.95M
 chk_4  2.53ms  4.95ms  5.29ms  2.09 29.12K  6.33M
 chk_5  2.53ms  8.93ms  9.11ms  3.60 29.12K 12.05M
 chk_6  2.53ms 10.03ms 10.32ms  4.08 29.12K 14.60M
 chk_7  2.53ms  9.25ms  9.51ms  3.76 29.12K 13.21M
   Avg  2.52 10.11 10.42
   Max  2.53 16.64 16.91
   Min  2.51  4.95  5.29
 Ratio  1.01  3.36  3.20
   Var  0.00 17.15 17.16
Profiling takes 2.141 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 37)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 37)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 37)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 37)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 37)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 37)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 37)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 6 initializing the weights for op[0, 37)...
+++++++++ Node 1 initializing the weights for op[0, 37)...
+++++++++ Node 7 initializing the weights for op[0, 37)...
+++++++++ Node 3 initializing the weights for op[0, 37)...
+++++++++ Node 4 initializing the weights for op[0, 37)...
+++++++++ Node 2 initializing the weights for op[0, 37)...
+++++++++ Node 5 initializing the weights for op[0, 37)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.9043	TrainAcc 0.0132	ValidAcc 0.0116	TestAcc 0.0115	BestValid 0.0116
	Epoch 50:	Loss 1.4539	TrainAcc 0.7231	ValidAcc 0.7392	TestAcc 0.7351	BestValid 0.7392
****** Epoch Time (Excluding Evaluation Cost): 0.112 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 3.078 ms (Max: 4.868, Min: 0.098, Sum: 24.626)
Cluster-Wide Average, Compute: 31.397 ms (Max: 48.394, Min: 18.259, Sum: 251.179)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.014 ms (Max: 0.015, Min: 0.013, Sum: 0.111)
Cluster-Wide Average, Communication-Graph: 76.139 ms (Max: 87.467, Min: 62.133, Sum: 609.110)
Cluster-Wide Average, Optimization: 0.636 ms (Max: 0.646, Min: 0.625, Sum: 5.084)
Cluster-Wide Average, Others: 0.537 ms (Max: 0.556, Min: 0.529, Sum: 4.298)
****** Breakdown Sum: 111.809 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.630 GB (Max: 5.032, Min: 4.555, Sum: 37.042)
Cluster-Wide Average, Graph-Level Communication Throughput: 27.434 Gbps (Max: 49.401, Min: 12.206, Sum: 219.475)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 1.810 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 1.816 GB
****** Accuracy Results ******
Highest valid_acc: 0.7392
Target test_acc: 0.7351
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
