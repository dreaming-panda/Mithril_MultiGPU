Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2

Initialized node 0 on machine gnerv2
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3

Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.906 seconds.
Building the CSC structure...
        It takes 2.299 seconds.
Building the CSC structure...
        It takes 2.349 seconds.
Building the CSC structure...
        It takes 2.377 seconds.
Building the CSC structure...
        It takes 2.415 seconds.
Building the CSC structure...
        It takes 2.469 seconds.
Building the CSC structure...
        It takes 2.503 seconds.
Building the CSC structure...
        It takes 2.703 seconds.
Building the CSC structure...
        It takes 1.818 seconds.
        It takes 2.280 seconds.
Building the Feature Vector...
        It takes 2.368 seconds.
        It takes 2.348 seconds.
        It takes 2.407 seconds.
        It takes 2.329 seconds.
        It takes 2.506 seconds.
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 2.308 seconds.
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 3)
GPU 1, layer [3, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.259 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.043 seconds.
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
        It takes 0.320 seconds.
Building the Label Vector...
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.043 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/32_parts
The number of GCNII layers: 4
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
GPU 0, layer [0, 3)
GPU 1, layer [3, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.043 seconds.
GPU 0, layer [0, 3)
GPU 1, layer [3, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 3)
GPU 1, layer [3, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 3)
GPU 1, layer [3, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 3)
GPU 1, layer [3, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 7497) 1-[7497, 14564) 2-[14564, 21631) 3-[21631, 28706) 4-[28706, 35773) 5-[35773, 43271) 6-[43271, 50769) 7-[50769, 57836) 8-[57836, 65334) ... 31-[225467, 232965)
GPU 0, layer [0, 3)
GPU 1, layer [3, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 3)
GPU 1, layer [3, 5)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 57.102 Gbps (per GPU), 456.818 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.862 Gbps (per GPU), 454.895 Gbps (aggregated)
The layer-level communication performance: 56.871 Gbps (per GPU), 454.967 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.616 Gbps (per GPU), 452.926 Gbps (aggregated)
The layer-level communication performance: 56.602 Gbps (per GPU), 452.815 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.401 Gbps (per GPU), 451.212 Gbps (aggregated)
The layer-level communication performance: 56.350 Gbps (per GPU), 450.798 Gbps (aggregated)
The layer-level communication performance: 56.332 Gbps (per GPU), 450.657 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.829 Gbps (per GPU), 1254.629 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.814 Gbps (per GPU), 1254.511 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.799 Gbps (per GPU), 1254.395 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.820 Gbps (per GPU), 1254.559 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.799 Gbps (per GPU), 1254.395 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.817 Gbps (per GPU), 1254.535 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.799 Gbps (per GPU), 1254.395 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.800 Gbps (per GPU), 1254.399 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.576 Gbps (per GPU), 804.604 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.578 Gbps (per GPU), 804.624 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.576 Gbps (per GPU), 804.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.577 Gbps (per GPU), 804.618 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.580 Gbps (per GPU), 804.637 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.577 Gbps (per GPU), 804.618 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.577 Gbps (per GPU), 804.618 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.579 Gbps (per GPU), 804.629 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.390 Gbps (per GPU), 283.122 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.390 Gbps (per GPU), 283.122 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.389 Gbps (per GPU), 283.115 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.389 Gbps (per GPU), 283.114 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.390 Gbps (per GPU), 283.117 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.389 Gbps (per GPU), 283.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.390 Gbps (per GPU), 283.118 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.388 Gbps (per GPU), 283.104 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.81ms  3.39ms  3.66ms  4.50  7.50K  5.22M
 chk_1  0.78ms  3.51ms  3.61ms  4.63  7.07K  5.00M
 chk_2  0.78ms  2.63ms  2.77ms  3.56  7.07K  3.66M
 chk_3  0.78ms  2.23ms  2.37ms  3.05  7.08K  3.05M
 chk_4  0.78ms  3.59ms  3.73ms  4.79  7.07K  5.22M
 chk_5  0.81ms  6.27ms  6.54ms  8.05  7.50K  9.29M
 chk_6  0.81ms  4.13ms  4.24ms  5.22  7.50K  6.01M
 chk_7  0.78ms  3.37ms  3.51ms  4.52  7.07K  4.64M
 chk_8  0.81ms  2.04ms  2.50ms  3.07  7.50K  2.67M
 chk_9  0.78ms  2.33ms  2.47ms  3.18  7.07K  3.20M
chk_10  0.78ms  2.02ms  2.20ms  2.81  7.12K  2.85M
chk_11  0.81ms  1.77ms  1.96ms  2.41  7.50K  2.35M
chk_12  0.81ms  4.76ms  5.19ms  6.37  7.50K  5.46M
chk_13  0.78ms  2.33ms  2.48ms  3.19  7.07K  3.20M
chk_14  0.81ms  2.28ms  2.44ms  3.00  7.50K  2.88M
chk_15  0.81ms  2.19ms  2.62ms  3.21  7.50K  3.06M
chk_16  0.81ms  2.11ms  2.24ms  2.76  7.50K  2.97M
chk_17  0.82ms  1.62ms  1.78ms  2.18  7.50K  1.98M
chk_18  0.81ms  1.06ms  1.22ms  1.50  7.50K  1.01M
chk_19  0.81ms  1.14ms  1.30ms  1.60  7.50K  1.17M
chk_20  0.78ms  1.87ms  2.04ms  2.63  7.06K  2.46M
chk_21  0.78ms  2.06ms  2.21ms  2.84  7.07K  2.80M
chk_22  0.78ms  1.78ms  1.96ms  2.53  7.06K  2.32M
chk_23  0.81ms  3.30ms  3.47ms  4.27  7.50K  4.36M
chk_24  0.78ms  1.67ms  2.21ms  2.83  7.07K  2.03M
chk_25  0.81ms  1.83ms  1.98ms  2.44  7.45K  2.34M
chk_26  0.78ms  3.29ms  3.42ms  4.41  7.00K  5.06M
chk_27  0.78ms  3.43ms  3.58ms  4.60  7.07K  4.98M
chk_28  0.78ms  2.37ms  2.50ms  3.21  7.07K  3.20M
chk_29  0.81ms  3.49ms  3.63ms  4.46  7.50K  5.03M
chk_30  0.78ms  1.78ms  1.94ms  2.48  7.07K  2.21M
chk_31  0.81ms  2.04ms  2.56ms  3.15  7.50K  2.90M
   Avg  0.80  2.62  2.82
   Max  0.82  6.27  6.54
   Min  0.78  1.06  1.22
 Ratio  1.05  5.93  5.36
   Var  0.00  1.17  1.19
Profiling takes 2.389 s
*** Node 0, starting model training...
Num Stages: 2 / 2
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 20)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 57836
*** Node 1, starting model training...
Num Stages: 2 / 2
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 20)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 57836, Num Local Vertices: 58746
*** Node 2, starting model training...
Num Stages: 2 / 2
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [0, 20)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 116582, Num Local Vertices: 58676
*** Node 3, starting model training...
Num Stages: 2 / 2
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [0, 20)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 175258, Num Local Vertices: 57707
*** Node 4, starting model training...
Num Stages: 2 / 2
*** Node 5, starting model training...
Num Stages: 2 / 2
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [20, 37)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 57836, Num Local Vertices: 58746
*** Node 6, starting model training...
Num Stages: 2 / 2
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [20, 37)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 116582, Num Local Vertices: 58676
*** Node 7, starting model training...
Num Stages: 2 / 2
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [20, 37)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 175258, Num Local Vertices: 57707
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [20, 37)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 57836
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[20, 37)...
+++++++++ Node 7 initializing the weights for op[20, 37)...
+++++++++ Node 4 initializing the weights for op[20, 37)...
+++++++++ Node 5 initializing the weights for op[20, 37)...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 1 initializing the weights for op[0, 20)...
+++++++++ Node 2 initializing the weights for op[0, 20)...
+++++++++ Node 3 initializing the weights for op[0, 20)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 909120
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.8200	TrainAcc 0.0127	ValidAcc 0.0116	TestAcc 0.0114	BestValid 0.0116
	Epoch 50:	Loss 1.4945	TrainAcc 0.7212	ValidAcc 0.7370	TestAcc 0.7317	BestValid 0.7370
****** Epoch Time (Excluding Evaluation Cost): 0.099 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 8.375 ms (Max: 10.856, Min: 5.896, Sum: 67.004)
Cluster-Wide Average, Compute: 41.034 ms (Max: 48.815, Min: 35.114, Sum: 328.272)
Cluster-Wide Average, Communication-Layer: 11.584 ms (Max: 12.345, Min: 10.583, Sum: 92.670)
Cluster-Wide Average, Bubble-Imbalance: 4.322 ms (Max: 6.238, Min: 2.600, Sum: 34.578)
Cluster-Wide Average, Communication-Graph: 29.465 ms (Max: 35.451, Min: 21.772, Sum: 235.716)
Cluster-Wide Average, Optimization: 1.132 ms (Max: 1.214, Min: 1.051, Sum: 9.059)
Cluster-Wide Average, Others: 2.868 ms (Max: 4.751, Min: 1.035, Sum: 22.941)
****** Breakdown Sum: 98.780 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.292 GB (Max: 5.575, Min: 3.786, Sum: 34.339)
Cluster-Wide Average, Graph-Level Communication Throughput: 55.089 Gbps (Max: 76.090, Min: 42.426, Sum: 440.713)
Cluster-Wide Average, Layer-Level Communication Throughput: 32.271 Gbps (Max: 34.974, Min: 30.224, Sum: 258.165)
Layer-level communication (cluster-wide, per-epoch): 0.347 GB
Graph-level communication (cluster-wide, per-epoch): 1.355 GB
Weight-sync communication (cluster-wide, per-epoch): 0.002 GB
Total communication (cluster-wide, per-epoch): 1.704 GB
****** Accuracy Results ******
Highest valid_acc: 0.7370
Target test_acc: 0.7317
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
