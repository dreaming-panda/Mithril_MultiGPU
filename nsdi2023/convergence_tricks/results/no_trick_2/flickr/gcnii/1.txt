Initialized node 0 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 3 on machine gnerv2
Initialized node 7 on machine gnerv3
Initialized node 1 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 2 on machine gnerv2
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
        It takes 0.016 seconds.
        It takes 0.021 seconds.
        It takes 0.023 seconds.
        It takes 0.018 seconds.
        It takes 0.018 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.006 seconds.
        It takes 0.105 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.112 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.112 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.913 Gbps (per GPU), 487.303 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.603 Gbps (per GPU), 484.828 Gbps (aggregated)
The layer-level communication performance: 60.603 Gbps (per GPU), 484.823 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.355 Gbps (per GPU), 482.838 Gbps (aggregated)
The layer-level communication performance: 60.325 Gbps (per GPU), 482.603 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.117 Gbps (per GPU), 480.938 Gbps (aggregated)
The layer-level communication performance: 60.071 Gbps (per GPU), 480.566 Gbps (aggregated)
The layer-level communication performance: 60.037 Gbps (per GPU), 480.292 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.476 Gbps (per GPU), 1267.805 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.446 Gbps (per GPU), 1267.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.404 Gbps (per GPU), 1267.233 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.446 Gbps (per GPU), 1267.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.467 Gbps (per GPU), 1267.739 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.461 Gbps (per GPU), 1267.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.413 Gbps (per GPU), 1267.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.461 Gbps (per GPU), 1267.688 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.261 Gbps (per GPU), 818.088 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.260 Gbps (per GPU), 818.082 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.261 Gbps (per GPU), 818.088 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.260 Gbps (per GPU), 818.082 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.256 Gbps (per GPU), 818.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.255 Gbps (per GPU), 818.042 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.258 Gbps (per GPU), 818.062 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.254 Gbps (per GPU), 818.035 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.687 Gbps (per GPU), 269.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.688 Gbps (per GPU), 269.502 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.688 Gbps (per GPU), 269.504 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.685 Gbps (per GPU), 269.479 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.688 Gbps (per GPU), 269.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.682 Gbps (per GPU), 269.457 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.684 Gbps (per GPU), 269.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.682 Gbps (per GPU), 269.454 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.34ms  0.50ms  1.46  2.81K  0.03M
 chk_1  0.41ms  0.35ms  0.50ms  1.46  2.82K  0.03M
 chk_2  0.40ms  0.34ms  0.51ms  1.47  2.80K  0.03M
 chk_3  0.41ms  0.35ms  0.51ms  1.46  2.80K  0.03M
 chk_4  0.41ms  0.35ms  0.51ms  1.47  2.82K  0.03M
 chk_5  0.40ms  0.35ms  0.51ms  1.46  2.75K  0.03M
 chk_6  0.40ms  0.34ms  0.50ms  1.46  2.71K  0.03M
 chk_7  0.40ms  0.35ms  0.51ms  1.47  2.76K  0.03M
 chk_8  0.40ms  0.35ms  0.50ms  1.45  2.79K  0.03M
 chk_9  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_10  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_11  0.40ms  0.35ms  0.51ms  1.45  2.74K  0.03M
chk_12  0.40ms  0.35ms  0.51ms  1.46  2.76K  0.03M
chk_13  0.40ms  0.35ms  0.51ms  1.46  2.75K  0.03M
chk_14  0.40ms  0.34ms  0.50ms  1.47  2.81K  0.03M
chk_15  0.40ms  0.34ms  0.50ms  1.46  2.77K  0.03M
chk_16  0.40ms  0.34ms  0.50ms  1.47  2.78K  0.03M
chk_17  0.40ms  0.35ms  0.51ms  1.46  2.79K  0.03M
chk_18  0.41ms  0.35ms  0.51ms  1.46  2.82K  0.03M
chk_19  0.40ms  0.35ms  0.50ms  1.44  2.81K  0.03M
chk_20  0.40ms  0.35ms  0.51ms  1.46  2.77K  0.03M
chk_21  0.41ms  0.35ms  0.51ms  1.46  2.84K  0.02M
chk_22  0.40ms  0.34ms  0.50ms  1.46  2.78K  0.03M
chk_23  0.41ms  0.34ms  0.50ms  1.46  2.80K  0.03M
chk_24  0.40ms  0.34ms  0.50ms  1.46  2.80K  0.03M
chk_25  0.41ms  0.34ms  0.51ms  1.47  2.81K  0.03M
chk_26  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_27  0.40ms  0.35ms  0.51ms  1.46  2.79K  0.03M
chk_28  0.40ms  0.35ms  0.51ms  1.46  2.77K  0.03M
chk_29  0.40ms  0.34ms  0.50ms  1.46  2.77K  0.03M
chk_30  0.41ms  0.35ms  0.51ms  1.46  2.80K  0.03M
chk_31  0.40ms  0.35ms  0.51ms  1.46  2.78K  0.03M
   Avg  0.40  0.35  0.51
   Max  0.41  0.35  0.51
   Min  0.40  0.34  0.50
 Ratio  1.03  1.03  1.02
   Var  0.00  0.00  0.00
Profiling takes 0.595 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 55.358 ms
Partition 0 [0, 4) has cost: 46.128 ms
Partition 1 [4, 8) has cost: 44.286 ms
Partition 2 [8, 12) has cost: 44.286 ms
Partition 3 [12, 16) has cost: 44.286 ms
Partition 4 [16, 20) has cost: 44.286 ms
Partition 5 [20, 24) has cost: 44.286 ms
Partition 6 [24, 29) has cost: 55.358 ms
Partition 7 [29, 33) has cost: 49.388 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 28.127 ms
GPU 0, Compute+Comm Time: 20.155 ms, Bubble Time: 5.101 ms, Imbalance Overhead: 2.871 ms
GPU 1, Compute+Comm Time: 19.887 ms, Bubble Time: 4.997 ms, Imbalance Overhead: 3.243 ms
GPU 2, Compute+Comm Time: 19.887 ms, Bubble Time: 4.883 ms, Imbalance Overhead: 3.357 ms
GPU 3, Compute+Comm Time: 19.887 ms, Bubble Time: 4.768 ms, Imbalance Overhead: 3.471 ms
GPU 4, Compute+Comm Time: 19.887 ms, Bubble Time: 4.661 ms, Imbalance Overhead: 3.578 ms
GPU 5, Compute+Comm Time: 19.887 ms, Bubble Time: 4.551 ms, Imbalance Overhead: 3.689 ms
GPU 6, Compute+Comm Time: 23.687 ms, Bubble Time: 4.440 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 20.819 ms, Bubble Time: 4.528 ms, Imbalance Overhead: 2.781 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.877 ms
GPU 0, Compute+Comm Time: 37.946 ms, Bubble Time: 7.905 ms, Imbalance Overhead: 3.025 ms
GPU 1, Compute+Comm Time: 41.048 ms, Bubble Time: 7.828 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 33.776 ms, Bubble Time: 8.011 ms, Imbalance Overhead: 7.089 ms
GPU 3, Compute+Comm Time: 33.776 ms, Bubble Time: 8.195 ms, Imbalance Overhead: 6.905 ms
GPU 4, Compute+Comm Time: 33.776 ms, Bubble Time: 8.353 ms, Imbalance Overhead: 6.747 ms
GPU 5, Compute+Comm Time: 33.776 ms, Bubble Time: 8.540 ms, Imbalance Overhead: 6.560 ms
GPU 6, Compute+Comm Time: 33.776 ms, Bubble Time: 8.720 ms, Imbalance Overhead: 6.380 ms
GPU 7, Compute+Comm Time: 35.350 ms, Bubble Time: 8.904 ms, Imbalance Overhead: 4.622 ms
The estimated cost of the whole pipeline: 80.854 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 99.645 ms
Partition 0 [0, 8) has cost: 90.414 ms
Partition 1 [8, 17) has cost: 99.645 ms
Partition 2 [17, 25) has cost: 88.573 ms
Partition 3 [25, 33) has cost: 93.674 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.379 ms
GPU 0, Compute+Comm Time: 23.244 ms, Bubble Time: 4.624 ms, Imbalance Overhead: 2.511 ms
GPU 1, Compute+Comm Time: 25.923 ms, Bubble Time: 4.456 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 23.564 ms, Bubble Time: 4.566 ms, Imbalance Overhead: 2.249 ms
GPU 3, Compute+Comm Time: 24.033 ms, Bubble Time: 4.696 ms, Imbalance Overhead: 1.650 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.929 ms
GPU 0, Compute+Comm Time: 39.607 ms, Bubble Time: 7.528 ms, Imbalance Overhead: 1.793 ms
GPU 1, Compute+Comm Time: 37.522 ms, Bubble Time: 7.389 ms, Imbalance Overhead: 4.018 ms
GPU 2, Compute+Comm Time: 41.626 ms, Bubble Time: 7.303 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 37.859 ms, Bubble Time: 7.568 ms, Imbalance Overhead: 3.503 ms
    The estimated cost with 2 DP ways is 83.273 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 190.059 ms
Partition 0 [0, 17) has cost: 190.059 ms
Partition 1 [17, 33) has cost: 182.247 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.575 ms
GPU 0, Compute+Comm Time: 37.949 ms, Bubble Time: 4.626 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 37.162 ms, Bubble Time: 4.722 ms, Imbalance Overhead: 0.690 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 59.673 ms
GPU 0, Compute+Comm Time: 52.005 ms, Bubble Time: 6.630 ms, Imbalance Overhead: 1.038 ms
GPU 1, Compute+Comm Time: 53.190 ms, Bubble Time: 6.482 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 107.360 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 372.306 ms
Partition 0 [0, 33) has cost: 372.306 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 136.134 ms
GPU 0, Compute+Comm Time: 136.134 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 151.189 ms
GPU 0, Compute+Comm Time: 151.189 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 301.689 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9379	TrainAcc 0.4170	ValidAcc 0.4194	TestAcc 0.4197	BestValid 0.4194
	Epoch 50:	Loss 1.6996	TrainAcc 0.2614	ValidAcc 0.2611	TestAcc 0.2572	BestValid 0.4194
	Epoch 100:	Loss 1.6487	TrainAcc 0.4707	ValidAcc 0.4708	TestAcc 0.4685	BestValid 0.4708
	Epoch 150:	Loss 1.6151	TrainAcc 0.4657	ValidAcc 0.4649	TestAcc 0.4666	BestValid 0.4708
	Epoch 200:	Loss 1.5988	TrainAcc 0.4281	ValidAcc 0.4313	TestAcc 0.4301	BestValid 0.4708
	Epoch 250:	Loss 1.5886	TrainAcc 0.4517	ValidAcc 0.4521	TestAcc 0.4535	BestValid 0.4708
	Epoch 300:	Loss 1.5715	TrainAcc 0.4783	ValidAcc 0.4780	TestAcc 0.4746	BestValid 0.4780
	Epoch 350:	Loss 1.5599	TrainAcc 0.4273	ValidAcc 0.4243	TestAcc 0.4240	BestValid 0.4780
	Epoch 400:	Loss 1.5644	TrainAcc 0.2697	ValidAcc 0.2681	TestAcc 0.2643	BestValid 0.4780
	Epoch 450:	Loss 1.5457	TrainAcc 0.4672	ValidAcc 0.4651	TestAcc 0.4666	BestValid 0.4780
	Epoch 500:	Loss 1.5511	TrainAcc 0.3111	ValidAcc 0.3069	TestAcc 0.3028	BestValid 0.4780
	Epoch 550:	Loss 1.5259	TrainAcc 0.4854	ValidAcc 0.4862	TestAcc 0.4847	BestValid 0.4862
	Epoch 600:	Loss 1.5236	TrainAcc 0.4307	ValidAcc 0.4314	TestAcc 0.4320	BestValid 0.4862
	Epoch 650:	Loss 1.5158	TrainAcc 0.4285	ValidAcc 0.4297	TestAcc 0.4301	BestValid 0.4862
	Epoch 700:	Loss 1.5324	TrainAcc 0.2788	ValidAcc 0.2755	TestAcc 0.2720	BestValid 0.4862
	Epoch 750:	Loss 1.5131	TrainAcc 0.3786	ValidAcc 0.3718	TestAcc 0.3698	BestValid 0.4862
	Epoch 800:	Loss 1.5133	TrainAcc 0.3255	ValidAcc 0.3209	TestAcc 0.3188	BestValid 0.4862
	Epoch 850:	Loss 1.5063	TrainAcc 0.3546	ValidAcc 0.3485	TestAcc 0.3459	BestValid 0.4862
	Epoch 900:	Loss 1.5472	TrainAcc 0.4322	ValidAcc 0.4329	TestAcc 0.4332	BestValid 0.4862
	Epoch 950:	Loss 1.5175	TrainAcc 0.4915	ValidAcc 0.4891	TestAcc 0.4867	BestValid 0.4891
	Epoch 1000:	Loss 1.5084	TrainAcc 0.4756	ValidAcc 0.4713	TestAcc 0.4709	BestValid 0.4891
	Epoch 1050:	Loss 1.5033	TrainAcc 0.4218	ValidAcc 0.4240	TestAcc 0.4234	BestValid 0.4891
	Epoch 1100:	Loss 1.5130	TrainAcc 0.3090	ValidAcc 0.3039	TestAcc 0.3016	BestValid 0.4891
	Epoch 1150:	Loss 1.5043	TrainAcc 0.2739	ValidAcc 0.2717	TestAcc 0.2683	BestValid 0.4891
	Epoch 1200:	Loss 1.4974	TrainAcc 0.3609	ValidAcc 0.3527	TestAcc 0.3513	BestValid 0.4891
	Epoch 1250:	Loss 1.4941	TrainAcc 0.3472	ValidAcc 0.3402	TestAcc 0.3390	BestValid 0.4891
	Epoch 1300:	Loss 1.4889	TrainAcc 0.4764	ValidAcc 0.4694	TestAcc 0.4727	BestValid 0.4891
	Epoch 1350:	Loss 1.4974	TrainAcc 0.3015	ValidAcc 0.2969	TestAcc 0.2931	BestValid 0.4891
	Epoch 1400:	Loss 1.4883	TrainAcc 0.4500	ValidAcc 0.4490	TestAcc 0.4505	BestValid 0.4891
	Epoch 1450:	Loss 1.4855	TrainAcc 0.4485	ValidAcc 0.4471	TestAcc 0.4492	BestValid 0.4891
	Epoch 1500:	Loss 1.4808	TrainAcc 0.4579	ValidAcc 0.4542	TestAcc 0.4527	BestValid 0.4891
	Epoch 1550:	Loss 1.4798	TrainAcc 0.4952	ValidAcc 0.4876	TestAcc 0.4907	BestValid 0.4891
	Epoch 1600:	Loss 1.4748	TrainAcc 0.3383	ValidAcc 0.3321	TestAcc 0.3315	BestValid 0.4891
	Epoch 1650:	Loss 1.4701	TrainAcc 0.3688	ValidAcc 0.3592	TestAcc 0.3584	BestValid 0.4891
	Epoch 1700:	Loss 1.4810	TrainAcc 0.4400	ValidAcc 0.4402	TestAcc 0.4407	BestValid 0.4891
	Epoch 1750:	Loss 1.4797	TrainAcc 0.4979	ValidAcc 0.4953	TestAcc 0.4950	BestValid 0.4953
	Epoch 1800:	Loss 1.4719	TrainAcc 0.4591	ValidAcc 0.4504	TestAcc 0.4512	BestValid 0.4953
	Epoch 1850:	Loss 1.4731	TrainAcc 0.3676	ValidAcc 0.3607	TestAcc 0.3595	BestValid 0.4953
	Epoch 1900:	Loss 1.4716	TrainAcc 0.3352	ValidAcc 0.3279	TestAcc 0.3269	BestValid 0.4953
	Epoch 1950:	Loss 1.4641	TrainAcc 0.4014	ValidAcc 0.3930	TestAcc 0.3904	BestValid 0.4953
	Epoch 2000:	Loss 1.4615	TrainAcc 0.5113	ValidAcc 0.5016	TestAcc 0.5028	BestValid 0.5016
	Epoch 2050:	Loss 1.4621	TrainAcc 0.4277	ValidAcc 0.4181	TestAcc 0.4170	BestValid 0.5016
	Epoch 2100:	Loss 1.4626	TrainAcc 0.5086	ValidAcc 0.4991	TestAcc 0.4992	BestValid 0.5016
	Epoch 2150:	Loss 1.4620	TrainAcc 0.3641	ValidAcc 0.3575	TestAcc 0.3549	BestValid 0.5016
	Epoch 2200:	Loss 1.4557	TrainAcc 0.4922	ValidAcc 0.4833	TestAcc 0.4844	BestValid 0.5016
	Epoch 2250:	Loss 1.4556	TrainAcc 0.5095	ValidAcc 0.5065	TestAcc 0.5036	BestValid 0.5065
	Epoch 2300:	Loss 1.4540	TrainAcc 0.4048	ValidAcc 0.3956	TestAcc 0.3921	BestValid 0.5065
	Epoch 2350:	Loss 1.4556	TrainAcc 0.5054	ValidAcc 0.4943	TestAcc 0.4937	BestValid 0.5065
	Epoch 2400:	Loss 1.4532	TrainAcc 0.4142	ValidAcc 0.4045	TestAcc 0.4033	BestValid 0.5065
	Epoch 2450:	Loss 1.4513	TrainAcc 0.5085	ValidAcc 0.4991	TestAcc 0.4993	BestValid 0.5065
	Epoch 2500:	Loss 1.4494	TrainAcc 0.3803	ValidAcc 0.3718	TestAcc 0.3698	BestValid 0.5065
	Epoch 2550:	Loss 1.4455	TrainAcc 0.5139	ValidAcc 0.5007	TestAcc 0.5045	BestValid 0.5065
	Epoch 2600:	Loss 1.4474	TrainAcc 0.4556	ValidAcc 0.4445	TestAcc 0.4444	BestValid 0.5065
	Epoch 2650:	Loss 1.4406	TrainAcc 0.5008	ValidAcc 0.4869	TestAcc 0.4879	BestValid 0.5065
	Epoch 2700:	Loss 1.4394	TrainAcc 0.4937	ValidAcc 0.4803	TestAcc 0.4812	BestValid 0.5065
	Epoch 2750:	Loss 1.4378	TrainAcc 0.4177	ValidAcc 0.4092	TestAcc 0.4066	BestValid 0.5065
	Epoch 2800:	Loss 1.4379	TrainAcc 0.5137	ValidAcc 0.5004	TestAcc 0.5017	BestValid 0.5065
	Epoch 2850:	Loss 1.4380	TrainAcc 0.4544	ValidAcc 0.4412	TestAcc 0.4431	BestValid 0.5065
	Epoch 2900:	Loss 1.4348	TrainAcc 0.4866	ValidAcc 0.4723	TestAcc 0.4734	BestValid 0.5065
	Epoch 2950:	Loss 1.4318	TrainAcc 0.4937	ValidAcc 0.4805	TestAcc 0.4795	BestValid 0.5065
	Epoch 3000:	Loss 1.4416	TrainAcc 0.4162	ValidAcc 0.4059	TestAcc 0.4069	BestValid 0.5065
	Epoch 3050:	Loss 1.4727	TrainAcc 0.2772	ValidAcc 0.2734	TestAcc 0.2703	BestValid 0.5065
	Epoch 3100:	Loss 1.4723	TrainAcc 0.4274	ValidAcc 0.4256	TestAcc 0.4251	BestValid 0.5065
	Epoch 3150:	Loss 1.4464	TrainAcc 0.4956	ValidAcc 0.4849	TestAcc 0.4842	BestValid 0.5065
	Epoch 3200:	Loss 1.4536	TrainAcc 0.4164	ValidAcc 0.4054	TestAcc 0.4035	BestValid 0.5065
	Epoch 3250:	Loss 1.4509	TrainAcc 0.5105	ValidAcc 0.5001	TestAcc 0.5007	BestValid 0.5065
	Epoch 3300:	Loss 1.4493	TrainAcc 0.3345	ValidAcc 0.3276	TestAcc 0.3266	BestValid 0.5065
	Epoch 3350:	Loss 1.4625	TrainAcc 0.2844	ValidAcc 0.2798	TestAcc 0.2760	BestValid 0.5065
	Epoch 3400:	Loss 1.4744	TrainAcc 0.4354	ValidAcc 0.4286	TestAcc 0.4253	BestValid 0.5065
	Epoch 3450:	Loss 1.4547	TrainAcc 0.4372	ValidAcc 0.4269	TestAcc 0.4260	BestValid 0.5065
	Epoch 3500:	Loss 1.4602	TrainAcc 0.4409	ValidAcc 0.4289	TestAcc 0.4281	BestValid 0.5065
	Epoch 3550:	Loss 1.4456	TrainAcc 0.5050	ValidAcc 0.4946	TestAcc 0.4921	BestValid 0.5065
	Epoch 3600:	Loss 1.4463	TrainAcc 0.5150	ValidAcc 0.5085	TestAcc 0.5049	BestValid 0.5085
	Epoch 3650:	Loss 1.4427	TrainAcc 0.4783	ValidAcc 0.4724	TestAcc 0.4721	BestValid 0.5085
	Epoch 3700:	Loss 1.4509	TrainAcc 0.4558	ValidAcc 0.4500	TestAcc 0.4522	BestValid 0.5085
	Epoch 3750:	Loss 1.4782	TrainAcc 0.4597	ValidAcc 0.4485	TestAcc 0.4529	BestValid 0.5085
	Epoch 3800:	Loss 1.4637	TrainAcc 0.4907	ValidAcc 0.4763	TestAcc 0.4769	BestValid 0.5085
	Epoch 3850:	Loss 1.4476	TrainAcc 0.4828	ValidAcc 0.4687	TestAcc 0.4706	BestValid 0.5085
	Epoch 3900:	Loss 1.4374	TrainAcc 0.4314	ValidAcc 0.4199	TestAcc 0.4169	BestValid 0.5085
	Epoch 3950:	Loss 1.4372	TrainAcc 0.4200	ValidAcc 0.4118	TestAcc 0.4085	BestValid 0.5085
	Epoch 4000:	Loss 1.4354	TrainAcc 0.4553	ValidAcc 0.4449	TestAcc 0.4425	BestValid 0.5085
	Epoch 4050:	Loss 1.4382	TrainAcc 0.4857	ValidAcc 0.4723	TestAcc 0.4712	BestValid 0.5085
	Epoch 4100:	Loss 1.4294	TrainAcc 0.4902	ValidAcc 0.4738	TestAcc 0.4736	BestValid 0.5085
	Epoch 4150:	Loss 1.4257	TrainAcc 0.5038	ValidAcc 0.4867	TestAcc 0.4872	BestValid 0.5085
	Epoch 4200:	Loss 1.4277	TrainAcc 0.4970	ValidAcc 0.4776	TestAcc 0.4794	BestValid 0.5085
	Epoch 4250:	Loss 1.4222	TrainAcc 0.5151	ValidAcc 0.4962	TestAcc 0.4963	BestValid 0.5085
	Epoch 4300:	Loss 1.4209	TrainAcc 0.5083	ValidAcc 0.4893	TestAcc 0.4907	BestValid 0.5085
	Epoch 4350:	Loss 1.4232	TrainAcc 0.5112	ValidAcc 0.4926	TestAcc 0.4950	BestValid 0.5085
	Epoch 4400:	Loss 1.4215	TrainAcc 0.5085	ValidAcc 0.4892	TestAcc 0.4901	BestValid 0.5085
	Epoch 4450:	Loss 1.4154	TrainAcc 0.5030	ValidAcc 0.4832	TestAcc 0.4839	BestValid 0.5085
	Epoch 4500:	Loss 1.4145	TrainAcc 0.5135	ValidAcc 0.4931	TestAcc 0.4946	BestValid 0.5085
	Epoch 4550:	Loss 1.4121	TrainAcc 0.5262	ValidAcc 0.5064	TestAcc 0.5047	BestValid 0.5085
	Epoch 4600:	Loss 1.4115	TrainAcc 0.5216	ValidAcc 0.5002	TestAcc 0.5008	BestValid 0.5085
	Epoch 4650:	Loss 1.4125	TrainAcc 0.5164	ValidAcc 0.4947	TestAcc 0.4961	BestValid 0.5085
	Epoch 4700:	Loss 1.4087	TrainAcc 0.5202	ValidAcc 0.4997	TestAcc 0.4977	BestValid 0.5085
	Epoch 4750:	Loss 1.4110	TrainAcc 0.5176	ValidAcc 0.5013	TestAcc 0.5017	BestValid 0.5085
	Epoch 4800:	Loss 1.4113	TrainAcc 0.5107	ValidAcc 0.4961	TestAcc 0.4954	BestValid 0.5085
	Epoch 4850:	Loss 1.4062	TrainAcc 0.5312	ValidAcc 0.5087	TestAcc 0.5089	BestValid 0.5087
	Epoch 4900:	Loss 1.4073	TrainAcc 0.5114	ValidAcc 0.4877	TestAcc 0.4889	BestValid 0.5087
	Epoch 4950:	Loss 1.4058	TrainAcc 0.5332	ValidAcc 0.5127	TestAcc 0.5108	BestValid 0.5127
	Epoch 5000:	Loss 1.4068	TrainAcc 0.4873	ValidAcc 0.4738	TestAcc 0.4753	BestValid 0.5127
****** Epoch Time (Excluding Evaluation Cost): 0.091 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.239 ms (Max: 17.953, Min: 14.843, Sum: 137.914)
Cluster-Wide Average, Compute: 49.511 ms (Max: 58.941, Min: 46.080, Sum: 396.090)
Cluster-Wide Average, Communication-Layer: 14.175 ms (Max: 16.511, Min: 10.740, Sum: 113.400)
Cluster-Wide Average, Bubble-Imbalance: 8.386 ms (Max: 12.091, Min: 2.492, Sum: 67.089)
Cluster-Wide Average, Communication-Graph: 0.443 ms (Max: 0.502, Min: 0.391, Sum: 3.548)
Cluster-Wide Average, Optimization: 0.095 ms (Max: 0.112, Min: 0.088, Sum: 0.764)
Cluster-Wide Average, Others: 0.978 ms (Max: 3.863, Min: 0.559, Sum: 7.824)
****** Breakdown Sum: 90.829 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.004 GB (Max: 4.174, Min: 2.821, Sum: 24.030)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 69.547 Gbps (Max: 83.429, Min: 52.534, Sum: 556.379)
Layer-level communication (cluster-wide, per-epoch): 0.931 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.931 GB
****** Accuracy Results ******
Highest valid_acc: 0.5127
Target test_acc: 0.5108
Epoch to reach the target acc: 4949
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
