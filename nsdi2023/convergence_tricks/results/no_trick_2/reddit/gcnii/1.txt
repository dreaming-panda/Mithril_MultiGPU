Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.831 seconds.
Building the CSC structure...
        It takes 1.938 seconds.
Building the CSC structure...
        It takes 1.944 seconds.
Building the CSC structure...
        It takes 2.031 seconds.
Building the CSC structure...
        It takes 2.421 seconds.
Building the CSC structure...
        It takes 2.449 seconds.
Building the CSC structure...
        It takes 2.563 seconds.
Building the CSC structure...
        It takes 2.652 seconds.
Building the CSC structure...
        It takes 1.850 seconds.
        It takes 1.859 seconds.
        It takes 1.875 seconds.
        It takes 1.884 seconds.
Building the Feature Vector...
        It takes 2.293 seconds.
        It takes 2.337 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.350 seconds.
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 2.299 seconds.
        It takes 0.039 seconds.
        It takes 0.274 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.302 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.043 seconds.
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.276 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.271 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.031 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.899 Gbps (per GPU), 447.191 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.647 Gbps (per GPU), 445.175 Gbps (aggregated)
The layer-level communication performance: 55.638 Gbps (per GPU), 445.105 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.434 Gbps (per GPU), 443.475 Gbps (aggregated)
The layer-level communication performance: 55.394 Gbps (per GPU), 443.151 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.219 Gbps (per GPU), 441.749 Gbps (aggregated)
The layer-level communication performance: 55.188 Gbps (per GPU), 441.503 Gbps (aggregated)
The layer-level communication performance: 55.146 Gbps (per GPU), 441.170 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.711 Gbps (per GPU), 1261.685 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.716 Gbps (per GPU), 1261.729 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.716 Gbps (per GPU), 1261.729 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.687 Gbps (per GPU), 1261.492 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.713 Gbps (per GPU), 1261.706 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.722 Gbps (per GPU), 1261.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.119 Gbps (per GPU), 1272.954 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.707 Gbps (per GPU), 1261.657 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.313 Gbps (per GPU), 818.501 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.310 Gbps (per GPU), 818.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.311 Gbps (per GPU), 818.487 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.310 Gbps (per GPU), 818.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.309 Gbps (per GPU), 818.474 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.308 Gbps (per GPU), 818.467 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.311 Gbps (per GPU), 818.488 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.310 Gbps (per GPU), 818.481 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.420 Gbps (per GPU), 291.361 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.420 Gbps (per GPU), 291.358 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.418 Gbps (per GPU), 291.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.419 Gbps (per GPU), 291.352 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.420 Gbps (per GPU), 291.361 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.419 Gbps (per GPU), 291.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.420 Gbps (per GPU), 291.359 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.420 Gbps (per GPU), 291.356 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.41ms  2.70ms  3.02  8.38K  3.53M
 chk_1  0.76ms  2.77ms  2.90ms  3.83  6.74K  3.60M
 chk_2  0.80ms  2.62ms  2.79ms  3.49  7.27K  3.53M
 chk_3  0.80ms  2.65ms  2.82ms  3.51  7.92K  3.61M
 chk_4  0.63ms  2.58ms  2.73ms  4.34  5.33K  3.68M
 chk_5  1.01ms  2.57ms  2.77ms  2.75 10.07K  3.45M
 chk_6  0.96ms  2.74ms  2.92ms  3.04  9.41K  3.48M
 chk_7  0.82ms  2.57ms  2.72ms  3.32  8.12K  3.60M
 chk_8  0.68ms  2.69ms  2.85ms  4.19  6.09K  3.64M
 chk_9  1.10ms  2.50ms  2.70ms  2.46 11.10K  3.38M
chk_10  0.65ms  2.72ms  2.88ms  4.42  5.67K  3.63M
chk_11  0.82ms  2.58ms  2.76ms  3.36  8.16K  3.54M
chk_12  0.80ms  2.78ms  2.96ms  3.72  7.24K  3.55M
chk_13  0.64ms  2.61ms  2.78ms  4.37  5.41K  3.68M
chk_14  0.78ms  2.86ms  3.04ms  3.89  7.14K  3.53M
chk_15  0.95ms  2.71ms  2.90ms  3.06  9.25K  3.49M
chk_16  0.60ms  2.54ms  2.69ms  4.51  4.78K  3.77M
chk_17  0.76ms  2.67ms  2.84ms  3.72  6.85K  3.60M
chk_18  0.81ms  2.48ms  2.64ms  3.27  7.47K  3.57M
chk_19  0.61ms  2.54ms  2.69ms  4.43  4.88K  3.75M
chk_20  0.77ms  2.56ms  2.71ms  3.52  7.00K  3.63M
chk_21  0.63ms  2.53ms  2.71ms  4.26  5.41K  3.68M
chk_22  1.12ms  2.76ms  2.96ms  2.65 11.07K  3.39M
chk_23  0.80ms  2.64ms  2.81ms  3.53  7.23K  3.64M
chk_24  1.01ms  2.70ms  2.89ms  2.86 10.13K  3.43M
chk_25  0.73ms  2.50ms  2.67ms  3.67  6.40K  3.57M
chk_26  0.66ms  2.72ms  2.87ms  4.35  5.78K  3.55M
chk_27  0.96ms  2.56ms  2.79ms  2.92  9.34K  3.48M
chk_28  0.73ms  2.90ms  3.07ms  4.23  6.37K  3.57M
chk_29  0.64ms  2.69ms  2.85ms  4.47  5.16K  3.78M
chk_30  0.64ms  2.58ms  2.75ms  4.32  5.44K  3.67M
chk_31  0.72ms  2.72ms  2.90ms  4.01  6.33K  3.63M
   Avg  0.79  2.64  2.82
   Max  1.12  2.90  3.07
   Min  0.60  2.41  2.64
 Ratio  1.87  1.20  1.16
   Var  0.02  0.01  0.01
Profiling takes 2.396 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 363.031 ms
Partition 0 [0, 5) has cost: 363.031 ms
Partition 1 [5, 9) has cost: 337.775 ms
Partition 2 [9, 13) has cost: 337.775 ms
Partition 3 [13, 17) has cost: 337.775 ms
Partition 4 [17, 21) has cost: 337.775 ms
Partition 5 [21, 25) has cost: 337.775 ms
Partition 6 [25, 29) has cost: 337.775 ms
Partition 7 [29, 33) has cost: 343.419 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.639 ms
GPU 0, Compute+Comm Time: 132.486 ms, Bubble Time: 28.851 ms, Imbalance Overhead: 3.302 ms
GPU 1, Compute+Comm Time: 125.420 ms, Bubble Time: 28.567 ms, Imbalance Overhead: 10.652 ms
GPU 2, Compute+Comm Time: 125.420 ms, Bubble Time: 28.626 ms, Imbalance Overhead: 10.593 ms
GPU 3, Compute+Comm Time: 125.420 ms, Bubble Time: 28.531 ms, Imbalance Overhead: 10.689 ms
GPU 4, Compute+Comm Time: 125.420 ms, Bubble Time: 28.447 ms, Imbalance Overhead: 10.773 ms
GPU 5, Compute+Comm Time: 125.420 ms, Bubble Time: 28.529 ms, Imbalance Overhead: 10.690 ms
GPU 6, Compute+Comm Time: 125.420 ms, Bubble Time: 28.792 ms, Imbalance Overhead: 10.427 ms
GPU 7, Compute+Comm Time: 126.651 ms, Bubble Time: 29.185 ms, Imbalance Overhead: 8.803 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 317.041 ms
GPU 0, Compute+Comm Time: 243.440 ms, Bubble Time: 56.613 ms, Imbalance Overhead: 16.987 ms
GPU 1, Compute+Comm Time: 239.027 ms, Bubble Time: 55.813 ms, Imbalance Overhead: 22.200 ms
GPU 2, Compute+Comm Time: 239.027 ms, Bubble Time: 55.228 ms, Imbalance Overhead: 22.785 ms
GPU 3, Compute+Comm Time: 239.027 ms, Bubble Time: 55.082 ms, Imbalance Overhead: 22.931 ms
GPU 4, Compute+Comm Time: 239.027 ms, Bubble Time: 55.175 ms, Imbalance Overhead: 22.838 ms
GPU 5, Compute+Comm Time: 239.027 ms, Bubble Time: 55.279 ms, Imbalance Overhead: 22.735 ms
GPU 6, Compute+Comm Time: 239.027 ms, Bubble Time: 55.058 ms, Imbalance Overhead: 22.955 ms
GPU 7, Compute+Comm Time: 257.218 ms, Bubble Time: 55.585 ms, Imbalance Overhead: 4.238 ms
The estimated cost of the whole pipeline: 505.764 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 700.806 ms
Partition 0 [0, 9) has cost: 700.806 ms
Partition 1 [9, 17) has cost: 675.549 ms
Partition 2 [17, 25) has cost: 675.549 ms
Partition 3 [25, 33) has cost: 681.194 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.963 ms
GPU 0, Compute+Comm Time: 141.427 ms, Bubble Time: 26.732 ms, Imbalance Overhead: 2.803 ms
GPU 1, Compute+Comm Time: 137.601 ms, Bubble Time: 26.292 ms, Imbalance Overhead: 7.069 ms
GPU 2, Compute+Comm Time: 137.601 ms, Bubble Time: 26.251 ms, Imbalance Overhead: 7.110 ms
GPU 3, Compute+Comm Time: 138.202 ms, Bubble Time: 26.019 ms, Imbalance Overhead: 6.742 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 316.318 ms
GPU 0, Compute+Comm Time: 255.235 ms, Bubble Time: 48.417 ms, Imbalance Overhead: 12.667 ms
GPU 1, Compute+Comm Time: 253.020 ms, Bubble Time: 48.626 ms, Imbalance Overhead: 14.673 ms
GPU 2, Compute+Comm Time: 253.020 ms, Bubble Time: 48.685 ms, Imbalance Overhead: 14.613 ms
GPU 3, Compute+Comm Time: 263.075 ms, Bubble Time: 49.663 ms, Imbalance Overhead: 3.580 ms
    The estimated cost with 2 DP ways is 511.645 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1376.355 ms
Partition 0 [0, 17) has cost: 1376.355 ms
Partition 1 [17, 33) has cost: 1356.743 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 217.543 ms
GPU 0, Compute+Comm Time: 189.094 ms, Bubble Time: 23.199 ms, Imbalance Overhead: 5.251 ms
GPU 1, Compute+Comm Time: 187.279 ms, Bubble Time: 23.836 ms, Imbalance Overhead: 6.429 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 354.600 ms
GPU 0, Compute+Comm Time: 306.498 ms, Bubble Time: 39.024 ms, Imbalance Overhead: 9.078 ms
GPU 1, Compute+Comm Time: 311.080 ms, Bubble Time: 37.951 ms, Imbalance Overhead: 5.569 ms
    The estimated cost with 4 DP ways is 600.750 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2733.098 ms
Partition 0 [0, 33) has cost: 2733.098 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 530.565 ms
GPU 0, Compute+Comm Time: 530.565 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 653.915 ms
GPU 0, Compute+Comm Time: 653.915 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1243.704 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 7, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7714	TrainAcc 0.0846	ValidAcc 0.0761	TestAcc 0.0770	BestValid 0.0761
	Epoch 50:	Loss 1.9446	TrainAcc 0.6245	ValidAcc 0.6472	TestAcc 0.6417	BestValid 0.6472
	Epoch 100:	Loss 1.3241	TrainAcc 0.7785	ValidAcc 0.7928	TestAcc 0.7870	BestValid 0.7928
	Epoch 150:	Loss 1.0488	TrainAcc 0.8373	ValidAcc 0.8471	TestAcc 0.8409	BestValid 0.8471
	Epoch 200:	Loss 0.9073	TrainAcc 0.8413	ValidAcc 0.8478	TestAcc 0.8421	BestValid 0.8478
	Epoch 250:	Loss 0.8183	TrainAcc 0.8121	ValidAcc 0.8200	TestAcc 0.8133	BestValid 0.8478
	Epoch 300:	Loss 0.7505	TrainAcc 0.8868	ValidAcc 0.8957	TestAcc 0.8885	BestValid 0.8957
	Epoch 350:	Loss 0.7006	TrainAcc 0.8651	ValidAcc 0.8769	TestAcc 0.8697	BestValid 0.8957
	Epoch 400:	Loss 0.6630	TrainAcc 0.3941	ValidAcc 0.3947	TestAcc 0.3897	BestValid 0.8957
	Epoch 450:	Loss 0.6438	TrainAcc 0.2546	ValidAcc 0.2517	TestAcc 0.2457	BestValid 0.8957
	Epoch 500:	Loss 0.6181	TrainAcc 0.8698	ValidAcc 0.8765	TestAcc 0.8696	BestValid 0.8957
	Epoch 550:	Loss 0.6001	TrainAcc 0.3881	ValidAcc 0.4002	TestAcc 0.3966	BestValid 0.8957
	Epoch 600:	Loss 0.6149	TrainAcc 0.2090	ValidAcc 0.2373	TestAcc 0.2389	BestValid 0.8957
	Epoch 650:	Loss 0.6153	TrainAcc 0.2446	ValidAcc 0.2700	TestAcc 0.2700	BestValid 0.8957
	Epoch 700:	Loss 0.6128	TrainAcc 0.1688	ValidAcc 0.1505	TestAcc 0.1484	BestValid 0.8957
	Epoch 750:	Loss 0.6659	TrainAcc 0.0261	ValidAcc 0.0222	TestAcc 0.0219	BestValid 0.8957
	Epoch 800:	Loss 0.7310	TrainAcc 0.0625	ValidAcc 0.0601	TestAcc 0.0592	BestValid 0.8957
	Epoch 850:	Loss 0.6674	TrainAcc 0.1544	ValidAcc 0.1461	TestAcc 0.1438	BestValid 0.8957
	Epoch 900:	Loss 1.0085	TrainAcc 0.2422	ValidAcc 0.2427	TestAcc 0.2426	BestValid 0.8957
	Epoch 950:	Loss 1.1566	TrainAcc 0.2360	ValidAcc 0.2326	TestAcc 0.2254	BestValid 0.8957
	Epoch 1000:	Loss 1.0593	TrainAcc 0.3191	ValidAcc 0.3040	TestAcc 0.2949	BestValid 0.8957
	Epoch 1050:	Loss 0.9008	TrainAcc 0.4993	ValidAcc 0.5138	TestAcc 0.5109	BestValid 0.8957
	Epoch 1100:	Loss 0.8051	TrainAcc 0.5898	ValidAcc 0.6274	TestAcc 0.6240	BestValid 0.8957
	Epoch 1150:	Loss 0.7300	TrainAcc 0.7480	ValidAcc 0.7703	TestAcc 0.7654	BestValid 0.8957
	Epoch 1200:	Loss 0.7105	TrainAcc 0.6526	ValidAcc 0.6555	TestAcc 0.6495	BestValid 0.8957
	Epoch 1250:	Loss 0.6989	TrainAcc 0.5017	ValidAcc 0.5098	TestAcc 0.5085	BestValid 0.8957
	Epoch 1300:	Loss 0.6683	TrainAcc 0.6254	ValidAcc 0.6283	TestAcc 0.6254	BestValid 0.8957
	Epoch 1350:	Loss 0.6419	TrainAcc 0.8637	ValidAcc 0.8696	TestAcc 0.8641	BestValid 0.8957
	Epoch 1400:	Loss 0.6267	TrainAcc 0.7070	ValidAcc 0.7289	TestAcc 0.7225	BestValid 0.8957
	Epoch 1450:	Loss 0.6388	TrainAcc 0.6049	ValidAcc 0.6114	TestAcc 0.6059	BestValid 0.8957
	Epoch 1500:	Loss 0.6143	TrainAcc 0.7303	ValidAcc 0.7293	TestAcc 0.7260	BestValid 0.8957
	Epoch 1550:	Loss 0.5992	TrainAcc 0.8260	ValidAcc 0.8238	TestAcc 0.8183	BestValid 0.8957
	Epoch 1600:	Loss 0.5922	TrainAcc 0.8474	ValidAcc 0.8533	TestAcc 0.8473	BestValid 0.8957
	Epoch 1650:	Loss 0.5839	TrainAcc 0.8699	ValidAcc 0.8793	TestAcc 0.8729	BestValid 0.8957
	Epoch 1700:	Loss 0.5728	TrainAcc 0.8885	ValidAcc 0.8962	TestAcc 0.8907	BestValid 0.8962
	Epoch 1750:	Loss 0.5656	TrainAcc 0.9148	ValidAcc 0.9173	TestAcc 0.9136	BestValid 0.9173
	Epoch 1800:	Loss 0.5588	TrainAcc 0.9175	ValidAcc 0.9201	TestAcc 0.9168	BestValid 0.9201
	Epoch 1850:	Loss 0.5468	TrainAcc 0.9170	ValidAcc 0.9196	TestAcc 0.9159	BestValid 0.9201
	Epoch 1900:	Loss 0.5504	TrainAcc 0.9206	ValidAcc 0.9232	TestAcc 0.9199	BestValid 0.9232
	Epoch 1950:	Loss 0.5349	TrainAcc 0.9243	ValidAcc 0.9277	TestAcc 0.9236	BestValid 0.9277
	Epoch 2000:	Loss 0.5332	TrainAcc 0.9251	ValidAcc 0.9284	TestAcc 0.9243	BestValid 0.9284
	Epoch 2050:	Loss 0.5280	TrainAcc 0.9241	ValidAcc 0.9272	TestAcc 0.9232	BestValid 0.9284
	Epoch 2100:	Loss 0.5287	TrainAcc 0.9257	ValidAcc 0.9282	TestAcc 0.9247	BestValid 0.9284
	Epoch 2150:	Loss 0.5164	TrainAcc 0.9274	ValidAcc 0.9297	TestAcc 0.9263	BestValid 0.9297
	Epoch 2200:	Loss 0.5202	TrainAcc 0.9280	ValidAcc 0.9302	TestAcc 0.9269	BestValid 0.9302
	Epoch 2250:	Loss 0.5120	TrainAcc 0.9266	ValidAcc 0.9292	TestAcc 0.9253	BestValid 0.9302
	Epoch 2300:	Loss 0.5050	TrainAcc 0.9288	ValidAcc 0.9316	TestAcc 0.9274	BestValid 0.9316
	Epoch 2350:	Loss 0.5021	TrainAcc 0.9307	ValidAcc 0.9334	TestAcc 0.9299	BestValid 0.9334
	Epoch 2400:	Loss 0.4983	TrainAcc 0.9303	ValidAcc 0.9331	TestAcc 0.9287	BestValid 0.9334
	Epoch 2450:	Loss 0.4949	TrainAcc 0.9313	ValidAcc 0.9342	TestAcc 0.9301	BestValid 0.9342
	Epoch 2500:	Loss 0.4883	TrainAcc 0.9319	ValidAcc 0.9342	TestAcc 0.9309	BestValid 0.9342
	Epoch 2550:	Loss 0.4866	TrainAcc 0.9188	ValidAcc 0.9205	TestAcc 0.9180	BestValid 0.9342
	Epoch 2600:	Loss 0.4796	TrainAcc 0.9165	ValidAcc 0.9173	TestAcc 0.9151	BestValid 0.9342
	Epoch 2650:	Loss 0.4810	TrainAcc 0.9169	ValidAcc 0.9188	TestAcc 0.9149	BestValid 0.9342
	Epoch 2700:	Loss 0.4724	TrainAcc 0.6435	ValidAcc 0.6463	TestAcc 0.6418	BestValid 0.9342
	Epoch 2750:	Loss 0.4797	TrainAcc 0.7138	ValidAcc 0.7281	TestAcc 0.7227	BestValid 0.9342
	Epoch 2800:	Loss 0.4706	TrainAcc 0.9035	ValidAcc 0.9056	TestAcc 0.9017	BestValid 0.9342
	Epoch 2850:	Loss 0.4715	TrainAcc 0.7159	ValidAcc 0.7143	TestAcc 0.7101	BestValid 0.9342
	Epoch 2900:	Loss 0.4631	TrainAcc 0.6660	ValidAcc 0.6682	TestAcc 0.6602	BestValid 0.9342
	Epoch 2950:	Loss 0.4654	TrainAcc 0.8533	ValidAcc 0.8460	TestAcc 0.8420	BestValid 0.9342
	Epoch 3000:	Loss 0.4631	TrainAcc 0.7259	ValidAcc 0.6996	TestAcc 0.6959	BestValid 0.9342
	Epoch 3050:	Loss 0.4716	TrainAcc 0.5011	ValidAcc 0.4949	TestAcc 0.4917	BestValid 0.9342
	Epoch 3100:	Loss 0.4621	TrainAcc 0.8381	ValidAcc 0.8421	TestAcc 0.8368	BestValid 0.9342
	Epoch 3150:	Loss 0.4621	TrainAcc 0.8115	ValidAcc 0.8033	TestAcc 0.7962	BestValid 0.9342
	Epoch 3200:	Loss 0.4590	TrainAcc 0.6158	ValidAcc 0.6145	TestAcc 0.6103	BestValid 0.9342
	Epoch 3250:	Loss 0.4542	TrainAcc 0.8815	ValidAcc 0.8859	TestAcc 0.8809	BestValid 0.9342
	Epoch 3300:	Loss 0.4459	TrainAcc 0.7675	ValidAcc 0.7769	TestAcc 0.7737	BestValid 0.9342
	Epoch 3350:	Loss 0.4480	TrainAcc 0.7800	ValidAcc 0.7725	TestAcc 0.7698	BestValid 0.9342
	Epoch 3400:	Loss 0.4467	TrainAcc 0.5999	ValidAcc 0.6036	TestAcc 0.5953	BestValid 0.9342
	Epoch 3450:	Loss 0.4450	TrainAcc 0.5905	ValidAcc 0.5909	TestAcc 0.5849	BestValid 0.9342
	Epoch 3500:	Loss 0.4551	TrainAcc 0.9131	ValidAcc 0.9117	TestAcc 0.9093	BestValid 0.9342
	Epoch 3550:	Loss 0.4498	TrainAcc 0.6645	ValidAcc 0.6368	TestAcc 0.6284	BestValid 0.9342
	Epoch 3600:	Loss 0.4504	TrainAcc 0.8927	ValidAcc 0.8902	TestAcc 0.8873	BestValid 0.9342
	Epoch 3650:	Loss 0.4487	TrainAcc 0.9147	ValidAcc 0.9146	TestAcc 0.9100	BestValid 0.9342
	Epoch 3700:	Loss 0.4407	TrainAcc 0.9298	ValidAcc 0.9285	TestAcc 0.9256	BestValid 0.9342
	Epoch 3750:	Loss 0.4364	TrainAcc 0.9284	ValidAcc 0.9301	TestAcc 0.9272	BestValid 0.9342
	Epoch 3800:	Loss 0.4334	TrainAcc 0.9303	ValidAcc 0.9331	TestAcc 0.9296	BestValid 0.9342
	Epoch 3850:	Loss 0.4298	TrainAcc 0.9126	ValidAcc 0.9129	TestAcc 0.9102	BestValid 0.9342
	Epoch 3900:	Loss 0.4259	TrainAcc 0.8974	ValidAcc 0.8998	TestAcc 0.8956	BestValid 0.9342
	Epoch 3950:	Loss 0.4191	TrainAcc 0.8903	ValidAcc 0.8929	TestAcc 0.8888	BestValid 0.9342
	Epoch 4000:	Loss 0.4182	TrainAcc 0.9405	ValidAcc 0.9406	TestAcc 0.9382	BestValid 0.9406
	Epoch 4050:	Loss 0.4104	TrainAcc 0.8722	ValidAcc 0.8701	TestAcc 0.8671	BestValid 0.9406
	Epoch 4100:	Loss 0.4161	TrainAcc 0.7546	ValidAcc 0.7494	TestAcc 0.7441	BestValid 0.9406
	Epoch 4150:	Loss 0.4141	TrainAcc 0.8540	ValidAcc 0.8462	TestAcc 0.8423	BestValid 0.9406
	Epoch 4200:	Loss 0.4082	TrainAcc 0.9406	ValidAcc 0.9388	TestAcc 0.9366	BestValid 0.9406
	Epoch 4250:	Loss 0.4103	TrainAcc 0.8625	ValidAcc 0.8628	TestAcc 0.8578	BestValid 0.9406
	Epoch 4300:	Loss 0.4081	TrainAcc 0.8811	ValidAcc 0.8843	TestAcc 0.8774	BestValid 0.9406
	Epoch 4350:	Loss 0.4069	TrainAcc 0.5912	ValidAcc 0.5916	TestAcc 0.5876	BestValid 0.9406
	Epoch 4400:	Loss 0.4041	TrainAcc 0.6780	ValidAcc 0.6875	TestAcc 0.6818	BestValid 0.9406
	Epoch 4450:	Loss 0.4009	TrainAcc 0.9139	ValidAcc 0.9144	TestAcc 0.9137	BestValid 0.9406
	Epoch 4500:	Loss 0.3985	TrainAcc 0.7776	ValidAcc 0.7676	TestAcc 0.7658	BestValid 0.9406
	Epoch 4550:	Loss 0.3991	TrainAcc 0.7669	ValidAcc 0.7553	TestAcc 0.7544	BestValid 0.9406
	Epoch 4600:	Loss 0.3997	TrainAcc 0.8700	ValidAcc 0.8682	TestAcc 0.8642	BestValid 0.9406
	Epoch 4650:	Loss 0.3967	TrainAcc 0.9427	ValidAcc 0.9405	TestAcc 0.9386	BestValid 0.9406
	Epoch 4700:	Loss 0.3981	TrainAcc 0.8307	ValidAcc 0.8318	TestAcc 0.8260	BestValid 0.9406
	Epoch 4750:	Loss 0.3949	TrainAcc 0.9403	ValidAcc 0.9405	TestAcc 0.9374	BestValid 0.9406
	Epoch 4800:	Loss 0.3955	TrainAcc 0.9286	ValidAcc 0.9290	TestAcc 0.9243	BestValid 0.9406
	Epoch 4850:	Loss 0.3866	TrainAcc 0.5477	ValidAcc 0.5351	TestAcc 0.5318	BestValid 0.9406
	Epoch 4900:	Loss 0.3853	TrainAcc 0.9456	ValidAcc 0.9457	TestAcc 0.9431	BestValid 0.9457
	Epoch 4950:	Loss 0.3824	TrainAcc 0.9289	ValidAcc 0.9294	TestAcc 0.9267	BestValid 0.9457
	Epoch 5000:	Loss 0.3827	TrainAcc 0.9177	ValidAcc 0.9170	TestAcc 0.9154	BestValid 0.9457
****** Epoch Time (Excluding Evaluation Cost): 0.413 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 79.799 ms (Max: 82.127, Min: 66.405, Sum: 638.393)
Cluster-Wide Average, Compute: 262.623 ms (Max: 284.189, Min: 252.198, Sum: 2100.981)
Cluster-Wide Average, Communication-Layer: 41.587 ms (Max: 47.005, Min: 32.127, Sum: 332.700)
Cluster-Wide Average, Bubble-Imbalance: 26.477 ms (Max: 33.644, Min: 13.912, Sum: 211.816)
Cluster-Wide Average, Communication-Graph: 0.473 ms (Max: 0.521, Min: 0.414, Sum: 3.780)
Cluster-Wide Average, Optimization: 0.104 ms (Max: 0.127, Min: 0.095, Sum: 0.831)
Cluster-Wide Average, Others: 3.210 ms (Max: 16.899, Min: 1.243, Sum: 25.680)
****** Breakdown Sum: 414.272 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.304 GB (Max: 8.059, Min: 6.018, Sum: 50.433)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 61.743 Gbps (Max: 73.436, Min: 45.722, Sum: 493.944)
Layer-level communication (cluster-wide, per-epoch): 2.430 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 2.430 GB
****** Accuracy Results ******
Highest valid_acc: 0.9457
Target test_acc: 0.9431
Epoch to reach the target acc: 4899
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
