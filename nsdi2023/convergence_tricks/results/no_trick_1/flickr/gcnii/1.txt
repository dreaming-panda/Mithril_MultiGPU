Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.030 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.019 seconds.
Building the Feature Vector...
        It takes 0.016 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.095 seconds.
Building the Label Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.102 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.079 Gbps (per GPU), 480.631 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.788 Gbps (per GPU), 478.304 Gbps (aggregated)
The layer-level communication performance: 59.782 Gbps (per GPU), 478.254 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.531 Gbps (per GPU), 476.246 Gbps (aggregated)
The layer-level communication performance: 59.493 Gbps (per GPU), 475.946 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.283 Gbps (per GPU), 474.261 Gbps (aggregated)
The layer-level communication performance: 59.237 Gbps (per GPU), 473.895 Gbps (aggregated)
The layer-level communication performance: 59.199 Gbps (per GPU), 473.596 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 151.354 Gbps (per GPU), 1210.829 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.318 Gbps (per GPU), 1210.543 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.345 Gbps (per GPU), 1210.761 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.321 Gbps (per GPU), 1210.565 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.342 Gbps (per GPU), 1210.739 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.321 Gbps (per GPU), 1210.565 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.356 Gbps (per GPU), 1210.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.324 Gbps (per GPU), 1210.590 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.546 Gbps (per GPU), 812.372 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.516 Gbps (per GPU), 812.128 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.550 Gbps (per GPU), 812.397 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.538 Gbps (per GPU), 812.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.550 Gbps (per GPU), 812.397 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.536 Gbps (per GPU), 812.286 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.548 Gbps (per GPU), 812.383 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.535 Gbps (per GPU), 812.279 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.475 Gbps (per GPU), 251.800 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.476 Gbps (per GPU), 251.805 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.477 Gbps (per GPU), 251.813 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.476 Gbps (per GPU), 251.804 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.474 Gbps (per GPU), 251.793 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.474 Gbps (per GPU), 251.796 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.474 Gbps (per GPU), 251.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.473 Gbps (per GPU), 251.785 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.42ms  0.36ms  0.68ms  1.87  2.81K  0.03M
 chk_1  0.41ms  0.38ms  0.53ms  1.39  2.82K  0.03M
 chk_2  0.41ms  0.39ms  0.53ms  1.35  2.80K  0.03M
 chk_3  0.41ms  0.47ms  0.53ms  1.28  2.80K  0.03M
 chk_4  0.41ms  0.39ms  0.53ms  1.36  2.82K  0.03M
 chk_5  0.41ms  0.46ms  0.53ms  1.31  2.75K  0.03M
 chk_6  0.41ms  0.39ms  0.53ms  1.35  2.71K  0.03M
 chk_7  0.41ms  0.39ms  0.53ms  1.35  2.76K  0.03M
 chk_8  0.41ms  0.58ms  0.53ms  1.40  2.79K  0.03M
 chk_9  0.41ms  0.39ms  0.53ms  1.36  2.81K  0.03M
chk_10  0.41ms  0.37ms  0.53ms  1.42  2.81K  0.03M
chk_11  0.41ms  0.40ms  0.53ms  1.35  2.74K  0.03M
chk_12  0.41ms  0.39ms  0.53ms  1.36  2.76K  0.03M
chk_13  0.41ms  0.40ms  0.53ms  1.34  2.75K  0.03M
chk_14  0.41ms  0.39ms  0.53ms  1.36  2.81K  0.03M
chk_15  0.41ms  0.39ms  0.53ms  1.36  2.77K  0.03M
chk_16  0.41ms  0.39ms  0.53ms  1.35  2.78K  0.03M
chk_17  0.41ms  0.40ms  0.54ms  1.35  2.79K  0.03M
chk_18  0.41ms  0.39ms  0.53ms  1.36  2.82K  0.03M
chk_19  0.41ms  0.39ms  0.52ms  1.36  2.81K  0.03M
chk_20  0.41ms  0.39ms  0.53ms  1.36  2.77K  0.03M
chk_21  0.41ms  0.39ms  0.53ms  1.36  2.84K  0.02M
chk_22  0.41ms  0.39ms  0.53ms  1.35  2.78K  0.03M
chk_23  0.42ms  0.39ms  0.53ms  1.36  2.80K  0.03M
chk_24  0.42ms  0.39ms  0.53ms  1.37  2.80K  0.03M
chk_25  0.42ms  0.39ms  0.53ms  1.35  2.81K  0.03M
chk_26  0.49ms  0.39ms  0.53ms  1.36  2.81K  0.03M
chk_27  0.43ms  0.40ms  0.53ms  1.35  2.79K  0.03M
chk_28  0.41ms  0.40ms  0.53ms  1.34  2.77K  0.03M
chk_29  0.41ms  0.39ms  0.53ms  1.34  2.77K  0.03M
chk_30  0.42ms  0.39ms  0.53ms  1.35  2.80K  0.03M
chk_31  0.42ms  0.40ms  0.53ms  1.35  2.78K  0.03M
   Avg  0.42  0.40  0.53
   Max  0.49  0.58  0.68
   Min  0.41  0.36  0.52
 Ratio  1.21  1.59  1.29
   Var  0.00  0.00  0.00
Profiling takes 0.653 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 64.002 ms
Partition 0 [0, 4) has cost: 51.683 ms
Partition 1 [4, 8) has cost: 51.202 ms
Partition 2 [8, 12) has cost: 51.202 ms
Partition 3 [12, 16) has cost: 51.202 ms
Partition 4 [16, 20) has cost: 51.202 ms
Partition 5 [20, 24) has cost: 51.202 ms
Partition 6 [24, 29) has cost: 64.002 ms
Partition 7 [29, 33) has cost: 55.519 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 36.126 ms
GPU 0, Compute+Comm Time: 22.575 ms, Bubble Time: 5.696 ms, Imbalance Overhead: 7.856 ms
GPU 1, Compute+Comm Time: 22.893 ms, Bubble Time: 5.530 ms, Imbalance Overhead: 7.704 ms
GPU 2, Compute+Comm Time: 22.893 ms, Bubble Time: 5.355 ms, Imbalance Overhead: 7.879 ms
GPU 3, Compute+Comm Time: 22.893 ms, Bubble Time: 5.198 ms, Imbalance Overhead: 8.036 ms
GPU 4, Compute+Comm Time: 22.893 ms, Bubble Time: 5.133 ms, Imbalance Overhead: 8.101 ms
GPU 5, Compute+Comm Time: 22.893 ms, Bubble Time: 5.092 ms, Imbalance Overhead: 8.142 ms
GPU 6, Compute+Comm Time: 27.428 ms, Bubble Time: 5.045 ms, Imbalance Overhead: 3.653 ms
GPU 7, Compute+Comm Time: 23.450 ms, Bubble Time: 5.124 ms, Imbalance Overhead: 7.553 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 55.074 ms
GPU 0, Compute+Comm Time: 41.577 ms, Bubble Time: 9.017 ms, Imbalance Overhead: 4.480 ms
GPU 1, Compute+Comm Time: 46.082 ms, Bubble Time: 8.944 ms, Imbalance Overhead: 0.048 ms
GPU 2, Compute+Comm Time: 37.816 ms, Bubble Time: 9.015 ms, Imbalance Overhead: 8.243 ms
GPU 3, Compute+Comm Time: 37.816 ms, Bubble Time: 9.085 ms, Imbalance Overhead: 8.173 ms
GPU 4, Compute+Comm Time: 37.816 ms, Bubble Time: 9.169 ms, Imbalance Overhead: 8.089 ms
GPU 5, Compute+Comm Time: 37.816 ms, Bubble Time: 9.409 ms, Imbalance Overhead: 7.849 ms
GPU 6, Compute+Comm Time: 37.816 ms, Bubble Time: 9.651 ms, Imbalance Overhead: 7.607 ms
GPU 7, Compute+Comm Time: 38.616 ms, Bubble Time: 9.929 ms, Imbalance Overhead: 6.529 ms
The estimated cost of the whole pipeline: 95.761 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 115.204 ms
Partition 0 [0, 8) has cost: 102.885 ms
Partition 1 [8, 16) has cost: 102.403 ms
Partition 2 [16, 25) has cost: 115.204 ms
Partition 3 [25, 33) has cost: 106.721 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 39.146 ms
GPU 0, Compute+Comm Time: 27.085 ms, Bubble Time: 5.232 ms, Imbalance Overhead: 6.829 ms
GPU 1, Compute+Comm Time: 27.831 ms, Bubble Time: 5.018 ms, Imbalance Overhead: 6.298 ms
GPU 2, Compute+Comm Time: 30.715 ms, Bubble Time: 4.831 ms, Imbalance Overhead: 3.600 ms
GPU 3, Compute+Comm Time: 27.998 ms, Bubble Time: 4.953 ms, Imbalance Overhead: 6.195 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 55.235 ms
GPU 0, Compute+Comm Time: 44.078 ms, Bubble Time: 8.048 ms, Imbalance Overhead: 3.110 ms
GPU 1, Compute+Comm Time: 46.885 ms, Bubble Time: 7.906 ms, Imbalance Overhead: 0.444 ms
GPU 2, Compute+Comm Time: 42.204 ms, Bubble Time: 8.191 ms, Imbalance Overhead: 4.841 ms
GPU 3, Compute+Comm Time: 42.108 ms, Bubble Time: 8.510 ms, Imbalance Overhead: 4.617 ms
    The estimated cost with 2 DP ways is 99.100 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 218.088 ms
Partition 0 [0, 17) has cost: 218.088 ms
Partition 1 [17, 33) has cost: 209.124 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 52.180 ms
GPU 0, Compute+Comm Time: 44.115 ms, Bubble Time: 4.925 ms, Imbalance Overhead: 3.141 ms
GPU 1, Compute+Comm Time: 43.010 ms, Bubble Time: 7.959 ms, Imbalance Overhead: 1.210 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 66.523 ms
GPU 0, Compute+Comm Time: 57.199 ms, Bubble Time: 7.111 ms, Imbalance Overhead: 2.212 ms
GPU 1, Compute+Comm Time: 58.577 ms, Bubble Time: 6.973 ms, Imbalance Overhead: 0.972 ms
    The estimated cost with 4 DP ways is 124.638 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 427.212 ms
Partition 0 [0, 33) has cost: 427.212 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 153.944 ms
GPU 0, Compute+Comm Time: 153.944 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 166.092 ms
GPU 0, Compute+Comm Time: 166.092 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 336.038 ms

*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ************ Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******

*** Node 2, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 7, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9415	TrainAcc 0.4141	ValidAcc 0.4175	TestAcc 0.4163	BestValid 0.4175
	Epoch 50:	Loss 1.6090	TrainAcc 0.4685	ValidAcc 0.4697	TestAcc 0.4674	BestValid 0.4697
	Epoch 100:	Loss 1.5775	TrainAcc 0.4509	ValidAcc 0.4521	TestAcc 0.4522	BestValid 0.4697
	Epoch 150:	Loss 1.5498	TrainAcc 0.4570	ValidAcc 0.4572	TestAcc 0.4570	BestValid 0.4697
	Epoch 200:	Loss 1.5284	TrainAcc 0.4865	ValidAcc 0.4843	TestAcc 0.4841	BestValid 0.4843
	Epoch 250:	Loss 1.5104	TrainAcc 0.4733	ValidAcc 0.4721	TestAcc 0.4712	BestValid 0.4843
	Epoch 300:	Loss 1.4933	TrainAcc 0.4953	ValidAcc 0.4911	TestAcc 0.4913	BestValid 0.4911
	Epoch 350:	Loss 1.4826	TrainAcc 0.4724	ValidAcc 0.4651	TestAcc 0.4640	BestValid 0.4911
	Epoch 400:	Loss 1.4741	TrainAcc 0.5014	ValidAcc 0.4935	TestAcc 0.4944	BestValid 0.4935
	Epoch 450:	Loss 1.4718	TrainAcc 0.4897	ValidAcc 0.4825	TestAcc 0.4823	BestValid 0.4935
	Epoch 500:	Loss 1.4651	TrainAcc 0.5026	ValidAcc 0.4915	TestAcc 0.4937	BestValid 0.4935
	Epoch 550:	Loss 1.4627	TrainAcc 0.5093	ValidAcc 0.5014	TestAcc 0.5031	BestValid 0.5014
	Epoch 600:	Loss 1.4589	TrainAcc 0.5037	ValidAcc 0.4941	TestAcc 0.4957	BestValid 0.5014
	Epoch 650:	Loss 1.4536	TrainAcc 0.5017	ValidAcc 0.4901	TestAcc 0.4917	BestValid 0.5014
	Epoch 700:	Loss 1.4553	TrainAcc 0.5075	ValidAcc 0.4966	TestAcc 0.4980	BestValid 0.5014
	Epoch 750:	Loss 1.4531	TrainAcc 0.5136	ValidAcc 0.5024	TestAcc 0.5028	BestValid 0.5024
	Epoch 800:	Loss 1.4479	TrainAcc 0.5141	ValidAcc 0.5031	TestAcc 0.5047	BestValid 0.5031
	Epoch 850:	Loss 1.4429	TrainAcc 0.5070	ValidAcc 0.4948	TestAcc 0.4959	BestValid 0.5031
	Epoch 900:	Loss 1.4442	TrainAcc 0.5196	ValidAcc 0.5094	TestAcc 0.5106	BestValid 0.5094
	Epoch 950:	Loss 1.4426	TrainAcc 0.5173	ValidAcc 0.5036	TestAcc 0.5050	BestValid 0.5094
	Epoch 1000:	Loss 1.4408	TrainAcc 0.5200	ValidAcc 0.5072	TestAcc 0.5067	BestValid 0.5094
	Epoch 1050:	Loss 1.4384	TrainAcc 0.5206	ValidAcc 0.5065	TestAcc 0.5076	BestValid 0.5094
	Epoch 1100:	Loss 1.4311	TrainAcc 0.5212	ValidAcc 0.5080	TestAcc 0.5108	BestValid 0.5094
	Epoch 1150:	Loss 1.4390	TrainAcc 0.5227	ValidAcc 0.5098	TestAcc 0.5097	BestValid 0.5098
	Epoch 1200:	Loss 1.4297	TrainAcc 0.5246	ValidAcc 0.5108	TestAcc 0.5130	BestValid 0.5108
	Epoch 1250:	Loss 1.4289	TrainAcc 0.5240	ValidAcc 0.5103	TestAcc 0.5121	BestValid 0.5108
	Epoch 1300:	Loss 1.4292	TrainAcc 0.5187	ValidAcc 0.5038	TestAcc 0.5039	BestValid 0.5108
	Epoch 1350:	Loss 1.4253	TrainAcc 0.5256	ValidAcc 0.5105	TestAcc 0.5109	BestValid 0.5108
	Epoch 1400:	Loss 1.4253	TrainAcc 0.5268	ValidAcc 0.5106	TestAcc 0.5131	BestValid 0.5108
	Epoch 1450:	Loss 1.4216	TrainAcc 0.5289	ValidAcc 0.5125	TestAcc 0.5145	BestValid 0.5125
	Epoch 1500:	Loss 1.4222	TrainAcc 0.5290	ValidAcc 0.5126	TestAcc 0.5143	BestValid 0.5126
	Epoch 1550:	Loss 1.4204	TrainAcc 0.5267	ValidAcc 0.5102	TestAcc 0.5126	BestValid 0.5126
	Epoch 1600:	Loss 1.4221	TrainAcc 0.5274	ValidAcc 0.5142	TestAcc 0.5136	BestValid 0.5142
	Epoch 1650:	Loss 1.4177	TrainAcc 0.5308	ValidAcc 0.5140	TestAcc 0.5136	BestValid 0.5142
	Epoch 1700:	Loss 1.4143	TrainAcc 0.5316	ValidAcc 0.5143	TestAcc 0.5135	BestValid 0.5143
	Epoch 1750:	Loss 1.4150	TrainAcc 0.5324	ValidAcc 0.5131	TestAcc 0.5145	BestValid 0.5143
	Epoch 1800:	Loss 1.4107	TrainAcc 0.5315	ValidAcc 0.5130	TestAcc 0.5158	BestValid 0.5143
	Epoch 1850:	Loss 1.4144	TrainAcc 0.5339	ValidAcc 0.5153	TestAcc 0.5155	BestValid 0.5153
	Epoch 1900:	Loss 1.4149	TrainAcc 0.5333	ValidAcc 0.5155	TestAcc 0.5153	BestValid 0.5155
	Epoch 1950:	Loss 1.4089	TrainAcc 0.5339	ValidAcc 0.5128	TestAcc 0.5154	BestValid 0.5155
	Epoch 2000:	Loss 1.4071	TrainAcc 0.5338	ValidAcc 0.5156	TestAcc 0.5145	BestValid 0.5156
	Epoch 2050:	Loss 1.4077	TrainAcc 0.5365	ValidAcc 0.5153	TestAcc 0.5156	BestValid 0.5156
	Epoch 2100:	Loss 1.4060	TrainAcc 0.5373	ValidAcc 0.5158	TestAcc 0.5176	BestValid 0.5158
	Epoch 2150:	Loss 1.4019	TrainAcc 0.5384	ValidAcc 0.5176	TestAcc 0.5171	BestValid 0.5176
	Epoch 2200:	Loss 1.4029	TrainAcc 0.5385	ValidAcc 0.5150	TestAcc 0.5172	BestValid 0.5176
	Epoch 2250:	Loss 1.4001	TrainAcc 0.5374	ValidAcc 0.5138	TestAcc 0.5164	BestValid 0.5176
	Epoch 2300:	Loss 1.3970	TrainAcc 0.5399	ValidAcc 0.5158	TestAcc 0.5176	BestValid 0.5176
	Epoch 2350:	Loss 1.3947	TrainAcc 0.5399	ValidAcc 0.5160	TestAcc 0.5167	BestValid 0.5176
	Epoch 2400:	Loss 1.3933	TrainAcc 0.5371	ValidAcc 0.5159	TestAcc 0.5147	BestValid 0.5176
	Epoch 2450:	Loss 1.3975	TrainAcc 0.5400	ValidAcc 0.5177	TestAcc 0.5166	BestValid 0.5177
	Epoch 2500:	Loss 1.3915	TrainAcc 0.5428	ValidAcc 0.5173	TestAcc 0.5179	BestValid 0.5177
	Epoch 2550:	Loss 1.3942	TrainAcc 0.5414	ValidAcc 0.5178	TestAcc 0.5159	BestValid 0.5178
	Epoch 2600:	Loss 1.3941	TrainAcc 0.5440	ValidAcc 0.5176	TestAcc 0.5178	BestValid 0.5178
	Epoch 2650:	Loss 1.3915	TrainAcc 0.5429	ValidAcc 0.5172	TestAcc 0.5166	BestValid 0.5178
	Epoch 2700:	Loss 1.3875	TrainAcc 0.5413	ValidAcc 0.5176	TestAcc 0.5153	BestValid 0.5178
	Epoch 2750:	Loss 1.3856	TrainAcc 0.5422	ValidAcc 0.5152	TestAcc 0.5141	BestValid 0.5178
	Epoch 2800:	Loss 1.3876	TrainAcc 0.5457	ValidAcc 0.5186	TestAcc 0.5175	BestValid 0.5186
	Epoch 2850:	Loss 1.3840	TrainAcc 0.5453	ValidAcc 0.5176	TestAcc 0.5187	BestValid 0.5186
	Epoch 2900:	Loss 1.3862	TrainAcc 0.5431	ValidAcc 0.5177	TestAcc 0.5146	BestValid 0.5186
	Epoch 2950:	Loss 1.3833	TrainAcc 0.5448	ValidAcc 0.5191	TestAcc 0.5171	BestValid 0.5191
	Epoch 3000:	Loss 1.3821	TrainAcc 0.5451	ValidAcc 0.5189	TestAcc 0.5151	BestValid 0.5191
	Epoch 3050:	Loss 1.3790	TrainAcc 0.5440	ValidAcc 0.5155	TestAcc 0.5136	BestValid 0.5191
	Epoch 3100:	Loss 1.3801	TrainAcc 0.5475	ValidAcc 0.5188	TestAcc 0.5153	BestValid 0.5191
	Epoch 3150:	Loss 1.3794	TrainAcc 0.5468	ValidAcc 0.5180	TestAcc 0.5149	BestValid 0.5191
	Epoch 3200:	Loss 1.3778	TrainAcc 0.5524	ValidAcc 0.5181	TestAcc 0.5195	BestValid 0.5191
	Epoch 3250:	Loss 1.3765	TrainAcc 0.5496	ValidAcc 0.5182	TestAcc 0.5166	BestValid 0.5191
	Epoch 3300:	Loss 1.3733	TrainAcc 0.5406	ValidAcc 0.5088	TestAcc 0.5087	BestValid 0.5191
	Epoch 3350:	Loss 1.3678	TrainAcc 0.5532	ValidAcc 0.5208	TestAcc 0.5197	BestValid 0.5208
	Epoch 3400:	Loss 1.3728	TrainAcc 0.5464	ValidAcc 0.5146	TestAcc 0.5128	BestValid 0.5208
	Epoch 3450:	Loss 1.3763	TrainAcc 0.5488	ValidAcc 0.5131	TestAcc 0.5141	BestValid 0.5208
	Epoch 3500:	Loss 1.3747	TrainAcc 0.5453	ValidAcc 0.5123	TestAcc 0.5108	BestValid 0.5208
	Epoch 3550:	Loss 1.3688	TrainAcc 0.5528	ValidAcc 0.5194	TestAcc 0.5184	BestValid 0.5208
	Epoch 3600:	Loss 1.3688	TrainAcc 0.5500	ValidAcc 0.5130	TestAcc 0.5130	BestValid 0.5208
	Epoch 3650:	Loss 1.3678	TrainAcc 0.5467	ValidAcc 0.5143	TestAcc 0.5118	BestValid 0.5208
	Epoch 3700:	Loss 1.3677	TrainAcc 0.5485	ValidAcc 0.5147	TestAcc 0.5136	BestValid 0.5208
	Epoch 3750:	Loss 1.3671	TrainAcc 0.5518	ValidAcc 0.5159	TestAcc 0.5135	BestValid 0.5208
	Epoch 3800:	Loss 1.3696	TrainAcc 0.5489	ValidAcc 0.5155	TestAcc 0.5124	BestValid 0.5208
	Epoch 3850:	Loss 1.3635	TrainAcc 0.5441	ValidAcc 0.5095	TestAcc 0.5077	BestValid 0.5208
	Epoch 3900:	Loss 1.3638	TrainAcc 0.5492	ValidAcc 0.5146	TestAcc 0.5123	BestValid 0.5208
	Epoch 3950:	Loss 1.3640	TrainAcc 0.5577	ValidAcc 0.5213	TestAcc 0.5182	BestValid 0.5213
	Epoch 4000:	Loss 1.3604	TrainAcc 0.5502	ValidAcc 0.5141	TestAcc 0.5127	BestValid 0.5213
	Epoch 4050:	Loss 1.3641	TrainAcc 0.5442	ValidAcc 0.5095	TestAcc 0.5071	BestValid 0.5213
	Epoch 4100:	Loss 1.3611	TrainAcc 0.5553	ValidAcc 0.5163	TestAcc 0.5147	BestValid 0.5213
	Epoch 4150:	Loss 1.3580	TrainAcc 0.5511	ValidAcc 0.5141	TestAcc 0.5124	BestValid 0.5213
	Epoch 4200:	Loss 1.3630	TrainAcc 0.5478	ValidAcc 0.5104	TestAcc 0.5091	BestValid 0.5213
	Epoch 4250:	Loss 1.3569	TrainAcc 0.5444	ValidAcc 0.5086	TestAcc 0.5053	BestValid 0.5213
	Epoch 4300:	Loss 1.3575	TrainAcc 0.5590	ValidAcc 0.5189	TestAcc 0.5177	BestValid 0.5213
	Epoch 4350:	Loss 1.3583	TrainAcc 0.5507	ValidAcc 0.5126	TestAcc 0.5095	BestValid 0.5213
	Epoch 4400:	Loss 1.3578	TrainAcc 0.5448	ValidAcc 0.5079	TestAcc 0.5052	BestValid 0.5213
	Epoch 4450:	Loss 1.3555	TrainAcc 0.5539	ValidAcc 0.5140	TestAcc 0.5132	BestValid 0.5213
	Epoch 4500:	Loss 1.3551	TrainAcc 0.5379	ValidAcc 0.5004	TestAcc 0.5000	BestValid 0.5213
	Epoch 4550:	Loss 1.3538	TrainAcc 0.5488	ValidAcc 0.5095	TestAcc 0.5071	BestValid 0.5213
	Epoch 4600:	Loss 1.3507	TrainAcc 0.5423	ValidAcc 0.5045	TestAcc 0.5018	BestValid 0.5213
	Epoch 4650:	Loss 1.3500	TrainAcc 0.5614	ValidAcc 0.5181	TestAcc 0.5185	BestValid 0.5213
	Epoch 4700:	Loss 1.3531	TrainAcc 0.5498	ValidAcc 0.5107	TestAcc 0.5080	BestValid 0.5213
	Epoch 4750:	Loss 1.3476	TrainAcc 0.5552	ValidAcc 0.5124	TestAcc 0.5105	BestValid 0.5213
	Epoch 4800:	Loss 1.3476	TrainAcc 0.5508	ValidAcc 0.5111	TestAcc 0.5081	BestValid 0.5213
	Epoch 4850:	Loss 1.3497	TrainAcc 0.5583	ValidAcc 0.5153	TestAcc 0.5130	BestValid 0.5213
	Epoch 4900:	Loss 1.3519	TrainAcc 0.5499	ValidAcc 0.5107	TestAcc 0.5075	BestValid 0.5213
	Epoch 4950:	Loss 1.3461	TrainAcc 0.5489	ValidAcc 0.5082	TestAcc 0.5055	BestValid 0.5213
	Epoch 5000:	Loss 1.3489	TrainAcc 0.5604	ValidAcc 0.5151	TestAcc 0.5141	BestValid 0.5213
****** Epoch Time (Excluding Evaluation Cost): 0.095 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.503 ms (Max: 18.699, Min: 15.848, Sum: 140.023)
Cluster-Wide Average, Compute: 51.133 ms (Max: 62.895, Min: 47.422, Sum: 409.066)
Cluster-Wide Average, Communication-Layer: 14.352 ms (Max: 16.769, Min: 10.846, Sum: 114.819)
Cluster-Wide Average, Bubble-Imbalance: 10.111 ms (Max: 13.734, Min: 2.215, Sum: 80.889)
Cluster-Wide Average, Communication-Graph: 0.448 ms (Max: 0.508, Min: 0.392, Sum: 3.584)
Cluster-Wide Average, Optimization: 0.094 ms (Max: 0.113, Min: 0.088, Sum: 0.752)
Cluster-Wide Average, Others: 1.180 ms (Max: 3.958, Min: 0.774, Sum: 9.439)
****** Breakdown Sum: 94.821 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.004 GB (Max: 4.174, Min: 2.821, Sum: 24.030)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 68.696 Gbps (Max: 82.536, Min: 51.409, Sum: 549.569)
Layer-level communication (cluster-wide, per-epoch): 0.931 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.931 GB
****** Accuracy Results ******
Highest valid_acc: 0.5213
Target test_acc: 0.5182
Epoch to reach the target acc: 3949
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
