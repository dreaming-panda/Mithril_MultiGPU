Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.886 seconds.
Building the CSC structure...
        It takes 1.929 seconds.
Building the CSC structure...
        It takes 2.056 seconds.
Building the CSC structure...
        It takes 2.098 seconds.
Building the CSC structure...
        It takes 2.331 seconds.
Building the CSC structure...
        It takes 2.390 seconds.
Building the CSC structure...
        It takes 2.587 seconds.
Building the CSC structure...
        It takes 2.602 seconds.
Building the CSC structure...
        It takes 1.809 seconds.
        It takes 1.842 seconds.
        It takes 1.879 seconds.
        It takes 1.878 seconds.
        It takes 2.314 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.454 seconds.
        It takes 2.324 seconds.
        It takes 2.326 seconds.
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
Building the Feature Vector...
        It takes 0.292 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
Building the Feature Vector...
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
Building the Feature Vector...
        It takes 0.273 seconds.
Building the Label Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.039 seconds.
Building the Feature Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
        It takes 0.279 seconds.
Building the Label Vector...
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.048 seconds.
Building the Feature Vector...
        It takes 0.039 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.256 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.210 Gbps (per GPU), 441.678 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.948 Gbps (per GPU), 439.587 Gbps (aggregated)
The layer-level communication performance: 55.301 Gbps (per GPU), 442.408 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.748 Gbps (per GPU), 437.987 Gbps (aggregated)
The layer-level communication performance: 54.715 Gbps (per GPU), 437.717 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.831 Gbps (per GPU), 438.651 Gbps (aggregated)
The layer-level communication performance: 54.487 Gbps (per GPU), 435.895 Gbps (aggregated)
The layer-level communication performance: 54.449 Gbps (per GPU), 435.589 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.414 Gbps (per GPU), 1259.314 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.423 Gbps (per GPU), 1259.385 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.482 Gbps (per GPU), 1259.859 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.429 Gbps (per GPU), 1259.433 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.423 Gbps (per GPU), 1259.385 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.323 Gbps (per GPU), 1258.583 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.479 Gbps (per GPU), 1259.835 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.317 Gbps (per GPU), 1258.535 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.761 Gbps (per GPU), 814.085 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.760 Gbps (per GPU), 814.079 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.761 Gbps (per GPU), 814.085 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.760 Gbps (per GPU), 814.079 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.759 Gbps (per GPU), 814.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.759 Gbps (per GPU), 814.072 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.762 Gbps (per GPU), 814.092 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.761 Gbps (per GPU), 814.085 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.111 Gbps (per GPU), 272.892 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.112 Gbps (per GPU), 272.899 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.111 Gbps (per GPU), 272.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.111 Gbps (per GPU), 272.890 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.111 Gbps (per GPU), 272.890 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.109 Gbps (per GPU), 272.876 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.112 Gbps (per GPU), 272.897 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.109 Gbps (per GPU), 272.873 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.88ms  2.42ms  2.70ms  3.05  8.38K  3.53M
 chk_1  0.75ms  2.72ms  2.91ms  3.88  6.74K  3.60M
 chk_2  0.79ms  2.62ms  2.78ms  3.52  7.27K  3.53M
 chk_3  0.80ms  2.64ms  2.81ms  3.52  7.92K  3.61M
 chk_4  0.62ms  2.56ms  2.72ms  4.38  5.33K  3.68M
 chk_5  1.00ms  2.55ms  2.76ms  2.77 10.07K  3.45M
 chk_6  0.95ms  2.75ms  2.92ms  3.06  9.41K  3.48M
 chk_7  0.81ms  2.58ms  2.73ms  3.37  8.12K  3.60M
 chk_8  0.67ms  2.61ms  2.84ms  4.21  6.09K  3.64M
 chk_9  1.09ms  2.50ms  2.70ms  2.48 11.10K  3.38M
chk_10  0.64ms  2.72ms  2.88ms  4.47  5.67K  3.63M
chk_11  0.82ms  2.59ms  2.76ms  3.38  8.16K  3.54M
chk_12  0.79ms  2.78ms  2.95ms  3.74  7.24K  3.55M
chk_13  0.63ms  2.62ms  2.78ms  4.40  5.41K  3.68M
chk_14  0.77ms  2.86ms  3.03ms  3.92  7.14K  3.53M
chk_15  0.94ms  2.71ms  2.89ms  3.07  9.25K  3.49M
chk_16  0.59ms  2.55ms  2.68ms  4.54  4.78K  3.77M
chk_17  0.76ms  2.66ms  2.83ms  3.74  6.85K  3.60M
chk_18  0.80ms  2.48ms  2.65ms  3.30  7.47K  3.57M
chk_19  0.60ms  2.54ms  2.69ms  4.49  4.88K  3.75M
chk_20  0.76ms  2.56ms  2.69ms  3.52  7.00K  3.63M
chk_21  0.63ms  2.54ms  2.68ms  4.27  5.41K  3.68M
chk_22  1.09ms  2.75ms  2.93ms  2.69 11.07K  3.39M
chk_23  0.79ms  2.63ms  2.79ms  3.54  7.23K  3.64M
chk_24  1.00ms  2.71ms  2.89ms  2.88 10.13K  3.43M
chk_25  0.72ms  2.50ms  2.66ms  3.69  6.40K  3.57M
chk_26  0.65ms  2.71ms  2.86ms  4.37  5.78K  3.55M
chk_27  0.95ms  2.57ms  2.77ms  2.91  9.34K  3.48M
chk_28  0.72ms  2.89ms  3.04ms  4.22  6.37K  3.57M
chk_29  0.62ms  2.69ms  2.83ms  4.54  5.16K  3.78M
chk_30  0.64ms  2.59ms  2.73ms  4.30  5.44K  3.67M
chk_31  0.72ms  2.71ms  2.88ms  4.01  6.33K  3.63M
   Avg  0.78  2.63  2.80
   Max  1.09  2.89  3.04
   Min  0.59  2.42  2.65
 Ratio  1.85  1.19  1.15
   Var  0.02  0.01  0.01
Profiling takes 2.402 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 362.273 ms
Partition 0 [0, 5) has cost: 362.273 ms
Partition 1 [5, 9) has cost: 337.261 ms
Partition 2 [9, 13) has cost: 337.261 ms
Partition 3 [13, 17) has cost: 337.261 ms
Partition 4 [17, 21) has cost: 337.261 ms
Partition 5 [21, 25) has cost: 337.261 ms
Partition 6 [25, 29) has cost: 337.261 ms
Partition 7 [29, 33) has cost: 342.695 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.459 ms
GPU 0, Compute+Comm Time: 132.309 ms, Bubble Time: 28.777 ms, Imbalance Overhead: 3.373 ms
GPU 1, Compute+Comm Time: 125.334 ms, Bubble Time: 28.490 ms, Imbalance Overhead: 10.634 ms
GPU 2, Compute+Comm Time: 125.334 ms, Bubble Time: 28.462 ms, Imbalance Overhead: 10.662 ms
GPU 3, Compute+Comm Time: 125.334 ms, Bubble Time: 28.378 ms, Imbalance Overhead: 10.746 ms
GPU 4, Compute+Comm Time: 125.334 ms, Bubble Time: 28.300 ms, Imbalance Overhead: 10.825 ms
GPU 5, Compute+Comm Time: 125.334 ms, Bubble Time: 28.304 ms, Imbalance Overhead: 10.821 ms
GPU 6, Compute+Comm Time: 125.334 ms, Bubble Time: 28.549 ms, Imbalance Overhead: 10.576 ms
GPU 7, Compute+Comm Time: 126.499 ms, Bubble Time: 28.976 ms, Imbalance Overhead: 8.983 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 316.658 ms
GPU 0, Compute+Comm Time: 243.202 ms, Bubble Time: 56.488 ms, Imbalance Overhead: 16.969 ms
GPU 1, Compute+Comm Time: 238.932 ms, Bubble Time: 55.618 ms, Imbalance Overhead: 22.109 ms
GPU 2, Compute+Comm Time: 238.932 ms, Bubble Time: 55.057 ms, Imbalance Overhead: 22.670 ms
GPU 3, Compute+Comm Time: 238.932 ms, Bubble Time: 54.947 ms, Imbalance Overhead: 22.780 ms
GPU 4, Compute+Comm Time: 238.932 ms, Bubble Time: 55.032 ms, Imbalance Overhead: 22.694 ms
GPU 5, Compute+Comm Time: 238.932 ms, Bubble Time: 55.124 ms, Imbalance Overhead: 22.603 ms
GPU 6, Compute+Comm Time: 238.932 ms, Bubble Time: 54.960 ms, Imbalance Overhead: 22.767 ms
GPU 7, Compute+Comm Time: 256.969 ms, Bubble Time: 55.414 ms, Imbalance Overhead: 4.275 ms
The estimated cost of the whole pipeline: 505.173 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 699.533 ms
Partition 0 [0, 9) has cost: 699.533 ms
Partition 1 [9, 17) has cost: 674.521 ms
Partition 2 [17, 25) has cost: 674.521 ms
Partition 3 [25, 33) has cost: 679.955 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.528 ms
GPU 0, Compute+Comm Time: 141.330 ms, Bubble Time: 26.703 ms, Imbalance Overhead: 2.495 ms
GPU 1, Compute+Comm Time: 137.552 ms, Bubble Time: 26.313 ms, Imbalance Overhead: 6.662 ms
GPU 2, Compute+Comm Time: 137.552 ms, Bubble Time: 26.094 ms, Imbalance Overhead: 6.881 ms
GPU 3, Compute+Comm Time: 138.104 ms, Bubble Time: 25.851 ms, Imbalance Overhead: 6.573 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 315.878 ms
GPU 0, Compute+Comm Time: 254.909 ms, Bubble Time: 48.267 ms, Imbalance Overhead: 12.703 ms
GPU 1, Compute+Comm Time: 252.746 ms, Bubble Time: 48.511 ms, Imbalance Overhead: 14.621 ms
GPU 2, Compute+Comm Time: 252.746 ms, Bubble Time: 48.718 ms, Imbalance Overhead: 14.414 ms
GPU 3, Compute+Comm Time: 262.714 ms, Bubble Time: 49.653 ms, Imbalance Overhead: 3.511 ms
    The estimated cost with 2 DP ways is 510.726 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1374.054 ms
Partition 0 [0, 17) has cost: 1374.054 ms
Partition 1 [17, 33) has cost: 1354.476 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 217.090 ms
GPU 0, Compute+Comm Time: 188.970 ms, Bubble Time: 23.193 ms, Imbalance Overhead: 4.926 ms
GPU 1, Compute+Comm Time: 187.179 ms, Bubble Time: 23.882 ms, Imbalance Overhead: 6.029 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 354.271 ms
GPU 0, Compute+Comm Time: 306.356 ms, Bubble Time: 39.114 ms, Imbalance Overhead: 8.801 ms
GPU 1, Compute+Comm Time: 310.883 ms, Bubble Time: 37.912 ms, Imbalance Overhead: 5.476 ms
    The estimated cost with 4 DP ways is 599.929 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2728.531 ms
Partition 0 [0, 33) has cost: 2728.531 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 557.347 ms
GPU 0, Compute+Comm Time: 557.347 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 680.710 ms
GPU 0, Compute+Comm Time: 680.710 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1299.960 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7787	TrainAcc 0.0846	ValidAcc 0.0762	TestAcc 0.0769	BestValid 0.0762
	Epoch 50:	Loss 1.8963	TrainAcc 0.6719	ValidAcc 0.6874	TestAcc 0.6820	BestValid 0.6874
	Epoch 100:	Loss 1.2806	TrainAcc 0.8066	ValidAcc 0.8182	TestAcc 0.8128	BestValid 0.8182
	Epoch 150:	Loss 1.0383	TrainAcc 0.8415	ValidAcc 0.8509	TestAcc 0.8454	BestValid 0.8509
	Epoch 200:	Loss 0.9090	TrainAcc 0.8635	ValidAcc 0.8718	TestAcc 0.8645	BestValid 0.8718
	Epoch 250:	Loss 0.8396	TrainAcc 0.8782	ValidAcc 0.8843	TestAcc 0.8782	BestValid 0.8843
	Epoch 300:	Loss 0.7776	TrainAcc 0.8876	ValidAcc 0.8919	TestAcc 0.8875	BestValid 0.8919
	Epoch 350:	Loss 0.7358	TrainAcc 0.8942	ValidAcc 0.8978	TestAcc 0.8929	BestValid 0.8978
	Epoch 400:	Loss 0.7081	TrainAcc 0.8996	ValidAcc 0.9039	TestAcc 0.8984	BestValid 0.9039
	Epoch 450:	Loss 0.6769	TrainAcc 0.9033	ValidAcc 0.9071	TestAcc 0.9015	BestValid 0.9071
	Epoch 500:	Loss 0.6527	TrainAcc 0.9078	ValidAcc 0.9118	TestAcc 0.9053	BestValid 0.9118
	Epoch 550:	Loss 0.6376	TrainAcc 0.9108	ValidAcc 0.9139	TestAcc 0.9084	BestValid 0.9139
	Epoch 600:	Loss 0.6189	TrainAcc 0.9134	ValidAcc 0.9156	TestAcc 0.9112	BestValid 0.9156
	Epoch 650:	Loss 0.6009	TrainAcc 0.9153	ValidAcc 0.9170	TestAcc 0.9130	BestValid 0.9170
	Epoch 700:	Loss 0.5932	TrainAcc 0.9174	ValidAcc 0.9188	TestAcc 0.9151	BestValid 0.9188
	Epoch 750:	Loss 0.5787	TrainAcc 0.9195	ValidAcc 0.9203	TestAcc 0.9172	BestValid 0.9203
	Epoch 800:	Loss 0.5688	TrainAcc 0.9216	ValidAcc 0.9234	TestAcc 0.9191	BestValid 0.9234
	Epoch 850:	Loss 0.5584	TrainAcc 0.9223	ValidAcc 0.9235	TestAcc 0.9195	BestValid 0.9235
	Epoch 900:	Loss 0.5531	TrainAcc 0.9233	ValidAcc 0.9242	TestAcc 0.9207	BestValid 0.9242
	Epoch 950:	Loss 0.5392	TrainAcc 0.9249	ValidAcc 0.9256	TestAcc 0.9224	BestValid 0.9256
	Epoch 1000:	Loss 0.5441	TrainAcc 0.9262	ValidAcc 0.9261	TestAcc 0.9235	BestValid 0.9261
	Epoch 1050:	Loss 0.5342	TrainAcc 0.9277	ValidAcc 0.9286	TestAcc 0.9246	BestValid 0.9286
	Epoch 1100:	Loss 0.5252	TrainAcc 0.9285	ValidAcc 0.9285	TestAcc 0.9250	BestValid 0.9286
	Epoch 1150:	Loss 0.5130	TrainAcc 0.9289	ValidAcc 0.9287	TestAcc 0.9254	BestValid 0.9287
	Epoch 1200:	Loss 0.5159	TrainAcc 0.9296	ValidAcc 0.9293	TestAcc 0.9262	BestValid 0.9293
	Epoch 1250:	Loss 0.5112	TrainAcc 0.9293	ValidAcc 0.9282	TestAcc 0.9261	BestValid 0.9293
	Epoch 1300:	Loss 0.5013	TrainAcc 0.9320	ValidAcc 0.9310	TestAcc 0.9284	BestValid 0.9310
	Epoch 1350:	Loss 0.4982	TrainAcc 0.9321	ValidAcc 0.9313	TestAcc 0.9285	BestValid 0.9313
	Epoch 1400:	Loss 0.4944	TrainAcc 0.9319	ValidAcc 0.9313	TestAcc 0.9285	BestValid 0.9313
	Epoch 1450:	Loss 0.4926	TrainAcc 0.9319	ValidAcc 0.9308	TestAcc 0.9284	BestValid 0.9313
	Epoch 1500:	Loss 0.4879	TrainAcc 0.9329	ValidAcc 0.9322	TestAcc 0.9289	BestValid 0.9322
	Epoch 1550:	Loss 0.4819	TrainAcc 0.9332	ValidAcc 0.9327	TestAcc 0.9292	BestValid 0.9327
	Epoch 1600:	Loss 0.4841	TrainAcc 0.9348	ValidAcc 0.9341	TestAcc 0.9307	BestValid 0.9341
	Epoch 1650:	Loss 0.4756	TrainAcc 0.9358	ValidAcc 0.9343	TestAcc 0.9317	BestValid 0.9343
	Epoch 1700:	Loss 0.4710	TrainAcc 0.9364	ValidAcc 0.9350	TestAcc 0.9316	BestValid 0.9350
	Epoch 1750:	Loss 0.4716	TrainAcc 0.9350	ValidAcc 0.9341	TestAcc 0.9305	BestValid 0.9350
	Epoch 1800:	Loss 0.4684	TrainAcc 0.9367	ValidAcc 0.9347	TestAcc 0.9317	BestValid 0.9350
	Epoch 1850:	Loss 0.4663	TrainAcc 0.9374	ValidAcc 0.9353	TestAcc 0.9328	BestValid 0.9353
	Epoch 1900:	Loss 0.4610	TrainAcc 0.9382	ValidAcc 0.9358	TestAcc 0.9333	BestValid 0.9358
	Epoch 1950:	Loss 0.4553	TrainAcc 0.9380	ValidAcc 0.9353	TestAcc 0.9334	BestValid 0.9358
	Epoch 2000:	Loss 0.4572	TrainAcc 0.9378	ValidAcc 0.9351	TestAcc 0.9332	BestValid 0.9358
	Epoch 2050:	Loss 0.4523	TrainAcc 0.9389	ValidAcc 0.9361	TestAcc 0.9343	BestValid 0.9361
	Epoch 2100:	Loss 0.4518	TrainAcc 0.9398	ValidAcc 0.9362	TestAcc 0.9352	BestValid 0.9362
	Epoch 2150:	Loss 0.4500	TrainAcc 0.9397	ValidAcc 0.9365	TestAcc 0.9351	BestValid 0.9365
	Epoch 2200:	Loss 0.4457	TrainAcc 0.9409	ValidAcc 0.9377	TestAcc 0.9361	BestValid 0.9377
	Epoch 2250:	Loss 0.4458	TrainAcc 0.9408	ValidAcc 0.9377	TestAcc 0.9358	BestValid 0.9377
	Epoch 2300:	Loss 0.4391	TrainAcc 0.9374	ValidAcc 0.9342	TestAcc 0.9319	BestValid 0.9377
	Epoch 2350:	Loss 0.4409	TrainAcc 0.9388	ValidAcc 0.9349	TestAcc 0.9334	BestValid 0.9377
	Epoch 2400:	Loss 0.4396	TrainAcc 0.9428	ValidAcc 0.9397	TestAcc 0.9374	BestValid 0.9397
	Epoch 2450:	Loss 0.4350	TrainAcc 0.9420	ValidAcc 0.9388	TestAcc 0.9367	BestValid 0.9397
	Epoch 2500:	Loss 0.4342	TrainAcc 0.9418	ValidAcc 0.9383	TestAcc 0.9362	BestValid 0.9397
	Epoch 2550:	Loss 0.4354	TrainAcc 0.9398	ValidAcc 0.9363	TestAcc 0.9339	BestValid 0.9397
	Epoch 2600:	Loss 0.4306	TrainAcc 0.9389	ValidAcc 0.9356	TestAcc 0.9331	BestValid 0.9397
	Epoch 2650:	Loss 0.4283	TrainAcc 0.9420	ValidAcc 0.9389	TestAcc 0.9359	BestValid 0.9397
	Epoch 2700:	Loss 0.4268	TrainAcc 0.9418	ValidAcc 0.9383	TestAcc 0.9360	BestValid 0.9397
	Epoch 2750:	Loss 0.4329	TrainAcc 0.9439	ValidAcc 0.9410	TestAcc 0.9380	BestValid 0.9410
	Epoch 2800:	Loss 0.4274	TrainAcc 0.9430	ValidAcc 0.9395	TestAcc 0.9374	BestValid 0.9410
	Epoch 2850:	Loss 0.4255	TrainAcc 0.9406	ValidAcc 0.9372	TestAcc 0.9339	BestValid 0.9410
	Epoch 2900:	Loss 0.4244	TrainAcc 0.9398	ValidAcc 0.9368	TestAcc 0.9331	BestValid 0.9410
	Epoch 2950:	Loss 0.4199	TrainAcc 0.9417	ValidAcc 0.9382	TestAcc 0.9353	BestValid 0.9410
	Epoch 3000:	Loss 0.4244	TrainAcc 0.9421	ValidAcc 0.9388	TestAcc 0.9363	BestValid 0.9410
	Epoch 3050:	Loss 0.4174	TrainAcc 0.9401	ValidAcc 0.9363	TestAcc 0.9339	BestValid 0.9410
	Epoch 3100:	Loss 0.4147	TrainAcc 0.9423	ValidAcc 0.9388	TestAcc 0.9356	BestValid 0.9410
	Epoch 3150:	Loss 0.4194	TrainAcc 0.9349	ValidAcc 0.9298	TestAcc 0.9276	BestValid 0.9410
	Epoch 3200:	Loss 0.4157	TrainAcc 0.9392	ValidAcc 0.9345	TestAcc 0.9320	BestValid 0.9410
	Epoch 3250:	Loss 0.4158	TrainAcc 0.9408	ValidAcc 0.9360	TestAcc 0.9334	BestValid 0.9410
	Epoch 3300:	Loss 0.4134	TrainAcc 0.9464	ValidAcc 0.9431	TestAcc 0.9400	BestValid 0.9431
	Epoch 3350:	Loss 0.4120	TrainAcc 0.9445	ValidAcc 0.9401	TestAcc 0.9375	BestValid 0.9431
	Epoch 3400:	Loss 0.4099	TrainAcc 0.9349	ValidAcc 0.9296	TestAcc 0.9274	BestValid 0.9431
	Epoch 3450:	Loss 0.4092	TrainAcc 0.9387	ValidAcc 0.9340	TestAcc 0.9312	BestValid 0.9431
	Epoch 3500:	Loss 0.4132	TrainAcc 0.9439	ValidAcc 0.9394	TestAcc 0.9368	BestValid 0.9431
	Epoch 3550:	Loss 0.4099	TrainAcc 0.9450	ValidAcc 0.9405	TestAcc 0.9373	BestValid 0.9431
	Epoch 3600:	Loss 0.4106	TrainAcc 0.9475	ValidAcc 0.9434	TestAcc 0.9407	BestValid 0.9434
	Epoch 3650:	Loss 0.4084	TrainAcc 0.9453	ValidAcc 0.9412	TestAcc 0.9389	BestValid 0.9434
	Epoch 3700:	Loss 0.4062	TrainAcc 0.9396	ValidAcc 0.9343	TestAcc 0.9321	BestValid 0.9434
	Epoch 3750:	Loss 0.4081	TrainAcc 0.9362	ValidAcc 0.9299	TestAcc 0.9277	BestValid 0.9434
	Epoch 3800:	Loss 0.4059	TrainAcc 0.9440	ValidAcc 0.9386	TestAcc 0.9361	BestValid 0.9434
	Epoch 3850:	Loss 0.4096	TrainAcc 0.9458	ValidAcc 0.9412	TestAcc 0.9383	BestValid 0.9434
	Epoch 3900:	Loss 0.4077	TrainAcc 0.9478	ValidAcc 0.9430	TestAcc 0.9412	BestValid 0.9434
	Epoch 3950:	Loss 0.4023	TrainAcc 0.9464	ValidAcc 0.9410	TestAcc 0.9387	BestValid 0.9434
	Epoch 4000:	Loss 0.4037	TrainAcc 0.9417	ValidAcc 0.9375	TestAcc 0.9344	BestValid 0.9434
	Epoch 4050:	Loss 0.4000	TrainAcc 0.9392	ValidAcc 0.9330	TestAcc 0.9306	BestValid 0.9434
	Epoch 4100:	Loss 0.3949	TrainAcc 0.9476	ValidAcc 0.9430	TestAcc 0.9411	BestValid 0.9434
	Epoch 4150:	Loss 0.3975	TrainAcc 0.9456	ValidAcc 0.9405	TestAcc 0.9386	BestValid 0.9434
	Epoch 4200:	Loss 0.3989	TrainAcc 0.9485	ValidAcc 0.9437	TestAcc 0.9418	BestValid 0.9437
	Epoch 4250:	Loss 0.4009	TrainAcc 0.9440	ValidAcc 0.9378	TestAcc 0.9359	BestValid 0.9437
	Epoch 4300:	Loss 0.3980	TrainAcc 0.9394	ValidAcc 0.9329	TestAcc 0.9310	BestValid 0.9437
	Epoch 4350:	Loss 0.3993	TrainAcc 0.9408	ValidAcc 0.9350	TestAcc 0.9328	BestValid 0.9437
	Epoch 4400:	Loss 0.4001	TrainAcc 0.9485	ValidAcc 0.9435	TestAcc 0.9415	BestValid 0.9437
	Epoch 4450:	Loss 0.4003	TrainAcc 0.9484	ValidAcc 0.9436	TestAcc 0.9412	BestValid 0.9437
	Epoch 4500:	Loss 0.3948	TrainAcc 0.9475	ValidAcc 0.9420	TestAcc 0.9404	BestValid 0.9437
	Epoch 4550:	Loss 0.3933	TrainAcc 0.9450	ValidAcc 0.9399	TestAcc 0.9372	BestValid 0.9437
	Epoch 4600:	Loss 0.3950	TrainAcc 0.9454	ValidAcc 0.9397	TestAcc 0.9378	BestValid 0.9437
	Epoch 4650:	Loss 0.3977	TrainAcc 0.9481	ValidAcc 0.9421	TestAcc 0.9408	BestValid 0.9437
	Epoch 4700:	Loss 0.3937	TrainAcc 0.9486	ValidAcc 0.9423	TestAcc 0.9415	BestValid 0.9437
	Epoch 4750:	Loss 0.3925	TrainAcc 0.9493	ValidAcc 0.9434	TestAcc 0.9425	BestValid 0.9437
	Epoch 4800:	Loss 0.3922	TrainAcc 0.9486	ValidAcc 0.9426	TestAcc 0.9416	BestValid 0.9437
	Epoch 4850:	Loss 0.3900	TrainAcc 0.9428	ValidAcc 0.9376	TestAcc 0.9354	BestValid 0.9437
	Epoch 4900:	Loss 0.3960	TrainAcc 0.9454	ValidAcc 0.9391	TestAcc 0.9373	BestValid 0.9437
	Epoch 4950:	Loss 0.3891	TrainAcc 0.9497	ValidAcc 0.9447	TestAcc 0.9428	BestValid 0.9447
	Epoch 5000:	Loss 0.3927	TrainAcc 0.9494	ValidAcc 0.9447	TestAcc 0.9424	BestValid 0.9447
****** Epoch Time (Excluding Evaluation Cost): 0.416 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 80.119 ms (Max: 82.849, Min: 66.965, Sum: 640.956)
Cluster-Wide Average, Compute: 262.424 ms (Max: 286.491, Min: 253.807, Sum: 2099.393)
Cluster-Wide Average, Communication-Layer: 41.839 ms (Max: 47.348, Min: 32.016, Sum: 334.709)
Cluster-Wide Average, Bubble-Imbalance: 28.274 ms (Max: 33.403, Min: 13.183, Sum: 226.192)
Cluster-Wide Average, Communication-Graph: 0.457 ms (Max: 0.492, Min: 0.406, Sum: 3.655)
Cluster-Wide Average, Optimization: 0.100 ms (Max: 0.121, Min: 0.093, Sum: 0.801)
Cluster-Wide Average, Others: 3.737 ms (Max: 17.421, Min: 1.775, Sum: 29.897)
****** Breakdown Sum: 416.950 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.304 GB (Max: 8.059, Min: 6.018, Sum: 50.433)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 61.332 Gbps (Max: 72.161, Min: 45.325, Sum: 490.656)
Layer-level communication (cluster-wide, per-epoch): 2.430 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 2.430 GB
****** Accuracy Results ******
Highest valid_acc: 0.9447
Target test_acc: 0.9428
Epoch to reach the target acc: 4949
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
