Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.030 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
        It takes 0.021 seconds.
        It takes 0.022 seconds.
        It takes 0.017 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.095 seconds.
Building the Label Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.463 Gbps (per GPU), 483.703 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.151 Gbps (per GPU), 481.209 Gbps (aggregated)
The layer-level communication performance: 60.149 Gbps (per GPU), 481.193 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.893 Gbps (per GPU), 479.141 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.629 Gbps (per GPU), 477.031 Gbps (aggregated)
The layer-level communication performance: 59.864 Gbps (per GPU), 478.913 Gbps (aggregated)
The layer-level communication performance: 59.582 Gbps (per GPU), 476.658 Gbps (aggregated)
The layer-level communication performance: 59.547 Gbps (per GPU), 476.376 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.957 Gbps (per GPU), 1247.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.983 Gbps (per GPU), 1247.864 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.041 Gbps (per GPU), 1248.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.986 Gbps (per GPU), 1247.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.968 Gbps (per GPU), 1247.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.963 Gbps (per GPU), 1247.701 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.015 Gbps (per GPU), 1248.118 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.983 Gbps (per GPU), 1247.865 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.914 Gbps (per GPU), 815.312 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.916 Gbps (per GPU), 815.325 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.913 Gbps (per GPU), 815.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.915 Gbps (per GPU), 815.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.915 Gbps (per GPU), 815.318 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.916 Gbps (per GPU), 815.325 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.913 Gbps (per GPU), 815.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.542 Gbps (per GPU), 812.338 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.206 Gbps (per GPU), 241.649 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.204 Gbps (per GPU), 241.633 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.203 Gbps (per GPU), 241.622 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.203 Gbps (per GPU), 241.625 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.203 Gbps (per GPU), 241.626 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.199 Gbps (per GPU), 241.593 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.202 Gbps (per GPU), 241.620 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.195 Gbps (per GPU), 241.560 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.34ms  0.50ms  1.46  2.81K  0.03M
 chk_1  0.41ms  0.35ms  0.50ms  1.46  2.82K  0.03M
 chk_2  0.40ms  0.34ms  0.50ms  1.47  2.80K  0.03M
 chk_3  0.41ms  0.34ms  0.50ms  1.46  2.80K  0.03M
 chk_4  0.41ms  0.35ms  0.50ms  1.45  2.82K  0.03M
 chk_5  0.40ms  0.35ms  0.51ms  1.46  2.75K  0.03M
 chk_6  0.40ms  0.34ms  0.50ms  1.47  2.71K  0.03M
 chk_7  0.40ms  0.34ms  0.50ms  1.46  2.76K  0.03M
 chk_8  0.40ms  0.35ms  0.50ms  1.46  2.79K  0.03M
 chk_9  0.41ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_10  0.41ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_11  0.40ms  0.35ms  0.51ms  1.45  2.74K  0.03M
chk_12  0.40ms  0.35ms  0.51ms  1.46  2.76K  0.03M
chk_13  0.40ms  0.35ms  0.51ms  1.46  2.75K  0.03M
chk_14  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_15  0.40ms  0.34ms  0.50ms  1.46  2.77K  0.03M
chk_16  0.40ms  0.34ms  0.50ms  1.46  2.78K  0.03M
chk_17  0.41ms  0.35ms  0.51ms  1.45  2.79K  0.03M
chk_18  0.41ms  0.35ms  0.51ms  1.45  2.82K  0.03M
chk_19  0.41ms  0.34ms  0.50ms  1.47  2.81K  0.03M
chk_20  0.40ms  0.35ms  0.51ms  1.46  2.77K  0.03M
chk_21  0.41ms  0.35ms  0.51ms  1.46  2.84K  0.02M
chk_22  0.40ms  0.34ms  0.50ms  1.46  2.78K  0.03M
chk_23  0.41ms  0.34ms  0.50ms  1.46  2.80K  0.03M
chk_24  0.40ms  0.35ms  0.50ms  1.46  2.80K  0.03M
chk_25  0.41ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_26  0.41ms  0.34ms  0.50ms  1.47  2.81K  0.03M
chk_27  0.40ms  0.35ms  0.51ms  1.45  2.79K  0.03M
chk_28  0.40ms  0.35ms  0.50ms  1.46  2.77K  0.03M
chk_29  0.40ms  0.34ms  0.50ms  1.47  2.77K  0.03M
chk_30  0.41ms  0.35ms  0.51ms  1.46  2.80K  0.03M
chk_31  0.40ms  0.35ms  0.51ms  1.46  2.78K  0.03M
   Avg  0.40  0.35  0.50
   Max  0.41  0.35  0.51
   Min  0.40  0.34  0.50
 Ratio  1.02  1.04  1.02
   Var  0.00  0.00  0.00
Profiling takes 0.592 s
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
*** Node 7, starting model training...
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9379	TrainAcc 0.4153	ValidAcc 0.4187	TestAcc 0.4183	BestValid 0.4187
	Epoch 50:	Loss 1.6744	TrainAcc 0.4544	ValidAcc 0.4541	TestAcc 0.4545	BestValid 0.4541
	Epoch 100:	Loss 1.6374	TrainAcc 0.4691	ValidAcc 0.4700	TestAcc 0.4698	BestValid 0.4700
	Epoch 150:	Loss 1.6122	TrainAcc 0.4709	ValidAcc 0.4707	TestAcc 0.4708	BestValid 0.4707
	Epoch 200:	Loss 1.6029	TrainAcc 0.4772	ValidAcc 0.4787	TestAcc 0.4789	BestValid 0.4787
	Epoch 250:	Loss 1.5983	TrainAcc 0.4753	ValidAcc 0.4756	TestAcc 0.4746	BestValid 0.4787
	Epoch 300:	Loss 1.5845	TrainAcc 0.4811	ValidAcc 0.4804	TestAcc 0.4785	BestValid 0.4804
	Epoch 350:	Loss 1.5755	TrainAcc 0.4712	ValidAcc 0.4693	TestAcc 0.4699	BestValid 0.4804
	Epoch 400:	Loss 1.5724	TrainAcc 0.4787	ValidAcc 0.4796	TestAcc 0.4778	BestValid 0.4804
	Epoch 450:	Loss 1.5537	TrainAcc 0.4813	ValidAcc 0.4808	TestAcc 0.4796	BestValid 0.4808
	Epoch 500:	Loss 1.5461	TrainAcc 0.4823	ValidAcc 0.4817	TestAcc 0.4797	BestValid 0.4817
	Epoch 550:	Loss 1.5328	TrainAcc 0.4812	ValidAcc 0.4778	TestAcc 0.4786	BestValid 0.4817
	Epoch 600:	Loss 1.5357	TrainAcc 0.4856	ValidAcc 0.4811	TestAcc 0.4808	BestValid 0.4817
	Epoch 650:	Loss 1.5283	TrainAcc 0.4870	ValidAcc 0.4864	TestAcc 0.4864	BestValid 0.4864
	Epoch 700:	Loss 1.5278	TrainAcc 0.4906	ValidAcc 0.4870	TestAcc 0.4862	BestValid 0.4870
	Epoch 750:	Loss 1.5181	TrainAcc 0.4841	ValidAcc 0.4804	TestAcc 0.4824	BestValid 0.4870
	Epoch 800:	Loss 1.5193	TrainAcc 0.4924	ValidAcc 0.4866	TestAcc 0.4898	BestValid 0.4870
	Epoch 850:	Loss 1.5079	TrainAcc 0.4961	ValidAcc 0.4904	TestAcc 0.4929	BestValid 0.4904
	Epoch 900:	Loss 1.5040	TrainAcc 0.4932	ValidAcc 0.4909	TestAcc 0.4904	BestValid 0.4909
	Epoch 950:	Loss 1.5020	TrainAcc 0.4874	ValidAcc 0.4813	TestAcc 0.4836	BestValid 0.4909
	Epoch 1000:	Loss 1.4974	TrainAcc 0.4977	ValidAcc 0.4940	TestAcc 0.4946	BestValid 0.4940
	Epoch 1050:	Loss 1.4919	TrainAcc 0.4946	ValidAcc 0.4879	TestAcc 0.4899	BestValid 0.4940
	Epoch 1100:	Loss 1.4910	TrainAcc 0.5005	ValidAcc 0.4974	TestAcc 0.4998	BestValid 0.4974
	Epoch 1150:	Loss 1.4935	TrainAcc 0.4979	ValidAcc 0.4914	TestAcc 0.4938	BestValid 0.4974
	Epoch 1200:	Loss 1.4867	TrainAcc 0.5036	ValidAcc 0.4987	TestAcc 0.5002	BestValid 0.4987
	Epoch 1250:	Loss 1.4879	TrainAcc 0.5050	ValidAcc 0.4972	TestAcc 0.4982	BestValid 0.4987
	Epoch 1300:	Loss 1.4791	TrainAcc 0.5057	ValidAcc 0.4976	TestAcc 0.5004	BestValid 0.4987
	Epoch 1350:	Loss 1.4830	TrainAcc 0.5034	ValidAcc 0.4993	TestAcc 0.5006	BestValid 0.4993
	Epoch 1400:	Loss 1.4951	TrainAcc 0.4964	ValidAcc 0.4900	TestAcc 0.4889	BestValid 0.4993
	Epoch 1450:	Loss 1.4859	TrainAcc 0.5032	ValidAcc 0.4984	TestAcc 0.4976	BestValid 0.4993
	Epoch 1500:	Loss 1.4849	TrainAcc 0.5031	ValidAcc 0.4980	TestAcc 0.4995	BestValid 0.4993
	Epoch 1550:	Loss 1.4808	TrainAcc 0.5040	ValidAcc 0.4942	TestAcc 0.4987	BestValid 0.4993
	Epoch 1600:	Loss 1.4865	TrainAcc 0.5054	ValidAcc 0.5024	TestAcc 0.5035	BestValid 0.5024
	Epoch 1650:	Loss 1.4735	TrainAcc 0.4988	ValidAcc 0.4909	TestAcc 0.4893	BestValid 0.5024
	Epoch 1700:	Loss 1.4808	TrainAcc 0.5090	ValidAcc 0.5013	TestAcc 0.5030	BestValid 0.5024
	Epoch 1750:	Loss 1.4736	TrainAcc 0.5072	ValidAcc 0.4982	TestAcc 0.4981	BestValid 0.5024
	Epoch 1800:	Loss 1.4768	TrainAcc 0.5090	ValidAcc 0.5036	TestAcc 0.5054	BestValid 0.5036
	Epoch 1850:	Loss 1.4688	TrainAcc 0.5065	ValidAcc 0.4970	TestAcc 0.5003	BestValid 0.5036
	Epoch 1900:	Loss 1.4673	TrainAcc 0.5090	ValidAcc 0.5044	TestAcc 0.5047	BestValid 0.5044
	Epoch 1950:	Loss 1.4657	TrainAcc 0.5098	ValidAcc 0.5000	TestAcc 0.5041	BestValid 0.5044
	Epoch 2000:	Loss 1.4628	TrainAcc 0.5019	ValidAcc 0.4948	TestAcc 0.4928	BestValid 0.5044
	Epoch 2050:	Loss 1.4609	TrainAcc 0.5068	ValidAcc 0.5011	TestAcc 0.4996	BestValid 0.5044
	Epoch 2100:	Loss 1.4681	TrainAcc 0.5099	ValidAcc 0.5048	TestAcc 0.5054	BestValid 0.5048
	Epoch 2150:	Loss 1.4706	TrainAcc 0.5066	ValidAcc 0.4973	TestAcc 0.4993	BestValid 0.5048
	Epoch 2200:	Loss 1.4628	TrainAcc 0.5098	ValidAcc 0.5050	TestAcc 0.5066	BestValid 0.5050
	Epoch 2250:	Loss 1.4596	TrainAcc 0.5100	ValidAcc 0.5001	TestAcc 0.5005	BestValid 0.5050
	Epoch 2300:	Loss 1.4623	TrainAcc 0.5130	ValidAcc 0.5052	TestAcc 0.5080	BestValid 0.5052
	Epoch 2350:	Loss 1.4638	TrainAcc 0.5086	ValidAcc 0.4996	TestAcc 0.4963	BestValid 0.5052
	Epoch 2400:	Loss 1.4708	TrainAcc 0.5057	ValidAcc 0.5018	TestAcc 0.5002	BestValid 0.5052
	Epoch 2450:	Loss 1.4639	TrainAcc 0.4969	ValidAcc 0.4857	TestAcc 0.4856	BestValid 0.5052
	Epoch 2500:	Loss 1.4526	TrainAcc 0.5113	ValidAcc 0.5057	TestAcc 0.5079	BestValid 0.5057
	Epoch 2550:	Loss 1.4559	TrainAcc 0.5135	ValidAcc 0.5061	TestAcc 0.5056	BestValid 0.5061
	Epoch 2600:	Loss 1.4568	TrainAcc 0.5158	ValidAcc 0.5037	TestAcc 0.5056	BestValid 0.5061
	Epoch 2650:	Loss 1.4581	TrainAcc 0.5156	ValidAcc 0.5092	TestAcc 0.5084	BestValid 0.5092
	Epoch 2700:	Loss 1.4490	TrainAcc 0.5115	ValidAcc 0.5006	TestAcc 0.4999	BestValid 0.5092
	Epoch 2750:	Loss 1.4562	TrainAcc 0.4953	ValidAcc 0.4876	TestAcc 0.4898	BestValid 0.5092
	Epoch 2800:	Loss 1.4588	TrainAcc 0.5011	ValidAcc 0.4941	TestAcc 0.4921	BestValid 0.5092
	Epoch 2850:	Loss 1.4580	TrainAcc 0.5155	ValidAcc 0.5044	TestAcc 0.5079	BestValid 0.5092
	Epoch 2900:	Loss 1.4687	TrainAcc 0.5015	ValidAcc 0.4925	TestAcc 0.4935	BestValid 0.5092
	Epoch 2950:	Loss 1.4444	TrainAcc 0.5125	ValidAcc 0.5035	TestAcc 0.5015	BestValid 0.5092
	Epoch 3000:	Loss 1.4613	TrainAcc 0.5138	ValidAcc 0.5020	TestAcc 0.5024	BestValid 0.5092
	Epoch 3050:	Loss 1.4502	TrainAcc 0.5143	ValidAcc 0.5076	TestAcc 0.5090	BestValid 0.5092
	Epoch 3100:	Loss 1.4469	TrainAcc 0.5133	ValidAcc 0.5031	TestAcc 0.5009	BestValid 0.5092
	Epoch 3150:	Loss 1.4476	TrainAcc 0.5122	ValidAcc 0.5017	TestAcc 0.5019	BestValid 0.5092
	Epoch 3200:	Loss 1.4403	TrainAcc 0.5183	ValidAcc 0.5070	TestAcc 0.5064	BestValid 0.5092
	Epoch 3250:	Loss 1.4444	TrainAcc 0.5205	ValidAcc 0.5072	TestAcc 0.5094	BestValid 0.5092
	Epoch 3300:	Loss 1.4482	TrainAcc 0.5168	ValidAcc 0.5090	TestAcc 0.5101	BestValid 0.5092
	Epoch 3350:	Loss 1.4477	TrainAcc 0.5137	ValidAcc 0.5003	TestAcc 0.4997	BestValid 0.5092
	Epoch 3400:	Loss 1.4414	TrainAcc 0.5079	ValidAcc 0.5028	TestAcc 0.5002	BestValid 0.5092
	Epoch 3450:	Loss 1.4469	TrainAcc 0.5178	ValidAcc 0.5073	TestAcc 0.5054	BestValid 0.5092
	Epoch 3500:	Loss 1.4404	TrainAcc 0.5114	ValidAcc 0.4998	TestAcc 0.5016	BestValid 0.5092
	Epoch 3550:	Loss 1.4363	TrainAcc 0.5167	ValidAcc 0.5048	TestAcc 0.5034	BestValid 0.5092
	Epoch 3600:	Loss 1.4440	TrainAcc 0.5223	ValidAcc 0.5086	TestAcc 0.5095	BestValid 0.5092
	Epoch 3650:	Loss 1.4464	TrainAcc 0.5039	ValidAcc 0.4963	TestAcc 0.4957	BestValid 0.5092
	Epoch 3700:	Loss 1.4379	TrainAcc 0.5183	ValidAcc 0.5058	TestAcc 0.5050	BestValid 0.5092
	Epoch 3750:	Loss 1.4448	TrainAcc 0.5158	ValidAcc 0.5012	TestAcc 0.5052	BestValid 0.5092
	Epoch 3800:	Loss 1.4300	TrainAcc 0.5223	ValidAcc 0.5110	TestAcc 0.5101	BestValid 0.5110
	Epoch 3850:	Loss 1.4414	TrainAcc 0.5232	ValidAcc 0.5099	TestAcc 0.5107	BestValid 0.5110
	Epoch 3900:	Loss 1.4449	TrainAcc 0.5117	ValidAcc 0.4996	TestAcc 0.5011	BestValid 0.5110
	Epoch 3950:	Loss 1.4296	TrainAcc 0.5143	ValidAcc 0.5011	TestAcc 0.4994	BestValid 0.5110
	Epoch 4000:	Loss 1.4324	TrainAcc 0.5218	ValidAcc 0.5085	TestAcc 0.5120	BestValid 0.5110
	Epoch 4050:	Loss 1.4502	TrainAcc 0.5155	ValidAcc 0.5038	TestAcc 0.5040	BestValid 0.5110
	Epoch 4100:	Loss 1.4338	TrainAcc 0.5169	ValidAcc 0.5017	TestAcc 0.5010	BestValid 0.5110
	Epoch 4150:	Loss 1.4244	TrainAcc 0.5213	ValidAcc 0.5087	TestAcc 0.5109	BestValid 0.5110
	Epoch 4200:	Loss 1.4267	TrainAcc 0.5238	ValidAcc 0.5077	TestAcc 0.5118	BestValid 0.5110
	Epoch 4250:	Loss 1.4306	TrainAcc 0.5232	ValidAcc 0.5117	TestAcc 0.5111	BestValid 0.5117
	Epoch 4300:	Loss 1.4329	TrainAcc 0.5217	ValidAcc 0.5048	TestAcc 0.5085	BestValid 0.5117
	Epoch 4350:	Loss 1.4437	TrainAcc 0.5218	ValidAcc 0.5104	TestAcc 0.5069	BestValid 0.5117
	Epoch 4400:	Loss 1.4372	TrainAcc 0.5089	ValidAcc 0.4970	TestAcc 0.4975	BestValid 0.5117
	Epoch 4450:	Loss 1.4339	TrainAcc 0.5220	ValidAcc 0.5051	TestAcc 0.5055	BestValid 0.5117
	Epoch 4500:	Loss 1.4393	TrainAcc 0.5224	ValidAcc 0.5087	TestAcc 0.5071	BestValid 0.5117
	Epoch 4550:	Loss 1.4253	TrainAcc 0.5136	ValidAcc 0.5001	TestAcc 0.5002	BestValid 0.5117
	Epoch 4600:	Loss 1.4217	TrainAcc 0.5261	ValidAcc 0.5081	TestAcc 0.5090	BestValid 0.5117
	Epoch 4650:	Loss 1.4247	TrainAcc 0.5271	ValidAcc 0.5126	TestAcc 0.5130	BestValid 0.5126
	Epoch 4700:	Loss 1.4217	TrainAcc 0.5219	ValidAcc 0.5044	TestAcc 0.5086	BestValid 0.5126
	Epoch 4750:	Loss 1.4286	TrainAcc 0.5261	ValidAcc 0.5107	TestAcc 0.5096	BestValid 0.5126
	Epoch 4800:	Loss 1.4278	TrainAcc 0.5184	ValidAcc 0.5049	TestAcc 0.5057	BestValid 0.5126
	Epoch 4850:	Loss 1.4276	TrainAcc 0.5231	ValidAcc 0.5061	TestAcc 0.5054	BestValid 0.5126
	Epoch 4900:	Loss 1.4204	TrainAcc 0.5248	ValidAcc 0.5115	TestAcc 0.5121	BestValid 0.5126
	Epoch 4950:	Loss 1.4165	TrainAcc 0.5276	ValidAcc 0.5101	TestAcc 0.5136	BestValid 0.5126
	Epoch 5000:	Loss 1.4162	TrainAcc 0.5297	ValidAcc 0.5115	TestAcc 0.5144	BestValid 0.5126
****** Epoch Time (Excluding Evaluation Cost): 0.093 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.245 ms (Max: 18.240, Min: 15.429, Sum: 137.962)
Cluster-Wide Average, Compute: 50.986 ms (Max: 60.578, Min: 47.349, Sum: 407.884)
Cluster-Wide Average, Communication-Layer: 14.247 ms (Max: 16.698, Min: 10.747, Sum: 113.973)
Cluster-Wide Average, Bubble-Imbalance: 8.679 ms (Max: 12.711, Min: 2.833, Sum: 69.435)
Cluster-Wide Average, Communication-Graph: 0.453 ms (Max: 0.498, Min: 0.394, Sum: 3.626)
Cluster-Wide Average, Optimization: 0.094 ms (Max: 0.110, Min: 0.086, Sum: 0.751)
Cluster-Wide Average, Others: 1.289 ms (Max: 4.049, Min: 0.855, Sum: 10.315)
****** Breakdown Sum: 92.993 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.004 GB (Max: 4.174, Min: 2.821, Sum: 24.030)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 69.205 Gbps (Max: 82.982, Min: 52.188, Sum: 553.641)
Layer-level communication (cluster-wide, per-epoch): 0.931 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.931 GB
****** Accuracy Results ******
Highest valid_acc: 0.5126
Target test_acc: 0.5130
Epoch to reach the target acc: 4649
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
