Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.018 seconds.
        It takes 0.016 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
        It takes 0.018 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.019 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.095 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.110 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.341 Gbps (per GPU), 482.732 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.021 Gbps (per GPU), 480.166 Gbps (aggregated)
The layer-level communication performance: 60.011 Gbps (per GPU), 480.086 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.754 Gbps (per GPU), 478.031 Gbps (aggregated)
The layer-level communication performance: 59.723 Gbps (per GPU), 477.786 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.513 Gbps (per GPU), 476.106 Gbps (aggregated)
The layer-level communication performance: 59.467 Gbps (per GPU), 475.736 Gbps (aggregated)
The layer-level communication performance: 59.433 Gbps (per GPU), 475.466 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.056 Gbps (per GPU), 1248.444 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.070 Gbps (per GPU), 1248.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.056 Gbps (per GPU), 1248.444 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.012 Gbps (per GPU), 1248.096 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.058 Gbps (per GPU), 1248.467 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.056 Gbps (per GPU), 1248.444 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.067 Gbps (per GPU), 1248.537 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.024 Gbps (per GPU), 1248.189 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.941 Gbps (per GPU), 807.529 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.938 Gbps (per GPU), 807.502 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.940 Gbps (per GPU), 807.522 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.941 Gbps (per GPU), 807.529 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.935 Gbps (per GPU), 807.483 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.940 Gbps (per GPU), 807.522 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.937 Gbps (per GPU), 807.496 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.939 Gbps (per GPU), 807.516 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 29.597 Gbps (per GPU), 236.776 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.597 Gbps (per GPU), 236.779 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.595 Gbps (per GPU), 236.763 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.595 Gbps (per GPU), 236.764 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.593 Gbps (per GPU), 236.743 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.594 Gbps (per GPU), 236.749 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.595 Gbps (per GPU), 236.759 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.591 Gbps (per GPU), 236.726 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.42ms  0.36ms  0.52ms  1.46  2.81K  0.03M
 chk_1  0.42ms  0.36ms  0.52ms  1.46  2.82K  0.03M
 chk_2  0.41ms  0.36ms  0.52ms  1.46  2.80K  0.03M
 chk_3  0.42ms  0.36ms  0.52ms  1.46  2.80K  0.03M
 chk_4  0.42ms  0.36ms  0.52ms  1.45  2.82K  0.03M
 chk_5  0.41ms  0.36ms  0.53ms  1.45  2.75K  0.03M
 chk_6  0.43ms  0.35ms  0.52ms  1.46  2.71K  0.03M
 chk_7  0.41ms  0.36ms  0.52ms  1.45  2.76K  0.03M
 chk_8  0.42ms  0.36ms  0.52ms  1.45  2.79K  0.03M
 chk_9  0.47ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_10  0.44ms  0.36ms  0.52ms  1.45  2.81K  0.03M
chk_11  0.43ms  0.36ms  0.53ms  1.45  2.74K  0.03M
chk_12  0.41ms  0.36ms  0.52ms  1.45  2.76K  0.03M
chk_13  0.41ms  0.36ms  0.52ms  1.45  2.75K  0.03M
chk_14  0.41ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_15  0.41ms  0.36ms  0.52ms  1.46  2.77K  0.03M
chk_16  0.41ms  0.36ms  0.52ms  1.46  2.78K  0.03M
chk_17  0.41ms  0.36ms  0.53ms  1.46  2.79K  0.03M
chk_18  0.42ms  0.36ms  0.53ms  1.45  2.82K  0.03M
chk_19  0.41ms  0.35ms  0.52ms  1.46  2.81K  0.03M
chk_20  0.41ms  0.36ms  0.53ms  1.46  2.77K  0.03M
chk_21  0.42ms  0.36ms  0.52ms  1.45  2.84K  0.02M
chk_22  0.41ms  0.36ms  0.52ms  1.46  2.78K  0.03M
chk_23  0.41ms  0.36ms  0.52ms  1.45  2.80K  0.03M
chk_24  0.41ms  0.36ms  0.52ms  1.46  2.80K  0.03M
chk_25  0.42ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_26  0.42ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_27  0.41ms  0.37ms  0.53ms  1.44  2.79K  0.03M
chk_28  0.41ms  0.36ms  0.52ms  1.45  2.77K  0.03M
chk_29  0.41ms  0.36ms  0.52ms  1.47  2.77K  0.03M
chk_30  0.42ms  0.36ms  0.53ms  1.46  2.80K  0.03M
chk_31  0.41ms  0.36ms  0.53ms  1.45  2.78K  0.03M
   Avg  0.42  0.36  0.52
   Max  0.47  0.37  0.53
   Min  0.41  0.35  0.52
 Ratio  1.13  1.03  1.02
   Var  0.00  0.00  0.00
Profiling takes 0.619 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 57.485 ms
Partition 0 [0, 4) has cost: 47.859 ms
Partition 1 [4, 8) has cost: 45.988 ms
Partition 2 [8, 12) has cost: 45.988 ms
Partition 3 [12, 16) has cost: 45.988 ms
Partition 4 [16, 20) has cost: 45.988 ms
Partition 5 [20, 24) has cost: 45.988 ms
Partition 6 [24, 29) has cost: 57.485 ms
Partition 7 [29, 33) has cost: 51.226 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 29.121 ms
GPU 0, Compute+Comm Time: 20.839 ms, Bubble Time: 5.281 ms, Imbalance Overhead: 3.001 ms
GPU 1, Compute+Comm Time: 20.570 ms, Bubble Time: 5.173 ms, Imbalance Overhead: 3.377 ms
GPU 2, Compute+Comm Time: 20.570 ms, Bubble Time: 5.055 ms, Imbalance Overhead: 3.496 ms
GPU 3, Compute+Comm Time: 20.570 ms, Bubble Time: 4.928 ms, Imbalance Overhead: 3.622 ms
GPU 4, Compute+Comm Time: 20.570 ms, Bubble Time: 4.819 ms, Imbalance Overhead: 3.731 ms
GPU 5, Compute+Comm Time: 20.570 ms, Bubble Time: 4.707 ms, Imbalance Overhead: 3.843 ms
GPU 6, Compute+Comm Time: 24.530 ms, Bubble Time: 4.591 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 21.562 ms, Bubble Time: 4.684 ms, Imbalance Overhead: 2.875 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.480 ms
GPU 0, Compute+Comm Time: 39.131 ms, Bubble Time: 8.142 ms, Imbalance Overhead: 3.207 ms
GPU 1, Compute+Comm Time: 42.421 ms, Bubble Time: 8.059 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 34.884 ms, Bubble Time: 8.252 ms, Imbalance Overhead: 7.344 ms
GPU 3, Compute+Comm Time: 34.884 ms, Bubble Time: 8.440 ms, Imbalance Overhead: 7.156 ms
GPU 4, Compute+Comm Time: 34.884 ms, Bubble Time: 8.612 ms, Imbalance Overhead: 6.984 ms
GPU 5, Compute+Comm Time: 34.884 ms, Bubble Time: 8.805 ms, Imbalance Overhead: 6.792 ms
GPU 6, Compute+Comm Time: 34.884 ms, Bubble Time: 9.016 ms, Imbalance Overhead: 6.581 ms
GPU 7, Compute+Comm Time: 36.486 ms, Bubble Time: 9.212 ms, Imbalance Overhead: 4.782 ms
The estimated cost of the whole pipeline: 83.581 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 103.473 ms
Partition 0 [0, 8) has cost: 93.847 ms
Partition 1 [8, 16) has cost: 91.976 ms
Partition 2 [16, 25) has cost: 103.473 ms
Partition 3 [25, 33) has cost: 97.214 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.343 ms
GPU 0, Compute+Comm Time: 24.011 ms, Bubble Time: 4.892 ms, Imbalance Overhead: 2.440 ms
GPU 1, Compute+Comm Time: 24.324 ms, Bubble Time: 4.724 ms, Imbalance Overhead: 2.294 ms
GPU 2, Compute+Comm Time: 26.773 ms, Bubble Time: 4.570 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 24.820 ms, Bubble Time: 4.691 ms, Imbalance Overhead: 1.832 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.388 ms
GPU 0, Compute+Comm Time: 40.828 ms, Bubble Time: 7.554 ms, Imbalance Overhead: 2.005 ms
GPU 1, Compute+Comm Time: 42.951 ms, Bubble Time: 7.437 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 38.704 ms, Bubble Time: 7.666 ms, Imbalance Overhead: 4.018 ms
GPU 3, Compute+Comm Time: 39.077 ms, Bubble Time: 7.923 ms, Imbalance Overhead: 3.388 ms
    The estimated cost with 2 DP ways is 85.818 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 197.320 ms
Partition 0 [0, 17) has cost: 197.320 ms
Partition 1 [17, 33) has cost: 189.190 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 43.710 ms
GPU 0, Compute+Comm Time: 38.941 ms, Bubble Time: 4.769 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.108 ms, Bubble Time: 4.848 ms, Imbalance Overhead: 0.754 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.294 ms
GPU 0, Compute+Comm Time: 53.362 ms, Bubble Time: 6.793 ms, Imbalance Overhead: 1.139 ms
GPU 1, Compute+Comm Time: 54.646 ms, Bubble Time: 6.649 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 110.255 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 386.509 ms
Partition 0 [0, 33) has cost: 386.509 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 153.488 ms
GPU 0, Compute+Comm Time: 153.488 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 169.018 ms
GPU 0, Compute+Comm Time: 169.018 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 338.632 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9415	TrainAcc 0.4155	ValidAcc 0.4189	TestAcc 0.4181	BestValid 0.4189
	Epoch 50:	Loss 1.9165	TrainAcc 0.4217	ValidAcc 0.4239	TestAcc 0.4232	BestValid 0.4239
	Epoch 100:	Loss 1.7662	TrainAcc 0.4258	ValidAcc 0.4278	TestAcc 0.4281	BestValid 0.4278
	Epoch 150:	Loss 1.7013	TrainAcc 0.4220	ValidAcc 0.4249	TestAcc 0.4240	BestValid 0.4278
	Epoch 200:	Loss 1.8429	TrainAcc 0.4225	ValidAcc 0.4256	TestAcc 0.4243	BestValid 0.4278
	Epoch 250:	Loss 1.7298	TrainAcc 0.2913	ValidAcc 0.2894	TestAcc 0.2855	BestValid 0.4278
	Epoch 300:	Loss 1.6959	TrainAcc 0.4467	ValidAcc 0.4461	TestAcc 0.4466	BestValid 0.4461
	Epoch 350:	Loss 1.6738	TrainAcc 0.4316	ValidAcc 0.4347	TestAcc 0.4349	BestValid 0.4461
	Epoch 400:	Loss 1.6607	TrainAcc 0.4379	ValidAcc 0.4397	TestAcc 0.4399	BestValid 0.4461
	Epoch 450:	Loss 1.6908	TrainAcc 0.4541	ValidAcc 0.4528	TestAcc 0.4542	BestValid 0.4528
	Epoch 500:	Loss 1.6564	TrainAcc 0.4541	ValidAcc 0.4533	TestAcc 0.4531	BestValid 0.4533
	Epoch 550:	Loss 1.6747	TrainAcc 0.4392	ValidAcc 0.4398	TestAcc 0.4410	BestValid 0.4533
	Epoch 600:	Loss 1.6633	TrainAcc 0.4534	ValidAcc 0.4544	TestAcc 0.4505	BestValid 0.4544
	Epoch 650:	Loss 1.6635	TrainAcc 0.4513	ValidAcc 0.4537	TestAcc 0.4518	BestValid 0.4544
	Epoch 700:	Loss 1.6875	TrainAcc 0.4429	ValidAcc 0.4421	TestAcc 0.4444	BestValid 0.4544
	Epoch 750:	Loss 1.6391	TrainAcc 0.4538	ValidAcc 0.4568	TestAcc 0.4532	BestValid 0.4568
	Epoch 800:	Loss 1.6743	TrainAcc 0.4086	ValidAcc 0.4080	TestAcc 0.4056	BestValid 0.4568
	Epoch 850:	Loss 1.6475	TrainAcc 0.4574	ValidAcc 0.4552	TestAcc 0.4584	BestValid 0.4568
	Epoch 900:	Loss 1.6301	TrainAcc 0.4672	ValidAcc 0.4677	TestAcc 0.4673	BestValid 0.4677
	Epoch 950:	Loss 1.6614	TrainAcc 0.3932	ValidAcc 0.3928	TestAcc 0.3880	BestValid 0.4677
	Epoch 1000:	Loss 1.6234	TrainAcc 0.4617	ValidAcc 0.4643	TestAcc 0.4624	BestValid 0.4677
	Epoch 1050:	Loss 1.6364	TrainAcc 0.4572	ValidAcc 0.4554	TestAcc 0.4571	BestValid 0.4677
	Epoch 1100:	Loss 1.6385	TrainAcc 0.4126	ValidAcc 0.4129	TestAcc 0.4075	BestValid 0.4677
	Epoch 1150:	Loss 1.6417	TrainAcc 0.4230	ValidAcc 0.4228	TestAcc 0.4202	BestValid 0.4677
	Epoch 1200:	Loss 1.6461	TrainAcc 0.4692	ValidAcc 0.4696	TestAcc 0.4702	BestValid 0.4696
	Epoch 1250:	Loss 1.6107	TrainAcc 0.4762	ValidAcc 0.4776	TestAcc 0.4771	BestValid 0.4776
	Epoch 1300:	Loss 1.6575	TrainAcc 0.3901	ValidAcc 0.3864	TestAcc 0.3845	BestValid 0.4776
	Epoch 1350:	Loss 1.6124	TrainAcc 0.4729	ValidAcc 0.4732	TestAcc 0.4719	BestValid 0.4776
	Epoch 1400:	Loss 1.6123	TrainAcc 0.4684	ValidAcc 0.4677	TestAcc 0.4686	BestValid 0.4776
	Epoch 1450:	Loss 1.6248	TrainAcc 0.4568	ValidAcc 0.4569	TestAcc 0.4548	BestValid 0.4776
	Epoch 1500:	Loss 1.6043	TrainAcc 0.4461	ValidAcc 0.4474	TestAcc 0.4429	BestValid 0.4776
	Epoch 1550:	Loss 1.6018	TrainAcc 0.4836	ValidAcc 0.4837	TestAcc 0.4850	BestValid 0.4837
	Epoch 1600:	Loss 1.5857	TrainAcc 0.4861	ValidAcc 0.4870	TestAcc 0.4868	BestValid 0.4870
	Epoch 1650:	Loss 1.5858	TrainAcc 0.4882	ValidAcc 0.4874	TestAcc 0.4883	BestValid 0.4874
	Epoch 1700:	Loss 1.5847	TrainAcc 0.4783	ValidAcc 0.4792	TestAcc 0.4788	BestValid 0.4874
	Epoch 1750:	Loss 1.5742	TrainAcc 0.4473	ValidAcc 0.4465	TestAcc 0.4441	BestValid 0.4874
	Epoch 1800:	Loss 1.5652	TrainAcc 0.4893	ValidAcc 0.4890	TestAcc 0.4896	BestValid 0.4890
	Epoch 1850:	Loss 1.5598	TrainAcc 0.4917	ValidAcc 0.4917	TestAcc 0.4927	BestValid 0.4917
	Epoch 1900:	Loss 1.5536	TrainAcc 0.4845	ValidAcc 0.4830	TestAcc 0.4841	BestValid 0.4917
	Epoch 1950:	Loss 1.5463	TrainAcc 0.4861	ValidAcc 0.4839	TestAcc 0.4840	BestValid 0.4917
	Epoch 2000:	Loss 1.5548	TrainAcc 0.4439	ValidAcc 0.4408	TestAcc 0.4397	BestValid 0.4917
	Epoch 2050:	Loss 1.5448	TrainAcc 0.4436	ValidAcc 0.4423	TestAcc 0.4404	BestValid 0.4917
	Epoch 2100:	Loss 1.5349	TrainAcc 0.4746	ValidAcc 0.4721	TestAcc 0.4720	BestValid 0.4917
	Epoch 2150:	Loss 1.5377	TrainAcc 0.4961	ValidAcc 0.4960	TestAcc 0.4959	BestValid 0.4960
	Epoch 2200:	Loss 1.5253	TrainAcc 0.4968	ValidAcc 0.4962	TestAcc 0.4957	BestValid 0.4962
	Epoch 2250:	Loss 1.5252	TrainAcc 0.4958	ValidAcc 0.4952	TestAcc 0.4959	BestValid 0.4962
	Epoch 2300:	Loss 1.5220	TrainAcc 0.4894	ValidAcc 0.4863	TestAcc 0.4881	BestValid 0.4962
	Epoch 2350:	Loss 1.5306	TrainAcc 0.4543	ValidAcc 0.4509	TestAcc 0.4496	BestValid 0.4962
	Epoch 2400:	Loss 1.5107	TrainAcc 0.4474	ValidAcc 0.4449	TestAcc 0.4423	BestValid 0.4962
	Epoch 2450:	Loss 1.5167	TrainAcc 0.4950	ValidAcc 0.4945	TestAcc 0.4942	BestValid 0.4962
	Epoch 2500:	Loss 1.5183	TrainAcc 0.5019	ValidAcc 0.4989	TestAcc 0.5002	BestValid 0.4989
	Epoch 2550:	Loss 1.5106	TrainAcc 0.4998	ValidAcc 0.4956	TestAcc 0.4986	BestValid 0.4989
	Epoch 2600:	Loss 1.5263	TrainAcc 0.4321	ValidAcc 0.4302	TestAcc 0.4256	BestValid 0.4989
	Epoch 2650:	Loss 1.5121	TrainAcc 0.4872	ValidAcc 0.4836	TestAcc 0.4836	BestValid 0.4989
	Epoch 2700:	Loss 1.5010	TrainAcc 0.4976	ValidAcc 0.4944	TestAcc 0.4958	BestValid 0.4989
	Epoch 2750:	Loss 1.5054	TrainAcc 0.4582	ValidAcc 0.4571	TestAcc 0.4552	BestValid 0.4989
	Epoch 2800:	Loss 1.5034	TrainAcc 0.4866	ValidAcc 0.4807	TestAcc 0.4803	BestValid 0.4989
	Epoch 2850:	Loss 1.4930	TrainAcc 0.4910	ValidAcc 0.4853	TestAcc 0.4861	BestValid 0.4989
	Epoch 2900:	Loss 1.4941	TrainAcc 0.4928	ValidAcc 0.4889	TestAcc 0.4895	BestValid 0.4989
	Epoch 2950:	Loss 1.5039	TrainAcc 0.4593	ValidAcc 0.4559	TestAcc 0.4559	BestValid 0.4989
	Epoch 3000:	Loss 1.5025	TrainAcc 0.4430	ValidAcc 0.4385	TestAcc 0.4377	BestValid 0.4989
	Epoch 3050:	Loss 1.4981	TrainAcc 0.4941	ValidAcc 0.4894	TestAcc 0.4900	BestValid 0.4989
	Epoch 3100:	Loss 1.5020	TrainAcc 0.5039	ValidAcc 0.5021	TestAcc 0.5023	BestValid 0.5021
	Epoch 3150:	Loss 1.4932	TrainAcc 0.4719	ValidAcc 0.4680	TestAcc 0.4665	BestValid 0.5021
	Epoch 3200:	Loss 1.4968	TrainAcc 0.4442	ValidAcc 0.4381	TestAcc 0.4373	BestValid 0.5021
	Epoch 3250:	Loss 1.4838	TrainAcc 0.4748	ValidAcc 0.4704	TestAcc 0.4689	BestValid 0.5021
	Epoch 3300:	Loss 1.4939	TrainAcc 0.5071	ValidAcc 0.5006	TestAcc 0.5032	BestValid 0.5021
	Epoch 3350:	Loss 1.5088	TrainAcc 0.4316	ValidAcc 0.4292	TestAcc 0.4266	BestValid 0.5021
	Epoch 3400:	Loss 1.4996	TrainAcc 0.4951	ValidAcc 0.4890	TestAcc 0.4912	BestValid 0.5021
	Epoch 3450:	Loss 1.4879	TrainAcc 0.4943	ValidAcc 0.4895	TestAcc 0.4902	BestValid 0.5021
	Epoch 3500:	Loss 1.4905	TrainAcc 0.4536	ValidAcc 0.4483	TestAcc 0.4472	BestValid 0.5021
	Epoch 3550:	Loss 1.4758	TrainAcc 0.4625	ValidAcc 0.4553	TestAcc 0.4551	BestValid 0.5021
	Epoch 3600:	Loss 1.4879	TrainAcc 0.5082	ValidAcc 0.5012	TestAcc 0.5044	BestValid 0.5021
	Epoch 3650:	Loss 1.4821	TrainAcc 0.4400	ValidAcc 0.4349	TestAcc 0.4332	BestValid 0.5021
	Epoch 3700:	Loss 1.4859	TrainAcc 0.4855	ValidAcc 0.4801	TestAcc 0.4783	BestValid 0.5021
	Epoch 3750:	Loss 1.5063	TrainAcc 0.5060	ValidAcc 0.5004	TestAcc 0.5009	BestValid 0.5021
	Epoch 3800:	Loss 1.5003	TrainAcc 0.4125	ValidAcc 0.4061	TestAcc 0.4042	BestValid 0.5021
	Epoch 3850:	Loss 1.4761	TrainAcc 0.5042	ValidAcc 0.4981	TestAcc 0.4978	BestValid 0.5021
	Epoch 3900:	Loss 1.4838	TrainAcc 0.5026	ValidAcc 0.4961	TestAcc 0.4975	BestValid 0.5021
	Epoch 3950:	Loss 1.4953	TrainAcc 0.4286	ValidAcc 0.4222	TestAcc 0.4206	BestValid 0.5021
	Epoch 4000:	Loss 1.4833	TrainAcc 0.5079	ValidAcc 0.5019	TestAcc 0.5032	BestValid 0.5021
	Epoch 4050:	Loss 1.4758	TrainAcc 0.4815	ValidAcc 0.4779	TestAcc 0.4746	BestValid 0.5021
	Epoch 4100:	Loss 1.4956	TrainAcc 0.4355	ValidAcc 0.4316	TestAcc 0.4276	BestValid 0.5021
	Epoch 4150:	Loss 1.4730	TrainAcc 0.5094	ValidAcc 0.5032	TestAcc 0.5061	BestValid 0.5032
	Epoch 4200:	Loss 1.4721	TrainAcc 0.4998	ValidAcc 0.4930	TestAcc 0.4946	BestValid 0.5032
	Epoch 4250:	Loss 1.4839	TrainAcc 0.4561	ValidAcc 0.4471	TestAcc 0.4464	BestValid 0.5032
	Epoch 4300:	Loss 1.4791	TrainAcc 0.5072	ValidAcc 0.4996	TestAcc 0.5029	BestValid 0.5032
	Epoch 4350:	Loss 1.4750	TrainAcc 0.4481	ValidAcc 0.4394	TestAcc 0.4397	BestValid 0.5032
	Epoch 4400:	Loss 1.4663	TrainAcc 0.4971	ValidAcc 0.4892	TestAcc 0.4903	BestValid 0.5032
	Epoch 4450:	Loss 1.4866	TrainAcc 0.5092	ValidAcc 0.5055	TestAcc 0.5057	BestValid 0.5055
	Epoch 4500:	Loss 1.4808	TrainAcc 0.4452	ValidAcc 0.4396	TestAcc 0.4371	BestValid 0.5055
	Epoch 4550:	Loss 1.4710	TrainAcc 0.5109	ValidAcc 0.5039	TestAcc 0.5061	BestValid 0.5055
	Epoch 4600:	Loss 1.4643	TrainAcc 0.4850	ValidAcc 0.4787	TestAcc 0.4790	BestValid 0.5055
	Epoch 4650:	Loss 1.4750	TrainAcc 0.4641	ValidAcc 0.4554	TestAcc 0.4559	BestValid 0.5055
	Epoch 4700:	Loss 1.4697	TrainAcc 0.5111	ValidAcc 0.5034	TestAcc 0.5053	BestValid 0.5055
	Epoch 4750:	Loss 1.4664	TrainAcc 0.4438	ValidAcc 0.4378	TestAcc 0.4352	BestValid 0.5055
	Epoch 4800:	Loss 1.4617	TrainAcc 0.5021	ValidAcc 0.4933	TestAcc 0.4951	BestValid 0.5055
	Epoch 4850:	Loss 1.4697	TrainAcc 0.5099	ValidAcc 0.5060	TestAcc 0.5060	BestValid 0.5060
	Epoch 4900:	Loss 1.4728	TrainAcc 0.4487	ValidAcc 0.4409	TestAcc 0.4398	BestValid 0.5060
	Epoch 4950:	Loss 1.4623	TrainAcc 0.5047	ValidAcc 0.4952	TestAcc 0.4993	BestValid 0.5060
	Epoch 5000:	Loss 1.4601	TrainAcc 0.5072	ValidAcc 0.4999	TestAcc 0.4999	BestValid 0.5060
****** Epoch Time (Excluding Evaluation Cost): 0.092 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.253 ms (Max: 18.277, Min: 15.477, Sum: 138.026)
Cluster-Wide Average, Compute: 50.143 ms (Max: 61.213, Min: 46.573, Sum: 401.141)
Cluster-Wide Average, Communication-Layer: 14.277 ms (Max: 16.625, Min: 10.863, Sum: 114.219)
Cluster-Wide Average, Bubble-Imbalance: 9.087 ms (Max: 11.954, Min: 1.882, Sum: 72.698)
Cluster-Wide Average, Communication-Graph: 0.464 ms (Max: 0.496, Min: 0.399, Sum: 3.710)
Cluster-Wide Average, Optimization: 0.094 ms (Max: 0.113, Min: 0.088, Sum: 0.751)
Cluster-Wide Average, Others: 1.069 ms (Max: 3.878, Min: 0.642, Sum: 8.550)
****** Breakdown Sum: 92.387 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.004 GB (Max: 4.174, Min: 2.821, Sum: 24.030)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 69.041 Gbps (Max: 82.715, Min: 51.707, Sum: 552.328)
Layer-level communication (cluster-wide, per-epoch): 0.931 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.931 GB
****** Accuracy Results ******
Highest valid_acc: 0.5060
Target test_acc: 0.5060
Epoch to reach the target acc: 4849
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
