Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.883 seconds.
Building the CSC structure...
        It takes 1.914 seconds.
Building the CSC structure...
        It takes 1.980 seconds.
Building the CSC structure...
        It takes 2.062 seconds.
Building the CSC structure...
        It takes 2.361 seconds.
Building the CSC structure...
        It takes 2.374 seconds.
Building the CSC structure...
        It takes 2.432 seconds.
Building the CSC structure...
        It takes 2.645 seconds.
Building the CSC structure...
        It takes 1.785 seconds.
        It takes 1.845 seconds.
        It takes 1.834 seconds.
        It takes 1.903 seconds.
        It takes 2.178 seconds.
Building the Feature Vector...
        It takes 2.346 seconds.
Building the Feature Vector...
        It takes 2.337 seconds.
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 2.293 seconds.
        It takes 0.031 seconds.
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
        It takes 0.253 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.241 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 54.912 Gbps (per GPU), 439.297 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.681 Gbps (per GPU), 437.446 Gbps (aggregated)
The layer-level communication performance: 54.678 Gbps (per GPU), 437.426 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.480 Gbps (per GPU), 435.839 Gbps (aggregated)
The layer-level communication performance: 54.456 Gbps (per GPU), 435.650 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.274 Gbps (per GPU), 434.193 Gbps (aggregated)
The layer-level communication performance: 54.236 Gbps (per GPU), 433.887 Gbps (aggregated)
The layer-level communication performance: 54.208 Gbps (per GPU), 433.664 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.802 Gbps (per GPU), 1254.418 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.647 Gbps (per GPU), 1253.177 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.673 Gbps (per GPU), 1253.386 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.597 Gbps (per GPU), 1252.779 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.670 Gbps (per GPU), 1253.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.647 Gbps (per GPU), 1253.177 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.670 Gbps (per GPU), 1253.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.650 Gbps (per GPU), 1253.200 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.243 Gbps (per GPU), 801.944 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.245 Gbps (per GPU), 801.957 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.242 Gbps (per GPU), 801.938 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.245 Gbps (per GPU), 801.957 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.243 Gbps (per GPU), 801.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.245 Gbps (per GPU), 801.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.243 Gbps (per GPU), 801.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.246 Gbps (per GPU), 801.970 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.822 Gbps (per GPU), 262.576 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.822 Gbps (per GPU), 262.575 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.821 Gbps (per GPU), 262.571 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.820 Gbps (per GPU), 262.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.822 Gbps (per GPU), 262.575 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.822 Gbps (per GPU), 262.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.821 Gbps (per GPU), 262.569 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.821 Gbps (per GPU), 262.567 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.40ms  2.71ms  3.04  8.38K  3.53M
 chk_1  0.76ms  2.77ms  2.91ms  3.85  6.74K  3.60M
 chk_2  0.80ms  2.62ms  2.79ms  3.51  7.27K  3.53M
 chk_3  0.81ms  2.65ms  2.82ms  3.50  7.92K  3.61M
 chk_4  0.63ms  2.58ms  2.74ms  4.34  5.33K  3.68M
 chk_5  1.06ms  2.58ms  3.02ms  2.84 10.07K  3.45M
 chk_6  1.03ms  2.74ms  2.92ms  2.82  9.41K  3.48M
 chk_7  0.82ms  2.58ms  2.75ms  3.36  8.12K  3.60M
 chk_8  0.68ms  2.69ms  2.89ms  4.25  6.09K  3.64M
 chk_9  1.10ms  2.51ms  2.70ms  2.46 11.10K  3.38M
chk_10  0.65ms  2.72ms  2.88ms  4.43  5.67K  3.63M
chk_11  0.82ms  2.59ms  2.75ms  3.35  8.16K  3.54M
chk_12  0.80ms  2.78ms  2.95ms  3.71  7.24K  3.55M
chk_13  0.64ms  2.61ms  2.77ms  4.36  5.41K  3.68M
chk_14  0.78ms  2.85ms  3.04ms  3.90  7.14K  3.53M
chk_15  0.95ms  2.71ms  2.91ms  3.05  9.25K  3.49M
chk_16  0.59ms  2.55ms  2.70ms  4.54  4.78K  3.77M
chk_17  0.76ms  2.67ms  2.84ms  3.73  6.85K  3.60M
chk_18  0.81ms  2.48ms  2.66ms  3.29  7.47K  3.57M
chk_19  0.61ms  2.55ms  2.70ms  4.45  4.88K  3.75M
chk_20  0.77ms  2.56ms  2.72ms  3.53  7.00K  3.63M
chk_21  0.64ms  2.55ms  2.71ms  4.27  5.41K  3.68M
chk_22  1.10ms  2.76ms  2.97ms  2.71 11.07K  3.39M
chk_23  0.79ms  2.64ms  2.82ms  3.55  7.23K  3.64M
chk_24  1.01ms  2.71ms  2.89ms  2.86 10.13K  3.43M
chk_25  0.73ms  2.50ms  2.67ms  3.67  6.40K  3.57M
chk_26  0.66ms  2.72ms  2.88ms  4.36  5.78K  3.55M
chk_27  0.96ms  2.58ms  2.78ms  2.91  9.34K  3.48M
chk_28  0.73ms  2.89ms  3.06ms  4.21  6.37K  3.57M
chk_29  0.63ms  2.68ms  2.84ms  4.49  5.16K  3.78M
chk_30  0.64ms  2.60ms  2.73ms  4.30  5.44K  3.67M
chk_31  0.72ms  2.72ms  2.89ms  3.99  6.33K  3.63M
   Avg  0.79  2.64  2.83
   Max  1.10  2.89  3.06
   Min  0.59  2.40  2.66
 Ratio  1.84  1.20  1.15
   Var  0.02  0.01  0.01
Profiling takes 2.403 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 363.640 ms
Partition 0 [0, 5) has cost: 363.640 ms
Partition 1 [5, 9) has cost: 338.285 ms
Partition 2 [9, 13) has cost: 338.285 ms
Partition 3 [13, 17) has cost: 338.285 ms
Partition 4 [17, 21) has cost: 338.285 ms
Partition 5 [21, 25) has cost: 338.285 ms
Partition 6 [25, 29) has cost: 338.285 ms
Partition 7 [29, 33) has cost: 344.139 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.079 ms
GPU 0, Compute+Comm Time: 132.920 ms, Bubble Time: 28.947 ms, Imbalance Overhead: 3.212 ms
GPU 1, Compute+Comm Time: 125.843 ms, Bubble Time: 28.651 ms, Imbalance Overhead: 10.584 ms
GPU 2, Compute+Comm Time: 125.843 ms, Bubble Time: 28.705 ms, Imbalance Overhead: 10.531 ms
GPU 3, Compute+Comm Time: 125.843 ms, Bubble Time: 28.597 ms, Imbalance Overhead: 10.639 ms
GPU 4, Compute+Comm Time: 125.843 ms, Bubble Time: 28.503 ms, Imbalance Overhead: 10.732 ms
GPU 5, Compute+Comm Time: 125.843 ms, Bubble Time: 28.585 ms, Imbalance Overhead: 10.651 ms
GPU 6, Compute+Comm Time: 125.843 ms, Bubble Time: 28.853 ms, Imbalance Overhead: 10.382 ms
GPU 7, Compute+Comm Time: 127.296 ms, Bubble Time: 29.281 ms, Imbalance Overhead: 8.501 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 317.453 ms
GPU 0, Compute+Comm Time: 243.994 ms, Bubble Time: 56.796 ms, Imbalance Overhead: 16.662 ms
GPU 1, Compute+Comm Time: 239.593 ms, Bubble Time: 55.937 ms, Imbalance Overhead: 21.922 ms
GPU 2, Compute+Comm Time: 239.593 ms, Bubble Time: 55.282 ms, Imbalance Overhead: 22.577 ms
GPU 3, Compute+Comm Time: 239.593 ms, Bubble Time: 55.142 ms, Imbalance Overhead: 22.717 ms
GPU 4, Compute+Comm Time: 239.593 ms, Bubble Time: 55.191 ms, Imbalance Overhead: 22.669 ms
GPU 5, Compute+Comm Time: 239.593 ms, Bubble Time: 55.277 ms, Imbalance Overhead: 22.583 ms
GPU 6, Compute+Comm Time: 239.593 ms, Bubble Time: 55.040 ms, Imbalance Overhead: 22.820 ms
GPU 7, Compute+Comm Time: 257.873 ms, Bubble Time: 55.580 ms, Imbalance Overhead: 4.001 ms
The estimated cost of the whole pipeline: 506.658 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 701.925 ms
Partition 0 [0, 9) has cost: 701.925 ms
Partition 1 [9, 17) has cost: 676.569 ms
Partition 2 [17, 25) has cost: 676.569 ms
Partition 3 [25, 33) has cost: 682.423 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 171.515 ms
GPU 0, Compute+Comm Time: 141.877 ms, Bubble Time: 26.761 ms, Imbalance Overhead: 2.876 ms
GPU 1, Compute+Comm Time: 138.060 ms, Bubble Time: 26.389 ms, Imbalance Overhead: 7.066 ms
GPU 2, Compute+Comm Time: 138.060 ms, Bubble Time: 26.375 ms, Imbalance Overhead: 7.080 ms
GPU 3, Compute+Comm Time: 138.898 ms, Bubble Time: 26.142 ms, Imbalance Overhead: 6.475 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 316.756 ms
GPU 0, Compute+Comm Time: 255.710 ms, Bubble Time: 48.527 ms, Imbalance Overhead: 12.519 ms
GPU 1, Compute+Comm Time: 253.508 ms, Bubble Time: 48.754 ms, Imbalance Overhead: 14.495 ms
GPU 2, Compute+Comm Time: 253.508 ms, Bubble Time: 48.771 ms, Imbalance Overhead: 14.477 ms
GPU 3, Compute+Comm Time: 263.609 ms, Bubble Time: 49.651 ms, Imbalance Overhead: 3.497 ms
    The estimated cost with 2 DP ways is 512.685 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1378.494 ms
Partition 0 [0, 17) has cost: 1378.494 ms
Partition 1 [17, 33) has cost: 1358.992 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 219.112 ms
GPU 0, Compute+Comm Time: 190.526 ms, Bubble Time: 23.392 ms, Imbalance Overhead: 5.194 ms
GPU 1, Compute+Comm Time: 188.931 ms, Bubble Time: 24.040 ms, Imbalance Overhead: 6.141 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 356.158 ms
GPU 0, Compute+Comm Time: 307.899 ms, Bubble Time: 39.256 ms, Imbalance Overhead: 9.003 ms
GPU 1, Compute+Comm Time: 312.530 ms, Bubble Time: 38.158 ms, Imbalance Overhead: 5.470 ms
    The estimated cost with 4 DP ways is 604.033 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2737.486 ms
Partition 0 [0, 33) has cost: 2737.486 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 575.769 ms
GPU 0, Compute+Comm Time: 575.769 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 698.612 ms
GPU 0, Compute+Comm Time: 698.612 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1338.101 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 3, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7787	TrainAcc 0.0846	ValidAcc 0.0762	TestAcc 0.0771	BestValid 0.0762
	Epoch 50:	Loss 1.8049	TrainAcc 0.6654	ValidAcc 0.6860	TestAcc 0.6819	BestValid 0.6860
	Epoch 100:	Loss 1.2517	TrainAcc 0.7823	ValidAcc 0.7960	TestAcc 0.7910	BestValid 0.7960
	Epoch 150:	Loss 1.0221	TrainAcc 0.8292	ValidAcc 0.8397	TestAcc 0.8348	BestValid 0.8397
	Epoch 200:	Loss 0.8929	TrainAcc 0.8552	ValidAcc 0.8654	TestAcc 0.8577	BestValid 0.8654
	Epoch 250:	Loss 0.8276	TrainAcc 0.8545	ValidAcc 0.8611	TestAcc 0.8571	BestValid 0.8654
	Epoch 300:	Loss 0.8348	TrainAcc 0.8675	ValidAcc 0.8725	TestAcc 0.8670	BestValid 0.8725
	Epoch 350:	Loss 0.7450	TrainAcc 0.8730	ValidAcc 0.8791	TestAcc 0.8757	BestValid 0.8791
	Epoch 400:	Loss 0.7137	TrainAcc 0.8837	ValidAcc 0.8910	TestAcc 0.8852	BestValid 0.8910
	Epoch 450:	Loss 0.6818	TrainAcc 0.8927	ValidAcc 0.8982	TestAcc 0.8934	BestValid 0.8982
	Epoch 500:	Loss 0.6614	TrainAcc 0.8981	ValidAcc 0.9042	TestAcc 0.8977	BestValid 0.9042
	Epoch 550:	Loss 0.6507	TrainAcc 0.8929	ValidAcc 0.8979	TestAcc 0.8941	BestValid 0.9042
	Epoch 600:	Loss 0.6314	TrainAcc 0.9005	ValidAcc 0.9054	TestAcc 0.8996	BestValid 0.9054
	Epoch 650:	Loss 0.6625	TrainAcc 0.8919	ValidAcc 0.8977	TestAcc 0.8925	BestValid 0.9054
	Epoch 700:	Loss 0.6294	TrainAcc 0.8967	ValidAcc 0.9023	TestAcc 0.8964	BestValid 0.9054
	Epoch 750:	Loss 0.6001	TrainAcc 0.9047	ValidAcc 0.9097	TestAcc 0.9043	BestValid 0.9097
	Epoch 800:	Loss 0.5879	TrainAcc 0.9116	ValidAcc 0.9154	TestAcc 0.9100	BestValid 0.9154
	Epoch 850:	Loss 0.5748	TrainAcc 0.9147	ValidAcc 0.9178	TestAcc 0.9128	BestValid 0.9178
	Epoch 900:	Loss 0.5680	TrainAcc 0.9156	ValidAcc 0.9188	TestAcc 0.9140	BestValid 0.9188
	Epoch 950:	Loss 0.5601	TrainAcc 0.9138	ValidAcc 0.9174	TestAcc 0.9119	BestValid 0.9188
	Epoch 1000:	Loss 0.5698	TrainAcc 0.9169	ValidAcc 0.9194	TestAcc 0.9152	BestValid 0.9194
	Epoch 1050:	Loss 0.5611	TrainAcc 0.9177	ValidAcc 0.9213	TestAcc 0.9156	BestValid 0.9213
	Epoch 1100:	Loss 0.5573	TrainAcc 0.9145	ValidAcc 0.9180	TestAcc 0.9136	BestValid 0.9213
	Epoch 1150:	Loss 0.5410	TrainAcc 0.9143	ValidAcc 0.9180	TestAcc 0.9142	BestValid 0.9213
	Epoch 1200:	Loss 0.5370	TrainAcc 0.9193	ValidAcc 0.9217	TestAcc 0.9186	BestValid 0.9217
	Epoch 1250:	Loss 0.5309	TrainAcc 0.9227	ValidAcc 0.9249	TestAcc 0.9209	BestValid 0.9249
	Epoch 1300:	Loss 0.5275	TrainAcc 0.9203	ValidAcc 0.9225	TestAcc 0.9184	BestValid 0.9249
	Epoch 1350:	Loss 0.5368	TrainAcc 0.9156	ValidAcc 0.9169	TestAcc 0.9130	BestValid 0.9249
	Epoch 1400:	Loss 0.5330	TrainAcc 0.9204	ValidAcc 0.9211	TestAcc 0.9175	BestValid 0.9249
	Epoch 1450:	Loss 0.5245	TrainAcc 0.9225	ValidAcc 0.9238	TestAcc 0.9204	BestValid 0.9249
	Epoch 1500:	Loss 0.5278	TrainAcc 0.9206	ValidAcc 0.9214	TestAcc 0.9186	BestValid 0.9249
	Epoch 1550:	Loss 0.5250	TrainAcc 0.9211	ValidAcc 0.9217	TestAcc 0.9184	BestValid 0.9249
	Epoch 1600:	Loss 0.5134	TrainAcc 0.9251	ValidAcc 0.9256	TestAcc 0.9229	BestValid 0.9256
	Epoch 1650:	Loss 0.5032	TrainAcc 0.9269	ValidAcc 0.9283	TestAcc 0.9250	BestValid 0.9283
	Epoch 1700:	Loss 0.4950	TrainAcc 0.9259	ValidAcc 0.9272	TestAcc 0.9242	BestValid 0.9283
	Epoch 1750:	Loss 0.5039	TrainAcc 0.9242	ValidAcc 0.9261	TestAcc 0.9233	BestValid 0.9283
	Epoch 1800:	Loss 0.4986	TrainAcc 0.9263	ValidAcc 0.9290	TestAcc 0.9251	BestValid 0.9290
	Epoch 1850:	Loss 0.4992	TrainAcc 0.9276	ValidAcc 0.9296	TestAcc 0.9254	BestValid 0.9296
	Epoch 1900:	Loss 0.4914	TrainAcc 0.9297	ValidAcc 0.9306	TestAcc 0.9277	BestValid 0.9306
	Epoch 1950:	Loss 0.4856	TrainAcc 0.9269	ValidAcc 0.9289	TestAcc 0.9250	BestValid 0.9306
	Epoch 2000:	Loss 0.4889	TrainAcc 0.9279	ValidAcc 0.9293	TestAcc 0.9261	BestValid 0.9306
	Epoch 2050:	Loss 0.4820	TrainAcc 0.9304	ValidAcc 0.9323	TestAcc 0.9289	BestValid 0.9323
	Epoch 2100:	Loss 0.4905	TrainAcc 0.9295	ValidAcc 0.9311	TestAcc 0.9271	BestValid 0.9323
	Epoch 2150:	Loss 0.4818	TrainAcc 0.9306	ValidAcc 0.9324	TestAcc 0.9285	BestValid 0.9324
	Epoch 2200:	Loss 0.4789	TrainAcc 0.9311	ValidAcc 0.9317	TestAcc 0.9288	BestValid 0.9324
	Epoch 2250:	Loss 0.4788	TrainAcc 0.9297	ValidAcc 0.9301	TestAcc 0.9271	BestValid 0.9324
	Epoch 2300:	Loss 0.4760	TrainAcc 0.9288	ValidAcc 0.9290	TestAcc 0.9256	BestValid 0.9324
	Epoch 2350:	Loss 0.4783	TrainAcc 0.9295	ValidAcc 0.9298	TestAcc 0.9268	BestValid 0.9324
	Epoch 2400:	Loss 0.4730	TrainAcc 0.9318	ValidAcc 0.9325	TestAcc 0.9294	BestValid 0.9325
	Epoch 2450:	Loss 0.4616	TrainAcc 0.9334	ValidAcc 0.9334	TestAcc 0.9303	BestValid 0.9334
	Epoch 2500:	Loss 0.4559	TrainAcc 0.9334	ValidAcc 0.9333	TestAcc 0.9304	BestValid 0.9334
	Epoch 2550:	Loss 0.4558	TrainAcc 0.9342	ValidAcc 0.9342	TestAcc 0.9311	BestValid 0.9342
	Epoch 2600:	Loss 0.4552	TrainAcc 0.9334	ValidAcc 0.9342	TestAcc 0.9300	BestValid 0.9342
	Epoch 2650:	Loss 0.4696	TrainAcc 0.9304	ValidAcc 0.9291	TestAcc 0.9254	BestValid 0.9342
	Epoch 2700:	Loss 0.4770	TrainAcc 0.9281	ValidAcc 0.9282	TestAcc 0.9242	BestValid 0.9342
	Epoch 2750:	Loss 0.4762	TrainAcc 0.9300	ValidAcc 0.9300	TestAcc 0.9262	BestValid 0.9342
	Epoch 2800:	Loss 0.4531	TrainAcc 0.9342	ValidAcc 0.9351	TestAcc 0.9309	BestValid 0.9351
	Epoch 2850:	Loss 0.4476	TrainAcc 0.9355	ValidAcc 0.9368	TestAcc 0.9330	BestValid 0.9368
	Epoch 2900:	Loss 0.4450	TrainAcc 0.9354	ValidAcc 0.9365	TestAcc 0.9332	BestValid 0.9368
	Epoch 2950:	Loss 0.4480	TrainAcc 0.9335	ValidAcc 0.9347	TestAcc 0.9317	BestValid 0.9368
	Epoch 3000:	Loss 0.4542	TrainAcc 0.9354	ValidAcc 0.9364	TestAcc 0.9327	BestValid 0.9368
	Epoch 3050:	Loss 0.4466	TrainAcc 0.9360	ValidAcc 0.9377	TestAcc 0.9333	BestValid 0.9377
	Epoch 3100:	Loss 0.4410	TrainAcc 0.9375	ValidAcc 0.9394	TestAcc 0.9349	BestValid 0.9394
	Epoch 3150:	Loss 0.4400	TrainAcc 0.9347	ValidAcc 0.9359	TestAcc 0.9323	BestValid 0.9394
	Epoch 3200:	Loss 0.4544	TrainAcc 0.9301	ValidAcc 0.9304	TestAcc 0.9275	BestValid 0.9394
	Epoch 3250:	Loss 0.4824	TrainAcc 0.9300	ValidAcc 0.9301	TestAcc 0.9269	BestValid 0.9394
	Epoch 3300:	Loss 0.4758	TrainAcc 0.9302	ValidAcc 0.9295	TestAcc 0.9263	BestValid 0.9394
	Epoch 3350:	Loss 0.4422	TrainAcc 0.9358	ValidAcc 0.9360	TestAcc 0.9317	BestValid 0.9394
	Epoch 3400:	Loss 0.4271	TrainAcc 0.9386	ValidAcc 0.9391	TestAcc 0.9352	BestValid 0.9394
	Epoch 3450:	Loss 0.4254	TrainAcc 0.9390	ValidAcc 0.9396	TestAcc 0.9357	BestValid 0.9396
	Epoch 3500:	Loss 0.4349	TrainAcc 0.9390	ValidAcc 0.9394	TestAcc 0.9358	BestValid 0.9396
	Epoch 3550:	Loss 0.4252	TrainAcc 0.9396	ValidAcc 0.9398	TestAcc 0.9362	BestValid 0.9398
	Epoch 3600:	Loss 0.4260	TrainAcc 0.9405	ValidAcc 0.9405	TestAcc 0.9366	BestValid 0.9405
	Epoch 3650:	Loss 0.4372	TrainAcc 0.9406	ValidAcc 0.9402	TestAcc 0.9367	BestValid 0.9405
	Epoch 3700:	Loss 0.4397	TrainAcc 0.9380	ValidAcc 0.9382	TestAcc 0.9351	BestValid 0.9405
	Epoch 3750:	Loss 0.4339	TrainAcc 0.9375	ValidAcc 0.9372	TestAcc 0.9347	BestValid 0.9405
	Epoch 3800:	Loss 0.4276	TrainAcc 0.9406	ValidAcc 0.9402	TestAcc 0.9377	BestValid 0.9405
	Epoch 3850:	Loss 0.4311	TrainAcc 0.9411	ValidAcc 0.9411	TestAcc 0.9383	BestValid 0.9411
	Epoch 3900:	Loss 0.4286	TrainAcc 0.9411	ValidAcc 0.9406	TestAcc 0.9384	BestValid 0.9411
	Epoch 3950:	Loss 0.4279	TrainAcc 0.9383	ValidAcc 0.9378	TestAcc 0.9356	BestValid 0.9411
	Epoch 4000:	Loss 0.4370	TrainAcc 0.9378	ValidAcc 0.9376	TestAcc 0.9347	BestValid 0.9411
	Epoch 4050:	Loss 0.4486	TrainAcc 0.9346	ValidAcc 0.9350	TestAcc 0.9313	BestValid 0.9411
	Epoch 4100:	Loss 0.4263	TrainAcc 0.9396	ValidAcc 0.9397	TestAcc 0.9357	BestValid 0.9411
	Epoch 4150:	Loss 0.4244	TrainAcc 0.9364	ValidAcc 0.9355	TestAcc 0.9318	BestValid 0.9411
	Epoch 4200:	Loss 0.4341	TrainAcc 0.9361	ValidAcc 0.9362	TestAcc 0.9316	BestValid 0.9411
	Epoch 4250:	Loss 0.4124	TrainAcc 0.9391	ValidAcc 0.9389	TestAcc 0.9354	BestValid 0.9411
	Epoch 4300:	Loss 0.4159	TrainAcc 0.9403	ValidAcc 0.9404	TestAcc 0.9368	BestValid 0.9411
	Epoch 4350:	Loss 0.4195	TrainAcc 0.9422	ValidAcc 0.9422	TestAcc 0.9382	BestValid 0.9422
	Epoch 4400:	Loss 0.4231	TrainAcc 0.9395	ValidAcc 0.9397	TestAcc 0.9365	BestValid 0.9422
	Epoch 4450:	Loss 0.4218	TrainAcc 0.9398	ValidAcc 0.9398	TestAcc 0.9374	BestValid 0.9422
	Epoch 4500:	Loss 0.4087	TrainAcc 0.9425	ValidAcc 0.9423	TestAcc 0.9393	BestValid 0.9423
	Epoch 4550:	Loss 0.4036	TrainAcc 0.9431	ValidAcc 0.9425	TestAcc 0.9391	BestValid 0.9425
	Epoch 4600:	Loss 0.4074	TrainAcc 0.9419	ValidAcc 0.9410	TestAcc 0.9374	BestValid 0.9425
	Epoch 4650:	Loss 0.4198	TrainAcc 0.9361	ValidAcc 0.9358	TestAcc 0.9307	BestValid 0.9425
	Epoch 4700:	Loss 0.4643	TrainAcc 0.9257	ValidAcc 0.9245	TestAcc 0.9209	BestValid 0.9425
	Epoch 4750:	Loss 0.4252	TrainAcc 0.9344	ValidAcc 0.9345	TestAcc 0.9297	BestValid 0.9425
	Epoch 4800:	Loss 0.4143	TrainAcc 0.9404	ValidAcc 0.9402	TestAcc 0.9361	BestValid 0.9425
	Epoch 4850:	Loss 0.4107	TrainAcc 0.9430	ValidAcc 0.9434	TestAcc 0.9396	BestValid 0.9434
	Epoch 4900:	Loss 0.4049	TrainAcc 0.9437	ValidAcc 0.9433	TestAcc 0.9399	BestValid 0.9434
	Epoch 4950:	Loss 0.3996	TrainAcc 0.9431	ValidAcc 0.9421	TestAcc 0.9392	BestValid 0.9434
	Epoch 5000:	Loss 0.4014	TrainAcc 0.9439	ValidAcc 0.9434	TestAcc 0.9401	BestValid 0.9434
****** Epoch Time (Excluding Evaluation Cost): 0.415 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 79.756 ms (Max: 82.579, Min: 66.787, Sum: 638.049)
Cluster-Wide Average, Compute: 262.593 ms (Max: 284.706, Min: 252.110, Sum: 2100.741)
Cluster-Wide Average, Communication-Layer: 41.722 ms (Max: 46.380, Min: 32.370, Sum: 333.780)
Cluster-Wide Average, Bubble-Imbalance: 28.247 ms (Max: 35.268, Min: 14.509, Sum: 225.974)
Cluster-Wide Average, Communication-Graph: 0.463 ms (Max: 0.508, Min: 0.412, Sum: 3.700)
Cluster-Wide Average, Optimization: 0.100 ms (Max: 0.124, Min: 0.091, Sum: 0.803)
Cluster-Wide Average, Others: 3.337 ms (Max: 17.028, Min: 1.358, Sum: 26.695)
****** Breakdown Sum: 416.218 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.304 GB (Max: 8.059, Min: 6.018, Sum: 50.433)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 61.486 Gbps (Max: 71.057, Min: 45.779, Sum: 491.887)
Layer-level communication (cluster-wide, per-epoch): 2.430 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 2.430 GB
****** Accuracy Results ******
Highest valid_acc: 0.9434
Target test_acc: 0.9396
Epoch to reach the target acc: 4849
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
