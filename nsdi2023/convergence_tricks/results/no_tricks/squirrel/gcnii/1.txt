Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
        It takes 0.006 seconds.
Building the Feature Vector...
        It takes 0.009 seconds.
        It takes 0.010 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.012 seconds.
Building the Feature Vector...
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 52.903 Gbps (per GPU), 423.227 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 52.678 Gbps (per GPU), 421.421 Gbps (aggregated)
The layer-level communication performance: 52.671 Gbps (per GPU), 421.367 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 52.487 Gbps (per GPU), 419.899 Gbps (aggregated)
The layer-level communication performance: 52.462 Gbps (per GPU), 419.698 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 52.292 Gbps (per GPU), 418.338 Gbps (aggregated)
The layer-level communication performance: 52.257 Gbps (per GPU), 418.058 Gbps (aggregated)
The layer-level communication performance: 52.232 Gbps (per GPU), 417.856 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.165 Gbps (per GPU), 1249.324 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.128 Gbps (per GPU), 1249.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.163 Gbps (per GPU), 1249.304 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.140 Gbps (per GPU), 1249.118 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.140 Gbps (per GPU), 1249.117 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.148 Gbps (per GPU), 1249.188 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.119 Gbps (per GPU), 1248.955 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.143 Gbps (per GPU), 1249.143 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 79.493 Gbps (per GPU), 635.942 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.495 Gbps (per GPU), 635.958 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.487 Gbps (per GPU), 635.898 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.492 Gbps (per GPU), 635.938 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.489 Gbps (per GPU), 635.914 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.493 Gbps (per GPU), 635.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.488 Gbps (per GPU), 635.902 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.493 Gbps (per GPU), 635.946 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.058 Gbps (per GPU), 240.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.057 Gbps (per GPU), 240.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.053 Gbps (per GPU), 240.427 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.059 Gbps (per GPU), 240.473 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.053 Gbps (per GPU), 240.421 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.054 Gbps (per GPU), 240.434 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.051 Gbps (per GPU), 240.410 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.048 Gbps (per GPU), 240.387 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.35ms  0.43ms  0.56ms  1.60  0.08K  0.02M
 chk_1  0.45ms  0.46ms  0.60ms  1.35  0.16K  0.01M
 chk_2  0.40ms  0.43ms  0.57ms  1.44  0.13K  0.01M
 chk_3  0.46ms  0.49ms  0.64ms  1.39  0.18K  0.01M
 chk_4  0.44ms  0.46ms  0.60ms  1.36  0.15K  0.01M
 chk_5  0.51ms  0.50ms  0.64ms  1.29  0.25K  0.01M
 chk_6  0.36ms  0.43ms  0.57ms  1.58  0.10K  0.02M
 chk_7  0.40ms  0.44ms  0.58ms  1.48  0.11K  0.02M
 chk_8  0.51ms  0.49ms  0.64ms  1.29  0.23K  0.01M
 chk_9  0.43ms  0.45ms  0.59ms  1.37  0.14K  0.01M
chk_10  0.50ms  0.49ms  0.63ms  1.29  0.20K  0.01M
chk_11  0.36ms  0.43ms  0.57ms  1.58  0.09K  0.02M
chk_12  0.44ms  0.46ms  0.60ms  1.35  0.16K  0.01M
chk_13  0.44ms  0.46ms  0.60ms  1.35  0.16K  0.01M
chk_14  0.44ms  0.46ms  0.60ms  1.37  0.14K  0.01M
chk_15  0.51ms  0.49ms  0.63ms  1.28  0.21K  0.01M
chk_16  0.46ms  0.49ms  0.64ms  1.39  0.18K  0.01M
chk_17  0.57ms  0.50ms  0.64ms  1.29  0.29K  0.01M
chk_18  0.58ms  0.50ms  0.65ms  1.29  0.31K  0.00M
chk_19  0.40ms  0.44ms  0.57ms  1.44  0.13K  0.01M
chk_20  0.40ms  0.43ms  0.57ms  1.45  0.13K  0.01M
chk_21  0.46ms  0.49ms  0.64ms  1.38  0.18K  0.01M
chk_22  0.40ms  0.43ms  0.57ms  1.44  0.13K  0.01M
chk_23  0.45ms  0.49ms  0.63ms  1.40  0.16K  0.01M
chk_24  0.36ms  0.43ms  0.56ms  1.57  0.09K  0.02M
chk_25  0.36ms  0.43ms  0.57ms  1.59  0.09K  0.02M
chk_26  0.46ms  0.49ms  0.64ms  1.38  0.18K  0.01M
chk_27  0.40ms  0.43ms  0.57ms  1.44  0.13K  0.01M
chk_28  0.48ms  0.49ms  0.63ms  1.31  0.17K  0.01M
chk_29  0.45ms  0.46ms  0.60ms  1.33  0.15K  0.01M
chk_30  0.51ms  0.49ms  0.64ms  1.29  0.24K  0.01M
chk_31  0.50ms  0.49ms  0.63ms  1.29  0.20K  0.01M
   Avg  0.44  0.46  0.61
   Max  0.58  0.50  0.65
   Min  0.35  0.43  0.56
 Ratio  1.64  1.18  1.15
   Var  0.00  0.00  0.00
Profiling takes 0.689 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 73.720 ms
Partition 0 [0, 5) has cost: 73.720 ms
Partition 1 [5, 9) has cost: 59.507 ms
Partition 2 [9, 13) has cost: 59.507 ms
Partition 3 [13, 17) has cost: 59.507 ms
Partition 4 [17, 21) has cost: 59.507 ms
Partition 5 [21, 25) has cost: 59.507 ms
Partition 6 [25, 29) has cost: 59.507 ms
Partition 7 [29, 33) has cost: 64.010 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 33.058 ms
GPU 0, Compute+Comm Time: 27.163 ms, Bubble Time: 5.508 ms, Imbalance Overhead: 0.386 ms
GPU 1, Compute+Comm Time: 22.822 ms, Bubble Time: 5.478 ms, Imbalance Overhead: 4.758 ms
GPU 2, Compute+Comm Time: 22.822 ms, Bubble Time: 5.543 ms, Imbalance Overhead: 4.693 ms
GPU 3, Compute+Comm Time: 22.822 ms, Bubble Time: 5.547 ms, Imbalance Overhead: 4.688 ms
GPU 4, Compute+Comm Time: 22.822 ms, Bubble Time: 5.660 ms, Imbalance Overhead: 4.576 ms
GPU 5, Compute+Comm Time: 22.822 ms, Bubble Time: 5.719 ms, Imbalance Overhead: 4.516 ms
GPU 6, Compute+Comm Time: 22.822 ms, Bubble Time: 5.838 ms, Imbalance Overhead: 4.398 ms
GPU 7, Compute+Comm Time: 24.081 ms, Bubble Time: 5.832 ms, Imbalance Overhead: 3.144 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 64.321 ms
GPU 0, Compute+Comm Time: 46.221 ms, Bubble Time: 11.301 ms, Imbalance Overhead: 6.799 ms
GPU 1, Compute+Comm Time: 42.977 ms, Bubble Time: 11.325 ms, Imbalance Overhead: 10.019 ms
GPU 2, Compute+Comm Time: 42.977 ms, Bubble Time: 11.043 ms, Imbalance Overhead: 10.301 ms
GPU 3, Compute+Comm Time: 42.977 ms, Bubble Time: 10.935 ms, Imbalance Overhead: 10.409 ms
GPU 4, Compute+Comm Time: 42.977 ms, Bubble Time: 10.709 ms, Imbalance Overhead: 10.635 ms
GPU 5, Compute+Comm Time: 42.977 ms, Bubble Time: 10.702 ms, Imbalance Overhead: 10.642 ms
GPU 6, Compute+Comm Time: 42.977 ms, Bubble Time: 10.609 ms, Imbalance Overhead: 10.735 ms
GPU 7, Compute+Comm Time: 52.848 ms, Bubble Time: 10.693 ms, Imbalance Overhead: 0.780 ms
The estimated cost of the whole pipeline: 102.247 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 133.227 ms
Partition 0 [0, 9) has cost: 133.227 ms
Partition 1 [9, 17) has cost: 119.014 ms
Partition 2 [17, 25) has cost: 119.014 ms
Partition 3 [25, 33) has cost: 123.518 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 33.920 ms
GPU 0, Compute+Comm Time: 28.090 ms, Bubble Time: 5.164 ms, Imbalance Overhead: 0.666 ms
GPU 1, Compute+Comm Time: 25.786 ms, Bubble Time: 5.201 ms, Imbalance Overhead: 2.933 ms
GPU 2, Compute+Comm Time: 25.786 ms, Bubble Time: 5.408 ms, Imbalance Overhead: 2.725 ms
GPU 3, Compute+Comm Time: 26.417 ms, Bubble Time: 5.628 ms, Imbalance Overhead: 1.874 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 63.261 ms
GPU 0, Compute+Comm Time: 48.721 ms, Bubble Time: 10.576 ms, Imbalance Overhead: 3.964 ms
GPU 1, Compute+Comm Time: 47.079 ms, Bubble Time: 10.093 ms, Imbalance Overhead: 6.089 ms
GPU 2, Compute+Comm Time: 47.079 ms, Bubble Time: 9.692 ms, Imbalance Overhead: 6.491 ms
GPU 3, Compute+Comm Time: 52.430 ms, Bubble Time: 9.552 ms, Imbalance Overhead: 1.279 ms
    The estimated cost with 2 DP ways is 102.040 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 252.241 ms
Partition 0 [0, 17) has cost: 252.241 ms
Partition 1 [17, 33) has cost: 242.532 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 49.586 ms
GPU 0, Compute+Comm Time: 43.018 ms, Bubble Time: 5.015 ms, Imbalance Overhead: 1.553 ms
GPU 1, Compute+Comm Time: 42.120 ms, Bubble Time: 5.276 ms, Imbalance Overhead: 2.190 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 76.694 ms
GPU 0, Compute+Comm Time: 65.297 ms, Bubble Time: 8.288 ms, Imbalance Overhead: 3.109 ms
GPU 1, Compute+Comm Time: 67.377 ms, Bubble Time: 7.854 ms, Imbalance Overhead: 1.464 ms
    The estimated cost with 4 DP ways is 132.595 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 494.773 ms
Partition 0 [0, 33) has cost: 494.773 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 144.463 ms
GPU 0, Compute+Comm Time: 144.463 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 168.369 ms
GPU 0, Compute+Comm Time: 168.369 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 328.473 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...



*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.7309	TrainAcc 0.2007	ValidAcc 0.2007	TestAcc 0.1988	BestValid 0.2007
	Epoch 50:	Loss 1.5212	TrainAcc 0.3117	ValidAcc 0.2416	TestAcc 0.2344	BestValid 0.2416
	Epoch 100:	Loss 1.4486	TrainAcc 0.5333	ValidAcc 0.3125	TestAcc 0.2988	BestValid 0.3125
	Epoch 150:	Loss 1.2292	TrainAcc 0.5721	ValidAcc 0.3125	TestAcc 0.2949	BestValid 0.3125
	Epoch 200:	Loss 1.1198	TrainAcc 0.5909	ValidAcc 0.3071	TestAcc 0.2872	BestValid 0.3125
	Epoch 250:	Loss 1.0927	TrainAcc 0.6446	ValidAcc 0.3215	TestAcc 0.3036	BestValid 0.3215
	Epoch 300:	Loss 1.0276	TrainAcc 0.7071	ValidAcc 0.3221	TestAcc 0.3112	BestValid 0.3221
	Epoch 350:	Loss 0.9686	TrainAcc 0.6595	ValidAcc 0.3047	TestAcc 0.2891	BestValid 0.3221
	Epoch 400:	Loss 0.9990	TrainAcc 0.7989	ValidAcc 0.3516	TestAcc 0.3343	BestValid 0.3516
	Epoch 450:	Loss 0.9055	TrainAcc 0.7200	ValidAcc 0.3137	TestAcc 0.2988	BestValid 0.3516
	Epoch 500:	Loss 0.8795	TrainAcc 0.7496	ValidAcc 0.3191	TestAcc 0.3055	BestValid 0.3516
	Epoch 550:	Loss 0.8225	TrainAcc 0.7989	ValidAcc 0.3299	TestAcc 0.3276	BestValid 0.3516
	Epoch 600:	Loss 0.8382	TrainAcc 0.7700	ValidAcc 0.3155	TestAcc 0.3084	BestValid 0.3516
	Epoch 650:	Loss 0.8041	TrainAcc 0.7704	ValidAcc 0.3119	TestAcc 0.3064	BestValid 0.3516
	Epoch 700:	Loss 0.7851	TrainAcc 0.8073	ValidAcc 0.3263	TestAcc 0.3180	BestValid 0.3516
	Epoch 750:	Loss 0.7702	TrainAcc 0.8405	ValidAcc 0.3365	TestAcc 0.3266	BestValid 0.3516
	Epoch 800:	Loss 0.6928	TrainAcc 0.7853	ValidAcc 0.3149	TestAcc 0.2997	BestValid 0.3516
	Epoch 850:	Loss 0.7596	TrainAcc 0.7756	ValidAcc 0.3101	TestAcc 0.2949	BestValid 0.3516
	Epoch 900:	Loss 0.6971	TrainAcc 0.8045	ValidAcc 0.3197	TestAcc 0.3084	BestValid 0.3516
	Epoch 950:	Loss 0.7064	TrainAcc 0.8498	ValidAcc 0.3323	TestAcc 0.3305	BestValid 0.3516
	Epoch 1000:	Loss 0.6757	TrainAcc 0.8726	ValidAcc 0.3407	TestAcc 0.3381	BestValid 0.3516
	Epoch 1050:	Loss 0.6894	TrainAcc 0.8666	ValidAcc 0.3365	TestAcc 0.3343	BestValid 0.3516
	Epoch 1100:	Loss 0.6290	TrainAcc 0.8530	ValidAcc 0.3317	TestAcc 0.3189	BestValid 0.3516
	Epoch 1150:	Loss 0.6150	TrainAcc 0.8934	ValidAcc 0.3468	TestAcc 0.3439	BestValid 0.3516
	Epoch 1200:	Loss 0.6238	TrainAcc 0.8986	ValidAcc 0.3510	TestAcc 0.3468	BestValid 0.3516
	Epoch 1250:	Loss 0.5860	TrainAcc 0.8874	ValidAcc 0.3407	TestAcc 0.3410	BestValid 0.3516
	Epoch 1300:	Loss 0.5913	TrainAcc 0.8866	ValidAcc 0.3401	TestAcc 0.3362	BestValid 0.3516
	Epoch 1350:	Loss 0.5733	TrainAcc 0.8730	ValidAcc 0.3311	TestAcc 0.3199	BestValid 0.3516
	Epoch 1400:	Loss 0.5599	TrainAcc 0.8782	ValidAcc 0.3311	TestAcc 0.3237	BestValid 0.3516
	Epoch 1450:	Loss 0.5283	TrainAcc 0.8750	ValidAcc 0.3317	TestAcc 0.3237	BestValid 0.3516
	Epoch 1500:	Loss 0.5332	TrainAcc 0.9042	ValidAcc 0.3474	TestAcc 0.3372	BestValid 0.3516
	Epoch 1550:	Loss 0.5623	TrainAcc 0.9415	ValidAcc 0.3540	TestAcc 0.3660	BestValid 0.3540
	Epoch 1600:	Loss 0.5096	TrainAcc 0.9263	ValidAcc 0.3510	TestAcc 0.3497	BestValid 0.3540
	Epoch 1650:	Loss 0.4970	TrainAcc 0.8994	ValidAcc 0.3317	TestAcc 0.3305	BestValid 0.3540
	Epoch 1700:	Loss 0.5095	TrainAcc 0.8890	ValidAcc 0.3287	TestAcc 0.3170	BestValid 0.3540
	Epoch 1750:	Loss 0.5013	TrainAcc 0.8866	ValidAcc 0.3311	TestAcc 0.3208	BestValid 0.3540
	Epoch 1800:	Loss 0.4775	TrainAcc 0.8850	ValidAcc 0.3287	TestAcc 0.3180	BestValid 0.3540
	Epoch 1850:	Loss 0.4640	TrainAcc 0.8978	ValidAcc 0.3347	TestAcc 0.3285	BestValid 0.3540
	Epoch 1900:	Loss 0.4576	TrainAcc 0.9058	ValidAcc 0.3317	TestAcc 0.3266	BestValid 0.3540
	Epoch 1950:	Loss 0.4293	TrainAcc 0.9195	ValidAcc 0.3413	TestAcc 0.3381	BestValid 0.3540
	Epoch 2000:	Loss 0.4734	TrainAcc 0.9251	ValidAcc 0.3407	TestAcc 0.3401	BestValid 0.3540
	Epoch 2050:	Loss 0.4697	TrainAcc 0.9215	ValidAcc 0.3371	TestAcc 0.3333	BestValid 0.3540
	Epoch 2100:	Loss 0.4320	TrainAcc 0.9026	ValidAcc 0.3263	TestAcc 0.3208	BestValid 0.3540
	Epoch 2150:	Loss 0.4444	TrainAcc 0.8938	ValidAcc 0.3269	TestAcc 0.3122	BestValid 0.3540
	Epoch 2200:	Loss 0.4377	TrainAcc 0.9107	ValidAcc 0.3305	TestAcc 0.3151	BestValid 0.3540
	Epoch 2250:	Loss 0.4457	TrainAcc 0.9315	ValidAcc 0.3395	TestAcc 0.3353	BestValid 0.3540
	Epoch 2300:	Loss 0.3914	TrainAcc 0.9371	ValidAcc 0.3419	TestAcc 0.3372	BestValid 0.3540
	Epoch 2350:	Loss 0.4220	TrainAcc 0.9367	ValidAcc 0.3419	TestAcc 0.3381	BestValid 0.3540
	Epoch 2400:	Loss 0.4203	TrainAcc 0.9319	ValidAcc 0.3401	TestAcc 0.3333	BestValid 0.3540
	Epoch 2450:	Loss 0.3737	TrainAcc 0.9239	ValidAcc 0.3317	TestAcc 0.3256	BestValid 0.3540
	Epoch 2500:	Loss 0.4055	TrainAcc 0.9143	ValidAcc 0.3305	TestAcc 0.3189	BestValid 0.3540
	Epoch 2550:	Loss 0.3431	TrainAcc 0.9231	ValidAcc 0.3311	TestAcc 0.3237	BestValid 0.3540
	Epoch 2600:	Loss 0.3750	TrainAcc 0.9239	ValidAcc 0.3335	TestAcc 0.3170	BestValid 0.3540
	Epoch 2650:	Loss 0.4349	TrainAcc 0.9263	ValidAcc 0.3353	TestAcc 0.3160	BestValid 0.3540
	Epoch 2700:	Loss 0.3579	TrainAcc 0.9211	ValidAcc 0.3329	TestAcc 0.3180	BestValid 0.3540
	Epoch 2750:	Loss 0.3143	TrainAcc 0.9183	ValidAcc 0.3269	TestAcc 0.3160	BestValid 0.3540
	Epoch 2800:	Loss 0.3275	TrainAcc 0.9267	ValidAcc 0.3335	TestAcc 0.3266	BestValid 0.3540
	Epoch 2850:	Loss 0.3199	TrainAcc 0.9271	ValidAcc 0.3305	TestAcc 0.3170	BestValid 0.3540
	Epoch 2900:	Loss 0.3857	TrainAcc 0.9075	ValidAcc 0.3251	TestAcc 0.3036	BestValid 0.3540
	Epoch 2950:	Loss 0.3685	TrainAcc 0.9199	ValidAcc 0.3329	TestAcc 0.3064	BestValid 0.3540
	Epoch 3000:	Loss 0.3503	TrainAcc 0.9599	ValidAcc 0.3504	TestAcc 0.3487	BestValid 0.3540
	Epoch 3050:	Loss 0.3233	TrainAcc 0.9555	ValidAcc 0.3456	TestAcc 0.3381	BestValid 0.3540
	Epoch 3100:	Loss 0.3813	TrainAcc 0.9487	ValidAcc 0.3456	TestAcc 0.3333	BestValid 0.3540
	Epoch 3150:	Loss 0.3144	TrainAcc 0.9415	ValidAcc 0.3371	TestAcc 0.3189	BestValid 0.3540
	Epoch 3200:	Loss 0.3323	TrainAcc 0.9547	ValidAcc 0.3492	TestAcc 0.3381	BestValid 0.3540
	Epoch 3250:	Loss 0.3080	TrainAcc 0.9587	ValidAcc 0.3528	TestAcc 0.3497	BestValid 0.3540
	Epoch 3300:	Loss 0.3714	TrainAcc 0.9579	ValidAcc 0.3462	TestAcc 0.3420	BestValid 0.3540
	Epoch 3350:	Loss 0.3175	TrainAcc 0.9483	ValidAcc 0.3444	TestAcc 0.3362	BestValid 0.3540
	Epoch 3400:	Loss 0.2913	TrainAcc 0.9459	ValidAcc 0.3389	TestAcc 0.3266	BestValid 0.3540
	Epoch 3450:	Loss 0.2860	TrainAcc 0.9415	ValidAcc 0.3365	TestAcc 0.3189	BestValid 0.3540
	Epoch 3500:	Loss 0.2877	TrainAcc 0.9363	ValidAcc 0.3341	TestAcc 0.3151	BestValid 0.3540
	Epoch 3550:	Loss 0.3025	TrainAcc 0.9519	ValidAcc 0.3401	TestAcc 0.3333	BestValid 0.3540
	Epoch 3600:	Loss 0.2935	TrainAcc 0.9619	ValidAcc 0.3516	TestAcc 0.3468	BestValid 0.3540
	Epoch 3650:	Loss 0.2956	TrainAcc 0.9595	ValidAcc 0.3468	TestAcc 0.3420	BestValid 0.3540
	Epoch 3700:	Loss 0.2748	TrainAcc 0.9563	ValidAcc 0.3425	TestAcc 0.3410	BestValid 0.3540
	Epoch 3750:	Loss 0.3065	TrainAcc 0.9523	ValidAcc 0.3383	TestAcc 0.3353	BestValid 0.3540
	Epoch 3800:	Loss 0.2699	TrainAcc 0.9507	ValidAcc 0.3335	TestAcc 0.3285	BestValid 0.3540
	Epoch 3850:	Loss 0.3272	TrainAcc 0.9527	ValidAcc 0.3383	TestAcc 0.3285	BestValid 0.3540
	Epoch 3900:	Loss 0.2914	TrainAcc 0.9527	ValidAcc 0.3353	TestAcc 0.3353	BestValid 0.3540
	Epoch 3950:	Loss 0.3702	TrainAcc 0.9619	ValidAcc 0.3438	TestAcc 0.3487	BestValid 0.3540
	Epoch 4000:	Loss 0.2935	TrainAcc 0.9671	ValidAcc 0.3492	TestAcc 0.3420	BestValid 0.3540
	Epoch 4050:	Loss 0.2975	TrainAcc 0.9696	ValidAcc 0.3516	TestAcc 0.3516	BestValid 0.3540
	Epoch 4100:	Loss 0.2698	TrainAcc 0.9716	ValidAcc 0.3522	TestAcc 0.3564	BestValid 0.3540
	Epoch 4150:	Loss 0.4682	TrainAcc 0.9700	ValidAcc 0.3462	TestAcc 0.3583	BestValid 0.3540
	Epoch 4200:	Loss 0.2576	TrainAcc 0.9567	ValidAcc 0.3329	TestAcc 0.3285	BestValid 0.3540
	Epoch 4250:	Loss 0.2657	TrainAcc 0.9599	ValidAcc 0.3371	TestAcc 0.3353	BestValid 0.3540
	Epoch 4300:	Loss 0.3080	TrainAcc 0.9635	ValidAcc 0.3419	TestAcc 0.3410	BestValid 0.3540
	Epoch 4350:	Loss 0.2589	TrainAcc 0.9543	ValidAcc 0.3347	TestAcc 0.3314	BestValid 0.3540
	Epoch 4400:	Loss 0.2765	TrainAcc 0.9575	ValidAcc 0.3329	TestAcc 0.3333	BestValid 0.3540
	Epoch 4450:	Loss 0.2372	TrainAcc 0.9611	ValidAcc 0.3377	TestAcc 0.3353	BestValid 0.3540
	Epoch 4500:	Loss 0.2446	TrainAcc 0.9599	ValidAcc 0.3359	TestAcc 0.3381	BestValid 0.3540
	Epoch 4550:	Loss 0.2352	TrainAcc 0.9623	ValidAcc 0.3401	TestAcc 0.3362	BestValid 0.3540
	Epoch 4600:	Loss 0.2333	TrainAcc 0.9675	ValidAcc 0.3407	TestAcc 0.3401	BestValid 0.3540
	Epoch 4650:	Loss 0.2337	TrainAcc 0.9659	ValidAcc 0.3419	TestAcc 0.3391	BestValid 0.3540
	Epoch 4700:	Loss 0.2457	TrainAcc 0.9591	ValidAcc 0.3377	TestAcc 0.3410	BestValid 0.3540
	Epoch 4750:	Loss 0.2255	TrainAcc 0.9663	ValidAcc 0.3371	TestAcc 0.3429	BestValid 0.3540
	Epoch 4800:	Loss 0.2220	TrainAcc 0.9728	ValidAcc 0.3504	TestAcc 0.3564	BestValid 0.3540
	Epoch 4850:	Loss 0.2169	TrainAcc 0.9728	ValidAcc 0.3450	TestAcc 0.3564	BestValid 0.3540
	Epoch 4900:	Loss 0.2350	TrainAcc 0.9724	ValidAcc 0.3444	TestAcc 0.3535	BestValid 0.3540
	Epoch 4950:	Loss 0.2419	TrainAcc 0.9736	ValidAcc 0.3462	TestAcc 0.3554	BestValid 0.3540
	Epoch 5000:	Loss 0.2052	TrainAcc 0.9720	ValidAcc 0.3438	TestAcc 0.3497	BestValid 0.3540
****** Epoch Time (Excluding Evaluation Cost): 0.108 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 18.017 ms (Max: 18.940, Min: 17.266, Sum: 144.138)
Cluster-Wide Average, Compute: 61.819 ms (Max: 75.326, Min: 58.202, Sum: 494.553)
Cluster-Wide Average, Communication-Layer: 11.526 ms (Max: 13.245, Min: 9.108, Sum: 92.209)
Cluster-Wide Average, Bubble-Imbalance: 13.720 ms (Max: 17.094, Min: 2.809, Sum: 109.758)
Cluster-Wide Average, Communication-Graph: 0.474 ms (Max: 0.527, Min: 0.418, Sum: 3.791)
Cluster-Wide Average, Optimization: 1.028 ms (Max: 1.476, Min: 0.951, Sum: 8.225)
Cluster-Wide Average, Others: 1.067 ms (Max: 1.943, Min: 0.535, Sum: 8.538)
****** Breakdown Sum: 107.651 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.080 GB (Max: 4.061, Min: 2.926, Sum: 24.640)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 49.803 Gbps (Max: 60.578, Min: 34.736, Sum: 398.421)
Layer-level communication (cluster-wide, per-epoch): 0.543 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.543 GB
****** Accuracy Results ******
Highest valid_acc: 0.3540
Target test_acc: 0.3660
Epoch to reach the target acc: 1549
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
