Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7

Initialized node 0 on machine gnerv7
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
Initialized node 6 on machine gnerv8

Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the Feature Vector...
        It takes 0.011 seconds.
        It takes 0.018 seconds.
        It takes 0.019 seconds.
        It takes 0.013 seconds.
        It takes 0.011 seconds.
        It takes 0.012 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.014 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.471 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.471 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.503 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.503 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.605 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/physics/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 300
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 8
        It takes 0.613 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.615 seconds.
Building the Label Vector...
        It takes 0.620 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 4311) 1-[4311, 8623) 2-[8623, 12935) 3-[12935, 17247) 4-[17247, 21558) 5-[21558, 25870) 6-[25870, 30181) 7-[30181, 34493)
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.395 Gbps (per GPU), 483.159 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.079 Gbps (per GPU), 480.629 Gbps (aggregated)
The layer-level communication performance: 60.085 Gbps (per GPU), 480.676 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.825 Gbps (per GPU), 478.598 Gbps (aggregated)
The layer-level communication performance: 59.795 Gbps (per GPU), 478.359 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.587 Gbps (per GPU), 476.698 Gbps (aggregated)
The layer-level communication performance: 59.538 Gbps (per GPU), 476.300 Gbps (aggregated)
The layer-level communication performance: 59.507 Gbps (per GPU), 476.057 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.843 Gbps (per GPU), 1286.744 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.855 Gbps (per GPU), 1286.841 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.834 Gbps (per GPU), 1286.670 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.809 Gbps (per GPU), 1286.469 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.840 Gbps (per GPU), 1286.720 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.849 Gbps (per GPU), 1286.795 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.831 Gbps (per GPU), 1286.646 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.809 Gbps (per GPU), 1286.473 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.639 Gbps (per GPU), 837.110 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.638 Gbps (per GPU), 837.103 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.639 Gbps (per GPU), 837.110 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.638 Gbps (per GPU), 837.103 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.638 Gbps (per GPU), 837.103 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.640 Gbps (per GPU), 837.117 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.639 Gbps (per GPU), 837.110 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.640 Gbps (per GPU), 837.117 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.354 Gbps (per GPU), 306.835 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.352 Gbps (per GPU), 306.818 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.352 Gbps (per GPU), 306.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.352 Gbps (per GPU), 306.820 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.354 Gbps (per GPU), 306.829 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.353 Gbps (per GPU), 306.827 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.353 Gbps (per GPU), 306.828 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.353 Gbps (per GPU), 306.826 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  4.18ms  0.43ms  0.55ms  9.66  4.31K  0.10M
 chk_1  4.23ms  0.43ms  0.55ms  9.88  4.31K  0.10M
 chk_2  4.24ms  0.41ms  0.54ms 10.25  4.31K  0.06M
 chk_3  4.23ms  0.41ms  0.98ms 10.27  4.31K  0.06M
 chk_4  4.69ms  0.40ms  0.52ms 11.83  4.31K  0.04M
 chk_5  4.22ms  0.39ms  0.52ms 10.80  4.31K  0.04M
 chk_6  4.23ms  0.39ms  0.52ms 10.75  4.31K  0.04M
 chk_7  4.23ms  0.39ms  0.52ms 10.73  4.31K  0.06M
   Avg  4.28  0.41  0.59
   Max  4.69  0.43  0.98
   Min  4.18  0.39  0.52
 Ratio  1.12  1.11  1.89
   Var  0.02  0.00  0.02
Profiling takes 0.565 s
*** Node 0, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 21558, Num Local Vertices: 4312
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 8623, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 30181, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 12935, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 4311, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 17247, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 25870, Num Local Vertices: 4311
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 34236
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.5992	TrainAcc 0.4100	ValidAcc 0.5420	TestAcc 0.5510	BestValid 0.5420
	Epoch 10:	Loss 0.9697	TrainAcc 0.8700	ValidAcc 0.9040	TestAcc 0.9080	BestValid 0.9040
	Epoch 20:	Loss 0.4174	TrainAcc 0.9500	ValidAcc 0.9380	TestAcc 0.9360	BestValid 0.9380
	Epoch 30:	Loss 0.2715	TrainAcc 0.9700	ValidAcc 0.9460	TestAcc 0.9430	BestValid 0.9460
	Epoch 40:	Loss 0.1969	TrainAcc 0.9900	ValidAcc 0.9500	TestAcc 0.9440	BestValid 0.9500
	Epoch 50:	Loss 0.1252	TrainAcc 0.9900	ValidAcc 0.9480	TestAcc 0.9430	BestValid 0.9500
	Epoch 60:	Loss 0.1181	TrainAcc 0.9900	ValidAcc 0.9560	TestAcc 0.9440	BestValid 0.9560
	Epoch 70:	Loss 0.0947	TrainAcc 1.0000	ValidAcc 0.9440	TestAcc 0.9430	BestValid 0.9560
	Epoch 80:	Loss 0.0918	TrainAcc 1.0000	ValidAcc 0.9520	TestAcc 0.9420	BestValid 0.9560
	Epoch 90:	Loss 0.0804	TrainAcc 1.0000	ValidAcc 0.9460	TestAcc 0.9390	BestValid 0.9560
	Epoch 100:	Loss 0.0744	TrainAcc 1.0000	ValidAcc 0.9440	TestAcc 0.9410	BestValid 0.9560
	Epoch 110:	Loss 0.0521	TrainAcc 1.0000	ValidAcc 0.9480	TestAcc 0.9390	BestValid 0.9560
	Epoch 120:	Loss 0.0421	TrainAcc 1.0000	ValidAcc 0.9460	TestAcc 0.9400	BestValid 0.9560
	Epoch 130:	Loss 0.0446	TrainAcc 1.0000	ValidAcc 0.9480	TestAcc 0.9430	BestValid 0.9560
	Epoch 140:	Loss 0.0291	TrainAcc 1.0000	ValidAcc 0.9480	TestAcc 0.9400	BestValid 0.9560
	Epoch 150:	Loss 0.0304	TrainAcc 1.0000	ValidAcc 0.9480	TestAcc 0.9400	BestValid 0.9560
	Epoch 160:	Loss 0.0775	TrainAcc 1.0000	ValidAcc 0.9460	TestAcc 0.9390	BestValid 0.9560
	Epoch 170:	Loss 0.0259	TrainAcc 1.0000	ValidAcc 0.9460	TestAcc 0.9370	BestValid 0.9560
	Epoch 180:	Loss 0.0226	TrainAcc 1.0000	ValidAcc 0.9460	TestAcc 0.9370	BestValid 0.9560
	Epoch 190:	Loss 0.0234	TrainAcc 1.0000	ValidAcc 0.9500	TestAcc 0.9380	BestValid 0.9560
	Epoch 200:	Loss 0.0238	TrainAcc 1.0000	ValidAcc 0.9520	TestAcc 0.9380	BestValid 0.9560
	Epoch 210:	Loss 0.0319	TrainAcc 1.0000	ValidAcc 0.9520	TestAcc 0.9390	BestValid 0.9560
	Epoch 220:	Loss 0.0267	TrainAcc 1.0000	ValidAcc 0.9460	TestAcc 0.9390	BestValid 0.9560
	Epoch 230:	Loss 0.0255	TrainAcc 1.0000	ValidAcc 0.9500	TestAcc 0.9370	BestValid 0.9560
	Epoch 240:	Loss 0.0205	TrainAcc 1.0000	ValidAcc 0.9500	TestAcc 0.9390	BestValid 0.9560
	Epoch 250:	Loss 0.0109	TrainAcc 1.0000	ValidAcc 0.9500	TestAcc 0.9390	BestValid 0.9560
	Epoch 260:	Loss 0.0191	TrainAcc 1.0000	ValidAcc 0.9480	TestAcc 0.9360	BestValid 0.9560
	Epoch 270:	Loss 0.0122	TrainAcc 1.0000	ValidAcc 0.9460	TestAcc 0.9370	BestValid 0.9560
	Epoch 280:	Loss 0.0174	TrainAcc 1.0000	ValidAcc 0.9500	TestAcc 0.9380	BestValid 0.9560
	Epoch 290:	Loss 0.0170	TrainAcc 1.0000	ValidAcc 0.9500	TestAcc 0.9380	BestValid 0.9560
	Epoch 300:	Loss 0.0188	TrainAcc 1.0000	ValidAcc 0.9500	TestAcc 0.9370	BestValid 0.9560
****** Epoch Time (Excluding Evaluation Cost): 0.060 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.111 ms (Max: 0.153, Min: 0.048, Sum: 0.886)
Cluster-Wide Average, Compute: 19.172 ms (Max: 19.477, Min: 18.589, Sum: 153.378)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.010, Min: 0.007, Sum: 0.067)
Cluster-Wide Average, Bubble-Imbalance: 0.014 ms (Max: 0.016, Min: 0.013, Sum: 0.116)
Cluster-Wide Average, Communication-Graph: 36.121 ms (Max: 36.702, Min: 35.826, Sum: 288.970)
Cluster-Wide Average, Optimization: 3.764 ms (Max: 3.784, Min: 3.742, Sum: 30.113)
Cluster-Wide Average, Others: 0.389 ms (Max: 0.417, Min: 0.352, Sum: 3.115)
****** Breakdown Sum: 59.581 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.796 GB (Max: 4.296, Min: 3.712, Sum: 30.370)
Cluster-Wide Average, Graph-Level Communication Throughput: 27.186 Gbps (Max: 41.304, Min: 8.793, Sum: 217.491)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.816 GB
Weight-sync communication (cluster-wide, per-epoch): 0.061 GB
Total communication (cluster-wide, per-epoch): 0.877 GB
****** Accuracy Results ******
Highest valid_acc: 0.9560
Target test_acc: 0.9440
Epoch to reach the target acc: 59
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
