Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7

Initialized node 1 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
        It takes 0.011 seconds.
        It takes 0.014 seconds.
        It takes 0.015 seconds.
        It takes 0.015 seconds.
Building the Feature Vector...
        It takes 0.014 seconds.
        It takes 0.013 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.019 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.467 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.472 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.505 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.511 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.602 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.615 seconds.
Building the Label Vector...
        It takes 0.004 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/physics/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 300
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 8
        It takes 0.617 seconds.
Building the Label Vector...
        It takes 0.615 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 4311) 1-[4311, 8623) 2-[8623, 12935) 3-[12935, 17247) 4-[17247, 21558) 5-[21558, 25870) 6-[25870, 30181) 7-[30181, 34493)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.891 Gbps (per GPU), 487.129 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.585 Gbps (per GPU), 484.676 Gbps (aggregated)
The layer-level communication performance: 60.576 Gbps (per GPU), 484.607 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.296 Gbps (per GPU), 482.367 Gbps (aggregated)
The layer-level communication performance: 60.274 Gbps (per GPU), 482.195 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.054 Gbps (per GPU), 480.432 Gbps (aggregated)
The layer-level communication performance: 60.004 Gbps (per GPU), 480.036 Gbps (aggregated)
The layer-level communication performance: 59.973 Gbps (per GPU), 479.787 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.879 Gbps (per GPU), 1263.035 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.838 Gbps (per GPU), 1262.703 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.770 Gbps (per GPU), 1262.157 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.882 Gbps (per GPU), 1263.059 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.841 Gbps (per GPU), 1262.726 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.604 Gbps (per GPU), 1260.829 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 2): 154.273 Gbps (per GPU), 1234.186 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.273 Gbps (per GPU), 1234.186 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.988 Gbps (per GPU), 839.904 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.990 Gbps (per GPU), 839.918 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.991 Gbps (per GPU), 839.932 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.990 Gbps (per GPU), 839.918 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.989 Gbps (per GPU), 839.911 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.991 Gbps (per GPU), 839.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.991 Gbps (per GPU), 839.931 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.925 Gbps (per GPU), 839.399 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.431 Gbps (per GPU), 307.448 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.432 Gbps (per GPU), 307.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.430 Gbps (per GPU), 307.441 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.431 Gbps (per GPU), 307.448 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.431 Gbps (per GPU), 307.445 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.430 Gbps (per GPU), 307.438 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.432 Gbps (per GPU), 307.452 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.428 Gbps (per GPU), 307.425 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  5.97ms  0.51ms  0.32ms 18.41  4.31K  0.10M
 chk_1  6.05ms  0.51ms  0.33ms 18.54  4.31K  0.10M
 chk_2  6.06ms  0.49ms  0.32ms 19.20  4.31K  0.06M
 chk_3  6.52ms  0.49ms  0.31ms 20.79  4.31K  0.06M
 chk_4  6.04ms  0.47ms  0.29ms 20.68  4.31K  0.04M
 chk_5  6.03ms  0.46ms  0.29ms 20.94  4.31K  0.04M
 chk_6  6.65ms  0.46ms  0.28ms 23.44  4.31K  0.04M
 chk_7  6.03ms  0.47ms  0.29ms 20.65  4.31K  0.06M
   Avg  6.17  0.48  0.30
   Max  6.65  0.51  0.33
   Min  5.97  0.46  0.28
 Ratio  1.11  1.11  1.15
   Var  0.06  0.00  0.00
Profiling takes 0.707 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 4311
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 17247, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 21558, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 30181, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 12935, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 4311, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 8623, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 25870, Num Local Vertices: 4311
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 34236
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.7875	TrainAcc 0.2000	ValidAcc 0.0740	TestAcc 0.0750	BestValid 0.0740
	Epoch 10:	Loss 1.5542	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 20:	Loss 1.4231	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 30:	Loss 1.3709	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 40:	Loss 1.3531	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 50:	Loss 1.3282	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 60:	Loss 1.3121	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 70:	Loss 1.3001	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 80:	Loss 1.2896	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 90:	Loss 1.2815	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 100:	Loss 1.3328	TrainAcc 0.3000	ValidAcc 0.1380	TestAcc 0.1480	BestValid 0.1380
	Epoch 110:	Loss 1.2568	TrainAcc 0.3300	ValidAcc 0.1620	TestAcc 0.1650	BestValid 0.1620
	Epoch 120:	Loss 1.2946	TrainAcc 0.2200	ValidAcc 0.0920	TestAcc 0.1060	BestValid 0.1620
	Epoch 130:	Loss 1.2518	TrainAcc 0.3700	ValidAcc 0.1700	TestAcc 0.1710	BestValid 0.1700
	Epoch 140:	Loss 1.2569	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0910	BestValid 0.1700
	Epoch 150:	Loss 1.2275	TrainAcc 0.3100	ValidAcc 0.1300	TestAcc 0.1410	BestValid 0.1700
	Epoch 160:	Loss 1.2340	TrainAcc 0.3100	ValidAcc 0.1360	TestAcc 0.1390	BestValid 0.1700
	Epoch 170:	Loss 1.2378	TrainAcc 0.3500	ValidAcc 0.1640	TestAcc 0.1640	BestValid 0.1700
	Epoch 180:	Loss 1.2129	TrainAcc 0.3500	ValidAcc 0.1620	TestAcc 0.1580	BestValid 0.1700
	Epoch 190:	Loss 1.2254	TrainAcc 0.3700	ValidAcc 0.1600	TestAcc 0.1590	BestValid 0.1700
	Epoch 200:	Loss 1.1992	TrainAcc 0.3700	ValidAcc 0.1620	TestAcc 0.1590	BestValid 0.1700
	Epoch 210:	Loss 1.1986	TrainAcc 0.3800	ValidAcc 0.1640	TestAcc 0.1560	BestValid 0.1700
	Epoch 220:	Loss 1.1900	TrainAcc 0.3300	ValidAcc 0.1640	TestAcc 0.1610	BestValid 0.1700
	Epoch 230:	Loss 1.1963	TrainAcc 0.3600	ValidAcc 0.1640	TestAcc 0.1550	BestValid 0.1700
	Epoch 240:	Loss 1.2114	TrainAcc 0.3600	ValidAcc 0.1660	TestAcc 0.1590	BestValid 0.1700
	Epoch 250:	Loss 1.1861	TrainAcc 0.3200	ValidAcc 0.1600	TestAcc 0.1590	BestValid 0.1700
	Epoch 260:	Loss 1.1612	TrainAcc 0.3700	ValidAcc 0.1680	TestAcc 0.1570	BestValid 0.1700
	Epoch 270:	Loss 1.1749	TrainAcc 0.3500	ValidAcc 0.1640	TestAcc 0.1600	BestValid 0.1700
	Epoch 280:	Loss 1.2111	TrainAcc 0.3600	ValidAcc 0.1640	TestAcc 0.1590	BestValid 0.1700
	Epoch 290:	Loss 1.1772	TrainAcc 0.3300	ValidAcc 0.1640	TestAcc 0.1600	BestValid 0.1700
	Epoch 300:	Loss 1.1228	TrainAcc 0.3700	ValidAcc 0.1700	TestAcc 0.1640	BestValid 0.1700
****** Epoch Time (Excluding Evaluation Cost): 0.069 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.104 ms (Max: 0.153, Min: 0.043, Sum: 0.830)
Cluster-Wide Average, Compute: 23.856 ms (Max: 24.495, Min: 23.328, Sum: 190.852)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.010, Min: 0.007, Sum: 0.065)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.121)
Cluster-Wide Average, Communication-Graph: 37.049 ms (Max: 37.582, Min: 36.377, Sum: 296.393)
Cluster-Wide Average, Optimization: 7.253 ms (Max: 7.281, Min: 7.227, Sum: 58.027)
Cluster-Wide Average, Others: 0.696 ms (Max: 0.761, Min: 0.655, Sum: 5.572)
****** Breakdown Sum: 68.982 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.768 GB (Max: 4.204, Min: 3.692, Sum: 30.142)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.471 Gbps (Max: 41.020, Min: 8.570, Sum: 211.771)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.816 GB
Weight-sync communication (cluster-wide, per-epoch): 0.119 GB
Total communication (cluster-wide, per-epoch): 0.935 GB
****** Accuracy Results ******
Highest valid_acc: 0.1700
Target test_acc: 0.1710
Epoch to reach the target acc: 129
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
