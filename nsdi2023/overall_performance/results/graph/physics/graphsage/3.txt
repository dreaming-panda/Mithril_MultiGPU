Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INITDONE MPI INIT
Initialized node 7 on machine gnerv8

Initialized node 5 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INITInitialized node 0 on machine gnerv7

Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
        It takes 0.011 seconds.
        It takes 0.012 seconds.
        It takes 0.012 seconds.
        It takes 0.015 seconds.
        It takes 0.015 seconds.
Building the Feature Vector...
        It takes 0.016 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.478 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.487 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.502 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.508 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.575 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.616 seconds.
Building the Label Vector...
        It takes 0.614 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/physics/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 300
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 8
        It takes 0.004 seconds.
        It takes 0.622 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 4311) 1-[4311, 8623) 2-[8623, 12935) 3-[12935, 17247) 4-[17247, 21558) 5-[21558, 25870) 6-[25870, 30181) 7-[30181, 34493)
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.304 Gbps (per GPU), 482.433 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.006 Gbps (per GPU), 480.051 Gbps (aggregated)
The layer-level communication performance: 60.006 Gbps (per GPU), 480.052 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.740 Gbps (per GPU), 477.918 Gbps (aggregated)
The layer-level communication performance: 59.715 Gbps (per GPU), 477.718 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.500 Gbps (per GPU), 476.002 Gbps (aggregated)
The layer-level communication performance: 59.447 Gbps (per GPU), 475.580 Gbps (aggregated)
The layer-level communication performance: 59.423 Gbps (per GPU), 475.382 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.644 Gbps (per GPU), 1269.150 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.620 Gbps (per GPU), 1268.959 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.635 Gbps (per GPU), 1269.078 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.620 Gbps (per GPU), 1268.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.611 Gbps (per GPU), 1268.889 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.611 Gbps (per GPU), 1268.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.632 Gbps (per GPU), 1269.060 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.536 Gbps (per GPU), 1268.287 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.682 Gbps (per GPU), 837.452 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.680 Gbps (per GPU), 837.437 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.682 Gbps (per GPU), 837.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.661 Gbps (per GPU), 837.291 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.681 Gbps (per GPU), 837.451 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.666 Gbps (per GPU), 837.327 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.609 Gbps (per GPU), 836.873 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.591 Gbps (per GPU), 836.727 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.563 Gbps (per GPU), 308.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.563 Gbps (per GPU), 308.508 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.563 Gbps (per GPU), 308.507 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.563 Gbps (per GPU), 308.502 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.561 Gbps (per GPU), 308.490 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.563 Gbps (per GPU), 308.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.563 Gbps (per GPU), 308.502 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.552 Gbps (per GPU), 308.414 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  5.95ms  0.49ms  0.31ms 19.21  4.31K  0.10M
 chk_1  6.05ms  0.49ms  0.32ms 19.20  4.31K  0.10M
 chk_2  6.03ms  0.47ms  0.30ms 20.37  4.31K  0.06M
 chk_3  6.03ms  0.47ms  0.30ms 20.37  4.31K  0.06M
 chk_4  6.01ms  0.45ms  0.27ms 22.13  4.31K  0.04M
 chk_5  5.96ms  0.45ms  0.28ms 21.68  4.31K  0.04M
 chk_6  6.02ms  0.45ms  0.27ms 22.43  4.31K  0.04M
 chk_7  6.03ms  0.45ms  0.28ms 21.82  4.31K  0.06M
   Avg  6.01  0.46  0.29
   Max  6.05  0.49  0.32
   Min  5.95  0.45  0.27
 Ratio  1.02  1.09  1.17
   Var  0.00  0.00  0.00
Profiling takes 0.700 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 3, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 21558, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 25870, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 30181, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 4311, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 12935, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 17247, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 8623, Num Local Vertices: 4312
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 34236
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2409	TrainAcc 0.2100	ValidAcc 0.1220	TestAcc 0.1060	BestValid 0.1220
	Epoch 10:	Loss 1.5688	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.1220
	Epoch 20:	Loss 1.4949	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.1220
	Epoch 30:	Loss 1.4079	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.1220
	Epoch 40:	Loss 1.3476	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.1220
	Epoch 50:	Loss 1.3543	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.1220
	Epoch 60:	Loss 1.3365	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.1220
	Epoch 70:	Loss 1.3242	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.1220
	Epoch 80:	Loss 1.2909	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.1220
	Epoch 90:	Loss 1.2749	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.1220
	Epoch 100:	Loss 1.2939	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0930	BestValid 0.1220
	Epoch 110:	Loss 1.2581	TrainAcc 0.2200	ValidAcc 0.0960	TestAcc 0.0990	BestValid 0.1220
	Epoch 120:	Loss 1.2599	TrainAcc 0.2500	ValidAcc 0.1060	TestAcc 0.1200	BestValid 0.1220
	Epoch 130:	Loss 1.3680	TrainAcc 0.3700	ValidAcc 0.1600	TestAcc 0.1590	BestValid 0.1600
	Epoch 140:	Loss 1.2634	TrainAcc 0.2100	ValidAcc 0.0860	TestAcc 0.0930	BestValid 0.1600
	Epoch 150:	Loss 1.2571	TrainAcc 0.3200	ValidAcc 0.1560	TestAcc 0.1640	BestValid 0.1600
	Epoch 160:	Loss 1.2538	TrainAcc 0.3700	ValidAcc 0.1640	TestAcc 0.1580	BestValid 0.1640
	Epoch 170:	Loss 1.2195	TrainAcc 0.4000	ValidAcc 0.1660	TestAcc 0.1530	BestValid 0.1660
	Epoch 180:	Loss 1.2345	TrainAcc 0.3900	ValidAcc 0.1660	TestAcc 0.1570	BestValid 0.1660
	Epoch 190:	Loss 1.2069	TrainAcc 0.3800	ValidAcc 0.1680	TestAcc 0.1560	BestValid 0.1680
	Epoch 200:	Loss 1.2245	TrainAcc 0.3900	ValidAcc 0.1680	TestAcc 0.1540	BestValid 0.1680
	Epoch 210:	Loss 1.2137	TrainAcc 0.4000	ValidAcc 0.1680	TestAcc 0.1520	BestValid 0.1680
	Epoch 220:	Loss 1.1880	TrainAcc 0.3700	ValidAcc 0.1660	TestAcc 0.1570	BestValid 0.1680
	Epoch 230:	Loss 1.1997	TrainAcc 0.3900	ValidAcc 0.1700	TestAcc 0.1540	BestValid 0.1700
	Epoch 240:	Loss 1.2027	TrainAcc 0.3900	ValidAcc 0.1680	TestAcc 0.1540	BestValid 0.1700
	Epoch 250:	Loss 1.2277	TrainAcc 0.3900	ValidAcc 0.1700	TestAcc 0.1550	BestValid 0.1700
	Epoch 260:	Loss 1.2244	TrainAcc 0.4000	ValidAcc 0.1640	TestAcc 0.1570	BestValid 0.1700
	Epoch 270:	Loss 1.2146	TrainAcc 0.3900	ValidAcc 0.1680	TestAcc 0.1570	BestValid 0.1700
	Epoch 280:	Loss 1.2008	TrainAcc 0.3400	ValidAcc 0.1620	TestAcc 0.1560	BestValid 0.1700
	Epoch 290:	Loss 1.1480	TrainAcc 0.3800	ValidAcc 0.1680	TestAcc 0.1570	BestValid 0.1700
	Epoch 300:	Loss 1.2466	TrainAcc 0.3300	ValidAcc 0.1640	TestAcc 0.1580	BestValid 0.1700
****** Epoch Time (Excluding Evaluation Cost): 0.068 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.112 ms (Max: 0.172, Min: 0.053, Sum: 0.896)
Cluster-Wide Average, Compute: 23.902 ms (Max: 24.483, Min: 23.412, Sum: 191.213)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.065)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.122)
Cluster-Wide Average, Communication-Graph: 36.411 ms (Max: 36.928, Min: 35.824, Sum: 291.288)
Cluster-Wide Average, Optimization: 7.245 ms (Max: 7.269, Min: 7.224, Sum: 57.959)
Cluster-Wide Average, Others: 0.702 ms (Max: 0.761, Min: 0.654, Sum: 5.615)
****** Breakdown Sum: 68.395 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.768 GB (Max: 4.204, Min: 3.692, Sum: 30.142)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.924 Gbps (Max: 41.962, Min: 8.613, Sum: 215.388)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.816 GB
Weight-sync communication (cluster-wide, per-epoch): 0.119 GB
Total communication (cluster-wide, per-epoch): 0.935 GB
****** Accuracy Results ******
Highest valid_acc: 0.1700
Target test_acc: 0.1540
Epoch to reach the target acc: 229
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
