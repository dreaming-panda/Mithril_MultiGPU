Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 7 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 3 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7

Initialized node 1 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.067 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.066 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
        It takes 0.055 seconds.
        It takes 0.051 seconds.
        It takes 0.050 seconds.
        It takes 0.054 seconds.
        It takes 0.051 seconds.
        It takes 0.053 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.070 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.025 seconds.
        It takes 0.028 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.360 Gbps (per GPU), 482.884 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.060 Gbps (per GPU), 480.482 Gbps (aggregated)
The layer-level communication performance: 60.051 Gbps (per GPU), 480.411 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.810 Gbps (per GPU), 478.481 Gbps (aggregated)
The layer-level communication performance: 59.778 Gbps (per GPU), 478.223 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.548 Gbps (per GPU), 476.380 Gbps (aggregated)
The layer-level communication performance: 59.496 Gbps (per GPU), 475.967 Gbps (aggregated)
The layer-level communication performance: 59.468 Gbps (per GPU), 475.743 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.249 Gbps (per GPU), 1265.990 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.237 Gbps (per GPU), 1265.894 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.234 Gbps (per GPU), 1265.873 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.237 Gbps (per GPU), 1265.894 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.249 Gbps (per GPU), 1265.989 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.243 Gbps (per GPU), 1265.942 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.237 Gbps (per GPU), 1265.896 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.243 Gbps (per GPU), 1265.942 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.360 Gbps (per GPU), 834.882 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.359 Gbps (per GPU), 834.874 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.354 Gbps (per GPU), 834.833 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.361 Gbps (per GPU), 834.888 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.354 Gbps (per GPU), 834.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.361 Gbps (per GPU), 834.888 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.358 Gbps (per GPU), 834.867 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.333 Gbps (per GPU), 834.667 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.793 Gbps (per GPU), 310.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.792 Gbps (per GPU), 310.339 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.794 Gbps (per GPU), 310.349 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.793 Gbps (per GPU), 310.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.794 Gbps (per GPU), 310.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.789 Gbps (per GPU), 310.310 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.791 Gbps (per GPU), 310.326 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.790 Gbps (per GPU), 310.324 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.52ms  1.36ms  0.86ms  1.77 21.17K  0.46M
 chk_1  2.23ms  1.44ms  0.93ms  2.38 21.17K  0.55M
 chk_2  1.50ms  1.32ms  0.82ms  1.83 21.17K  0.39M
 chk_3  1.43ms  1.27ms  0.78ms  1.85 21.17K  0.24M
 chk_4  1.38ms  1.22ms  0.73ms  1.91 21.17K  0.17M
 chk_5  1.39ms  1.23ms  0.75ms  1.85 21.17K  0.22M
 chk_6  1.40ms  1.23ms  0.73ms  1.90 21.17K  0.16M
 chk_7  1.38ms  1.22ms  0.72ms  1.92 21.17K  0.12M
   Avg  1.53  1.29  0.79
   Max  2.23  1.44  0.93
   Min  1.38  1.22  0.72
 Ratio  1.61  1.19  1.30
   Var  0.07  0.01  0.00
Profiling takes 0.431 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 21167
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 105839, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 21167, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 84671, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 127007, Num Local Vertices: 21168
Num Stages: 1 / 1
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 42335, Num Local Vertices: 21168
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 148175, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 63503, Num Local Vertices: 21168
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 114305
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 13.0647	TrainAcc 0.0057	ValidAcc 0.0063	TestAcc 0.0078	BestValid 0.0063
	Epoch 50:	Loss 3.6700	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 100:	Loss 2.9234	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 150:	Loss 2.8623	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 200:	Loss 2.8377	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 250:	Loss 2.8356	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 300:	Loss 2.8278	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 350:	Loss 2.8451	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 400:	Loss 2.8137	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 450:	Loss 2.8188	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 500:	Loss 2.8071	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 550:	Loss 2.8083	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 600:	Loss 2.8043	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 650:	Loss 2.7862	TrainAcc 0.1790	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 700:	Loss 2.7774	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 750:	Loss 2.7373	TrainAcc 0.1099	ValidAcc 0.2295	TestAcc 0.2155	BestValid 0.2295
	Epoch 800:	Loss 2.6894	TrainAcc 0.1470	ValidAcc 0.2451	TestAcc 0.2291	BestValid 0.2451
	Epoch 850:	Loss 2.6018	TrainAcc 0.1512	ValidAcc 0.2465	TestAcc 0.2303	BestValid 0.2465
	Epoch 900:	Loss 2.5522	TrainAcc 0.1527	ValidAcc 0.2472	TestAcc 0.2306	BestValid 0.2472
	Epoch 950:	Loss 2.5693	TrainAcc 0.1518	ValidAcc 0.2465	TestAcc 0.2298	BestValid 0.2472
	Epoch 1000:	Loss 2.4873	TrainAcc 0.1514	ValidAcc 0.2461	TestAcc 0.2297	BestValid 0.2472
	Epoch 1050:	Loss 2.4983	TrainAcc 0.1512	ValidAcc 0.2457	TestAcc 0.2295	BestValid 0.2472
	Epoch 1100:	Loss 2.4078	TrainAcc 0.1503	ValidAcc 0.2452	TestAcc 0.2289	BestValid 0.2472
	Epoch 1150:	Loss 2.4326	TrainAcc 0.1527	ValidAcc 0.2462	TestAcc 0.2299	BestValid 0.2472
	Epoch 1200:	Loss 2.4298	TrainAcc 0.1530	ValidAcc 0.2462	TestAcc 0.2298	BestValid 0.2472
	Epoch 1250:	Loss 2.4357	TrainAcc 0.1535	ValidAcc 0.2466	TestAcc 0.2298	BestValid 0.2472
	Epoch 1300:	Loss 2.3531	TrainAcc 0.1539	ValidAcc 0.2464	TestAcc 0.2297	BestValid 0.2472
	Epoch 1350:	Loss 2.3677	TrainAcc 0.1568	ValidAcc 0.2471	TestAcc 0.2301	BestValid 0.2472
	Epoch 1400:	Loss 2.3336	TrainAcc 0.1572	ValidAcc 0.2474	TestAcc 0.2303	BestValid 0.2474
	Epoch 1450:	Loss 2.3654	TrainAcc 0.0917	ValidAcc 0.1129	TestAcc 0.1100	BestValid 0.2474
	Epoch 1500:	Loss 2.2894	TrainAcc 0.0919	ValidAcc 0.1127	TestAcc 0.1099	BestValid 0.2474
	Epoch 1550:	Loss 2.2874	TrainAcc 0.0932	ValidAcc 0.1138	TestAcc 0.1107	BestValid 0.2474
	Epoch 1600:	Loss 2.2558	TrainAcc 0.0938	ValidAcc 0.1138	TestAcc 0.1107	BestValid 0.2474
	Epoch 1650:	Loss 2.2206	TrainAcc 0.0940	ValidAcc 0.1138	TestAcc 0.1110	BestValid 0.2474
	Epoch 1700:	Loss 2.4794	TrainAcc 0.0945	ValidAcc 0.1136	TestAcc 0.1109	BestValid 0.2474
	Epoch 1750:	Loss 2.2367	TrainAcc 0.0958	ValidAcc 0.1146	TestAcc 0.1115	BestValid 0.2474
	Epoch 1800:	Loss 2.1742	TrainAcc 0.0959	ValidAcc 0.1140	TestAcc 0.1117	BestValid 0.2474
	Epoch 1850:	Loss 2.1511	TrainAcc 0.0713	ValidAcc 0.1030	TestAcc 0.1015	BestValid 0.2474
	Epoch 1900:	Loss 2.1632	TrainAcc 0.0959	ValidAcc 0.1127	TestAcc 0.1105	BestValid 0.2474
	Epoch 1950:	Loss 2.1214	TrainAcc 0.0675	ValidAcc 0.1021	TestAcc 0.1004	BestValid 0.2474
	Epoch 2000:	Loss 2.1100	TrainAcc 0.0652	ValidAcc 0.1013	TestAcc 0.0995	BestValid 0.2474
	Epoch 2050:	Loss 2.1621	TrainAcc 0.0642	ValidAcc 0.1005	TestAcc 0.0993	BestValid 0.2474
	Epoch 2100:	Loss 2.0792	TrainAcc 0.0583	ValidAcc 0.0997	TestAcc 0.0981	BestValid 0.2474
	Epoch 2150:	Loss 2.1188	TrainAcc 0.0635	ValidAcc 0.1010	TestAcc 0.0996	BestValid 0.2474
	Epoch 2200:	Loss 2.0783	TrainAcc 0.0613	ValidAcc 0.1010	TestAcc 0.0996	BestValid 0.2474
	Epoch 2250:	Loss 2.0460	TrainAcc 0.0624	ValidAcc 0.1012	TestAcc 0.1002	BestValid 0.2474
	Epoch 2300:	Loss 2.1077	TrainAcc 0.0595	ValidAcc 0.1004	TestAcc 0.0988	BestValid 0.2474
	Epoch 2350:	Loss 2.0829	TrainAcc 0.0652	ValidAcc 0.1029	TestAcc 0.1015	BestValid 0.2474
	Epoch 2400:	Loss 2.0413	TrainAcc 0.0712	ValidAcc 0.1044	TestAcc 0.1031	BestValid 0.2474
	Epoch 2450:	Loss 2.0319	TrainAcc 0.0583	ValidAcc 0.1021	TestAcc 0.1001	BestValid 0.2474
	Epoch 2500:	Loss 2.0292	TrainAcc 0.0643	ValidAcc 0.1018	TestAcc 0.1007	BestValid 0.2474
	Epoch 2550:	Loss 2.0003	TrainAcc 0.0732	ValidAcc 0.1054	TestAcc 0.1047	BestValid 0.2474
	Epoch 2600:	Loss 2.0203	TrainAcc 0.0615	ValidAcc 0.1010	TestAcc 0.1000	BestValid 0.2474
	Epoch 2650:	Loss 1.9730	TrainAcc 0.0671	ValidAcc 0.1030	TestAcc 0.1020	BestValid 0.2474
	Epoch 2700:	Loss 1.9863	TrainAcc 0.0653	ValidAcc 0.1028	TestAcc 0.1017	BestValid 0.2474
	Epoch 2750:	Loss 1.9817	TrainAcc 0.0622	ValidAcc 0.1015	TestAcc 0.1006	BestValid 0.2474
	Epoch 2800:	Loss 2.0012	TrainAcc 0.0650	ValidAcc 0.1025	TestAcc 0.1017	BestValid 0.2474
	Epoch 2850:	Loss 1.9653	TrainAcc 0.0670	ValidAcc 0.1038	TestAcc 0.1026	BestValid 0.2474
	Epoch 2900:	Loss 1.9825	TrainAcc 0.0641	ValidAcc 0.1028	TestAcc 0.1016	BestValid 0.2474
	Epoch 2950:	Loss 1.9510	TrainAcc 0.0656	ValidAcc 0.1036	TestAcc 0.1023	BestValid 0.2474
	Epoch 3000:	Loss 1.9455	TrainAcc 0.0637	ValidAcc 0.1025	TestAcc 0.1015	BestValid 0.2474
	Epoch 3050:	Loss 1.9400	TrainAcc 0.0669	ValidAcc 0.1033	TestAcc 0.1021	BestValid 0.2474
	Epoch 3100:	Loss 1.9355	TrainAcc 0.0721	ValidAcc 0.1060	TestAcc 0.1050	BestValid 0.2474
	Epoch 3150:	Loss 1.9270	TrainAcc 0.0654	ValidAcc 0.1017	TestAcc 0.1008	BestValid 0.2474
	Epoch 3200:	Loss 1.9141	TrainAcc 0.0656	ValidAcc 0.1020	TestAcc 0.1016	BestValid 0.2474
	Epoch 3250:	Loss 1.9137	TrainAcc 0.0691	ValidAcc 0.1031	TestAcc 0.1026	BestValid 0.2474
	Epoch 3300:	Loss 1.9005	TrainAcc 0.0709	ValidAcc 0.1036	TestAcc 0.1030	BestValid 0.2474
	Epoch 3350:	Loss 1.9082	TrainAcc 0.0711	ValidAcc 0.1039	TestAcc 0.1034	BestValid 0.2474
	Epoch 3400:	Loss 1.8811	TrainAcc 0.0717	ValidAcc 0.1042	TestAcc 0.1042	BestValid 0.2474
	Epoch 3450:	Loss 1.9017	TrainAcc 0.0718	ValidAcc 0.1043	TestAcc 0.1043	BestValid 0.2474
	Epoch 3500:	Loss 1.8527	TrainAcc 0.0783	ValidAcc 0.1069	TestAcc 0.1067	BestValid 0.2474
	Epoch 3550:	Loss 1.8465	TrainAcc 0.0721	ValidAcc 0.1041	TestAcc 0.1035	BestValid 0.2474
	Epoch 3600:	Loss 1.8698	TrainAcc 0.0713	ValidAcc 0.1042	TestAcc 0.1027	BestValid 0.2474
	Epoch 3650:	Loss 1.8599	TrainAcc 0.0701	ValidAcc 0.1045	TestAcc 0.1039	BestValid 0.2474
	Epoch 3700:	Loss 1.8449	TrainAcc 0.0748	ValidAcc 0.1060	TestAcc 0.1048	BestValid 0.2474
	Epoch 3750:	Loss 1.8376	TrainAcc 0.0777	ValidAcc 0.1069	TestAcc 0.1055	BestValid 0.2474
	Epoch 3800:	Loss 1.8408	TrainAcc 0.0754	ValidAcc 0.1061	TestAcc 0.1049	BestValid 0.2474
	Epoch 3850:	Loss 1.8066	TrainAcc 0.0793	ValidAcc 0.1094	TestAcc 0.1075	BestValid 0.2474
	Epoch 3900:	Loss 1.8489	TrainAcc 0.0841	ValidAcc 0.1123	TestAcc 0.1089	BestValid 0.2474
	Epoch 3950:	Loss 1.8339	TrainAcc 0.0819	ValidAcc 0.1107	TestAcc 0.1065	BestValid 0.2474
	Epoch 4000:	Loss 1.7953	TrainAcc 0.0894	ValidAcc 0.1129	TestAcc 0.1095	BestValid 0.2474
	Epoch 4050:	Loss 1.7792	TrainAcc 0.0909	ValidAcc 0.1150	TestAcc 0.1121	BestValid 0.2474
	Epoch 4100:	Loss 1.7633	TrainAcc 0.0899	ValidAcc 0.1145	TestAcc 0.1106	BestValid 0.2474
	Epoch 4150:	Loss 1.8275	TrainAcc 0.1031	ValidAcc 0.1204	TestAcc 0.1159	BestValid 0.2474
	Epoch 4200:	Loss 1.7644	TrainAcc 0.1102	ValidAcc 0.1242	TestAcc 0.1197	BestValid 0.2474
	Epoch 4250:	Loss 1.7619	TrainAcc 0.1065	ValidAcc 0.1210	TestAcc 0.1171	BestValid 0.2474
	Epoch 4300:	Loss 1.7534	TrainAcc 0.1020	ValidAcc 0.1207	TestAcc 0.1156	BestValid 0.2474
	Epoch 4350:	Loss 1.7272	TrainAcc 0.1151	ValidAcc 0.1250	TestAcc 0.1200	BestValid 0.2474
	Epoch 4400:	Loss 1.7419	TrainAcc 0.1212	ValidAcc 0.1271	TestAcc 0.1215	BestValid 0.2474
	Epoch 4450:	Loss 1.7379	TrainAcc 0.1172	ValidAcc 0.1266	TestAcc 0.1227	BestValid 0.2474
	Epoch 4500:	Loss 1.7391	TrainAcc 0.1187	ValidAcc 0.1277	TestAcc 0.1223	BestValid 0.2474
	Epoch 4550:	Loss 1.7398	TrainAcc 0.1194	ValidAcc 0.1272	TestAcc 0.1216	BestValid 0.2474
	Epoch 4600:	Loss 1.7634	TrainAcc 0.1228	ValidAcc 0.1295	TestAcc 0.1238	BestValid 0.2474
	Epoch 4650:	Loss 1.7138	TrainAcc 0.1329	ValidAcc 0.1341	TestAcc 0.1272	BestValid 0.2474
	Epoch 4700:	Loss 1.7050	TrainAcc 0.1282	ValidAcc 0.1303	TestAcc 0.1246	BestValid 0.2474
	Epoch 4750:	Loss 1.6909	TrainAcc 0.1293	ValidAcc 0.1314	TestAcc 0.1256	BestValid 0.2474
	Epoch 4800:	Loss 1.7297	TrainAcc 0.1364	ValidAcc 0.1372	TestAcc 0.1318	BestValid 0.2474
	Epoch 4850:	Loss 1.6880	TrainAcc 0.1271	ValidAcc 0.1312	TestAcc 0.1257	BestValid 0.2474
	Epoch 4900:	Loss 1.7595	TrainAcc 0.1482	ValidAcc 0.1546	TestAcc 0.1510	BestValid 0.2474
	Epoch 4950:	Loss 1.7005	TrainAcc 0.1583	ValidAcc 0.1780	TestAcc 0.1761	BestValid 0.2474
	Epoch 5000:	Loss 1.6783	TrainAcc 0.1385	ValidAcc 0.1390	TestAcc 0.1332	BestValid 0.2474
****** Epoch Time (Excluding Evaluation Cost): 0.147 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.314 ms (Max: 0.550, Min: 0.034, Sum: 2.509)
Cluster-Wide Average, Compute: 45.283 ms (Max: 48.471, Min: 43.948, Sum: 362.267)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.064)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.119)
Cluster-Wide Average, Communication-Graph: 95.316 ms (Max: 96.466, Min: 92.397, Sum: 762.526)
Cluster-Wide Average, Optimization: 5.079 ms (Max: 5.099, Min: 5.042, Sum: 40.631)
Cluster-Wide Average, Others: 1.107 ms (Max: 1.142, Min: 1.040, Sum: 8.860)
****** Breakdown Sum: 147.122 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.456 GB (Max: 5.952, Min: 5.372, Sum: 43.647)
Cluster-Wide Average, Graph-Level Communication Throughput: 33.396 Gbps (Max: 64.611, Min: 10.778, Sum: 267.168)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.725 GB
Weight-sync communication (cluster-wide, per-epoch): 0.033 GB
Total communication (cluster-wide, per-epoch): 2.758 GB
****** Accuracy Results ******
Highest valid_acc: 0.2474
Target test_acc: 0.2303
Epoch to reach the target acc: 1399
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
