Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv8

Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INIT
Initialized node 0 on machine gnerv7

Initialized node 1 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.067 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
        It takes 0.054 seconds.
        It takes 0.056 seconds.
        It takes 0.051 seconds.
        It takes 0.049 seconds.
        It takes 0.055 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.027 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.025 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 61.328 Gbps (per GPU), 490.624 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 61.037 Gbps (per GPU), 488.295 Gbps (aggregated)
The layer-level communication performance: 61.032 Gbps (per GPU), 488.256 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.771 Gbps (per GPU), 486.165 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.522 Gbps (per GPU), 484.179 Gbps (aggregated)
The layer-level communication performance: 60.473 Gbps (per GPU), 483.784 Gbps (aggregated)
The layer-level communication performance: 60.440 Gbps (per GPU), 483.524 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.736 Gbps (per GPU), 485.888 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.386 Gbps (per GPU), 1267.090 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.377 Gbps (per GPU), 1267.018 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.374 Gbps (per GPU), 1266.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.380 Gbps (per GPU), 1267.042 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.389 Gbps (per GPU), 1267.113 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.384 Gbps (per GPU), 1267.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.377 Gbps (per GPU), 1267.018 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.380 Gbps (per GPU), 1267.042 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.181 Gbps (per GPU), 833.450 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.174 Gbps (per GPU), 833.395 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.181 Gbps (per GPU), 833.449 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.176 Gbps (per GPU), 833.409 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.182 Gbps (per GPU), 833.457 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.155 Gbps (per GPU), 833.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.181 Gbps (per GPU), 833.450 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.140 Gbps (per GPU), 833.119 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.849 Gbps (per GPU), 286.792 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.850 Gbps (per GPU), 286.802 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.849 Gbps (per GPU), 286.792 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.846 Gbps (per GPU), 286.767 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.848 Gbps (per GPU), 286.785 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.849 Gbps (per GPU), 286.791 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.843 Gbps (per GPU), 286.746 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.842 Gbps (per GPU), 286.737 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.52ms  1.35ms  0.85ms  1.78 21.17K  0.46M
 chk_1  1.59ms  1.43ms  0.93ms  1.71 21.17K  0.55M
 chk_2  1.47ms  1.32ms  0.81ms  1.81 21.17K  0.39M
 chk_3  1.43ms  1.27ms  0.77ms  1.85 21.17K  0.24M
 chk_4  1.38ms  1.22ms  0.72ms  1.92 21.17K  0.17M
 chk_5  1.39ms  1.23ms  0.73ms  1.90 21.17K  0.22M
 chk_6  1.39ms  1.23ms  0.73ms  1.91 21.17K  0.16M
 chk_7  1.46ms  1.21ms  0.71ms  2.05 21.17K  0.12M
   Avg  1.45  1.28  0.78
   Max  1.59  1.43  0.93
   Min  1.38  1.21  0.71
 Ratio  1.15  1.18  1.30
   Var  0.00  0.01  0.01
Profiling takes 0.425 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 21167
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 42335, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 84671, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 21167, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 105839, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 63503, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 148175, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 127007, Num Local Vertices: 21168
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 114305
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 11.7302	TrainAcc 0.0248	ValidAcc 0.0100	TestAcc 0.0098	BestValid 0.0100
	Epoch 50:	Loss 3.6464	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 100:	Loss 3.1224	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 150:	Loss 2.8718	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 200:	Loss 2.8481	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 250:	Loss 2.8465	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 300:	Loss 2.8284	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 350:	Loss 2.8229	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 400:	Loss 2.8386	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 450:	Loss 2.8133	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 500:	Loss 2.8113	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 550:	Loss 2.8283	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 600:	Loss 2.7908	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 650:	Loss 2.7638	TrainAcc 0.1100	ValidAcc 0.2297	TestAcc 0.2155	BestValid 0.2297
	Epoch 700:	Loss 2.7282	TrainAcc 0.1100	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 750:	Loss 2.6613	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 800:	Loss 2.7294	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 850:	Loss 2.5949	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 900:	Loss 2.6161	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 950:	Loss 2.5800	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 1000:	Loss 2.5703	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 1050:	Loss 2.5469	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 1100:	Loss 2.5108	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 1150:	Loss 2.4759	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 1200:	Loss 2.4510	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 1250:	Loss 2.4669	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1300:	Loss 2.4799	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1350:	Loss 2.4684	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1400:	Loss 2.4019	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1450:	Loss 2.4125	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1500:	Loss 2.3991	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1550:	Loss 2.4090	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1600:	Loss 2.3645	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1650:	Loss 2.4952	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1700:	Loss 2.3718	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1750:	Loss 2.3287	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1800:	Loss 2.3515	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1850:	Loss 2.3093	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1900:	Loss 2.3605	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 1950:	Loss 2.4482	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2000:	Loss 2.3201	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2050:	Loss 2.3235	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2100:	Loss 2.3436	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2150:	Loss 2.3140	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2200:	Loss 2.2799	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2250:	Loss 2.2629	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2300:	Loss 2.2480	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2350:	Loss 2.2675	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2400:	Loss 2.2611	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2450:	Loss 2.2466	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2500:	Loss 2.2297	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2550:	Loss 2.2564	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2600:	Loss 2.2129	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2650:	Loss 2.2049	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2700:	Loss 2.2044	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2750:	Loss 2.1728	TrainAcc 0.0477	ValidAcc 0.0956	TestAcc 0.0953	BestValid 0.2297
	Epoch 2800:	Loss 2.1279	TrainAcc 0.0477	ValidAcc 0.0958	TestAcc 0.0954	BestValid 0.2297
	Epoch 2850:	Loss 2.0977	TrainAcc 0.0571	ValidAcc 0.0995	TestAcc 0.0981	BestValid 0.2297
	Epoch 2900:	Loss 2.1172	TrainAcc 0.0643	ValidAcc 0.1020	TestAcc 0.0998	BestValid 0.2297
	Epoch 2950:	Loss 2.0553	TrainAcc 0.0696	ValidAcc 0.1034	TestAcc 0.1016	BestValid 0.2297
	Epoch 3000:	Loss 2.1010	TrainAcc 0.0629	ValidAcc 0.1014	TestAcc 0.0993	BestValid 0.2297
	Epoch 3050:	Loss 2.0400	TrainAcc 0.0659	ValidAcc 0.1020	TestAcc 0.1004	BestValid 0.2297
	Epoch 3100:	Loss 2.0031	TrainAcc 0.0687	ValidAcc 0.1032	TestAcc 0.1009	BestValid 0.2297
	Epoch 3150:	Loss 1.9832	TrainAcc 0.0495	ValidAcc 0.0967	TestAcc 0.0962	BestValid 0.2297
	Epoch 3200:	Loss 1.9787	TrainAcc 0.0768	ValidAcc 0.1051	TestAcc 0.1035	BestValid 0.2297
	Epoch 3250:	Loss 1.9624	TrainAcc 0.0520	ValidAcc 0.0976	TestAcc 0.0968	BestValid 0.2297
	Epoch 3300:	Loss 1.9532	TrainAcc 0.0553	ValidAcc 0.0987	TestAcc 0.0975	BestValid 0.2297
	Epoch 3350:	Loss 1.9441	TrainAcc 0.0590	ValidAcc 0.0996	TestAcc 0.0987	BestValid 0.2297
	Epoch 3400:	Loss 1.9774	TrainAcc 0.0614	ValidAcc 0.0998	TestAcc 0.0987	BestValid 0.2297
	Epoch 3450:	Loss 1.9168	TrainAcc 0.0607	ValidAcc 0.0998	TestAcc 0.0984	BestValid 0.2297
	Epoch 3500:	Loss 1.9010	TrainAcc 0.0815	ValidAcc 0.1074	TestAcc 0.1055	BestValid 0.2297
	Epoch 3550:	Loss 1.9062	TrainAcc 0.0812	ValidAcc 0.1074	TestAcc 0.1053	BestValid 0.2297
	Epoch 3600:	Loss 1.8711	TrainAcc 0.0835	ValidAcc 0.1081	TestAcc 0.1059	BestValid 0.2297
	Epoch 3650:	Loss 1.8770	TrainAcc 0.0829	ValidAcc 0.1084	TestAcc 0.1062	BestValid 0.2297
	Epoch 3700:	Loss 1.8797	TrainAcc 0.0847	ValidAcc 0.1086	TestAcc 0.1064	BestValid 0.2297
	Epoch 3750:	Loss 1.9038	TrainAcc 0.0842	ValidAcc 0.1084	TestAcc 0.1065	BestValid 0.2297
	Epoch 3800:	Loss 1.8421	TrainAcc 0.0869	ValidAcc 0.1092	TestAcc 0.1075	BestValid 0.2297
	Epoch 3850:	Loss 1.8301	TrainAcc 0.0851	ValidAcc 0.1085	TestAcc 0.1069	BestValid 0.2297
	Epoch 3900:	Loss 1.8324	TrainAcc 0.0853	ValidAcc 0.1084	TestAcc 0.1068	BestValid 0.2297
	Epoch 3950:	Loss 1.8348	TrainAcc 0.0900	ValidAcc 0.1105	TestAcc 0.1084	BestValid 0.2297
	Epoch 4000:	Loss 1.8285	TrainAcc 0.0925	ValidAcc 0.1121	TestAcc 0.1098	BestValid 0.2297
	Epoch 4050:	Loss 1.8826	TrainAcc 0.0871	ValidAcc 0.1095	TestAcc 0.1077	BestValid 0.2297
	Epoch 4100:	Loss 1.8179	TrainAcc 0.0886	ValidAcc 0.1102	TestAcc 0.1083	BestValid 0.2297
	Epoch 4150:	Loss 1.8393	TrainAcc 0.0913	ValidAcc 0.1116	TestAcc 0.1093	BestValid 0.2297
	Epoch 4200:	Loss 1.8294	TrainAcc 0.0907	ValidAcc 0.1105	TestAcc 0.1087	BestValid 0.2297
	Epoch 4250:	Loss 1.7896	TrainAcc 0.0909	ValidAcc 0.1106	TestAcc 0.1088	BestValid 0.2297
	Epoch 4300:	Loss 1.7926	TrainAcc 0.0918	ValidAcc 0.1109	TestAcc 0.1091	BestValid 0.2297
	Epoch 4350:	Loss 1.7849	TrainAcc 0.0904	ValidAcc 0.1108	TestAcc 0.1088	BestValid 0.2297
	Epoch 4400:	Loss 1.7771	TrainAcc 0.0913	ValidAcc 0.1112	TestAcc 0.1092	BestValid 0.2297
	Epoch 4450:	Loss 1.7736	TrainAcc 0.0924	ValidAcc 0.1121	TestAcc 0.1100	BestValid 0.2297
	Epoch 4500:	Loss 1.7625	TrainAcc 0.0914	ValidAcc 0.1114	TestAcc 0.1096	BestValid 0.2297
	Epoch 4550:	Loss 1.7623	TrainAcc 0.0936	ValidAcc 0.1117	TestAcc 0.1097	BestValid 0.2297
	Epoch 4600:	Loss 1.7870	TrainAcc 0.0934	ValidAcc 0.1121	TestAcc 0.1100	BestValid 0.2297
	Epoch 4650:	Loss 1.7812	TrainAcc 0.0918	ValidAcc 0.1109	TestAcc 0.1091	BestValid 0.2297
	Epoch 4700:	Loss 1.7469	TrainAcc 0.0949	ValidAcc 0.1133	TestAcc 0.1108	BestValid 0.2297
	Epoch 4750:	Loss 1.7479	TrainAcc 0.0951	ValidAcc 0.1128	TestAcc 0.1109	BestValid 0.2297
	Epoch 4800:	Loss 1.7324	TrainAcc 0.0936	ValidAcc 0.1121	TestAcc 0.1100	BestValid 0.2297
	Epoch 4850:	Loss 1.7379	TrainAcc 0.0971	ValidAcc 0.1134	TestAcc 0.1118	BestValid 0.2297
	Epoch 4900:	Loss 1.7396	TrainAcc 0.0958	ValidAcc 0.1126	TestAcc 0.1107	BestValid 0.2297
	Epoch 4950:	Loss 1.7130	TrainAcc 0.1015	ValidAcc 0.1153	TestAcc 0.1124	BestValid 0.2297
	Epoch 5000:	Loss 1.7037	TrainAcc 0.1013	ValidAcc 0.1156	TestAcc 0.1119	BestValid 0.2297
****** Epoch Time (Excluding Evaluation Cost): 0.147 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.319 ms (Max: 0.569, Min: 0.034, Sum: 2.550)
Cluster-Wide Average, Compute: 45.101 ms (Max: 48.376, Min: 43.610, Sum: 360.807)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.117)
Cluster-Wide Average, Communication-Graph: 95.440 ms (Max: 96.740, Min: 92.421, Sum: 763.517)
Cluster-Wide Average, Optimization: 5.074 ms (Max: 5.102, Min: 5.044, Sum: 40.591)
Cluster-Wide Average, Others: 1.084 ms (Max: 1.134, Min: 1.034, Sum: 8.670)
****** Breakdown Sum: 147.039 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.456 GB (Max: 5.952, Min: 5.372, Sum: 43.647)
Cluster-Wide Average, Graph-Level Communication Throughput: 33.369 Gbps (Max: 64.534, Min: 10.682, Sum: 266.950)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.725 GB
Weight-sync communication (cluster-wide, per-epoch): 0.033 GB
Total communication (cluster-wide, per-epoch): 2.758 GB
****** Accuracy Results ******
Highest valid_acc: 0.2297
Target test_acc: 0.2156
Epoch to reach the target acc: 699
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
