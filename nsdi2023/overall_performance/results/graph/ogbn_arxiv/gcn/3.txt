Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INITDONE MPI INIT
Initialized node 5 on machine gnerv8

Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
        It takes 0.053 seconds.
        It takes 0.050 seconds.
        It takes 0.054 seconds.
        It takes 0.050 seconds.
        It takes 0.050 seconds.
        It takes 0.053 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.069 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
        It takes 0.071 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/ogbn_arxiv/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
        It takes 0.029 seconds.
        It takes 0.024 seconds.
        It takes 0.028 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 61.342 Gbps (per GPU), 490.738 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 61.050 Gbps (per GPU), 488.401 Gbps (aggregated)
The layer-level communication performance: 61.041 Gbps (per GPU), 488.329 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.781 Gbps (per GPU), 486.245 Gbps (aggregated)
The layer-level communication performance: 60.747 Gbps (per GPU), 485.979 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.536 Gbps (per GPU), 484.289 Gbps (aggregated)
The layer-level communication performance: 60.485 Gbps (per GPU), 483.880 Gbps (aggregated)
The layer-level communication performance: 60.450 Gbps (per GPU), 483.603 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.165 Gbps (per GPU), 1273.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.159 Gbps (per GPU), 1273.268 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.095 Gbps (per GPU), 1272.759 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.159 Gbps (per GPU), 1273.268 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.162 Gbps (per GPU), 1273.292 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.165 Gbps (per GPU), 1273.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.095 Gbps (per GPU), 1272.761 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.183 Gbps (per GPU), 1273.461 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.883 Gbps (per GPU), 807.063 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.876 Gbps (per GPU), 807.004 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.870 Gbps (per GPU), 806.959 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.863 Gbps (per GPU), 806.907 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.875 Gbps (per GPU), 806.998 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.876 Gbps (per GPU), 807.011 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.873 Gbps (per GPU), 806.985 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.880 Gbps (per GPU), 807.037 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.071 Gbps (per GPU), 312.569 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.071 Gbps (per GPU), 312.567 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.071 Gbps (per GPU), 312.567 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.066 Gbps (per GPU), 312.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.065 Gbps (per GPU), 312.523 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.065 Gbps (per GPU), 312.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.063 Gbps (per GPU), 312.502 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.064 Gbps (per GPU), 312.512 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.81ms  0.95ms  0.59ms  1.62 21.17K  0.46M
 chk_1  0.88ms  0.94ms  0.66ms  1.42 21.17K  0.55M
 chk_2  0.77ms  0.83ms  0.55ms  1.50 21.17K  0.39M
 chk_3  0.73ms  0.78ms  0.51ms  1.55 21.17K  0.24M
 chk_4  0.68ms  0.74ms  0.46ms  1.61 21.17K  0.17M
 chk_5  0.68ms  0.74ms  0.46ms  1.60 21.17K  0.22M
 chk_6  0.69ms  0.74ms  0.47ms  1.59 21.17K  0.16M
 chk_7  0.67ms  0.73ms  0.45ms  1.62 21.17K  0.12M
   Avg  0.74  0.81  0.52
   Max  0.88  0.95  0.66
   Min  0.67  0.73  0.45
 Ratio  1.31  1.31  1.47
   Var  0.01  0.01  0.01
Profiling takes 0.275 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 21167
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 21167, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 148175, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 105839, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 42335, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 63503, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 127007, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 84671, Num Local Vertices: 21168
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 114305
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 6, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.6889	TrainAcc 0.0756	ValidAcc 0.1538	TestAcc 0.1361	BestValid 0.1538
	Epoch 50:	Loss 2.8300	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 100:	Loss 2.7025	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 150:	Loss 2.5747	TrainAcc 0.1098	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 200:	Loss 2.5079	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 250:	Loss 2.5367	TrainAcc 0.1098	ValidAcc 0.2298	TestAcc 0.2156	BestValid 0.2298
	Epoch 300:	Loss 2.4219	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2298
	Epoch 350:	Loss 3.1626	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.2298
	Epoch 400:	Loss 2.8413	TrainAcc 0.1795	ValidAcc 0.0763	TestAcc 0.0588	BestValid 0.2298
	Epoch 450:	Loss 2.7195	TrainAcc 0.1100	ValidAcc 0.2297	TestAcc 0.2155	BestValid 0.2298
	Epoch 500:	Loss 2.4876	TrainAcc 0.1199	ValidAcc 0.2349	TestAcc 0.2188	BestValid 0.2349
	Epoch 550:	Loss 2.4381	TrainAcc 0.1195	ValidAcc 0.2349	TestAcc 0.2188	BestValid 0.2349
	Epoch 600:	Loss 2.3880	TrainAcc 0.1198	ValidAcc 0.2349	TestAcc 0.2189	BestValid 0.2349
	Epoch 650:	Loss 2.3473	TrainAcc 0.1230	ValidAcc 0.2370	TestAcc 0.2200	BestValid 0.2370
	Epoch 700:	Loss 2.3500	TrainAcc 0.0567	ValidAcc 0.1007	TestAcc 0.0986	BestValid 0.2370
	Epoch 750:	Loss 2.3207	TrainAcc 0.1373	ValidAcc 0.2874	TestAcc 0.2757	BestValid 0.2874
	Epoch 800:	Loss 2.3514	TrainAcc 0.0593	ValidAcc 0.1032	TestAcc 0.1001	BestValid 0.2874
	Epoch 850:	Loss 2.4279	TrainAcc 0.1607	ValidAcc 0.3205	TestAcc 0.3044	BestValid 0.3205
	Epoch 900:	Loss 2.1484	TrainAcc 0.0681	ValidAcc 0.1075	TestAcc 0.1048	BestValid 0.3205
	Epoch 950:	Loss 2.1485	TrainAcc 0.1202	ValidAcc 0.1286	TestAcc 0.1232	BestValid 0.3205
	Epoch 1000:	Loss 2.0542	TrainAcc 0.1389	ValidAcc 0.1399	TestAcc 0.1327	BestValid 0.3205
	Epoch 1050:	Loss 2.0106	TrainAcc 0.1414	ValidAcc 0.1412	TestAcc 0.1347	BestValid 0.3205
	Epoch 1100:	Loss 2.9116	TrainAcc 0.1682	ValidAcc 0.2550	TestAcc 0.2362	BestValid 0.3205
	Epoch 1150:	Loss 2.5689	TrainAcc 0.2405	ValidAcc 0.1069	TestAcc 0.0832	BestValid 0.3205
	Epoch 1200:	Loss 2.4020	TrainAcc 0.1850	ValidAcc 0.2784	TestAcc 0.2559	BestValid 0.3205
	Epoch 1250:	Loss 2.1585	TrainAcc 0.1264	ValidAcc 0.1399	TestAcc 0.1339	BestValid 0.3205
	Epoch 1300:	Loss 2.1267	TrainAcc 0.1129	ValidAcc 0.1283	TestAcc 0.1240	BestValid 0.3205
	Epoch 1350:	Loss 2.0331	TrainAcc 0.1721	ValidAcc 0.1608	TestAcc 0.1556	BestValid 0.3205
	Epoch 1400:	Loss 1.9962	TrainAcc 0.1873	ValidAcc 0.1745	TestAcc 0.1702	BestValid 0.3205
	Epoch 1450:	Loss 1.9785	TrainAcc 0.1809	ValidAcc 0.1659	TestAcc 0.1616	BestValid 0.3205
	Epoch 1500:	Loss 1.9623	TrainAcc 0.1857	ValidAcc 0.1695	TestAcc 0.1641	BestValid 0.3205
	Epoch 1550:	Loss 1.9484	TrainAcc 0.2047	ValidAcc 0.1881	TestAcc 0.1849	BestValid 0.3205
	Epoch 1600:	Loss 1.9212	TrainAcc 0.2156	ValidAcc 0.2018	TestAcc 0.2052	BestValid 0.3205
	Epoch 1650:	Loss 1.9017	TrainAcc 0.2005	ValidAcc 0.1785	TestAcc 0.1742	BestValid 0.3205
	Epoch 1700:	Loss 1.8989	TrainAcc 0.1973	ValidAcc 0.1768	TestAcc 0.1741	BestValid 0.3205
	Epoch 1750:	Loss 1.9080	TrainAcc 0.1986	ValidAcc 0.1785	TestAcc 0.1749	BestValid 0.3205
	Epoch 1800:	Loss 1.8751	TrainAcc 0.2099	ValidAcc 0.2037	TestAcc 0.2094	BestValid 0.3205
	Epoch 1850:	Loss 1.8930	TrainAcc 0.2158	ValidAcc 0.2110	TestAcc 0.2148	BestValid 0.3205
	Epoch 1900:	Loss 1.9216	TrainAcc 0.2663	ValidAcc 0.3557	TestAcc 0.3637	BestValid 0.3557
	Epoch 1950:	Loss 1.8644	TrainAcc 0.2220	ValidAcc 0.2280	TestAcc 0.2313	BestValid 0.3557
	Epoch 2000:	Loss 1.8584	TrainAcc 0.2056	ValidAcc 0.2066	TestAcc 0.2087	BestValid 0.3557
	Epoch 2050:	Loss 1.9203	TrainAcc 0.2056	ValidAcc 0.1922	TestAcc 0.1889	BestValid 0.3557
	Epoch 2100:	Loss 1.8348	TrainAcc 0.2044	ValidAcc 0.2174	TestAcc 0.2211	BestValid 0.3557
	Epoch 2150:	Loss 1.8356	TrainAcc 0.2041	ValidAcc 0.1961	TestAcc 0.1927	BestValid 0.3557
	Epoch 2200:	Loss 1.8176	TrainAcc 0.2106	ValidAcc 0.2094	TestAcc 0.2105	BestValid 0.3557
	Epoch 2250:	Loss 1.8424	TrainAcc 0.2439	ValidAcc 0.2865	TestAcc 0.2923	BestValid 0.3557
	Epoch 2300:	Loss 1.8005	TrainAcc 0.2007	ValidAcc 0.1932	TestAcc 0.1875	BestValid 0.3557
	Epoch 2350:	Loss 1.8533	TrainAcc 0.1983	ValidAcc 0.1801	TestAcc 0.1742	BestValid 0.3557
	Epoch 2400:	Loss 1.8050	TrainAcc 0.2044	ValidAcc 0.2194	TestAcc 0.2233	BestValid 0.3557
	Epoch 2450:	Loss 1.7964	TrainAcc 0.2402	ValidAcc 0.2950	TestAcc 0.3013	BestValid 0.3557
	Epoch 2500:	Loss 1.7904	TrainAcc 0.2796	ValidAcc 0.3968	TestAcc 0.4022	BestValid 0.3968
	Epoch 2550:	Loss 1.7774	TrainAcc 0.2832	ValidAcc 0.4099	TestAcc 0.4230	BestValid 0.4099
	Epoch 2600:	Loss 2.5389	TrainAcc 0.2714	ValidAcc 0.1291	TestAcc 0.1078	BestValid 0.4099
	Epoch 2650:	Loss 2.2152	TrainAcc 0.3515	ValidAcc 0.3656	TestAcc 0.3658	BestValid 0.4099
	Epoch 2700:	Loss 2.2914	TrainAcc 0.3707	ValidAcc 0.3573	TestAcc 0.3253	BestValid 0.4099
	Epoch 2750:	Loss 2.0697	TrainAcc 0.4222	ValidAcc 0.4292	TestAcc 0.4337	BestValid 0.4292
	Epoch 2800:	Loss 2.0106	TrainAcc 0.4505	ValidAcc 0.4733	TestAcc 0.4944	BestValid 0.4733
	Epoch 2850:	Loss 1.9454	TrainAcc 0.4534	ValidAcc 0.4939	TestAcc 0.5063	BestValid 0.4939
	Epoch 2900:	Loss 1.9154	TrainAcc 0.3763	ValidAcc 0.3233	TestAcc 0.3378	BestValid 0.4939
	Epoch 2950:	Loss 1.8799	TrainAcc 0.4259	ValidAcc 0.4504	TestAcc 0.4493	BestValid 0.4939
	Epoch 3000:	Loss 1.8502	TrainAcc 0.3533	ValidAcc 0.2685	TestAcc 0.2599	BestValid 0.4939
	Epoch 3050:	Loss 1.8210	TrainAcc 0.3287	ValidAcc 0.2345	TestAcc 0.2240	BestValid 0.4939
	Epoch 3100:	Loss 1.8084	TrainAcc 0.3643	ValidAcc 0.2631	TestAcc 0.2466	BestValid 0.4939
	Epoch 3150:	Loss 1.7960	TrainAcc 0.3717	ValidAcc 0.2669	TestAcc 0.2507	BestValid 0.4939
	Epoch 3200:	Loss 1.8023	TrainAcc 0.3458	ValidAcc 0.2459	TestAcc 0.2308	BestValid 0.4939
	Epoch 3250:	Loss 1.7923	TrainAcc 0.2991	ValidAcc 0.2371	TestAcc 0.2276	BestValid 0.4939
	Epoch 3300:	Loss 1.7772	TrainAcc 0.3184	ValidAcc 0.2540	TestAcc 0.2489	BestValid 0.4939
	Epoch 3350:	Loss 1.7748	TrainAcc 0.3349	ValidAcc 0.2457	TestAcc 0.2334	BestValid 0.4939
	Epoch 3400:	Loss 1.7579	TrainAcc 0.3738	ValidAcc 0.3040	TestAcc 0.2889	BestValid 0.4939
	Epoch 3450:	Loss 1.7590	TrainAcc 0.4538	ValidAcc 0.4887	TestAcc 0.4873	BestValid 0.4939
	Epoch 3500:	Loss 1.7554	TrainAcc 0.3858	ValidAcc 0.3424	TestAcc 0.3336	BestValid 0.4939
	Epoch 3550:	Loss 1.7753	TrainAcc 0.4189	ValidAcc 0.4386	TestAcc 0.4356	BestValid 0.4939
	Epoch 3600:	Loss 1.7680	TrainAcc 0.4101	ValidAcc 0.4172	TestAcc 0.4167	BestValid 0.4939
	Epoch 3650:	Loss 1.7421	TrainAcc 0.3706	ValidAcc 0.3398	TestAcc 0.3379	BestValid 0.4939
	Epoch 3700:	Loss 1.7494	TrainAcc 0.4167	ValidAcc 0.4446	TestAcc 0.4462	BestValid 0.4939
	Epoch 3750:	Loss 1.7348	TrainAcc 0.4067	ValidAcc 0.3863	TestAcc 0.3986	BestValid 0.4939
	Epoch 3800:	Loss 1.7326	TrainAcc 0.4319	ValidAcc 0.4686	TestAcc 0.4808	BestValid 0.4939
	Epoch 3850:	Loss 1.7570	TrainAcc 0.4114	ValidAcc 0.3843	TestAcc 0.3950	BestValid 0.4939
	Epoch 3900:	Loss 1.7015	TrainAcc 0.4458	ValidAcc 0.4795	TestAcc 0.4939	BestValid 0.4939
	Epoch 3950:	Loss 1.7154	TrainAcc 0.4313	ValidAcc 0.4382	TestAcc 0.4469	BestValid 0.4939
	Epoch 4000:	Loss 1.7046	TrainAcc 0.4250	ValidAcc 0.4669	TestAcc 0.4779	BestValid 0.4939
	Epoch 4050:	Loss 1.6938	TrainAcc 0.4444	ValidAcc 0.4811	TestAcc 0.4949	BestValid 0.4939
	Epoch 4100:	Loss 1.7161	TrainAcc 0.4574	ValidAcc 0.5009	TestAcc 0.5093	BestValid 0.5009
	Epoch 4150:	Loss 1.7327	TrainAcc 0.4608	ValidAcc 0.4874	TestAcc 0.4863	BestValid 0.5009
	Epoch 4200:	Loss 1.6724	TrainAcc 0.4271	ValidAcc 0.4690	TestAcc 0.4830	BestValid 0.5009
	Epoch 4250:	Loss 1.6857	TrainAcc 0.4574	ValidAcc 0.5023	TestAcc 0.5162	BestValid 0.5023
	Epoch 4300:	Loss 1.7010	TrainAcc 0.4531	ValidAcc 0.4712	TestAcc 0.4890	BestValid 0.5023
	Epoch 4350:	Loss 1.6788	TrainAcc 0.4439	ValidAcc 0.4749	TestAcc 0.4799	BestValid 0.5023
	Epoch 4400:	Loss 1.6823	TrainAcc 0.4483	ValidAcc 0.4774	TestAcc 0.4819	BestValid 0.5023
	Epoch 4450:	Loss 1.6776	TrainAcc 0.4503	ValidAcc 0.4591	TestAcc 0.4843	BestValid 0.5023
	Epoch 4500:	Loss 1.6549	TrainAcc 0.4681	ValidAcc 0.5140	TestAcc 0.5313	BestValid 0.5140
	Epoch 4550:	Loss 1.6666	TrainAcc 0.4702	ValidAcc 0.5168	TestAcc 0.5334	BestValid 0.5168
	Epoch 4600:	Loss 1.6722	TrainAcc 0.4715	ValidAcc 0.5129	TestAcc 0.5331	BestValid 0.5168
	Epoch 4650:	Loss 1.6621	TrainAcc 0.4752	ValidAcc 0.5134	TestAcc 0.5220	BestValid 0.5168
	Epoch 4700:	Loss 1.6707	TrainAcc 0.4734	ValidAcc 0.5045	TestAcc 0.5098	BestValid 0.5168
	Epoch 4750:	Loss 1.6482	TrainAcc 0.4776	ValidAcc 0.5147	TestAcc 0.5333	BestValid 0.5168
	Epoch 4800:	Loss 1.6530	TrainAcc 0.4923	ValidAcc 0.5298	TestAcc 0.5448	BestValid 0.5298
	Epoch 4850:	Loss 1.6814	TrainAcc 0.4786	ValidAcc 0.5237	TestAcc 0.5286	BestValid 0.5298
	Epoch 4900:	Loss 1.6332	TrainAcc 0.4763	ValidAcc 0.5116	TestAcc 0.5329	BestValid 0.5298
	Epoch 4950:	Loss 1.6461	TrainAcc 0.4894	ValidAcc 0.5282	TestAcc 0.5468	BestValid 0.5298
	Epoch 5000:	Loss 1.6245	TrainAcc 0.4869	ValidAcc 0.5258	TestAcc 0.5469	BestValid 0.5298
****** Epoch Time (Excluding Evaluation Cost): 0.124 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.293 ms (Max: 0.523, Min: 0.033, Sum: 2.343)
Cluster-Wide Average, Compute: 26.096 ms (Max: 28.893, Min: 24.619, Sum: 208.768)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.010, Min: 0.007, Sum: 0.064)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.013, Sum: 0.118)
Cluster-Wide Average, Communication-Graph: 94.449 ms (Max: 95.757, Min: 91.959, Sum: 755.594)
Cluster-Wide Average, Optimization: 2.540 ms (Max: 2.560, Min: 2.521, Sum: 20.321)
Cluster-Wide Average, Others: 0.785 ms (Max: 0.831, Min: 0.753, Sum: 6.278)
****** Breakdown Sum: 124.186 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.179 GB (Max: 4.606, Min: 4.104, Sum: 33.429)
Cluster-Wide Average, Graph-Level Communication Throughput: 33.720 Gbps (Max: 64.807, Min: 10.820, Sum: 269.764)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.725 GB
Weight-sync communication (cluster-wide, per-epoch): 0.017 GB
Total communication (cluster-wide, per-epoch): 2.742 GB
****** Accuracy Results ******
Highest valid_acc: 0.5298
Target test_acc: 0.5448
Epoch to reach the target acc: 4799
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
