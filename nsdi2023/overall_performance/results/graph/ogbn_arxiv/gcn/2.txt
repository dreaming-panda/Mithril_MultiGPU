Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8

Initialized node 4 on machine gnerv8
DONE MPI INITDONE MPI INIT

Initialized node 1 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv7

Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.066 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.050 seconds.
        It takes 0.055 seconds.
        It takes 0.051 seconds.
        It takes 0.056 seconds.
        It takes 0.052 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.069 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.075 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/ogbn_arxiv/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.371 Gbps (per GPU), 482.965 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.061 Gbps (per GPU), 480.488 Gbps (aggregated)
The layer-level communication performance: 60.066 Gbps (per GPU), 480.525 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.803 Gbps (per GPU), 478.423 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.773 Gbps (per GPU), 478.182 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.564 Gbps (per GPU), 476.516 Gbps (aggregated)
The layer-level communication performance: 59.514 Gbps (per GPU), 476.112 Gbps (aggregated)
The layer-level communication performance: 59.480 Gbps (per GPU), 475.841 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.877 Gbps (per GPU), 1279.019 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.868 Gbps (per GPU), 1278.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.877 Gbps (per GPU), 1279.019 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.859 Gbps (per GPU), 1278.873 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.874 Gbps (per GPU), 1278.995 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.853 Gbps (per GPU), 1278.827 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.869 Gbps (per GPU), 1278.950 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.871 Gbps (per GPU), 1278.971 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.803 Gbps (per GPU), 838.421 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.803 Gbps (per GPU), 838.428 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.803 Gbps (per GPU), 838.428 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.803 Gbps (per GPU), 838.428 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.799 Gbps (per GPU), 838.393 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.802 Gbps (per GPU), 838.414 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.802 Gbps (per GPU), 838.414 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.792 Gbps (per GPU), 838.337 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.647 Gbps (per GPU), 293.175 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.647 Gbps (per GPU), 293.178 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.647 Gbps (per GPU), 293.178 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.644 Gbps (per GPU), 293.152 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.645 Gbps (per GPU), 293.160 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.643 Gbps (per GPU), 293.142 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.641 Gbps (per GPU), 293.129 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.643 Gbps (per GPU), 293.148 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.81ms  0.86ms  0.58ms  1.47 21.17K  0.46M
 chk_1  0.88ms  0.95ms  0.68ms  1.40 21.17K  0.55M
 chk_2  0.77ms  0.82ms  0.55ms  1.49 21.17K  0.39M
 chk_3  0.75ms  0.78ms  0.50ms  1.55 21.17K  0.24M
 chk_4  0.68ms  0.73ms  0.45ms  1.61 21.17K  0.17M
 chk_5  0.70ms  0.76ms  0.48ms  1.57 21.17K  0.22M
 chk_6  0.68ms  0.74ms  0.47ms  1.59 21.17K  0.16M
 chk_7  0.67ms  0.74ms  0.45ms  1.64 21.17K  0.12M
   Avg  0.74  0.80  0.52
   Max  0.88  0.95  0.68
   Min  0.67  0.73  0.45
 Ratio  1.32  1.30  1.51
   Var  0.00  0.01  0.01
Profiling takes 0.275 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 21167
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 84671, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 105839, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 63503, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 21167, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 127007, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 42335, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 148175, Num Local Vertices: 21168
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 114305
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.6889	TrainAcc 0.0092	ValidAcc 0.0091	TestAcc 0.0071	BestValid 0.0091
	Epoch 50:	Loss 2.8620	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 100:	Loss 2.7167	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 150:	Loss 2.6205	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 200:	Loss 2.5200	TrainAcc 0.1098	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 250:	Loss 2.4508	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 300:	Loss 3.4272	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 350:	Loss 2.5125	TrainAcc 0.2884	ValidAcc 0.3095	TestAcc 0.2816	BestValid 0.3095
	Epoch 400:	Loss 2.4137	TrainAcc 0.3017	ValidAcc 0.3236	TestAcc 0.3056	BestValid 0.3236
	Epoch 450:	Loss 2.3818	TrainAcc 0.3075	ValidAcc 0.3345	TestAcc 0.3285	BestValid 0.3345
	Epoch 500:	Loss 2.3937	TrainAcc 0.1382	ValidAcc 0.2624	TestAcc 0.2703	BestValid 0.3345
	Epoch 550:	Loss 2.3298	TrainAcc 0.1456	ValidAcc 0.2975	TestAcc 0.3350	BestValid 0.3345
	Epoch 600:	Loss 2.3234	TrainAcc 0.1466	ValidAcc 0.3044	TestAcc 0.3488	BestValid 0.3345
	Epoch 650:	Loss 2.2939	TrainAcc 0.1306	ValidAcc 0.2702	TestAcc 0.3377	BestValid 0.3345
	Epoch 700:	Loss 2.2606	TrainAcc 0.1436	ValidAcc 0.3021	TestAcc 0.3629	BestValid 0.3345
	Epoch 750:	Loss 2.2106	TrainAcc 0.1382	ValidAcc 0.2889	TestAcc 0.3530	BestValid 0.3345
	Epoch 800:	Loss 2.1609	TrainAcc 0.1953	ValidAcc 0.3168	TestAcc 0.3487	BestValid 0.3345
	Epoch 850:	Loss 2.0993	TrainAcc 0.3944	ValidAcc 0.4141	TestAcc 0.4339	BestValid 0.4141
	Epoch 900:	Loss 2.0764	TrainAcc 0.3983	ValidAcc 0.4180	TestAcc 0.4445	BestValid 0.4180
	Epoch 950:	Loss 2.0806	TrainAcc 0.3771	ValidAcc 0.4101	TestAcc 0.4366	BestValid 0.4180
	Epoch 1000:	Loss 2.0309	TrainAcc 0.3702	ValidAcc 0.3868	TestAcc 0.3872	BestValid 0.4180
	Epoch 1050:	Loss 2.0010	TrainAcc 0.3789	ValidAcc 0.4102	TestAcc 0.4345	BestValid 0.4180
	Epoch 1100:	Loss 2.1238	TrainAcc 0.4063	ValidAcc 0.4132	TestAcc 0.4145	BestValid 0.4180
	Epoch 1150:	Loss 1.9483	TrainAcc 0.3564	ValidAcc 0.3928	TestAcc 0.4110	BestValid 0.4180
	Epoch 1200:	Loss 1.9830	TrainAcc 0.3815	ValidAcc 0.3819	TestAcc 0.4184	BestValid 0.4180
	Epoch 1250:	Loss 1.9044	TrainAcc 0.3767	ValidAcc 0.4048	TestAcc 0.4325	BestValid 0.4180
	Epoch 1300:	Loss 1.9068	TrainAcc 0.3821	ValidAcc 0.4006	TestAcc 0.4053	BestValid 0.4180
	Epoch 1350:	Loss 1.8917	TrainAcc 0.3956	ValidAcc 0.4090	TestAcc 0.4102	BestValid 0.4180
	Epoch 1400:	Loss 1.8530	TrainAcc 0.3573	ValidAcc 0.4052	TestAcc 0.4333	BestValid 0.4180
	Epoch 1450:	Loss 1.8628	TrainAcc 0.4049	ValidAcc 0.4192	TestAcc 0.4196	BestValid 0.4192
	Epoch 1500:	Loss 1.8204	TrainAcc 0.3540	ValidAcc 0.4043	TestAcc 0.4095	BestValid 0.4192
	Epoch 1550:	Loss 1.8513	TrainAcc 0.3824	ValidAcc 0.4188	TestAcc 0.4271	BestValid 0.4192
	Epoch 1600:	Loss 1.7804	TrainAcc 0.3859	ValidAcc 0.4113	TestAcc 0.4082	BestValid 0.4192
	Epoch 1650:	Loss 1.8611	TrainAcc 0.3802	ValidAcc 0.4202	TestAcc 0.4280	BestValid 0.4202
	Epoch 1700:	Loss 1.7887	TrainAcc 0.3503	ValidAcc 0.4106	TestAcc 0.4297	BestValid 0.4202
	Epoch 1750:	Loss 1.7989	TrainAcc 0.3516	ValidAcc 0.3994	TestAcc 0.4098	BestValid 0.4202
	Epoch 1800:	Loss 1.7516	TrainAcc 0.3772	ValidAcc 0.4008	TestAcc 0.3953	BestValid 0.4202
	Epoch 1850:	Loss 1.7785	TrainAcc 0.4065	ValidAcc 0.4163	TestAcc 0.4142	BestValid 0.4202
	Epoch 1900:	Loss 1.7406	TrainAcc 0.3917	ValidAcc 0.4244	TestAcc 0.4458	BestValid 0.4244
	Epoch 1950:	Loss 1.7666	TrainAcc 0.3888	ValidAcc 0.4242	TestAcc 0.4475	BestValid 0.4244
	Epoch 2000:	Loss 1.7333	TrainAcc 0.4151	ValidAcc 0.4242	TestAcc 0.4247	BestValid 0.4244
	Epoch 2050:	Loss 1.7182	TrainAcc 0.3845	ValidAcc 0.4054	TestAcc 0.4036	BestValid 0.4244
	Epoch 2100:	Loss 1.7099	TrainAcc 0.4130	ValidAcc 0.4342	TestAcc 0.4408	BestValid 0.4342
	Epoch 2150:	Loss 1.6965	TrainAcc 0.3933	ValidAcc 0.3959	TestAcc 0.4191	BestValid 0.4342
	Epoch 2200:	Loss 1.7475	TrainAcc 0.3088	ValidAcc 0.3913	TestAcc 0.3974	BestValid 0.4342
	Epoch 2250:	Loss 1.7002	TrainAcc 0.4187	ValidAcc 0.4340	TestAcc 0.4435	BestValid 0.4342
	Epoch 2300:	Loss 1.7005	TrainAcc 0.3950	ValidAcc 0.4150	TestAcc 0.4173	BestValid 0.4342
	Epoch 2350:	Loss 1.6950	TrainAcc 0.4089	ValidAcc 0.4133	TestAcc 0.4050	BestValid 0.4342
	Epoch 2400:	Loss 1.6816	TrainAcc 0.4251	ValidAcc 0.4295	TestAcc 0.4310	BestValid 0.4342
	Epoch 2450:	Loss 1.6647	TrainAcc 0.4163	ValidAcc 0.4213	TestAcc 0.4173	BestValid 0.4342
	Epoch 2500:	Loss 1.6832	TrainAcc 0.4114	ValidAcc 0.4097	TestAcc 0.3955	BestValid 0.4342
	Epoch 2550:	Loss 1.6609	TrainAcc 0.4104	ValidAcc 0.4147	TestAcc 0.4049	BestValid 0.4342
	Epoch 2600:	Loss 1.6731	TrainAcc 0.4068	ValidAcc 0.4113	TestAcc 0.4154	BestValid 0.4342
	Epoch 2650:	Loss 1.6905	TrainAcc 0.4051	ValidAcc 0.4124	TestAcc 0.4030	BestValid 0.4342
	Epoch 2700:	Loss 1.6369	TrainAcc 0.3973	ValidAcc 0.4111	TestAcc 0.4045	BestValid 0.4342
	Epoch 2750:	Loss 1.6593	TrainAcc 0.4252	ValidAcc 0.4234	TestAcc 0.4161	BestValid 0.4342
	Epoch 2800:	Loss 1.6455	TrainAcc 0.3862	ValidAcc 0.3946	TestAcc 0.3953	BestValid 0.4342
	Epoch 2850:	Loss 1.6418	TrainAcc 0.4130	ValidAcc 0.4162	TestAcc 0.4206	BestValid 0.4342
	Epoch 2900:	Loss 1.6306	TrainAcc 0.4074	ValidAcc 0.4151	TestAcc 0.4266	BestValid 0.4342
	Epoch 2950:	Loss 1.6467	TrainAcc 0.3969	ValidAcc 0.4018	TestAcc 0.4032	BestValid 0.4342
	Epoch 3000:	Loss 1.6341	TrainAcc 0.4188	ValidAcc 0.4240	TestAcc 0.4201	BestValid 0.4342
	Epoch 3050:	Loss 1.6123	TrainAcc 0.4073	ValidAcc 0.4165	TestAcc 0.4232	BestValid 0.4342
	Epoch 3100:	Loss 1.6389	TrainAcc 0.4194	ValidAcc 0.4385	TestAcc 0.4491	BestValid 0.4385
	Epoch 3150:	Loss 1.6069	TrainAcc 0.4008	ValidAcc 0.4134	TestAcc 0.4156	BestValid 0.4385
	Epoch 3200:	Loss 1.6182	TrainAcc 0.4203	ValidAcc 0.4158	TestAcc 0.4101	BestValid 0.4385
	Epoch 3250:	Loss 1.6071	TrainAcc 0.4031	ValidAcc 0.3946	TestAcc 0.3726	BestValid 0.4385
	Epoch 3300:	Loss 1.6168	TrainAcc 0.4062	ValidAcc 0.4048	TestAcc 0.3964	BestValid 0.4385
	Epoch 3350:	Loss 1.6206	TrainAcc 0.4213	ValidAcc 0.4249	TestAcc 0.4214	BestValid 0.4385
	Epoch 3400:	Loss 1.6112	TrainAcc 0.3881	ValidAcc 0.3611	TestAcc 0.3558	BestValid 0.4385
	Epoch 3450:	Loss 1.6375	TrainAcc 0.4178	ValidAcc 0.4129	TestAcc 0.4056	BestValid 0.4385
	Epoch 3500:	Loss 1.6069	TrainAcc 0.4056	ValidAcc 0.3986	TestAcc 0.3826	BestValid 0.4385
	Epoch 3550:	Loss 1.6002	TrainAcc 0.4286	ValidAcc 0.4315	TestAcc 0.4330	BestValid 0.4385
	Epoch 3600:	Loss 1.6048	TrainAcc 0.4142	ValidAcc 0.4083	TestAcc 0.4081	BestValid 0.4385
	Epoch 3650:	Loss 1.5837	TrainAcc 0.4138	ValidAcc 0.4096	TestAcc 0.4027	BestValid 0.4385
	Epoch 3700:	Loss 1.5866	TrainAcc 0.4120	ValidAcc 0.4201	TestAcc 0.4220	BestValid 0.4385
	Epoch 3750:	Loss 1.6326	TrainAcc 0.4101	ValidAcc 0.4057	TestAcc 0.3973	BestValid 0.4385
	Epoch 3800:	Loss 1.5757	TrainAcc 0.4306	ValidAcc 0.4094	TestAcc 0.3994	BestValid 0.4385
	Epoch 3850:	Loss 1.6141	TrainAcc 0.4149	ValidAcc 0.4039	TestAcc 0.3889	BestValid 0.4385
	Epoch 3900:	Loss 1.5619	TrainAcc 0.4058	ValidAcc 0.3703	TestAcc 0.3536	BestValid 0.4385
	Epoch 3950:	Loss 1.5761	TrainAcc 0.4311	ValidAcc 0.4200	TestAcc 0.4133	BestValid 0.4385
	Epoch 4000:	Loss 1.5641	TrainAcc 0.4223	ValidAcc 0.4117	TestAcc 0.4029	BestValid 0.4385
	Epoch 4050:	Loss 1.5515	TrainAcc 0.4237	ValidAcc 0.4292	TestAcc 0.4335	BestValid 0.4385
	Epoch 4100:	Loss 1.6088	TrainAcc 0.3879	ValidAcc 0.3843	TestAcc 0.3729	BestValid 0.4385
	Epoch 4150:	Loss 1.5557	TrainAcc 0.4186	ValidAcc 0.4065	TestAcc 0.3940	BestValid 0.4385
	Epoch 4200:	Loss 1.6171	TrainAcc 0.4149	ValidAcc 0.4047	TestAcc 0.4003	BestValid 0.4385
	Epoch 4250:	Loss 1.5573	TrainAcc 0.4134	ValidAcc 0.3984	TestAcc 0.3793	BestValid 0.4385
	Epoch 4300:	Loss 1.7874	TrainAcc 0.4185	ValidAcc 0.4084	TestAcc 0.4043	BestValid 0.4385
	Epoch 4350:	Loss 1.5556	TrainAcc 0.4137	ValidAcc 0.3972	TestAcc 0.3786	BestValid 0.4385
	Epoch 4400:	Loss 1.5415	TrainAcc 0.4095	ValidAcc 0.3895	TestAcc 0.3791	BestValid 0.4385
	Epoch 4450:	Loss 1.5506	TrainAcc 0.3736	ValidAcc 0.2696	TestAcc 0.2578	BestValid 0.4385
	Epoch 4500:	Loss 1.5608	TrainAcc 0.4201	ValidAcc 0.4029	TestAcc 0.3911	BestValid 0.4385
	Epoch 4550:	Loss 1.5721	TrainAcc 0.3726	ValidAcc 0.2781	TestAcc 0.2619	BestValid 0.4385
	Epoch 4600:	Loss 1.5548	TrainAcc 0.3666	ValidAcc 0.2606	TestAcc 0.2449	BestValid 0.4385
	Epoch 4650:	Loss 1.5477	TrainAcc 0.4127	ValidAcc 0.3910	TestAcc 0.3734	BestValid 0.4385
	Epoch 4700:	Loss 1.5347	TrainAcc 0.4115	ValidAcc 0.3842	TestAcc 0.3711	BestValid 0.4385
	Epoch 4750:	Loss 1.5385	TrainAcc 0.4201	ValidAcc 0.4024	TestAcc 0.3888	BestValid 0.4385
	Epoch 4800:	Loss 1.5303	TrainAcc 0.3619	ValidAcc 0.2439	TestAcc 0.2316	BestValid 0.4385
	Epoch 4850:	Loss 1.5283	TrainAcc 0.3755	ValidAcc 0.2958	TestAcc 0.2805	BestValid 0.4385
	Epoch 4900:	Loss 1.5750	TrainAcc 0.4030	ValidAcc 0.3724	TestAcc 0.3685	BestValid 0.4385
	Epoch 4950:	Loss 1.5563	TrainAcc 0.4341	ValidAcc 0.4186	TestAcc 0.4085	BestValid 0.4385
	Epoch 5000:	Loss 1.5421	TrainAcc 0.4236	ValidAcc 0.4005	TestAcc 0.3789	BestValid 0.4385
****** Epoch Time (Excluding Evaluation Cost): 0.124 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.321 ms (Max: 0.562, Min: 0.034, Sum: 2.569)
Cluster-Wide Average, Compute: 26.024 ms (Max: 29.158, Min: 24.511, Sum: 208.193)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.014 ms (Max: 0.015, Min: 0.013, Sum: 0.115)
Cluster-Wide Average, Communication-Graph: 94.642 ms (Max: 95.943, Min: 91.773, Sum: 757.139)
Cluster-Wide Average, Optimization: 2.533 ms (Max: 2.548, Min: 2.515, Sum: 20.261)
Cluster-Wide Average, Others: 0.777 ms (Max: 0.804, Min: 0.751, Sum: 6.217)
****** Breakdown Sum: 124.319 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.179 GB (Max: 4.606, Min: 4.104, Sum: 33.429)
Cluster-Wide Average, Graph-Level Communication Throughput: 33.652 Gbps (Max: 65.044, Min: 10.858, Sum: 269.213)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.725 GB
Weight-sync communication (cluster-wide, per-epoch): 0.017 GB
Total communication (cluster-wide, per-epoch): 2.742 GB
****** Accuracy Results ******
Highest valid_acc: 0.4385
Target test_acc: 0.4491
Epoch to reach the target acc: 3099
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
