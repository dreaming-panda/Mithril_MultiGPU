Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv8
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
        It takes 0.048 seconds.
        It takes 0.050 seconds.
        It takes 0.050 seconds.
        It takes 0.052 seconds.
        It takes 0.054 seconds.
        It takes 0.056 seconds.
        It takes 0.054 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.028 seconds.
        It takes 0.028 seconds.
        It takes 0.028 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
169343, 2484941, 2484941
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
Number of vertices per chunk: 21168
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
169343, 2484941, 2484941
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.566 Gbps (per GPU), 484.527 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.272 Gbps (per GPU), 482.173 Gbps (aggregated)
The layer-level communication performance: 60.259 Gbps (per GPU), 482.073 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.014 Gbps (per GPU), 480.110 Gbps (aggregated)
The layer-level communication performance: 59.981 Gbps (per GPU), 479.846 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.751 Gbps (per GPU), 478.011 Gbps (aggregated)
The layer-level communication performance: 59.705 Gbps (per GPU), 477.636 Gbps (aggregated)
The layer-level communication performance: 59.672 Gbps (per GPU), 477.372 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.668 Gbps (per GPU), 1269.341 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.674 Gbps (per GPU), 1269.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.611 Gbps (per GPU), 1268.886 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.662 Gbps (per GPU), 1269.294 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.674 Gbps (per GPU), 1269.392 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.656 Gbps (per GPU), 1269.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.629 Gbps (per GPU), 1269.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.671 Gbps (per GPU), 1269.366 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 105.007 Gbps (per GPU), 840.058 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.007 Gbps (per GPU), 840.058 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.007 Gbps (per GPU), 840.058 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.999 Gbps (per GPU), 839.995 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.005 Gbps (per GPU), 840.044 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.999 Gbps (per GPU), 839.995 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.989 Gbps (per GPU), 839.911 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.973 Gbps (per GPU), 839.785 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.812 Gbps (per GPU), 310.492 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.813 Gbps (per GPU), 310.500 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.809 Gbps (per GPU), 310.468 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.805 Gbps (per GPU), 310.437 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.808 Gbps (per GPU), 310.463 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.805 Gbps (per GPU), 310.442 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.803 Gbps (per GPU), 310.422 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.798 Gbps (per GPU), 310.383 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.30ms  2.92ms  4.79ms  3.67 21.17K  0.46M
 chk_1  1.36ms  2.99ms  4.86ms  3.57 21.17K  0.55M
 chk_2  1.25ms  2.88ms  4.76ms  3.81 21.17K  0.39M
 chk_3  1.20ms  2.84ms  4.71ms  3.91 21.17K  0.24M
 chk_4  1.16ms  2.78ms  4.66ms  4.03 21.17K  0.17M
 chk_5  1.16ms  2.79ms  4.66ms  4.03 21.17K  0.22M
 chk_6  1.16ms  2.79ms  4.67ms  4.01 21.17K  0.16M
 chk_7  1.14ms  2.77ms  4.65ms  4.06 21.17K  0.12M
   Avg  1.22  2.85  4.72
   Max  1.36  2.99  4.86
   Min  1.14  2.77  4.65
 Ratio  1.19  1.08  1.04
   Var  0.01  0.01  0.00
Profiling takes 0.884 s
*** Node 0, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 421)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 21167
*** Node 2, starting model training...
*** Node 7, starting model training...
*** Node 3, starting model training...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 421)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 84671, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 127007, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 148175, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 421)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 63503, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 421)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 42335, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 421)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 21167, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 421)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 105839, Num Local Vertices: 21168
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 421)...
+++++++++ Node 6 initializing the weights for op[0, 421)...
+++++++++ Node 3 initializing the weights for op[0, 421)...
+++++++++ Node 2 initializing the weights for op[0, 421)...
+++++++++ Node 7 initializing the weights for op[0, 421)...
+++++++++ Node 4 initializing the weights for op[0, 421)...
+++++++++ Node 0 initializing the weights for op[0, 421)...
+++++++++ Node 5 initializing the weights for op[0, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 114305
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.1688	TrainAcc 0.2541	ValidAcc 0.2682	TestAcc 0.2424	BestValid 0.2682
	Epoch 50:	Loss 1.9883	TrainAcc 0.5220	ValidAcc 0.5633	TestAcc 0.5660	BestValid 0.5633
	Epoch 100:	Loss 1.6096	TrainAcc 0.6131	ValidAcc 0.6272	TestAcc 0.6266	BestValid 0.6272
	Epoch 150:	Loss 1.4191	TrainAcc 0.6546	ValidAcc 0.6601	TestAcc 0.6525	BestValid 0.6601
	Epoch 200:	Loss 1.3169	TrainAcc 0.6732	ValidAcc 0.6788	TestAcc 0.6702	BestValid 0.6788
	Epoch 250:	Loss 1.2632	TrainAcc 0.6880	ValidAcc 0.6888	TestAcc 0.6767	BestValid 0.6888
	Epoch 300:	Loss 1.2250	TrainAcc 0.6955	ValidAcc 0.6970	TestAcc 0.6862	BestValid 0.6970
	Epoch 350:	Loss 1.2020	TrainAcc 0.7013	ValidAcc 0.6975	TestAcc 0.6824	BestValid 0.6975
	Epoch 400:	Loss 1.1782	TrainAcc 0.7067	ValidAcc 0.7035	TestAcc 0.6937	BestValid 0.7035
	Epoch 450:	Loss 1.1610	TrainAcc 0.7092	ValidAcc 0.7042	TestAcc 0.6939	BestValid 0.7042
	Epoch 500:	Loss 1.1481	TrainAcc 0.7133	ValidAcc 0.7061	TestAcc 0.6960	BestValid 0.7061
	Epoch 550:	Loss 1.1358	TrainAcc 0.7154	ValidAcc 0.7088	TestAcc 0.7015	BestValid 0.7088
	Epoch 600:	Loss 1.1270	TrainAcc 0.7185	ValidAcc 0.7089	TestAcc 0.6960	BestValid 0.7089
	Epoch 650:	Loss 1.1212	TrainAcc 0.7202	ValidAcc 0.7098	TestAcc 0.7005	BestValid 0.7098
	Epoch 700:	Loss 1.1064	TrainAcc 0.7216	ValidAcc 0.7119	TestAcc 0.7027	BestValid 0.7119
	Epoch 750:	Loss 1.1014	TrainAcc 0.7234	ValidAcc 0.7127	TestAcc 0.7017	BestValid 0.7127
	Epoch 800:	Loss 1.0938	TrainAcc 0.7256	ValidAcc 0.7113	TestAcc 0.6981	BestValid 0.7127
	Epoch 850:	Loss 1.0817	TrainAcc 0.7257	ValidAcc 0.7142	TestAcc 0.7061	BestValid 0.7142
	Epoch 900:	Loss 1.0790	TrainAcc 0.7276	ValidAcc 0.7146	TestAcc 0.7042	BestValid 0.7146
	Epoch 950:	Loss 1.0715	TrainAcc 0.7284	ValidAcc 0.7128	TestAcc 0.7008	BestValid 0.7146
	Epoch 1000:	Loss 1.0632	TrainAcc 0.7298	ValidAcc 0.7155	TestAcc 0.7060	BestValid 0.7155
	Epoch 1050:	Loss 1.0655	TrainAcc 0.7305	ValidAcc 0.7157	TestAcc 0.7041	BestValid 0.7157
	Epoch 1100:	Loss 1.0565	TrainAcc 0.7315	ValidAcc 0.7160	TestAcc 0.7053	BestValid 0.7160
	Epoch 1150:	Loss 1.0573	TrainAcc 0.7315	ValidAcc 0.7183	TestAcc 0.7102	BestValid 0.7183
	Epoch 1200:	Loss 1.0547	TrainAcc 0.7329	ValidAcc 0.7175	TestAcc 0.7064	BestValid 0.7183
	Epoch 1250:	Loss 1.0452	TrainAcc 0.7343	ValidAcc 0.7176	TestAcc 0.7062	BestValid 0.7183
	Epoch 1300:	Loss 1.0430	TrainAcc 0.7346	ValidAcc 0.7152	TestAcc 0.7005	BestValid 0.7183
	Epoch 1350:	Loss 1.0389	TrainAcc 0.7350	ValidAcc 0.7200	TestAcc 0.7111	BestValid 0.7200
	Epoch 1400:	Loss 1.0343	TrainAcc 0.7350	ValidAcc 0.7186	TestAcc 0.7076	BestValid 0.7200
	Epoch 1450:	Loss 1.0302	TrainAcc 0.7374	ValidAcc 0.7193	TestAcc 0.7077	BestValid 0.7200
	Epoch 1500:	Loss 1.0288	TrainAcc 0.7348	ValidAcc 0.7191	TestAcc 0.7096	BestValid 0.7200
	Epoch 1550:	Loss 1.0219	TrainAcc 0.7379	ValidAcc 0.7190	TestAcc 0.7057	BestValid 0.7200
	Epoch 1600:	Loss 1.0201	TrainAcc 0.7376	ValidAcc 0.7201	TestAcc 0.7115	BestValid 0.7201
	Epoch 1650:	Loss 1.0185	TrainAcc 0.7394	ValidAcc 0.7205	TestAcc 0.7079	BestValid 0.7205
	Epoch 1700:	Loss 1.0163	TrainAcc 0.7398	ValidAcc 0.7223	TestAcc 0.7126	BestValid 0.7223
	Epoch 1750:	Loss 1.0136	TrainAcc 0.7397	ValidAcc 0.7216	TestAcc 0.7099	BestValid 0.7223
	Epoch 1800:	Loss 1.0095	TrainAcc 0.7402	ValidAcc 0.7229	TestAcc 0.7132	BestValid 0.7229
	Epoch 1850:	Loss 1.0032	TrainAcc 0.7415	ValidAcc 0.7223	TestAcc 0.7105	BestValid 0.7229
	Epoch 1900:	Loss 1.0022	TrainAcc 0.7409	ValidAcc 0.7224	TestAcc 0.7120	BestValid 0.7229
	Epoch 1950:	Loss 0.9983	TrainAcc 0.7418	ValidAcc 0.7218	TestAcc 0.7099	BestValid 0.7229
	Epoch 2000:	Loss 0.9991	TrainAcc 0.7426	ValidAcc 0.7228	TestAcc 0.7111	BestValid 0.7229
	Epoch 2050:	Loss 0.9982	TrainAcc 0.7421	ValidAcc 0.7209	TestAcc 0.7100	BestValid 0.7229
	Epoch 2100:	Loss 0.9933	TrainAcc 0.7442	ValidAcc 0.7218	TestAcc 0.7102	BestValid 0.7229
	Epoch 2150:	Loss 0.9914	TrainAcc 0.7447	ValidAcc 0.7237	TestAcc 0.7140	BestValid 0.7237
	Epoch 2200:	Loss 0.9860	TrainAcc 0.7457	ValidAcc 0.7223	TestAcc 0.7121	BestValid 0.7237
	Epoch 2250:	Loss 0.9859	TrainAcc 0.7455	ValidAcc 0.7215	TestAcc 0.7084	BestValid 0.7237
	Epoch 2300:	Loss 0.9820	TrainAcc 0.7458	ValidAcc 0.7242	TestAcc 0.7139	BestValid 0.7242
	Epoch 2350:	Loss 0.9819	TrainAcc 0.7464	ValidAcc 0.7230	TestAcc 0.7116	BestValid 0.7242
	Epoch 2400:	Loss 0.9797	TrainAcc 0.7462	ValidAcc 0.7234	TestAcc 0.7127	BestValid 0.7242
	Epoch 2450:	Loss 0.9763	TrainAcc 0.7476	ValidAcc 0.7240	TestAcc 0.7141	BestValid 0.7242
	Epoch 2500:	Loss 0.9765	TrainAcc 0.7469	ValidAcc 0.7244	TestAcc 0.7145	BestValid 0.7244
	Epoch 2550:	Loss 0.9751	TrainAcc 0.7477	ValidAcc 0.7252	TestAcc 0.7157	BestValid 0.7252
	Epoch 2600:	Loss 0.9692	TrainAcc 0.7478	ValidAcc 0.7246	TestAcc 0.7142	BestValid 0.7252
	Epoch 2650:	Loss 0.9703	TrainAcc 0.7476	ValidAcc 0.7222	TestAcc 0.7092	BestValid 0.7252
	Epoch 2700:	Loss 0.9663	TrainAcc 0.7500	ValidAcc 0.7242	TestAcc 0.7132	BestValid 0.7252
	Epoch 2750:	Loss 0.9652	TrainAcc 0.7492	ValidAcc 0.7254	TestAcc 0.7147	BestValid 0.7254
	Epoch 2800:	Loss 0.9640	TrainAcc 0.7504	ValidAcc 0.7267	TestAcc 0.7191	BestValid 0.7267
	Epoch 2850:	Loss 0.9650	TrainAcc 0.7502	ValidAcc 0.7244	TestAcc 0.7121	BestValid 0.7267
	Epoch 2900:	Loss 0.9571	TrainAcc 0.7503	ValidAcc 0.7259	TestAcc 0.7139	BestValid 0.7267
	Epoch 2950:	Loss 0.9569	TrainAcc 0.7512	ValidAcc 0.7247	TestAcc 0.7115	BestValid 0.7267
	Epoch 3000:	Loss 0.9581	TrainAcc 0.7509	ValidAcc 0.7271	TestAcc 0.7192	BestValid 0.7271
	Epoch 3050:	Loss 0.9501	TrainAcc 0.7519	ValidAcc 0.7261	TestAcc 0.7157	BestValid 0.7271
	Epoch 3100:	Loss 0.9524	TrainAcc 0.7510	ValidAcc 0.7257	TestAcc 0.7155	BestValid 0.7271
	Epoch 3150:	Loss 0.9490	TrainAcc 0.7524	ValidAcc 0.7268	TestAcc 0.7167	BestValid 0.7271
	Epoch 3200:	Loss 0.9508	TrainAcc 0.7525	ValidAcc 0.7257	TestAcc 0.7157	BestValid 0.7271
	Epoch 3250:	Loss 0.9464	TrainAcc 0.7527	ValidAcc 0.7265	TestAcc 0.7173	BestValid 0.7271
	Epoch 3300:	Loss 0.9495	TrainAcc 0.7541	ValidAcc 0.7259	TestAcc 0.7153	BestValid 0.7271
	Epoch 3350:	Loss 0.9436	TrainAcc 0.7539	ValidAcc 0.7278	TestAcc 0.7166	BestValid 0.7278
	Epoch 3400:	Loss 0.9450	TrainAcc 0.7538	ValidAcc 0.7274	TestAcc 0.7195	BestValid 0.7278
	Epoch 3450:	Loss 0.9423	TrainAcc 0.7546	ValidAcc 0.7272	TestAcc 0.7164	BestValid 0.7278
	Epoch 3500:	Loss 0.9388	TrainAcc 0.7535	ValidAcc 0.7271	TestAcc 0.7201	BestValid 0.7278
	Epoch 3550:	Loss 0.9415	TrainAcc 0.7554	ValidAcc 0.7273	TestAcc 0.7169	BestValid 0.7278
	Epoch 3600:	Loss 0.9362	TrainAcc 0.7554	ValidAcc 0.7273	TestAcc 0.7157	BestValid 0.7278
	Epoch 3650:	Loss 0.9325	TrainAcc 0.7558	ValidAcc 0.7266	TestAcc 0.7181	BestValid 0.7278
	Epoch 3700:	Loss 0.9338	TrainAcc 0.7562	ValidAcc 0.7276	TestAcc 0.7179	BestValid 0.7278
	Epoch 3750:	Loss 0.9320	TrainAcc 0.7561	ValidAcc 0.7268	TestAcc 0.7179	BestValid 0.7278
	Epoch 3800:	Loss 0.9312	TrainAcc 0.7573	ValidAcc 0.7278	TestAcc 0.7185	BestValid 0.7278
	Epoch 3850:	Loss 0.9242	TrainAcc 0.7581	ValidAcc 0.7287	TestAcc 0.7183	BestValid 0.7287
	Epoch 3900:	Loss 0.9289	TrainAcc 0.7574	ValidAcc 0.7274	TestAcc 0.7174	BestValid 0.7287
	Epoch 3950:	Loss 0.9282	TrainAcc 0.7574	ValidAcc 0.7266	TestAcc 0.7159	BestValid 0.7287
	Epoch 4000:	Loss 0.9277	TrainAcc 0.7571	ValidAcc 0.7278	TestAcc 0.7201	BestValid 0.7287
	Epoch 4050:	Loss 0.9209	TrainAcc 0.7578	ValidAcc 0.7286	TestAcc 0.7214	BestValid 0.7287
	Epoch 4100:	Loss 0.9208	TrainAcc 0.7578	ValidAcc 0.7271	TestAcc 0.7161	BestValid 0.7287
	Epoch 4150:	Loss 0.9197	TrainAcc 0.7583	ValidAcc 0.7276	TestAcc 0.7211	BestValid 0.7287
	Epoch 4200:	Loss 0.9184	TrainAcc 0.7582	ValidAcc 0.7270	TestAcc 0.7187	BestValid 0.7287
	Epoch 4250:	Loss 0.9183	TrainAcc 0.7592	ValidAcc 0.7282	TestAcc 0.7218	BestValid 0.7287
	Epoch 4300:	Loss 0.9178	TrainAcc 0.7586	ValidAcc 0.7271	TestAcc 0.7173	BestValid 0.7287
	Epoch 4350:	Loss 0.9184	TrainAcc 0.7604	ValidAcc 0.7288	TestAcc 0.7182	BestValid 0.7288
	Epoch 4400:	Loss 0.9157	TrainAcc 0.7596	ValidAcc 0.7281	TestAcc 0.7199	BestValid 0.7288
	Epoch 4450:	Loss 0.9134	TrainAcc 0.7608	ValidAcc 0.7281	TestAcc 0.7221	BestValid 0.7288
	Epoch 4500:	Loss 0.9122	TrainAcc 0.7611	ValidAcc 0.7269	TestAcc 0.7172	BestValid 0.7288
	Epoch 4550:	Loss 0.9098	TrainAcc 0.7615	ValidAcc 0.7288	TestAcc 0.7183	BestValid 0.7288
	Epoch 4600:	Loss 0.9099	TrainAcc 0.7614	ValidAcc 0.7282	TestAcc 0.7199	BestValid 0.7288
	Epoch 4650:	Loss 0.9084	TrainAcc 0.7622	ValidAcc 0.7286	TestAcc 0.7194	BestValid 0.7288
	Epoch 4700:	Loss 0.9048	TrainAcc 0.7621	ValidAcc 0.7281	TestAcc 0.7186	BestValid 0.7288
	Epoch 4750:	Loss 0.9006	TrainAcc 0.7626	ValidAcc 0.7288	TestAcc 0.7201	BestValid 0.7288
	Epoch 4800:	Loss 0.9034	TrainAcc 0.7627	ValidAcc 0.7281	TestAcc 0.7174	BestValid 0.7288
	Epoch 4850:	Loss 0.9005	TrainAcc 0.7627	ValidAcc 0.7276	TestAcc 0.7181	BestValid 0.7288
	Epoch 4900:	Loss 0.8980	TrainAcc 0.7628	ValidAcc 0.7271	TestAcc 0.7166	BestValid 0.7288
