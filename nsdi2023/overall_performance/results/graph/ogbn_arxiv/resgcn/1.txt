Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INITDONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7

Initialized node 0 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.048 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.068 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
        It takes 0.051 seconds.
        It takes 0.049 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.050 seconds.
        It takes 0.055 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.059 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
        It takes 0.027 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.624 Gbps (per GPU), 484.996 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.318 Gbps (per GPU), 482.544 Gbps (aggregated)
The layer-level communication performance: 60.310 Gbps (per GPU), 482.481 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.068 Gbps (per GPU), 480.540 Gbps (aggregated)
The layer-level communication performance: 60.031 Gbps (per GPU), 480.249 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.820 Gbps (per GPU), 478.563 Gbps (aggregated)
The layer-level communication performance: 59.773 Gbps (per GPU), 478.183 Gbps (aggregated)
The layer-level communication performance: 59.742 Gbps (per GPU), 477.937 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.744 Gbps (per GPU), 1285.955 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.757 Gbps (per GPU), 1286.054 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.741 Gbps (per GPU), 1285.931 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.726 Gbps (per GPU), 1285.808 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.747 Gbps (per GPU), 1285.978 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.736 Gbps (per GPU), 1285.885 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.738 Gbps (per GPU), 1285.905 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.738 Gbps (per GPU), 1285.904 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.713 Gbps (per GPU), 837.702 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.705 Gbps (per GPU), 837.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.699 Gbps (per GPU), 837.590 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.699 Gbps (per GPU), 837.591 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.714 Gbps (per GPU), 837.716 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.701 Gbps (per GPU), 837.604 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.714 Gbps (per GPU), 837.716 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.681 Gbps (per GPU), 837.451 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.097 Gbps (per GPU), 296.779 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.099 Gbps (per GPU), 296.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.099 Gbps (per GPU), 296.788 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.094 Gbps (per GPU), 296.754 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.099 Gbps (per GPU), 296.793 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.095 Gbps (per GPU), 296.763 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.097 Gbps (per GPU), 296.778 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.093 Gbps (per GPU), 296.742 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.30ms  2.97ms  4.73ms  3.63 21.17K  0.46M
 chk_1  1.35ms  2.97ms  5.02ms  3.71 21.17K  0.55M
 chk_2  1.24ms  3.00ms  4.71ms  3.78 21.17K  0.39M
 chk_3  1.20ms  2.81ms  4.66ms  3.89 21.17K  0.24M
 chk_4  1.15ms  2.85ms  4.61ms  4.01 21.17K  0.17M
 chk_5  1.15ms  2.77ms  4.61ms  4.00 21.17K  0.22M
 chk_6  1.16ms  2.77ms  4.62ms  3.99 21.17K  0.16M
 chk_7  1.14ms  2.75ms  4.60ms  4.05 21.17K  0.12M
   Avg  1.21  2.86  4.69
   Max  1.35  3.00  5.02
   Min  1.14  2.75  4.60
 Ratio  1.19  1.09  1.09
   Var  0.01  0.01  0.02
Profiling takes 0.880 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 421)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 21167
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 421)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 21167, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 421)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 63503, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 421)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 84671, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 421)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 42335, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 421)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 105839, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 127007, Num Local Vertices: 21168
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 148175, Num Local Vertices: 21168
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 421)...
+++++++++ Node 4 initializing the weights for op[0, 421)...
+++++++++ Node 7 initializing the weights for op[0, 421)...
+++++++++ Node 1 initializing the weights for op[0, 421)...
+++++++++ Node 3 initializing the weights for op[0, 421)...
+++++++++ Node 5 initializing the weights for op[0, 421)...
+++++++++ Node 6 initializing the weights for op[0, 421)...
+++++++++ Node 2 initializing the weights for op[0, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 114305
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.0812	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 50:	Loss 2.0663	TrainAcc 0.4951	ValidAcc 0.5184	TestAcc 0.5057	BestValid 0.5184
	Epoch 100:	Loss 1.6286	TrainAcc 0.6046	ValidAcc 0.6249	TestAcc 0.6234	BestValid 0.6249
	Epoch 150:	Loss 1.4363	TrainAcc 0.6508	ValidAcc 0.6594	TestAcc 0.6519	BestValid 0.6594
	Epoch 200:	Loss 1.3361	TrainAcc 0.6719	ValidAcc 0.6748	TestAcc 0.6694	BestValid 0.6748
	Epoch 250:	Loss 1.2736	TrainAcc 0.6860	ValidAcc 0.6891	TestAcc 0.6835	BestValid 0.6891
	Epoch 300:	Loss 1.2402	TrainAcc 0.6932	ValidAcc 0.6951	TestAcc 0.6873	BestValid 0.6951
	Epoch 350:	Loss 1.2123	TrainAcc 0.6995	ValidAcc 0.6974	TestAcc 0.6889	BestValid 0.6974
	Epoch 400:	Loss 1.1982	TrainAcc 0.7043	ValidAcc 0.6994	TestAcc 0.6896	BestValid 0.6994
	Epoch 450:	Loss 1.1737	TrainAcc 0.7077	ValidAcc 0.7024	TestAcc 0.6931	BestValid 0.7024
	Epoch 500:	Loss 1.1592	TrainAcc 0.7108	ValidAcc 0.7048	TestAcc 0.6983	BestValid 0.7048
	Epoch 550:	Loss 1.1475	TrainAcc 0.7137	ValidAcc 0.7065	TestAcc 0.7008	BestValid 0.7065
	Epoch 600:	Loss 1.1351	TrainAcc 0.7156	ValidAcc 0.7079	TestAcc 0.7012	BestValid 0.7079
	Epoch 650:	Loss 1.1261	TrainAcc 0.7180	ValidAcc 0.7087	TestAcc 0.7018	BestValid 0.7087
	Epoch 700:	Loss 1.1218	TrainAcc 0.7194	ValidAcc 0.7100	TestAcc 0.7014	BestValid 0.7100
	Epoch 750:	Loss 1.1131	TrainAcc 0.7201	ValidAcc 0.7108	TestAcc 0.7020	BestValid 0.7108
	Epoch 800:	Loss 1.1046	TrainAcc 0.7237	ValidAcc 0.7126	TestAcc 0.7078	BestValid 0.7126
	Epoch 850:	Loss 1.0984	TrainAcc 0.7241	ValidAcc 0.7133	TestAcc 0.7056	BestValid 0.7133
	Epoch 900:	Loss 1.0891	TrainAcc 0.7259	ValidAcc 0.7141	TestAcc 0.7058	BestValid 0.7141
	Epoch 950:	Loss 1.0833	TrainAcc 0.7277	ValidAcc 0.7146	TestAcc 0.7073	BestValid 0.7146
	Epoch 1000:	Loss 1.0783	TrainAcc 0.7276	ValidAcc 0.7149	TestAcc 0.7091	BestValid 0.7149
	Epoch 1050:	Loss 1.0702	TrainAcc 0.7292	ValidAcc 0.7159	TestAcc 0.7086	BestValid 0.7159
	Epoch 1100:	Loss 1.0665	TrainAcc 0.7298	ValidAcc 0.7153	TestAcc 0.7084	BestValid 0.7159
	Epoch 1150:	Loss 1.0610	TrainAcc 0.7304	ValidAcc 0.7174	TestAcc 0.7106	BestValid 0.7174
	Epoch 1200:	Loss 1.0565	TrainAcc 0.7323	ValidAcc 0.7162	TestAcc 0.7084	BestValid 0.7174
	Epoch 1250:	Loss 1.0512	TrainAcc 0.7326	ValidAcc 0.7162	TestAcc 0.7080	BestValid 0.7174
	Epoch 1300:	Loss 1.0472	TrainAcc 0.7334	ValidAcc 0.7162	TestAcc 0.7082	BestValid 0.7174
	Epoch 1350:	Loss 1.0446	TrainAcc 0.7339	ValidAcc 0.7162	TestAcc 0.7096	BestValid 0.7174
	Epoch 1400:	Loss 1.0402	TrainAcc 0.7353	ValidAcc 0.7172	TestAcc 0.7086	BestValid 0.7174
	Epoch 1450:	Loss 1.0337	TrainAcc 0.7359	ValidAcc 0.7177	TestAcc 0.7086	BestValid 0.7177
	Epoch 1500:	Loss 1.0328	TrainAcc 0.7354	ValidAcc 0.7163	TestAcc 0.7090	BestValid 0.7177
	Epoch 1550:	Loss 1.0276	TrainAcc 0.7369	ValidAcc 0.7184	TestAcc 0.7094	BestValid 0.7184
	Epoch 1600:	Loss 1.0257	TrainAcc 0.7376	ValidAcc 0.7192	TestAcc 0.7112	BestValid 0.7192
	Epoch 1650:	Loss 1.0217	TrainAcc 0.7376	ValidAcc 0.7198	TestAcc 0.7118	BestValid 0.7198
	Epoch 1700:	Loss 1.0176	TrainAcc 0.7388	ValidAcc 0.7211	TestAcc 0.7130	BestValid 0.7211
	Epoch 1750:	Loss 1.0158	TrainAcc 0.7383	ValidAcc 0.7204	TestAcc 0.7119	BestValid 0.7211
	Epoch 1800:	Loss 1.0126	TrainAcc 0.7391	ValidAcc 0.7206	TestAcc 0.7133	BestValid 0.7211
	Epoch 1850:	Loss 1.0102	TrainAcc 0.7406	ValidAcc 0.7192	TestAcc 0.7110	BestValid 0.7211
	Epoch 1900:	Loss 1.0099	TrainAcc 0.7404	ValidAcc 0.7213	TestAcc 0.7128	BestValid 0.7213
	Epoch 1950:	Loss 1.0050	TrainAcc 0.7402	ValidAcc 0.7205	TestAcc 0.7133	BestValid 0.7213
	Epoch 2000:	Loss 1.0012	TrainAcc 0.7412	ValidAcc 0.7213	TestAcc 0.7128	BestValid 0.7213
	Epoch 2050:	Loss 0.9988	TrainAcc 0.7414	ValidAcc 0.7205	TestAcc 0.7145	BestValid 0.7213
	Epoch 2100:	Loss 0.9992	TrainAcc 0.7429	ValidAcc 0.7214	TestAcc 0.7101	BestValid 0.7214
	Epoch 2150:	Loss 0.9924	TrainAcc 0.7426	ValidAcc 0.7219	TestAcc 0.7128	BestValid 0.7219
	Epoch 2200:	Loss 0.9979	TrainAcc 0.7436	ValidAcc 0.7222	TestAcc 0.7149	BestValid 0.7222
	Epoch 2250:	Loss 0.9898	TrainAcc 0.7446	ValidAcc 0.7221	TestAcc 0.7121	BestValid 0.7222
	Epoch 2300:	Loss 0.9899	TrainAcc 0.7442	ValidAcc 0.7223	TestAcc 0.7137	BestValid 0.7223
	Epoch 2350:	Loss 0.9845	TrainAcc 0.7438	ValidAcc 0.7223	TestAcc 0.7152	BestValid 0.7223
	Epoch 2400:	Loss 0.9827	TrainAcc 0.7452	ValidAcc 0.7240	TestAcc 0.7165	BestValid 0.7240
	Epoch 2450:	Loss 0.9801	TrainAcc 0.7456	ValidAcc 0.7247	TestAcc 0.7164	BestValid 0.7247
	Epoch 2500:	Loss 0.9776	TrainAcc 0.7461	ValidAcc 0.7219	TestAcc 0.7110	BestValid 0.7247
	Epoch 2550:	Loss 0.9761	TrainAcc 0.7462	ValidAcc 0.7238	TestAcc 0.7146	BestValid 0.7247
	Epoch 2600:	Loss 0.9796	TrainAcc 0.7465	ValidAcc 0.7221	TestAcc 0.7134	BestValid 0.7247
	Epoch 2650:	Loss 0.9763	TrainAcc 0.7467	ValidAcc 0.7242	TestAcc 0.7150	BestValid 0.7247
	Epoch 2700:	Loss 0.9683	TrainAcc 0.7474	ValidAcc 0.7235	TestAcc 0.7134	BestValid 0.7247
	Epoch 2750:	Loss 0.9669	TrainAcc 0.7479	ValidAcc 0.7235	TestAcc 0.7144	BestValid 0.7247
	Epoch 2800:	Loss 0.9658	TrainAcc 0.7487	ValidAcc 0.7245	TestAcc 0.7151	BestValid 0.7247
	Epoch 2850:	Loss 0.9611	TrainAcc 0.7482	ValidAcc 0.7232	TestAcc 0.7136	BestValid 0.7247
	Epoch 2900:	Loss 0.9620	TrainAcc 0.7482	ValidAcc 0.7254	TestAcc 0.7153	BestValid 0.7254
	Epoch 2950:	Loss 0.9602	TrainAcc 0.7492	ValidAcc 0.7242	TestAcc 0.7152	BestValid 0.7254
	Epoch 3000:	Loss 0.9611	TrainAcc 0.7486	ValidAcc 0.7231	TestAcc 0.7107	BestValid 0.7254
	Epoch 3050:	Loss 0.9571	TrainAcc 0.7488	ValidAcc 0.7241	TestAcc 0.7141	BestValid 0.7254
	Epoch 3100:	Loss 0.9609	TrainAcc 0.7503	ValidAcc 0.7244	TestAcc 0.7149	BestValid 0.7254
	Epoch 3150:	Loss 0.9545	TrainAcc 0.7502	ValidAcc 0.7246	TestAcc 0.7148	BestValid 0.7254
	Epoch 3200:	Loss 0.9536	TrainAcc 0.7510	ValidAcc 0.7249	TestAcc 0.7146	BestValid 0.7254
	Epoch 3250:	Loss 0.9506	TrainAcc 0.7510	ValidAcc 0.7257	TestAcc 0.7129	BestValid 0.7257
	Epoch 3300:	Loss 0.9488	TrainAcc 0.7508	ValidAcc 0.7254	TestAcc 0.7146	BestValid 0.7257
	Epoch 3350:	Loss 0.9458	TrainAcc 0.7512	ValidAcc 0.7264	TestAcc 0.7174	BestValid 0.7264
	Epoch 3400:	Loss 0.9442	TrainAcc 0.7520	ValidAcc 0.7264	TestAcc 0.7164	BestValid 0.7264
	Epoch 3450:	Loss 0.9462	TrainAcc 0.7516	ValidAcc 0.7249	TestAcc 0.7182	BestValid 0.7264
	Epoch 3500:	Loss 0.9397	TrainAcc 0.7523	ValidAcc 0.7268	TestAcc 0.7174	BestValid 0.7268
	Epoch 3550:	Loss 0.9420	TrainAcc 0.7529	ValidAcc 0.7265	TestAcc 0.7163	BestValid 0.7268
	Epoch 3600:	Loss 0.9413	TrainAcc 0.7528	ValidAcc 0.7268	TestAcc 0.7154	BestValid 0.7268
	Epoch 3650:	Loss 0.9377	TrainAcc 0.7540	ValidAcc 0.7260	TestAcc 0.7149	BestValid 0.7268
	Epoch 3700:	Loss 0.9332	TrainAcc 0.7533	ValidAcc 0.7247	TestAcc 0.7128	BestValid 0.7268
	Epoch 3750:	Loss 0.9350	TrainAcc 0.7542	ValidAcc 0.7270	TestAcc 0.7153	BestValid 0.7270
	Epoch 3800:	Loss 0.9293	TrainAcc 0.7541	ValidAcc 0.7274	TestAcc 0.7180	BestValid 0.7274
	Epoch 3850:	Loss 0.9333	TrainAcc 0.7549	ValidAcc 0.7272	TestAcc 0.7152	BestValid 0.7274
	Epoch 3900:	Loss 0.9315	TrainAcc 0.7548	ValidAcc 0.7265	TestAcc 0.7165	BestValid 0.7274
	Epoch 3950:	Loss 0.9306	TrainAcc 0.7546	ValidAcc 0.7268	TestAcc 0.7166	BestValid 0.7274
	Epoch 4000:	Loss 0.9276	TrainAcc 0.7554	ValidAcc 0.7270	TestAcc 0.7171	BestValid 0.7274
	Epoch 4050:	Loss 0.9267	TrainAcc 0.7561	ValidAcc 0.7278	TestAcc 0.7167	BestValid 0.7278
	Epoch 4100:	Loss 0.9255	TrainAcc 0.7563	ValidAcc 0.7276	TestAcc 0.7174	BestValid 0.7278
	Epoch 4150:	Loss 0.9219	TrainAcc 0.7560	ValidAcc 0.7255	TestAcc 0.7120	BestValid 0.7278
	Epoch 4200:	Loss 0.9227	TrainAcc 0.7572	ValidAcc 0.7281	TestAcc 0.7144	BestValid 0.7281
	Epoch 4250:	Loss 0.9197	TrainAcc 0.7563	ValidAcc 0.7259	TestAcc 0.7173	BestValid 0.7281
	Epoch 4300:	Loss 0.9197	TrainAcc 0.7568	ValidAcc 0.7260	TestAcc 0.7149	BestValid 0.7281
	Epoch 4350:	Loss 0.9206	TrainAcc 0.7570	ValidAcc 0.7269	TestAcc 0.7196	BestValid 0.7281
	Epoch 4400:	Loss 0.9176	TrainAcc 0.7584	ValidAcc 0.7268	TestAcc 0.7141	BestValid 0.7281
	Epoch 4450:	Loss 0.9163	TrainAcc 0.7576	ValidAcc 0.7294	TestAcc 0.7158	BestValid 0.7294
	Epoch 4500:	Loss 0.9149	TrainAcc 0.7593	ValidAcc 0.7281	TestAcc 0.7169	BestValid 0.7294
	Epoch 4550:	Loss 0.9109	TrainAcc 0.7581	ValidAcc 0.7274	TestAcc 0.7157	BestValid 0.7294
	Epoch 4600:	Loss 0.9117	TrainAcc 0.7593	ValidAcc 0.7286	TestAcc 0.7183	BestValid 0.7294
	Epoch 4650:	Loss 0.9148	TrainAcc 0.7596	ValidAcc 0.7292	TestAcc 0.7164	BestValid 0.7294
	Epoch 4700:	Loss 0.9088	TrainAcc 0.7594	ValidAcc 0.7283	TestAcc 0.7181	BestValid 0.7294
	Epoch 4750:	Loss 0.9070	TrainAcc 0.7600	ValidAcc 0.7295	TestAcc 0.7203	BestValid 0.7295
	Epoch 4800:	Loss 0.9081	TrainAcc 0.7593	ValidAcc 0.7292	TestAcc 0.7200	BestValid 0.7295
	Epoch 4850:	Loss 0.9068	TrainAcc 0.7597	ValidAcc 0.7280	TestAcc 0.7143	BestValid 0.7295
	Epoch 4900:	Loss 0.9041	TrainAcc 0.7610	ValidAcc 0.7287	TestAcc 0.7205	BestValid 0.7295
	Epoch 4950:	Loss 0.9017	TrainAcc 0.7605	ValidAcc 0.7284	TestAcc 0.7160	BestValid 0.7295
	Epoch 5000:	Loss 0.8988	TrainAcc 0.7614	ValidAcc 0.7287	TestAcc 0.7146	BestValid 0.7295
****** Epoch Time (Excluding Evaluation Cost): 0.190 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.314 ms (Max: 0.579, Min: 0.034, Sum: 2.509)
Cluster-Wide Average, Compute: 85.072 ms (Max: 88.268, Min: 83.583, Sum: 680.577)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.121)
Cluster-Wide Average, Communication-Graph: 95.640 ms (Max: 97.171, Min: 92.597, Sum: 765.119)
Cluster-Wide Average, Optimization: 7.652 ms (Max: 7.711, Min: 7.574, Sum: 61.215)
Cluster-Wide Average, Others: 1.686 ms (Max: 1.761, Min: 1.566, Sum: 13.486)
****** Breakdown Sum: 190.386 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.424 GB (Max: 8.026, Min: 7.325, Sum: 59.394)
Cluster-Wide Average, Graph-Level Communication Throughput: 33.271 Gbps (Max: 64.441, Min: 10.845, Sum: 266.167)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.725 GB
Weight-sync communication (cluster-wide, per-epoch): 0.035 GB
Total communication (cluster-wide, per-epoch): 2.760 GB
****** Accuracy Results ******
Highest valid_acc: 0.7295
Target test_acc: 0.7203
Epoch to reach the target acc: 4749
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
