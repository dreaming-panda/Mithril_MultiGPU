Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8

Initialized node 5 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INITInitialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7

Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.069 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
        It takes 0.049 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.050 seconds.
        It takes 0.055 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.069 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.070 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.025 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.028 seconds.
        It takes 0.028 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.749 Gbps (per GPU), 485.993 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.453 Gbps (per GPU), 483.621 Gbps (aggregated)
The layer-level communication performance: 60.449 Gbps (per GPU), 483.595 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.199 Gbps (per GPU), 481.592 Gbps (aggregated)
The layer-level communication performance: 60.171 Gbps (per GPU), 481.367 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.950 Gbps (per GPU), 479.596 Gbps (aggregated)
The layer-level communication performance: 59.897 Gbps (per GPU), 479.174 Gbps (aggregated)
The layer-level communication performance: 59.861 Gbps (per GPU), 478.890 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.386 Gbps (per GPU), 1267.090 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.389 Gbps (per GPU), 1267.113 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.374 Gbps (per GPU), 1266.995 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.386 Gbps (per GPU), 1267.090 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.389 Gbps (per GPU), 1267.113 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.392 Gbps (per GPU), 1267.137 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.380 Gbps (per GPU), 1267.042 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.371 Gbps (per GPU), 1266.970 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.752 Gbps (per GPU), 838.016 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.741 Gbps (per GPU), 837.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.750 Gbps (per GPU), 838.002 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.724 Gbps (per GPU), 837.793 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.754 Gbps (per GPU), 838.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.741 Gbps (per GPU), 837.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.753 Gbps (per GPU), 838.023 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.744 Gbps (per GPU), 837.953 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.751 Gbps (per GPU), 294.005 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.751 Gbps (per GPU), 294.010 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.751 Gbps (per GPU), 294.006 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.747 Gbps (per GPU), 293.977 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.748 Gbps (per GPU), 293.986 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.744 Gbps (per GPU), 293.953 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.746 Gbps (per GPU), 293.967 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.741 Gbps (per GPU), 293.929 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.72ms  1.34ms  1.62ms  2.26 21.17K  0.46M
 chk_1  0.72ms  1.41ms  1.69ms  2.36 21.17K  0.55M
 chk_2  0.72ms  1.30ms  1.58ms  2.21 21.17K  0.39M
 chk_3  0.71ms  1.26ms  1.53ms  2.15 21.17K  0.24M
 chk_4  0.71ms  1.21ms  1.49ms  2.08 21.17K  0.17M
 chk_5  0.71ms  1.21ms  1.49ms  2.08 21.17K  0.22M
 chk_6  0.72ms  1.22ms  1.50ms  2.09 21.17K  0.16M
 chk_7  0.72ms  1.29ms  1.48ms  2.07 21.17K  0.12M
   Avg  0.72  1.28  1.55
   Max  0.72  1.41  1.69
   Min  0.71  1.21  1.48
 Ratio  1.01  1.17  1.14
   Var  0.00  0.00  0.01
Profiling takes 0.421 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 4, starting model training...
*** Node 3, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 21167
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 63503, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 42335, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 105839, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 127007, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 84671, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 148175, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 21167, Num Local Vertices: 21168
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 114305
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7106	TrainAcc 0.0237	ValidAcc 0.0204	TestAcc 0.0182	BestValid 0.0204
	Epoch 50:	Loss 2.8131	TrainAcc 0.2898	ValidAcc 0.3061	TestAcc 0.2761	BestValid 0.3061
	Epoch 100:	Loss 2.3429	TrainAcc 0.4263	ValidAcc 0.4000	TestAcc 0.3734	BestValid 0.4000
	Epoch 150:	Loss 2.1206	TrainAcc 0.4878	ValidAcc 0.4660	TestAcc 0.4414	BestValid 0.4660
	Epoch 200:	Loss 1.9928	TrainAcc 0.5378	ValidAcc 0.5321	TestAcc 0.5138	BestValid 0.5321
	Epoch 250:	Loss 1.9008	TrainAcc 0.5578	ValidAcc 0.5576	TestAcc 0.5433	BestValid 0.5576
	Epoch 300:	Loss 1.8362	TrainAcc 0.5734	ValidAcc 0.5829	TestAcc 0.5718	BestValid 0.5829
	Epoch 350:	Loss 1.7851	TrainAcc 0.5855	ValidAcc 0.5998	TestAcc 0.5891	BestValid 0.5998
	Epoch 400:	Loss 1.7437	TrainAcc 0.5945	ValidAcc 0.6039	TestAcc 0.5929	BestValid 0.6039
	Epoch 450:	Loss 1.7167	TrainAcc 0.6029	ValidAcc 0.6133	TestAcc 0.6005	BestValid 0.6133
	Epoch 500:	Loss 1.6815	TrainAcc 0.6087	ValidAcc 0.6129	TestAcc 0.6019	BestValid 0.6133
	Epoch 550:	Loss 1.6614	TrainAcc 0.6142	ValidAcc 0.6208	TestAcc 0.6094	BestValid 0.6208
	Epoch 600:	Loss 1.6431	TrainAcc 0.6191	ValidAcc 0.6297	TestAcc 0.6184	BestValid 0.6297
	Epoch 650:	Loss 1.6227	TrainAcc 0.6228	ValidAcc 0.6293	TestAcc 0.6178	BestValid 0.6297
	Epoch 700:	Loss 1.6048	TrainAcc 0.6257	ValidAcc 0.6325	TestAcc 0.6216	BestValid 0.6325
	Epoch 750:	Loss 1.5845	TrainAcc 0.6291	ValidAcc 0.6380	TestAcc 0.6257	BestValid 0.6380
	Epoch 800:	Loss 1.5719	TrainAcc 0.6307	ValidAcc 0.6381	TestAcc 0.6269	BestValid 0.6381
	Epoch 850:	Loss 1.5630	TrainAcc 0.6338	ValidAcc 0.6395	TestAcc 0.6281	BestValid 0.6395
	Epoch 900:	Loss 1.5539	TrainAcc 0.6357	ValidAcc 0.6433	TestAcc 0.6315	BestValid 0.6433
	Epoch 950:	Loss 1.5481	TrainAcc 0.6379	ValidAcc 0.6446	TestAcc 0.6342	BestValid 0.6446
	Epoch 1000:	Loss 1.5321	TrainAcc 0.6401	ValidAcc 0.6461	TestAcc 0.6344	BestValid 0.6461
	Epoch 1050:	Loss 1.5324	TrainAcc 0.6419	ValidAcc 0.6459	TestAcc 0.6337	BestValid 0.6461
	Epoch 1100:	Loss 1.5170	TrainAcc 0.6427	ValidAcc 0.6458	TestAcc 0.6345	BestValid 0.6461
	Epoch 1150:	Loss 1.5133	TrainAcc 0.6446	ValidAcc 0.6486	TestAcc 0.6377	BestValid 0.6486
	Epoch 1200:	Loss 1.4997	TrainAcc 0.6470	ValidAcc 0.6505	TestAcc 0.6383	BestValid 0.6505
	Epoch 1250:	Loss 1.4964	TrainAcc 0.6487	ValidAcc 0.6528	TestAcc 0.6422	BestValid 0.6528
	Epoch 1300:	Loss 1.4914	TrainAcc 0.6494	ValidAcc 0.6528	TestAcc 0.6406	BestValid 0.6528
	Epoch 1350:	Loss 1.4867	TrainAcc 0.6505	ValidAcc 0.6529	TestAcc 0.6410	BestValid 0.6529
	Epoch 1400:	Loss 1.4842	TrainAcc 0.6517	ValidAcc 0.6542	TestAcc 0.6436	BestValid 0.6542
	Epoch 1450:	Loss 1.4754	TrainAcc 0.6521	ValidAcc 0.6549	TestAcc 0.6451	BestValid 0.6549
	Epoch 1500:	Loss 1.4628	TrainAcc 0.6541	ValidAcc 0.6561	TestAcc 0.6466	BestValid 0.6561
	Epoch 1550:	Loss 1.4662	TrainAcc 0.6550	ValidAcc 0.6577	TestAcc 0.6481	BestValid 0.6577
	Epoch 1600:	Loss 1.4632	TrainAcc 0.6554	ValidAcc 0.6564	TestAcc 0.6463	BestValid 0.6577
	Epoch 1650:	Loss 1.4584	TrainAcc 0.6567	ValidAcc 0.6578	TestAcc 0.6481	BestValid 0.6578
	Epoch 1700:	Loss 1.4480	TrainAcc 0.6574	ValidAcc 0.6593	TestAcc 0.6493	BestValid 0.6593
	Epoch 1750:	Loss 1.4548	TrainAcc 0.6586	ValidAcc 0.6602	TestAcc 0.6507	BestValid 0.6602
	Epoch 1800:	Loss 1.4427	TrainAcc 0.6592	ValidAcc 0.6606	TestAcc 0.6512	BestValid 0.6606
	Epoch 1850:	Loss 1.4397	TrainAcc 0.6604	ValidAcc 0.6592	TestAcc 0.6493	BestValid 0.6606
	Epoch 1900:	Loss 1.4378	TrainAcc 0.6613	ValidAcc 0.6618	TestAcc 0.6510	BestValid 0.6618
	Epoch 1950:	Loss 1.4321	TrainAcc 0.6622	ValidAcc 0.6623	TestAcc 0.6541	BestValid 0.6623
	Epoch 2000:	Loss 1.4335	TrainAcc 0.6631	ValidAcc 0.6615	TestAcc 0.6511	BestValid 0.6623
	Epoch 2050:	Loss 1.4294	TrainAcc 0.6638	ValidAcc 0.6631	TestAcc 0.6535	BestValid 0.6631
	Epoch 2100:	Loss 1.4251	TrainAcc 0.6640	ValidAcc 0.6618	TestAcc 0.6510	BestValid 0.6631
	Epoch 2150:	Loss 1.4215	TrainAcc 0.6655	ValidAcc 0.6656	TestAcc 0.6555	BestValid 0.6656
	Epoch 2200:	Loss 1.4205	TrainAcc 0.6657	ValidAcc 0.6644	TestAcc 0.6551	BestValid 0.6656
	Epoch 2250:	Loss 1.4120	TrainAcc 0.6661	ValidAcc 0.6644	TestAcc 0.6548	BestValid 0.6656
	Epoch 2300:	Loss 1.4147	TrainAcc 0.6671	ValidAcc 0.6653	TestAcc 0.6552	BestValid 0.6656
	Epoch 2350:	Loss 1.4159	TrainAcc 0.6675	ValidAcc 0.6669	TestAcc 0.6574	BestValid 0.6669
	Epoch 2400:	Loss 1.4164	TrainAcc 0.6678	ValidAcc 0.6668	TestAcc 0.6568	BestValid 0.6669
	Epoch 2450:	Loss 1.4118	TrainAcc 0.6684	ValidAcc 0.6674	TestAcc 0.6575	BestValid 0.6674
	Epoch 2500:	Loss 1.4066	TrainAcc 0.6693	ValidAcc 0.6693	TestAcc 0.6622	BestValid 0.6693
	Epoch 2550:	Loss 1.4013	TrainAcc 0.6703	ValidAcc 0.6682	TestAcc 0.6584	BestValid 0.6693
	Epoch 2600:	Loss 1.3948	TrainAcc 0.6701	ValidAcc 0.6693	TestAcc 0.6602	BestValid 0.6693
	Epoch 2650:	Loss 1.3989	TrainAcc 0.6710	ValidAcc 0.6701	TestAcc 0.6606	BestValid 0.6701
	Epoch 2700:	Loss 1.3942	TrainAcc 0.6713	ValidAcc 0.6712	TestAcc 0.6626	BestValid 0.6712
	Epoch 2750:	Loss 1.3941	TrainAcc 0.6713	ValidAcc 0.6704	TestAcc 0.6616	BestValid 0.6712
	Epoch 2800:	Loss 1.3981	TrainAcc 0.6722	ValidAcc 0.6714	TestAcc 0.6618	BestValid 0.6714
	Epoch 2850:	Loss 1.3901	TrainAcc 0.6730	ValidAcc 0.6722	TestAcc 0.6639	BestValid 0.6722
	Epoch 2900:	Loss 1.3812	TrainAcc 0.6729	ValidAcc 0.6736	TestAcc 0.6644	BestValid 0.6736
	Epoch 2950:	Loss 1.3879	TrainAcc 0.6735	ValidAcc 0.6732	TestAcc 0.6628	BestValid 0.6736
	Epoch 3000:	Loss 1.3799	TrainAcc 0.6734	ValidAcc 0.6733	TestAcc 0.6643	BestValid 0.6736
	Epoch 3050:	Loss 1.3835	TrainAcc 0.6744	ValidAcc 0.6743	TestAcc 0.6649	BestValid 0.6743
	Epoch 3100:	Loss 1.3804	TrainAcc 0.6742	ValidAcc 0.6746	TestAcc 0.6652	BestValid 0.6746
	Epoch 3150:	Loss 1.3800	TrainAcc 0.6752	ValidAcc 0.6744	TestAcc 0.6641	BestValid 0.6746
	Epoch 3200:	Loss 1.3828	TrainAcc 0.6753	ValidAcc 0.6744	TestAcc 0.6649	BestValid 0.6746
	Epoch 3250:	Loss 1.3762	TrainAcc 0.6761	ValidAcc 0.6747	TestAcc 0.6662	BestValid 0.6747
	Epoch 3300:	Loss 1.3741	TrainAcc 0.6756	ValidAcc 0.6770	TestAcc 0.6692	BestValid 0.6770
	Epoch 3350:	Loss 1.3797	TrainAcc 0.6766	ValidAcc 0.6771	TestAcc 0.6680	BestValid 0.6771
	Epoch 3400:	Loss 1.3746	TrainAcc 0.6767	ValidAcc 0.6756	TestAcc 0.6672	BestValid 0.6771
	Epoch 3450:	Loss 1.3579	TrainAcc 0.6769	ValidAcc 0.6762	TestAcc 0.6678	BestValid 0.6771
	Epoch 3500:	Loss 1.3678	TrainAcc 0.6770	ValidAcc 0.6782	TestAcc 0.6709	BestValid 0.6782
	Epoch 3550:	Loss 1.3675	TrainAcc 0.6776	ValidAcc 0.6764	TestAcc 0.6677	BestValid 0.6782
	Epoch 3600:	Loss 1.3674	TrainAcc 0.6781	ValidAcc 0.6779	TestAcc 0.6708	BestValid 0.6782
	Epoch 3650:	Loss 1.3694	TrainAcc 0.6775	ValidAcc 0.6784	TestAcc 0.6701	BestValid 0.6784
	Epoch 3700:	Loss 1.3599	TrainAcc 0.6785	ValidAcc 0.6785	TestAcc 0.6714	BestValid 0.6785
	Epoch 3750:	Loss 1.3611	TrainAcc 0.6781	ValidAcc 0.6792	TestAcc 0.6714	BestValid 0.6792
	Epoch 3800:	Loss 1.3616	TrainAcc 0.6790	ValidAcc 0.6775	TestAcc 0.6684	BestValid 0.6792
	Epoch 3850:	Loss 1.3582	TrainAcc 0.6796	ValidAcc 0.6782	TestAcc 0.6703	BestValid 0.6792
	Epoch 3900:	Loss 1.3571	TrainAcc 0.6800	ValidAcc 0.6803	TestAcc 0.6720	BestValid 0.6803
	Epoch 3950:	Loss 1.3499	TrainAcc 0.6806	ValidAcc 0.6795	TestAcc 0.6708	BestValid 0.6803
	Epoch 4000:	Loss 1.3574	TrainAcc 0.6804	ValidAcc 0.6798	TestAcc 0.6721	BestValid 0.6803
	Epoch 4050:	Loss 1.3550	TrainAcc 0.6806	ValidAcc 0.6812	TestAcc 0.6730	BestValid 0.6812
	Epoch 4100:	Loss 1.3523	TrainAcc 0.6813	ValidAcc 0.6812	TestAcc 0.6739	BestValid 0.6812
	Epoch 4150:	Loss 1.3536	TrainAcc 0.6808	ValidAcc 0.6811	TestAcc 0.6738	BestValid 0.6812
	Epoch 4200:	Loss 1.3465	TrainAcc 0.6819	ValidAcc 0.6805	TestAcc 0.6718	BestValid 0.6812
	Epoch 4250:	Loss 1.3481	TrainAcc 0.6817	ValidAcc 0.6819	TestAcc 0.6740	BestValid 0.6819
	Epoch 4300:	Loss 1.3412	TrainAcc 0.6816	ValidAcc 0.6827	TestAcc 0.6749	BestValid 0.6827
	Epoch 4350:	Loss 1.3426	TrainAcc 0.6817	ValidAcc 0.6814	TestAcc 0.6727	BestValid 0.6827
	Epoch 4400:	Loss 1.3486	TrainAcc 0.6824	ValidAcc 0.6829	TestAcc 0.6763	BestValid 0.6829
	Epoch 4450:	Loss 1.3444	TrainAcc 0.6824	ValidAcc 0.6828	TestAcc 0.6759	BestValid 0.6829
	Epoch 4500:	Loss 1.3418	TrainAcc 0.6833	ValidAcc 0.6828	TestAcc 0.6743	BestValid 0.6829
	Epoch 4550:	Loss 1.3413	TrainAcc 0.6830	ValidAcc 0.6839	TestAcc 0.6752	BestValid 0.6839
	Epoch 4600:	Loss 1.3390	TrainAcc 0.6834	ValidAcc 0.6850	TestAcc 0.6782	BestValid 0.6850
	Epoch 4650:	Loss 1.3279	TrainAcc 0.6836	ValidAcc 0.6838	TestAcc 0.6772	BestValid 0.6850
	Epoch 4700:	Loss 1.3355	TrainAcc 0.6835	ValidAcc 0.6848	TestAcc 0.6768	BestValid 0.6850
	Epoch 4750:	Loss 1.3376	TrainAcc 0.6840	ValidAcc 0.6837	TestAcc 0.6759	BestValid 0.6850
	Epoch 4800:	Loss 1.3400	TrainAcc 0.6849	ValidAcc 0.6850	TestAcc 0.6775	BestValid 0.6850
	Epoch 4850:	Loss 1.3365	TrainAcc 0.6851	ValidAcc 0.6849	TestAcc 0.6780	BestValid 0.6850
	Epoch 4900:	Loss 1.3365	TrainAcc 0.6836	ValidAcc 0.6852	TestAcc 0.6787	BestValid 0.6852
	Epoch 4950:	Loss 1.3305	TrainAcc 0.6855	ValidAcc 0.6858	TestAcc 0.6787	BestValid 0.6858
	Epoch 5000:	Loss 1.3389	TrainAcc 0.6857	ValidAcc 0.6853	TestAcc 0.6770	BestValid 0.6858
****** Epoch Time (Excluding Evaluation Cost): 0.143 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.317 ms (Max: 0.553, Min: 0.034, Sum: 2.536)
Cluster-Wide Average, Compute: 43.620 ms (Max: 46.721, Min: 42.056, Sum: 348.964)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.014 ms (Max: 0.015, Min: 0.013, Sum: 0.113)
Cluster-Wide Average, Communication-Graph: 96.003 ms (Max: 97.381, Min: 93.174, Sum: 768.024)
Cluster-Wide Average, Optimization: 2.687 ms (Max: 2.703, Min: 2.670, Sum: 21.496)
Cluster-Wide Average, Others: 0.801 ms (Max: 0.828, Min: 0.772, Sum: 6.411)
****** Breakdown Sum: 143.451 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.615 GB (Max: 6.182, Min: 5.520, Sum: 44.919)
Cluster-Wide Average, Graph-Level Communication Throughput: 33.165 Gbps (Max: 63.996, Min: 10.634, Sum: 265.323)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.725 GB
Weight-sync communication (cluster-wide, per-epoch): 0.018 GB
Total communication (cluster-wide, per-epoch): 2.743 GB
****** Accuracy Results ******
Highest valid_acc: 0.6858
Target test_acc: 0.6787
Epoch to reach the target acc: 4949
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
