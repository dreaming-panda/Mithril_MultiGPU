Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INITDONE MPI INIT

Initialized node 1 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
        It takes 0.051 seconds.
        It takes 0.054 seconds.
        It takes 0.050 seconds.
        It takes 0.049 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
        It takes 0.068 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
        It takes 0.028 seconds.
        It takes 0.028 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.645 Gbps (per GPU), 485.162 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.344 Gbps (per GPU), 482.751 Gbps (aggregated)
The layer-level communication performance: 60.327 Gbps (per GPU), 482.619 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.081 Gbps (per GPU), 480.646 Gbps (aggregated)
The layer-level communication performance: 60.050 Gbps (per GPU), 480.398 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.832 Gbps (per GPU), 478.657 Gbps (aggregated)
The layer-level communication performance: 59.779 Gbps (per GPU), 478.233 Gbps (aggregated)
The layer-level communication performance: 59.746 Gbps (per GPU), 477.967 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.899 Gbps (per GPU), 1271.192 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.896 Gbps (per GPU), 1271.168 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.896 Gbps (per GPU), 1271.170 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.899 Gbps (per GPU), 1271.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.899 Gbps (per GPU), 1271.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.890 Gbps (per GPU), 1271.123 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.881 Gbps (per GPU), 1271.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.878 Gbps (per GPU), 1271.025 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.758 Gbps (per GPU), 838.065 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.755 Gbps (per GPU), 838.044 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.743 Gbps (per GPU), 837.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.756 Gbps (per GPU), 838.051 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.759 Gbps (per GPU), 838.071 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.756 Gbps (per GPU), 838.051 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.755 Gbps (per GPU), 838.043 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.756 Gbps (per GPU), 838.051 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.457 Gbps (per GPU), 291.657 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.456 Gbps (per GPU), 291.645 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.458 Gbps (per GPU), 291.660 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.453 Gbps (per GPU), 291.621 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.456 Gbps (per GPU), 291.645 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.454 Gbps (per GPU), 291.635 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.454 Gbps (per GPU), 291.631 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.451 Gbps (per GPU), 291.605 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.72ms  1.34ms  1.62ms  2.25 21.17K  0.46M
 chk_1  0.72ms  1.41ms  1.69ms  2.34 21.17K  0.55M
 chk_2  0.72ms  1.31ms  1.58ms  2.20 21.17K  0.39M
 chk_3  0.73ms  1.26ms  1.53ms  2.10 21.17K  0.24M
 chk_4  0.99ms  1.21ms  1.49ms  1.50 21.17K  0.17M
 chk_5  0.72ms  1.22ms  1.49ms  2.07 21.17K  0.22M
 chk_6  0.72ms  1.22ms  1.50ms  2.08 21.17K  0.16M
 chk_7  0.72ms  1.20ms  1.48ms  2.06 21.17K  0.12M
   Avg  0.75  1.27  1.55
   Max  0.99  1.41  1.69
   Min  0.72  1.20  1.48
 Ratio  1.38  1.18  1.14
   Var  0.01  0.01  0.00
Profiling takes 0.421 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 21167
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 42335, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 21167, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 63503, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 127007, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 84671, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 105839, Num Local Vertices: 21168
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 148175, Num Local Vertices: 21168
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 114305
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7420	TrainAcc 0.0173	ValidAcc 0.0094	TestAcc 0.0097	BestValid 0.0094
	Epoch 50:	Loss 2.9045	TrainAcc 0.2799	ValidAcc 0.3012	TestAcc 0.2707	BestValid 0.3012
	Epoch 100:	Loss 2.4427	TrainAcc 0.4016	ValidAcc 0.3817	TestAcc 0.3530	BestValid 0.3817
	Epoch 150:	Loss 2.2047	TrainAcc 0.4609	ValidAcc 0.4363	TestAcc 0.4101	BestValid 0.4363
	Epoch 200:	Loss 2.0563	TrainAcc 0.5043	ValidAcc 0.4933	TestAcc 0.4773	BestValid 0.4933
	Epoch 250:	Loss 1.9595	TrainAcc 0.5411	ValidAcc 0.5420	TestAcc 0.5255	BestValid 0.5420
	Epoch 300:	Loss 1.8887	TrainAcc 0.5547	ValidAcc 0.5571	TestAcc 0.5418	BestValid 0.5571
	Epoch 350:	Loss 1.8370	TrainAcc 0.5706	ValidAcc 0.5762	TestAcc 0.5627	BestValid 0.5762
	Epoch 400:	Loss 1.7891	TrainAcc 0.5850	ValidAcc 0.5938	TestAcc 0.5804	BestValid 0.5938
	Epoch 450:	Loss 1.7492	TrainAcc 0.5925	ValidAcc 0.5970	TestAcc 0.5836	BestValid 0.5970
	Epoch 500:	Loss 1.7081	TrainAcc 0.6006	ValidAcc 0.6067	TestAcc 0.5943	BestValid 0.6067
	Epoch 550:	Loss 1.6811	TrainAcc 0.6091	ValidAcc 0.6149	TestAcc 0.6029	BestValid 0.6149
	Epoch 600:	Loss 1.6550	TrainAcc 0.6138	ValidAcc 0.6199	TestAcc 0.6104	BestValid 0.6199
	Epoch 650:	Loss 1.6260	TrainAcc 0.6190	ValidAcc 0.6239	TestAcc 0.6132	BestValid 0.6239
	Epoch 700:	Loss 1.6209	TrainAcc 0.6230	ValidAcc 0.6291	TestAcc 0.6185	BestValid 0.6291
	Epoch 750:	Loss 1.5939	TrainAcc 0.6254	ValidAcc 0.6294	TestAcc 0.6201	BestValid 0.6294
	Epoch 800:	Loss 1.5783	TrainAcc 0.6274	ValidAcc 0.6309	TestAcc 0.6207	BestValid 0.6309
	Epoch 850:	Loss 1.5719	TrainAcc 0.6328	ValidAcc 0.6393	TestAcc 0.6289	BestValid 0.6393
	Epoch 900:	Loss 1.5550	TrainAcc 0.6350	ValidAcc 0.6364	TestAcc 0.6266	BestValid 0.6393
	Epoch 950:	Loss 1.5467	TrainAcc 0.6374	ValidAcc 0.6392	TestAcc 0.6280	BestValid 0.6393
	Epoch 1000:	Loss 1.5374	TrainAcc 0.6403	ValidAcc 0.6419	TestAcc 0.6312	BestValid 0.6419
	Epoch 1050:	Loss 1.5243	TrainAcc 0.6420	ValidAcc 0.6439	TestAcc 0.6345	BestValid 0.6439
	Epoch 1100:	Loss 1.5133	TrainAcc 0.6434	ValidAcc 0.6454	TestAcc 0.6354	BestValid 0.6454
	Epoch 1150:	Loss 1.5137	TrainAcc 0.6448	ValidAcc 0.6481	TestAcc 0.6391	BestValid 0.6481
	Epoch 1200:	Loss 1.5033	TrainAcc 0.6469	ValidAcc 0.6510	TestAcc 0.6428	BestValid 0.6510
	Epoch 1250:	Loss 1.5022	TrainAcc 0.6481	ValidAcc 0.6518	TestAcc 0.6423	BestValid 0.6518
	Epoch 1300:	Loss 1.4869	TrainAcc 0.6488	ValidAcc 0.6518	TestAcc 0.6425	BestValid 0.6518
	Epoch 1350:	Loss 1.4885	TrainAcc 0.6503	ValidAcc 0.6526	TestAcc 0.6422	BestValid 0.6526
	Epoch 1400:	Loss 1.4788	TrainAcc 0.6514	ValidAcc 0.6535	TestAcc 0.6441	BestValid 0.6535
	Epoch 1450:	Loss 1.4686	TrainAcc 0.6521	ValidAcc 0.6522	TestAcc 0.6418	BestValid 0.6535
	Epoch 1500:	Loss 1.4668	TrainAcc 0.6536	ValidAcc 0.6534	TestAcc 0.6431	BestValid 0.6535
	Epoch 1550:	Loss 1.4651	TrainAcc 0.6553	ValidAcc 0.6555	TestAcc 0.6447	BestValid 0.6555
	Epoch 1600:	Loss 1.4611	TrainAcc 0.6561	ValidAcc 0.6561	TestAcc 0.6459	BestValid 0.6561
	Epoch 1650:	Loss 1.4581	TrainAcc 0.6566	ValidAcc 0.6566	TestAcc 0.6471	BestValid 0.6566
	Epoch 1700:	Loss 1.4533	TrainAcc 0.6584	ValidAcc 0.6567	TestAcc 0.6464	BestValid 0.6567
	Epoch 1750:	Loss 1.4514	TrainAcc 0.6583	ValidAcc 0.6567	TestAcc 0.6478	BestValid 0.6567
	Epoch 1800:	Loss 1.4436	TrainAcc 0.6590	ValidAcc 0.6562	TestAcc 0.6469	BestValid 0.6567
	Epoch 1850:	Loss 1.4388	TrainAcc 0.6597	ValidAcc 0.6585	TestAcc 0.6498	BestValid 0.6585
	Epoch 1900:	Loss 1.4302	TrainAcc 0.6615	ValidAcc 0.6603	TestAcc 0.6523	BestValid 0.6603
	Epoch 1950:	Loss 1.4286	TrainAcc 0.6624	ValidAcc 0.6603	TestAcc 0.6514	BestValid 0.6603
	Epoch 2000:	Loss 1.4206	TrainAcc 0.6627	ValidAcc 0.6584	TestAcc 0.6494	BestValid 0.6603
	Epoch 2050:	Loss 1.4216	TrainAcc 0.6638	ValidAcc 0.6619	TestAcc 0.6530	BestValid 0.6619
	Epoch 2100:	Loss 1.4241	TrainAcc 0.6639	ValidAcc 0.6606	TestAcc 0.6512	BestValid 0.6619
	Epoch 2150:	Loss 1.4205	TrainAcc 0.6647	ValidAcc 0.6609	TestAcc 0.6517	BestValid 0.6619
	Epoch 2200:	Loss 1.4171	TrainAcc 0.6665	ValidAcc 0.6657	TestAcc 0.6579	BestValid 0.6657
	Epoch 2250:	Loss 1.4055	TrainAcc 0.6668	ValidAcc 0.6644	TestAcc 0.6550	BestValid 0.6657
	Epoch 2300:	Loss 1.4109	TrainAcc 0.6668	ValidAcc 0.6646	TestAcc 0.6561	BestValid 0.6657
	Epoch 2350:	Loss 1.4099	TrainAcc 0.6666	ValidAcc 0.6626	TestAcc 0.6533	BestValid 0.6657
	Epoch 2400:	Loss 1.4041	TrainAcc 0.6690	ValidAcc 0.6670	TestAcc 0.6575	BestValid 0.6670
	Epoch 2450:	Loss 1.4021	TrainAcc 0.6695	ValidAcc 0.6688	TestAcc 0.6597	BestValid 0.6688
	Epoch 2500:	Loss 1.4018	TrainAcc 0.6697	ValidAcc 0.6658	TestAcc 0.6561	BestValid 0.6688
	Epoch 2550:	Loss 1.3999	TrainAcc 0.6700	ValidAcc 0.6663	TestAcc 0.6563	BestValid 0.6688
	Epoch 2600:	Loss 1.3952	TrainAcc 0.6707	ValidAcc 0.6672	TestAcc 0.6583	BestValid 0.6688
	Epoch 2650:	Loss 1.3952	TrainAcc 0.6724	ValidAcc 0.6717	TestAcc 0.6625	BestValid 0.6717
	Epoch 2700:	Loss 1.3892	TrainAcc 0.6728	ValidAcc 0.6695	TestAcc 0.6604	BestValid 0.6717
	Epoch 2750:	Loss 1.3930	TrainAcc 0.6730	ValidAcc 0.6682	TestAcc 0.6584	BestValid 0.6717
	Epoch 2800:	Loss 1.3952	TrainAcc 0.6741	ValidAcc 0.6710	TestAcc 0.6617	BestValid 0.6717
	Epoch 2850:	Loss 1.3815	TrainAcc 0.6744	ValidAcc 0.6726	TestAcc 0.6642	BestValid 0.6726
	Epoch 2900:	Loss 1.3882	TrainAcc 0.6752	ValidAcc 0.6719	TestAcc 0.6634	BestValid 0.6726
	Epoch 2950:	Loss 1.3778	TrainAcc 0.6758	ValidAcc 0.6727	TestAcc 0.6649	BestValid 0.6727
	Epoch 3000:	Loss 1.3747	TrainAcc 0.6764	ValidAcc 0.6716	TestAcc 0.6631	BestValid 0.6727
	Epoch 3050:	Loss 1.3762	TrainAcc 0.6769	ValidAcc 0.6732	TestAcc 0.6644	BestValid 0.6732
	Epoch 3100:	Loss 1.3769	TrainAcc 0.6767	ValidAcc 0.6731	TestAcc 0.6645	BestValid 0.6732
	Epoch 3150:	Loss 1.3795	TrainAcc 0.6771	ValidAcc 0.6746	TestAcc 0.6660	BestValid 0.6746
	Epoch 3200:	Loss 1.3720	TrainAcc 0.6777	ValidAcc 0.6751	TestAcc 0.6674	BestValid 0.6751
	Epoch 3250:	Loss 1.3677	TrainAcc 0.6777	ValidAcc 0.6758	TestAcc 0.6678	BestValid 0.6758
	Epoch 3300:	Loss 1.3697	TrainAcc 0.6788	ValidAcc 0.6758	TestAcc 0.6679	BestValid 0.6758
	Epoch 3350:	Loss 1.3709	TrainAcc 0.6785	ValidAcc 0.6745	TestAcc 0.6648	BestValid 0.6758
	Epoch 3400:	Loss 1.3690	TrainAcc 0.6787	ValidAcc 0.6744	TestAcc 0.6650	BestValid 0.6758
	Epoch 3450:	Loss 1.3682	TrainAcc 0.6790	ValidAcc 0.6742	TestAcc 0.6652	BestValid 0.6758
	Epoch 3500:	Loss 1.3597	TrainAcc 0.6802	ValidAcc 0.6771	TestAcc 0.6684	BestValid 0.6771
	Epoch 3550:	Loss 1.3578	TrainAcc 0.6797	ValidAcc 0.6761	TestAcc 0.6673	BestValid 0.6771
	Epoch 3600:	Loss 1.3581	TrainAcc 0.6805	ValidAcc 0.6789	TestAcc 0.6717	BestValid 0.6789
	Epoch 3650:	Loss 1.3548	TrainAcc 0.6802	ValidAcc 0.6761	TestAcc 0.6675	BestValid 0.6789
	Epoch 3700:	Loss 1.3570	TrainAcc 0.6810	ValidAcc 0.6767	TestAcc 0.6674	BestValid 0.6789
	Epoch 3750:	Loss 1.3531	TrainAcc 0.6816	ValidAcc 0.6785	TestAcc 0.6698	BestValid 0.6789
	Epoch 3800:	Loss 1.3480	TrainAcc 0.6810	ValidAcc 0.6786	TestAcc 0.6699	BestValid 0.6789
	Epoch 3850:	Loss 1.3564	TrainAcc 0.6817	ValidAcc 0.6777	TestAcc 0.6680	BestValid 0.6789
	Epoch 3900:	Loss 1.3487	TrainAcc 0.6818	ValidAcc 0.6782	TestAcc 0.6691	BestValid 0.6789
	Epoch 3950:	Loss 1.3399	TrainAcc 0.6814	ValidAcc 0.6779	TestAcc 0.6681	BestValid 0.6789
	Epoch 4000:	Loss 1.3485	TrainAcc 0.6820	ValidAcc 0.6779	TestAcc 0.6679	BestValid 0.6789
	Epoch 4050:	Loss 1.3455	TrainAcc 0.6828	ValidAcc 0.6795	TestAcc 0.6703	BestValid 0.6795
	Epoch 4100:	Loss 1.3454	TrainAcc 0.6826	ValidAcc 0.6788	TestAcc 0.6709	BestValid 0.6795
	Epoch 4150:	Loss 1.3412	TrainAcc 0.6834	ValidAcc 0.6811	TestAcc 0.6750	BestValid 0.6811
	Epoch 4200:	Loss 1.3412	TrainAcc 0.6838	ValidAcc 0.6800	TestAcc 0.6711	BestValid 0.6811
	Epoch 4250:	Loss 1.3390	TrainAcc 0.6836	ValidAcc 0.6806	TestAcc 0.6720	BestValid 0.6811
	Epoch 4300:	Loss 1.3331	TrainAcc 0.6847	ValidAcc 0.6815	TestAcc 0.6732	BestValid 0.6815
	Epoch 4350:	Loss 1.3423	TrainAcc 0.6842	ValidAcc 0.6818	TestAcc 0.6738	BestValid 0.6818
	Epoch 4400:	Loss 1.3366	TrainAcc 0.6848	ValidAcc 0.6812	TestAcc 0.6738	BestValid 0.6818
	Epoch 4450:	Loss 1.3373	TrainAcc 0.6845	ValidAcc 0.6796	TestAcc 0.6703	BestValid 0.6818
	Epoch 4500:	Loss 1.3355	TrainAcc 0.6850	ValidAcc 0.6813	TestAcc 0.6732	BestValid 0.6818
	Epoch 4550:	Loss 1.3331	TrainAcc 0.6853	ValidAcc 0.6822	TestAcc 0.6753	BestValid 0.6822
	Epoch 4600:	Loss 1.3318	TrainAcc 0.6852	ValidAcc 0.6824	TestAcc 0.6734	BestValid 0.6824
	Epoch 4650:	Loss 1.3380	TrainAcc 0.6859	ValidAcc 0.6826	TestAcc 0.6754	BestValid 0.6826
	Epoch 4700:	Loss 1.3355	TrainAcc 0.6857	ValidAcc 0.6817	TestAcc 0.6731	BestValid 0.6826
	Epoch 4750:	Loss 1.3255	TrainAcc 0.6861	ValidAcc 0.6818	TestAcc 0.6756	BestValid 0.6826
	Epoch 4800:	Loss 1.3290	TrainAcc 0.6857	ValidAcc 0.6822	TestAcc 0.6731	BestValid 0.6826
	Epoch 4850:	Loss 1.3359	TrainAcc 0.6859	ValidAcc 0.6816	TestAcc 0.6740	BestValid 0.6826
	Epoch 4900:	Loss 1.3264	TrainAcc 0.6863	ValidAcc 0.6834	TestAcc 0.6750	BestValid 0.6834
	Epoch 4950:	Loss 1.3292	TrainAcc 0.6856	ValidAcc 0.6795	TestAcc 0.6698	BestValid 0.6834
	Epoch 5000:	Loss 1.3329	TrainAcc 0.6868	ValidAcc 0.6833	TestAcc 0.6769	BestValid 0.6834
****** Epoch Time (Excluding Evaluation Cost): 0.143 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.297 ms (Max: 0.536, Min: 0.034, Sum: 2.378)
Cluster-Wide Average, Compute: 43.727 ms (Max: 46.545, Min: 42.344, Sum: 349.814)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.065)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.119)
Cluster-Wide Average, Communication-Graph: 95.141 ms (Max: 96.322, Min: 92.617, Sum: 761.127)
Cluster-Wide Average, Optimization: 2.688 ms (Max: 2.698, Min: 2.671, Sum: 21.505)
Cluster-Wide Average, Others: 0.817 ms (Max: 0.835, Min: 0.788, Sum: 6.535)
****** Breakdown Sum: 142.693 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.615 GB (Max: 6.182, Min: 5.520, Sum: 44.919)
Cluster-Wide Average, Graph-Level Communication Throughput: 33.455 Gbps (Max: 64.367, Min: 10.804, Sum: 267.639)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.725 GB
Weight-sync communication (cluster-wide, per-epoch): 0.018 GB
Total communication (cluster-wide, per-epoch): 2.743 GB
****** Accuracy Results ******
Highest valid_acc: 0.6834
Target test_acc: 0.6750
Epoch to reach the target acc: 4899
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
