Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
        It takes 0.008 seconds.
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.007 seconds.
        It takes 0.009 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.013 seconds.
Building the Feature Vector...
        It takes 0.085 seconds.
        It takes 0.081 seconds.
        It takes 0.075 seconds.
        It takes 0.080 seconds.
        It takes 0.087 seconds.
        It takes 0.091 seconds.
        It takes 0.087 seconds.
        It takes 0.091 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...Building the Label Vector...
Building the Label Vector...
Building the Label Vector...

        It takes 0.001 seconds.
        It takes 0.001 seconds.
        It takes 0.001 seconds.
        It takes 0.001 seconds.
        It takes 0.001 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCN layers: 32
The number of hidden units: 1000
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.001 seconds.
        It takes 0.001 seconds.
        It takes 0.002 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.169 Gbps (per GPU), 449.354 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.911 Gbps (per GPU), 447.290 Gbps (aggregated)
The layer-level communication performance: 55.904 Gbps (per GPU), 447.232 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.681 Gbps (per GPU), 445.444 Gbps (aggregated)
The layer-level communication performance: 55.649 Gbps (per GPU), 445.188 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.464 Gbps (per GPU), 443.713 Gbps (aggregated)
The layer-level communication performance: 55.426 Gbps (per GPU), 443.405 Gbps (aggregated)
The layer-level communication performance: 55.394 Gbps (per GPU), 443.152 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 146.265 Gbps (per GPU), 1170.123 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.247 Gbps (per GPU), 1169.980 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.260 Gbps (per GPU), 1170.083 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.245 Gbps (per GPU), 1169.959 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.258 Gbps (per GPU), 1170.063 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.250 Gbps (per GPU), 1169.998 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.253 Gbps (per GPU), 1170.023 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.227 Gbps (per GPU), 1169.817 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 94.805 Gbps (per GPU), 758.441 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.790 Gbps (per GPU), 758.321 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.805 Gbps (per GPU), 758.441 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.770 Gbps (per GPU), 758.161 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.812 Gbps (per GPU), 758.498 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.739 Gbps (per GPU), 757.910 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.791 Gbps (per GPU), 758.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.688 Gbps (per GPU), 757.505 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.321 Gbps (per GPU), 266.569 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.322 Gbps (per GPU), 266.576 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.317 Gbps (per GPU), 266.533 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.322 Gbps (per GPU), 266.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.315 Gbps (per GPU), 266.521 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.314 Gbps (per GPU), 266.509 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.313 Gbps (per GPU), 266.501 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.311 Gbps (per GPU), 266.489 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.64ms  1.52ms  1.08ms  1.51  0.65K  0.22M
 chk_1  0.98ms  0.82ms  0.40ms  2.47  0.65K  0.05M
 chk_2  0.87ms  0.69ms  0.27ms  3.27  0.65K  0.02M
 chk_3  0.88ms  0.70ms  0.28ms  3.14  0.65K  0.02M
 chk_4  0.82ms  0.65ms  0.22ms  3.74  0.65K  0.01M
 chk_5  0.82ms  0.64ms  0.22ms  3.77  0.65K  0.01M
 chk_6  1.01ms  0.83ms  0.41ms  2.48  0.65K  0.06M
 chk_7  0.84ms  0.66ms  0.24ms  3.51  0.65K  0.01M
   Avg  0.98  0.81  0.39
   Max  1.64  1.52  1.08
   Min  0.82  0.64  0.22
 Ratio  2.00  2.38  4.99
   Var  0.07  0.08  0.07
Profiling takes 0.264 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 27.350 ms
Partition 0 [0, 4) has cost: 27.350 ms
Partition 1 [4, 8) has cost: 25.996 ms
Partition 2 [8, 12) has cost: 25.996 ms
Partition 3 [12, 16) has cost: 25.996 ms
Partition 4 [16, 20) has cost: 25.996 ms
Partition 5 [20, 24) has cost: 25.996 ms
Partition 6 [24, 28) has cost: 25.996 ms
Partition 7 [28, 32) has cost: 22.606 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 28.190 ms
GPU 0, Compute+Comm Time: 11.865 ms, Bubble Time: 9.412 ms, Imbalance Overhead: 6.913 ms
GPU 1, Compute+Comm Time: 11.131 ms, Bubble Time: 10.424 ms, Imbalance Overhead: 6.635 ms
GPU 2, Compute+Comm Time: 11.131 ms, Bubble Time: 11.365 ms, Imbalance Overhead: 5.694 ms
GPU 3, Compute+Comm Time: 11.131 ms, Bubble Time: 12.306 ms, Imbalance Overhead: 4.753 ms
GPU 4, Compute+Comm Time: 11.131 ms, Bubble Time: 13.247 ms, Imbalance Overhead: 3.812 ms
GPU 5, Compute+Comm Time: 11.131 ms, Bubble Time: 14.188 ms, Imbalance Overhead: 2.871 ms
GPU 6, Compute+Comm Time: 11.131 ms, Bubble Time: 15.255 ms, Imbalance Overhead: 1.803 ms
GPU 7, Compute+Comm Time: 10.121 ms, Bubble Time: 16.555 ms, Imbalance Overhead: 1.514 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 52.966 ms
GPU 0, Compute+Comm Time: 18.411 ms, Bubble Time: 31.290 ms, Imbalance Overhead: 3.265 ms
GPU 1, Compute+Comm Time: 20.791 ms, Bubble Time: 28.726 ms, Imbalance Overhead: 3.449 ms
GPU 2, Compute+Comm Time: 20.791 ms, Bubble Time: 26.619 ms, Imbalance Overhead: 5.556 ms
GPU 3, Compute+Comm Time: 20.791 ms, Bubble Time: 24.809 ms, Imbalance Overhead: 7.366 ms
GPU 4, Compute+Comm Time: 20.791 ms, Bubble Time: 22.998 ms, Imbalance Overhead: 9.177 ms
GPU 5, Compute+Comm Time: 20.791 ms, Bubble Time: 21.188 ms, Imbalance Overhead: 10.987 ms
GPU 6, Compute+Comm Time: 20.791 ms, Bubble Time: 19.377 ms, Imbalance Overhead: 12.798 ms
GPU 7, Compute+Comm Time: 21.411 ms, Bubble Time: 17.516 ms, Imbalance Overhead: 14.039 ms
The estimated cost of the whole pipeline: 85.214 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 53.346 ms
Partition 0 [0, 8) has cost: 53.346 ms
Partition 1 [8, 16) has cost: 51.993 ms
Partition 2 [16, 24) has cost: 51.993 ms
Partition 3 [24, 32) has cost: 48.603 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.661 ms
GPU 0, Compute+Comm Time: 15.446 ms, Bubble Time: 9.607 ms, Imbalance Overhead: 5.608 ms
GPU 1, Compute+Comm Time: 15.100 ms, Bubble Time: 11.560 ms, Imbalance Overhead: 4.001 ms
GPU 2, Compute+Comm Time: 15.100 ms, Bubble Time: 13.568 ms, Imbalance Overhead: 1.993 ms
GPU 3, Compute+Comm Time: 14.593 ms, Bubble Time: 15.907 ms, Imbalance Overhead: 0.162 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 54.065 ms
GPU 0, Compute+Comm Time: 25.142 ms, Bubble Time: 28.535 ms, Imbalance Overhead: 0.388 ms
GPU 1, Compute+Comm Time: 26.336 ms, Bubble Time: 23.928 ms, Imbalance Overhead: 3.801 ms
GPU 2, Compute+Comm Time: 26.336 ms, Bubble Time: 20.010 ms, Imbalance Overhead: 7.719 ms
GPU 3, Compute+Comm Time: 26.627 ms, Bubble Time: 16.338 ms, Imbalance Overhead: 11.100 ms
    The estimated cost with 2 DP ways is 88.963 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 105.339 ms
Partition 0 [0, 16) has cost: 105.339 ms
Partition 1 [16, 32) has cost: 100.595 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 40.043 ms
GPU 0, Compute+Comm Time: 25.692 ms, Bubble Time: 10.721 ms, Imbalance Overhead: 3.630 ms
GPU 1, Compute+Comm Time: 25.281 ms, Bubble Time: 14.762 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.389 ms
GPU 0, Compute+Comm Time: 38.215 ms, Bubble Time: 23.174 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.941 ms, Bubble Time: 15.394 ms, Imbalance Overhead: 7.053 ms
    The estimated cost with 4 DP ways is 106.504 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 205.934 ms
Partition 0 [0, 32) has cost: 205.934 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 85.832 ms
GPU 0, Compute+Comm Time: 85.832 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 102.504 ms
GPU 0, Compute+Comm Time: 102.504 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 197.754 ms

*** Node 0, starting model training...
*** Node 4, starting model training...
Num Stages: 1 / 1
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2055
