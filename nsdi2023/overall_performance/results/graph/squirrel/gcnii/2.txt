Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.006 seconds.
Building the Feature Vector...
        It takes 0.010 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.015 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.028 seconds.
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
Building the Label Vector...
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
        It takes 0.000 seconds.
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.033 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.254 Gbps (per GPU), 458.030 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.959 Gbps (per GPU), 455.670 Gbps (aggregated)
The layer-level communication performance: 56.955 Gbps (per GPU), 455.644 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.744 Gbps (per GPU), 453.949 Gbps (aggregated)
The layer-level communication performance: 56.708 Gbps (per GPU), 453.667 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.509 Gbps (per GPU), 452.076 Gbps (aggregated)
The layer-level communication performance: 56.467 Gbps (per GPU), 451.737 Gbps (aggregated)
The layer-level communication performance: 56.440 Gbps (per GPU), 451.520 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.913 Gbps (per GPU), 1247.307 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.925 Gbps (per GPU), 1247.400 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.914 Gbps (per GPU), 1247.309 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.869 Gbps (per GPU), 1246.956 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.911 Gbps (per GPU), 1247.289 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.919 Gbps (per GPU), 1247.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.919 Gbps (per GPU), 1247.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.873 Gbps (per GPU), 1246.988 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 98.515 Gbps (per GPU), 788.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.531 Gbps (per GPU), 788.249 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.516 Gbps (per GPU), 788.131 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.533 Gbps (per GPU), 788.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.516 Gbps (per GPU), 788.131 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.533 Gbps (per GPU), 788.267 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.481 Gbps (per GPU), 787.848 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.514 Gbps (per GPU), 788.113 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.352 Gbps (per GPU), 242.815 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.353 Gbps (per GPU), 242.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.349 Gbps (per GPU), 242.791 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.350 Gbps (per GPU), 242.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.346 Gbps (per GPU), 242.768 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.349 Gbps (per GPU), 242.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.345 Gbps (per GPU), 242.759 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.350 Gbps (per GPU), 242.799 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.03ms  1.68ms  1.80ms  1.74  0.65K  0.22M
 chk_1  1.03ms  0.98ms  1.12ms  1.14  0.65K  0.05M
 chk_2  1.05ms  0.84ms  0.98ms  1.25  0.65K  0.02M
 chk_3  1.05ms  0.85ms  1.00ms  1.23  0.65K  0.02M
 chk_4  1.05ms  0.80ms  0.94ms  1.30  0.65K  0.01M
 chk_5  1.05ms  0.79ms  0.94ms  1.33  0.65K  0.01M
 chk_6  1.05ms  0.99ms  1.13ms  1.14  0.65K  0.06M
 chk_7  1.05ms  0.81ms  0.96ms  1.29  0.65K  0.01M
   Avg  1.04  0.97  1.11
   Max  1.05  1.68  1.80
   Min  1.03  0.79  0.94
 Ratio  1.02  2.13  1.92
   Var  0.00  0.08  0.07
Profiling takes 0.362 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 38.778 ms
Partition 0 [0, 4) has cost: 31.624 ms
Partition 1 [4, 8) has cost: 31.023 ms
Partition 2 [8, 12) has cost: 31.023 ms
Partition 3 [12, 16) has cost: 31.023 ms
Partition 4 [16, 20) has cost: 31.023 ms
Partition 5 [20, 24) has cost: 31.023 ms
Partition 6 [24, 29) has cost: 38.778 ms
Partition 7 [29, 33) has cost: 32.140 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.838 ms
GPU 0, Compute+Comm Time: 12.446 ms, Bubble Time: 11.425 ms, Imbalance Overhead: 7.967 ms
GPU 1, Compute+Comm Time: 12.504 ms, Bubble Time: 12.044 ms, Imbalance Overhead: 7.291 ms
GPU 2, Compute+Comm Time: 12.504 ms, Bubble Time: 12.883 ms, Imbalance Overhead: 6.451 ms
GPU 3, Compute+Comm Time: 12.504 ms, Bubble Time: 13.801 ms, Imbalance Overhead: 5.533 ms
GPU 4, Compute+Comm Time: 12.504 ms, Bubble Time: 14.719 ms, Imbalance Overhead: 4.615 ms
GPU 5, Compute+Comm Time: 12.504 ms, Bubble Time: 15.329 ms, Imbalance Overhead: 4.005 ms
GPU 6, Compute+Comm Time: 14.903 ms, Bubble Time: 16.215 ms, Imbalance Overhead: 0.719 ms
GPU 7, Compute+Comm Time: 12.768 ms, Bubble Time: 17.875 ms, Imbalance Overhead: 1.195 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 63.072 ms
GPU 0, Compute+Comm Time: 25.185 ms, Bubble Time: 35.428 ms, Imbalance Overhead: 2.458 ms
GPU 1, Compute+Comm Time: 29.689 ms, Bubble Time: 32.061 ms, Imbalance Overhead: 1.322 ms
GPU 2, Compute+Comm Time: 24.333 ms, Bubble Time: 30.317 ms, Imbalance Overhead: 8.422 ms
GPU 3, Compute+Comm Time: 24.333 ms, Bubble Time: 29.150 ms, Imbalance Overhead: 9.589 ms
GPU 4, Compute+Comm Time: 24.333 ms, Bubble Time: 27.312 ms, Imbalance Overhead: 11.427 ms
GPU 5, Compute+Comm Time: 24.333 ms, Bubble Time: 25.509 ms, Imbalance Overhead: 13.231 ms
GPU 6, Compute+Comm Time: 24.333 ms, Bubble Time: 23.878 ms, Imbalance Overhead: 14.861 ms
GPU 7, Compute+Comm Time: 24.992 ms, Bubble Time: 22.598 ms, Imbalance Overhead: 15.481 ms
The estimated cost of the whole pipeline: 99.655 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 69.801 ms
Partition 0 [0, 8) has cost: 62.647 ms
Partition 1 [8, 16) has cost: 62.045 ms
Partition 2 [16, 25) has cost: 69.801 ms
Partition 3 [25, 33) has cost: 63.162 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 33.793 ms
GPU 0, Compute+Comm Time: 15.821 ms, Bubble Time: 11.242 ms, Imbalance Overhead: 6.731 ms
GPU 1, Compute+Comm Time: 16.320 ms, Bubble Time: 12.393 ms, Imbalance Overhead: 5.080 ms
GPU 2, Compute+Comm Time: 17.997 ms, Bubble Time: 14.197 ms, Imbalance Overhead: 1.599 ms
GPU 3, Compute+Comm Time: 16.448 ms, Bubble Time: 16.968 ms, Imbalance Overhead: 0.378 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 63.064 ms
GPU 0, Compute+Comm Time: 30.263 ms, Bubble Time: 32.084 ms, Imbalance Overhead: 0.717 ms
GPU 1, Compute+Comm Time: 33.216 ms, Bubble Time: 26.532 ms, Imbalance Overhead: 3.316 ms
GPU 2, Compute+Comm Time: 29.848 ms, Bubble Time: 22.936 ms, Imbalance Overhead: 10.279 ms
GPU 3, Compute+Comm Time: 29.503 ms, Bubble Time: 20.445 ms, Imbalance Overhead: 13.115 ms
    The estimated cost with 2 DP ways is 101.700 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 132.448 ms
Partition 0 [0, 17) has cost: 132.448 ms
Partition 1 [17, 33) has cost: 125.207 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.965 ms
GPU 0, Compute+Comm Time: 27.081 ms, Bubble Time: 11.403 ms, Imbalance Overhead: 3.482 ms
GPU 1, Compute+Comm Time: 26.550 ms, Bubble Time: 15.416 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 68.073 ms
GPU 0, Compute+Comm Time: 42.423 ms, Bubble Time: 25.649 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 43.739 ms, Bubble Time: 17.440 ms, Imbalance Overhead: 6.893 ms
    The estimated cost with 4 DP ways is 115.540 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 257.655 ms
Partition 0 [0, 33) has cost: 257.655 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 94.383 ms
GPU 0, Compute+Comm Time: 94.383 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 114.451 ms
GPU 0, Compute+Comm Time: 114.451 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 219.276 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.1647	TrainAcc 0.5909	ValidAcc 0.3173	TestAcc 0.3045	BestValid 0.3173
	Epoch 100:	Loss 0.9756	TrainAcc 0.6246	ValidAcc 0.3113	TestAcc 0.2891	BestValid 0.3173
	Epoch 150:	Loss 0.8900	TrainAcc 0.6835	ValidAcc 0.3101	TestAcc 0.2920	BestValid 0.3173
	Epoch 200:	Loss 0.8009	TrainAcc 0.7188	ValidAcc 0.3143	TestAcc 0.2978	BestValid 0.3173
	Epoch 250:	Loss 0.7589	TrainAcc 0.7688	ValidAcc 0.3173	TestAcc 0.3045	BestValid 0.3173
	Epoch 300:	Loss 0.7241	TrainAcc 0.7560	ValidAcc 0.3185	TestAcc 0.2901	BestValid 0.3185
	Epoch 350:	Loss 0.6797	TrainAcc 0.7676	ValidAcc 0.3143	TestAcc 0.2930	BestValid 0.3185
	Epoch 400:	Loss 0.6284	TrainAcc 0.7853	ValidAcc 0.3179	TestAcc 0.2988	BestValid 0.3185
	Epoch 450:	Loss 0.6156	TrainAcc 0.7724	ValidAcc 0.3113	TestAcc 0.2911	BestValid 0.3185
	Epoch 500:	Loss 0.5711	TrainAcc 0.8017	ValidAcc 0.3185	TestAcc 0.2959	BestValid 0.3185
	Epoch 550:	Loss 0.5483	TrainAcc 0.7857	ValidAcc 0.3065	TestAcc 0.2882	BestValid 0.3185
	Epoch 600:	Loss 0.5258	TrainAcc 0.8089	ValidAcc 0.3155	TestAcc 0.2920	BestValid 0.3185
	Epoch 650:	Loss 0.4962	TrainAcc 0.8165	ValidAcc 0.3143	TestAcc 0.2939	BestValid 0.3185
	Epoch 700:	Loss 0.4821	TrainAcc 0.8249	ValidAcc 0.3113	TestAcc 0.2968	BestValid 0.3185
	Epoch 750:	Loss 0.4571	TrainAcc 0.8570	ValidAcc 0.3245	TestAcc 0.3055	BestValid 0.3245
	Epoch 800:	Loss 0.4381	TrainAcc 0.8502	ValidAcc 0.3209	TestAcc 0.3007	BestValid 0.3245
	Epoch 850:	Loss 0.4323	TrainAcc 0.8522	ValidAcc 0.3179	TestAcc 0.2978	BestValid 0.3245
	Epoch 900:	Loss 0.4090	TrainAcc 0.8534	ValidAcc 0.3173	TestAcc 0.2988	BestValid 0.3245
	Epoch 950:	Loss 0.3808	TrainAcc 0.8602	ValidAcc 0.3215	TestAcc 0.2997	BestValid 0.3245
	Epoch 1000:	Loss 0.3779	TrainAcc 0.8786	ValidAcc 0.3215	TestAcc 0.3026	BestValid 0.3245
****** Epoch Time (Excluding Evaluation Cost): 0.199 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.122 ms (Max: 0.172, Min: 0.039, Sum: 0.975)
Cluster-Wide Average, Compute: 32.893 ms (Max: 47.345, Min: 29.466, Sum: 263.141)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.122)
Cluster-Wide Average, Communication-Graph: 114.061 ms (Max: 117.518, Min: 99.587, Sum: 912.491)
Cluster-Wide Average, Optimization: 47.302 ms (Max: 47.446, Min: 47.168, Sum: 378.419)
Cluster-Wide Average, Others: 4.152 ms (Max: 4.227, Min: 4.100, Sum: 33.215)
****** Breakdown Sum: 198.553 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.532 GB (Max: 5.975, Min: 5.456, Sum: 44.259)
Cluster-Wide Average, Graph-Level Communication Throughput: 27.875 Gbps (Max: 48.006, Min: 9.709, Sum: 223.003)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.753 GB
Weight-sync communication (cluster-wide, per-epoch): 1.778 GB
Total communication (cluster-wide, per-epoch): 4.531 GB
****** Accuracy Results ******
Highest valid_acc: 0.3245
Target test_acc: 0.3055
Epoch to reach the target acc: 749
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
