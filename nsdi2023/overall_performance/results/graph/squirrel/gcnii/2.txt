Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.006 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the Feature Vector...
Building the Feature Vector...        It takes 0.013 seconds.

        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
Building the Feature Vector...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.000 seconds.
        It takes 0.009 seconds.
        It takes 0.010 seconds.
        It takes 0.010 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
Building the Feature Vector...
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.668 Gbps (per GPU), 477.343 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.384 Gbps (per GPU), 475.074 Gbps (aggregated)
The layer-level communication performance: 59.395 Gbps (per GPU), 475.164 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.141 Gbps (per GPU), 473.127 Gbps (aggregated)
The layer-level communication performance: 59.115 Gbps (per GPU), 472.917 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.895 Gbps (per GPU), 471.157 Gbps (aggregated)
The layer-level communication performance: 58.820 Gbps (per GPU), 470.556 Gbps (aggregated)
The layer-level communication performance: 58.848 Gbps (per GPU), 470.782 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.067 Gbps (per GPU), 1256.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.078 Gbps (per GPU), 1256.626 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.067 Gbps (per GPU), 1256.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.019 Gbps (per GPU), 1256.156 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.057 Gbps (per GPU), 1256.459 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.084 Gbps (per GPU), 1256.673 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.055 Gbps (per GPU), 1256.438 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.028 Gbps (per GPU), 1256.224 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.910 Gbps (per GPU), 807.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.912 Gbps (per GPU), 807.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.910 Gbps (per GPU), 807.283 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.912 Gbps (per GPU), 807.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.912 Gbps (per GPU), 807.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.912 Gbps (per GPU), 807.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.913 Gbps (per GPU), 807.302 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.913 Gbps (per GPU), 807.302 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.160 Gbps (per GPU), 257.278 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.161 Gbps (per GPU), 257.286 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.155 Gbps (per GPU), 257.238 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.159 Gbps (per GPU), 257.275 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.151 Gbps (per GPU), 257.208 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.154 Gbps (per GPU), 257.235 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.151 Gbps (per GPU), 257.204 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.156 Gbps (per GPU), 257.249 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.02ms  1.66ms  1.79ms  1.75  0.65K  0.22M
 chk_1  1.02ms  0.96ms  1.10ms  1.14  0.65K  0.05M
 chk_2  1.02ms  0.83ms  0.97ms  1.23  0.65K  0.02M
 chk_3  1.04ms  0.84ms  0.98ms  1.23  0.65K  0.02M
 chk_4  1.04ms  0.79ms  0.93ms  1.31  0.65K  0.01M
 chk_5  1.04ms  0.78ms  0.92ms  1.33  0.65K  0.01M
 chk_6  1.04ms  0.98ms  1.12ms  1.14  0.65K  0.06M
 chk_7  1.04ms  0.80ms  0.94ms  1.30  0.65K  0.01M
   Avg  1.03  0.96  1.09
   Max  1.04  1.66  1.79
   Min  1.02  0.78  0.92
 Ratio  1.02  2.12  1.94
   Var  0.00  0.08  0.07
Profiling takes 0.361 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 38.246 ms
Partition 0 [0, 4) has cost: 31.195 ms
Partition 1 [4, 8) has cost: 30.597 ms
Partition 2 [8, 12) has cost: 30.597 ms
Partition 3 [12, 16) has cost: 30.597 ms
Partition 4 [16, 20) has cost: 30.597 ms
Partition 5 [20, 24) has cost: 30.597 ms
Partition 6 [24, 29) has cost: 38.246 ms
Partition 7 [29, 33) has cost: 31.687 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.288 ms
GPU 0, Compute+Comm Time: 12.193 ms, Bubble Time: 11.206 ms, Imbalance Overhead: 7.889 ms
GPU 1, Compute+Comm Time: 12.257 ms, Bubble Time: 11.812 ms, Imbalance Overhead: 7.219 ms
GPU 2, Compute+Comm Time: 12.257 ms, Bubble Time: 12.643 ms, Imbalance Overhead: 6.388 ms
GPU 3, Compute+Comm Time: 12.257 ms, Bubble Time: 13.551 ms, Imbalance Overhead: 5.480 ms
GPU 4, Compute+Comm Time: 12.257 ms, Bubble Time: 14.460 ms, Imbalance Overhead: 4.571 ms
GPU 5, Compute+Comm Time: 12.257 ms, Bubble Time: 15.064 ms, Imbalance Overhead: 3.967 ms
GPU 6, Compute+Comm Time: 14.624 ms, Bubble Time: 15.941 ms, Imbalance Overhead: 0.723 ms
GPU 7, Compute+Comm Time: 12.512 ms, Bubble Time: 17.581 ms, Imbalance Overhead: 1.195 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.960 ms
GPU 0, Compute+Comm Time: 24.753 ms, Bubble Time: 34.747 ms, Imbalance Overhead: 2.460 ms
GPU 1, Compute+Comm Time: 29.201 ms, Bubble Time: 31.442 ms, Imbalance Overhead: 1.317 ms
GPU 2, Compute+Comm Time: 23.919 ms, Bubble Time: 29.745 ms, Imbalance Overhead: 8.297 ms
GPU 3, Compute+Comm Time: 23.919 ms, Bubble Time: 28.618 ms, Imbalance Overhead: 9.424 ms
GPU 4, Compute+Comm Time: 23.919 ms, Bubble Time: 26.831 ms, Imbalance Overhead: 11.211 ms
GPU 5, Compute+Comm Time: 23.919 ms, Bubble Time: 25.072 ms, Imbalance Overhead: 12.970 ms
GPU 6, Compute+Comm Time: 23.919 ms, Bubble Time: 23.492 ms, Imbalance Overhead: 14.550 ms
GPU 7, Compute+Comm Time: 24.581 ms, Bubble Time: 22.257 ms, Imbalance Overhead: 15.122 ms
The estimated cost of the whole pipeline: 97.911 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 68.844 ms
Partition 0 [0, 8) has cost: 61.793 ms
Partition 1 [8, 16) has cost: 61.194 ms
Partition 2 [16, 25) has cost: 68.844 ms
Partition 3 [25, 33) has cost: 62.284 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 33.244 ms
GPU 0, Compute+Comm Time: 15.536 ms, Bubble Time: 11.038 ms, Imbalance Overhead: 6.670 ms
GPU 1, Compute+Comm Time: 16.032 ms, Bubble Time: 12.178 ms, Imbalance Overhead: 5.035 ms
GPU 2, Compute+Comm Time: 17.687 ms, Bubble Time: 13.963 ms, Imbalance Overhead: 1.594 ms
GPU 3, Compute+Comm Time: 16.158 ms, Bubble Time: 16.704 ms, Imbalance Overhead: 0.382 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.987 ms
GPU 0, Compute+Comm Time: 29.761 ms, Bubble Time: 31.490 ms, Imbalance Overhead: 0.736 ms
GPU 1, Compute+Comm Time: 32.671 ms, Bubble Time: 26.071 ms, Imbalance Overhead: 3.246 ms
GPU 2, Compute+Comm Time: 29.351 ms, Bubble Time: 22.570 ms, Imbalance Overhead: 10.067 ms
GPU 3, Compute+Comm Time: 29.024 ms, Bubble Time: 20.152 ms, Imbalance Overhead: 12.812 ms
    The estimated cost with 2 DP ways is 99.993 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 130.636 ms
Partition 0 [0, 17) has cost: 130.636 ms
Partition 1 [17, 33) has cost: 123.478 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.134 ms
GPU 0, Compute+Comm Time: 26.528 ms, Bubble Time: 11.142 ms, Imbalance Overhead: 3.465 ms
GPU 1, Compute+Comm Time: 26.008 ms, Bubble Time: 15.126 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 66.704 ms
GPU 0, Compute+Comm Time: 41.568 ms, Bubble Time: 25.137 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 42.866 ms, Bubble Time: 17.082 ms, Imbalance Overhead: 6.756 ms
    The estimated cost with 4 DP ways is 113.230 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 254.114 ms
Partition 0 [0, 33) has cost: 254.114 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 89.856 ms
GPU 0, Compute+Comm Time: 89.856 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 109.486 ms
GPU 0, Compute+Comm Time: 109.486 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 209.309 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6137	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 1.1647	TrainAcc 0.5909	ValidAcc 0.3173	TestAcc 0.3045	BestValid 0.3173
	Epoch 100:	Loss 0.9756	TrainAcc 0.6246	ValidAcc 0.3113	TestAcc 0.2891	BestValid 0.3173
	Epoch 150:	Loss 0.8900	TrainAcc 0.6835	ValidAcc 0.3101	TestAcc 0.2920	BestValid 0.3173
	Epoch 200:	Loss 0.8009	TrainAcc 0.7188	ValidAcc 0.3143	TestAcc 0.2978	BestValid 0.3173
	Epoch 250:	Loss 0.7589	TrainAcc 0.7692	ValidAcc 0.3167	TestAcc 0.3045	BestValid 0.3173
	Epoch 300:	Loss 0.7241	TrainAcc 0.7560	ValidAcc 0.3185	TestAcc 0.2911	BestValid 0.3185
	Epoch 350:	Loss 0.6797	TrainAcc 0.7668	ValidAcc 0.3137	TestAcc 0.2930	BestValid 0.3185
	Epoch 400:	Loss 0.6284	TrainAcc 0.7849	ValidAcc 0.3179	TestAcc 0.2988	BestValid 0.3185
	Epoch 450:	Loss 0.6156	TrainAcc 0.7724	ValidAcc 0.3113	TestAcc 0.2911	BestValid 0.3185
	Epoch 500:	Loss 0.5710	TrainAcc 0.8013	ValidAcc 0.3185	TestAcc 0.2959	BestValid 0.3185
	Epoch 550:	Loss 0.5483	TrainAcc 0.7857	ValidAcc 0.3065	TestAcc 0.2882	BestValid 0.3185
	Epoch 600:	Loss 0.5257	TrainAcc 0.8085	ValidAcc 0.3143	TestAcc 0.2920	BestValid 0.3185
	Epoch 650:	Loss 0.4962	TrainAcc 0.8161	ValidAcc 0.3143	TestAcc 0.2939	BestValid 0.3185
	Epoch 700:	Loss 0.4820	TrainAcc 0.8253	ValidAcc 0.3113	TestAcc 0.2968	BestValid 0.3185
	Epoch 750:	Loss 0.4570	TrainAcc 0.8566	ValidAcc 0.3239	TestAcc 0.3045	BestValid 0.3239
	Epoch 800:	Loss 0.4381	TrainAcc 0.8502	ValidAcc 0.3209	TestAcc 0.3007	BestValid 0.3239
	Epoch 850:	Loss 0.4322	TrainAcc 0.8522	ValidAcc 0.3179	TestAcc 0.2968	BestValid 0.3239
	Epoch 900:	Loss 0.4090	TrainAcc 0.8534	ValidAcc 0.3167	TestAcc 0.2988	BestValid 0.3239
	Epoch 950:	Loss 0.3808	TrainAcc 0.8606	ValidAcc 0.3215	TestAcc 0.3007	BestValid 0.3239
	Epoch 1000:	Loss 0.3779	TrainAcc 0.8786	ValidAcc 0.3215	TestAcc 0.3016	BestValid 0.3239
	Epoch 1050:	Loss 0.3516	TrainAcc 0.8782	ValidAcc 0.3251	TestAcc 0.3016	BestValid 0.3251
	Epoch 1100:	Loss 0.3456	TrainAcc 0.8734	ValidAcc 0.3209	TestAcc 0.3026	BestValid 0.3251
	Epoch 1150:	Loss 0.3446	TrainAcc 0.8758	ValidAcc 0.3209	TestAcc 0.3064	BestValid 0.3251
	Epoch 1200:	Loss 0.3325	TrainAcc 0.8838	ValidAcc 0.3233	TestAcc 0.3112	BestValid 0.3251
	Epoch 1250:	Loss 0.3132	TrainAcc 0.8914	ValidAcc 0.3251	TestAcc 0.3103	BestValid 0.3251
	Epoch 1300:	Loss 0.3060	TrainAcc 0.8890	ValidAcc 0.3239	TestAcc 0.3064	BestValid 0.3251
	Epoch 1350:	Loss 0.3045	TrainAcc 0.9083	ValidAcc 0.3269	TestAcc 0.3199	BestValid 0.3269
	Epoch 1400:	Loss 0.2913	TrainAcc 0.9018	ValidAcc 0.3239	TestAcc 0.3122	BestValid 0.3269
	Epoch 1450:	Loss 0.2902	TrainAcc 0.9071	ValidAcc 0.3263	TestAcc 0.3132	BestValid 0.3269
	Epoch 1500:	Loss 0.2676	TrainAcc 0.9135	ValidAcc 0.3281	TestAcc 0.3122	BestValid 0.3281
	Epoch 1550:	Loss 0.2761	TrainAcc 0.9018	ValidAcc 0.3233	TestAcc 0.3074	BestValid 0.3281
	Epoch 1600:	Loss 0.2814	TrainAcc 0.9111	ValidAcc 0.3275	TestAcc 0.3112	BestValid 0.3281
	Epoch 1650:	Loss 0.2580	TrainAcc 0.9123	ValidAcc 0.3269	TestAcc 0.3122	BestValid 0.3281
	Epoch 1700:	Loss 0.2521	TrainAcc 0.9115	ValidAcc 0.3269	TestAcc 0.3141	BestValid 0.3281
	Epoch 1750:	Loss 0.2438	TrainAcc 0.9151	ValidAcc 0.3275	TestAcc 0.3112	BestValid 0.3281
	Epoch 1800:	Loss 0.2505	TrainAcc 0.9219	ValidAcc 0.3257	TestAcc 0.3122	BestValid 0.3281
	Epoch 1850:	Loss 0.2401	TrainAcc 0.9207	ValidAcc 0.3287	TestAcc 0.3132	BestValid 0.3287
	Epoch 1900:	Loss 0.2326	TrainAcc 0.9291	ValidAcc 0.3329	TestAcc 0.3151	BestValid 0.3329
	Epoch 1950:	Loss 0.2180	TrainAcc 0.9255	ValidAcc 0.3299	TestAcc 0.3132	BestValid 0.3329
	Epoch 2000:	Loss 0.2386	TrainAcc 0.9175	ValidAcc 0.3269	TestAcc 0.3122	BestValid 0.3329
	Epoch 2050:	Loss 0.2345	TrainAcc 0.9255	ValidAcc 0.3305	TestAcc 0.3141	BestValid 0.3329
	Epoch 2100:	Loss 0.2132	TrainAcc 0.9383	ValidAcc 0.3341	TestAcc 0.3228	BestValid 0.3341
	Epoch 2150:	Loss 0.2170	TrainAcc 0.9363	ValidAcc 0.3299	TestAcc 0.3180	BestValid 0.3341
	Epoch 2200:	Loss 0.2239	TrainAcc 0.9255	ValidAcc 0.3203	TestAcc 0.3112	BestValid 0.3341
	Epoch 2250:	Loss 0.2065	TrainAcc 0.9303	ValidAcc 0.3245	TestAcc 0.3103	BestValid 0.3341
	Epoch 2300:	Loss 0.1974	TrainAcc 0.9399	ValidAcc 0.3317	TestAcc 0.3170	BestValid 0.3341
	Epoch 2350:	Loss 0.2034	TrainAcc 0.9431	ValidAcc 0.3293	TestAcc 0.3228	BestValid 0.3341
	Epoch 2400:	Loss 0.2002	TrainAcc 0.9463	ValidAcc 0.3293	TestAcc 0.3266	BestValid 0.3341
	Epoch 2450:	Loss 0.1848	TrainAcc 0.9475	ValidAcc 0.3281	TestAcc 0.3247	BestValid 0.3341
	Epoch 2500:	Loss 0.1982	TrainAcc 0.9511	ValidAcc 0.3323	TestAcc 0.3266	BestValid 0.3341
	Epoch 2550:	Loss 0.1895	TrainAcc 0.9431	ValidAcc 0.3263	TestAcc 0.3237	BestValid 0.3341
	Epoch 2600:	Loss 0.1927	TrainAcc 0.9487	ValidAcc 0.3281	TestAcc 0.3285	BestValid 0.3341
	Epoch 2650:	Loss 0.1868	TrainAcc 0.9495	ValidAcc 0.3293	TestAcc 0.3276	BestValid 0.3341
	Epoch 2700:	Loss 0.1944	TrainAcc 0.9511	ValidAcc 0.3299	TestAcc 0.3256	BestValid 0.3341
	Epoch 2750:	Loss 0.1791	TrainAcc 0.9527	ValidAcc 0.3311	TestAcc 0.3266	BestValid 0.3341
	Epoch 2800:	Loss 0.1612	TrainAcc 0.9547	ValidAcc 0.3323	TestAcc 0.3362	BestValid 0.3341
	Epoch 2850:	Loss 0.1744	TrainAcc 0.9543	ValidAcc 0.3317	TestAcc 0.3295	BestValid 0.3341
	Epoch 2900:	Loss 0.1718	TrainAcc 0.9567	ValidAcc 0.3311	TestAcc 0.3305	BestValid 0.3341
	Epoch 2950:	Loss 0.1861	TrainAcc 0.9547	ValidAcc 0.3305	TestAcc 0.3266	BestValid 0.3341
	Epoch 3000:	Loss 0.1716	TrainAcc 0.9591	ValidAcc 0.3323	TestAcc 0.3343	BestValid 0.3341
	Epoch 3050:	Loss 0.1759	TrainAcc 0.9591	ValidAcc 0.3311	TestAcc 0.3324	BestValid 0.3341
	Epoch 3100:	Loss 0.1763	TrainAcc 0.9535	ValidAcc 0.3257	TestAcc 0.3247	BestValid 0.3341
	Epoch 3150:	Loss 0.1727	TrainAcc 0.9531	ValidAcc 0.3299	TestAcc 0.3285	BestValid 0.3341
	Epoch 3200:	Loss 0.1613	TrainAcc 0.9543	ValidAcc 0.3287	TestAcc 0.3276	BestValid 0.3341
	Epoch 3250:	Loss 0.1646	TrainAcc 0.9651	ValidAcc 0.3347	TestAcc 0.3343	BestValid 0.3347
	Epoch 3300:	Loss 0.1647	TrainAcc 0.9643	ValidAcc 0.3311	TestAcc 0.3324	BestValid 0.3347
	Epoch 3350:	Loss 0.1593	TrainAcc 0.9603	ValidAcc 0.3335	TestAcc 0.3314	BestValid 0.3347
	Epoch 3400:	Loss 0.1447	TrainAcc 0.9631	ValidAcc 0.3299	TestAcc 0.3372	BestValid 0.3347
	Epoch 3450:	Loss 0.1642	TrainAcc 0.9627	ValidAcc 0.3293	TestAcc 0.3333	BestValid 0.3347
	Epoch 3500:	Loss 0.1569	TrainAcc 0.9635	ValidAcc 0.3317	TestAcc 0.3305	BestValid 0.3347
	Epoch 3550:	Loss 0.1480	TrainAcc 0.9635	ValidAcc 0.3317	TestAcc 0.3362	BestValid 0.3347
	Epoch 3600:	Loss 0.1442	TrainAcc 0.9635	ValidAcc 0.3269	TestAcc 0.3276	BestValid 0.3347
	Epoch 3650:	Loss 0.1563	TrainAcc 0.9708	ValidAcc 0.3347	TestAcc 0.3410	BestValid 0.3347
	Epoch 3700:	Loss 0.1572	TrainAcc 0.9692	ValidAcc 0.3365	TestAcc 0.3420	BestValid 0.3365
	Epoch 3750:	Loss 0.1377	TrainAcc 0.9683	ValidAcc 0.3353	TestAcc 0.3401	BestValid 0.3365
	Epoch 3800:	Loss 0.1509	TrainAcc 0.9728	ValidAcc 0.3401	TestAcc 0.3468	BestValid 0.3401
	Epoch 3850:	Loss 0.1404	TrainAcc 0.9716	ValidAcc 0.3419	TestAcc 0.3429	BestValid 0.3419
	Epoch 3900:	Loss 0.1377	TrainAcc 0.9704	ValidAcc 0.3383	TestAcc 0.3401	BestValid 0.3419
	Epoch 3950:	Loss 0.1433	TrainAcc 0.9704	ValidAcc 0.3407	TestAcc 0.3353	BestValid 0.3419
	Epoch 4000:	Loss 0.1201	TrainAcc 0.9688	ValidAcc 0.3347	TestAcc 0.3333	BestValid 0.3419
	Epoch 4050:	Loss 0.1403	TrainAcc 0.9736	ValidAcc 0.3377	TestAcc 0.3381	BestValid 0.3419
	Epoch 4100:	Loss 0.1304	TrainAcc 0.9708	ValidAcc 0.3341	TestAcc 0.3420	BestValid 0.3419
	Epoch 4150:	Loss 0.1353	TrainAcc 0.9772	ValidAcc 0.3456	TestAcc 0.3458	BestValid 0.3456
	Epoch 4200:	Loss 0.1352	TrainAcc 0.9764	ValidAcc 0.3371	TestAcc 0.3477	BestValid 0.3456
	Epoch 4250:	Loss 0.1328	TrainAcc 0.9732	ValidAcc 0.3371	TestAcc 0.3381	BestValid 0.3456
	Epoch 4300:	Loss 0.1350	TrainAcc 0.9776	ValidAcc 0.3407	TestAcc 0.3497	BestValid 0.3456
	Epoch 4350:	Loss 0.1309	TrainAcc 0.9748	ValidAcc 0.3419	TestAcc 0.3391	BestValid 0.3456
	Epoch 4400:	Loss 0.1201	TrainAcc 0.9744	ValidAcc 0.3389	TestAcc 0.3458	BestValid 0.3456
	Epoch 4450:	Loss 0.1257	TrainAcc 0.9764	ValidAcc 0.3371	TestAcc 0.3477	BestValid 0.3456
	Epoch 4500:	Loss 0.1368	TrainAcc 0.9776	ValidAcc 0.3413	TestAcc 0.3487	BestValid 0.3456
	Epoch 4550:	Loss 0.1321	TrainAcc 0.9804	ValidAcc 0.3480	TestAcc 0.3497	BestValid 0.3480
	Epoch 4600:	Loss 0.1245	TrainAcc 0.9756	ValidAcc 0.3395	TestAcc 0.3477	BestValid 0.3480
	Epoch 4650:	Loss 0.1151	TrainAcc 0.9764	ValidAcc 0.3395	TestAcc 0.3477	BestValid 0.3480
	Epoch 4700:	Loss 0.1405	TrainAcc 0.9752	ValidAcc 0.3407	TestAcc 0.3516	BestValid 0.3480
	Epoch 4750:	Loss 0.1252	TrainAcc 0.9752	ValidAcc 0.3365	TestAcc 0.3401	BestValid 0.3480
	Epoch 4800:	Loss 0.1306	TrainAcc 0.9780	ValidAcc 0.3444	TestAcc 0.3439	BestValid 0.3480
	Epoch 4850:	Loss 0.1267	TrainAcc 0.9812	ValidAcc 0.3498	TestAcc 0.3497	BestValid 0.3498
	Epoch 4900:	Loss 0.1079	TrainAcc 0.9784	ValidAcc 0.3462	TestAcc 0.3487	BestValid 0.3498
	Epoch 4950:	Loss 0.1166	TrainAcc 0.9796	ValidAcc 0.3450	TestAcc 0.3487	BestValid 0.3498
	Epoch 5000:	Loss 0.1047	TrainAcc 0.9776	ValidAcc 0.3450	TestAcc 0.3420	BestValid 0.3498
****** Epoch Time (Excluding Evaluation Cost): 0.198 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.146 ms (Max: 0.195, Min: 0.038, Sum: 1.169)
Cluster-Wide Average, Compute: 32.730 ms (Max: 46.888, Min: 28.961, Sum: 261.843)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.121)
Cluster-Wide Average, Communication-Graph: 113.362 ms (Max: 117.177, Min: 99.227, Sum: 906.898)
Cluster-Wide Average, Optimization: 47.145 ms (Max: 47.264, Min: 46.984, Sum: 377.159)
Cluster-Wide Average, Others: 4.137 ms (Max: 4.195, Min: 4.068, Sum: 33.095)
****** Breakdown Sum: 197.543 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.532 GB (Max: 5.975, Min: 5.456, Sum: 44.259)
Cluster-Wide Average, Graph-Level Communication Throughput: 28.053 Gbps (Max: 48.147, Min: 9.714, Sum: 224.422)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.753 GB
Weight-sync communication (cluster-wide, per-epoch): 1.778 GB
Total communication (cluster-wide, per-epoch): 4.531 GB
****** Accuracy Results ******
Highest valid_acc: 0.3498
Target test_acc: 0.3497
Epoch to reach the target acc: 4849
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
