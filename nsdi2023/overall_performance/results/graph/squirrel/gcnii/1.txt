Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.009 seconds.
Building the Feature Vector...
        It takes 0.009 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.010 seconds.
        It takes 0.012 seconds.
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.033 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.029 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.328 Gbps (per GPU), 482.620 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.024 Gbps (per GPU), 480.192 Gbps (aggregated)
The layer-level communication performance: 60.013 Gbps (per GPU), 480.106 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.787 Gbps (per GPU), 478.297 Gbps (aggregated)
The layer-level communication performance: 59.750 Gbps (per GPU), 478.002 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.553 Gbps (per GPU), 476.421 Gbps (aggregated)
The layer-level communication performance: 59.506 Gbps (per GPU), 476.047 Gbps (aggregated)
The layer-level communication performance: 59.472 Gbps (per GPU), 475.772 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.547 Gbps (per GPU), 1260.379 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.541 Gbps (per GPU), 1260.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.541 Gbps (per GPU), 1260.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.535 Gbps (per GPU), 1260.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.533 Gbps (per GPU), 1260.260 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.541 Gbps (per GPU), 1260.329 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.545 Gbps (per GPU), 1260.359 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.541 Gbps (per GPU), 1260.331 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 97.887 Gbps (per GPU), 783.098 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.883 Gbps (per GPU), 783.067 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.887 Gbps (per GPU), 783.092 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.883 Gbps (per GPU), 783.061 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.888 Gbps (per GPU), 783.104 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.881 Gbps (per GPU), 783.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.887 Gbps (per GPU), 783.097 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.880 Gbps (per GPU), 783.037 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.858 Gbps (per GPU), 278.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.859 Gbps (per GPU), 278.872 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.854 Gbps (per GPU), 278.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.858 Gbps (per GPU), 278.868 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.854 Gbps (per GPU), 278.828 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.855 Gbps (per GPU), 278.842 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.852 Gbps (per GPU), 278.814 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.850 Gbps (per GPU), 278.797 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.01ms  1.62ms  1.74ms  1.73  0.65K  0.22M
 chk_1  1.01ms  0.95ms  1.08ms  1.14  0.65K  0.05M
 chk_2  1.01ms  0.81ms  0.95ms  1.24  0.65K  0.02M
 chk_3  1.01ms  0.92ms  0.96ms  1.11  0.65K  0.02M
 chk_4  1.01ms  0.77ms  0.91ms  1.31  0.65K  0.01M
 chk_5  1.01ms  0.76ms  0.90ms  1.33  0.65K  0.01M
 chk_6  1.02ms  0.96ms  1.10ms  1.14  0.65K  0.06M
 chk_7  1.02ms  0.79ms  0.92ms  1.30  0.65K  0.01M
   Avg  1.01  0.95  1.07
   Max  1.02  1.62  1.74
   Min  1.01  0.76  0.90
 Ratio  1.01  2.13  1.92
   Var  0.00  0.07  0.07
Profiling takes 0.361 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 37.887 ms
Partition 0 [0, 4) has cost: 30.827 ms
Partition 1 [4, 8) has cost: 30.310 ms
Partition 2 [8, 12) has cost: 30.310 ms
Partition 3 [12, 16) has cost: 30.310 ms
Partition 4 [16, 20) has cost: 30.310 ms
Partition 5 [20, 24) has cost: 30.310 ms
Partition 6 [24, 29) has cost: 37.887 ms
Partition 7 [29, 33) has cost: 31.305 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.749 ms
GPU 0, Compute+Comm Time: 12.040 ms, Bubble Time: 11.145 ms, Imbalance Overhead: 7.564 ms
GPU 1, Compute+Comm Time: 12.123 ms, Bubble Time: 11.735 ms, Imbalance Overhead: 6.890 ms
GPU 2, Compute+Comm Time: 12.123 ms, Bubble Time: 12.372 ms, Imbalance Overhead: 6.254 ms
GPU 3, Compute+Comm Time: 12.123 ms, Bubble Time: 13.253 ms, Imbalance Overhead: 5.372 ms
GPU 4, Compute+Comm Time: 12.123 ms, Bubble Time: 14.135 ms, Imbalance Overhead: 4.491 ms
GPU 5, Compute+Comm Time: 12.123 ms, Bubble Time: 14.720 ms, Imbalance Overhead: 3.905 ms
GPU 6, Compute+Comm Time: 14.464 ms, Bubble Time: 15.568 ms, Imbalance Overhead: 0.717 ms
GPU 7, Compute+Comm Time: 12.349 ms, Bubble Time: 17.167 ms, Imbalance Overhead: 1.233 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.125 ms
GPU 0, Compute+Comm Time: 24.473 ms, Bubble Time: 34.165 ms, Imbalance Overhead: 2.486 ms
GPU 1, Compute+Comm Time: 28.941 ms, Bubble Time: 30.921 ms, Imbalance Overhead: 1.263 ms
GPU 2, Compute+Comm Time: 23.704 ms, Bubble Time: 29.244 ms, Imbalance Overhead: 8.177 ms
GPU 3, Compute+Comm Time: 23.704 ms, Bubble Time: 28.124 ms, Imbalance Overhead: 9.297 ms
GPU 4, Compute+Comm Time: 23.704 ms, Bubble Time: 26.353 ms, Imbalance Overhead: 11.068 ms
GPU 5, Compute+Comm Time: 23.704 ms, Bubble Time: 24.613 ms, Imbalance Overhead: 12.807 ms
GPU 6, Compute+Comm Time: 23.704 ms, Bubble Time: 23.327 ms, Imbalance Overhead: 14.094 ms
GPU 7, Compute+Comm Time: 24.304 ms, Bubble Time: 22.094 ms, Imbalance Overhead: 14.726 ms
The estimated cost of the whole pipeline: 96.468 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 68.197 ms
Partition 0 [0, 8) has cost: 61.137 ms
Partition 1 [8, 16) has cost: 60.620 ms
Partition 2 [16, 25) has cost: 68.197 ms
Partition 3 [25, 33) has cost: 61.615 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 33.038 ms
GPU 0, Compute+Comm Time: 15.482 ms, Bubble Time: 11.319 ms, Imbalance Overhead: 6.237 ms
GPU 1, Compute+Comm Time: 16.005 ms, Bubble Time: 12.420 ms, Imbalance Overhead: 4.613 ms
GPU 2, Compute+Comm Time: 17.661 ms, Bubble Time: 13.913 ms, Imbalance Overhead: 1.464 ms
GPU 3, Compute+Comm Time: 16.095 ms, Bubble Time: 16.342 ms, Imbalance Overhead: 0.602 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.641 ms
GPU 0, Compute+Comm Time: 29.649 ms, Bubble Time: 30.979 ms, Imbalance Overhead: 1.013 ms
GPU 1, Compute+Comm Time: 32.623 ms, Bubble Time: 26.021 ms, Imbalance Overhead: 2.997 ms
GPU 2, Compute+Comm Time: 29.305 ms, Bubble Time: 22.884 ms, Imbalance Overhead: 9.452 ms
GPU 3, Compute+Comm Time: 28.917 ms, Bubble Time: 20.492 ms, Imbalance Overhead: 12.232 ms
    The estimated cost with 2 DP ways is 99.413 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 129.334 ms
Partition 0 [0, 17) has cost: 129.334 ms
Partition 1 [17, 33) has cost: 122.235 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.003 ms
GPU 0, Compute+Comm Time: 26.477 ms, Bubble Time: 11.185 ms, Imbalance Overhead: 3.341 ms
GPU 1, Compute+Comm Time: 25.965 ms, Bubble Time: 15.038 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 66.304 ms
GPU 0, Compute+Comm Time: 41.356 ms, Bubble Time: 24.949 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 42.634 ms, Bubble Time: 17.053 ms, Imbalance Overhead: 6.618 ms
    The estimated cost with 4 DP ways is 112.673 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 251.569 ms
Partition 0 [0, 33) has cost: 251.569 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 83.787 ms
GPU 0, Compute+Comm Time: 83.787 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 103.219 ms
GPU 0, Compute+Comm Time: 103.219 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 196.356 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6070	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1988	BestValid 0.2007
	Epoch 50:	Loss 1.1579	TrainAcc 0.5954	ValidAcc 0.3221	TestAcc 0.3180	BestValid 0.3221
	Epoch 100:	Loss 0.9873	TrainAcc 0.6655	ValidAcc 0.3203	TestAcc 0.3026	BestValid 0.3221
	Epoch 150:	Loss 0.8887	TrainAcc 0.7023	ValidAcc 0.3179	TestAcc 0.3026	BestValid 0.3221
	Epoch 200:	Loss 0.8329	TrainAcc 0.7476	ValidAcc 0.3131	TestAcc 0.3160	BestValid 0.3221
	Epoch 250:	Loss 0.7720	TrainAcc 0.7524	ValidAcc 0.3173	TestAcc 0.3026	BestValid 0.3221
	Epoch 300:	Loss 0.7498	TrainAcc 0.7368	ValidAcc 0.3149	TestAcc 0.2891	BestValid 0.3221
	Epoch 350:	Loss 0.6882	TrainAcc 0.7520	ValidAcc 0.3137	TestAcc 0.2882	BestValid 0.3221
	Epoch 400:	Loss 0.6389	TrainAcc 0.7857	ValidAcc 0.3185	TestAcc 0.2968	BestValid 0.3221
	Epoch 450:	Loss 0.6337	TrainAcc 0.7841	ValidAcc 0.3143	TestAcc 0.2949	BestValid 0.3221
	Epoch 500:	Loss 0.6013	TrainAcc 0.8009	ValidAcc 0.3167	TestAcc 0.2959	BestValid 0.3221
	Epoch 550:	Loss 0.5594	TrainAcc 0.8061	ValidAcc 0.3143	TestAcc 0.2968	BestValid 0.3221
	Epoch 600:	Loss 0.5349	TrainAcc 0.8101	ValidAcc 0.3155	TestAcc 0.2978	BestValid 0.3221
	Epoch 650:	Loss 0.5020	TrainAcc 0.8245	ValidAcc 0.3137	TestAcc 0.2968	BestValid 0.3221
	Epoch 700:	Loss 0.4955	TrainAcc 0.8221	ValidAcc 0.3137	TestAcc 0.2891	BestValid 0.3221
	Epoch 750:	Loss 0.4749	TrainAcc 0.8470	ValidAcc 0.3149	TestAcc 0.2968	BestValid 0.3221
	Epoch 800:	Loss 0.4474	TrainAcc 0.8421	ValidAcc 0.3167	TestAcc 0.2959	BestValid 0.3221
	Epoch 850:	Loss 0.4246	TrainAcc 0.8494	ValidAcc 0.3149	TestAcc 0.2988	BestValid 0.3221
	Epoch 900:	Loss 0.4129	TrainAcc 0.8542	ValidAcc 0.3143	TestAcc 0.2968	BestValid 0.3221
	Epoch 950:	Loss 0.4028	TrainAcc 0.8698	ValidAcc 0.3197	TestAcc 0.3112	BestValid 0.3221
	Epoch 1000:	Loss 0.3961	TrainAcc 0.8682	ValidAcc 0.3179	TestAcc 0.3093	BestValid 0.3221
	Epoch 1050:	Loss 0.3944	TrainAcc 0.8782	ValidAcc 0.3197	TestAcc 0.3055	BestValid 0.3221
	Epoch 1100:	Loss 0.3549	TrainAcc 0.8846	ValidAcc 0.3209	TestAcc 0.3074	BestValid 0.3221
	Epoch 1150:	Loss 0.3509	TrainAcc 0.8910	ValidAcc 0.3233	TestAcc 0.3103	BestValid 0.3233
	Epoch 1200:	Loss 0.3265	TrainAcc 0.8878	ValidAcc 0.3215	TestAcc 0.2988	BestValid 0.3233
	Epoch 1250:	Loss 0.3222	TrainAcc 0.8950	ValidAcc 0.3233	TestAcc 0.2997	BestValid 0.3233
	Epoch 1300:	Loss 0.3140	TrainAcc 0.8990	ValidAcc 0.3209	TestAcc 0.3093	BestValid 0.3233
	Epoch 1350:	Loss 0.3141	TrainAcc 0.9071	ValidAcc 0.3197	TestAcc 0.3112	BestValid 0.3233
	Epoch 1400:	Loss 0.2862	TrainAcc 0.8974	ValidAcc 0.3197	TestAcc 0.3064	BestValid 0.3233
	Epoch 1450:	Loss 0.3097	TrainAcc 0.9042	ValidAcc 0.3239	TestAcc 0.3074	BestValid 0.3239
	Epoch 1500:	Loss 0.2987	TrainAcc 0.9119	ValidAcc 0.3245	TestAcc 0.3093	BestValid 0.3245
	Epoch 1550:	Loss 0.2550	TrainAcc 0.9131	ValidAcc 0.3221	TestAcc 0.3112	BestValid 0.3245
	Epoch 1600:	Loss 0.2718	TrainAcc 0.9151	ValidAcc 0.3209	TestAcc 0.3122	BestValid 0.3245
	Epoch 1650:	Loss 0.2566	TrainAcc 0.9127	ValidAcc 0.3227	TestAcc 0.3074	BestValid 0.3245
	Epoch 1700:	Loss 0.2649	TrainAcc 0.9239	ValidAcc 0.3221	TestAcc 0.3122	BestValid 0.3245
	Epoch 1750:	Loss 0.2598	TrainAcc 0.9315	ValidAcc 0.3293	TestAcc 0.3151	BestValid 0.3293
	Epoch 1800:	Loss 0.2648	TrainAcc 0.9247	ValidAcc 0.3233	TestAcc 0.3151	BestValid 0.3293
	Epoch 1850:	Loss 0.2346	TrainAcc 0.9227	ValidAcc 0.3239	TestAcc 0.3122	BestValid 0.3293
	Epoch 1900:	Loss 0.2409	TrainAcc 0.9351	ValidAcc 0.3305	TestAcc 0.3237	BestValid 0.3305
	Epoch 1950:	Loss 0.2552	TrainAcc 0.9427	ValidAcc 0.3305	TestAcc 0.3305	BestValid 0.3305
	Epoch 2000:	Loss 0.2246	TrainAcc 0.9399	ValidAcc 0.3299	TestAcc 0.3266	BestValid 0.3305
	Epoch 2050:	Loss 0.2181	TrainAcc 0.9287	ValidAcc 0.3245	TestAcc 0.3180	BestValid 0.3305
	Epoch 2100:	Loss 0.2233	TrainAcc 0.9427	ValidAcc 0.3323	TestAcc 0.3324	BestValid 0.3323
	Epoch 2150:	Loss 0.2256	TrainAcc 0.9415	ValidAcc 0.3323	TestAcc 0.3295	BestValid 0.3323
	Epoch 2200:	Loss 0.2184	TrainAcc 0.9443	ValidAcc 0.3287	TestAcc 0.3324	BestValid 0.3323
	Epoch 2250:	Loss 0.2101	TrainAcc 0.9427	ValidAcc 0.3299	TestAcc 0.3314	BestValid 0.3323
	Epoch 2300:	Loss 0.2151	TrainAcc 0.9363	ValidAcc 0.3275	TestAcc 0.3199	BestValid 0.3323
	Epoch 2350:	Loss 0.2061	TrainAcc 0.9463	ValidAcc 0.3323	TestAcc 0.3372	BestValid 0.3323
	Epoch 2400:	Loss 0.2038	TrainAcc 0.9495	ValidAcc 0.3323	TestAcc 0.3401	BestValid 0.3323
	Epoch 2450:	Loss 0.1972	TrainAcc 0.9523	ValidAcc 0.3341	TestAcc 0.3401	BestValid 0.3341
	Epoch 2500:	Loss 0.2069	TrainAcc 0.9527	ValidAcc 0.3329	TestAcc 0.3420	BestValid 0.3341
	Epoch 2550:	Loss 0.2036	TrainAcc 0.9519	ValidAcc 0.3341	TestAcc 0.3353	BestValid 0.3341
	Epoch 2600:	Loss 0.1996	TrainAcc 0.9483	ValidAcc 0.3287	TestAcc 0.3333	BestValid 0.3341
	Epoch 2650:	Loss 0.1995	TrainAcc 0.9539	ValidAcc 0.3323	TestAcc 0.3381	BestValid 0.3341
	Epoch 2700:	Loss 0.1906	TrainAcc 0.9539	ValidAcc 0.3305	TestAcc 0.3324	BestValid 0.3341
	Epoch 2750:	Loss 0.1873	TrainAcc 0.9603	ValidAcc 0.3341	TestAcc 0.3401	BestValid 0.3341
	Epoch 2800:	Loss 0.1796	TrainAcc 0.9591	ValidAcc 0.3371	TestAcc 0.3429	BestValid 0.3371
	Epoch 2850:	Loss 0.1779	TrainAcc 0.9559	ValidAcc 0.3335	TestAcc 0.3343	BestValid 0.3371
	Epoch 2900:	Loss 0.1837	TrainAcc 0.9591	ValidAcc 0.3377	TestAcc 0.3353	BestValid 0.3377
	Epoch 2950:	Loss 0.1752	TrainAcc 0.9599	ValidAcc 0.3371	TestAcc 0.3381	BestValid 0.3377
	Epoch 3000:	Loss 0.1758	TrainAcc 0.9595	ValidAcc 0.3371	TestAcc 0.3410	BestValid 0.3377
	Epoch 3050:	Loss 0.1742	TrainAcc 0.9591	ValidAcc 0.3341	TestAcc 0.3391	BestValid 0.3377
	Epoch 3100:	Loss 0.1674	TrainAcc 0.9607	ValidAcc 0.3377	TestAcc 0.3381	BestValid 0.3377
	Epoch 3150:	Loss 0.1612	TrainAcc 0.9599	ValidAcc 0.3401	TestAcc 0.3362	BestValid 0.3401
	Epoch 3200:	Loss 0.1559	TrainAcc 0.9651	ValidAcc 0.3407	TestAcc 0.3391	BestValid 0.3407
	Epoch 3250:	Loss 0.1478	TrainAcc 0.9696	ValidAcc 0.3413	TestAcc 0.3449	BestValid 0.3413
	Epoch 3300:	Loss 0.1467	TrainAcc 0.9663	ValidAcc 0.3383	TestAcc 0.3391	BestValid 0.3413
	Epoch 3350:	Loss 0.1574	TrainAcc 0.9696	ValidAcc 0.3413	TestAcc 0.3420	BestValid 0.3413
	Epoch 3400:	Loss 0.1548	TrainAcc 0.9655	ValidAcc 0.3401	TestAcc 0.3391	BestValid 0.3413
	Epoch 3450:	Loss 0.1622	TrainAcc 0.9712	ValidAcc 0.3407	TestAcc 0.3487	BestValid 0.3413
	Epoch 3500:	Loss 0.1540	TrainAcc 0.9692	ValidAcc 0.3425	TestAcc 0.3410	BestValid 0.3425
	Epoch 3550:	Loss 0.1433	TrainAcc 0.9663	ValidAcc 0.3353	TestAcc 0.3372	BestValid 0.3425
	Epoch 3600:	Loss 0.1619	TrainAcc 0.9756	ValidAcc 0.3438	TestAcc 0.3497	BestValid 0.3438
	Epoch 3650:	Loss 0.1398	TrainAcc 0.9708	ValidAcc 0.3431	TestAcc 0.3458	BestValid 0.3438
	Epoch 3700:	Loss 0.1419	TrainAcc 0.9748	ValidAcc 0.3431	TestAcc 0.3468	BestValid 0.3438
	Epoch 3750:	Loss 0.1471	TrainAcc 0.9683	ValidAcc 0.3353	TestAcc 0.3362	BestValid 0.3438
	Epoch 3800:	Loss 0.1374	TrainAcc 0.9748	ValidAcc 0.3401	TestAcc 0.3449	BestValid 0.3438
	Epoch 3850:	Loss 0.1335	TrainAcc 0.9716	ValidAcc 0.3456	TestAcc 0.3391	BestValid 0.3456
	Epoch 3900:	Loss 0.1385	TrainAcc 0.9748	ValidAcc 0.3431	TestAcc 0.3420	BestValid 0.3456
	Epoch 3950:	Loss 0.1396	TrainAcc 0.9752	ValidAcc 0.3425	TestAcc 0.3449	BestValid 0.3456
	Epoch 4000:	Loss 0.1436	TrainAcc 0.9740	ValidAcc 0.3425	TestAcc 0.3439	BestValid 0.3456
	Epoch 4050:	Loss 0.1298	TrainAcc 0.9720	ValidAcc 0.3377	TestAcc 0.3420	BestValid 0.3456
	Epoch 4100:	Loss 0.1372	TrainAcc 0.9732	ValidAcc 0.3383	TestAcc 0.3410	BestValid 0.3456
	Epoch 4150:	Loss 0.1366	TrainAcc 0.9712	ValidAcc 0.3365	TestAcc 0.3353	BestValid 0.3456
	Epoch 4200:	Loss 0.1320	TrainAcc 0.9712	ValidAcc 0.3395	TestAcc 0.3401	BestValid 0.3456
	Epoch 4250:	Loss 0.1407	TrainAcc 0.9716	ValidAcc 0.3377	TestAcc 0.3429	BestValid 0.3456
	Epoch 4300:	Loss 0.1286	TrainAcc 0.9760	ValidAcc 0.3462	TestAcc 0.3487	BestValid 0.3462
	Epoch 4350:	Loss 0.1418	TrainAcc 0.9744	ValidAcc 0.3413	TestAcc 0.3420	BestValid 0.3462
	Epoch 4400:	Loss 0.1278	TrainAcc 0.9796	ValidAcc 0.3492	TestAcc 0.3516	BestValid 0.3492
	Epoch 4450:	Loss 0.1204	TrainAcc 0.9784	ValidAcc 0.3395	TestAcc 0.3458	BestValid 0.3492
	Epoch 4500:	Loss 0.1298	TrainAcc 0.9732	ValidAcc 0.3365	TestAcc 0.3362	BestValid 0.3492
	Epoch 4550:	Loss 0.1347	TrainAcc 0.9784	ValidAcc 0.3431	TestAcc 0.3573	BestValid 0.3492
	Epoch 4600:	Loss 0.1261	TrainAcc 0.9772	ValidAcc 0.3413	TestAcc 0.3487	BestValid 0.3492
	Epoch 4650:	Loss 0.1258	TrainAcc 0.9780	ValidAcc 0.3444	TestAcc 0.3477	BestValid 0.3492
	Epoch 4700:	Loss 0.1184	TrainAcc 0.9764	ValidAcc 0.3371	TestAcc 0.3458	BestValid 0.3492
	Epoch 4750:	Loss 0.1224	TrainAcc 0.9804	ValidAcc 0.3450	TestAcc 0.3564	BestValid 0.3492
	Epoch 4800:	Loss 0.1202	TrainAcc 0.9808	ValidAcc 0.3516	TestAcc 0.3497	BestValid 0.3516
	Epoch 4850:	Loss 0.1173	TrainAcc 0.9796	ValidAcc 0.3444	TestAcc 0.3487	BestValid 0.3516
	Epoch 4900:	Loss 0.1133	TrainAcc 0.9784	ValidAcc 0.3425	TestAcc 0.3535	BestValid 0.3516
	Epoch 4950:	Loss 0.1185	TrainAcc 0.9800	ValidAcc 0.3419	TestAcc 0.3573	BestValid 0.3516
	Epoch 5000:	Loss 0.1167	TrainAcc 0.9768	ValidAcc 0.3395	TestAcc 0.3449	BestValid 0.3516
****** Epoch Time (Excluding Evaluation Cost): 0.198 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.131 ms (Max: 0.171, Min: 0.036, Sum: 1.049)
Cluster-Wide Average, Compute: 32.735 ms (Max: 46.816, Min: 29.165, Sum: 261.881)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.122)
Cluster-Wide Average, Communication-Graph: 113.394 ms (Max: 117.023, Min: 99.303, Sum: 907.150)
Cluster-Wide Average, Optimization: 47.025 ms (Max: 47.147, Min: 46.869, Sum: 376.204)
Cluster-Wide Average, Others: 4.144 ms (Max: 4.208, Min: 4.091, Sum: 33.154)
****** Breakdown Sum: 197.453 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.532 GB (Max: 5.975, Min: 5.456, Sum: 44.259)
Cluster-Wide Average, Graph-Level Communication Throughput: 28.049 Gbps (Max: 48.105, Min: 9.740, Sum: 224.394)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.753 GB
Weight-sync communication (cluster-wide, per-epoch): 1.778 GB
Total communication (cluster-wide, per-epoch): 4.531 GB
****** Accuracy Results ******
Highest valid_acc: 0.3516
Target test_acc: 0.3497
Epoch to reach the target acc: 4799
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
