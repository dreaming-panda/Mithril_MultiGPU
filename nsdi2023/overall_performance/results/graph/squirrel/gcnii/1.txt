Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
        It takes 0.009 seconds.
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.007 seconds.
        It takes 0.009 seconds.
        It takes 0.007 seconds.
        It takes 0.010 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
        It takes 0.000 seconds.
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.193 Gbps (per GPU), 457.546 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.923 Gbps (per GPU), 455.383 Gbps (aggregated)
The layer-level communication performance: 56.921 Gbps (per GPU), 455.366 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.704 Gbps (per GPU), 453.636 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.471 Gbps (per GPU), 451.764 Gbps (aggregated)
The layer-level communication performance: 56.429 Gbps (per GPU), 451.431 Gbps (aggregated)
The layer-level communication performance: 56.398 Gbps (per GPU), 451.186 Gbps (aggregated)
The layer-level communication performance: 56.671 Gbps (per GPU), 453.371 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.972 Gbps (per GPU), 1255.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.961 Gbps (per GPU), 1255.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.899 Gbps (per GPU), 1255.190 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.958 Gbps (per GPU), 1255.662 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.987 Gbps (per GPU), 1255.897 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.967 Gbps (per GPU), 1255.733 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.914 Gbps (per GPU), 1255.310 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.967 Gbps (per GPU), 1255.733 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.465 Gbps (per GPU), 819.720 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.464 Gbps (per GPU), 819.714 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.465 Gbps (per GPU), 819.720 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.473 Gbps (per GPU), 819.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.463 Gbps (per GPU), 819.707 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.463 Gbps (per GPU), 819.708 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.467 Gbps (per GPU), 819.734 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.478 Gbps (per GPU), 819.827 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.832 Gbps (per GPU), 270.652 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.833 Gbps (per GPU), 270.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.832 Gbps (per GPU), 270.652 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.830 Gbps (per GPU), 270.642 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.828 Gbps (per GPU), 270.622 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.828 Gbps (per GPU), 270.627 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.825 Gbps (per GPU), 270.602 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.820 Gbps (per GPU), 270.558 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.02ms  1.67ms  1.78ms  1.74  0.65K  0.22M
 chk_1  1.02ms  0.96ms  1.11ms  1.15  0.65K  0.05M
 chk_2  1.04ms  0.82ms  0.97ms  1.26  0.65K  0.02M
 chk_3  1.04ms  0.84ms  0.98ms  1.23  0.65K  0.02M
 chk_4  1.04ms  0.79ms  0.92ms  1.32  0.65K  0.01M
 chk_5  1.04ms  0.79ms  0.92ms  1.32  0.65K  0.01M
 chk_6  1.04ms  0.98ms  1.11ms  1.13  0.65K  0.06M
 chk_7  1.04ms  0.80ms  0.94ms  1.30  0.65K  0.01M
   Avg  1.03  0.96  1.09
   Max  1.04  1.67  1.78
   Min  1.02  0.79  0.92
 Ratio  1.02  2.13  1.94
   Var  0.00  0.08  0.07
Profiling takes 0.357 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 38.212 ms
Partition 0 [0, 4) has cost: 31.196 ms
Partition 1 [4, 8) has cost: 30.570 ms
Partition 2 [8, 12) has cost: 30.570 ms
Partition 3 [12, 16) has cost: 30.570 ms
Partition 4 [16, 20) has cost: 30.570 ms
Partition 5 [20, 24) has cost: 30.570 ms
Partition 6 [24, 29) has cost: 38.212 ms
Partition 7 [29, 33) has cost: 31.655 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.517 ms
GPU 0, Compute+Comm Time: 12.314 ms, Bubble Time: 11.279 ms, Imbalance Overhead: 7.924 ms
GPU 1, Compute+Comm Time: 12.369 ms, Bubble Time: 11.906 ms, Imbalance Overhead: 7.243 ms
GPU 2, Compute+Comm Time: 12.369 ms, Bubble Time: 12.750 ms, Imbalance Overhead: 6.398 ms
GPU 3, Compute+Comm Time: 12.369 ms, Bubble Time: 13.664 ms, Imbalance Overhead: 5.485 ms
GPU 4, Compute+Comm Time: 12.369 ms, Bubble Time: 14.577 ms, Imbalance Overhead: 4.571 ms
GPU 5, Compute+Comm Time: 12.369 ms, Bubble Time: 15.187 ms, Imbalance Overhead: 3.961 ms
GPU 6, Compute+Comm Time: 14.733 ms, Bubble Time: 16.069 ms, Imbalance Overhead: 0.715 ms
GPU 7, Compute+Comm Time: 12.621 ms, Bubble Time: 17.723 ms, Imbalance Overhead: 1.173 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 62.440 ms
GPU 0, Compute+Comm Time: 24.854 ms, Bubble Time: 35.147 ms, Imbalance Overhead: 2.439 ms
GPU 1, Compute+Comm Time: 29.299 ms, Bubble Time: 31.783 ms, Imbalance Overhead: 1.359 ms
GPU 2, Compute+Comm Time: 24.021 ms, Bubble Time: 30.043 ms, Imbalance Overhead: 8.377 ms
GPU 3, Compute+Comm Time: 24.021 ms, Bubble Time: 28.880 ms, Imbalance Overhead: 9.539 ms
GPU 4, Compute+Comm Time: 24.021 ms, Bubble Time: 27.040 ms, Imbalance Overhead: 11.379 ms
GPU 5, Compute+Comm Time: 24.021 ms, Bubble Time: 25.224 ms, Imbalance Overhead: 13.195 ms
GPU 6, Compute+Comm Time: 24.021 ms, Bubble Time: 23.581 ms, Imbalance Overhead: 14.838 ms
GPU 7, Compute+Comm Time: 24.702 ms, Bubble Time: 22.291 ms, Imbalance Overhead: 15.448 ms
The estimated cost of the whole pipeline: 98.656 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 68.782 ms
Partition 0 [0, 8) has cost: 61.766 ms
Partition 1 [8, 16) has cost: 61.140 ms
Partition 2 [16, 25) has cost: 68.782 ms
Partition 3 [25, 33) has cost: 62.225 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 33.478 ms
GPU 0, Compute+Comm Time: 15.653 ms, Bubble Time: 11.115 ms, Imbalance Overhead: 6.710 ms
GPU 1, Compute+Comm Time: 16.147 ms, Bubble Time: 12.263 ms, Imbalance Overhead: 5.067 ms
GPU 2, Compute+Comm Time: 17.802 ms, Bubble Time: 14.058 ms, Imbalance Overhead: 1.617 ms
GPU 3, Compute+Comm Time: 16.271 ms, Bubble Time: 16.826 ms, Imbalance Overhead: 0.380 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 62.506 ms
GPU 0, Compute+Comm Time: 29.926 ms, Bubble Time: 31.834 ms, Imbalance Overhead: 0.746 ms
GPU 1, Compute+Comm Time: 32.847 ms, Bubble Time: 26.286 ms, Imbalance Overhead: 3.373 ms
GPU 2, Compute+Comm Time: 29.521 ms, Bubble Time: 22.705 ms, Imbalance Overhead: 10.281 ms
GPU 3, Compute+Comm Time: 29.189 ms, Bubble Time: 20.224 ms, Imbalance Overhead: 13.093 ms
    The estimated cost with 2 DP ways is 100.783 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 130.548 ms
Partition 0 [0, 17) has cost: 130.548 ms
Partition 1 [17, 33) has cost: 123.365 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.126 ms
GPU 0, Compute+Comm Time: 26.516 ms, Bubble Time: 11.127 ms, Imbalance Overhead: 3.484 ms
GPU 1, Compute+Comm Time: 25.993 ms, Bubble Time: 15.133 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 66.974 ms
GPU 0, Compute+Comm Time: 41.684 ms, Bubble Time: 25.290 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 42.984 ms, Bubble Time: 17.056 ms, Imbalance Overhead: 6.935 ms
    The estimated cost with 4 DP ways is 113.505 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 253.914 ms
Partition 0 [0, 33) has cost: 253.914 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 86.303 ms
GPU 0, Compute+Comm Time: 86.303 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 106.222 ms
GPU 0, Compute+Comm Time: 106.222 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 202.151 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.1579	TrainAcc 0.5954	ValidAcc 0.3227	TestAcc 0.3180	BestValid 0.3227
	Epoch 100:	Loss 0.9873	TrainAcc 0.6655	ValidAcc 0.3203	TestAcc 0.3026	BestValid 0.3227
	Epoch 150:	Loss 0.8887	TrainAcc 0.7023	ValidAcc 0.3179	TestAcc 0.3026	BestValid 0.3227
	Epoch 200:	Loss 0.8329	TrainAcc 0.7476	ValidAcc 0.3131	TestAcc 0.3160	BestValid 0.3227
	Epoch 250:	Loss 0.7720	TrainAcc 0.7524	ValidAcc 0.3173	TestAcc 0.3026	BestValid 0.3227
	Epoch 300:	Loss 0.7499	TrainAcc 0.7360	ValidAcc 0.3149	TestAcc 0.2891	BestValid 0.3227
	Epoch 350:	Loss 0.6882	TrainAcc 0.7520	ValidAcc 0.3137	TestAcc 0.2882	BestValid 0.3227
	Epoch 400:	Loss 0.6389	TrainAcc 0.7857	ValidAcc 0.3185	TestAcc 0.2968	BestValid 0.3227
	Epoch 450:	Loss 0.6337	TrainAcc 0.7841	ValidAcc 0.3143	TestAcc 0.2949	BestValid 0.3227
	Epoch 500:	Loss 0.6012	TrainAcc 0.8005	ValidAcc 0.3161	TestAcc 0.2968	BestValid 0.3227
	Epoch 550:	Loss 0.5594	TrainAcc 0.8061	ValidAcc 0.3143	TestAcc 0.2968	BestValid 0.3227
	Epoch 600:	Loss 0.5349	TrainAcc 0.8101	ValidAcc 0.3161	TestAcc 0.2978	BestValid 0.3227
	Epoch 650:	Loss 0.5020	TrainAcc 0.8249	ValidAcc 0.3137	TestAcc 0.2959	BestValid 0.3227
	Epoch 700:	Loss 0.4956	TrainAcc 0.8213	ValidAcc 0.3137	TestAcc 0.2901	BestValid 0.3227
	Epoch 750:	Loss 0.4749	TrainAcc 0.8474	ValidAcc 0.3149	TestAcc 0.2959	BestValid 0.3227
	Epoch 800:	Loss 0.4474	TrainAcc 0.8421	ValidAcc 0.3173	TestAcc 0.2959	BestValid 0.3227
	Epoch 850:	Loss 0.4246	TrainAcc 0.8494	ValidAcc 0.3149	TestAcc 0.2988	BestValid 0.3227
	Epoch 900:	Loss 0.4129	TrainAcc 0.8546	ValidAcc 0.3143	TestAcc 0.2968	BestValid 0.3227
	Epoch 950:	Loss 0.4028	TrainAcc 0.8698	ValidAcc 0.3209	TestAcc 0.3112	BestValid 0.3227
	Epoch 1000:	Loss 0.3961	TrainAcc 0.8682	ValidAcc 0.3179	TestAcc 0.3103	BestValid 0.3227
****** Epoch Time (Excluding Evaluation Cost): 0.198 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.125 ms (Max: 0.161, Min: 0.036, Sum: 1.000)
Cluster-Wide Average, Compute: 32.816 ms (Max: 46.841, Min: 29.451, Sum: 262.528)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.120)
Cluster-Wide Average, Communication-Graph: 113.455 ms (Max: 116.852, Min: 99.424, Sum: 907.637)
Cluster-Wide Average, Optimization: 47.218 ms (Max: 47.358, Min: 47.059, Sum: 377.742)
Cluster-Wide Average, Others: 4.150 ms (Max: 4.203, Min: 4.112, Sum: 33.203)
****** Breakdown Sum: 197.786 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.532 GB (Max: 5.975, Min: 5.456, Sum: 44.259)
Cluster-Wide Average, Graph-Level Communication Throughput: 28.007 Gbps (Max: 48.013, Min: 9.765, Sum: 224.054)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.753 GB
Weight-sync communication (cluster-wide, per-epoch): 1.778 GB
Total communication (cluster-wide, per-epoch): 4.531 GB
****** Accuracy Results ******
Highest valid_acc: 0.3227
Target test_acc: 0.3180
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
