Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.006 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
        It takes 0.015 seconds.
        It takes 0.011 seconds.
Building the Feature Vector...
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.012 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.500 Gbps (per GPU), 475.997 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.192 Gbps (per GPU), 473.537 Gbps (aggregated)
The layer-level communication performance: 59.188 Gbps (per GPU), 473.505 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.946 Gbps (per GPU), 471.572 Gbps (aggregated)
The layer-level communication performance: 58.906 Gbps (per GPU), 471.252 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.692 Gbps (per GPU), 469.535 Gbps (aggregated)
The layer-level communication performance: 58.640 Gbps (per GPU), 469.121 Gbps (aggregated)
The layer-level communication performance: 58.608 Gbps (per GPU), 468.863 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.978 Gbps (per GPU), 1263.820 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.969 Gbps (per GPU), 1263.749 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.972 Gbps (per GPU), 1263.773 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.966 Gbps (per GPU), 1263.725 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.879 Gbps (per GPU), 1263.035 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.966 Gbps (per GPU), 1263.725 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.888 Gbps (per GPU), 1263.107 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.972 Gbps (per GPU), 1263.772 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.636 Gbps (per GPU), 813.086 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.631 Gbps (per GPU), 813.047 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.632 Gbps (per GPU), 813.060 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.632 Gbps (per GPU), 813.053 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.634 Gbps (per GPU), 813.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.632 Gbps (per GPU), 813.060 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.635 Gbps (per GPU), 813.078 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.630 Gbps (per GPU), 813.040 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.228 Gbps (per GPU), 281.820 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.233 Gbps (per GPU), 281.862 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.230 Gbps (per GPU), 281.840 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.227 Gbps (per GPU), 281.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.231 Gbps (per GPU), 281.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.227 Gbps (per GPU), 281.814 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.227 Gbps (per GPU), 281.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.218 Gbps (per GPU), 281.747 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.02ms  1.66ms  1.78ms  1.74  0.65K  0.22M
 chk_1  1.02ms  0.97ms  1.10ms  1.14  0.65K  0.05M
 chk_2  1.02ms  0.84ms  0.97ms  1.22  0.65K  0.02M
 chk_3  1.04ms  0.85ms  0.98ms  1.22  0.65K  0.02M
 chk_4  1.04ms  0.79ms  0.93ms  1.31  0.65K  0.01M
 chk_5  1.04ms  0.79ms  0.93ms  1.32  0.65K  0.01M
 chk_6  1.04ms  0.99ms  1.12ms  1.14  0.65K  0.06M
 chk_7  1.04ms  0.81ms  0.94ms  1.30  0.65K  0.01M
   Avg  1.03  0.96  1.10
   Max  1.04  1.66  1.78
   Min  1.02  0.79  0.93
 Ratio  1.03  2.11  1.93
   Var  0.00  0.07  0.07
Profiling takes 0.359 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 38.396 ms
Partition 0 [0, 4) has cost: 31.301 ms
Partition 1 [4, 8) has cost: 30.717 ms
Partition 2 [8, 12) has cost: 30.717 ms
Partition 3 [12, 16) has cost: 30.717 ms
Partition 4 [16, 20) has cost: 30.717 ms
Partition 5 [20, 24) has cost: 30.717 ms
Partition 6 [24, 29) has cost: 38.396 ms
Partition 7 [29, 33) has cost: 31.803 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.323 ms
GPU 0, Compute+Comm Time: 12.228 ms, Bubble Time: 11.272 ms, Imbalance Overhead: 7.824 ms
GPU 1, Compute+Comm Time: 12.296 ms, Bubble Time: 11.865 ms, Imbalance Overhead: 7.162 ms
GPU 2, Compute+Comm Time: 12.296 ms, Bubble Time: 12.681 ms, Imbalance Overhead: 6.346 ms
GPU 3, Compute+Comm Time: 12.296 ms, Bubble Time: 13.576 ms, Imbalance Overhead: 5.451 ms
GPU 4, Compute+Comm Time: 12.296 ms, Bubble Time: 14.471 ms, Imbalance Overhead: 4.556 ms
GPU 5, Compute+Comm Time: 12.296 ms, Bubble Time: 15.061 ms, Imbalance Overhead: 3.966 ms
GPU 6, Compute+Comm Time: 14.670 ms, Bubble Time: 15.923 ms, Imbalance Overhead: 0.730 ms
GPU 7, Compute+Comm Time: 12.555 ms, Bubble Time: 17.552 ms, Imbalance Overhead: 1.215 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 62.059 ms
GPU 0, Compute+Comm Time: 24.842 ms, Bubble Time: 34.742 ms, Imbalance Overhead: 2.475 ms
GPU 1, Compute+Comm Time: 29.320 ms, Bubble Time: 31.448 ms, Imbalance Overhead: 1.291 ms
GPU 2, Compute+Comm Time: 24.015 ms, Bubble Time: 29.770 ms, Imbalance Overhead: 8.274 ms
GPU 3, Compute+Comm Time: 24.015 ms, Bubble Time: 28.670 ms, Imbalance Overhead: 9.374 ms
GPU 4, Compute+Comm Time: 24.015 ms, Bubble Time: 26.895 ms, Imbalance Overhead: 11.149 ms
GPU 5, Compute+Comm Time: 24.015 ms, Bubble Time: 25.147 ms, Imbalance Overhead: 12.897 ms
GPU 6, Compute+Comm Time: 24.015 ms, Bubble Time: 23.576 ms, Imbalance Overhead: 14.468 ms
GPU 7, Compute+Comm Time: 24.667 ms, Bubble Time: 22.363 ms, Imbalance Overhead: 15.029 ms
The estimated cost of the whole pipeline: 98.051 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 69.113 ms
Partition 0 [0, 8) has cost: 62.018 ms
Partition 1 [8, 16) has cost: 61.434 ms
Partition 2 [16, 25) has cost: 69.113 ms
Partition 3 [25, 33) has cost: 62.520 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 33.250 ms
GPU 0, Compute+Comm Time: 15.556 ms, Bubble Time: 11.085 ms, Imbalance Overhead: 6.609 ms
GPU 1, Compute+Comm Time: 16.049 ms, Bubble Time: 12.199 ms, Imbalance Overhead: 5.001 ms
GPU 2, Compute+Comm Time: 17.705 ms, Bubble Time: 13.957 ms, Imbalance Overhead: 1.588 ms
GPU 3, Compute+Comm Time: 16.178 ms, Bubble Time: 16.672 ms, Imbalance Overhead: 0.399 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 62.053 ms
GPU 0, Compute+Comm Time: 29.834 ms, Bubble Time: 31.477 ms, Imbalance Overhead: 0.743 ms
GPU 1, Compute+Comm Time: 32.756 ms, Bubble Time: 26.071 ms, Imbalance Overhead: 3.226 ms
GPU 2, Compute+Comm Time: 29.427 ms, Bubble Time: 22.611 ms, Imbalance Overhead: 10.015 ms
GPU 3, Compute+Comm Time: 29.099 ms, Bubble Time: 20.238 ms, Imbalance Overhead: 12.716 ms
    The estimated cost with 2 DP ways is 100.068 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 131.131 ms
Partition 0 [0, 17) has cost: 131.131 ms
Partition 1 [17, 33) has cost: 123.953 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.004 ms
GPU 0, Compute+Comm Time: 26.447 ms, Bubble Time: 11.111 ms, Imbalance Overhead: 3.445 ms
GPU 1, Compute+Comm Time: 25.928 ms, Bubble Time: 15.075 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 66.665 ms
GPU 0, Compute+Comm Time: 41.567 ms, Bubble Time: 25.098 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 42.870 ms, Bubble Time: 17.122 ms, Imbalance Overhead: 6.673 ms
    The estimated cost with 4 DP ways is 113.052 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 255.084 ms
Partition 0 [0, 33) has cost: 255.084 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 83.489 ms
GPU 0, Compute+Comm Time: 83.489 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 103.142 ms
GPU 0, Compute+Comm Time: 103.142 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 195.962 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
*** Node 6, starting model training...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 2, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6082	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 1.1463	TrainAcc 0.5813	ValidAcc 0.3161	TestAcc 0.3112	BestValid 0.3161
	Epoch 100:	Loss 0.9751	TrainAcc 0.6715	ValidAcc 0.3131	TestAcc 0.3084	BestValid 0.3161
	Epoch 150:	Loss 0.8735	TrainAcc 0.7188	ValidAcc 0.3179	TestAcc 0.3103	BestValid 0.3179
	Epoch 200:	Loss 0.8063	TrainAcc 0.7520	ValidAcc 0.3167	TestAcc 0.3132	BestValid 0.3179
	Epoch 250:	Loss 0.7708	TrainAcc 0.7392	ValidAcc 0.3131	TestAcc 0.2949	BestValid 0.3179
	Epoch 300:	Loss 0.7258	TrainAcc 0.7244	ValidAcc 0.3125	TestAcc 0.2872	BestValid 0.3179
	Epoch 350:	Loss 0.6843	TrainAcc 0.7744	ValidAcc 0.3167	TestAcc 0.2968	BestValid 0.3179
	Epoch 400:	Loss 0.6338	TrainAcc 0.7592	ValidAcc 0.3089	TestAcc 0.2891	BestValid 0.3179
	Epoch 450:	Loss 0.6222	TrainAcc 0.7792	ValidAcc 0.3155	TestAcc 0.2901	BestValid 0.3179
	Epoch 500:	Loss 0.5786	TrainAcc 0.7949	ValidAcc 0.3113	TestAcc 0.2978	BestValid 0.3179
	Epoch 550:	Loss 0.5516	TrainAcc 0.8033	ValidAcc 0.3113	TestAcc 0.2911	BestValid 0.3179
	Epoch 600:	Loss 0.5541	TrainAcc 0.8213	ValidAcc 0.3179	TestAcc 0.3007	BestValid 0.3179
	Epoch 650:	Loss 0.5152	TrainAcc 0.8089	ValidAcc 0.3119	TestAcc 0.2863	BestValid 0.3179
	Epoch 700:	Loss 0.4853	TrainAcc 0.8301	ValidAcc 0.3173	TestAcc 0.2959	BestValid 0.3179
	Epoch 750:	Loss 0.4639	TrainAcc 0.8357	ValidAcc 0.3155	TestAcc 0.2920	BestValid 0.3179
	Epoch 800:	Loss 0.4326	TrainAcc 0.8462	ValidAcc 0.3197	TestAcc 0.2920	BestValid 0.3197
	Epoch 850:	Loss 0.4315	TrainAcc 0.8494	ValidAcc 0.3221	TestAcc 0.2911	BestValid 0.3221
	Epoch 900:	Loss 0.4174	TrainAcc 0.8526	ValidAcc 0.3239	TestAcc 0.2997	BestValid 0.3239
	Epoch 950:	Loss 0.3913	TrainAcc 0.8514	ValidAcc 0.3215	TestAcc 0.2968	BestValid 0.3239
	Epoch 1000:	Loss 0.3789	TrainAcc 0.8694	ValidAcc 0.3239	TestAcc 0.2939	BestValid 0.3239
	Epoch 1050:	Loss 0.3581	TrainAcc 0.8682	ValidAcc 0.3203	TestAcc 0.2978	BestValid 0.3239
	Epoch 1100:	Loss 0.3724	TrainAcc 0.8662	ValidAcc 0.3239	TestAcc 0.3007	BestValid 0.3239
	Epoch 1150:	Loss 0.3350	TrainAcc 0.8790	ValidAcc 0.3233	TestAcc 0.2997	BestValid 0.3239
	Epoch 1200:	Loss 0.3356	TrainAcc 0.8830	ValidAcc 0.3251	TestAcc 0.3007	BestValid 0.3251
	Epoch 1250:	Loss 0.3134	TrainAcc 0.8834	ValidAcc 0.3209	TestAcc 0.2978	BestValid 0.3251
	Epoch 1300:	Loss 0.3152	TrainAcc 0.8838	ValidAcc 0.3209	TestAcc 0.2988	BestValid 0.3251
	Epoch 1350:	Loss 0.3144	TrainAcc 0.8850	ValidAcc 0.3215	TestAcc 0.2997	BestValid 0.3251
	Epoch 1400:	Loss 0.3000	TrainAcc 0.8994	ValidAcc 0.3251	TestAcc 0.3103	BestValid 0.3251
	Epoch 1450:	Loss 0.2866	TrainAcc 0.8966	ValidAcc 0.3245	TestAcc 0.3026	BestValid 0.3251
	Epoch 1500:	Loss 0.2778	TrainAcc 0.8970	ValidAcc 0.3233	TestAcc 0.3045	BestValid 0.3251
	Epoch 1550:	Loss 0.2625	TrainAcc 0.9010	ValidAcc 0.3263	TestAcc 0.3055	BestValid 0.3263
	Epoch 1600:	Loss 0.2807	TrainAcc 0.8938	ValidAcc 0.3137	TestAcc 0.3036	BestValid 0.3263
	Epoch 1650:	Loss 0.2613	TrainAcc 0.9099	ValidAcc 0.3269	TestAcc 0.3151	BestValid 0.3269
	Epoch 1700:	Loss 0.2556	TrainAcc 0.9103	ValidAcc 0.3239	TestAcc 0.3074	BestValid 0.3269
	Epoch 1750:	Loss 0.2436	TrainAcc 0.9131	ValidAcc 0.3263	TestAcc 0.3112	BestValid 0.3269
	Epoch 1800:	Loss 0.2561	TrainAcc 0.9215	ValidAcc 0.3317	TestAcc 0.3170	BestValid 0.3317
	Epoch 1850:	Loss 0.2371	TrainAcc 0.9251	ValidAcc 0.3323	TestAcc 0.3170	BestValid 0.3323
	Epoch 1900:	Loss 0.2356	TrainAcc 0.9267	ValidAcc 0.3323	TestAcc 0.3199	BestValid 0.3323
	Epoch 1950:	Loss 0.2450	TrainAcc 0.9155	ValidAcc 0.3293	TestAcc 0.3093	BestValid 0.3323
	Epoch 2000:	Loss 0.2240	TrainAcc 0.9263	ValidAcc 0.3287	TestAcc 0.3151	BestValid 0.3323
	Epoch 2050:	Loss 0.2200	TrainAcc 0.9331	ValidAcc 0.3299	TestAcc 0.3208	BestValid 0.3323
	Epoch 2100:	Loss 0.2303	TrainAcc 0.9331	ValidAcc 0.3269	TestAcc 0.3208	BestValid 0.3323
	Epoch 2150:	Loss 0.2065	TrainAcc 0.9427	ValidAcc 0.3329	TestAcc 0.3314	BestValid 0.3329
	Epoch 2200:	Loss 0.2286	TrainAcc 0.9355	ValidAcc 0.3293	TestAcc 0.3180	BestValid 0.3329
	Epoch 2250:	Loss 0.2052	TrainAcc 0.9315	ValidAcc 0.3269	TestAcc 0.3141	BestValid 0.3329
	Epoch 2300:	Loss 0.2116	TrainAcc 0.9343	ValidAcc 0.3299	TestAcc 0.3208	BestValid 0.3329
	Epoch 2350:	Loss 0.1959	TrainAcc 0.9399	ValidAcc 0.3323	TestAcc 0.3314	BestValid 0.3329
	Epoch 2400:	Loss 0.2085	TrainAcc 0.9387	ValidAcc 0.3281	TestAcc 0.3305	BestValid 0.3329
	Epoch 2450:	Loss 0.2003	TrainAcc 0.9379	ValidAcc 0.3251	TestAcc 0.3228	BestValid 0.3329
	Epoch 2500:	Loss 0.2014	TrainAcc 0.9423	ValidAcc 0.3311	TestAcc 0.3256	BestValid 0.3329
	Epoch 2550:	Loss 0.1745	TrainAcc 0.9351	ValidAcc 0.3287	TestAcc 0.3160	BestValid 0.3329
	Epoch 2600:	Loss 0.1833	TrainAcc 0.9415	ValidAcc 0.3281	TestAcc 0.3208	BestValid 0.3329
	Epoch 2650:	Loss 0.1910	TrainAcc 0.9483	ValidAcc 0.3335	TestAcc 0.3266	BestValid 0.3335
	Epoch 2700:	Loss 0.1838	TrainAcc 0.9451	ValidAcc 0.3281	TestAcc 0.3237	BestValid 0.3335
	Epoch 2750:	Loss 0.1933	TrainAcc 0.9519	ValidAcc 0.3311	TestAcc 0.3285	BestValid 0.3335
	Epoch 2800:	Loss 0.1825	TrainAcc 0.9523	ValidAcc 0.3383	TestAcc 0.3324	BestValid 0.3383
	Epoch 2850:	Loss 0.1695	TrainAcc 0.9443	ValidAcc 0.3275	TestAcc 0.3199	BestValid 0.3383
	Epoch 2900:	Loss 0.1621	TrainAcc 0.9499	ValidAcc 0.3341	TestAcc 0.3247	BestValid 0.3383
	Epoch 2950:	Loss 0.1729	TrainAcc 0.9487	ValidAcc 0.3305	TestAcc 0.3218	BestValid 0.3383
	Epoch 3000:	Loss 0.1597	TrainAcc 0.9527	ValidAcc 0.3341	TestAcc 0.3295	BestValid 0.3383
	Epoch 3050:	Loss 0.1707	TrainAcc 0.9531	ValidAcc 0.3305	TestAcc 0.3256	BestValid 0.3383
	Epoch 3100:	Loss 0.1749	TrainAcc 0.9495	ValidAcc 0.3317	TestAcc 0.3305	BestValid 0.3383
	Epoch 3150:	Loss 0.1660	TrainAcc 0.9499	ValidAcc 0.3317	TestAcc 0.3285	BestValid 0.3383
	Epoch 3200:	Loss 0.1682	TrainAcc 0.9535	ValidAcc 0.3329	TestAcc 0.3324	BestValid 0.3383
	Epoch 3250:	Loss 0.1561	TrainAcc 0.9535	ValidAcc 0.3329	TestAcc 0.3305	BestValid 0.3383
	Epoch 3300:	Loss 0.1466	TrainAcc 0.9575	ValidAcc 0.3395	TestAcc 0.3362	BestValid 0.3395
	Epoch 3350:	Loss 0.1676	TrainAcc 0.9571	ValidAcc 0.3365	TestAcc 0.3314	BestValid 0.3395
	Epoch 3400:	Loss 0.1553	TrainAcc 0.9571	ValidAcc 0.3347	TestAcc 0.3314	BestValid 0.3395
	Epoch 3450:	Loss 0.1512	TrainAcc 0.9567	ValidAcc 0.3329	TestAcc 0.3324	BestValid 0.3395
	Epoch 3500:	Loss 0.1448	TrainAcc 0.9567	ValidAcc 0.3383	TestAcc 0.3324	BestValid 0.3395
	Epoch 3550:	Loss 0.1451	TrainAcc 0.9607	ValidAcc 0.3383	TestAcc 0.3343	BestValid 0.3395
	Epoch 3600:	Loss 0.1525	TrainAcc 0.9615	ValidAcc 0.3353	TestAcc 0.3420	BestValid 0.3395
	Epoch 3650:	Loss 0.1655	TrainAcc 0.9555	ValidAcc 0.3245	TestAcc 0.3256	BestValid 0.3395
	Epoch 3700:	Loss 0.1468	TrainAcc 0.9603	ValidAcc 0.3305	TestAcc 0.3362	BestValid 0.3395
	Epoch 3750:	Loss 0.1467	TrainAcc 0.9716	ValidAcc 0.3419	TestAcc 0.3429	BestValid 0.3419
	Epoch 3800:	Loss 0.1417	TrainAcc 0.9603	ValidAcc 0.3335	TestAcc 0.3266	BestValid 0.3419
	Epoch 3850:	Loss 0.1372	TrainAcc 0.9647	ValidAcc 0.3347	TestAcc 0.3362	BestValid 0.3419
	Epoch 3900:	Loss 0.1403	TrainAcc 0.9679	ValidAcc 0.3305	TestAcc 0.3372	BestValid 0.3419
	Epoch 3950:	Loss 0.1502	TrainAcc 0.9700	ValidAcc 0.3365	TestAcc 0.3353	BestValid 0.3419
	Epoch 4000:	Loss 0.1383	TrainAcc 0.9647	ValidAcc 0.3329	TestAcc 0.3333	BestValid 0.3419
	Epoch 4050:	Loss 0.1422	TrainAcc 0.9667	ValidAcc 0.3257	TestAcc 0.3333	BestValid 0.3419
	Epoch 4100:	Loss 0.1426	TrainAcc 0.9700	ValidAcc 0.3395	TestAcc 0.3429	BestValid 0.3419
	Epoch 4150:	Loss 0.1313	TrainAcc 0.9708	ValidAcc 0.3353	TestAcc 0.3401	BestValid 0.3419
	Epoch 4200:	Loss 0.1327	TrainAcc 0.9683	ValidAcc 0.3281	TestAcc 0.3372	BestValid 0.3419
	Epoch 4250:	Loss 0.1395	TrainAcc 0.9748	ValidAcc 0.3383	TestAcc 0.3449	BestValid 0.3419
	Epoch 4300:	Loss 0.1307	TrainAcc 0.9700	ValidAcc 0.3347	TestAcc 0.3381	BestValid 0.3419
	Epoch 4350:	Loss 0.1336	TrainAcc 0.9700	ValidAcc 0.3329	TestAcc 0.3401	BestValid 0.3419
	Epoch 4400:	Loss 0.1310	TrainAcc 0.9675	ValidAcc 0.3341	TestAcc 0.3362	BestValid 0.3419
	Epoch 4450:	Loss 0.1185	TrainAcc 0.9756	ValidAcc 0.3383	TestAcc 0.3429	BestValid 0.3419
	Epoch 4500:	Loss 0.1282	TrainAcc 0.9704	ValidAcc 0.3311	TestAcc 0.3343	BestValid 0.3419
	Epoch 4550:	Loss 0.1175	TrainAcc 0.9736	ValidAcc 0.3317	TestAcc 0.3324	BestValid 0.3419
	Epoch 4600:	Loss 0.1178	TrainAcc 0.9748	ValidAcc 0.3335	TestAcc 0.3420	BestValid 0.3419
	Epoch 4650:	Loss 0.1353	TrainAcc 0.9768	ValidAcc 0.3401	TestAcc 0.3401	BestValid 0.3419
	Epoch 4700:	Loss 0.1103	TrainAcc 0.9776	ValidAcc 0.3450	TestAcc 0.3429	BestValid 0.3450
	Epoch 4750:	Loss 0.1215	TrainAcc 0.9728	ValidAcc 0.3317	TestAcc 0.3391	BestValid 0.3450
	Epoch 4800:	Loss 0.1207	TrainAcc 0.9792	ValidAcc 0.3438	TestAcc 0.3525	BestValid 0.3450
	Epoch 4850:	Loss 0.1028	TrainAcc 0.9764	ValidAcc 0.3377	TestAcc 0.3487	BestValid 0.3450
	Epoch 4900:	Loss 0.1134	TrainAcc 0.9756	ValidAcc 0.3438	TestAcc 0.3477	BestValid 0.3450
	Epoch 4950:	Loss 0.1143	TrainAcc 0.9736	ValidAcc 0.3365	TestAcc 0.3429	BestValid 0.3450
	Epoch 5000:	Loss 0.1252	TrainAcc 0.9784	ValidAcc 0.3419	TestAcc 0.3497	BestValid 0.3450
****** Epoch Time (Excluding Evaluation Cost): 0.198 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.117 ms (Max: 0.161, Min: 0.034, Sum: 0.933)
Cluster-Wide Average, Compute: 32.824 ms (Max: 46.995, Min: 29.004, Sum: 262.593)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.013, Sum: 0.123)
Cluster-Wide Average, Communication-Graph: 113.306 ms (Max: 117.174, Min: 99.156, Sum: 906.449)
Cluster-Wide Average, Optimization: 47.039 ms (Max: 47.177, Min: 46.909, Sum: 376.315)
Cluster-Wide Average, Others: 4.142 ms (Max: 4.199, Min: 4.093, Sum: 33.140)
****** Breakdown Sum: 197.452 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.532 GB (Max: 5.975, Min: 5.456, Sum: 44.259)
Cluster-Wide Average, Graph-Level Communication Throughput: 28.066 Gbps (Max: 48.199, Min: 9.714, Sum: 224.529)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.753 GB
Weight-sync communication (cluster-wide, per-epoch): 1.778 GB
Total communication (cluster-wide, per-epoch): 4.531 GB
****** Accuracy Results ******
Highest valid_acc: 0.3450
Target test_acc: 0.3429
Epoch to reach the target acc: 4699
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
