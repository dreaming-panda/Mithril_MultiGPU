Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.015 seconds.
Building the CSC structure...
Building the Feature Vector...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the Feature Vector...
        It takes 0.012 seconds.
        It takes 0.018 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.010 seconds.
        It takes 0.012 seconds.
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.357 Gbps (per GPU), 482.858 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.060 Gbps (per GPU), 480.478 Gbps (aggregated)
The layer-level communication performance: 60.068 Gbps (per GPU), 480.544 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.814 Gbps (per GPU), 478.515 Gbps (aggregated)
The layer-level communication performance: 59.780 Gbps (per GPU), 478.243 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.557 Gbps (per GPU), 476.453 Gbps (aggregated)
The layer-level communication performance: 59.480 Gbps (per GPU), 475.839 Gbps (aggregated)
The layer-level communication performance: 59.508 Gbps (per GPU), 476.064 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.207 Gbps (per GPU), 1249.653 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.172 Gbps (per GPU), 1249.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.183 Gbps (per GPU), 1249.467 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.224 Gbps (per GPU), 1249.793 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.151 Gbps (per GPU), 1249.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.186 Gbps (per GPU), 1249.490 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.183 Gbps (per GPU), 1249.467 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.102 Gbps (per GPU), 1248.816 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 93.718 Gbps (per GPU), 749.742 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.719 Gbps (per GPU), 749.753 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.717 Gbps (per GPU), 749.736 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.720 Gbps (per GPU), 749.758 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.720 Gbps (per GPU), 749.758 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.709 Gbps (per GPU), 749.670 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.719 Gbps (per GPU), 749.753 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.478 Gbps (per GPU), 747.826 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.997 Gbps (per GPU), 279.972 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.996 Gbps (per GPU), 279.966 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.997 Gbps (per GPU), 279.975 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.993 Gbps (per GPU), 279.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.992 Gbps (per GPU), 279.938 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.988 Gbps (per GPU), 279.901 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.984 Gbps (per GPU), 279.870 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.981 Gbps (per GPU), 279.848 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.03ms  1.66ms  1.82ms  1.76  0.65K  0.22M
 chk_1  1.03ms  0.97ms  1.12ms  1.16  0.65K  0.05M
 chk_2  1.03ms  0.84ms  0.99ms  1.22  0.65K  0.02M
 chk_3  1.09ms  0.86ms  1.01ms  1.28  0.65K  0.02M
 chk_4  1.07ms  0.80ms  0.95ms  1.33  0.65K  0.01M
 chk_5  1.05ms  0.79ms  1.08ms  1.36  0.65K  0.01M
 chk_6  1.05ms  0.99ms  1.13ms  1.14  0.65K  0.06M
 chk_7  1.05ms  0.90ms  0.96ms  1.16  0.65K  0.01M
   Avg  1.05  0.98  1.13
   Max  1.09  1.66  1.82
   Min  1.03  0.79  0.95
 Ratio  1.06  2.09  1.92
   Var  0.00  0.07  0.07
Profiling takes 0.395 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 39.084 ms
Partition 0 [0, 4) has cost: 31.839 ms
Partition 1 [4, 8) has cost: 31.267 ms
Partition 2 [8, 12) has cost: 31.267 ms
Partition 3 [12, 16) has cost: 31.267 ms
Partition 4 [16, 20) has cost: 31.267 ms
Partition 5 [20, 24) has cost: 31.267 ms
Partition 6 [24, 29) has cost: 39.084 ms
Partition 7 [29, 33) has cost: 32.500 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.672 ms
GPU 0, Compute+Comm Time: 12.369 ms, Bubble Time: 11.529 ms, Imbalance Overhead: 7.774 ms
GPU 1, Compute+Comm Time: 12.455 ms, Bubble Time: 12.129 ms, Imbalance Overhead: 7.088 ms
GPU 2, Compute+Comm Time: 12.455 ms, Bubble Time: 12.945 ms, Imbalance Overhead: 6.272 ms
GPU 3, Compute+Comm Time: 12.455 ms, Bubble Time: 13.851 ms, Imbalance Overhead: 5.366 ms
GPU 4, Compute+Comm Time: 12.455 ms, Bubble Time: 14.757 ms, Imbalance Overhead: 4.460 ms
GPU 5, Compute+Comm Time: 12.455 ms, Bubble Time: 15.356 ms, Imbalance Overhead: 3.861 ms
GPU 6, Compute+Comm Time: 14.879 ms, Bubble Time: 16.085 ms, Imbalance Overhead: 0.708 ms
GPU 7, Compute+Comm Time: 12.768 ms, Bubble Time: 17.624 ms, Imbalance Overhead: 1.281 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 62.681 ms
GPU 0, Compute+Comm Time: 25.247 ms, Bubble Time: 34.772 ms, Imbalance Overhead: 2.661 ms
GPU 1, Compute+Comm Time: 29.720 ms, Bubble Time: 31.674 ms, Imbalance Overhead: 1.286 ms
GPU 2, Compute+Comm Time: 24.327 ms, Bubble Time: 30.275 ms, Imbalance Overhead: 8.078 ms
GPU 3, Compute+Comm Time: 24.327 ms, Bubble Time: 29.177 ms, Imbalance Overhead: 9.176 ms
GPU 4, Compute+Comm Time: 24.327 ms, Bubble Time: 27.420 ms, Imbalance Overhead: 10.933 ms
GPU 5, Compute+Comm Time: 24.327 ms, Bubble Time: 25.688 ms, Imbalance Overhead: 12.665 ms
GPU 6, Compute+Comm Time: 24.327 ms, Bubble Time: 24.139 ms, Imbalance Overhead: 14.214 ms
GPU 7, Compute+Comm Time: 24.985 ms, Bubble Time: 22.935 ms, Imbalance Overhead: 14.760 ms
The estimated cost of the whole pipeline: 99.070 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 70.351 ms
Partition 0 [0, 8) has cost: 63.107 ms
Partition 1 [8, 16) has cost: 62.535 ms
Partition 2 [16, 25) has cost: 70.351 ms
Partition 3 [25, 33) has cost: 63.767 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 33.639 ms
GPU 0, Compute+Comm Time: 15.738 ms, Bubble Time: 11.351 ms, Imbalance Overhead: 6.550 ms
GPU 1, Compute+Comm Time: 16.241 ms, Bubble Time: 12.491 ms, Imbalance Overhead: 4.907 ms
GPU 2, Compute+Comm Time: 17.927 ms, Bubble Time: 14.162 ms, Imbalance Overhead: 1.550 ms
GPU 3, Compute+Comm Time: 16.376 ms, Bubble Time: 16.760 ms, Imbalance Overhead: 0.503 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 62.538 ms
GPU 0, Compute+Comm Time: 30.142 ms, Bubble Time: 31.534 ms, Imbalance Overhead: 0.862 ms
GPU 1, Compute+Comm Time: 33.099 ms, Bubble Time: 26.376 ms, Imbalance Overhead: 3.063 ms
GPU 2, Compute+Comm Time: 29.727 ms, Bubble Time: 23.022 ms, Imbalance Overhead: 9.789 ms
GPU 3, Compute+Comm Time: 29.427 ms, Bubble Time: 20.636 ms, Imbalance Overhead: 12.474 ms
    The estimated cost with 2 DP ways is 100.986 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 133.458 ms
Partition 0 [0, 17) has cost: 133.458 ms
Partition 1 [17, 33) has cost: 126.302 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.349 ms
GPU 0, Compute+Comm Time: 27.323 ms, Bubble Time: 11.526 ms, Imbalance Overhead: 3.501 ms
GPU 1, Compute+Comm Time: 26.808 ms, Bubble Time: 15.541 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 67.950 ms
GPU 0, Compute+Comm Time: 42.404 ms, Bubble Time: 25.546 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 43.745 ms, Bubble Time: 17.513 ms, Imbalance Overhead: 6.693 ms
    The estimated cost with 4 DP ways is 115.814 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 259.760 ms
Partition 0 [0, 33) has cost: 259.760 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 84.066 ms
GPU 0, Compute+Comm Time: 84.066 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 103.711 ms
GPU 0, Compute+Comm Time: 103.711 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 197.166 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.1463	TrainAcc 0.5813	ValidAcc 0.3161	TestAcc 0.3112	BestValid 0.3161
	Epoch 100:	Loss 0.9751	TrainAcc 0.6715	ValidAcc 0.3131	TestAcc 0.3084	BestValid 0.3161
	Epoch 150:	Loss 0.8735	TrainAcc 0.7192	ValidAcc 0.3179	TestAcc 0.3103	BestValid 0.3179
	Epoch 200:	Loss 0.8063	TrainAcc 0.7524	ValidAcc 0.3161	TestAcc 0.3132	BestValid 0.3179
	Epoch 250:	Loss 0.7709	TrainAcc 0.7388	ValidAcc 0.3131	TestAcc 0.2949	BestValid 0.3179
	Epoch 300:	Loss 0.7258	TrainAcc 0.7244	ValidAcc 0.3131	TestAcc 0.2872	BestValid 0.3179
	Epoch 350:	Loss 0.6843	TrainAcc 0.7744	ValidAcc 0.3167	TestAcc 0.2968	BestValid 0.3179
	Epoch 400:	Loss 0.6338	TrainAcc 0.7592	ValidAcc 0.3089	TestAcc 0.2891	BestValid 0.3179
	Epoch 450:	Loss 0.6222	TrainAcc 0.7792	ValidAcc 0.3155	TestAcc 0.2901	BestValid 0.3179
	Epoch 500:	Loss 0.5787	TrainAcc 0.7945	ValidAcc 0.3113	TestAcc 0.2978	BestValid 0.3179
	Epoch 550:	Loss 0.5517	TrainAcc 0.8033	ValidAcc 0.3113	TestAcc 0.2911	BestValid 0.3179
	Epoch 600:	Loss 0.5541	TrainAcc 0.8213	ValidAcc 0.3179	TestAcc 0.3007	BestValid 0.3179
	Epoch 650:	Loss 0.5152	TrainAcc 0.8089	ValidAcc 0.3119	TestAcc 0.2863	BestValid 0.3179
	Epoch 700:	Loss 0.4853	TrainAcc 0.8301	ValidAcc 0.3173	TestAcc 0.2959	BestValid 0.3179
	Epoch 750:	Loss 0.4640	TrainAcc 0.8353	ValidAcc 0.3149	TestAcc 0.2920	BestValid 0.3179
	Epoch 800:	Loss 0.4326	TrainAcc 0.8462	ValidAcc 0.3197	TestAcc 0.2911	BestValid 0.3197
	Epoch 850:	Loss 0.4315	TrainAcc 0.8490	ValidAcc 0.3215	TestAcc 0.2911	BestValid 0.3215
	Epoch 900:	Loss 0.4175	TrainAcc 0.8526	ValidAcc 0.3239	TestAcc 0.2997	BestValid 0.3239
	Epoch 950:	Loss 0.3914	TrainAcc 0.8514	ValidAcc 0.3215	TestAcc 0.2968	BestValid 0.3239
	Epoch 1000:	Loss 0.3789	TrainAcc 0.8694	ValidAcc 0.3239	TestAcc 0.2949	BestValid 0.3239
****** Epoch Time (Excluding Evaluation Cost): 0.199 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.114 ms (Max: 0.160, Min: 0.039, Sum: 0.913)
Cluster-Wide Average, Compute: 32.797 ms (Max: 47.225, Min: 29.391, Sum: 262.378)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.122)
Cluster-Wide Average, Communication-Graph: 114.056 ms (Max: 117.480, Min: 99.603, Sum: 912.446)
Cluster-Wide Average, Optimization: 47.297 ms (Max: 47.422, Min: 47.174, Sum: 378.379)
Cluster-Wide Average, Others: 4.150 ms (Max: 4.214, Min: 4.094, Sum: 33.199)
****** Breakdown Sum: 198.437 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.532 GB (Max: 5.975, Min: 5.456, Sum: 44.259)
Cluster-Wide Average, Graph-Level Communication Throughput: 27.864 Gbps (Max: 47.971, Min: 9.710, Sum: 222.908)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.753 GB
Weight-sync communication (cluster-wide, per-epoch): 1.778 GB
Total communication (cluster-wide, per-epoch): 4.531 GB
****** Accuracy Results ******
Highest valid_acc: 0.3239
Target test_acc: 0.2997
Epoch to reach the target acc: 899
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
