Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.018 seconds.
        It takes 0.017 seconds.
        It takes 0.018 seconds.
        It takes 0.016 seconds.
Building the Feature Vector...
        It takes 0.009 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.571 Gbps (per GPU), 484.567 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.275 Gbps (per GPU), 482.197 Gbps (aggregated)
The layer-level communication performance: 60.263 Gbps (per GPU), 482.103 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.989 Gbps (per GPU), 479.913 Gbps (aggregated)
The layer-level communication performance: 59.962 Gbps (per GPU), 479.693 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.756 Gbps (per GPU), 478.044 Gbps (aggregated)
The layer-level communication performance: 59.710 Gbps (per GPU), 477.680 Gbps (aggregated)
The layer-level communication performance: 59.670 Gbps (per GPU), 477.359 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.228 Gbps (per GPU), 1273.827 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.204 Gbps (per GPU), 1273.631 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.171 Gbps (per GPU), 1273.365 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.219 Gbps (per GPU), 1273.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.243 Gbps (per GPU), 1273.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.228 Gbps (per GPU), 1273.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.159 Gbps (per GPU), 1273.268 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.207 Gbps (per GPU), 1273.655 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.801 Gbps (per GPU), 838.407 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.797 Gbps (per GPU), 838.379 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.801 Gbps (per GPU), 838.407 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.795 Gbps (per GPU), 838.358 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.801 Gbps (per GPU), 838.407 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.793 Gbps (per GPU), 838.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.800 Gbps (per GPU), 838.400 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.797 Gbps (per GPU), 838.379 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.450 Gbps (per GPU), 275.598 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.449 Gbps (per GPU), 275.596 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.446 Gbps (per GPU), 275.569 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.450 Gbps (per GPU), 275.600 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.444 Gbps (per GPU), 275.555 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.449 Gbps (per GPU), 275.591 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.447 Gbps (per GPU), 275.575 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.444 Gbps (per GPU), 275.555 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.00ms  2.06ms  2.76ms  1.38  0.65K  0.22M
 chk_1  1.30ms  1.36ms  2.07ms  1.59  0.65K  0.05M
 chk_2  1.23ms  1.22ms  1.93ms  1.58  0.65K  0.02M
 chk_3  1.25ms  1.23ms  1.93ms  1.57  0.65K  0.02M
 chk_4  1.18ms  1.17ms  1.89ms  1.61  0.65K  0.01M
 chk_5  1.17ms  1.17ms  1.88ms  1.61  0.65K  0.01M
 chk_6  1.39ms  1.37ms  2.08ms  1.52  0.65K  0.06M
 chk_7  1.20ms  1.19ms  1.91ms  1.61  0.65K  0.01M
   Avg  1.34  1.35  2.06
   Max  2.00  2.06  2.76
   Min  1.17  1.17  1.88
 Ratio  1.71  1.76  1.47
   Var  0.07  0.08  0.08
Profiling takes 0.499 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 229)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 229)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 229)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 229)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 229)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 229)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 229)...
+++++++++ Node 0 initializing the weights for op[0, 229)...
+++++++++ Node 5 initializing the weights for op[0, 229)...
+++++++++ Node 2 initializing the weights for op[0, 229)...
+++++++++ Node 3 initializing the weights for op[0, 229)...
+++++++++ Node 7 initializing the weights for op[0, 229)...
+++++++++ Node 4 initializing the weights for op[0, 229)...
+++++++++ Node 6 initializing the weights for op[0, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.0423	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2055
	Epoch 50:	Loss 1.7251	TrainAcc 0.2059	ValidAcc 0.1953	TestAcc 0.1931	BestValid 0.2055
	Epoch 100:	Loss 1.6639	TrainAcc 0.2592	ValidAcc 0.2452	TestAcc 0.2738	BestValid 0.2452
	Epoch 150:	Loss 1.5992	TrainAcc 0.2484	ValidAcc 0.2476	TestAcc 0.2699	BestValid 0.2476
	Epoch 200:	Loss 1.5368	TrainAcc 0.3029	ValidAcc 0.3005	TestAcc 0.2978	BestValid 0.3005
	Epoch 250:	Loss 1.3383	TrainAcc 0.4275	ValidAcc 0.3690	TestAcc 0.3833	BestValid 0.3690
	Epoch 300:	Loss 1.2264	TrainAcc 0.4904	ValidAcc 0.3774	TestAcc 0.4063	BestValid 0.3774
	Epoch 350:	Loss 1.2343	TrainAcc 0.4964	ValidAcc 0.3684	TestAcc 0.3900	BestValid 0.3774
	Epoch 400:	Loss 1.0977	TrainAcc 0.5693	ValidAcc 0.4123	TestAcc 0.4159	BestValid 0.4123
	Epoch 450:	Loss 0.9951	TrainAcc 0.6174	ValidAcc 0.4219	TestAcc 0.4304	BestValid 0.4219
	Epoch 500:	Loss 0.8347	TrainAcc 0.6875	ValidAcc 0.4411	TestAcc 0.4640	BestValid 0.4411
	Epoch 550:	Loss 0.8575	TrainAcc 0.6787	ValidAcc 0.4387	TestAcc 0.4592	BestValid 0.4411
	Epoch 600:	Loss 0.6405	TrainAcc 0.7672	ValidAcc 0.4892	TestAcc 0.5034	BestValid 0.4892
	Epoch 650:	Loss 0.5661	TrainAcc 0.7965	ValidAcc 0.4958	TestAcc 0.5168	BestValid 0.4958
	Epoch 700:	Loss 0.5147	TrainAcc 0.8349	ValidAcc 0.5024	TestAcc 0.5226	BestValid 0.5024
	Epoch 750:	Loss 0.4628	TrainAcc 0.8377	ValidAcc 0.5042	TestAcc 0.5226	BestValid 0.5042
	Epoch 800:	Loss 0.4215	TrainAcc 0.8678	ValidAcc 0.5132	TestAcc 0.5312	BestValid 0.5132
	Epoch 850:	Loss 0.3751	TrainAcc 0.8914	ValidAcc 0.5180	TestAcc 0.5399	BestValid 0.5180
	Epoch 900:	Loss 0.3415	TrainAcc 0.8946	ValidAcc 0.5228	TestAcc 0.5427	BestValid 0.5228
	Epoch 950:	Loss 0.3058	TrainAcc 0.9054	ValidAcc 0.5150	TestAcc 0.5427	BestValid 0.5228
	Epoch 1000:	Loss 0.2871	TrainAcc 0.9107	ValidAcc 0.5216	TestAcc 0.5514	BestValid 0.5228
	Epoch 1050:	Loss 0.2509	TrainAcc 0.9239	ValidAcc 0.5270	TestAcc 0.5504	BestValid 0.5270
	Epoch 1100:	Loss 0.2352	TrainAcc 0.9375	ValidAcc 0.5331	TestAcc 0.5610	BestValid 0.5331
	Epoch 1150:	Loss 0.2005	TrainAcc 0.9455	ValidAcc 0.5312	TestAcc 0.5648	BestValid 0.5331
	Epoch 1200:	Loss 0.1904	TrainAcc 0.9539	ValidAcc 0.5319	TestAcc 0.5639	BestValid 0.5331
	Epoch 1250:	Loss 0.1963	TrainAcc 0.9479	ValidAcc 0.5337	TestAcc 0.5658	BestValid 0.5337
	Epoch 1300:	Loss 0.1664	TrainAcc 0.9559	ValidAcc 0.5355	TestAcc 0.5677	BestValid 0.5355
	Epoch 1350:	Loss 0.1564	TrainAcc 0.9575	ValidAcc 0.5403	TestAcc 0.5639	BestValid 0.5403
	Epoch 1400:	Loss 0.1373	TrainAcc 0.9683	ValidAcc 0.5439	TestAcc 0.5773	BestValid 0.5439
	Epoch 1450:	Loss 0.1359	TrainAcc 0.9696	ValidAcc 0.5457	TestAcc 0.5735	BestValid 0.5457
	Epoch 1500:	Loss 0.1280	TrainAcc 0.9675	ValidAcc 0.5433	TestAcc 0.5696	BestValid 0.5457
	Epoch 1550:	Loss 0.1229	TrainAcc 0.9700	ValidAcc 0.5397	TestAcc 0.5658	BestValid 0.5457
	Epoch 1600:	Loss 0.1108	TrainAcc 0.9744	ValidAcc 0.5379	TestAcc 0.5706	BestValid 0.5457
	Epoch 1650:	Loss 0.1239	TrainAcc 0.9740	ValidAcc 0.5355	TestAcc 0.5639	BestValid 0.5457
	Epoch 1700:	Loss 0.1028	TrainAcc 0.9776	ValidAcc 0.5403	TestAcc 0.5696	BestValid 0.5457
	Epoch 1750:	Loss 0.1063	TrainAcc 0.9776	ValidAcc 0.5373	TestAcc 0.5677	BestValid 0.5457
	Epoch 1800:	Loss 0.1013	TrainAcc 0.9780	ValidAcc 0.5397	TestAcc 0.5629	BestValid 0.5457
	Epoch 1850:	Loss 0.0898	TrainAcc 0.9828	ValidAcc 0.5343	TestAcc 0.5744	BestValid 0.5457
	Epoch 1900:	Loss 0.0874	TrainAcc 0.9820	ValidAcc 0.5409	TestAcc 0.5706	BestValid 0.5457
	Epoch 1950:	Loss 0.0763	TrainAcc 0.9812	ValidAcc 0.5391	TestAcc 0.5725	BestValid 0.5457
	Epoch 2000:	Loss 0.0730	TrainAcc 0.9832	ValidAcc 0.5361	TestAcc 0.5668	BestValid 0.5457
	Epoch 2050:	Loss 0.0636	TrainAcc 0.9836	ValidAcc 0.5379	TestAcc 0.5744	BestValid 0.5457
	Epoch 2100:	Loss 0.0720	TrainAcc 0.9876	ValidAcc 0.5337	TestAcc 0.5744	BestValid 0.5457
	Epoch 2150:	Loss 0.0589	TrainAcc 0.9864	ValidAcc 0.5385	TestAcc 0.5658	BestValid 0.5457
	Epoch 2200:	Loss 0.0631	TrainAcc 0.9860	ValidAcc 0.5373	TestAcc 0.5706	BestValid 0.5457
	Epoch 2250:	Loss 0.0612	TrainAcc 0.9860	ValidAcc 0.5355	TestAcc 0.5716	BestValid 0.5457
	Epoch 2300:	Loss 0.0657	TrainAcc 0.9876	ValidAcc 0.5391	TestAcc 0.5735	BestValid 0.5457
	Epoch 2350:	Loss 0.0550	TrainAcc 0.9912	ValidAcc 0.5379	TestAcc 0.5735	BestValid 0.5457
	Epoch 2400:	Loss 0.0500	TrainAcc 0.9920	ValidAcc 0.5373	TestAcc 0.5725	BestValid 0.5457
	Epoch 2450:	Loss 0.0655	TrainAcc 0.9892	ValidAcc 0.5331	TestAcc 0.5677	BestValid 0.5457
	Epoch 2500:	Loss 0.0455	TrainAcc 0.9920	ValidAcc 0.5409	TestAcc 0.5677	BestValid 0.5457
	Epoch 2550:	Loss 0.0443	TrainAcc 0.9900	ValidAcc 0.5385	TestAcc 0.5668	BestValid 0.5457
	Epoch 2600:	Loss 0.0419	TrainAcc 0.9928	ValidAcc 0.5397	TestAcc 0.5706	BestValid 0.5457
	Epoch 2650:	Loss 0.0427	TrainAcc 0.9908	ValidAcc 0.5343	TestAcc 0.5658	BestValid 0.5457
	Epoch 2700:	Loss 0.0423	TrainAcc 0.9932	ValidAcc 0.5379	TestAcc 0.5725	BestValid 0.5457
	Epoch 2750:	Loss 0.0381	TrainAcc 0.9932	ValidAcc 0.5331	TestAcc 0.5696	BestValid 0.5457
	Epoch 2800:	Loss 0.0409	TrainAcc 0.9940	ValidAcc 0.5427	TestAcc 0.5648	BestValid 0.5457
	Epoch 2850:	Loss 0.0276	TrainAcc 0.9944	ValidAcc 0.5409	TestAcc 0.5629	BestValid 0.5457
	Epoch 2900:	Loss 0.0285	TrainAcc 0.9948	ValidAcc 0.5355	TestAcc 0.5677	BestValid 0.5457
	Epoch 2950:	Loss 0.0312	TrainAcc 0.9956	ValidAcc 0.5385	TestAcc 0.5620	BestValid 0.5457
	Epoch 3000:	Loss 0.0292	TrainAcc 0.9952	ValidAcc 0.5361	TestAcc 0.5629	BestValid 0.5457
	Epoch 3050:	Loss 0.0280	TrainAcc 0.9940	ValidAcc 0.5355	TestAcc 0.5687	BestValid 0.5457
	Epoch 3100:	Loss 0.0304	TrainAcc 0.9948	ValidAcc 0.5349	TestAcc 0.5610	BestValid 0.5457
	Epoch 3150:	Loss 0.0331	TrainAcc 0.9952	ValidAcc 0.5373	TestAcc 0.5658	BestValid 0.5457
	Epoch 3200:	Loss 0.0250	TrainAcc 0.9952	ValidAcc 0.5325	TestAcc 0.5620	BestValid 0.5457
	Epoch 3250:	Loss 0.0261	TrainAcc 0.9960	ValidAcc 0.5306	TestAcc 0.5639	BestValid 0.5457
	Epoch 3300:	Loss 0.0288	TrainAcc 0.9964	ValidAcc 0.5385	TestAcc 0.5648	BestValid 0.5457
	Epoch 3350:	Loss 0.0224	TrainAcc 0.9956	ValidAcc 0.5343	TestAcc 0.5658	BestValid 0.5457
	Epoch 3400:	Loss 0.0264	TrainAcc 0.9940	ValidAcc 0.5325	TestAcc 0.5591	BestValid 0.5457
	Epoch 3450:	Loss 0.0313	TrainAcc 0.9968	ValidAcc 0.5367	TestAcc 0.5677	BestValid 0.5457
	Epoch 3500:	Loss 0.0191	TrainAcc 0.9964	ValidAcc 0.5276	TestAcc 0.5572	BestValid 0.5457
	Epoch 3550:	Loss 0.0221	TrainAcc 0.9976	ValidAcc 0.5264	TestAcc 0.5629	BestValid 0.5457
	Epoch 3600:	Loss 0.0177	TrainAcc 0.9968	ValidAcc 0.5306	TestAcc 0.5687	BestValid 0.5457
	Epoch 3650:	Loss 0.0214	TrainAcc 0.9964	ValidAcc 0.5325	TestAcc 0.5639	BestValid 0.5457
	Epoch 3700:	Loss 0.0187	TrainAcc 0.9976	ValidAcc 0.5367	TestAcc 0.5600	BestValid 0.5457
	Epoch 3750:	Loss 0.0211	TrainAcc 0.9948	ValidAcc 0.5300	TestAcc 0.5658	BestValid 0.5457
	Epoch 3800:	Loss 0.0198	TrainAcc 0.9960	ValidAcc 0.5331	TestAcc 0.5639	BestValid 0.5457
	Epoch 3850:	Loss 0.0216	TrainAcc 0.9968	ValidAcc 0.5252	TestAcc 0.5668	BestValid 0.5457
	Epoch 3900:	Loss 0.0180	TrainAcc 0.9980	ValidAcc 0.5294	TestAcc 0.5610	BestValid 0.5457
	Epoch 3950:	Loss 0.0179	TrainAcc 0.9976	ValidAcc 0.5264	TestAcc 0.5648	BestValid 0.5457
	Epoch 4000:	Loss 0.0135	TrainAcc 0.9980	ValidAcc 0.5252	TestAcc 0.5620	BestValid 0.5457
	Epoch 4050:	Loss 0.0169	TrainAcc 0.9956	ValidAcc 0.5361	TestAcc 0.5610	BestValid 0.5457
	Epoch 4100:	Loss 0.0162	TrainAcc 0.9976	ValidAcc 0.5228	TestAcc 0.5629	BestValid 0.5457
	Epoch 4150:	Loss 0.0173	TrainAcc 0.9980	ValidAcc 0.5252	TestAcc 0.5687	BestValid 0.5457
	Epoch 4200:	Loss 0.0221	TrainAcc 0.9976	ValidAcc 0.5264	TestAcc 0.5648	BestValid 0.5457
	Epoch 4250:	Loss 0.0161	TrainAcc 0.9976	ValidAcc 0.5234	TestAcc 0.5687	BestValid 0.5457
	Epoch 4300:	Loss 0.0134	TrainAcc 0.9980	ValidAcc 0.5252	TestAcc 0.5639	BestValid 0.5457
	Epoch 4350:	Loss 0.0192	TrainAcc 0.9976	ValidAcc 0.5300	TestAcc 0.5648	BestValid 0.5457
	Epoch 4400:	Loss 0.0117	TrainAcc 0.9984	ValidAcc 0.5276	TestAcc 0.5677	BestValid 0.5457
	Epoch 4450:	Loss 0.0145	TrainAcc 0.9980	ValidAcc 0.5276	TestAcc 0.5648	BestValid 0.5457
	Epoch 4500:	Loss 0.0115	TrainAcc 0.9976	ValidAcc 0.5210	TestAcc 0.5639	BestValid 0.5457
	Epoch 4550:	Loss 0.0177	TrainAcc 0.9976	ValidAcc 0.5228	TestAcc 0.5716	BestValid 0.5457
	Epoch 4600:	Loss 0.0180	TrainAcc 0.9980	ValidAcc 0.5228	TestAcc 0.5610	BestValid 0.5457
	Epoch 4650:	Loss 0.0110	TrainAcc 0.9988	ValidAcc 0.5210	TestAcc 0.5668	BestValid 0.5457
	Epoch 4700:	Loss 0.0092	TrainAcc 0.9984	ValidAcc 0.5222	TestAcc 0.5706	BestValid 0.5457
	Epoch 4750:	Loss 0.0147	TrainAcc 0.9980	ValidAcc 0.5234	TestAcc 0.5658	BestValid 0.5457
	Epoch 4800:	Loss 0.0111	TrainAcc 0.9984	ValidAcc 0.5222	TestAcc 0.5668	BestValid 0.5457
	Epoch 4850:	Loss 0.0166	TrainAcc 0.9976	ValidAcc 0.5240	TestAcc 0.5687	BestValid 0.5457
	Epoch 4900:	Loss 0.0124	TrainAcc 0.9984	ValidAcc 0.5240	TestAcc 0.5629	BestValid 0.5457
	Epoch 4950:	Loss 0.0128	TrainAcc 0.9992	ValidAcc 0.5300	TestAcc 0.5725	BestValid 0.5457
	Epoch 5000:	Loss 0.0069	TrainAcc 0.9984	ValidAcc 0.5258	TestAcc 0.5658	BestValid 0.5457
****** Epoch Time (Excluding Evaluation Cost): 0.202 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.427 ms (Max: 0.671, Min: 0.040, Sum: 3.420)
Cluster-Wide Average, Compute: 38.271 ms (Max: 54.087, Min: 34.764, Sum: 306.169)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.013, Sum: 0.123)
Cluster-Wide Average, Communication-Graph: 114.845 ms (Max: 118.249, Min: 99.281, Sum: 918.761)
Cluster-Wide Average, Optimization: 47.231 ms (Max: 47.331, Min: 47.131, Sum: 377.852)
Cluster-Wide Average, Others: 0.974 ms (Max: 1.012, Min: 0.953, Sum: 7.792)
****** Breakdown Sum: 201.772 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.861 GB (Max: 4.266, Min: 3.790, Sum: 30.888)
Cluster-Wide Average, Graph-Level Communication Throughput: 27.656 Gbps (Max: 48.123, Min: 9.676, Sum: 221.250)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.753 GB
Weight-sync communication (cluster-wide, per-epoch): 1.778 GB
Total communication (cluster-wide, per-epoch): 4.531 GB
****** Accuracy Results ******
Highest valid_acc: 0.5457
Target test_acc: 0.5735
Epoch to reach the target acc: 1449
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
