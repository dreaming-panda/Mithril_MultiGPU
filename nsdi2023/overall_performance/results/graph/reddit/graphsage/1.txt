Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.017 seconds.
Building the CSC structure...
        It takes 2.057 seconds.
Building the CSC structure...
        It takes 2.256 seconds.
Building the CSC structure...
        It takes 2.262 seconds.
Building the CSC structure...
        It takes 2.358 seconds.
Building the CSC structure...
        It takes 2.375 seconds.
Building the CSC structure...
        It takes 2.630 seconds.
Building the CSC structure...
        It takes 2.662 seconds.
Building the CSC structure...
        It takes 1.867 seconds.
        It takes 1.934 seconds.
        It takes 2.276 seconds.
        It takes 2.387 seconds.
        It takes 2.321 seconds.
        It takes 2.318 seconds.
        It takes 2.386 seconds.
        It takes 2.480 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.283 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.044 seconds.
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.311 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.290 seconds.
Building the Label Vector...
        It takes 0.273 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.037 seconds.
        It takes 0.308 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.298 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.044 seconds.
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.048 Gbps (per GPU), 440.382 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.779 Gbps (per GPU), 438.231 Gbps (aggregated)
The layer-level communication performance: 54.772 Gbps (per GPU), 438.177 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.575 Gbps (per GPU), 436.599 Gbps (aggregated)
The layer-level communication performance: 54.544 Gbps (per GPU), 436.355 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.370 Gbps (per GPU), 434.959 Gbps (aggregated)
The layer-level communication performance: 54.334 Gbps (per GPU), 434.674 Gbps (aggregated)
The layer-level communication performance: 54.300 Gbps (per GPU), 434.400 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.824 Gbps (per GPU), 1270.592 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.803 Gbps (per GPU), 1270.424 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.824 Gbps (per GPU), 1270.591 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.803 Gbps (per GPU), 1270.424 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.821 Gbps (per GPU), 1270.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.803 Gbps (per GPU), 1270.424 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.821 Gbps (per GPU), 1270.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.806 Gbps (per GPU), 1270.449 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 96.131 Gbps (per GPU), 769.044 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.131 Gbps (per GPU), 769.044 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.131 Gbps (per GPU), 769.044 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.130 Gbps (per GPU), 769.039 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.131 Gbps (per GPU), 769.044 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.130 Gbps (per GPU), 769.039 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.131 Gbps (per GPU), 769.044 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.131 Gbps (per GPU), 769.050 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.209 Gbps (per GPU), 289.675 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.209 Gbps (per GPU), 289.673 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.209 Gbps (per GPU), 289.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.208 Gbps (per GPU), 289.667 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.210 Gbps (per GPU), 289.677 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.210 Gbps (per GPU), 289.679 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.209 Gbps (per GPU), 289.673 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.210 Gbps (per GPU), 289.676 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.26ms  9.50ms  8.96ms  1.37 29.12K 14.23M
 chk_1  7.72ms  5.15ms  4.60ms  1.68 29.12K  6.56M
 chk_2 18.95ms 16.54ms 16.05ms  1.18 29.12K 24.68M
 chk_3 19.14ms 16.67ms 16.15ms  1.19 29.12K 22.95M
 chk_4  7.48ms  4.99ms  4.41ms  1.70 29.12K  6.33M
 chk_5 11.44ms  8.87ms  8.36ms  1.37 29.12K 12.05M
 chk_6 12.59ms 10.21ms  9.53ms  1.32 29.12K 14.60M
 chk_7 11.85ms  9.26ms  8.59ms  1.38 29.12K 13.21M
   Avg 12.68 10.15  9.58
   Max 19.14 16.67 16.15
   Min  7.48  4.99  4.41
 Ratio  2.56  3.34  3.66
   Var 16.90 17.22 17.42
Profiling takes 2.960 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 345.009 ms
Partition 0 [0, 4) has cost: 345.009 ms
Partition 1 [4, 8) has cost: 324.772 ms
Partition 2 [8, 12) has cost: 324.772 ms
Partition 3 [12, 16) has cost: 324.772 ms
Partition 4 [16, 20) has cost: 324.772 ms
Partition 5 [20, 24) has cost: 324.772 ms
Partition 6 [24, 28) has cost: 324.772 ms
Partition 7 [28, 32) has cost: 320.222 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 300.914 ms
GPU 0, Compute+Comm Time: 124.843 ms, Bubble Time: 128.797 ms, Imbalance Overhead: 47.274 ms
GPU 1, Compute+Comm Time: 119.360 ms, Bubble Time: 119.870 ms, Imbalance Overhead: 61.684 ms
GPU 2, Compute+Comm Time: 119.360 ms, Bubble Time: 110.236 ms, Imbalance Overhead: 71.318 ms
GPU 3, Compute+Comm Time: 119.360 ms, Bubble Time: 110.829 ms, Imbalance Overhead: 70.725 ms
GPU 4, Compute+Comm Time: 119.360 ms, Bubble Time: 120.329 ms, Imbalance Overhead: 61.225 ms
GPU 5, Compute+Comm Time: 119.360 ms, Bubble Time: 129.175 ms, Imbalance Overhead: 52.379 ms
GPU 6, Compute+Comm Time: 119.360 ms, Bubble Time: 138.170 ms, Imbalance Overhead: 43.383 ms
GPU 7, Compute+Comm Time: 118.130 ms, Bubble Time: 148.419 ms, Imbalance Overhead: 34.365 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 590.848 ms
GPU 0, Compute+Comm Time: 229.178 ms, Bubble Time: 291.795 ms, Imbalance Overhead: 69.876 ms
GPU 1, Compute+Comm Time: 232.497 ms, Bubble Time: 271.731 ms, Imbalance Overhead: 86.620 ms
GPU 2, Compute+Comm Time: 232.497 ms, Bubble Time: 254.217 ms, Imbalance Overhead: 104.133 ms
GPU 3, Compute+Comm Time: 232.497 ms, Bubble Time: 237.242 ms, Imbalance Overhead: 121.109 ms
GPU 4, Compute+Comm Time: 232.497 ms, Bubble Time: 218.450 ms, Imbalance Overhead: 139.901 ms
GPU 5, Compute+Comm Time: 232.497 ms, Bubble Time: 216.635 ms, Imbalance Overhead: 141.716 ms
GPU 6, Compute+Comm Time: 232.497 ms, Bubble Time: 235.688 ms, Imbalance Overhead: 122.663 ms
GPU 7, Compute+Comm Time: 247.251 ms, Bubble Time: 252.684 ms, Imbalance Overhead: 90.913 ms
The estimated cost of the whole pipeline: 936.350 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 669.782 ms
Partition 0 [0, 8) has cost: 669.782 ms
Partition 1 [8, 16) has cost: 649.545 ms
Partition 2 [16, 24) has cost: 649.545 ms
Partition 3 [24, 32) has cost: 644.995 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 311.130 ms
GPU 0, Compute+Comm Time: 160.985 ms, Bubble Time: 149.077 ms, Imbalance Overhead: 1.068 ms
GPU 1, Compute+Comm Time: 158.284 ms, Bubble Time: 130.516 ms, Imbalance Overhead: 22.330 ms
GPU 2, Compute+Comm Time: 158.284 ms, Bubble Time: 111.250 ms, Imbalance Overhead: 41.597 ms
GPU 3, Compute+Comm Time: 157.676 ms, Bubble Time: 111.657 ms, Imbalance Overhead: 41.796 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 593.981 ms
GPU 0, Compute+Comm Time: 298.715 ms, Bubble Time: 212.195 ms, Imbalance Overhead: 83.071 ms
GPU 1, Compute+Comm Time: 300.167 ms, Bubble Time: 210.727 ms, Imbalance Overhead: 83.087 ms
GPU 2, Compute+Comm Time: 300.167 ms, Bubble Time: 248.834 ms, Imbalance Overhead: 44.980 ms
GPU 3, Compute+Comm Time: 307.692 ms, Bubble Time: 284.883 ms, Imbalance Overhead: 1.405 ms
    The estimated cost with 2 DP ways is 950.366 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1319.326 ms
Partition 0 [0, 16) has cost: 1319.326 ms
Partition 1 [16, 32) has cost: 1294.539 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 354.560 ms
GPU 0, Compute+Comm Time: 236.653 ms, Bubble Time: 117.906 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 235.089 ms, Bubble Time: 117.944 ms, Imbalance Overhead: 1.527 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 628.071 ms
GPU 0, Compute+Comm Time: 415.366 ms, Bubble Time: 209.143 ms, Imbalance Overhead: 3.562 ms
GPU 1, Compute+Comm Time: 419.702 ms, Bubble Time: 208.369 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1031.762 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2613.865 ms
Partition 0 [0, 32) has cost: 2613.865 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 464.782 ms
GPU 0, Compute+Comm Time: 464.782 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 647.095 ms
GPU 0, Compute+Comm Time: 647.095 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1167.471 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 3.2648	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.0193	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 2.9570	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 2.9147	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 250:	Loss 2.8889	TrainAcc 0.0849	ValidAcc 0.0741	TestAcc 0.0730	BestValid 0.0741
	Epoch 300:	Loss 2.8447	TrainAcc 0.0217	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.0741
	Epoch 350:	Loss 2.8121	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.0741
	Epoch 400:	Loss 2.6021	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0741
	Epoch 450:	Loss 2.4722	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0741
	Epoch 500:	Loss 2.3004	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0741
	Epoch 550:	Loss 2.1245	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0741
	Epoch 600:	Loss 1.9110	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0741
	Epoch 650:	Loss 1.8986	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0741
	Epoch 700:	Loss 1.7805	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0741
	Epoch 750:	Loss 1.7451	TrainAcc 0.0133	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.0741
	Epoch 800:	Loss 1.5407	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.0741
	Epoch 850:	Loss 1.5769	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.0741
	Epoch 900:	Loss 1.4417	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0118	BestValid 0.0741
	Epoch 950:	Loss 1.3971	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0118	BestValid 0.0741
	Epoch 1000:	Loss 1.3302	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.0741
	Epoch 1050:	Loss 1.3417	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.0741
	Epoch 1100:	Loss 1.2488	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0118	BestValid 0.0741
	Epoch 1150:	Loss 1.4164	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.0741
	Epoch 1200:	Loss 1.2122	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.0741
	Epoch 1250:	Loss 1.2667	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0118	BestValid 0.0741
	Epoch 1300:	Loss 1.1817	TrainAcc 0.0132	ValidAcc 0.0126	TestAcc 0.0119	BestValid 0.0741
	Epoch 1350:	Loss 1.2379	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0118	BestValid 0.0741
	Epoch 1400:	Loss 1.1600	TrainAcc 0.0132	ValidAcc 0.0126	TestAcc 0.0119	BestValid 0.0741
	Epoch 1450:	Loss 1.2847	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0118	BestValid 0.0741
	Epoch 1500:	Loss 1.1357	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0118	BestValid 0.0741
	Epoch 1550:	Loss 1.0178	TrainAcc 0.0131	ValidAcc 0.0125	TestAcc 0.0118	BestValid 0.0741
	Epoch 1600:	Loss 1.1802	TrainAcc 0.0132	ValidAcc 0.0126	TestAcc 0.0120	BestValid 0.0741
	Epoch 1650:	Loss 1.0239	TrainAcc 0.0132	ValidAcc 0.0126	TestAcc 0.0119	BestValid 0.0741
	Epoch 1700:	Loss 1.0133	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.0741
	Epoch 1750:	Loss 0.9687	TrainAcc 0.0132	ValidAcc 0.0126	TestAcc 0.0119	BestValid 0.0741
	Epoch 1800:	Loss 1.0782	TrainAcc 0.0132	ValidAcc 0.0126	TestAcc 0.0119	BestValid 0.0741
	Epoch 1850:	Loss 1.0236	TrainAcc 0.0132	ValidAcc 0.0126	TestAcc 0.0119	BestValid 0.0741
	Epoch 1900:	Loss 1.9893	TrainAcc 0.0133	ValidAcc 0.0126	TestAcc 0.0119	BestValid 0.0741
	Epoch 1950:	Loss 1.0091	TrainAcc 0.0133	ValidAcc 0.0126	TestAcc 0.0119	BestValid 0.0741
	Epoch 2000:	Loss 0.9285	TrainAcc 0.0132	ValidAcc 0.0126	TestAcc 0.0119	BestValid 0.0741
	Epoch 2050:	Loss 0.9011	TrainAcc 0.0132	ValidAcc 0.0126	TestAcc 0.0119	BestValid 0.0741
	Epoch 2100:	Loss 0.9289	TrainAcc 0.0293	ValidAcc 0.0243	TestAcc 0.0233	BestValid 0.0741
	Epoch 2150:	Loss 0.8668	TrainAcc 0.0335	ValidAcc 0.0289	TestAcc 0.0278	BestValid 0.0741
	Epoch 2200:	Loss 0.8441	TrainAcc 0.0337	ValidAcc 0.0291	TestAcc 0.0281	BestValid 0.0741
	Epoch 2250:	Loss 0.8537	TrainAcc 0.0343	ValidAcc 0.0300	TestAcc 0.0287	BestValid 0.0741
	Epoch 2300:	Loss 0.7918	TrainAcc 0.0382	ValidAcc 0.0338	TestAcc 0.0317	BestValid 0.0741
	Epoch 2350:	Loss 0.7958	TrainAcc 0.0458	ValidAcc 0.0414	TestAcc 0.0385	BestValid 0.0741
	Epoch 2400:	Loss 0.8138	TrainAcc 0.0503	ValidAcc 0.0459	TestAcc 0.0423	BestValid 0.0741
	Epoch 2450:	Loss 0.8375	TrainAcc 0.0532	ValidAcc 0.0483	TestAcc 0.0452	BestValid 0.0741
	Epoch 2500:	Loss 0.7782	TrainAcc 0.0220	ValidAcc 0.0210	TestAcc 0.0193	BestValid 0.0741
	Epoch 2550:	Loss 0.7387	TrainAcc 0.0534	ValidAcc 0.0489	TestAcc 0.0453	BestValid 0.0741
	Epoch 2600:	Loss 0.7480	TrainAcc 0.0517	ValidAcc 0.0474	TestAcc 0.0437	BestValid 0.0741
	Epoch 2650:	Loss 0.7814	TrainAcc 0.0549	ValidAcc 0.0509	TestAcc 0.0467	BestValid 0.0741
	Epoch 2700:	Loss 0.7109	TrainAcc 0.0542	ValidAcc 0.0504	TestAcc 0.0462	BestValid 0.0741
	Epoch 2750:	Loss 0.6962	TrainAcc 0.0466	ValidAcc 0.0431	TestAcc 0.0392	BestValid 0.0741
	Epoch 2800:	Loss 0.7033	TrainAcc 0.0432	ValidAcc 0.0407	TestAcc 0.0370	BestValid 0.0741
	Epoch 2850:	Loss 0.6872	TrainAcc 0.0215	ValidAcc 0.0215	TestAcc 0.0188	BestValid 0.0741
	Epoch 2900:	Loss 0.6622	TrainAcc 0.0214	ValidAcc 0.0213	TestAcc 0.0187	BestValid 0.0741
	Epoch 2950:	Loss 0.6643	TrainAcc 0.0215	ValidAcc 0.0216	TestAcc 0.0188	BestValid 0.0741
	Epoch 3000:	Loss 0.7290	TrainAcc 0.0213	ValidAcc 0.0212	TestAcc 0.0187	BestValid 0.0741
	Epoch 3050:	Loss 0.6681	TrainAcc 0.0215	ValidAcc 0.0217	TestAcc 0.0189	BestValid 0.0741
	Epoch 3100:	Loss 0.6479	TrainAcc 0.0215	ValidAcc 0.0217	TestAcc 0.0188	BestValid 0.0741
	Epoch 3150:	Loss 0.6379	TrainAcc 0.0248	ValidAcc 0.0247	TestAcc 0.0217	BestValid 0.0741
	Epoch 3200:	Loss 0.6272	TrainAcc 0.0213	ValidAcc 0.0216	TestAcc 0.0190	BestValid 0.0741
	Epoch 3250:	Loss 0.5950	TrainAcc 0.0218	ValidAcc 0.0227	TestAcc 0.0195	BestValid 0.0741
	Epoch 3300:	Loss 0.6406	TrainAcc 0.0225	ValidAcc 0.0227	TestAcc 0.0196	BestValid 0.0741
	Epoch 3350:	Loss 0.5926	TrainAcc 0.0238	ValidAcc 0.0231	TestAcc 0.0206	BestValid 0.0741
	Epoch 3400:	Loss 0.9058	TrainAcc 0.0306	ValidAcc 0.0298	TestAcc 0.0273	BestValid 0.0741
	Epoch 3450:	Loss 0.6605	TrainAcc 0.0222	ValidAcc 0.0212	TestAcc 0.0193	BestValid 0.0741
	Epoch 3500:	Loss 0.6080	TrainAcc 0.0232	ValidAcc 0.0246	TestAcc 0.0215	BestValid 0.0741
	Epoch 3550:	Loss 0.5761	TrainAcc 0.0214	ValidAcc 0.0206	TestAcc 0.0188	BestValid 0.0741
	Epoch 3600:	Loss 0.5742	TrainAcc 0.0220	ValidAcc 0.0224	TestAcc 0.0198	BestValid 0.0741
	Epoch 3650:	Loss 0.5629	TrainAcc 0.0426	ValidAcc 0.0390	TestAcc 0.0357	BestValid 0.0741
	Epoch 3700:	Loss 0.5867	TrainAcc 0.0444	ValidAcc 0.0412	TestAcc 0.0373	BestValid 0.0741
	Epoch 3750:	Loss 0.5588	TrainAcc 0.0279	ValidAcc 0.0252	TestAcc 0.0235	BestValid 0.0741
	Epoch 3800:	Loss 0.5451	TrainAcc 0.0316	ValidAcc 0.0293	TestAcc 0.0265	BestValid 0.0741
	Epoch 3850:	Loss 0.5553	TrainAcc 0.0232	ValidAcc 0.0209	TestAcc 0.0192	BestValid 0.0741
	Epoch 3900:	Loss 0.5577	TrainAcc 0.0248	ValidAcc 0.0219	TestAcc 0.0207	BestValid 0.0741
	Epoch 3950:	Loss 0.5870	TrainAcc 0.0221	ValidAcc 0.0196	TestAcc 0.0185	BestValid 0.0741
	Epoch 4000:	Loss 0.5381	TrainAcc 0.0228	ValidAcc 0.0209	TestAcc 0.0193	BestValid 0.0741
	Epoch 4050:	Loss 0.6902	TrainAcc 0.0229	ValidAcc 0.0205	TestAcc 0.0189	BestValid 0.0741
	Epoch 4100:	Loss 0.5382	TrainAcc 0.0223	ValidAcc 0.0198	TestAcc 0.0185	BestValid 0.0741
	Epoch 4150:	Loss 0.5183	TrainAcc 0.0222	ValidAcc 0.0198	TestAcc 0.0184	BestValid 0.0741
	Epoch 4200:	Loss 0.5314	TrainAcc 0.0230	ValidAcc 0.0203	TestAcc 0.0188	BestValid 0.0741
	Epoch 4250:	Loss 0.5266	TrainAcc 0.0225	ValidAcc 0.0197	TestAcc 0.0185	BestValid 0.0741
	Epoch 4300:	Loss 0.5206	TrainAcc 0.0225	ValidAcc 0.0198	TestAcc 0.0185	BestValid 0.0741
	Epoch 4350:	Loss 0.5180	TrainAcc 0.0222	ValidAcc 0.0198	TestAcc 0.0185	BestValid 0.0741
	Epoch 4400:	Loss 0.5169	TrainAcc 0.0222	ValidAcc 0.0196	TestAcc 0.0185	BestValid 0.0741
	Epoch 4450:	Loss 0.5054	TrainAcc 0.0223	ValidAcc 0.0198	TestAcc 0.0186	BestValid 0.0741
	Epoch 4500:	Loss 0.4970	TrainAcc 0.0223	ValidAcc 0.0197	TestAcc 0.0184	BestValid 0.0741
	Epoch 4550:	Loss 0.4963	TrainAcc 0.0223	ValidAcc 0.0199	TestAcc 0.0184	BestValid 0.0741
	Epoch 4600:	Loss 0.5116	TrainAcc 0.0223	ValidAcc 0.0197	TestAcc 0.0185	BestValid 0.0741
	Epoch 4650:	Loss 0.5308	TrainAcc 0.0222	ValidAcc 0.0193	TestAcc 0.0183	BestValid 0.0741
	Epoch 4700:	Loss 0.4886	TrainAcc 0.0224	ValidAcc 0.0196	TestAcc 0.0186	BestValid 0.0741
	Epoch 4750:	Loss 0.5194	TrainAcc 0.0221	ValidAcc 0.0193	TestAcc 0.0181	BestValid 0.0741
	Epoch 4800:	Loss 0.5621	TrainAcc 0.0229	ValidAcc 0.0199	TestAcc 0.0190	BestValid 0.0741
	Epoch 4850:	Loss 0.4911	TrainAcc 0.0230	ValidAcc 0.0200	TestAcc 0.0190	BestValid 0.0741
	Epoch 4900:	Loss 0.5103	TrainAcc 0.0227	ValidAcc 0.0199	TestAcc 0.0188	BestValid 0.0741
	Epoch 4950:	Loss 0.7660	TrainAcc 0.0372	ValidAcc 0.0342	TestAcc 0.0313	BestValid 0.0741
	Epoch 5000:	Loss 0.4860	TrainAcc 0.0220	ValidAcc 0.0191	TestAcc 0.0181	BestValid 0.0741
****** Epoch Time (Excluding Evaluation Cost): 1.084 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 1.020 ms (Max: 1.428, Min: 0.063, Sum: 8.159)
Cluster-Wide Average, Compute: 348.260 ms (Max: 565.254, Min: 181.099, Sum: 2786.082)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.010, Min: 0.008, Sum: 0.069)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.018, Min: 0.016, Sum: 0.138)
Cluster-Wide Average, Communication-Graph: 714.388 ms (Max: 881.185, Min: 498.214, Sum: 5715.101)
Cluster-Wide Average, Optimization: 5.859 ms (Max: 5.931, Min: 5.815, Sum: 46.868)
Cluster-Wide Average, Others: 14.039 ms (Max: 14.104, Min: 14.000, Sum: 112.312)
****** Breakdown Sum: 1083.591 ms ******
Cluster-Wide Average, GPU Memory Consumption: 16.266 GB (Max: 16.909, Min: 16.157, Sum: 130.132)
Cluster-Wide Average, Graph-Level Communication Throughput: 24.546 Gbps (Max: 49.616, Min: 9.683, Sum: 196.369)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.038 GB
Total communication (cluster-wide, per-epoch): 14.520 GB
****** Accuracy Results ******
Highest valid_acc: 0.0741
Target test_acc: 0.0730
Epoch to reach the target acc: 249
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
