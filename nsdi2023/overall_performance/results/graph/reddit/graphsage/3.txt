Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.874 seconds.
Building the CSC structure...
        It takes 2.024 seconds.
Building the CSC structure...
        It takes 2.036 seconds.
Building the CSC structure...
        It takes 2.043 seconds.
Building the CSC structure...
        It takes 2.314 seconds.
Building the CSC structure...
        It takes 2.366 seconds.
Building the CSC structure...
        It takes 2.389 seconds.
Building the CSC structure...
        It takes 2.627 seconds.
Building the CSC structure...
        It takes 1.789 seconds.
        It takes 1.857 seconds.
        It takes 1.855 seconds.
        It takes 2.051 seconds.
        It takes 2.295 seconds.
Building the Feature Vector...
        It takes 2.319 seconds.
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 2.309 seconds.
        It takes 0.037 seconds.
        It takes 2.737 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.298 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.030 seconds.
        It takes 0.299 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.039 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.256 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.044 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 54.916 Gbps (per GPU), 439.331 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.674 Gbps (per GPU), 437.395 Gbps (aggregated)
The layer-level communication performance: 54.670 Gbps (per GPU), 437.357 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.476 Gbps (per GPU), 435.807 Gbps (aggregated)
The layer-level communication performance: 54.445 Gbps (per GPU), 435.559 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.268 Gbps (per GPU), 434.143 Gbps (aggregated)
The layer-level communication performance: 54.241 Gbps (per GPU), 433.930 Gbps (aggregated)
The layer-level communication performance: 54.202 Gbps (per GPU), 433.617 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.741 Gbps (per GPU), 1253.928 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.755 Gbps (per GPU), 1254.043 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.744 Gbps (per GPU), 1253.949 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.756 Gbps (per GPU), 1254.045 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.744 Gbps (per GPU), 1253.949 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.758 Gbps (per GPU), 1254.067 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.738 Gbps (per GPU), 1253.903 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.668 Gbps (per GPU), 1253.340 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.596 Gbps (per GPU), 812.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.597 Gbps (per GPU), 812.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.628 Gbps (per GPU), 813.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.598 Gbps (per GPU), 812.784 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.595 Gbps (per GPU), 812.758 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.598 Gbps (per GPU), 812.784 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.628 Gbps (per GPU), 813.020 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.599 Gbps (per GPU), 812.791 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.048 Gbps (per GPU), 272.386 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.048 Gbps (per GPU), 272.385 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.047 Gbps (per GPU), 272.378 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.048 Gbps (per GPU), 272.381 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.048 Gbps (per GPU), 272.386 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.048 Gbps (per GPU), 272.381 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.048 Gbps (per GPU), 272.385 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.047 Gbps (per GPU), 272.374 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.13ms  9.51ms  8.95ms  1.35 29.12K 14.23M
 chk_1  7.72ms  5.19ms  4.59ms  1.68 29.12K  6.56M
 chk_2 19.17ms 16.80ms 16.17ms  1.19 29.12K 24.68M
 chk_3 19.20ms 16.73ms 16.19ms  1.19 29.12K 22.95M
 chk_4  7.47ms  5.02ms  4.43ms  1.69 29.12K  6.33M
 chk_5 11.50ms  8.93ms  8.42ms  1.37 29.12K 12.05M
 chk_6 12.75ms 10.09ms  9.62ms  1.32 29.12K 14.60M
 chk_7 11.89ms  9.32ms  8.66ms  1.37 29.12K 13.21M
   Avg 12.73 10.20  9.63
   Max 19.20 16.80 16.19
   Min  7.47  5.02  4.43
 Ratio  2.57  3.35  3.66
   Var 17.32 17.62 17.65
Profiling takes 2.972 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 346.606 ms
Partition 0 [0, 4) has cost: 346.606 ms
Partition 1 [4, 8) has cost: 326.394 ms
Partition 2 [8, 12) has cost: 326.394 ms
Partition 3 [12, 16) has cost: 326.394 ms
Partition 4 [16, 20) has cost: 326.394 ms
Partition 5 [20, 24) has cost: 326.394 ms
Partition 6 [24, 28) has cost: 326.394 ms
Partition 7 [28, 32) has cost: 321.831 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 302.099 ms
GPU 0, Compute+Comm Time: 125.330 ms, Bubble Time: 129.508 ms, Imbalance Overhead: 47.260 ms
GPU 1, Compute+Comm Time: 119.869 ms, Bubble Time: 120.317 ms, Imbalance Overhead: 61.913 ms
GPU 2, Compute+Comm Time: 119.869 ms, Bubble Time: 110.418 ms, Imbalance Overhead: 71.812 ms
GPU 3, Compute+Comm Time: 119.869 ms, Bubble Time: 111.106 ms, Imbalance Overhead: 71.124 ms
GPU 4, Compute+Comm Time: 119.869 ms, Bubble Time: 120.605 ms, Imbalance Overhead: 61.625 ms
GPU 5, Compute+Comm Time: 119.869 ms, Bubble Time: 129.479 ms, Imbalance Overhead: 52.751 ms
GPU 6, Compute+Comm Time: 119.869 ms, Bubble Time: 138.483 ms, Imbalance Overhead: 43.747 ms
GPU 7, Compute+Comm Time: 118.663 ms, Bubble Time: 148.777 ms, Imbalance Overhead: 34.659 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 593.564 ms
GPU 0, Compute+Comm Time: 230.318 ms, Bubble Time: 294.572 ms, Imbalance Overhead: 68.674 ms
GPU 1, Compute+Comm Time: 233.675 ms, Bubble Time: 274.188 ms, Imbalance Overhead: 85.701 ms
GPU 2, Compute+Comm Time: 233.675 ms, Bubble Time: 255.781 ms, Imbalance Overhead: 104.108 ms
GPU 3, Compute+Comm Time: 233.675 ms, Bubble Time: 237.710 ms, Imbalance Overhead: 122.179 ms
GPU 4, Compute+Comm Time: 233.675 ms, Bubble Time: 218.181 ms, Imbalance Overhead: 141.708 ms
GPU 5, Compute+Comm Time: 233.675 ms, Bubble Time: 215.693 ms, Imbalance Overhead: 144.196 ms
GPU 6, Compute+Comm Time: 233.675 ms, Bubble Time: 234.660 ms, Imbalance Overhead: 125.229 ms
GPU 7, Compute+Comm Time: 248.425 ms, Bubble Time: 252.106 ms, Imbalance Overhead: 93.033 ms
The estimated cost of the whole pipeline: 940.446 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 673.000 ms
Partition 0 [0, 8) has cost: 673.000 ms
Partition 1 [8, 16) has cost: 652.788 ms
Partition 2 [16, 24) has cost: 652.788 ms
Partition 3 [24, 32) has cost: 648.225 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 312.069 ms
GPU 0, Compute+Comm Time: 161.600 ms, Bubble Time: 149.874 ms, Imbalance Overhead: 0.595 ms
GPU 1, Compute+Comm Time: 158.915 ms, Bubble Time: 130.784 ms, Imbalance Overhead: 22.370 ms
GPU 2, Compute+Comm Time: 158.915 ms, Bubble Time: 110.986 ms, Imbalance Overhead: 42.168 ms
GPU 3, Compute+Comm Time: 158.344 ms, Bubble Time: 111.565 ms, Imbalance Overhead: 42.160 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 598.714 ms
GPU 0, Compute+Comm Time: 301.393 ms, Bubble Time: 214.665 ms, Imbalance Overhead: 82.656 ms
GPU 1, Compute+Comm Time: 303.068 ms, Bubble Time: 211.795 ms, Imbalance Overhead: 83.852 ms
GPU 2, Compute+Comm Time: 303.068 ms, Bubble Time: 250.030 ms, Imbalance Overhead: 45.617 ms
GPU 3, Compute+Comm Time: 310.392 ms, Bubble Time: 286.825 ms, Imbalance Overhead: 1.497 ms
    The estimated cost with 2 DP ways is 956.322 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1325.787 ms
Partition 0 [0, 16) has cost: 1325.787 ms
Partition 1 [16, 32) has cost: 1301.012 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 352.506 ms
GPU 0, Compute+Comm Time: 235.380 ms, Bubble Time: 117.126 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 233.790 ms, Bubble Time: 117.487 ms, Imbalance Overhead: 1.229 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 628.767 ms
GPU 0, Compute+Comm Time: 416.725 ms, Bubble Time: 211.267 ms, Imbalance Overhead: 0.775 ms
GPU 1, Compute+Comm Time: 421.134 ms, Bubble Time: 207.633 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1030.336 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2626.799 ms
Partition 0 [0, 32) has cost: 2626.799 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 483.960 ms
GPU 0, Compute+Comm Time: 483.960 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 669.329 ms
GPU 0, Compute+Comm Time: 669.329 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1210.953 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 3.2291	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.0099	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 2.9698	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 2.9250	TrainAcc 0.0843	ValidAcc 0.0695	TestAcc 0.0681	BestValid 0.0695
	Epoch 250:	Loss 2.8819	TrainAcc 0.0888	ValidAcc 0.0741	TestAcc 0.0731	BestValid 0.0741
	Epoch 300:	Loss 2.8048	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.0741
	Epoch 350:	Loss 2.8205	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.0741
	Epoch 400:	Loss 2.6536	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0451	BestValid 0.0741
	Epoch 450:	Loss 2.3164	TrainAcc 0.0501	ValidAcc 0.0417	TestAcc 0.0452	BestValid 0.0741
	Epoch 500:	Loss 2.1053	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0741
	Epoch 550:	Loss 2.0427	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0741
	Epoch 600:	Loss 1.8549	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0741
	Epoch 650:	Loss 1.7343	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0451	BestValid 0.0741
	Epoch 700:	Loss 1.6075	TrainAcc 0.0172	ValidAcc 0.0149	TestAcc 0.0149	BestValid 0.0741
	Epoch 750:	Loss 1.5546	TrainAcc 0.0140	ValidAcc 0.0129	TestAcc 0.0124	BestValid 0.0741
	Epoch 800:	Loss 1.4595	TrainAcc 0.0143	ValidAcc 0.0130	TestAcc 0.0126	BestValid 0.0741
	Epoch 850:	Loss 1.4366	TrainAcc 0.0135	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.0741
	Epoch 900:	Loss 2.3976	TrainAcc 0.0136	ValidAcc 0.0126	TestAcc 0.0120	BestValid 0.0741
	Epoch 950:	Loss 1.3831	TrainAcc 0.0135	ValidAcc 0.0127	TestAcc 0.0120	BestValid 0.0741
	Epoch 1000:	Loss 1.5302	TrainAcc 0.0134	ValidAcc 0.0126	TestAcc 0.0120	BestValid 0.0741
	Epoch 1050:	Loss 1.4289	TrainAcc 0.0134	ValidAcc 0.0125	TestAcc 0.0120	BestValid 0.0741
	Epoch 1100:	Loss 1.2381	TrainAcc 0.0134	ValidAcc 0.0125	TestAcc 0.0120	BestValid 0.0741
	Epoch 1150:	Loss 1.3476	TrainAcc 0.0134	ValidAcc 0.0126	TestAcc 0.0121	BestValid 0.0741
	Epoch 1200:	Loss 1.2986	TrainAcc 0.0134	ValidAcc 0.0125	TestAcc 0.0122	BestValid 0.0741
	Epoch 1250:	Loss 1.1776	TrainAcc 0.0133	ValidAcc 0.0125	TestAcc 0.0121	BestValid 0.0741
	Epoch 1300:	Loss 1.1774	TrainAcc 0.0134	ValidAcc 0.0125	TestAcc 0.0122	BestValid 0.0741
	Epoch 1350:	Loss 1.1285	TrainAcc 0.0135	ValidAcc 0.0126	TestAcc 0.0122	BestValid 0.0741
	Epoch 1400:	Loss 1.2590	TrainAcc 0.0135	ValidAcc 0.0126	TestAcc 0.0122	BestValid 0.0741
	Epoch 1450:	Loss 1.1500	TrainAcc 0.0136	ValidAcc 0.0127	TestAcc 0.0123	BestValid 0.0741
	Epoch 1500:	Loss 1.0483	TrainAcc 0.0135	ValidAcc 0.0127	TestAcc 0.0123	BestValid 0.0741
	Epoch 1550:	Loss 1.0916	TrainAcc 0.0135	ValidAcc 0.0127	TestAcc 0.0123	BestValid 0.0741
	Epoch 1600:	Loss 1.0295	TrainAcc 0.0137	ValidAcc 0.0130	TestAcc 0.0124	BestValid 0.0741
	Epoch 1650:	Loss 0.9701	TrainAcc 0.0136	ValidAcc 0.0130	TestAcc 0.0124	BestValid 0.0741
	Epoch 1700:	Loss 0.9951	TrainAcc 0.0137	ValidAcc 0.0129	TestAcc 0.0124	BestValid 0.0741
	Epoch 1750:	Loss 0.9196	TrainAcc 0.0137	ValidAcc 0.0129	TestAcc 0.0125	BestValid 0.0741
	Epoch 1800:	Loss 1.1527	TrainAcc 0.0136	ValidAcc 0.0130	TestAcc 0.0125	BestValid 0.0741
	Epoch 1850:	Loss 0.8998	TrainAcc 0.0139	ValidAcc 0.0131	TestAcc 0.0126	BestValid 0.0741
	Epoch 1900:	Loss 0.9592	TrainAcc 0.0176	ValidAcc 0.0162	TestAcc 0.0151	BestValid 0.0741
	Epoch 1950:	Loss 0.9089	TrainAcc 0.0186	ValidAcc 0.0157	TestAcc 0.0147	BestValid 0.0741
	Epoch 2000:	Loss 0.9227	TrainAcc 0.0202	ValidAcc 0.0176	TestAcc 0.0162	BestValid 0.0741
	Epoch 2050:	Loss 0.8529	TrainAcc 0.0252	ValidAcc 0.0224	TestAcc 0.0213	BestValid 0.0741
	Epoch 2100:	Loss 0.8117	TrainAcc 0.0286	ValidAcc 0.0266	TestAcc 0.0250	BestValid 0.0741
	Epoch 2150:	Loss 0.8381	TrainAcc 0.0190	ValidAcc 0.0183	TestAcc 0.0170	BestValid 0.0741
	Epoch 2200:	Loss 0.7907	TrainAcc 0.0194	ValidAcc 0.0184	TestAcc 0.0173	BestValid 0.0741
	Epoch 2250:	Loss 0.7835	TrainAcc 0.0200	ValidAcc 0.0191	TestAcc 0.0179	BestValid 0.0741
	Epoch 2300:	Loss 0.7340	TrainAcc 0.0200	ValidAcc 0.0190	TestAcc 0.0179	BestValid 0.0741
	Epoch 2350:	Loss 0.7695	TrainAcc 0.0196	ValidAcc 0.0186	TestAcc 0.0175	BestValid 0.0741
	Epoch 2400:	Loss 0.7475	TrainAcc 0.0192	ValidAcc 0.0183	TestAcc 0.0171	BestValid 0.0741
	Epoch 2450:	Loss 0.7659	TrainAcc 0.0310	ValidAcc 0.0290	TestAcc 0.0275	BestValid 0.0741
	Epoch 2500:	Loss 0.7094	TrainAcc 0.0251	ValidAcc 0.0231	TestAcc 0.0221	BestValid 0.0741
	Epoch 2550:	Loss 0.7114	TrainAcc 0.0310	ValidAcc 0.0292	TestAcc 0.0275	BestValid 0.0741
	Epoch 2600:	Loss 0.7077	TrainAcc 0.0310	ValidAcc 0.0291	TestAcc 0.0275	BestValid 0.0741
	Epoch 2650:	Loss 0.7411	TrainAcc 0.0257	ValidAcc 0.0235	TestAcc 0.0224	BestValid 0.0741
	Epoch 2700:	Loss 0.9469	TrainAcc 0.0204	ValidAcc 0.0196	TestAcc 0.0181	BestValid 0.0741
	Epoch 2750:	Loss 0.7454	TrainAcc 0.0196	ValidAcc 0.0186	TestAcc 0.0174	BestValid 0.0741
	Epoch 2800:	Loss 0.6690	TrainAcc 0.0263	ValidAcc 0.0240	TestAcc 0.0231	BestValid 0.0741
	Epoch 2850:	Loss 0.6732	TrainAcc 0.0310	ValidAcc 0.0292	TestAcc 0.0276	BestValid 0.0741
	Epoch 2900:	Loss 0.7328	TrainAcc 0.0310	ValidAcc 0.0290	TestAcc 0.0275	BestValid 0.0741
	Epoch 2950:	Loss 0.6527	TrainAcc 0.0254	ValidAcc 0.0235	TestAcc 0.0221	BestValid 0.0741
	Epoch 3000:	Loss 0.6333	TrainAcc 0.0308	ValidAcc 0.0291	TestAcc 0.0275	BestValid 0.0741
	Epoch 3050:	Loss 0.7052	TrainAcc 0.0192	ValidAcc 0.0183	TestAcc 0.0172	BestValid 0.0741
	Epoch 3100:	Loss 0.6208	TrainAcc 0.0210	ValidAcc 0.0200	TestAcc 0.0183	BestValid 0.0741
	Epoch 3150:	Loss 0.6253	TrainAcc 0.0298	ValidAcc 0.0279	TestAcc 0.0267	BestValid 0.0741
	Epoch 3200:	Loss 0.7101	TrainAcc 0.0304	ValidAcc 0.0286	TestAcc 0.0272	BestValid 0.0741
	Epoch 3250:	Loss 0.6301	TrainAcc 0.0229	ValidAcc 0.0214	TestAcc 0.0199	BestValid 0.0741
	Epoch 3300:	Loss 0.5990	TrainAcc 0.0305	ValidAcc 0.0289	TestAcc 0.0273	BestValid 0.0741
	Epoch 3350:	Loss 0.5981	TrainAcc 0.0306	ValidAcc 0.0287	TestAcc 0.0271	BestValid 0.0741
	Epoch 3400:	Loss 0.6542	TrainAcc 0.0229	ValidAcc 0.0213	TestAcc 0.0200	BestValid 0.0741
	Epoch 3450:	Loss 0.6277	TrainAcc 0.0233	ValidAcc 0.0216	TestAcc 0.0203	BestValid 0.0741
	Epoch 3500:	Loss 0.5953	TrainAcc 0.0274	ValidAcc 0.0253	TestAcc 0.0242	BestValid 0.0741
	Epoch 3550:	Loss 0.5967	TrainAcc 0.0202	ValidAcc 0.0195	TestAcc 0.0179	BestValid 0.0741
	Epoch 3600:	Loss 0.6027	TrainAcc 0.0233	ValidAcc 0.0216	TestAcc 0.0203	BestValid 0.0741
	Epoch 3650:	Loss 0.5778	TrainAcc 0.0187	ValidAcc 0.0174	TestAcc 0.0166	BestValid 0.0741
	Epoch 3700:	Loss 0.6065	TrainAcc 0.0185	ValidAcc 0.0172	TestAcc 0.0162	BestValid 0.0741
	Epoch 3750:	Loss 0.5676	TrainAcc 0.0186	ValidAcc 0.0173	TestAcc 0.0165	BestValid 0.0741
	Epoch 3800:	Loss 0.5593	TrainAcc 0.0186	ValidAcc 0.0173	TestAcc 0.0165	BestValid 0.0741
	Epoch 3850:	Loss 0.5902	TrainAcc 0.0184	ValidAcc 0.0171	TestAcc 0.0161	BestValid 0.0741
	Epoch 3900:	Loss 0.5628	TrainAcc 0.0184	ValidAcc 0.0171	TestAcc 0.0161	BestValid 0.0741
	Epoch 3950:	Loss 0.6110	TrainAcc 0.0182	ValidAcc 0.0169	TestAcc 0.0160	BestValid 0.0741
	Epoch 4000:	Loss 0.6151	TrainAcc 0.0309	ValidAcc 0.0291	TestAcc 0.0275	BestValid 0.0741
	Epoch 4050:	Loss 0.5475	TrainAcc 0.0184	ValidAcc 0.0171	TestAcc 0.0161	BestValid 0.0741
	Epoch 4100:	Loss 0.5637	TrainAcc 0.0182	ValidAcc 0.0169	TestAcc 0.0159	BestValid 0.0741
	Epoch 4150:	Loss 0.5302	TrainAcc 0.0183	ValidAcc 0.0169	TestAcc 0.0160	BestValid 0.0741
	Epoch 4200:	Loss 0.5556	TrainAcc 0.0183	ValidAcc 0.0170	TestAcc 0.0159	BestValid 0.0741
	Epoch 4250:	Loss 0.5374	TrainAcc 0.0183	ValidAcc 0.0170	TestAcc 0.0160	BestValid 0.0741
	Epoch 4300:	Loss 0.5308	TrainAcc 0.0183	ValidAcc 0.0170	TestAcc 0.0160	BestValid 0.0741
	Epoch 4350:	Loss 0.5176	TrainAcc 0.0183	ValidAcc 0.0169	TestAcc 0.0160	BestValid 0.0741
	Epoch 4400:	Loss 0.5197	TrainAcc 0.0183	ValidAcc 0.0171	TestAcc 0.0160	BestValid 0.0741
	Epoch 4450:	Loss 0.5149	TrainAcc 0.0183	ValidAcc 0.0170	TestAcc 0.0160	BestValid 0.0741
	Epoch 4500:	Loss 0.5113	TrainAcc 0.0184	ValidAcc 0.0170	TestAcc 0.0160	BestValid 0.0741
	Epoch 4550:	Loss 0.5111	TrainAcc 0.0184	ValidAcc 0.0170	TestAcc 0.0161	BestValid 0.0741
	Epoch 4600:	Loss 0.5121	TrainAcc 0.0184	ValidAcc 0.0171	TestAcc 0.0161	BestValid 0.0741
	Epoch 4650:	Loss 0.4971	TrainAcc 0.0185	ValidAcc 0.0171	TestAcc 0.0162	BestValid 0.0741
	Epoch 4700:	Loss 0.5169	TrainAcc 0.0185	ValidAcc 0.0171	TestAcc 0.0162	BestValid 0.0741
	Epoch 4750:	Loss 0.4984	TrainAcc 0.0185	ValidAcc 0.0171	TestAcc 0.0162	BestValid 0.0741
	Epoch 4800:	Loss 0.5177	TrainAcc 0.0185	ValidAcc 0.0172	TestAcc 0.0163	BestValid 0.0741
	Epoch 4850:	Loss 0.5105	TrainAcc 0.0185	ValidAcc 0.0172	TestAcc 0.0162	BestValid 0.0741
	Epoch 4900:	Loss 0.4849	TrainAcc 0.0186	ValidAcc 0.0172	TestAcc 0.0163	BestValid 0.0741
	Epoch 4950:	Loss 0.5148	TrainAcc 0.0186	ValidAcc 0.0172	TestAcc 0.0162	BestValid 0.0741
	Epoch 5000:	Loss 0.6760	TrainAcc 0.0186	ValidAcc 0.0171	TestAcc 0.0163	BestValid 0.0741
****** Epoch Time (Excluding Evaluation Cost): 1.081 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.915 ms (Max: 1.336, Min: 0.081, Sum: 7.324)
Cluster-Wide Average, Compute: 346.946 ms (Max: 562.101, Min: 179.931, Sum: 2775.567)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.010, Min: 0.008, Sum: 0.069)
Cluster-Wide Average, Bubble-Imbalance: 0.018 ms (Max: 0.020, Min: 0.016, Sum: 0.142)
Cluster-Wide Average, Communication-Graph: 713.292 ms (Max: 879.974, Min: 498.901, Sum: 5706.333)
Cluster-Wide Average, Optimization: 5.861 ms (Max: 5.891, Min: 5.825, Sum: 46.886)
Cluster-Wide Average, Others: 14.037 ms (Max: 14.071, Min: 13.990, Sum: 112.299)
****** Breakdown Sum: 1081.078 ms ******
Cluster-Wide Average, GPU Memory Consumption: 16.266 GB (Max: 16.909, Min: 16.157, Sum: 130.132)
Cluster-Wide Average, Graph-Level Communication Throughput: 24.568 Gbps (Max: 49.517, Min: 9.695, Sum: 196.543)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.038 GB
Total communication (cluster-wide, per-epoch): 14.520 GB
****** Accuracy Results ******
Highest valid_acc: 0.0741
Target test_acc: 0.0731
Epoch to reach the target acc: 249
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
