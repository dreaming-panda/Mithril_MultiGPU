Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.350 seconds.
Building the CSC structure...
        It takes 2.389 seconds.
Building the CSC structure...
        It takes 2.404 seconds.
Building the CSC structure...
        It takes 2.439 seconds.
Building the CSC structure...
        It takes 2.578 seconds.
Building the CSC structure...
        It takes 2.610 seconds.
Building the CSC structure...
        It takes 2.614 seconds.
Building the CSC structure...
        It takes 2.639 seconds.
Building the CSC structure...
        It takes 2.294 seconds.
        It takes 2.315 seconds.
        It takes 2.335 seconds.
        It takes 2.309 seconds.
        It takes 2.285 seconds.
        It takes 2.276 seconds.
        It takes 2.291 seconds.
        It takes 2.455 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.275 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.039 seconds.
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.312 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 0.039 seconds.
        It takes 0.308 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.278 seconds.
Building the Label Vector...
        It takes 0.256 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.031 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.598 Gbps (per GPU), 468.786 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.322 Gbps (per GPU), 466.574 Gbps (aggregated)
The layer-level communication performance: 58.321 Gbps (per GPU), 466.567 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.071 Gbps (per GPU), 464.572 Gbps (aggregated)
The layer-level communication performance: 58.043 Gbps (per GPU), 464.347 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.838 Gbps (per GPU), 462.702 Gbps (aggregated)
The layer-level communication performance: 57.790 Gbps (per GPU), 462.322 Gbps (aggregated)
The layer-level communication performance: 57.762 Gbps (per GPU), 462.094 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.166 Gbps (per GPU), 1249.326 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.155 Gbps (per GPU), 1249.237 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.163 Gbps (per GPU), 1249.304 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.157 Gbps (per GPU), 1249.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.169 Gbps (per GPU), 1249.356 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.154 Gbps (per GPU), 1249.234 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.169 Gbps (per GPU), 1249.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.155 Gbps (per GPU), 1249.236 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 98.957 Gbps (per GPU), 791.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.956 Gbps (per GPU), 791.652 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.956 Gbps (per GPU), 791.646 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.958 Gbps (per GPU), 791.664 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.957 Gbps (per GPU), 791.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.959 Gbps (per GPU), 791.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.956 Gbps (per GPU), 791.652 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.958 Gbps (per GPU), 791.664 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.640 Gbps (per GPU), 317.117 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.638 Gbps (per GPU), 317.103 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.637 Gbps (per GPU), 317.099 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.639 Gbps (per GPU), 317.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.640 Gbps (per GPU), 317.117 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.639 Gbps (per GPU), 317.114 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.638 Gbps (per GPU), 317.103 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.638 Gbps (per GPU), 317.108 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.04ms  9.51ms  8.97ms  1.34 29.12K 14.23M
 chk_1  7.68ms  5.15ms  4.60ms  1.67 29.12K  6.56M
 chk_2 19.14ms 16.54ms 16.11ms  1.19 29.12K 24.68M
 chk_3 19.22ms 16.91ms 16.23ms  1.18 29.12K 22.95M
 chk_4  7.55ms  4.98ms  4.44ms  1.70 29.12K  6.33M
 chk_5 11.51ms  8.95ms  8.38ms  1.37 29.12K 12.05M
 chk_6 12.66ms 10.04ms  9.54ms  1.33 29.12K 14.60M
 chk_7 11.79ms  9.37ms  8.59ms  1.37 29.12K 13.21M
   Avg 12.70 10.18  9.61
   Max 19.22 16.91 16.23
   Min  7.55  4.98  4.44
 Ratio  2.55  3.39  3.66
   Var 17.30 17.58 17.62
Profiling takes 2.973 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 345.927 ms
Partition 0 [0, 4) has cost: 345.927 ms
Partition 1 [4, 8) has cost: 325.789 ms
Partition 2 [8, 12) has cost: 325.789 ms
Partition 3 [12, 16) has cost: 325.789 ms
Partition 4 [16, 20) has cost: 325.789 ms
Partition 5 [20, 24) has cost: 325.789 ms
Partition 6 [24, 28) has cost: 325.789 ms
Partition 7 [28, 32) has cost: 321.191 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 300.039 ms
GPU 0, Compute+Comm Time: 124.070 ms, Bubble Time: 128.428 ms, Imbalance Overhead: 47.541 ms
GPU 1, Compute+Comm Time: 118.601 ms, Bubble Time: 119.190 ms, Imbalance Overhead: 62.248 ms
GPU 2, Compute+Comm Time: 118.601 ms, Bubble Time: 109.308 ms, Imbalance Overhead: 72.130 ms
GPU 3, Compute+Comm Time: 118.601 ms, Bubble Time: 109.871 ms, Imbalance Overhead: 71.567 ms
GPU 4, Compute+Comm Time: 118.601 ms, Bubble Time: 119.488 ms, Imbalance Overhead: 61.950 ms
GPU 5, Compute+Comm Time: 118.601 ms, Bubble Time: 128.473 ms, Imbalance Overhead: 52.965 ms
GPU 6, Compute+Comm Time: 118.601 ms, Bubble Time: 137.583 ms, Imbalance Overhead: 43.856 ms
GPU 7, Compute+Comm Time: 117.471 ms, Bubble Time: 147.900 ms, Imbalance Overhead: 34.667 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 594.358 ms
GPU 0, Compute+Comm Time: 229.164 ms, Bubble Time: 294.535 ms, Imbalance Overhead: 70.658 ms
GPU 1, Compute+Comm Time: 232.632 ms, Bubble Time: 273.924 ms, Imbalance Overhead: 87.801 ms
GPU 2, Compute+Comm Time: 232.632 ms, Bubble Time: 255.088 ms, Imbalance Overhead: 106.638 ms
GPU 3, Compute+Comm Time: 232.632 ms, Bubble Time: 236.627 ms, Imbalance Overhead: 125.098 ms
GPU 4, Compute+Comm Time: 232.632 ms, Bubble Time: 216.482 ms, Imbalance Overhead: 145.243 ms
GPU 5, Compute+Comm Time: 232.632 ms, Bubble Time: 215.224 ms, Imbalance Overhead: 146.502 ms
GPU 6, Compute+Comm Time: 232.632 ms, Bubble Time: 234.919 ms, Imbalance Overhead: 126.807 ms
GPU 7, Compute+Comm Time: 247.301 ms, Bubble Time: 252.732 ms, Imbalance Overhead: 94.325 ms
The estimated cost of the whole pipeline: 939.116 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 671.716 ms
Partition 0 [0, 8) has cost: 671.716 ms
Partition 1 [8, 16) has cost: 651.577 ms
Partition 2 [16, 24) has cost: 651.577 ms
Partition 3 [24, 32) has cost: 646.980 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 310.381 ms
GPU 0, Compute+Comm Time: 160.323 ms, Bubble Time: 149.276 ms, Imbalance Overhead: 0.782 ms
GPU 1, Compute+Comm Time: 157.624 ms, Bubble Time: 130.156 ms, Imbalance Overhead: 22.601 ms
GPU 2, Compute+Comm Time: 157.624 ms, Bubble Time: 110.392 ms, Imbalance Overhead: 42.365 ms
GPU 3, Compute+Comm Time: 157.114 ms, Bubble Time: 110.666 ms, Imbalance Overhead: 42.601 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 599.790 ms
GPU 0, Compute+Comm Time: 300.443 ms, Bubble Time: 212.367 ms, Imbalance Overhead: 86.979 ms
GPU 1, Compute+Comm Time: 302.151 ms, Bubble Time: 212.274 ms, Imbalance Overhead: 85.365 ms
GPU 2, Compute+Comm Time: 302.151 ms, Bubble Time: 251.663 ms, Imbalance Overhead: 45.976 ms
GPU 3, Compute+Comm Time: 309.452 ms, Bubble Time: 289.171 ms, Imbalance Overhead: 1.167 ms
    The estimated cost with 2 DP ways is 955.679 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1323.293 ms
Partition 0 [0, 16) has cost: 1323.293 ms
Partition 1 [16, 32) has cost: 1298.557 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 352.359 ms
GPU 0, Compute+Comm Time: 235.064 ms, Bubble Time: 117.294 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 233.500 ms, Bubble Time: 116.990 ms, Imbalance Overhead: 1.869 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 631.272 ms
GPU 0, Compute+Comm Time: 416.385 ms, Bubble Time: 208.195 ms, Imbalance Overhead: 6.691 ms
GPU 1, Compute+Comm Time: 420.841 ms, Bubble Time: 210.431 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1032.812 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2621.850 ms
Partition 0 [0, 32) has cost: 2621.850 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 440.753 ms
GPU 0, Compute+Comm Time: 440.753 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 628.461 ms
GPU 0, Compute+Comm Time: 628.461 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1122.675 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 10.1708	TrainAcc 0.1437	ValidAcc 0.1794	TestAcc 0.1793	BestValid 0.1794
	Epoch 50:	Loss 3.2291	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1794
	Epoch 100:	Loss 3.0095	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1794
	Epoch 150:	Loss 2.9697	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1794
	Epoch 200:	Loss 2.9244	TrainAcc 0.0845	ValidAcc 0.0698	TestAcc 0.0683	BestValid 0.1794
	Epoch 250:	Loss 2.8801	TrainAcc 0.0888	ValidAcc 0.0742	TestAcc 0.0731	BestValid 0.1794
	Epoch 300:	Loss 2.7774	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.1794
	Epoch 350:	Loss 2.9241	TrainAcc 0.0694	ValidAcc 0.0588	TestAcc 0.0577	BestValid 0.1794
	Epoch 400:	Loss 2.6927	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.1794
	Epoch 450:	Loss 2.5355	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0451	BestValid 0.1794
	Epoch 500:	Loss 2.3800	TrainAcc 0.0501	ValidAcc 0.0417	TestAcc 0.0452	BestValid 0.1794
	Epoch 550:	Loss 2.0833	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0451	BestValid 0.1794
	Epoch 600:	Loss 2.0395	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0451	BestValid 0.1794
	Epoch 650:	Loss 1.8176	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1794
	Epoch 700:	Loss 1.6976	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1794
	Epoch 750:	Loss 1.6275	TrainAcc 0.0162	ValidAcc 0.0138	TestAcc 0.0137	BestValid 0.1794
	Epoch 800:	Loss 1.5516	TrainAcc 0.0149	ValidAcc 0.0130	TestAcc 0.0131	BestValid 0.1794
	Epoch 850:	Loss 1.5406	TrainAcc 0.0136	ValidAcc 0.0125	TestAcc 0.0120	BestValid 0.1794
	Epoch 900:	Loss 1.4966	TrainAcc 0.0136	ValidAcc 0.0125	TestAcc 0.0120	BestValid 0.1794
	Epoch 950:	Loss 1.4117	TrainAcc 0.0134	ValidAcc 0.0125	TestAcc 0.0120	BestValid 0.1794
	Epoch 1000:	Loss 1.4576	TrainAcc 0.0134	ValidAcc 0.0125	TestAcc 0.0120	BestValid 0.1794
	Epoch 1050:	Loss 1.6173	TrainAcc 0.0134	ValidAcc 0.0125	TestAcc 0.0120	BestValid 0.1794
	Epoch 1100:	Loss 1.2837	TrainAcc 0.0134	ValidAcc 0.0126	TestAcc 0.0121	BestValid 0.1794
	Epoch 1150:	Loss 1.4158	TrainAcc 0.0135	ValidAcc 0.0126	TestAcc 0.0122	BestValid 0.1794
	Epoch 1200:	Loss 1.3117	TrainAcc 0.0136	ValidAcc 0.0129	TestAcc 0.0123	BestValid 0.1794
	Epoch 1250:	Loss 1.1936	TrainAcc 0.0136	ValidAcc 0.0129	TestAcc 0.0123	BestValid 0.1794
	Epoch 1300:	Loss 1.2020	TrainAcc 0.0137	ValidAcc 0.0130	TestAcc 0.0124	BestValid 0.1794
	Epoch 1350:	Loss 1.2293	TrainAcc 0.0136	ValidAcc 0.0129	TestAcc 0.0124	BestValid 0.1794
	Epoch 1400:	Loss 1.2742	TrainAcc 0.0137	ValidAcc 0.0130	TestAcc 0.0124	BestValid 0.1794
	Epoch 1450:	Loss 1.1708	TrainAcc 0.0139	ValidAcc 0.0131	TestAcc 0.0125	BestValid 0.1794
	Epoch 1500:	Loss 1.1150	TrainAcc 0.0137	ValidAcc 0.0131	TestAcc 0.0125	BestValid 0.1794
	Epoch 1550:	Loss 1.1462	TrainAcc 0.0138	ValidAcc 0.0131	TestAcc 0.0124	BestValid 0.1794
	Epoch 1600:	Loss 1.1448	TrainAcc 0.0139	ValidAcc 0.0133	TestAcc 0.0124	BestValid 0.1794
	Epoch 1650:	Loss 1.0395	TrainAcc 0.0140	ValidAcc 0.0133	TestAcc 0.0125	BestValid 0.1794
	Epoch 1700:	Loss 1.0596	TrainAcc 0.0240	ValidAcc 0.0213	TestAcc 0.0199	BestValid 0.1794
	Epoch 1750:	Loss 1.0068	TrainAcc 0.0179	ValidAcc 0.0164	TestAcc 0.0156	BestValid 0.1794
	Epoch 1800:	Loss 1.0848	TrainAcc 0.0179	ValidAcc 0.0165	TestAcc 0.0156	BestValid 0.1794
	Epoch 1850:	Loss 0.9465	TrainAcc 0.0183	ValidAcc 0.0166	TestAcc 0.0159	BestValid 0.1794
	Epoch 1900:	Loss 0.9889	TrainAcc 0.0186	ValidAcc 0.0170	TestAcc 0.0163	BestValid 0.1794
	Epoch 1950:	Loss 0.9500	TrainAcc 0.0187	ValidAcc 0.0171	TestAcc 0.0165	BestValid 0.1794
	Epoch 2000:	Loss 0.9573	TrainAcc 0.0200	ValidAcc 0.0189	TestAcc 0.0177	BestValid 0.1794
	Epoch 2050:	Loss 0.8699	TrainAcc 0.0309	ValidAcc 0.0287	TestAcc 0.0274	BestValid 0.1794
	Epoch 2100:	Loss 0.8597	TrainAcc 0.0308	ValidAcc 0.0286	TestAcc 0.0273	BestValid 0.1794
	Epoch 2150:	Loss 0.8785	TrainAcc 0.0307	ValidAcc 0.0286	TestAcc 0.0272	BestValid 0.1794
	Epoch 2200:	Loss 0.8725	TrainAcc 0.0308	ValidAcc 0.0287	TestAcc 0.0274	BestValid 0.1794
	Epoch 2250:	Loss 0.8800	TrainAcc 0.0243	ValidAcc 0.0220	TestAcc 0.0211	BestValid 0.1794
	Epoch 2300:	Loss 0.8021	TrainAcc 0.0189	ValidAcc 0.0179	TestAcc 0.0169	BestValid 0.1794
	Epoch 2350:	Loss 0.7858	TrainAcc 0.0286	ValidAcc 0.0263	TestAcc 0.0253	BestValid 0.1794
	Epoch 2400:	Loss 0.7886	TrainAcc 0.0238	ValidAcc 0.0214	TestAcc 0.0208	BestValid 0.1794
	Epoch 2450:	Loss 0.7639	TrainAcc 0.0256	ValidAcc 0.0230	TestAcc 0.0223	BestValid 0.1794
	Epoch 2500:	Loss 0.8007	TrainAcc 0.0136	ValidAcc 0.0128	TestAcc 0.0124	BestValid 0.1794
	Epoch 2550:	Loss 0.7160	TrainAcc 0.0134	ValidAcc 0.0128	TestAcc 0.0122	BestValid 0.1794
	Epoch 2600:	Loss 0.7361	TrainAcc 0.0135	ValidAcc 0.0128	TestAcc 0.0123	BestValid 0.1794
	Epoch 2650:	Loss 0.6945	TrainAcc 0.0133	ValidAcc 0.0127	TestAcc 0.0121	BestValid 0.1794
	Epoch 2700:	Loss 0.7502	TrainAcc 0.0258	ValidAcc 0.0231	TestAcc 0.0220	BestValid 0.1794
	Epoch 2750:	Loss 0.6818	TrainAcc 0.0304	ValidAcc 0.0282	TestAcc 0.0269	BestValid 0.1794
	Epoch 2800:	Loss 0.6737	TrainAcc 0.0309	ValidAcc 0.0289	TestAcc 0.0274	BestValid 0.1794
	Epoch 2850:	Loss 0.6639	TrainAcc 0.0309	ValidAcc 0.0289	TestAcc 0.0274	BestValid 0.1794
	Epoch 2900:	Loss 0.7003	TrainAcc 0.0309	ValidAcc 0.0290	TestAcc 0.0274	BestValid 0.1794
	Epoch 2950:	Loss 0.6682	TrainAcc 0.0185	ValidAcc 0.0170	TestAcc 0.0162	BestValid 0.1794
	Epoch 3000:	Loss 0.6371	TrainAcc 0.0309	ValidAcc 0.0288	TestAcc 0.0275	BestValid 0.1794
	Epoch 3050:	Loss 0.6790	TrainAcc 0.0223	ValidAcc 0.0207	TestAcc 0.0192	BestValid 0.1794
	Epoch 3100:	Loss 0.6153	TrainAcc 0.0189	ValidAcc 0.0178	TestAcc 0.0168	BestValid 0.1794
	Epoch 3150:	Loss 0.6295	TrainAcc 0.0184	ValidAcc 0.0170	TestAcc 0.0162	BestValid 0.1794
	Epoch 3200:	Loss 0.6461	TrainAcc 0.0186	ValidAcc 0.0172	TestAcc 0.0165	BestValid 0.1794
	Epoch 3250:	Loss 0.6206	TrainAcc 0.0183	ValidAcc 0.0170	TestAcc 0.0160	BestValid 0.1794
	Epoch 3300:	Loss 0.6053	TrainAcc 0.0182	ValidAcc 0.0167	TestAcc 0.0159	BestValid 0.1794
	Epoch 3350:	Loss 0.6551	TrainAcc 0.0182	ValidAcc 0.0168	TestAcc 0.0159	BestValid 0.1794
	Epoch 3400:	Loss 0.7097	TrainAcc 0.0181	ValidAcc 0.0167	TestAcc 0.0158	BestValid 0.1794
	Epoch 3450:	Loss 0.5874	TrainAcc 0.0182	ValidAcc 0.0168	TestAcc 0.0159	BestValid 0.1794
	Epoch 3500:	Loss 0.6030	TrainAcc 0.0182	ValidAcc 0.0168	TestAcc 0.0159	BestValid 0.1794
	Epoch 3550:	Loss 0.5619	TrainAcc 0.0182	ValidAcc 0.0168	TestAcc 0.0159	BestValid 0.1794
	Epoch 3600:	Loss 0.5649	TrainAcc 0.0182	ValidAcc 0.0167	TestAcc 0.0159	BestValid 0.1794
	Epoch 3650:	Loss 0.5555	TrainAcc 0.0183	ValidAcc 0.0168	TestAcc 0.0159	BestValid 0.1794
	Epoch 3700:	Loss 0.6332	TrainAcc 0.0182	ValidAcc 0.0168	TestAcc 0.0159	BestValid 0.1794
	Epoch 3750:	Loss 0.5593	TrainAcc 0.0182	ValidAcc 0.0167	TestAcc 0.0158	BestValid 0.1794
	Epoch 3800:	Loss 0.5684	TrainAcc 0.0182	ValidAcc 0.0169	TestAcc 0.0159	BestValid 0.1794
	Epoch 3850:	Loss 0.5484	TrainAcc 0.0182	ValidAcc 0.0168	TestAcc 0.0159	BestValid 0.1794
	Epoch 3900:	Loss 0.5491	TrainAcc 0.0183	ValidAcc 0.0170	TestAcc 0.0162	BestValid 0.1794
	Epoch 3950:	Loss 0.5931	TrainAcc 0.0183	ValidAcc 0.0170	TestAcc 0.0162	BestValid 0.1794
	Epoch 4000:	Loss 0.5480	TrainAcc 0.0184	ValidAcc 0.0171	TestAcc 0.0162	BestValid 0.1794
	Epoch 4050:	Loss 0.5364	TrainAcc 0.0184	ValidAcc 0.0170	TestAcc 0.0163	BestValid 0.1794
	Epoch 4100:	Loss 0.5428	TrainAcc 0.0184	ValidAcc 0.0171	TestAcc 0.0161	BestValid 0.1794
	Epoch 4150:	Loss 0.5496	TrainAcc 0.0185	ValidAcc 0.0173	TestAcc 0.0165	BestValid 0.1794
	Epoch 4200:	Loss 0.5207	TrainAcc 0.0184	ValidAcc 0.0171	TestAcc 0.0162	BestValid 0.1794
	Epoch 4250:	Loss 0.6412	TrainAcc 0.0183	ValidAcc 0.0168	TestAcc 0.0159	BestValid 0.1794
	Epoch 4300:	Loss 0.5275	TrainAcc 0.0181	ValidAcc 0.0167	TestAcc 0.0158	BestValid 0.1794
	Epoch 4350:	Loss 0.5091	TrainAcc 0.0182	ValidAcc 0.0167	TestAcc 0.0158	BestValid 0.1794
	Epoch 4400:	Loss 0.5373	TrainAcc 0.0185	ValidAcc 0.0171	TestAcc 0.0162	BestValid 0.1794
	Epoch 4450:	Loss 0.5069	TrainAcc 0.0184	ValidAcc 0.0170	TestAcc 0.0162	BestValid 0.1794
	Epoch 4500:	Loss 0.4978	TrainAcc 0.0186	ValidAcc 0.0171	TestAcc 0.0163	BestValid 0.1794
	Epoch 4550:	Loss 0.5059	TrainAcc 0.0186	ValidAcc 0.0172	TestAcc 0.0163	BestValid 0.1794
	Epoch 4600:	Loss 0.5231	TrainAcc 0.0186	ValidAcc 0.0172	TestAcc 0.0163	BestValid 0.1794
	Epoch 4650:	Loss 0.4975	TrainAcc 0.0187	ValidAcc 0.0172	TestAcc 0.0164	BestValid 0.1794
	Epoch 4700:	Loss 0.5107	TrainAcc 0.0187	ValidAcc 0.0172	TestAcc 0.0165	BestValid 0.1794
	Epoch 4750:	Loss 0.5033	TrainAcc 0.0187	ValidAcc 0.0172	TestAcc 0.0164	BestValid 0.1794
	Epoch 4800:	Loss 0.4911	TrainAcc 0.0187	ValidAcc 0.0172	TestAcc 0.0164	BestValid 0.1794
	Epoch 4850:	Loss 0.5051	TrainAcc 0.0186	ValidAcc 0.0171	TestAcc 0.0163	BestValid 0.1794
	Epoch 4900:	Loss 0.5844	TrainAcc 0.0187	ValidAcc 0.0175	TestAcc 0.0165	BestValid 0.1794
	Epoch 4950:	Loss 0.5078	TrainAcc 0.0190	ValidAcc 0.0175	TestAcc 0.0167	BestValid 0.1794
	Epoch 5000:	Loss 0.4860	TrainAcc 0.0188	ValidAcc 0.0172	TestAcc 0.0166	BestValid 0.1794
****** Epoch Time (Excluding Evaluation Cost): 1.081 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.941 ms (Max: 1.340, Min: 0.068, Sum: 7.528)
Cluster-Wide Average, Compute: 347.659 ms (Max: 563.247, Min: 180.722, Sum: 2781.269)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.010, Min: 0.008, Sum: 0.067)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.019, Min: 0.016, Sum: 0.139)
Cluster-Wide Average, Communication-Graph: 712.458 ms (Max: 879.072, Min: 497.692, Sum: 5699.665)
Cluster-Wide Average, Optimization: 5.839 ms (Max: 5.867, Min: 5.805, Sum: 46.716)
Cluster-Wide Average, Others: 14.032 ms (Max: 14.057, Min: 13.991, Sum: 112.258)
****** Breakdown Sum: 1080.955 ms ******
Cluster-Wide Average, GPU Memory Consumption: 16.266 GB (Max: 16.909, Min: 16.157, Sum: 130.132)
Cluster-Wide Average, Graph-Level Communication Throughput: 24.607 Gbps (Max: 49.651, Min: 9.705, Sum: 196.859)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.038 GB
Total communication (cluster-wide, per-epoch): 14.520 GB
****** Accuracy Results ******
Highest valid_acc: 0.1794
Target test_acc: 0.1793
Epoch to reach the target acc: 0
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
