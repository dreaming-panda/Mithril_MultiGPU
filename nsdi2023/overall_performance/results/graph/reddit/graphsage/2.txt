Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.862 seconds.
Building the CSC structure...
        It takes 1.874 seconds.
Building the CSC structure...
        It takes 2.048 seconds.
Building the CSC structure...
        It takes 2.404 seconds.
Building the CSC structure...
        It takes 2.407 seconds.
Building the CSC structure...
        It takes 2.422 seconds.
Building the CSC structure...
        It takes 2.602 seconds.
Building the CSC structure...
        It takes 2.618 seconds.
Building the CSC structure...
        It takes 1.828 seconds.
        It takes 1.928 seconds.
        It takes 1.859 seconds.
Building the Feature Vector...
        It takes 2.311 seconds.
        It takes 2.338 seconds.
        It takes 2.397 seconds.
Building the Feature Vector...
        It takes 2.284 seconds.
        It takes 0.235 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 2.367 seconds.
Building the Feature Vector...
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
Building the Feature Vector...
        It takes 0.245 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.029 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.287 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.649 Gbps (per GPU), 445.188 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.398 Gbps (per GPU), 443.184 Gbps (aggregated)
The layer-level communication performance: 55.396 Gbps (per GPU), 443.168 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.167 Gbps (per GPU), 441.336 Gbps (aggregated)
The layer-level communication performance: 55.193 Gbps (per GPU), 441.542 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.962 Gbps (per GPU), 439.693 Gbps (aggregated)
The layer-level communication performance: 54.928 Gbps (per GPU), 439.422 Gbps (aggregated)
The layer-level communication performance: 54.896 Gbps (per GPU), 439.165 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.437 Gbps (per GPU), 1251.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.448 Gbps (per GPU), 1251.587 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.437 Gbps (per GPU), 1251.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.448 Gbps (per GPU), 1251.587 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.437 Gbps (per GPU), 1251.495 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.451 Gbps (per GPU), 1251.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.437 Gbps (per GPU), 1251.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.454 Gbps (per GPU), 1251.633 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.765 Gbps (per GPU), 806.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.764 Gbps (per GPU), 806.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.763 Gbps (per GPU), 806.106 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.764 Gbps (per GPU), 806.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.764 Gbps (per GPU), 806.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.764 Gbps (per GPU), 806.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.763 Gbps (per GPU), 806.106 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.765 Gbps (per GPU), 806.119 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.230 Gbps (per GPU), 281.839 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.230 Gbps (per GPU), 281.841 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.228 Gbps (per GPU), 281.821 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.230 Gbps (per GPU), 281.840 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.230 Gbps (per GPU), 281.838 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.230 Gbps (per GPU), 281.839 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.229 Gbps (per GPU), 281.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.229 Gbps (per GPU), 281.834 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.18ms  9.56ms  9.01ms  1.35 29.12K 14.23M
 chk_1  7.70ms  5.19ms  4.62ms  1.67 29.12K  6.56M
 chk_2 19.07ms 16.67ms 16.10ms  1.18 29.12K 24.68M
 chk_3 19.24ms 16.76ms 16.22ms  1.19 29.12K 22.95M
 chk_4  7.52ms  5.00ms  4.45ms  1.69 29.12K  6.33M
 chk_5 11.53ms  8.91ms  8.36ms  1.38 29.12K 12.05M
 chk_6 12.68ms 10.12ms  9.61ms  1.32 29.12K 14.60M
 chk_7 11.92ms  9.37ms  8.65ms  1.38 29.12K 13.21M
   Avg 12.73 10.20  9.63
   Max 19.24 16.76 16.22
   Min  7.52  5.00  4.45
 Ratio  2.56  3.35  3.64
   Var 17.18 17.48 17.53
Profiling takes 2.973 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 346.605 ms
Partition 0 [0, 4) has cost: 346.605 ms
Partition 1 [4, 8) has cost: 326.354 ms
Partition 2 [8, 12) has cost: 326.354 ms
Partition 3 [12, 16) has cost: 326.354 ms
Partition 4 [16, 20) has cost: 326.354 ms
Partition 5 [20, 24) has cost: 326.354 ms
Partition 6 [24, 28) has cost: 326.354 ms
Partition 7 [28, 32) has cost: 321.794 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 302.109 ms
GPU 0, Compute+Comm Time: 125.178 ms, Bubble Time: 129.449 ms, Imbalance Overhead: 47.482 ms
GPU 1, Compute+Comm Time: 119.730 ms, Bubble Time: 120.404 ms, Imbalance Overhead: 61.974 ms
GPU 2, Compute+Comm Time: 119.730 ms, Bubble Time: 110.637 ms, Imbalance Overhead: 71.741 ms
GPU 3, Compute+Comm Time: 119.730 ms, Bubble Time: 111.287 ms, Imbalance Overhead: 71.092 ms
GPU 4, Compute+Comm Time: 119.730 ms, Bubble Time: 120.768 ms, Imbalance Overhead: 61.611 ms
GPU 5, Compute+Comm Time: 119.730 ms, Bubble Time: 129.585 ms, Imbalance Overhead: 52.794 ms
GPU 6, Compute+Comm Time: 119.730 ms, Bubble Time: 138.550 ms, Imbalance Overhead: 43.829 ms
GPU 7, Compute+Comm Time: 118.519 ms, Bubble Time: 148.871 ms, Imbalance Overhead: 34.718 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 592.318 ms
GPU 0, Compute+Comm Time: 230.068 ms, Bubble Time: 293.435 ms, Imbalance Overhead: 68.815 ms
GPU 1, Compute+Comm Time: 233.417 ms, Bubble Time: 273.474 ms, Imbalance Overhead: 85.426 ms
GPU 2, Compute+Comm Time: 233.417 ms, Bubble Time: 255.342 ms, Imbalance Overhead: 103.558 ms
GPU 3, Compute+Comm Time: 233.417 ms, Bubble Time: 237.573 ms, Imbalance Overhead: 121.327 ms
GPU 4, Compute+Comm Time: 233.417 ms, Bubble Time: 217.992 ms, Imbalance Overhead: 140.909 ms
GPU 5, Compute+Comm Time: 233.417 ms, Bubble Time: 216.055 ms, Imbalance Overhead: 142.845 ms
GPU 6, Compute+Comm Time: 233.417 ms, Bubble Time: 235.111 ms, Imbalance Overhead: 123.789 ms
GPU 7, Compute+Comm Time: 248.220 ms, Bubble Time: 252.269 ms, Imbalance Overhead: 91.828 ms
The estimated cost of the whole pipeline: 939.147 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 672.959 ms
Partition 0 [0, 8) has cost: 672.959 ms
Partition 1 [8, 16) has cost: 652.709 ms
Partition 2 [16, 24) has cost: 652.709 ms
Partition 3 [24, 32) has cost: 648.149 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 312.285 ms
GPU 0, Compute+Comm Time: 161.535 ms, Bubble Time: 149.729 ms, Imbalance Overhead: 1.021 ms
GPU 1, Compute+Comm Time: 158.844 ms, Bubble Time: 130.917 ms, Imbalance Overhead: 22.523 ms
GPU 2, Compute+Comm Time: 158.844 ms, Bubble Time: 111.383 ms, Imbalance Overhead: 42.057 ms
GPU 3, Compute+Comm Time: 158.245 ms, Bubble Time: 111.933 ms, Imbalance Overhead: 42.107 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 597.647 ms
GPU 0, Compute+Comm Time: 300.606 ms, Bubble Time: 213.791 ms, Imbalance Overhead: 83.250 ms
GPU 1, Compute+Comm Time: 302.211 ms, Bubble Time: 212.109 ms, Imbalance Overhead: 83.326 ms
GPU 2, Compute+Comm Time: 302.211 ms, Bubble Time: 250.220 ms, Imbalance Overhead: 45.215 ms
GPU 3, Compute+Comm Time: 309.636 ms, Bubble Time: 286.434 ms, Imbalance Overhead: 1.577 ms
    The estimated cost with 2 DP ways is 955.428 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1325.668 ms
Partition 0 [0, 16) has cost: 1325.668 ms
Partition 1 [16, 32) has cost: 1300.857 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 352.762 ms
GPU 0, Compute+Comm Time: 235.562 ms, Bubble Time: 117.200 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 233.965 ms, Bubble Time: 117.550 ms, Imbalance Overhead: 1.247 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 628.144 ms
GPU 0, Compute+Comm Time: 415.500 ms, Bubble Time: 209.428 ms, Imbalance Overhead: 3.217 ms
GPU 1, Compute+Comm Time: 419.889 ms, Bubble Time: 208.255 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1029.952 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2626.526 ms
Partition 0 [0, 32) has cost: 2626.526 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 473.677 ms
GPU 0, Compute+Comm Time: 473.677 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 657.181 ms
GPU 0, Compute+Comm Time: 657.181 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1187.401 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 6.9772	TrainAcc 0.0433	ValidAcc 0.0373	TestAcc 0.0390	BestValid 0.0373
	Epoch 50:	Loss 3.1420	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 2.9680	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 2.9107	TrainAcc 0.0743	ValidAcc 0.0615	TestAcc 0.0608	BestValid 0.0615
	Epoch 200:	Loss 2.8723	TrainAcc 0.1220	ValidAcc 0.1569	TestAcc 0.1580	BestValid 0.1569
	Epoch 250:	Loss 2.8216	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.1569
	Epoch 300:	Loss 2.6170	TrainAcc 0.0511	ValidAcc 0.0444	TestAcc 0.0471	BestValid 0.1569
	Epoch 350:	Loss 2.3976	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1569
	Epoch 400:	Loss 2.6314	TrainAcc 0.0502	ValidAcc 0.0418	TestAcc 0.0452	BestValid 0.1569
	Epoch 450:	Loss 2.0312	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1569
	Epoch 500:	Loss 1.7494	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1569
	Epoch 550:	Loss 1.6555	TrainAcc 0.0185	ValidAcc 0.0162	TestAcc 0.0156	BestValid 0.1569
	Epoch 600:	Loss 1.6026	TrainAcc 0.0179	ValidAcc 0.0164	TestAcc 0.0155	BestValid 0.1569
	Epoch 650:	Loss 1.9478	TrainAcc 0.0159	ValidAcc 0.0139	TestAcc 0.0132	BestValid 0.1569
	Epoch 700:	Loss 1.3875	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0120	BestValid 0.1569
	Epoch 750:	Loss 1.3678	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 800:	Loss 1.2828	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 850:	Loss 1.2857	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 900:	Loss 1.2014	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 950:	Loss 1.3738	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1000:	Loss 1.2068	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1050:	Loss 1.1615	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1100:	Loss 1.1612	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1150:	Loss 1.0914	TrainAcc 0.0132	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1200:	Loss 1.0589	TrainAcc 0.0133	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1250:	Loss 1.1028	TrainAcc 0.0133	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1300:	Loss 1.0358	TrainAcc 0.0133	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1350:	Loss 1.0545	TrainAcc 0.0133	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1400:	Loss 1.0021	TrainAcc 0.0133	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1450:	Loss 0.9692	TrainAcc 0.0133	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1500:	Loss 0.9536	TrainAcc 0.0133	ValidAcc 0.0125	TestAcc 0.0119	BestValid 0.1569
	Epoch 1550:	Loss 0.8985	TrainAcc 0.0188	ValidAcc 0.0153	TestAcc 0.0151	BestValid 0.1569
	Epoch 1600:	Loss 0.9682	TrainAcc 0.0178	ValidAcc 0.0161	TestAcc 0.0153	BestValid 0.1569
	Epoch 1650:	Loss 1.1072	TrainAcc 0.0190	ValidAcc 0.0175	TestAcc 0.0167	BestValid 0.1569
	Epoch 1700:	Loss 0.8819	TrainAcc 0.0306	ValidAcc 0.0285	TestAcc 0.0270	BestValid 0.1569
	Epoch 1750:	Loss 0.8885	TrainAcc 0.0187	ValidAcc 0.0171	TestAcc 0.0164	BestValid 0.1569
	Epoch 1800:	Loss 0.8171	TrainAcc 0.0307	ValidAcc 0.0285	TestAcc 0.0270	BestValid 0.1569
	Epoch 1850:	Loss 0.7962	TrainAcc 0.0306	ValidAcc 0.0285	TestAcc 0.0269	BestValid 0.1569
	Epoch 1900:	Loss 0.7898	TrainAcc 0.0182	ValidAcc 0.0167	TestAcc 0.0159	BestValid 0.1569
	Epoch 1950:	Loss 0.7829	TrainAcc 0.0180	ValidAcc 0.0164	TestAcc 0.0156	BestValid 0.1569
	Epoch 2000:	Loss 1.0457	TrainAcc 0.0306	ValidAcc 0.0285	TestAcc 0.0271	BestValid 0.1569
	Epoch 2050:	Loss 0.7695	TrainAcc 0.0306	ValidAcc 0.0285	TestAcc 0.0270	BestValid 0.1569
	Epoch 2100:	Loss 0.8036	TrainAcc 0.0306	ValidAcc 0.0285	TestAcc 0.0270	BestValid 0.1569
	Epoch 2150:	Loss 0.7914	TrainAcc 0.0183	ValidAcc 0.0169	TestAcc 0.0161	BestValid 0.1569
	Epoch 2200:	Loss 0.7256	TrainAcc 0.0194	ValidAcc 0.0180	TestAcc 0.0169	BestValid 0.1569
	Epoch 2250:	Loss 0.7147	TrainAcc 0.0305	ValidAcc 0.0284	TestAcc 0.0269	BestValid 0.1569
	Epoch 2300:	Loss 1.1065	TrainAcc 0.0299	ValidAcc 0.0276	TestAcc 0.0262	BestValid 0.1569
	Epoch 2350:	Loss 0.7436	TrainAcc 0.0307	ValidAcc 0.0285	TestAcc 0.0271	BestValid 0.1569
	Epoch 2400:	Loss 0.7855	TrainAcc 0.0195	ValidAcc 0.0183	TestAcc 0.0171	BestValid 0.1569
	Epoch 2450:	Loss 0.6972	TrainAcc 0.0303	ValidAcc 0.0280	TestAcc 0.0266	BestValid 0.1569
	Epoch 2500:	Loss 0.7054	TrainAcc 0.0255	ValidAcc 0.0231	TestAcc 0.0221	BestValid 0.1569
	Epoch 2550:	Loss 0.6801	TrainAcc 0.0305	ValidAcc 0.0283	TestAcc 0.0268	BestValid 0.1569
	Epoch 2600:	Loss 0.7182	TrainAcc 0.0179	ValidAcc 0.0164	TestAcc 0.0156	BestValid 0.1569
	Epoch 2650:	Loss 0.6735	TrainAcc 0.0308	ValidAcc 0.0286	TestAcc 0.0272	BestValid 0.1569
	Epoch 2700:	Loss 0.6549	TrainAcc 0.0198	ValidAcc 0.0185	TestAcc 0.0175	BestValid 0.1569
	Epoch 2750:	Loss 0.6624	TrainAcc 0.0307	ValidAcc 0.0285	TestAcc 0.0271	BestValid 0.1569
	Epoch 2800:	Loss 0.6415	TrainAcc 0.0179	ValidAcc 0.0165	TestAcc 0.0156	BestValid 0.1569
	Epoch 2850:	Loss 0.6357	TrainAcc 0.0179	ValidAcc 0.0164	TestAcc 0.0156	BestValid 0.1569
	Epoch 2900:	Loss 0.6519	TrainAcc 0.0307	ValidAcc 0.0285	TestAcc 0.0270	BestValid 0.1569
	Epoch 2950:	Loss 0.6344	TrainAcc 0.0178	ValidAcc 0.0161	TestAcc 0.0153	BestValid 0.1569
	Epoch 3000:	Loss 0.6169	TrainAcc 0.0179	ValidAcc 0.0163	TestAcc 0.0156	BestValid 0.1569
	Epoch 3050:	Loss 0.6089	TrainAcc 0.0307	ValidAcc 0.0284	TestAcc 0.0270	BestValid 0.1569
	Epoch 3100:	Loss 0.6940	TrainAcc 0.0308	ValidAcc 0.0285	TestAcc 0.0272	BestValid 0.1569
	Epoch 3150:	Loss 0.6326	TrainAcc 0.0310	ValidAcc 0.0288	TestAcc 0.0274	BestValid 0.1569
	Epoch 3200:	Loss 0.5809	TrainAcc 0.0309	ValidAcc 0.0286	TestAcc 0.0273	BestValid 0.1569
	Epoch 3250:	Loss 0.5835	TrainAcc 0.0311	ValidAcc 0.0287	TestAcc 0.0274	BestValid 0.1569
	Epoch 3300:	Loss 0.5728	TrainAcc 0.0194	ValidAcc 0.0179	TestAcc 0.0172	BestValid 0.1569
	Epoch 3350:	Loss 0.7872	TrainAcc 0.0311	ValidAcc 0.0289	TestAcc 0.0276	BestValid 0.1569
	Epoch 3400:	Loss 0.6225	TrainAcc 0.0299	ValidAcc 0.0274	TestAcc 0.0260	BestValid 0.1569
	Epoch 3450:	Loss 0.6670	TrainAcc 0.0182	ValidAcc 0.0164	TestAcc 0.0158	BestValid 0.1569
	Epoch 3500:	Loss 0.5581	TrainAcc 0.0185	ValidAcc 0.0167	TestAcc 0.0163	BestValid 0.1569
	Epoch 3550:	Loss 0.5985	TrainAcc 0.0186	ValidAcc 0.0168	TestAcc 0.0163	BestValid 0.1569
	Epoch 3600:	Loss 0.5414	TrainAcc 0.0185	ValidAcc 0.0167	TestAcc 0.0162	BestValid 0.1569
	Epoch 3650:	Loss 0.5403	TrainAcc 0.0189	ValidAcc 0.0172	TestAcc 0.0165	BestValid 0.1569
	Epoch 3700:	Loss 0.5389	TrainAcc 0.0186	ValidAcc 0.0172	TestAcc 0.0163	BestValid 0.1569
	Epoch 3750:	Loss 0.6429	TrainAcc 0.0209	ValidAcc 0.0197	TestAcc 0.0184	BestValid 0.1569
	Epoch 3800:	Loss 0.5551	TrainAcc 0.0184	ValidAcc 0.0167	TestAcc 0.0162	BestValid 0.1569
	Epoch 3850:	Loss 0.5296	TrainAcc 0.0190	ValidAcc 0.0171	TestAcc 0.0167	BestValid 0.1569
	Epoch 3900:	Loss 0.5326	TrainAcc 0.0199	ValidAcc 0.0185	TestAcc 0.0175	BestValid 0.1569
	Epoch 3950:	Loss 0.5710	TrainAcc 0.0210	ValidAcc 0.0198	TestAcc 0.0189	BestValid 0.1569
	Epoch 4000:	Loss 0.5070	TrainAcc 0.0194	ValidAcc 0.0175	TestAcc 0.0173	BestValid 0.1569
	Epoch 4050:	Loss 0.5170	TrainAcc 0.0227	ValidAcc 0.0207	TestAcc 0.0202	BestValid 0.1569
	Epoch 4100:	Loss 0.5302	TrainAcc 0.0189	ValidAcc 0.0171	TestAcc 0.0166	BestValid 0.1569
	Epoch 4150:	Loss 0.5048	TrainAcc 0.0194	ValidAcc 0.0175	TestAcc 0.0171	BestValid 0.1569
	Epoch 4200:	Loss 0.5070	TrainAcc 0.0200	ValidAcc 0.0179	TestAcc 0.0177	BestValid 0.1569
	Epoch 4250:	Loss 0.5116	TrainAcc 0.0196	ValidAcc 0.0179	TestAcc 0.0175	BestValid 0.1569
	Epoch 4300:	Loss 0.4945	TrainAcc 0.0325	ValidAcc 0.0304	TestAcc 0.0292	BestValid 0.1569
	Epoch 4350:	Loss 0.5187	TrainAcc 0.0196	ValidAcc 0.0182	TestAcc 0.0174	BestValid 0.1569
	Epoch 4400:	Loss 0.5033	TrainAcc 0.0200	ValidAcc 0.0183	TestAcc 0.0177	BestValid 0.1569
	Epoch 4450:	Loss 0.4954	TrainAcc 0.0200	ValidAcc 0.0185	TestAcc 0.0178	BestValid 0.1569
	Epoch 4500:	Loss 0.4980	TrainAcc 0.0199	ValidAcc 0.0185	TestAcc 0.0175	BestValid 0.1569
	Epoch 4550:	Loss 0.5065	TrainAcc 0.0282	ValidAcc 0.0264	TestAcc 0.0251	BestValid 0.1569
	Epoch 4600:	Loss 0.4857	TrainAcc 0.0324	ValidAcc 0.0304	TestAcc 0.0289	BestValid 0.1569
	Epoch 4650:	Loss 1.4560	TrainAcc 0.0133	ValidAcc 0.0126	TestAcc 0.0120	BestValid 0.1569
	Epoch 4700:	Loss 0.6482	TrainAcc 0.0150	ValidAcc 0.0141	TestAcc 0.0139	BestValid 0.1569
	Epoch 4750:	Loss 0.5766	TrainAcc 0.0321	ValidAcc 0.0302	TestAcc 0.0287	BestValid 0.1569
	Epoch 4800:	Loss 0.5204	TrainAcc 0.0320	ValidAcc 0.0300	TestAcc 0.0287	BestValid 0.1569
	Epoch 4850:	Loss 0.5497	TrainAcc 0.0316	ValidAcc 0.0295	TestAcc 0.0281	BestValid 0.1569
	Epoch 4900:	Loss 0.5423	TrainAcc 0.0320	ValidAcc 0.0299	TestAcc 0.0285	BestValid 0.1569
	Epoch 4950:	Loss 0.5090	TrainAcc 0.0319	ValidAcc 0.0299	TestAcc 0.0286	BestValid 0.1569
	Epoch 5000:	Loss 0.5106	TrainAcc 0.0316	ValidAcc 0.0295	TestAcc 0.0283	BestValid 0.1569
****** Epoch Time (Excluding Evaluation Cost): 1.080 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.920 ms (Max: 1.289, Min: 0.077, Sum: 7.361)
Cluster-Wide Average, Compute: 346.935 ms (Max: 562.262, Min: 180.504, Sum: 2775.477)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.066)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.019, Min: 0.015, Sum: 0.137)
Cluster-Wide Average, Communication-Graph: 712.370 ms (Max: 878.425, Min: 497.813, Sum: 5698.959)
Cluster-Wide Average, Optimization: 5.751 ms (Max: 5.796, Min: 5.727, Sum: 46.011)
Cluster-Wide Average, Others: 14.029 ms (Max: 14.065, Min: 14.010, Sum: 112.228)
****** Breakdown Sum: 1080.030 ms ******
Cluster-Wide Average, GPU Memory Consumption: 16.266 GB (Max: 16.909, Min: 16.157, Sum: 130.132)
Cluster-Wide Average, Graph-Level Communication Throughput: 24.606 Gbps (Max: 49.636, Min: 9.714, Sum: 196.844)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.038 GB
Total communication (cluster-wide, per-epoch): 14.520 GB
****** Accuracy Results ******
Highest valid_acc: 0.1569
Target test_acc: 0.1580
Epoch to reach the target acc: 199
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
