Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.966 seconds.
Building the CSC structure...
        It takes 2.022 seconds.
Building the CSC structure...
        It takes 2.361 seconds.
Building the CSC structure...
        It takes 2.370 seconds.
Building the CSC structure...
        It takes 2.411 seconds.
Building the CSC structure...
        It takes 2.417 seconds.
Building the CSC structure...
        It takes 2.631 seconds.
Building the CSC structure...
        It takes 2.654 seconds.
Building the CSC structure...
        It takes 1.861 seconds.
        It takes 1.877 seconds.
        It takes 2.298 seconds.
        It takes 2.336 seconds.
        It takes 2.292 seconds.
        It takes 2.481 seconds.
        It takes 2.337 seconds.
Building the Feature Vector...
        It takes 2.340 seconds.
Building the Feature Vector...
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.250 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.039 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.283 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.038 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.814 Gbps (per GPU), 446.512 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.551 Gbps (per GPU), 444.408 Gbps (aggregated)
The layer-level communication performance: 55.558 Gbps (per GPU), 444.467 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.340 Gbps (per GPU), 442.724 Gbps (aggregated)
The layer-level communication performance: 55.311 Gbps (per GPU), 442.490 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.128 Gbps (per GPU), 441.027 Gbps (aggregated)
The layer-level communication performance: 55.090 Gbps (per GPU), 440.718 Gbps (aggregated)
The layer-level communication performance: 55.059 Gbps (per GPU), 440.471 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.524 Gbps (per GPU), 1268.191 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.554 Gbps (per GPU), 1268.431 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.533 Gbps (per GPU), 1268.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.539 Gbps (per GPU), 1268.311 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.524 Gbps (per GPU), 1268.191 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.554 Gbps (per GPU), 1268.432 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.527 Gbps (per GPU), 1268.213 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.543 Gbps (per GPU), 1268.341 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.804 Gbps (per GPU), 806.429 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.800 Gbps (per GPU), 806.403 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.802 Gbps (per GPU), 806.416 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.801 Gbps (per GPU), 806.409 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.802 Gbps (per GPU), 806.416 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.801 Gbps (per GPU), 806.409 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.804 Gbps (per GPU), 806.428 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.803 Gbps (per GPU), 806.422 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.913 Gbps (per GPU), 271.300 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.912 Gbps (per GPU), 271.294 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.911 Gbps (per GPU), 271.285 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.912 Gbps (per GPU), 271.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.912 Gbps (per GPU), 271.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.912 Gbps (per GPU), 271.299 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.910 Gbps (per GPU), 271.278 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.911 Gbps (per GPU), 271.292 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.51ms  9.49ms  9.81ms  3.91 29.12K 14.23M
 chk_1  2.51ms  5.14ms  5.51ms  2.19 29.12K  6.56M
 chk_2  2.53ms 16.52ms 16.89ms  6.67 29.12K 24.68M
 chk_3  2.53ms 16.64ms 17.01ms  6.72 29.12K 22.95M
 chk_4  2.53ms  4.97ms  5.34ms  2.11 29.12K  6.33M
 chk_5  2.53ms  9.01ms  9.29ms  3.67 29.12K 12.05M
 chk_6  2.53ms 10.01ms 10.46ms  4.13 29.12K 14.60M
 chk_7  2.54ms  9.25ms  9.66ms  3.80 29.12K 13.21M
   Avg  2.53 10.13 10.50
   Max  2.54 16.64 17.01
   Min  2.51  4.97  5.34
 Ratio  1.01  3.35  3.19
   Var  0.00 17.16 17.19
Profiling takes 2.156 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 344.323 ms
Partition 0 [0, 5) has cost: 344.323 ms
Partition 1 [5, 9) has cost: 324.111 ms
Partition 2 [9, 13) has cost: 324.111 ms
Partition 3 [13, 17) has cost: 324.111 ms
Partition 4 [17, 21) has cost: 324.111 ms
Partition 5 [21, 25) has cost: 324.111 ms
Partition 6 [25, 29) has cost: 324.111 ms
Partition 7 [29, 33) has cost: 327.046 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 302.157 ms
GPU 0, Compute+Comm Time: 125.015 ms, Bubble Time: 130.033 ms, Imbalance Overhead: 47.109 ms
GPU 1, Compute+Comm Time: 119.947 ms, Bubble Time: 120.928 ms, Imbalance Overhead: 61.283 ms
GPU 2, Compute+Comm Time: 119.947 ms, Bubble Time: 111.196 ms, Imbalance Overhead: 71.014 ms
GPU 3, Compute+Comm Time: 119.947 ms, Bubble Time: 111.586 ms, Imbalance Overhead: 70.625 ms
GPU 4, Compute+Comm Time: 119.947 ms, Bubble Time: 120.990 ms, Imbalance Overhead: 61.221 ms
GPU 5, Compute+Comm Time: 119.947 ms, Bubble Time: 129.759 ms, Imbalance Overhead: 52.452 ms
GPU 6, Compute+Comm Time: 119.947 ms, Bubble Time: 138.429 ms, Imbalance Overhead: 43.781 ms
GPU 7, Compute+Comm Time: 120.552 ms, Bubble Time: 148.392 ms, Imbalance Overhead: 33.213 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 588.406 ms
GPU 0, Compute+Comm Time: 233.208 ms, Bubble Time: 290.807 ms, Imbalance Overhead: 64.391 ms
GPU 1, Compute+Comm Time: 230.877 ms, Bubble Time: 271.607 ms, Imbalance Overhead: 85.921 ms
GPU 2, Compute+Comm Time: 230.877 ms, Bubble Time: 254.212 ms, Imbalance Overhead: 103.316 ms
GPU 3, Compute+Comm Time: 230.877 ms, Bubble Time: 236.471 ms, Imbalance Overhead: 121.057 ms
GPU 4, Compute+Comm Time: 230.877 ms, Bubble Time: 216.836 ms, Imbalance Overhead: 140.693 ms
GPU 5, Compute+Comm Time: 230.877 ms, Bubble Time: 215.558 ms, Imbalance Overhead: 141.971 ms
GPU 6, Compute+Comm Time: 230.877 ms, Bubble Time: 234.432 ms, Imbalance Overhead: 123.096 ms
GPU 7, Compute+Comm Time: 246.022 ms, Bubble Time: 251.426 ms, Imbalance Overhead: 90.959 ms
The estimated cost of the whole pipeline: 935.091 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 668.434 ms
Partition 0 [0, 9) has cost: 668.434 ms
Partition 1 [9, 17) has cost: 648.222 ms
Partition 2 [17, 25) has cost: 648.222 ms
Partition 3 [25, 33) has cost: 651.157 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 311.438 ms
GPU 0, Compute+Comm Time: 161.474 ms, Bubble Time: 149.526 ms, Imbalance Overhead: 0.438 ms
GPU 1, Compute+Comm Time: 158.932 ms, Bubble Time: 130.698 ms, Imbalance Overhead: 21.809 ms
GPU 2, Compute+Comm Time: 158.932 ms, Bubble Time: 111.235 ms, Imbalance Overhead: 41.272 ms
GPU 3, Compute+Comm Time: 159.196 ms, Bubble Time: 111.461 ms, Imbalance Overhead: 40.781 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 592.711 ms
GPU 0, Compute+Comm Time: 300.958 ms, Bubble Time: 211.730 ms, Imbalance Overhead: 80.023 ms
GPU 1, Compute+Comm Time: 299.881 ms, Bubble Time: 210.780 ms, Imbalance Overhead: 82.050 ms
GPU 2, Compute+Comm Time: 299.881 ms, Bubble Time: 248.529 ms, Imbalance Overhead: 44.301 ms
GPU 3, Compute+Comm Time: 307.473 ms, Bubble Time: 284.382 ms, Imbalance Overhead: 0.857 ms
    The estimated cost with 2 DP ways is 949.357 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1316.656 ms
Partition 0 [0, 17) has cost: 1316.656 ms
Partition 1 [17, 33) has cost: 1299.378 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 352.045 ms
GPU 0, Compute+Comm Time: 234.848 ms, Bubble Time: 117.197 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 233.739 ms, Bubble Time: 117.096 ms, Imbalance Overhead: 1.209 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 623.415 ms
GPU 0, Compute+Comm Time: 413.026 ms, Bubble Time: 207.463 ms, Imbalance Overhead: 2.927 ms
GPU 1, Compute+Comm Time: 416.245 ms, Bubble Time: 207.170 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1024.233 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2616.034 ms
Partition 0 [0, 33) has cost: 2616.034 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 484.985 ms
GPU 0, Compute+Comm Time: 484.985 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 665.989 ms
GPU 0, Compute+Comm Time: 665.989 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1208.523 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.9994	TrainAcc 0.0095	ValidAcc 0.0085	TestAcc 0.0091	BestValid 0.0085
	Epoch 50:	Loss 1.7021	TrainAcc 0.6862	ValidAcc 0.7066	TestAcc 0.7001	BestValid 0.7066
	Epoch 100:	Loss 1.1713	TrainAcc 0.7892	ValidAcc 0.8032	TestAcc 0.7979	BestValid 0.8032
	Epoch 150:	Loss 0.9357	TrainAcc 0.8417	ValidAcc 0.8524	TestAcc 0.8466	BestValid 0.8524
	Epoch 200:	Loss 0.8062	TrainAcc 0.8673	ValidAcc 0.8753	TestAcc 0.8704	BestValid 0.8753
	Epoch 250:	Loss 0.7241	TrainAcc 0.8814	ValidAcc 0.8887	TestAcc 0.8832	BestValid 0.8887
	Epoch 300:	Loss 0.6721	TrainAcc 0.8908	ValidAcc 0.8975	TestAcc 0.8924	BestValid 0.8975
	Epoch 350:	Loss 0.6336	TrainAcc 0.8966	ValidAcc 0.9026	TestAcc 0.8985	BestValid 0.9026
	Epoch 400:	Loss 0.6094	TrainAcc 0.9023	ValidAcc 0.9087	TestAcc 0.9040	BestValid 0.9087
	Epoch 450:	Loss 0.5820	TrainAcc 0.9049	ValidAcc 0.9119	TestAcc 0.9064	BestValid 0.9119
	Epoch 500:	Loss 0.5625	TrainAcc 0.9074	ValidAcc 0.9141	TestAcc 0.9092	BestValid 0.9141
	Epoch 550:	Loss 0.5505	TrainAcc 0.9075	ValidAcc 0.9135	TestAcc 0.9089	BestValid 0.9141
	Epoch 600:	Loss 0.5322	TrainAcc 0.9099	ValidAcc 0.9153	TestAcc 0.9113	BestValid 0.9153
	Epoch 650:	Loss 0.5201	TrainAcc 0.9110	ValidAcc 0.9162	TestAcc 0.9119	BestValid 0.9162
	Epoch 700:	Loss 0.5136	TrainAcc 0.9141	ValidAcc 0.9189	TestAcc 0.9145	BestValid 0.9189
	Epoch 750:	Loss 0.5031	TrainAcc 0.9157	ValidAcc 0.9201	TestAcc 0.9156	BestValid 0.9201
	Epoch 800:	Loss 0.4949	TrainAcc 0.9169	ValidAcc 0.9212	TestAcc 0.9170	BestValid 0.9212
	Epoch 850:	Loss 0.4891	TrainAcc 0.9146	ValidAcc 0.9179	TestAcc 0.9147	BestValid 0.9212
	Epoch 900:	Loss 0.4870	TrainAcc 0.9144	ValidAcc 0.9177	TestAcc 0.9148	BestValid 0.9212
	Epoch 950:	Loss 0.4750	TrainAcc 0.9183	ValidAcc 0.9220	TestAcc 0.9185	BestValid 0.9220
	Epoch 1000:	Loss 0.4670	TrainAcc 0.9163	ValidAcc 0.9192	TestAcc 0.9170	BestValid 0.9220
	Epoch 1050:	Loss 0.4649	TrainAcc 0.9167	ValidAcc 0.9195	TestAcc 0.9172	BestValid 0.9220
	Epoch 1100:	Loss 0.4599	TrainAcc 0.9220	ValidAcc 0.9247	TestAcc 0.9220	BestValid 0.9247
	Epoch 1150:	Loss 0.4547	TrainAcc 0.9202	ValidAcc 0.9233	TestAcc 0.9203	BestValid 0.9247
	Epoch 1200:	Loss 0.4511	TrainAcc 0.9239	ValidAcc 0.9270	TestAcc 0.9233	BestValid 0.9270
	Epoch 1250:	Loss 0.4497	TrainAcc 0.9226	ValidAcc 0.9253	TestAcc 0.9223	BestValid 0.9270
	Epoch 1300:	Loss 0.4422	TrainAcc 0.9246	ValidAcc 0.9274	TestAcc 0.9242	BestValid 0.9274
	Epoch 1350:	Loss 0.4396	TrainAcc 0.9249	ValidAcc 0.9269	TestAcc 0.9246	BestValid 0.9274
	Epoch 1400:	Loss 0.4395	TrainAcc 0.9212	ValidAcc 0.9231	TestAcc 0.9202	BestValid 0.9274
	Epoch 1450:	Loss 0.4408	TrainAcc 0.9200	ValidAcc 0.9208	TestAcc 0.9190	BestValid 0.9274
	Epoch 1500:	Loss 0.4271	TrainAcc 0.9256	ValidAcc 0.9280	TestAcc 0.9248	BestValid 0.9280
	Epoch 1550:	Loss 0.4292	TrainAcc 0.9261	ValidAcc 0.9281	TestAcc 0.9253	BestValid 0.9281
	Epoch 1600:	Loss 0.4275	TrainAcc 0.9267	ValidAcc 0.9282	TestAcc 0.9254	BestValid 0.9282
	Epoch 1650:	Loss 0.4294	TrainAcc 0.9209	ValidAcc 0.9227	TestAcc 0.9199	BestValid 0.9282
	Epoch 1700:	Loss 0.4194	TrainAcc 0.9265	ValidAcc 0.9283	TestAcc 0.9258	BestValid 0.9283
	Epoch 1750:	Loss 0.4187	TrainAcc 0.9298	ValidAcc 0.9311	TestAcc 0.9283	BestValid 0.9311
	Epoch 1800:	Loss 0.4197	TrainAcc 0.9269	ValidAcc 0.9286	TestAcc 0.9256	BestValid 0.9311
	Epoch 1850:	Loss 0.4220	TrainAcc 0.9251	ValidAcc 0.9264	TestAcc 0.9239	BestValid 0.9311
	Epoch 1900:	Loss 0.4109	TrainAcc 0.9315	ValidAcc 0.9324	TestAcc 0.9295	BestValid 0.9324
	Epoch 1950:	Loss 0.4116	TrainAcc 0.9292	ValidAcc 0.9302	TestAcc 0.9271	BestValid 0.9324
	Epoch 2000:	Loss 0.4280	TrainAcc 0.9279	ValidAcc 0.9292	TestAcc 0.9264	BestValid 0.9324
	Epoch 2050:	Loss 0.4064	TrainAcc 0.9305	ValidAcc 0.9308	TestAcc 0.9288	BestValid 0.9324
	Epoch 2100:	Loss 0.4159	TrainAcc 0.9257	ValidAcc 0.9266	TestAcc 0.9237	BestValid 0.9324
	Epoch 2150:	Loss 0.4019	TrainAcc 0.9288	ValidAcc 0.9295	TestAcc 0.9269	BestValid 0.9324
	Epoch 2200:	Loss 0.4024	TrainAcc 0.9298	ValidAcc 0.9302	TestAcc 0.9279	BestValid 0.9324
	Epoch 2250:	Loss 0.3985	TrainAcc 0.9302	ValidAcc 0.9305	TestAcc 0.9284	BestValid 0.9324
	Epoch 2300:	Loss 0.3985	TrainAcc 0.9352	ValidAcc 0.9356	TestAcc 0.9330	BestValid 0.9356
	Epoch 2350:	Loss 0.3964	TrainAcc 0.9276	ValidAcc 0.9280	TestAcc 0.9259	BestValid 0.9356
	Epoch 2400:	Loss 0.3926	TrainAcc 0.9361	ValidAcc 0.9365	TestAcc 0.9335	BestValid 0.9365
	Epoch 2450:	Loss 0.4004	TrainAcc 0.9293	ValidAcc 0.9292	TestAcc 0.9270	BestValid 0.9365
	Epoch 2500:	Loss 0.4020	TrainAcc 0.9321	ValidAcc 0.9321	TestAcc 0.9295	BestValid 0.9365
	Epoch 2550:	Loss 0.3883	TrainAcc 0.9330	ValidAcc 0.9329	TestAcc 0.9305	BestValid 0.9365
	Epoch 2600:	Loss 0.3968	TrainAcc 0.9341	ValidAcc 0.9340	TestAcc 0.9316	BestValid 0.9365
	Epoch 2650:	Loss 0.3909	TrainAcc 0.9265	ValidAcc 0.9259	TestAcc 0.9245	BestValid 0.9365
	Epoch 2700:	Loss 0.3920	TrainAcc 0.9303	ValidAcc 0.9301	TestAcc 0.9281	BestValid 0.9365
	Epoch 2750:	Loss 0.3839	TrainAcc 0.9349	ValidAcc 0.9343	TestAcc 0.9322	BestValid 0.9365
	Epoch 2800:	Loss 0.3858	TrainAcc 0.9355	ValidAcc 0.9355	TestAcc 0.9329	BestValid 0.9365
	Epoch 2850:	Loss 0.3805	TrainAcc 0.9342	ValidAcc 0.9341	TestAcc 0.9317	BestValid 0.9365
	Epoch 2900:	Loss 0.3823	TrainAcc 0.9265	ValidAcc 0.9268	TestAcc 0.9245	BestValid 0.9365
	Epoch 2950:	Loss 0.3763	TrainAcc 0.9354	ValidAcc 0.9353	TestAcc 0.9327	BestValid 0.9365
	Epoch 3000:	Loss 0.3777	TrainAcc 0.9303	ValidAcc 0.9298	TestAcc 0.9280	BestValid 0.9365
	Epoch 3050:	Loss 0.3774	TrainAcc 0.9361	ValidAcc 0.9356	TestAcc 0.9332	BestValid 0.9365
	Epoch 3100:	Loss 0.3719	TrainAcc 0.9359	ValidAcc 0.9357	TestAcc 0.9330	BestValid 0.9365
	Epoch 3150:	Loss 0.3721	TrainAcc 0.9365	ValidAcc 0.9359	TestAcc 0.9336	BestValid 0.9365
	Epoch 3200:	Loss 0.3721	TrainAcc 0.9331	ValidAcc 0.9324	TestAcc 0.9304	BestValid 0.9365
	Epoch 3250:	Loss 0.3696	TrainAcc 0.9382	ValidAcc 0.9376	TestAcc 0.9352	BestValid 0.9376
	Epoch 3300:	Loss 0.3679	TrainAcc 0.9372	ValidAcc 0.9375	TestAcc 0.9341	BestValid 0.9376
	Epoch 3350:	Loss 0.3659	TrainAcc 0.9351	ValidAcc 0.9343	TestAcc 0.9323	BestValid 0.9376
	Epoch 3400:	Loss 0.3684	TrainAcc 0.9357	ValidAcc 0.9349	TestAcc 0.9329	BestValid 0.9376
	Epoch 3450:	Loss 0.3636	TrainAcc 0.9321	ValidAcc 0.9319	TestAcc 0.9296	BestValid 0.9376
	Epoch 3500:	Loss 0.3625	TrainAcc 0.9323	ValidAcc 0.9315	TestAcc 0.9297	BestValid 0.9376
	Epoch 3550:	Loss 0.3646	TrainAcc 0.9340	ValidAcc 0.9332	TestAcc 0.9311	BestValid 0.9376
	Epoch 3600:	Loss 0.3653	TrainAcc 0.9312	ValidAcc 0.9308	TestAcc 0.9289	BestValid 0.9376
	Epoch 3650:	Loss 0.3647	TrainAcc 0.9375	ValidAcc 0.9370	TestAcc 0.9345	BestValid 0.9376
	Epoch 3700:	Loss 0.3661	TrainAcc 0.9403	ValidAcc 0.9395	TestAcc 0.9375	BestValid 0.9395
	Epoch 3750:	Loss 0.3594	TrainAcc 0.9367	ValidAcc 0.9362	TestAcc 0.9338	BestValid 0.9395
	Epoch 3800:	Loss 0.3593	TrainAcc 0.9369	ValidAcc 0.9361	TestAcc 0.9337	BestValid 0.9395
	Epoch 3850:	Loss 0.3615	TrainAcc 0.9394	ValidAcc 0.9384	TestAcc 0.9359	BestValid 0.9395
	Epoch 3900:	Loss 0.3556	TrainAcc 0.9383	ValidAcc 0.9379	TestAcc 0.9350	BestValid 0.9395
	Epoch 3950:	Loss 0.3600	TrainAcc 0.9371	ValidAcc 0.9367	TestAcc 0.9341	BestValid 0.9395
	Epoch 4000:	Loss 0.3557	TrainAcc 0.9375	ValidAcc 0.9371	TestAcc 0.9343	BestValid 0.9395
	Epoch 4050:	Loss 0.3569	TrainAcc 0.9399	ValidAcc 0.9391	TestAcc 0.9363	BestValid 0.9395
	Epoch 4100:	Loss 0.3528	TrainAcc 0.9364	ValidAcc 0.9358	TestAcc 0.9330	BestValid 0.9395
	Epoch 4150:	Loss 0.3572	TrainAcc 0.9377	ValidAcc 0.9371	TestAcc 0.9343	BestValid 0.9395
	Epoch 4200:	Loss 0.3511	TrainAcc 0.9402	ValidAcc 0.9398	TestAcc 0.9369	BestValid 0.9398
	Epoch 4250:	Loss 0.3509	TrainAcc 0.9409	ValidAcc 0.9402	TestAcc 0.9376	BestValid 0.9402
	Epoch 4300:	Loss 0.3499	TrainAcc 0.9389	ValidAcc 0.9382	TestAcc 0.9356	BestValid 0.9402
	Epoch 4350:	Loss 0.3520	TrainAcc 0.9399	ValidAcc 0.9390	TestAcc 0.9365	BestValid 0.9402
	Epoch 4400:	Loss 0.3510	TrainAcc 0.9364	ValidAcc 0.9353	TestAcc 0.9336	BestValid 0.9402
	Epoch 4450:	Loss 0.3505	TrainAcc 0.9400	ValidAcc 0.9389	TestAcc 0.9371	BestValid 0.9402
	Epoch 4500:	Loss 0.3491	TrainAcc 0.9385	ValidAcc 0.9381	TestAcc 0.9351	BestValid 0.9402
	Epoch 4550:	Loss 0.3529	TrainAcc 0.9396	ValidAcc 0.9386	TestAcc 0.9363	BestValid 0.9402
	Epoch 4600:	Loss 0.3455	TrainAcc 0.9400	ValidAcc 0.9393	TestAcc 0.9364	BestValid 0.9402
	Epoch 4650:	Loss 0.3506	TrainAcc 0.9379	ValidAcc 0.9366	TestAcc 0.9346	BestValid 0.9402
	Epoch 4700:	Loss 0.3455	TrainAcc 0.9388	ValidAcc 0.9385	TestAcc 0.9354	BestValid 0.9402
	Epoch 4750:	Loss 0.3498	TrainAcc 0.9415	ValidAcc 0.9405	TestAcc 0.9378	BestValid 0.9405
	Epoch 4800:	Loss 0.3517	TrainAcc 0.9350	ValidAcc 0.9335	TestAcc 0.9317	BestValid 0.9405
	Epoch 4850:	Loss 0.3397	TrainAcc 0.9382	ValidAcc 0.9366	TestAcc 0.9344	BestValid 0.9405
	Epoch 4900:	Loss 0.3447	TrainAcc 0.9360	ValidAcc 0.9346	TestAcc 0.9323	BestValid 0.9405
	Epoch 4950:	Loss 0.3436	TrainAcc 0.9402	ValidAcc 0.9394	TestAcc 0.9369	BestValid 0.9405
	Epoch 5000:	Loss 0.3431	TrainAcc 0.9380	ValidAcc 0.9366	TestAcc 0.9345	BestValid 0.9405
****** Epoch Time (Excluding Evaluation Cost): 0.903 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.950 ms (Max: 1.359, Min: 0.102, Sum: 7.600)
Cluster-Wide Average, Compute: 248.157 ms (Max: 386.381, Min: 141.106, Sum: 1985.259)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.011, Min: 0.008, Sum: 0.073)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.018, Min: 0.015, Sum: 0.134)
Cluster-Wide Average, Communication-Graph: 636.907 ms (Max: 743.666, Min: 499.468, Sum: 5095.258)
Cluster-Wide Average, Optimization: 3.224 ms (Max: 3.257, Min: 3.203, Sum: 25.788)
Cluster-Wide Average, Others: 13.844 ms (Max: 13.885, Min: 13.820, Sum: 110.749)
****** Breakdown Sum: 903.108 ms ******
Cluster-Wide Average, GPU Memory Consumption: 16.524 GB (Max: 17.239, Min: 16.405, Sum: 132.192)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.620 Gbps (Max: 49.468, Min: 11.503, Sum: 212.957)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.020 GB
Total communication (cluster-wide, per-epoch): 14.502 GB
****** Accuracy Results ******
Highest valid_acc: 0.9405
Target test_acc: 0.9378
Epoch to reach the target acc: 4749
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
