Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...Building the CSR structure...
Building the CSR structure...

Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.874 seconds.
Building the CSC structure...
        It takes 2.191 seconds.
Building the CSC structure...
        It takes 2.374 seconds.
Building the CSC structure...
        It takes 2.379 seconds.
Building the CSC structure...
        It takes 2.389 seconds.
Building the CSC structure...
        It takes 2.488 seconds.
Building the CSC structure...
        It takes 2.598 seconds.
Building the CSC structure...
        It takes 2.653 seconds.
Building the CSC structure...
        It takes 1.808 seconds.
        It takes 2.262 seconds.
        It takes 2.234 seconds.
Building the Feature Vector...
        It takes 2.300 seconds.
        It takes 2.371 seconds.
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 2.335 seconds.
        It takes 0.038 seconds.
        It takes 2.482 seconds.
        It takes 2.347 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
        It takes 0.272 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.045 seconds.
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.241 seconds.
Building the Label Vector...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.032 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.201 Gbps (per GPU), 449.608 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.936 Gbps (per GPU), 447.491 Gbps (aggregated)
The layer-level communication performance: 55.929 Gbps (per GPU), 447.433 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.709 Gbps (per GPU), 445.674 Gbps (aggregated)
The layer-level communication performance: 55.682 Gbps (per GPU), 445.458 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.496 Gbps (per GPU), 443.966 Gbps (aggregated)
The layer-level communication performance: 55.464 Gbps (per GPU), 443.709 Gbps (aggregated)
The layer-level communication performance: 55.429 Gbps (per GPU), 443.433 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.494 Gbps (per GPU), 1267.954 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.482 Gbps (per GPU), 1267.856 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.491 Gbps (per GPU), 1267.927 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.488 Gbps (per GPU), 1267.906 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.494 Gbps (per GPU), 1267.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.491 Gbps (per GPU), 1267.926 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.494 Gbps (per GPU), 1267.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.497 Gbps (per GPU), 1267.975 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.719 Gbps (per GPU), 805.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.718 Gbps (per GPU), 805.745 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.719 Gbps (per GPU), 805.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.718 Gbps (per GPU), 805.745 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.719 Gbps (per GPU), 805.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.718 Gbps (per GPU), 805.745 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.720 Gbps (per GPU), 805.758 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.719 Gbps (per GPU), 805.751 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.928 Gbps (per GPU), 263.426 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.929 Gbps (per GPU), 263.431 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.928 Gbps (per GPU), 263.423 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.928 Gbps (per GPU), 263.424 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.928 Gbps (per GPU), 263.423 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.923 Gbps (per GPU), 263.382 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.928 Gbps (per GPU), 263.422 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.922 Gbps (per GPU), 263.379 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.51ms  9.47ms  9.81ms  3.91 29.12K 14.23M
 chk_1  2.52ms  5.06ms  5.50ms  2.18 29.12K  6.56M
 chk_2  2.52ms 16.51ms 16.89ms  6.71 29.12K 24.68M
 chk_3  2.52ms 16.62ms 16.99ms  6.73 29.12K 22.95M
 chk_4  2.52ms  4.96ms  5.31ms  2.10 29.12K  6.33M
 chk_5  2.53ms  8.90ms  9.18ms  3.63 29.12K 12.05M
 chk_6  2.52ms  9.99ms 10.32ms  4.09 29.12K 14.60M
 chk_7  2.52ms  9.24ms  9.72ms  3.85 29.12K 13.21M
   Avg  2.52 10.09 10.46
   Max  2.53 16.62 16.99
   Min  2.51  4.96  5.31
 Ratio  1.01  3.35  3.20
   Var  0.00 17.24 17.22
Profiling takes 2.151 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 343.148 ms
Partition 0 [0, 5) has cost: 343.148 ms
Partition 1 [5, 9) has cost: 322.984 ms
Partition 2 [9, 13) has cost: 322.984 ms
Partition 3 [13, 17) has cost: 322.984 ms
Partition 4 [17, 21) has cost: 322.984 ms
Partition 5 [21, 25) has cost: 322.984 ms
Partition 6 [25, 29) has cost: 322.984 ms
Partition 7 [29, 33) has cost: 325.948 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 300.734 ms
GPU 0, Compute+Comm Time: 124.132 ms, Bubble Time: 129.416 ms, Imbalance Overhead: 47.187 ms
GPU 1, Compute+Comm Time: 119.086 ms, Bubble Time: 120.204 ms, Imbalance Overhead: 61.444 ms
GPU 2, Compute+Comm Time: 119.086 ms, Bubble Time: 110.370 ms, Imbalance Overhead: 71.279 ms
GPU 3, Compute+Comm Time: 119.086 ms, Bubble Time: 110.800 ms, Imbalance Overhead: 70.848 ms
GPU 4, Compute+Comm Time: 119.086 ms, Bubble Time: 120.222 ms, Imbalance Overhead: 61.426 ms
GPU 5, Compute+Comm Time: 119.086 ms, Bubble Time: 129.012 ms, Imbalance Overhead: 52.636 ms
GPU 6, Compute+Comm Time: 119.086 ms, Bubble Time: 137.738 ms, Imbalance Overhead: 43.911 ms
GPU 7, Compute+Comm Time: 119.791 ms, Bubble Time: 147.667 ms, Imbalance Overhead: 33.276 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 588.161 ms
GPU 0, Compute+Comm Time: 232.686 ms, Bubble Time: 290.786 ms, Imbalance Overhead: 64.689 ms
GPU 1, Compute+Comm Time: 230.428 ms, Bubble Time: 271.675 ms, Imbalance Overhead: 86.058 ms
GPU 2, Compute+Comm Time: 230.428 ms, Bubble Time: 254.221 ms, Imbalance Overhead: 103.511 ms
GPU 3, Compute+Comm Time: 230.428 ms, Bubble Time: 236.497 ms, Imbalance Overhead: 121.235 ms
GPU 4, Compute+Comm Time: 230.428 ms, Bubble Time: 216.883 ms, Imbalance Overhead: 140.850 ms
GPU 5, Compute+Comm Time: 230.428 ms, Bubble Time: 215.607 ms, Imbalance Overhead: 142.125 ms
GPU 6, Compute+Comm Time: 230.428 ms, Bubble Time: 234.364 ms, Imbalance Overhead: 123.369 ms
GPU 7, Compute+Comm Time: 245.546 ms, Bubble Time: 251.236 ms, Imbalance Overhead: 91.379 ms
The estimated cost of the whole pipeline: 933.340 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 666.132 ms
Partition 0 [0, 9) has cost: 666.132 ms
Partition 1 [9, 17) has cost: 645.969 ms
Partition 2 [17, 25) has cost: 645.969 ms
Partition 3 [25, 33) has cost: 648.932 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 310.016 ms
GPU 0, Compute+Comm Time: 160.497 ms, Bubble Time: 149.054 ms, Imbalance Overhead: 0.465 ms
GPU 1, Compute+Comm Time: 157.967 ms, Bubble Time: 130.018 ms, Imbalance Overhead: 22.032 ms
GPU 2, Compute+Comm Time: 157.967 ms, Bubble Time: 110.349 ms, Imbalance Overhead: 41.701 ms
GPU 3, Compute+Comm Time: 158.310 ms, Bubble Time: 110.676 ms, Imbalance Overhead: 41.030 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 592.657 ms
GPU 0, Compute+Comm Time: 300.380 ms, Bubble Time: 211.972 ms, Imbalance Overhead: 80.305 ms
GPU 1, Compute+Comm Time: 299.356 ms, Bubble Time: 211.031 ms, Imbalance Overhead: 82.271 ms
GPU 2, Compute+Comm Time: 299.356 ms, Bubble Time: 248.543 ms, Imbalance Overhead: 44.759 ms
GPU 3, Compute+Comm Time: 306.924 ms, Bubble Time: 284.164 ms, Imbalance Overhead: 1.569 ms
    The estimated cost with 2 DP ways is 947.807 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1312.101 ms
Partition 0 [0, 17) has cost: 1312.101 ms
Partition 1 [17, 33) has cost: 1294.901 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 351.360 ms
GPU 0, Compute+Comm Time: 234.458 ms, Bubble Time: 116.902 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 233.366 ms, Bubble Time: 117.016 ms, Imbalance Overhead: 0.978 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 622.997 ms
GPU 0, Compute+Comm Time: 412.750 ms, Bubble Time: 207.303 ms, Imbalance Overhead: 2.944 ms
GPU 1, Compute+Comm Time: 415.961 ms, Bubble Time: 207.036 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1023.075 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2607.002 ms
Partition 0 [0, 33) has cost: 2607.002 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 493.633 ms
GPU 0, Compute+Comm Time: 493.633 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 674.983 ms
GPU 0, Compute+Comm Time: 674.983 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1227.046 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.7021	TrainAcc 0.6862	ValidAcc 0.7066	TestAcc 0.7001	BestValid 0.7066
	Epoch 100:	Loss 1.1713	TrainAcc 0.7892	ValidAcc 0.8032	TestAcc 0.7979	BestValid 0.8032
	Epoch 150:	Loss 0.9357	TrainAcc 0.8417	ValidAcc 0.8525	TestAcc 0.8466	BestValid 0.8525
	Epoch 200:	Loss 0.8062	TrainAcc 0.8673	ValidAcc 0.8753	TestAcc 0.8704	BestValid 0.8753
	Epoch 250:	Loss 0.7241	TrainAcc 0.8814	ValidAcc 0.8887	TestAcc 0.8832	BestValid 0.8887
	Epoch 300:	Loss 0.6721	TrainAcc 0.8908	ValidAcc 0.8975	TestAcc 0.8924	BestValid 0.8975
	Epoch 350:	Loss 0.6336	TrainAcc 0.8966	ValidAcc 0.9026	TestAcc 0.8985	BestValid 0.9026
	Epoch 400:	Loss 0.6094	TrainAcc 0.9023	ValidAcc 0.9087	TestAcc 0.9039	BestValid 0.9087
	Epoch 450:	Loss 0.5820	TrainAcc 0.9049	ValidAcc 0.9119	TestAcc 0.9064	BestValid 0.9119
	Epoch 500:	Loss 0.5625	TrainAcc 0.9074	ValidAcc 0.9141	TestAcc 0.9092	BestValid 0.9141
	Epoch 550:	Loss 0.5505	TrainAcc 0.9075	ValidAcc 0.9135	TestAcc 0.9089	BestValid 0.9141
	Epoch 600:	Loss 0.5322	TrainAcc 0.9099	ValidAcc 0.9153	TestAcc 0.9113	BestValid 0.9153
	Epoch 650:	Loss 0.5201	TrainAcc 0.9110	ValidAcc 0.9162	TestAcc 0.9119	BestValid 0.9162
	Epoch 700:	Loss 0.5136	TrainAcc 0.9142	ValidAcc 0.9189	TestAcc 0.9145	BestValid 0.9189
	Epoch 750:	Loss 0.5031	TrainAcc 0.9157	ValidAcc 0.9201	TestAcc 0.9156	BestValid 0.9201
	Epoch 800:	Loss 0.4949	TrainAcc 0.9169	ValidAcc 0.9212	TestAcc 0.9170	BestValid 0.9212
	Epoch 850:	Loss 0.4891	TrainAcc 0.9146	ValidAcc 0.9179	TestAcc 0.9147	BestValid 0.9212
	Epoch 900:	Loss 0.4870	TrainAcc 0.9144	ValidAcc 0.9177	TestAcc 0.9148	BestValid 0.9212
	Epoch 950:	Loss 0.4750	TrainAcc 0.9183	ValidAcc 0.9220	TestAcc 0.9185	BestValid 0.9220
	Epoch 1000:	Loss 0.4670	TrainAcc 0.9163	ValidAcc 0.9192	TestAcc 0.9170	BestValid 0.9220
	Epoch 1050:	Loss 0.4649	TrainAcc 0.9167	ValidAcc 0.9195	TestAcc 0.9172	BestValid 0.9220
	Epoch 1100:	Loss 0.4599	TrainAcc 0.9220	ValidAcc 0.9247	TestAcc 0.9220	BestValid 0.9247
	Epoch 1150:	Loss 0.4547	TrainAcc 0.9202	ValidAcc 0.9234	TestAcc 0.9203	BestValid 0.9247
	Epoch 1200:	Loss 0.4511	TrainAcc 0.9239	ValidAcc 0.9270	TestAcc 0.9233	BestValid 0.9270
	Epoch 1250:	Loss 0.4497	TrainAcc 0.9226	ValidAcc 0.9253	TestAcc 0.9223	BestValid 0.9270
	Epoch 1300:	Loss 0.4422	TrainAcc 0.9246	ValidAcc 0.9274	TestAcc 0.9242	BestValid 0.9274
	Epoch 1350:	Loss 0.4396	TrainAcc 0.9249	ValidAcc 0.9269	TestAcc 0.9246	BestValid 0.9274
	Epoch 1400:	Loss 0.4395	TrainAcc 0.9212	ValidAcc 0.9231	TestAcc 0.9202	BestValid 0.9274
	Epoch 1450:	Loss 0.4408	TrainAcc 0.9201	ValidAcc 0.9208	TestAcc 0.9190	BestValid 0.9274
	Epoch 1500:	Loss 0.4271	TrainAcc 0.9256	ValidAcc 0.9280	TestAcc 0.9248	BestValid 0.9280
	Epoch 1550:	Loss 0.4292	TrainAcc 0.9261	ValidAcc 0.9281	TestAcc 0.9253	BestValid 0.9281
	Epoch 1600:	Loss 0.4275	TrainAcc 0.9267	ValidAcc 0.9282	TestAcc 0.9254	BestValid 0.9282
	Epoch 1650:	Loss 0.4294	TrainAcc 0.9209	ValidAcc 0.9227	TestAcc 0.9199	BestValid 0.9282
	Epoch 1700:	Loss 0.4194	TrainAcc 0.9265	ValidAcc 0.9283	TestAcc 0.9259	BestValid 0.9283
	Epoch 1750:	Loss 0.4187	TrainAcc 0.9298	ValidAcc 0.9311	TestAcc 0.9283	BestValid 0.9311
	Epoch 1800:	Loss 0.4197	TrainAcc 0.9269	ValidAcc 0.9286	TestAcc 0.9256	BestValid 0.9311
	Epoch 1850:	Loss 0.4220	TrainAcc 0.9252	ValidAcc 0.9264	TestAcc 0.9239	BestValid 0.9311
	Epoch 1900:	Loss 0.4109	TrainAcc 0.9315	ValidAcc 0.9324	TestAcc 0.9295	BestValid 0.9324
	Epoch 1950:	Loss 0.4116	TrainAcc 0.9292	ValidAcc 0.9302	TestAcc 0.9271	BestValid 0.9324
	Epoch 2000:	Loss 0.4280	TrainAcc 0.9279	ValidAcc 0.9292	TestAcc 0.9264	BestValid 0.9324
	Epoch 2050:	Loss 0.4064	TrainAcc 0.9305	ValidAcc 0.9308	TestAcc 0.9288	BestValid 0.9324
	Epoch 2100:	Loss 0.4159	TrainAcc 0.9257	ValidAcc 0.9266	TestAcc 0.9237	BestValid 0.9324
	Epoch 2150:	Loss 0.4019	TrainAcc 0.9288	ValidAcc 0.9295	TestAcc 0.9269	BestValid 0.9324
	Epoch 2200:	Loss 0.4024	TrainAcc 0.9298	ValidAcc 0.9302	TestAcc 0.9278	BestValid 0.9324
	Epoch 2250:	Loss 0.3985	TrainAcc 0.9302	ValidAcc 0.9305	TestAcc 0.9283	BestValid 0.9324
	Epoch 2300:	Loss 0.3985	TrainAcc 0.9352	ValidAcc 0.9356	TestAcc 0.9329	BestValid 0.9356
	Epoch 2350:	Loss 0.3964	TrainAcc 0.9276	ValidAcc 0.9280	TestAcc 0.9259	BestValid 0.9356
	Epoch 2400:	Loss 0.3926	TrainAcc 0.9361	ValidAcc 0.9366	TestAcc 0.9335	BestValid 0.9366
	Epoch 2450:	Loss 0.4004	TrainAcc 0.9293	ValidAcc 0.9292	TestAcc 0.9270	BestValid 0.9366
	Epoch 2500:	Loss 0.4020	TrainAcc 0.9321	ValidAcc 0.9320	TestAcc 0.9295	BestValid 0.9366
	Epoch 2550:	Loss 0.3883	TrainAcc 0.9330	ValidAcc 0.9329	TestAcc 0.9305	BestValid 0.9366
	Epoch 2600:	Loss 0.3968	TrainAcc 0.9341	ValidAcc 0.9340	TestAcc 0.9316	BestValid 0.9366
	Epoch 2650:	Loss 0.3909	TrainAcc 0.9266	ValidAcc 0.9259	TestAcc 0.9245	BestValid 0.9366
	Epoch 2700:	Loss 0.3920	TrainAcc 0.9303	ValidAcc 0.9302	TestAcc 0.9281	BestValid 0.9366
	Epoch 2750:	Loss 0.3839	TrainAcc 0.9349	ValidAcc 0.9343	TestAcc 0.9322	BestValid 0.9366
	Epoch 2800:	Loss 0.3858	TrainAcc 0.9355	ValidAcc 0.9355	TestAcc 0.9329	BestValid 0.9366
	Epoch 2850:	Loss 0.3805	TrainAcc 0.9342	ValidAcc 0.9341	TestAcc 0.9317	BestValid 0.9366
	Epoch 2900:	Loss 0.3823	TrainAcc 0.9265	ValidAcc 0.9267	TestAcc 0.9245	BestValid 0.9366
	Epoch 2950:	Loss 0.3763	TrainAcc 0.9354	ValidAcc 0.9353	TestAcc 0.9328	BestValid 0.9366
	Epoch 3000:	Loss 0.3777	TrainAcc 0.9303	ValidAcc 0.9298	TestAcc 0.9280	BestValid 0.9366
	Epoch 3050:	Loss 0.3774	TrainAcc 0.9361	ValidAcc 0.9356	TestAcc 0.9332	BestValid 0.9366
	Epoch 3100:	Loss 0.3719	TrainAcc 0.9359	ValidAcc 0.9357	TestAcc 0.9330	BestValid 0.9366
	Epoch 3150:	Loss 0.3721	TrainAcc 0.9365	ValidAcc 0.9360	TestAcc 0.9336	BestValid 0.9366
	Epoch 3200:	Loss 0.3721	TrainAcc 0.9331	ValidAcc 0.9324	TestAcc 0.9304	BestValid 0.9366
	Epoch 3250:	Loss 0.3696	TrainAcc 0.9382	ValidAcc 0.9376	TestAcc 0.9352	BestValid 0.9376
	Epoch 3300:	Loss 0.3679	TrainAcc 0.9372	ValidAcc 0.9375	TestAcc 0.9341	BestValid 0.9376
	Epoch 3350:	Loss 0.3659	TrainAcc 0.9351	ValidAcc 0.9343	TestAcc 0.9322	BestValid 0.9376
	Epoch 3400:	Loss 0.3684	TrainAcc 0.9357	ValidAcc 0.9349	TestAcc 0.9328	BestValid 0.9376
	Epoch 3450:	Loss 0.3636	TrainAcc 0.9321	ValidAcc 0.9319	TestAcc 0.9296	BestValid 0.9376
	Epoch 3500:	Loss 0.3625	TrainAcc 0.9323	ValidAcc 0.9315	TestAcc 0.9297	BestValid 0.9376
	Epoch 3550:	Loss 0.3646	TrainAcc 0.9340	ValidAcc 0.9332	TestAcc 0.9312	BestValid 0.9376
	Epoch 3600:	Loss 0.3653	TrainAcc 0.9312	ValidAcc 0.9308	TestAcc 0.9289	BestValid 0.9376
	Epoch 3650:	Loss 0.3647	TrainAcc 0.9375	ValidAcc 0.9370	TestAcc 0.9345	BestValid 0.9376
	Epoch 3700:	Loss 0.3661	TrainAcc 0.9403	ValidAcc 0.9395	TestAcc 0.9375	BestValid 0.9395
	Epoch 3750:	Loss 0.3594	TrainAcc 0.9367	ValidAcc 0.9362	TestAcc 0.9338	BestValid 0.9395
	Epoch 3800:	Loss 0.3593	TrainAcc 0.9369	ValidAcc 0.9361	TestAcc 0.9337	BestValid 0.9395
	Epoch 3850:	Loss 0.3615	TrainAcc 0.9394	ValidAcc 0.9384	TestAcc 0.9359	BestValid 0.9395
	Epoch 3900:	Loss 0.3556	TrainAcc 0.9383	ValidAcc 0.9379	TestAcc 0.9350	BestValid 0.9395
	Epoch 3950:	Loss 0.3600	TrainAcc 0.9371	ValidAcc 0.9367	TestAcc 0.9341	BestValid 0.9395
	Epoch 4000:	Loss 0.3557	TrainAcc 0.9375	ValidAcc 0.9371	TestAcc 0.9342	BestValid 0.9395
	Epoch 4050:	Loss 0.3569	TrainAcc 0.9398	ValidAcc 0.9391	TestAcc 0.9363	BestValid 0.9395
	Epoch 4100:	Loss 0.3528	TrainAcc 0.9364	ValidAcc 0.9359	TestAcc 0.9330	BestValid 0.9395
	Epoch 4150:	Loss 0.3572	TrainAcc 0.9377	ValidAcc 0.9371	TestAcc 0.9343	BestValid 0.9395
	Epoch 4200:	Loss 0.3512	TrainAcc 0.9402	ValidAcc 0.9398	TestAcc 0.9369	BestValid 0.9398
	Epoch 4250:	Loss 0.3509	TrainAcc 0.9409	ValidAcc 0.9402	TestAcc 0.9376	BestValid 0.9402
	Epoch 4300:	Loss 0.3499	TrainAcc 0.9389	ValidAcc 0.9382	TestAcc 0.9356	BestValid 0.9402
	Epoch 4350:	Loss 0.3520	TrainAcc 0.9399	ValidAcc 0.9390	TestAcc 0.9365	BestValid 0.9402
	Epoch 4400:	Loss 0.3510	TrainAcc 0.9364	ValidAcc 0.9353	TestAcc 0.9336	BestValid 0.9402
	Epoch 4450:	Loss 0.3505	TrainAcc 0.9400	ValidAcc 0.9389	TestAcc 0.9371	BestValid 0.9402
	Epoch 4500:	Loss 0.3491	TrainAcc 0.9385	ValidAcc 0.9381	TestAcc 0.9351	BestValid 0.9402
	Epoch 4550:	Loss 0.3529	TrainAcc 0.9396	ValidAcc 0.9386	TestAcc 0.9363	BestValid 0.9402
	Epoch 4600:	Loss 0.3455	TrainAcc 0.9400	ValidAcc 0.9394	TestAcc 0.9364	BestValid 0.9402
	Epoch 4650:	Loss 0.3506	TrainAcc 0.9379	ValidAcc 0.9366	TestAcc 0.9346	BestValid 0.9402
	Epoch 4700:	Loss 0.3455	TrainAcc 0.9388	ValidAcc 0.9385	TestAcc 0.9354	BestValid 0.9402
	Epoch 4750:	Loss 0.3498	TrainAcc 0.9415	ValidAcc 0.9405	TestAcc 0.9378	BestValid 0.9405
	Epoch 4800:	Loss 0.3517	TrainAcc 0.9350	ValidAcc 0.9334	TestAcc 0.9317	BestValid 0.9405
	Epoch 4850:	Loss 0.3397	TrainAcc 0.9382	ValidAcc 0.9366	TestAcc 0.9344	BestValid 0.9405
	Epoch 4900:	Loss 0.3447	TrainAcc 0.9360	ValidAcc 0.9346	TestAcc 0.9323	BestValid 0.9405
	Epoch 4950:	Loss 0.3436	TrainAcc 0.9402	ValidAcc 0.9394	TestAcc 0.9369	BestValid 0.9405
	Epoch 5000:	Loss 0.3431	TrainAcc 0.9380	ValidAcc 0.9366	TestAcc 0.9345	BestValid 0.9405
****** Epoch Time (Excluding Evaluation Cost): 0.902 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.939 ms (Max: 1.335, Min: 0.101, Sum: 7.510)
Cluster-Wide Average, Compute: 247.354 ms (Max: 385.591, Min: 140.614, Sum: 1978.835)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.011, Min: 0.008, Sum: 0.076)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.019, Min: 0.015, Sum: 0.138)
Cluster-Wide Average, Communication-Graph: 636.985 ms (Max: 743.406, Min: 499.512, Sum: 5095.882)
Cluster-Wide Average, Optimization: 3.241 ms (Max: 3.275, Min: 3.217, Sum: 25.928)
Cluster-Wide Average, Others: 13.849 ms (Max: 13.890, Min: 13.817, Sum: 110.793)
****** Breakdown Sum: 902.395 ms ******
Cluster-Wide Average, GPU Memory Consumption: 16.524 GB (Max: 17.239, Min: 16.405, Sum: 132.192)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.614 Gbps (Max: 49.456, Min: 11.507, Sum: 212.914)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.020 GB
Total communication (cluster-wide, per-epoch): 14.502 GB
****** Accuracy Results ******
Highest valid_acc: 0.9405
Target test_acc: 0.9378
Epoch to reach the target acc: 4749
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
