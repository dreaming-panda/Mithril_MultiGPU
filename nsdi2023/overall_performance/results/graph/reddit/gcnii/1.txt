Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.885 seconds.
Building the CSC structure...
        It takes 1.977 seconds.
Building the CSC structure...
        It takes 2.379 seconds.
Building the CSC structure...
        It takes 2.401 seconds.
Building the CSC structure...
        It takes 2.420 seconds.
Building the CSC structure...
        It takes 2.580 seconds.
Building the CSC structure...
        It takes 2.639 seconds.
Building the CSC structure...
        It takes 2.660 seconds.
Building the CSC structure...
        It takes 1.784 seconds.
        It takes 1.870 seconds.
Building the Feature Vector...
        It takes 2.326 seconds.
        It takes 2.336 seconds.
        It takes 2.382 seconds.
        It takes 0.247 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 2.407 seconds.
        It takes 2.466 seconds.
        It takes 2.476 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.287 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
        It takes 0.305 seconds.
Building the Label Vector...
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.035 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.283 seconds.
Building the Label Vector...
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
        It takes 0.031 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.737 Gbps (per GPU), 445.892 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.487 Gbps (per GPU), 443.893 Gbps (aggregated)
The layer-level communication performance: 55.475 Gbps (per GPU), 443.802 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.243 Gbps (per GPU), 441.947 Gbps (aggregated)
The layer-level communication performance: 55.274 Gbps (per GPU), 442.196 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.044 Gbps (per GPU), 440.350 Gbps (aggregated)
The layer-level communication performance: 55.009 Gbps (per GPU), 440.070 Gbps (aggregated)
The layer-level communication performance: 54.976 Gbps (per GPU), 439.808 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.615 Gbps (per GPU), 1260.923 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.630 Gbps (per GPU), 1261.042 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.618 Gbps (per GPU), 1260.947 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.604 Gbps (per GPU), 1260.829 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.621 Gbps (per GPU), 1260.971 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.642 Gbps (per GPU), 1261.135 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.621 Gbps (per GPU), 1260.971 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.612 Gbps (per GPU), 1260.900 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.817 Gbps (per GPU), 806.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.817 Gbps (per GPU), 806.540 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.818 Gbps (per GPU), 806.545 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.819 Gbps (per GPU), 806.552 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.818 Gbps (per GPU), 806.545 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.818 Gbps (per GPU), 806.545 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.817 Gbps (per GPU), 806.533 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.819 Gbps (per GPU), 806.552 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.558 Gbps (per GPU), 292.468 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.556 Gbps (per GPU), 292.452 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.557 Gbps (per GPU), 292.459 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.558 Gbps (per GPU), 292.461 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.558 Gbps (per GPU), 292.461 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.557 Gbps (per GPU), 292.459 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.558 Gbps (per GPU), 292.463 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.556 Gbps (per GPU), 292.452 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.50ms  9.14ms  9.80ms  3.91 29.12K 14.23M
 chk_1  2.52ms  5.12ms  5.47ms  2.17 29.12K  6.56M
 chk_2  2.53ms 16.51ms 16.88ms  6.69 29.12K 24.68M
 chk_3  2.53ms 16.58ms 16.97ms  6.72 29.12K 22.95M
 chk_4  2.53ms  4.93ms  5.30ms  2.10 29.12K  6.33M
 chk_5  2.53ms  9.06ms  9.42ms  3.73 29.12K 12.05M
 chk_6  2.53ms 10.05ms 10.30ms  4.08 29.12K 14.60M
 chk_7  2.53ms  9.24ms  9.72ms  3.85 29.12K 13.21M
   Avg  2.52 10.08 10.48
   Max  2.53 16.58 16.97
   Min  2.50  4.93  5.30
 Ratio  1.01  3.36  3.20
   Var  0.00 17.17 17.16
Profiling takes 2.148 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 342.731 ms
Partition 0 [0, 5) has cost: 342.731 ms
Partition 1 [5, 9) has cost: 322.554 ms
Partition 2 [9, 13) has cost: 322.554 ms
Partition 3 [13, 17) has cost: 322.554 ms
Partition 4 [17, 21) has cost: 322.554 ms
Partition 5 [21, 25) has cost: 322.554 ms
Partition 6 [25, 29) has cost: 322.554 ms
Partition 7 [29, 33) has cost: 325.777 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 300.365 ms
GPU 0, Compute+Comm Time: 124.126 ms, Bubble Time: 129.829 ms, Imbalance Overhead: 46.410 ms
GPU 1, Compute+Comm Time: 119.072 ms, Bubble Time: 120.151 ms, Imbalance Overhead: 61.142 ms
GPU 2, Compute+Comm Time: 119.072 ms, Bubble Time: 109.847 ms, Imbalance Overhead: 71.446 ms
GPU 3, Compute+Comm Time: 119.072 ms, Bubble Time: 110.229 ms, Imbalance Overhead: 71.063 ms
GPU 4, Compute+Comm Time: 119.072 ms, Bubble Time: 119.596 ms, Imbalance Overhead: 61.697 ms
GPU 5, Compute+Comm Time: 119.072 ms, Bubble Time: 128.329 ms, Imbalance Overhead: 52.963 ms
GPU 6, Compute+Comm Time: 119.072 ms, Bubble Time: 137.012 ms, Imbalance Overhead: 44.281 ms
GPU 7, Compute+Comm Time: 119.952 ms, Bubble Time: 146.867 ms, Imbalance Overhead: 33.546 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 585.894 ms
GPU 0, Compute+Comm Time: 232.575 ms, Bubble Time: 288.520 ms, Imbalance Overhead: 64.799 ms
GPU 1, Compute+Comm Time: 230.232 ms, Bubble Time: 269.515 ms, Imbalance Overhead: 86.147 ms
GPU 2, Compute+Comm Time: 230.232 ms, Bubble Time: 252.347 ms, Imbalance Overhead: 103.315 ms
GPU 3, Compute+Comm Time: 230.232 ms, Bubble Time: 234.981 ms, Imbalance Overhead: 120.681 ms
GPU 4, Compute+Comm Time: 230.232 ms, Bubble Time: 215.723 ms, Imbalance Overhead: 139.939 ms
GPU 5, Compute+Comm Time: 230.232 ms, Bubble Time: 214.229 ms, Imbalance Overhead: 141.434 ms
GPU 6, Compute+Comm Time: 230.232 ms, Bubble Time: 233.814 ms, Imbalance Overhead: 121.848 ms
GPU 7, Compute+Comm Time: 245.356 ms, Bubble Time: 251.386 ms, Imbalance Overhead: 89.152 ms
The estimated cost of the whole pipeline: 930.572 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 665.285 ms
Partition 0 [0, 9) has cost: 665.285 ms
Partition 1 [9, 17) has cost: 645.108 ms
Partition 2 [17, 25) has cost: 645.108 ms
Partition 3 [25, 33) has cost: 648.331 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 309.169 ms
GPU 0, Compute+Comm Time: 159.910 ms, Bubble Time: 149.259 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 157.377 ms, Bubble Time: 129.284 ms, Imbalance Overhead: 22.508 ms
GPU 2, Compute+Comm Time: 157.377 ms, Bubble Time: 109.287 ms, Imbalance Overhead: 42.505 ms
GPU 3, Compute+Comm Time: 157.914 ms, Bubble Time: 109.574 ms, Imbalance Overhead: 41.681 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 589.450 ms
GPU 0, Compute+Comm Time: 299.855 ms, Bubble Time: 209.954 ms, Imbalance Overhead: 79.641 ms
GPU 1, Compute+Comm Time: 298.614 ms, Bubble Time: 208.617 ms, Imbalance Overhead: 82.220 ms
GPU 2, Compute+Comm Time: 298.614 ms, Bubble Time: 246.253 ms, Imbalance Overhead: 44.583 ms
GPU 3, Compute+Comm Time: 306.187 ms, Bubble Time: 283.263 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 943.550 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1310.393 ms
Partition 0 [0, 17) has cost: 1310.393 ms
Partition 1 [17, 33) has cost: 1293.439 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 351.607 ms
GPU 0, Compute+Comm Time: 234.596 ms, Bubble Time: 117.011 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 233.559 ms, Bubble Time: 117.103 ms, Imbalance Overhead: 0.944 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 621.806 ms
GPU 0, Compute+Comm Time: 412.175 ms, Bubble Time: 207.401 ms, Imbalance Overhead: 2.229 ms
GPU 1, Compute+Comm Time: 415.429 ms, Bubble Time: 206.377 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1022.083 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2603.831 ms
Partition 0 [0, 33) has cost: 2603.831 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 462.264 ms
GPU 0, Compute+Comm Time: 462.264 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 642.174 ms
GPU 0, Compute+Comm Time: 642.174 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1159.659 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.7023	TrainAcc 0.6900	ValidAcc 0.7117	TestAcc 0.7060	BestValid 0.7117
	Epoch 100:	Loss 1.1560	TrainAcc 0.8004	ValidAcc 0.8135	TestAcc 0.8086	BestValid 0.8135
	Epoch 150:	Loss 0.9211	TrainAcc 0.8443	ValidAcc 0.8540	TestAcc 0.8497	BestValid 0.8540
	Epoch 200:	Loss 0.7920	TrainAcc 0.8682	ValidAcc 0.8765	TestAcc 0.8712	BestValid 0.8765
	Epoch 250:	Loss 0.7178	TrainAcc 0.8860	ValidAcc 0.8922	TestAcc 0.8873	BestValid 0.8922
	Epoch 300:	Loss 0.6625	TrainAcc 0.8949	ValidAcc 0.9012	TestAcc 0.8967	BestValid 0.9012
	Epoch 350:	Loss 0.6259	TrainAcc 0.9015	ValidAcc 0.9074	TestAcc 0.9027	BestValid 0.9074
	Epoch 400:	Loss 0.5983	TrainAcc 0.9029	ValidAcc 0.9091	TestAcc 0.9030	BestValid 0.9091
	Epoch 450:	Loss 0.5727	TrainAcc 0.9080	ValidAcc 0.9146	TestAcc 0.9086	BestValid 0.9146
	Epoch 500:	Loss 0.5558	TrainAcc 0.9097	ValidAcc 0.9151	TestAcc 0.9097	BestValid 0.9151
	Epoch 550:	Loss 0.5390	TrainAcc 0.9135	ValidAcc 0.9191	TestAcc 0.9136	BestValid 0.9191
	Epoch 600:	Loss 0.5282	TrainAcc 0.9160	ValidAcc 0.9218	TestAcc 0.9161	BestValid 0.9218
	Epoch 650:	Loss 0.5184	TrainAcc 0.9177	ValidAcc 0.9235	TestAcc 0.9174	BestValid 0.9235
	Epoch 700:	Loss 0.5065	TrainAcc 0.9188	ValidAcc 0.9247	TestAcc 0.9186	BestValid 0.9247
	Epoch 750:	Loss 0.4954	TrainAcc 0.9166	ValidAcc 0.9215	TestAcc 0.9161	BestValid 0.9247
	Epoch 800:	Loss 0.4910	TrainAcc 0.9202	ValidAcc 0.9256	TestAcc 0.9196	BestValid 0.9256
	Epoch 850:	Loss 0.4821	TrainAcc 0.9235	ValidAcc 0.9292	TestAcc 0.9233	BestValid 0.9292
	Epoch 900:	Loss 0.4794	TrainAcc 0.9239	ValidAcc 0.9285	TestAcc 0.9235	BestValid 0.9292
	Epoch 950:	Loss 0.4723	TrainAcc 0.9229	ValidAcc 0.9271	TestAcc 0.9224	BestValid 0.9292
	Epoch 1000:	Loss 0.4642	TrainAcc 0.9249	ValidAcc 0.9296	TestAcc 0.9243	BestValid 0.9296
	Epoch 1050:	Loss 0.4622	TrainAcc 0.9193	ValidAcc 0.9235	TestAcc 0.9190	BestValid 0.9296
	Epoch 1100:	Loss 0.4513	TrainAcc 0.9235	ValidAcc 0.9276	TestAcc 0.9229	BestValid 0.9296
	Epoch 1150:	Loss 0.4475	TrainAcc 0.9218	ValidAcc 0.9253	TestAcc 0.9209	BestValid 0.9296
	Epoch 1200:	Loss 0.4564	TrainAcc 0.9272	ValidAcc 0.9303	TestAcc 0.9261	BestValid 0.9303
	Epoch 1250:	Loss 0.4411	TrainAcc 0.9287	ValidAcc 0.9318	TestAcc 0.9273	BestValid 0.9318
	Epoch 1300:	Loss 0.4397	TrainAcc 0.9282	ValidAcc 0.9313	TestAcc 0.9272	BestValid 0.9318
	Epoch 1350:	Loss 0.4345	TrainAcc 0.9270	ValidAcc 0.9301	TestAcc 0.9259	BestValid 0.9318
	Epoch 1400:	Loss 0.4318	TrainAcc 0.9287	ValidAcc 0.9318	TestAcc 0.9276	BestValid 0.9318
	Epoch 1450:	Loss 0.4271	TrainAcc 0.9320	ValidAcc 0.9340	TestAcc 0.9307	BestValid 0.9340
	Epoch 1500:	Loss 0.4242	TrainAcc 0.9272	ValidAcc 0.9296	TestAcc 0.9256	BestValid 0.9340
	Epoch 1550:	Loss 0.4207	TrainAcc 0.9310	ValidAcc 0.9328	TestAcc 0.9297	BestValid 0.9340
	Epoch 1600:	Loss 0.4202	TrainAcc 0.9288	ValidAcc 0.9306	TestAcc 0.9274	BestValid 0.9340
	Epoch 1650:	Loss 0.4160	TrainAcc 0.9305	ValidAcc 0.9324	TestAcc 0.9284	BestValid 0.9340
	Epoch 1700:	Loss 0.4133	TrainAcc 0.9316	ValidAcc 0.9333	TestAcc 0.9299	BestValid 0.9340
	Epoch 1750:	Loss 0.4171	TrainAcc 0.9345	ValidAcc 0.9352	TestAcc 0.9329	BestValid 0.9352
	Epoch 1800:	Loss 0.4091	TrainAcc 0.9318	ValidAcc 0.9334	TestAcc 0.9296	BestValid 0.9352
	Epoch 1850:	Loss 0.4157	TrainAcc 0.9334	ValidAcc 0.9347	TestAcc 0.9313	BestValid 0.9352
	Epoch 1900:	Loss 0.4041	TrainAcc 0.9323	ValidAcc 0.9335	TestAcc 0.9299	BestValid 0.9352
	Epoch 1950:	Loss 0.4025	TrainAcc 0.9302	ValidAcc 0.9311	TestAcc 0.9277	BestValid 0.9352
	Epoch 2000:	Loss 0.4017	TrainAcc 0.9301	ValidAcc 0.9316	TestAcc 0.9277	BestValid 0.9352
	Epoch 2050:	Loss 0.3980	TrainAcc 0.9330	ValidAcc 0.9342	TestAcc 0.9305	BestValid 0.9352
	Epoch 2100:	Loss 0.3970	TrainAcc 0.9332	ValidAcc 0.9347	TestAcc 0.9306	BestValid 0.9352
	Epoch 2150:	Loss 0.3961	TrainAcc 0.9327	ValidAcc 0.9342	TestAcc 0.9301	BestValid 0.9352
	Epoch 2200:	Loss 0.3966	TrainAcc 0.9350	ValidAcc 0.9361	TestAcc 0.9326	BestValid 0.9361
	Epoch 2250:	Loss 0.3903	TrainAcc 0.9353	ValidAcc 0.9366	TestAcc 0.9328	BestValid 0.9366
	Epoch 2300:	Loss 0.3924	TrainAcc 0.9327	ValidAcc 0.9340	TestAcc 0.9298	BestValid 0.9366
	Epoch 2350:	Loss 0.3886	TrainAcc 0.9338	ValidAcc 0.9349	TestAcc 0.9309	BestValid 0.9366
	Epoch 2400:	Loss 0.3849	TrainAcc 0.9311	ValidAcc 0.9321	TestAcc 0.9285	BestValid 0.9366
	Epoch 2450:	Loss 0.3883	TrainAcc 0.9352	ValidAcc 0.9362	TestAcc 0.9327	BestValid 0.9366
	Epoch 2500:	Loss 0.3825	TrainAcc 0.9358	ValidAcc 0.9365	TestAcc 0.9331	BestValid 0.9366
	Epoch 2550:	Loss 0.3918	TrainAcc 0.9333	ValidAcc 0.9336	TestAcc 0.9302	BestValid 0.9366
	Epoch 2600:	Loss 0.3789	TrainAcc 0.9356	ValidAcc 0.9361	TestAcc 0.9326	BestValid 0.9366
	Epoch 2650:	Loss 0.3793	TrainAcc 0.9350	ValidAcc 0.9356	TestAcc 0.9321	BestValid 0.9366
	Epoch 2700:	Loss 0.3782	TrainAcc 0.9359	ValidAcc 0.9360	TestAcc 0.9329	BestValid 0.9366
	Epoch 2750:	Loss 0.3755	TrainAcc 0.9357	ValidAcc 0.9363	TestAcc 0.9329	BestValid 0.9366
	Epoch 2800:	Loss 0.3771	TrainAcc 0.9370	ValidAcc 0.9373	TestAcc 0.9340	BestValid 0.9373
	Epoch 2850:	Loss 0.3721	TrainAcc 0.9365	ValidAcc 0.9371	TestAcc 0.9338	BestValid 0.9373
	Epoch 2900:	Loss 0.3701	TrainAcc 0.9366	ValidAcc 0.9368	TestAcc 0.9336	BestValid 0.9373
	Epoch 2950:	Loss 0.3771	TrainAcc 0.9395	ValidAcc 0.9400	TestAcc 0.9369	BestValid 0.9400
	Epoch 3000:	Loss 0.3758	TrainAcc 0.9346	ValidAcc 0.9353	TestAcc 0.9308	BestValid 0.9400
	Epoch 3050:	Loss 0.3676	TrainAcc 0.9352	ValidAcc 0.9351	TestAcc 0.9314	BestValid 0.9400
	Epoch 3100:	Loss 0.3666	TrainAcc 0.9340	ValidAcc 0.9339	TestAcc 0.9306	BestValid 0.9400
	Epoch 3150:	Loss 0.3668	TrainAcc 0.9357	ValidAcc 0.9354	TestAcc 0.9321	BestValid 0.9400
	Epoch 3200:	Loss 0.3668	TrainAcc 0.9388	ValidAcc 0.9390	TestAcc 0.9354	BestValid 0.9400
	Epoch 3250:	Loss 0.3614	TrainAcc 0.9391	ValidAcc 0.9394	TestAcc 0.9358	BestValid 0.9400
	Epoch 3300:	Loss 0.3701	TrainAcc 0.9384	ValidAcc 0.9382	TestAcc 0.9352	BestValid 0.9400
	Epoch 3350:	Loss 0.3624	TrainAcc 0.9376	ValidAcc 0.9379	TestAcc 0.9343	BestValid 0.9400
	Epoch 3400:	Loss 0.3611	TrainAcc 0.9387	ValidAcc 0.9390	TestAcc 0.9352	BestValid 0.9400
	Epoch 3450:	Loss 0.3615	TrainAcc 0.9398	ValidAcc 0.9397	TestAcc 0.9364	BestValid 0.9400
	Epoch 3500:	Loss 0.3609	TrainAcc 0.9384	ValidAcc 0.9384	TestAcc 0.9348	BestValid 0.9400
	Epoch 3550:	Loss 0.3678	TrainAcc 0.9422	ValidAcc 0.9420	TestAcc 0.9387	BestValid 0.9420
	Epoch 3600:	Loss 0.3779	TrainAcc 0.9412	ValidAcc 0.9412	TestAcc 0.9377	BestValid 0.9420
	Epoch 3650:	Loss 0.3566	TrainAcc 0.9390	ValidAcc 0.9391	TestAcc 0.9354	BestValid 0.9420
	Epoch 3700:	Loss 0.3648	TrainAcc 0.9376	ValidAcc 0.9376	TestAcc 0.9334	BestValid 0.9420
	Epoch 3750:	Loss 0.3550	TrainAcc 0.9374	ValidAcc 0.9366	TestAcc 0.9334	BestValid 0.9420
	Epoch 3800:	Loss 0.3558	TrainAcc 0.9397	ValidAcc 0.9393	TestAcc 0.9360	BestValid 0.9420
	Epoch 3850:	Loss 0.3527	TrainAcc 0.9375	ValidAcc 0.9373	TestAcc 0.9336	BestValid 0.9420
	Epoch 3900:	Loss 0.3546	TrainAcc 0.9391	ValidAcc 0.9389	TestAcc 0.9355	BestValid 0.9420
	Epoch 3950:	Loss 0.3492	TrainAcc 0.9395	ValidAcc 0.9396	TestAcc 0.9358	BestValid 0.9420
	Epoch 4000:	Loss 0.3547	TrainAcc 0.9410	ValidAcc 0.9408	TestAcc 0.9372	BestValid 0.9420
	Epoch 4050:	Loss 0.3510	TrainAcc 0.9414	ValidAcc 0.9413	TestAcc 0.9378	BestValid 0.9420
	Epoch 4100:	Loss 0.3570	TrainAcc 0.9430	ValidAcc 0.9426	TestAcc 0.9395	BestValid 0.9426
	Epoch 4150:	Loss 0.3504	TrainAcc 0.9421	ValidAcc 0.9418	TestAcc 0.9382	BestValid 0.9426
	Epoch 4200:	Loss 0.3462	TrainAcc 0.9391	ValidAcc 0.9390	TestAcc 0.9351	BestValid 0.9426
	Epoch 4250:	Loss 0.3469	TrainAcc 0.9404	ValidAcc 0.9394	TestAcc 0.9365	BestValid 0.9426
	Epoch 4300:	Loss 0.3468	TrainAcc 0.9420	ValidAcc 0.9413	TestAcc 0.9380	BestValid 0.9426
	Epoch 4350:	Loss 0.3464	TrainAcc 0.9417	ValidAcc 0.9415	TestAcc 0.9379	BestValid 0.9426
	Epoch 4400:	Loss 0.3484	TrainAcc 0.9360	ValidAcc 0.9355	TestAcc 0.9319	BestValid 0.9426
	Epoch 4450:	Loss 0.3468	TrainAcc 0.9415	ValidAcc 0.9417	TestAcc 0.9378	BestValid 0.9426
	Epoch 4500:	Loss 0.3408	TrainAcc 0.9421	ValidAcc 0.9421	TestAcc 0.9384	BestValid 0.9426
	Epoch 4550:	Loss 0.3484	TrainAcc 0.9392	ValidAcc 0.9384	TestAcc 0.9350	BestValid 0.9426
	Epoch 4600:	Loss 0.3436	TrainAcc 0.9436	ValidAcc 0.9429	TestAcc 0.9391	BestValid 0.9429
	Epoch 4650:	Loss 0.3421	TrainAcc 0.9403	ValidAcc 0.9399	TestAcc 0.9362	BestValid 0.9429
	Epoch 4700:	Loss 0.3419	TrainAcc 0.9456	ValidAcc 0.9448	TestAcc 0.9415	BestValid 0.9448
	Epoch 4750:	Loss 0.3419	TrainAcc 0.9398	ValidAcc 0.9389	TestAcc 0.9355	BestValid 0.9448
	Epoch 4800:	Loss 0.3363	TrainAcc 0.9429	ValidAcc 0.9424	TestAcc 0.9385	BestValid 0.9448
	Epoch 4850:	Loss 0.3409	TrainAcc 0.9421	ValidAcc 0.9420	TestAcc 0.9381	BestValid 0.9448
	Epoch 4900:	Loss 0.3353	TrainAcc 0.9394	ValidAcc 0.9389	TestAcc 0.9349	BestValid 0.9448
	Epoch 4950:	Loss 0.3402	TrainAcc 0.9441	ValidAcc 0.9432	TestAcc 0.9396	BestValid 0.9448
	Epoch 5000:	Loss 0.3408	TrainAcc 0.9380	ValidAcc 0.9366	TestAcc 0.9337	BestValid 0.9448
****** Epoch Time (Excluding Evaluation Cost): 0.901 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.914 ms (Max: 1.289, Min: 0.101, Sum: 7.315)
Cluster-Wide Average, Compute: 247.269 ms (Max: 385.348, Min: 140.385, Sum: 1978.151)
Cluster-Wide Average, Communication-Layer: 0.010 ms (Max: 0.011, Min: 0.008, Sum: 0.076)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.019, Min: 0.016, Sum: 0.139)
Cluster-Wide Average, Communication-Graph: 635.291 ms (Max: 741.854, Min: 497.948, Sum: 5082.330)
Cluster-Wide Average, Optimization: 3.242 ms (Max: 3.276, Min: 3.226, Sum: 25.939)
Cluster-Wide Average, Others: 13.851 ms (Max: 13.888, Min: 13.831, Sum: 110.812)
****** Breakdown Sum: 900.595 ms ******
Cluster-Wide Average, GPU Memory Consumption: 16.524 GB (Max: 17.239, Min: 16.405, Sum: 132.192)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.689 Gbps (Max: 49.624, Min: 11.531, Sum: 213.510)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.020 GB
Total communication (cluster-wide, per-epoch): 14.502 GB
****** Accuracy Results ******
Highest valid_acc: 0.9448
Target test_acc: 0.9415
Epoch to reach the target acc: 4699
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
