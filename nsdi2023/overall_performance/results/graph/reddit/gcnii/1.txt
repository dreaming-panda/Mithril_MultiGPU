Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.869 seconds.
Building the CSC structure...
        It takes 1.876 seconds.
Building the CSC structure...
        It takes 2.045 seconds.
Building the CSC structure...
        It takes 2.242 seconds.
Building the CSC structure...
        It takes 2.446 seconds.
Building the CSC structure...
        It takes 2.511 seconds.
Building the CSC structure...
        It takes 2.612 seconds.
Building the CSC structure...
        It takes 2.632 seconds.
Building the CSC structure...
        It takes 1.786 seconds.
        It takes 1.808 seconds.
        It takes 1.853 seconds.
        It takes 2.196 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.413 seconds.
        It takes 2.350 seconds.
        It takes 0.247 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 2.319 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 2.364 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.252 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.688 Gbps (per GPU), 453.501 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.423 Gbps (per GPU), 451.384 Gbps (aggregated)
The layer-level communication performance: 56.415 Gbps (per GPU), 451.322 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.203 Gbps (per GPU), 449.625 Gbps (aggregated)
The layer-level communication performance: 56.168 Gbps (per GPU), 449.344 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.978 Gbps (per GPU), 447.820 Gbps (aggregated)
The layer-level communication performance: 55.945 Gbps (per GPU), 447.557 Gbps (aggregated)
The layer-level communication performance: 55.907 Gbps (per GPU), 447.258 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.320 Gbps (per GPU), 1250.559 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.318 Gbps (per GPU), 1250.540 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.460 Gbps (per GPU), 1251.681 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.320 Gbps (per GPU), 1250.561 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.448 Gbps (per GPU), 1251.587 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.317 Gbps (per GPU), 1250.538 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.204 Gbps (per GPU), 1249.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.317 Gbps (per GPU), 1250.538 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.976 Gbps (per GPU), 815.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.978 Gbps (per GPU), 815.821 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.977 Gbps (per GPU), 815.814 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.978 Gbps (per GPU), 815.821 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.977 Gbps (per GPU), 815.814 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.978 Gbps (per GPU), 815.821 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.978 Gbps (per GPU), 815.827 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.978 Gbps (per GPU), 815.828 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.293 Gbps (per GPU), 274.347 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.294 Gbps (per GPU), 274.354 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.294 Gbps (per GPU), 274.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.293 Gbps (per GPU), 274.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.294 Gbps (per GPU), 274.355 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.295 Gbps (per GPU), 274.358 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.294 Gbps (per GPU), 274.356 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.294 Gbps (per GPU), 274.349 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.52ms  9.49ms  9.80ms  3.89 29.12K 14.23M
 chk_1  2.53ms  5.11ms  5.50ms  2.18 29.12K  6.56M
 chk_2  2.53ms 16.41ms 17.04ms  6.72 29.12K 24.68M
 chk_3  2.53ms 16.51ms 16.94ms  6.69 29.12K 22.95M
 chk_4  2.54ms  4.93ms  5.29ms  2.08 29.12K  6.33M
 chk_5  2.54ms  8.95ms  9.22ms  3.63 29.12K 12.05M
 chk_6  2.54ms 10.01ms 10.38ms  4.09 29.12K 14.60M
 chk_7  2.54ms  9.19ms  9.59ms  3.78 29.12K 13.21M
   Avg  2.53 10.07 10.47
   Max  2.54 16.51 17.04
   Min  2.52  4.93  5.29
 Ratio  1.01  3.35  3.22
   Var  0.00 16.87 17.41
Profiling takes 2.149 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 342.667 ms
Partition 0 [0, 5) has cost: 342.667 ms
Partition 1 [5, 9) has cost: 322.398 ms
Partition 2 [9, 13) has cost: 322.398 ms
Partition 3 [13, 17) has cost: 322.398 ms
Partition 4 [17, 21) has cost: 322.398 ms
Partition 5 [21, 25) has cost: 322.398 ms
Partition 6 [25, 29) has cost: 322.398 ms
Partition 7 [29, 33) has cost: 325.550 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 299.737 ms
GPU 0, Compute+Comm Time: 123.983 ms, Bubble Time: 128.934 ms, Imbalance Overhead: 46.820 ms
GPU 1, Compute+Comm Time: 118.899 ms, Bubble Time: 120.104 ms, Imbalance Overhead: 60.733 ms
GPU 2, Compute+Comm Time: 118.899 ms, Bubble Time: 110.644 ms, Imbalance Overhead: 70.193 ms
GPU 3, Compute+Comm Time: 118.899 ms, Bubble Time: 110.893 ms, Imbalance Overhead: 69.944 ms
GPU 4, Compute+Comm Time: 118.899 ms, Bubble Time: 120.206 ms, Imbalance Overhead: 60.632 ms
GPU 5, Compute+Comm Time: 118.899 ms, Bubble Time: 128.883 ms, Imbalance Overhead: 51.955 ms
GPU 6, Compute+Comm Time: 118.899 ms, Bubble Time: 137.471 ms, Imbalance Overhead: 43.366 ms
GPU 7, Compute+Comm Time: 119.610 ms, Bubble Time: 147.291 ms, Imbalance Overhead: 32.836 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 585.406 ms
GPU 0, Compute+Comm Time: 232.242 ms, Bubble Time: 288.988 ms, Imbalance Overhead: 64.175 ms
GPU 1, Compute+Comm Time: 229.800 ms, Bubble Time: 269.952 ms, Imbalance Overhead: 85.654 ms
GPU 2, Compute+Comm Time: 229.800 ms, Bubble Time: 252.926 ms, Imbalance Overhead: 102.679 ms
GPU 3, Compute+Comm Time: 229.800 ms, Bubble Time: 235.622 ms, Imbalance Overhead: 119.984 ms
GPU 4, Compute+Comm Time: 229.800 ms, Bubble Time: 216.420 ms, Imbalance Overhead: 139.186 ms
GPU 5, Compute+Comm Time: 229.800 ms, Bubble Time: 214.951 ms, Imbalance Overhead: 140.654 ms
GPU 6, Compute+Comm Time: 229.800 ms, Bubble Time: 233.909 ms, Imbalance Overhead: 121.696 ms
GPU 7, Compute+Comm Time: 244.986 ms, Bubble Time: 250.616 ms, Imbalance Overhead: 89.804 ms
The estimated cost of the whole pipeline: 929.399 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 665.064 ms
Partition 0 [0, 9) has cost: 665.064 ms
Partition 1 [9, 17) has cost: 644.795 ms
Partition 2 [17, 25) has cost: 644.795 ms
Partition 3 [25, 33) has cost: 647.948 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 309.497 ms
GPU 0, Compute+Comm Time: 160.169 ms, Bubble Time: 148.338 ms, Imbalance Overhead: 0.989 ms
GPU 1, Compute+Comm Time: 157.621 ms, Bubble Time: 130.057 ms, Imbalance Overhead: 21.818 ms
GPU 2, Compute+Comm Time: 157.621 ms, Bubble Time: 111.137 ms, Imbalance Overhead: 40.738 ms
GPU 3, Compute+Comm Time: 158.002 ms, Bubble Time: 111.113 ms, Imbalance Overhead: 40.381 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 589.576 ms
GPU 0, Compute+Comm Time: 299.845 ms, Bubble Time: 211.160 ms, Imbalance Overhead: 78.570 ms
GPU 1, Compute+Comm Time: 298.586 ms, Bubble Time: 209.801 ms, Imbalance Overhead: 81.188 ms
GPU 2, Compute+Comm Time: 298.586 ms, Bubble Time: 247.240 ms, Imbalance Overhead: 43.749 ms
GPU 3, Compute+Comm Time: 306.195 ms, Bubble Time: 282.527 ms, Imbalance Overhead: 0.854 ms
    The estimated cost with 2 DP ways is 944.026 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1309.860 ms
Partition 0 [0, 17) has cost: 1309.860 ms
Partition 1 [17, 33) has cost: 1292.743 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 348.230 ms
GPU 0, Compute+Comm Time: 232.137 ms, Bubble Time: 116.092 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 231.130 ms, Bubble Time: 115.521 ms, Imbalance Overhead: 1.578 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 619.075 ms
GPU 0, Compute+Comm Time: 410.613 ms, Bubble Time: 206.585 ms, Imbalance Overhead: 1.877 ms
GPU 1, Compute+Comm Time: 413.624 ms, Bubble Time: 205.452 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1015.670 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2602.603 ms
Partition 0 [0, 33) has cost: 2602.603 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 480.039 ms
GPU 0, Compute+Comm Time: 480.039 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 659.952 ms
GPU 0, Compute+Comm Time: 659.952 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1196.990 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.9197	TrainAcc 0.0822	ValidAcc 0.0739	TestAcc 0.0736	BestValid 0.0739
	Epoch 50:	Loss 1.7023	TrainAcc 0.6900	ValidAcc 0.7117	TestAcc 0.7060	BestValid 0.7117
	Epoch 100:	Loss 1.1560	TrainAcc 0.8004	ValidAcc 0.8135	TestAcc 0.8086	BestValid 0.8135
	Epoch 150:	Loss 0.9211	TrainAcc 0.8443	ValidAcc 0.8540	TestAcc 0.8497	BestValid 0.8540
	Epoch 200:	Loss 0.7920	TrainAcc 0.8682	ValidAcc 0.8765	TestAcc 0.8712	BestValid 0.8765
	Epoch 250:	Loss 0.7178	TrainAcc 0.8860	ValidAcc 0.8922	TestAcc 0.8873	BestValid 0.8922
	Epoch 300:	Loss 0.6625	TrainAcc 0.8950	ValidAcc 0.9011	TestAcc 0.8966	BestValid 0.9011
	Epoch 350:	Loss 0.6259	TrainAcc 0.9015	ValidAcc 0.9074	TestAcc 0.9027	BestValid 0.9074
	Epoch 400:	Loss 0.5983	TrainAcc 0.9029	ValidAcc 0.9091	TestAcc 0.9030	BestValid 0.9091
	Epoch 450:	Loss 0.5727	TrainAcc 0.9080	ValidAcc 0.9146	TestAcc 0.9086	BestValid 0.9146
	Epoch 500:	Loss 0.5558	TrainAcc 0.9097	ValidAcc 0.9151	TestAcc 0.9097	BestValid 0.9151
	Epoch 550:	Loss 0.5390	TrainAcc 0.9135	ValidAcc 0.9191	TestAcc 0.9136	BestValid 0.9191
	Epoch 600:	Loss 0.5282	TrainAcc 0.9160	ValidAcc 0.9218	TestAcc 0.9161	BestValid 0.9218
	Epoch 650:	Loss 0.5184	TrainAcc 0.9177	ValidAcc 0.9235	TestAcc 0.9174	BestValid 0.9235
	Epoch 700:	Loss 0.5065	TrainAcc 0.9188	ValidAcc 0.9247	TestAcc 0.9186	BestValid 0.9247
	Epoch 750:	Loss 0.4954	TrainAcc 0.9166	ValidAcc 0.9216	TestAcc 0.9161	BestValid 0.9247
	Epoch 800:	Loss 0.4910	TrainAcc 0.9202	ValidAcc 0.9255	TestAcc 0.9196	BestValid 0.9255
	Epoch 850:	Loss 0.4821	TrainAcc 0.9236	ValidAcc 0.9292	TestAcc 0.9233	BestValid 0.9292
	Epoch 900:	Loss 0.4794	TrainAcc 0.9239	ValidAcc 0.9285	TestAcc 0.9235	BestValid 0.9292
	Epoch 950:	Loss 0.4723	TrainAcc 0.9229	ValidAcc 0.9271	TestAcc 0.9224	BestValid 0.9292
	Epoch 1000:	Loss 0.4642	TrainAcc 0.9249	ValidAcc 0.9296	TestAcc 0.9243	BestValid 0.9296
	Epoch 1050:	Loss 0.4622	TrainAcc 0.9193	ValidAcc 0.9235	TestAcc 0.9190	BestValid 0.9296
	Epoch 1100:	Loss 0.4513	TrainAcc 0.9235	ValidAcc 0.9276	TestAcc 0.9229	BestValid 0.9296
	Epoch 1150:	Loss 0.4475	TrainAcc 0.9218	ValidAcc 0.9253	TestAcc 0.9210	BestValid 0.9296
	Epoch 1200:	Loss 0.4564	TrainAcc 0.9272	ValidAcc 0.9303	TestAcc 0.9261	BestValid 0.9303
	Epoch 1250:	Loss 0.4411	TrainAcc 0.9287	ValidAcc 0.9318	TestAcc 0.9273	BestValid 0.9318
	Epoch 1300:	Loss 0.4397	TrainAcc 0.9282	ValidAcc 0.9313	TestAcc 0.9272	BestValid 0.9318
	Epoch 1350:	Loss 0.4345	TrainAcc 0.9270	ValidAcc 0.9301	TestAcc 0.9259	BestValid 0.9318
	Epoch 1400:	Loss 0.4318	TrainAcc 0.9287	ValidAcc 0.9318	TestAcc 0.9276	BestValid 0.9318
	Epoch 1450:	Loss 0.4271	TrainAcc 0.9320	ValidAcc 0.9340	TestAcc 0.9307	BestValid 0.9340
	Epoch 1500:	Loss 0.4242	TrainAcc 0.9272	ValidAcc 0.9296	TestAcc 0.9256	BestValid 0.9340
	Epoch 1550:	Loss 0.4207	TrainAcc 0.9310	ValidAcc 0.9328	TestAcc 0.9297	BestValid 0.9340
	Epoch 1600:	Loss 0.4202	TrainAcc 0.9288	ValidAcc 0.9306	TestAcc 0.9274	BestValid 0.9340
	Epoch 1650:	Loss 0.4160	TrainAcc 0.9305	ValidAcc 0.9324	TestAcc 0.9284	BestValid 0.9340
	Epoch 1700:	Loss 0.4133	TrainAcc 0.9316	ValidAcc 0.9333	TestAcc 0.9299	BestValid 0.9340
	Epoch 1750:	Loss 0.4171	TrainAcc 0.9345	ValidAcc 0.9352	TestAcc 0.9329	BestValid 0.9352
	Epoch 1800:	Loss 0.4091	TrainAcc 0.9318	ValidAcc 0.9334	TestAcc 0.9296	BestValid 0.9352
	Epoch 1850:	Loss 0.4158	TrainAcc 0.9334	ValidAcc 0.9347	TestAcc 0.9314	BestValid 0.9352
	Epoch 1900:	Loss 0.4041	TrainAcc 0.9323	ValidAcc 0.9336	TestAcc 0.9299	BestValid 0.9352
	Epoch 1950:	Loss 0.4025	TrainAcc 0.9302	ValidAcc 0.9311	TestAcc 0.9277	BestValid 0.9352
	Epoch 2000:	Loss 0.4017	TrainAcc 0.9301	ValidAcc 0.9316	TestAcc 0.9277	BestValid 0.9352
	Epoch 2050:	Loss 0.3980	TrainAcc 0.9330	ValidAcc 0.9342	TestAcc 0.9305	BestValid 0.9352
	Epoch 2100:	Loss 0.3970	TrainAcc 0.9332	ValidAcc 0.9347	TestAcc 0.9306	BestValid 0.9352
	Epoch 2150:	Loss 0.3961	TrainAcc 0.9327	ValidAcc 0.9342	TestAcc 0.9301	BestValid 0.9352
	Epoch 2200:	Loss 0.3966	TrainAcc 0.9349	ValidAcc 0.9361	TestAcc 0.9326	BestValid 0.9361
	Epoch 2250:	Loss 0.3903	TrainAcc 0.9353	ValidAcc 0.9366	TestAcc 0.9328	BestValid 0.9366
	Epoch 2300:	Loss 0.3924	TrainAcc 0.9328	ValidAcc 0.9340	TestAcc 0.9298	BestValid 0.9366
	Epoch 2350:	Loss 0.3886	TrainAcc 0.9338	ValidAcc 0.9349	TestAcc 0.9309	BestValid 0.9366
	Epoch 2400:	Loss 0.3849	TrainAcc 0.9310	ValidAcc 0.9321	TestAcc 0.9285	BestValid 0.9366
	Epoch 2450:	Loss 0.3883	TrainAcc 0.9352	ValidAcc 0.9362	TestAcc 0.9327	BestValid 0.9366
	Epoch 2500:	Loss 0.3825	TrainAcc 0.9358	ValidAcc 0.9365	TestAcc 0.9331	BestValid 0.9366
	Epoch 2550:	Loss 0.3918	TrainAcc 0.9333	ValidAcc 0.9336	TestAcc 0.9301	BestValid 0.9366
	Epoch 2600:	Loss 0.3789	TrainAcc 0.9356	ValidAcc 0.9361	TestAcc 0.9326	BestValid 0.9366
	Epoch 2650:	Loss 0.3793	TrainAcc 0.9350	ValidAcc 0.9357	TestAcc 0.9321	BestValid 0.9366
	Epoch 2700:	Loss 0.3782	TrainAcc 0.9359	ValidAcc 0.9360	TestAcc 0.9329	BestValid 0.9366
	Epoch 2750:	Loss 0.3755	TrainAcc 0.9357	ValidAcc 0.9363	TestAcc 0.9329	BestValid 0.9366
	Epoch 2800:	Loss 0.3771	TrainAcc 0.9370	ValidAcc 0.9373	TestAcc 0.9340	BestValid 0.9373
	Epoch 2850:	Loss 0.3721	TrainAcc 0.9365	ValidAcc 0.9371	TestAcc 0.9338	BestValid 0.9373
	Epoch 2900:	Loss 0.3701	TrainAcc 0.9366	ValidAcc 0.9368	TestAcc 0.9336	BestValid 0.9373
	Epoch 2950:	Loss 0.3771	TrainAcc 0.9394	ValidAcc 0.9400	TestAcc 0.9369	BestValid 0.9400
	Epoch 3000:	Loss 0.3758	TrainAcc 0.9346	ValidAcc 0.9353	TestAcc 0.9308	BestValid 0.9400
	Epoch 3050:	Loss 0.3676	TrainAcc 0.9352	ValidAcc 0.9352	TestAcc 0.9314	BestValid 0.9400
	Epoch 3100:	Loss 0.3666	TrainAcc 0.9340	ValidAcc 0.9339	TestAcc 0.9306	BestValid 0.9400
	Epoch 3150:	Loss 0.3668	TrainAcc 0.9357	ValidAcc 0.9354	TestAcc 0.9321	BestValid 0.9400
	Epoch 3200:	Loss 0.3668	TrainAcc 0.9388	ValidAcc 0.9390	TestAcc 0.9354	BestValid 0.9400
	Epoch 3250:	Loss 0.3614	TrainAcc 0.9391	ValidAcc 0.9394	TestAcc 0.9358	BestValid 0.9400
	Epoch 3300:	Loss 0.3701	TrainAcc 0.9384	ValidAcc 0.9382	TestAcc 0.9352	BestValid 0.9400
	Epoch 3350:	Loss 0.3624	TrainAcc 0.9376	ValidAcc 0.9379	TestAcc 0.9343	BestValid 0.9400
	Epoch 3400:	Loss 0.3611	TrainAcc 0.9387	ValidAcc 0.9390	TestAcc 0.9352	BestValid 0.9400
	Epoch 3450:	Loss 0.3615	TrainAcc 0.9398	ValidAcc 0.9397	TestAcc 0.9364	BestValid 0.9400
	Epoch 3500:	Loss 0.3609	TrainAcc 0.9384	ValidAcc 0.9384	TestAcc 0.9348	BestValid 0.9400
	Epoch 3550:	Loss 0.3678	TrainAcc 0.9421	ValidAcc 0.9420	TestAcc 0.9387	BestValid 0.9420
	Epoch 3600:	Loss 0.3779	TrainAcc 0.9411	ValidAcc 0.9412	TestAcc 0.9377	BestValid 0.9420
	Epoch 3650:	Loss 0.3566	TrainAcc 0.9390	ValidAcc 0.9391	TestAcc 0.9354	BestValid 0.9420
	Epoch 3700:	Loss 0.3648	TrainAcc 0.9376	ValidAcc 0.9376	TestAcc 0.9334	BestValid 0.9420
	Epoch 3750:	Loss 0.3550	TrainAcc 0.9374	ValidAcc 0.9366	TestAcc 0.9334	BestValid 0.9420
	Epoch 3800:	Loss 0.3558	TrainAcc 0.9397	ValidAcc 0.9393	TestAcc 0.9360	BestValid 0.9420
	Epoch 3850:	Loss 0.3527	TrainAcc 0.9376	ValidAcc 0.9373	TestAcc 0.9336	BestValid 0.9420
	Epoch 3900:	Loss 0.3546	TrainAcc 0.9391	ValidAcc 0.9389	TestAcc 0.9355	BestValid 0.9420
	Epoch 3950:	Loss 0.3492	TrainAcc 0.9395	ValidAcc 0.9396	TestAcc 0.9358	BestValid 0.9420
	Epoch 4000:	Loss 0.3547	TrainAcc 0.9410	ValidAcc 0.9408	TestAcc 0.9372	BestValid 0.9420
	Epoch 4050:	Loss 0.3510	TrainAcc 0.9414	ValidAcc 0.9413	TestAcc 0.9378	BestValid 0.9420
	Epoch 4100:	Loss 0.3570	TrainAcc 0.9430	ValidAcc 0.9426	TestAcc 0.9395	BestValid 0.9426
	Epoch 4150:	Loss 0.3504	TrainAcc 0.9421	ValidAcc 0.9418	TestAcc 0.9382	BestValid 0.9426
	Epoch 4200:	Loss 0.3462	TrainAcc 0.9391	ValidAcc 0.9390	TestAcc 0.9351	BestValid 0.9426
	Epoch 4250:	Loss 0.3469	TrainAcc 0.9404	ValidAcc 0.9394	TestAcc 0.9365	BestValid 0.9426
	Epoch 4300:	Loss 0.3468	TrainAcc 0.9420	ValidAcc 0.9413	TestAcc 0.9380	BestValid 0.9426
	Epoch 4350:	Loss 0.3464	TrainAcc 0.9417	ValidAcc 0.9416	TestAcc 0.9379	BestValid 0.9426
	Epoch 4400:	Loss 0.3484	TrainAcc 0.9360	ValidAcc 0.9355	TestAcc 0.9319	BestValid 0.9426
	Epoch 4450:	Loss 0.3468	TrainAcc 0.9415	ValidAcc 0.9417	TestAcc 0.9378	BestValid 0.9426
	Epoch 4500:	Loss 0.3408	TrainAcc 0.9421	ValidAcc 0.9421	TestAcc 0.9384	BestValid 0.9426
	Epoch 4550:	Loss 0.3484	TrainAcc 0.9392	ValidAcc 0.9384	TestAcc 0.9350	BestValid 0.9426
	Epoch 4600:	Loss 0.3436	TrainAcc 0.9435	ValidAcc 0.9429	TestAcc 0.9391	BestValid 0.9429
	Epoch 4650:	Loss 0.3421	TrainAcc 0.9402	ValidAcc 0.9399	TestAcc 0.9362	BestValid 0.9429
	Epoch 4700:	Loss 0.3419	TrainAcc 0.9456	ValidAcc 0.9448	TestAcc 0.9415	BestValid 0.9448
	Epoch 4750:	Loss 0.3419	TrainAcc 0.9398	ValidAcc 0.9390	TestAcc 0.9355	BestValid 0.9448
	Epoch 4800:	Loss 0.3363	TrainAcc 0.9429	ValidAcc 0.9424	TestAcc 0.9385	BestValid 0.9448
	Epoch 4850:	Loss 0.3409	TrainAcc 0.9421	ValidAcc 0.9421	TestAcc 0.9380	BestValid 0.9448
	Epoch 4900:	Loss 0.3353	TrainAcc 0.9394	ValidAcc 0.9389	TestAcc 0.9349	BestValid 0.9448
	Epoch 4950:	Loss 0.3402	TrainAcc 0.9441	ValidAcc 0.9431	TestAcc 0.9396	BestValid 0.9448
	Epoch 5000:	Loss 0.3408	TrainAcc 0.9380	ValidAcc 0.9366	TestAcc 0.9337	BestValid 0.9448
****** Epoch Time (Excluding Evaluation Cost): 0.901 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.948 ms (Max: 1.338, Min: 0.095, Sum: 7.586)
Cluster-Wide Average, Compute: 247.854 ms (Max: 385.760, Min: 141.145, Sum: 1982.833)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.011, Min: 0.008, Sum: 0.072)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.018, Min: 0.015, Sum: 0.133)
Cluster-Wide Average, Communication-Graph: 635.258 ms (Max: 741.684, Min: 498.168, Sum: 5082.063)
Cluster-Wide Average, Optimization: 3.220 ms (Max: 3.240, Min: 3.206, Sum: 25.757)
Cluster-Wide Average, Others: 13.844 ms (Max: 13.870, Min: 13.822, Sum: 110.751)
****** Breakdown Sum: 901.150 ms ******
Cluster-Wide Average, GPU Memory Consumption: 16.524 GB (Max: 17.239, Min: 16.405, Sum: 132.192)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.690 Gbps (Max: 49.592, Min: 11.535, Sum: 213.523)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.020 GB
Total communication (cluster-wide, per-epoch): 14.502 GB
****** Accuracy Results ******
Highest valid_acc: 0.9448
Target test_acc: 0.9415
Epoch to reach the target acc: 4699
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
