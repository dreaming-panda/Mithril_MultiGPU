Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.021 seconds.
Building the CSC structure...
        It takes 2.049 seconds.
Building the CSC structure...
        It takes 2.371 seconds.
Building the CSC structure...
        It takes 2.403 seconds.
Building the CSC structure...
        It takes 2.404 seconds.
Building the CSC structure...
        It takes 2.440 seconds.
Building the CSC structure...
        It takes 2.495 seconds.
Building the CSC structure...
        It takes 2.620 seconds.
Building the CSC structure...
        It takes 1.855 seconds.
        It takes 1.856 seconds.
        It takes 2.289 seconds.
        It takes 2.361 seconds.
        It takes 2.354 seconds.
        It takes 2.335 seconds.
        It takes 2.342 seconds.
        It takes 2.280 seconds.
Building the Feature Vector...
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
        It takes 0.272 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.039 seconds.
        It takes 0.241 seconds.
Building the Label Vector...
        It takes 0.294 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.039 seconds.
        It takes 0.039 seconds.
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.034 seconds.
        It takes 0.280 seconds.
Building the Label Vector...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.040 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 54.708 Gbps (per GPU), 437.662 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.462 Gbps (per GPU), 435.700 Gbps (aggregated)
The layer-level communication performance: 54.466 Gbps (per GPU), 435.732 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.249 Gbps (per GPU), 433.988 Gbps (aggregated)
The layer-level communication performance: 54.223 Gbps (per GPU), 433.785 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.049 Gbps (per GPU), 432.393 Gbps (aggregated)
The layer-level communication performance: 54.015 Gbps (per GPU), 432.123 Gbps (aggregated)
The layer-level communication performance: 53.984 Gbps (per GPU), 431.873 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.914 Gbps (per GPU), 1255.310 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.932 Gbps (per GPU), 1255.452 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.911 Gbps (per GPU), 1255.289 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.931 Gbps (per GPU), 1255.451 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.914 Gbps (per GPU), 1255.308 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.937 Gbps (per GPU), 1255.498 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.914 Gbps (per GPU), 1255.310 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.937 Gbps (per GPU), 1255.498 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.168 Gbps (per GPU), 809.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.155 Gbps (per GPU), 809.243 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.167 Gbps (per GPU), 809.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.167 Gbps (per GPU), 809.334 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.167 Gbps (per GPU), 809.334 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.168 Gbps (per GPU), 809.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.168 Gbps (per GPU), 809.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.168 Gbps (per GPU), 809.345 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.192 Gbps (per GPU), 281.533 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.192 Gbps (per GPU), 281.535 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.191 Gbps (per GPU), 281.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.191 Gbps (per GPU), 281.531 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.192 Gbps (per GPU), 281.533 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.191 Gbps (per GPU), 281.528 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.191 Gbps (per GPU), 281.529 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.191 Gbps (per GPU), 281.529 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.51ms  9.39ms  9.87ms  3.93 29.12K 14.23M
 chk_1  2.52ms  5.11ms  5.48ms  2.18 29.12K  6.56M
 chk_2  2.52ms 16.52ms 17.11ms  6.78 29.12K 24.68M
 chk_3  2.53ms 16.61ms 17.06ms  6.74 29.12K 22.95M
 chk_4  2.53ms  4.93ms  5.28ms  2.09 29.12K  6.33M
 chk_5  2.53ms  8.87ms  9.32ms  3.69 29.12K 12.05M
 chk_6  2.53ms 10.05ms 10.41ms  4.12 29.12K 14.60M
 chk_7  2.53ms  9.13ms  9.60ms  3.80 29.12K 13.21M
   Avg  2.52 10.08 10.52
   Max  2.53 16.61 17.11
   Min  2.51  4.93  5.28
 Ratio  1.01  3.37  3.24
   Var  0.00 17.28 17.72
Profiling takes 2.148 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 342.602 ms
Partition 0 [0, 5) has cost: 342.602 ms
Partition 1 [5, 9) has cost: 322.405 ms
Partition 2 [9, 13) has cost: 322.405 ms
Partition 3 [13, 17) has cost: 322.405 ms
Partition 4 [17, 21) has cost: 322.405 ms
Partition 5 [21, 25) has cost: 322.405 ms
Partition 6 [25, 29) has cost: 322.405 ms
Partition 7 [29, 33) has cost: 325.947 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 302.079 ms
GPU 0, Compute+Comm Time: 124.478 ms, Bubble Time: 130.242 ms, Imbalance Overhead: 47.359 ms
GPU 1, Compute+Comm Time: 119.417 ms, Bubble Time: 120.850 ms, Imbalance Overhead: 61.812 ms
GPU 2, Compute+Comm Time: 119.417 ms, Bubble Time: 110.833 ms, Imbalance Overhead: 71.830 ms
GPU 3, Compute+Comm Time: 119.417 ms, Bubble Time: 111.129 ms, Imbalance Overhead: 71.534 ms
GPU 4, Compute+Comm Time: 119.417 ms, Bubble Time: 120.586 ms, Imbalance Overhead: 62.076 ms
GPU 5, Compute+Comm Time: 119.417 ms, Bubble Time: 129.410 ms, Imbalance Overhead: 53.253 ms
GPU 6, Compute+Comm Time: 119.417 ms, Bubble Time: 138.049 ms, Imbalance Overhead: 44.614 ms
GPU 7, Compute+Comm Time: 120.552 ms, Bubble Time: 148.070 ms, Imbalance Overhead: 33.458 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 588.061 ms
GPU 0, Compute+Comm Time: 232.649 ms, Bubble Time: 290.324 ms, Imbalance Overhead: 65.088 ms
GPU 1, Compute+Comm Time: 230.242 ms, Bubble Time: 270.875 ms, Imbalance Overhead: 86.945 ms
GPU 2, Compute+Comm Time: 230.242 ms, Bubble Time: 253.617 ms, Imbalance Overhead: 104.202 ms
GPU 3, Compute+Comm Time: 230.242 ms, Bubble Time: 236.178 ms, Imbalance Overhead: 121.641 ms
GPU 4, Compute+Comm Time: 230.242 ms, Bubble Time: 216.844 ms, Imbalance Overhead: 140.975 ms
GPU 5, Compute+Comm Time: 230.242 ms, Bubble Time: 215.436 ms, Imbalance Overhead: 142.384 ms
GPU 6, Compute+Comm Time: 230.242 ms, Bubble Time: 234.650 ms, Imbalance Overhead: 123.170 ms
GPU 7, Compute+Comm Time: 245.378 ms, Bubble Time: 251.647 ms, Imbalance Overhead: 91.036 ms
The estimated cost of the whole pipeline: 934.647 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 665.007 ms
Partition 0 [0, 9) has cost: 665.007 ms
Partition 1 [9, 17) has cost: 644.810 ms
Partition 2 [17, 25) has cost: 644.810 ms
Partition 3 [25, 33) has cost: 648.352 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 311.056 ms
GPU 0, Compute+Comm Time: 160.742 ms, Bubble Time: 149.907 ms, Imbalance Overhead: 0.408 ms
GPU 1, Compute+Comm Time: 158.207 ms, Bubble Time: 130.504 ms, Imbalance Overhead: 22.345 ms
GPU 2, Compute+Comm Time: 158.207 ms, Bubble Time: 110.469 ms, Imbalance Overhead: 42.381 ms
GPU 3, Compute+Comm Time: 158.836 ms, Bubble Time: 110.605 ms, Imbalance Overhead: 41.615 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 592.000 ms
GPU 0, Compute+Comm Time: 300.504 ms, Bubble Time: 211.495 ms, Imbalance Overhead: 80.002 ms
GPU 1, Compute+Comm Time: 299.163 ms, Bubble Time: 210.294 ms, Imbalance Overhead: 82.543 ms
GPU 2, Compute+Comm Time: 299.163 ms, Bubble Time: 248.180 ms, Imbalance Overhead: 44.657 ms
GPU 3, Compute+Comm Time: 306.741 ms, Bubble Time: 284.055 ms, Imbalance Overhead: 1.205 ms
    The estimated cost with 2 DP ways is 948.210 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1309.817 ms
Partition 0 [0, 17) has cost: 1309.817 ms
Partition 1 [17, 33) has cost: 1293.161 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 352.340 ms
GPU 0, Compute+Comm Time: 234.984 ms, Bubble Time: 117.356 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 233.944 ms, Bubble Time: 117.170 ms, Imbalance Overhead: 1.226 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 622.623 ms
GPU 0, Compute+Comm Time: 412.904 ms, Bubble Time: 207.517 ms, Imbalance Overhead: 2.202 ms
GPU 1, Compute+Comm Time: 415.882 ms, Bubble Time: 206.740 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1023.710 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2602.978 ms
Partition 0 [0, 33) has cost: 2602.978 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 473.903 ms
GPU 0, Compute+Comm Time: 473.903 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 654.058 ms
GPU 0, Compute+Comm Time: 654.058 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1184.359 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.6909	TrainAcc 0.6860	ValidAcc 0.7080	TestAcc 0.7034	BestValid 0.7080
	Epoch 100:	Loss 1.1790	TrainAcc 0.7865	ValidAcc 0.7998	TestAcc 0.7960	BestValid 0.7998
	Epoch 150:	Loss 0.9540	TrainAcc 0.8347	ValidAcc 0.8458	TestAcc 0.8401	BestValid 0.8458
	Epoch 200:	Loss 0.8218	TrainAcc 0.8648	ValidAcc 0.8731	TestAcc 0.8677	BestValid 0.8731
	Epoch 250:	Loss 0.7395	TrainAcc 0.8796	ValidAcc 0.8868	TestAcc 0.8814	BestValid 0.8868
	Epoch 300:	Loss 0.6805	TrainAcc 0.8870	ValidAcc 0.8934	TestAcc 0.8881	BestValid 0.8934
	Epoch 350:	Loss 0.6423	TrainAcc 0.8941	ValidAcc 0.9006	TestAcc 0.8949	BestValid 0.9006
	Epoch 400:	Loss 0.6069	TrainAcc 0.8979	ValidAcc 0.9045	TestAcc 0.8995	BestValid 0.9045
	Epoch 450:	Loss 0.5867	TrainAcc 0.8986	ValidAcc 0.9045	TestAcc 0.8997	BestValid 0.9045
	Epoch 500:	Loss 0.5617	TrainAcc 0.9056	ValidAcc 0.9116	TestAcc 0.9065	BestValid 0.9116
	Epoch 550:	Loss 0.5511	TrainAcc 0.9116	ValidAcc 0.9183	TestAcc 0.9129	BestValid 0.9183
	Epoch 600:	Loss 0.5344	TrainAcc 0.9136	ValidAcc 0.9208	TestAcc 0.9148	BestValid 0.9208
	Epoch 650:	Loss 0.5218	TrainAcc 0.9105	ValidAcc 0.9159	TestAcc 0.9110	BestValid 0.9208
	Epoch 700:	Loss 0.5103	TrainAcc 0.9103	ValidAcc 0.9155	TestAcc 0.9108	BestValid 0.9208
	Epoch 750:	Loss 0.5020	TrainAcc 0.9169	ValidAcc 0.9227	TestAcc 0.9177	BestValid 0.9227
	Epoch 800:	Loss 0.4920	TrainAcc 0.9137	ValidAcc 0.9189	TestAcc 0.9144	BestValid 0.9227
	Epoch 850:	Loss 0.4895	TrainAcc 0.9179	ValidAcc 0.9233	TestAcc 0.9181	BestValid 0.9233
	Epoch 900:	Loss 0.4740	TrainAcc 0.9172	ValidAcc 0.9224	TestAcc 0.9174	BestValid 0.9233
	Epoch 950:	Loss 0.4767	TrainAcc 0.9129	ValidAcc 0.9170	TestAcc 0.9131	BestValid 0.9233
	Epoch 1000:	Loss 0.4690	TrainAcc 0.9164	ValidAcc 0.9213	TestAcc 0.9170	BestValid 0.9233
	Epoch 1050:	Loss 0.4674	TrainAcc 0.9188	ValidAcc 0.9228	TestAcc 0.9188	BestValid 0.9233
	Epoch 1100:	Loss 0.4541	TrainAcc 0.9093	ValidAcc 0.9129	TestAcc 0.9082	BestValid 0.9233
	Epoch 1150:	Loss 0.4501	TrainAcc 0.9143	ValidAcc 0.9177	TestAcc 0.9142	BestValid 0.9233
	Epoch 1200:	Loss 0.4435	TrainAcc 0.9170	ValidAcc 0.9201	TestAcc 0.9166	BestValid 0.9233
	Epoch 1250:	Loss 0.4442	TrainAcc 0.9208	ValidAcc 0.9240	TestAcc 0.9205	BestValid 0.9240
	Epoch 1300:	Loss 0.4371	TrainAcc 0.9218	ValidAcc 0.9250	TestAcc 0.9210	BestValid 0.9250
	Epoch 1350:	Loss 0.4323	TrainAcc 0.9252	ValidAcc 0.9280	TestAcc 0.9242	BestValid 0.9280
	Epoch 1400:	Loss 0.4316	TrainAcc 0.9264	ValidAcc 0.9290	TestAcc 0.9254	BestValid 0.9290
	Epoch 1450:	Loss 0.4267	TrainAcc 0.9212	ValidAcc 0.9233	TestAcc 0.9203	BestValid 0.9290
	Epoch 1500:	Loss 0.4243	TrainAcc 0.9228	ValidAcc 0.9248	TestAcc 0.9224	BestValid 0.9290
	Epoch 1550:	Loss 0.4198	TrainAcc 0.9216	ValidAcc 0.9235	TestAcc 0.9208	BestValid 0.9290
	Epoch 1600:	Loss 0.4176	TrainAcc 0.9173	ValidAcc 0.9186	TestAcc 0.9168	BestValid 0.9290
	Epoch 1650:	Loss 0.4176	TrainAcc 0.9231	ValidAcc 0.9248	TestAcc 0.9221	BestValid 0.9290
	Epoch 1700:	Loss 0.4135	TrainAcc 0.9128	ValidAcc 0.9139	TestAcc 0.9113	BestValid 0.9290
	Epoch 1750:	Loss 0.4128	TrainAcc 0.9179	ValidAcc 0.9187	TestAcc 0.9165	BestValid 0.9290
	Epoch 1800:	Loss 0.4191	TrainAcc 0.9222	ValidAcc 0.9228	TestAcc 0.9210	BestValid 0.9290
	Epoch 1850:	Loss 0.4086	TrainAcc 0.9262	ValidAcc 0.9273	TestAcc 0.9250	BestValid 0.9290
	Epoch 1900:	Loss 0.4059	TrainAcc 0.9274	ValidAcc 0.9284	TestAcc 0.9262	BestValid 0.9290
	Epoch 1950:	Loss 0.4012	TrainAcc 0.9205	ValidAcc 0.9214	TestAcc 0.9195	BestValid 0.9290
	Epoch 2000:	Loss 0.4037	TrainAcc 0.9242	ValidAcc 0.9248	TestAcc 0.9225	BestValid 0.9290
	Epoch 2050:	Loss 0.3944	TrainAcc 0.9241	ValidAcc 0.9241	TestAcc 0.9225	BestValid 0.9290
	Epoch 2100:	Loss 0.3945	TrainAcc 0.9292	ValidAcc 0.9300	TestAcc 0.9276	BestValid 0.9300
	Epoch 2150:	Loss 0.3968	TrainAcc 0.9232	ValidAcc 0.9235	TestAcc 0.9215	BestValid 0.9300
	Epoch 2200:	Loss 0.3969	TrainAcc 0.9258	ValidAcc 0.9255	TestAcc 0.9242	BestValid 0.9300
	Epoch 2250:	Loss 0.3884	TrainAcc 0.9271	ValidAcc 0.9267	TestAcc 0.9252	BestValid 0.9300
	Epoch 2300:	Loss 0.3915	TrainAcc 0.9230	ValidAcc 0.9227	TestAcc 0.9213	BestValid 0.9300
	Epoch 2350:	Loss 0.3933	TrainAcc 0.9240	ValidAcc 0.9239	TestAcc 0.9223	BestValid 0.9300
	Epoch 2400:	Loss 0.3834	TrainAcc 0.9211	ValidAcc 0.9210	TestAcc 0.9185	BestValid 0.9300
	Epoch 2450:	Loss 0.3818	TrainAcc 0.9231	ValidAcc 0.9229	TestAcc 0.9209	BestValid 0.9300
	Epoch 2500:	Loss 0.3915	TrainAcc 0.9142	ValidAcc 0.9141	TestAcc 0.9111	BestValid 0.9300
	Epoch 2550:	Loss 0.3806	TrainAcc 0.9248	ValidAcc 0.9248	TestAcc 0.9229	BestValid 0.9300
	Epoch 2600:	Loss 0.3859	TrainAcc 0.9286	ValidAcc 0.9278	TestAcc 0.9263	BestValid 0.9300
	Epoch 2650:	Loss 0.3794	TrainAcc 0.9197	ValidAcc 0.9200	TestAcc 0.9176	BestValid 0.9300
	Epoch 2700:	Loss 0.3785	TrainAcc 0.9229	ValidAcc 0.9228	TestAcc 0.9210	BestValid 0.9300
	Epoch 2750:	Loss 0.3765	TrainAcc 0.9192	ValidAcc 0.9192	TestAcc 0.9167	BestValid 0.9300
	Epoch 2800:	Loss 0.3741	TrainAcc 0.9239	ValidAcc 0.9233	TestAcc 0.9217	BestValid 0.9300
	Epoch 2850:	Loss 0.3808	TrainAcc 0.9220	ValidAcc 0.9216	TestAcc 0.9192	BestValid 0.9300
	Epoch 2900:	Loss 0.3703	TrainAcc 0.9239	ValidAcc 0.9238	TestAcc 0.9218	BestValid 0.9300
	Epoch 2950:	Loss 0.3702	TrainAcc 0.9276	ValidAcc 0.9269	TestAcc 0.9255	BestValid 0.9300
	Epoch 3000:	Loss 0.3713	TrainAcc 0.9311	ValidAcc 0.9303	TestAcc 0.9287	BestValid 0.9303
	Epoch 3050:	Loss 0.3727	TrainAcc 0.9315	ValidAcc 0.9305	TestAcc 0.9293	BestValid 0.9305
	Epoch 3100:	Loss 0.3681	TrainAcc 0.9298	ValidAcc 0.9296	TestAcc 0.9278	BestValid 0.9305
	Epoch 3150:	Loss 0.3704	TrainAcc 0.9216	ValidAcc 0.9206	TestAcc 0.9187	BestValid 0.9305
	Epoch 3200:	Loss 0.3655	TrainAcc 0.9154	ValidAcc 0.9147	TestAcc 0.9121	BestValid 0.9305
	Epoch 3250:	Loss 0.3666	TrainAcc 0.9281	ValidAcc 0.9273	TestAcc 0.9260	BestValid 0.9305
	Epoch 3300:	Loss 0.3624	TrainAcc 0.9163	ValidAcc 0.9152	TestAcc 0.9131	BestValid 0.9305
	Epoch 3350:	Loss 0.3596	TrainAcc 0.9219	ValidAcc 0.9211	TestAcc 0.9191	BestValid 0.9305
	Epoch 3400:	Loss 0.3735	TrainAcc 0.9118	ValidAcc 0.9104	TestAcc 0.9084	BestValid 0.9305
	Epoch 3450:	Loss 0.3675	TrainAcc 0.9197	ValidAcc 0.9180	TestAcc 0.9159	BestValid 0.9305
	Epoch 3500:	Loss 0.3613	TrainAcc 0.9200	ValidAcc 0.9192	TestAcc 0.9172	BestValid 0.9305
	Epoch 3550:	Loss 0.3620	TrainAcc 0.9217	ValidAcc 0.9206	TestAcc 0.9189	BestValid 0.9305
	Epoch 3600:	Loss 0.3596	TrainAcc 0.9343	ValidAcc 0.9334	TestAcc 0.9312	BestValid 0.9334
	Epoch 3650:	Loss 0.3563	TrainAcc 0.9210	ValidAcc 0.9198	TestAcc 0.9176	BestValid 0.9334
	Epoch 3700:	Loss 0.3551	TrainAcc 0.9264	ValidAcc 0.9254	TestAcc 0.9240	BestValid 0.9334
	Epoch 3750:	Loss 0.3597	TrainAcc 0.9178	ValidAcc 0.9162	TestAcc 0.9142	BestValid 0.9334
	Epoch 3800:	Loss 0.3567	TrainAcc 0.9285	ValidAcc 0.9272	TestAcc 0.9253	BestValid 0.9334
	Epoch 3850:	Loss 0.3675	TrainAcc 0.9102	ValidAcc 0.9089	TestAcc 0.9062	BestValid 0.9334
	Epoch 3900:	Loss 0.3511	TrainAcc 0.9262	ValidAcc 0.9245	TestAcc 0.9229	BestValid 0.9334
	Epoch 3950:	Loss 0.3598	TrainAcc 0.9152	ValidAcc 0.9139	TestAcc 0.9108	BestValid 0.9334
	Epoch 4000:	Loss 0.3509	TrainAcc 0.9208	ValidAcc 0.9201	TestAcc 0.9179	BestValid 0.9334
	Epoch 4050:	Loss 0.3566	TrainAcc 0.9326	ValidAcc 0.9319	TestAcc 0.9292	BestValid 0.9334
	Epoch 4100:	Loss 0.3565	TrainAcc 0.9278	ValidAcc 0.9265	TestAcc 0.9249	BestValid 0.9334
	Epoch 4150:	Loss 0.3511	TrainAcc 0.9168	ValidAcc 0.9145	TestAcc 0.9129	BestValid 0.9334
	Epoch 4200:	Loss 0.3476	TrainAcc 0.9197	ValidAcc 0.9183	TestAcc 0.9163	BestValid 0.9334
	Epoch 4250:	Loss 0.3510	TrainAcc 0.9232	ValidAcc 0.9215	TestAcc 0.9193	BestValid 0.9334
	Epoch 4300:	Loss 0.3457	TrainAcc 0.9165	ValidAcc 0.9145	TestAcc 0.9125	BestValid 0.9334
	Epoch 4350:	Loss 0.3520	TrainAcc 0.9214	ValidAcc 0.9196	TestAcc 0.9180	BestValid 0.9334
	Epoch 4400:	Loss 0.3444	TrainAcc 0.9368	ValidAcc 0.9364	TestAcc 0.9337	BestValid 0.9364
	Epoch 4450:	Loss 0.3437	TrainAcc 0.9319	ValidAcc 0.9313	TestAcc 0.9289	BestValid 0.9364
	Epoch 4500:	Loss 0.3474	TrainAcc 0.9332	ValidAcc 0.9321	TestAcc 0.9303	BestValid 0.9364
	Epoch 4550:	Loss 0.3482	TrainAcc 0.9316	ValidAcc 0.9308	TestAcc 0.9291	BestValid 0.9364
	Epoch 4600:	Loss 0.3427	TrainAcc 0.9235	ValidAcc 0.9223	TestAcc 0.9206	BestValid 0.9364
	Epoch 4650:	Loss 0.3465	TrainAcc 0.9239	ValidAcc 0.9225	TestAcc 0.9203	BestValid 0.9364
	Epoch 4700:	Loss 0.3400	TrainAcc 0.9310	ValidAcc 0.9301	TestAcc 0.9279	BestValid 0.9364
	Epoch 4750:	Loss 0.3445	TrainAcc 0.9289	ValidAcc 0.9280	TestAcc 0.9259	BestValid 0.9364
	Epoch 4800:	Loss 0.3457	TrainAcc 0.9213	ValidAcc 0.9195	TestAcc 0.9170	BestValid 0.9364
	Epoch 4850:	Loss 0.3485	TrainAcc 0.9312	ValidAcc 0.9302	TestAcc 0.9279	BestValid 0.9364
	Epoch 4900:	Loss 0.3383	TrainAcc 0.9314	ValidAcc 0.9304	TestAcc 0.9285	BestValid 0.9364
	Epoch 4950:	Loss 0.3388	TrainAcc 0.9283	ValidAcc 0.9271	TestAcc 0.9249	BestValid 0.9364
	Epoch 5000:	Loss 0.3438	TrainAcc 0.9282	ValidAcc 0.9272	TestAcc 0.9246	BestValid 0.9364
****** Epoch Time (Excluding Evaluation Cost): 0.901 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.901 ms (Max: 1.273, Min: 0.094, Sum: 7.206)
Cluster-Wide Average, Compute: 247.263 ms (Max: 385.201, Min: 139.994, Sum: 1978.103)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.010, Min: 0.008, Sum: 0.070)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.018, Min: 0.015, Sum: 0.132)
Cluster-Wide Average, Communication-Graph: 635.520 ms (Max: 742.487, Min: 498.342, Sum: 5084.163)
Cluster-Wide Average, Optimization: 3.222 ms (Max: 3.242, Min: 3.199, Sum: 25.774)
Cluster-Wide Average, Others: 13.845 ms (Max: 13.874, Min: 13.819, Sum: 110.760)
****** Breakdown Sum: 900.776 ms ******
Cluster-Wide Average, GPU Memory Consumption: 16.524 GB (Max: 17.239, Min: 16.405, Sum: 132.192)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.678 Gbps (Max: 49.570, Min: 11.520, Sum: 213.423)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.020 GB
Total communication (cluster-wide, per-epoch): 14.502 GB
****** Accuracy Results ******
Highest valid_acc: 0.9364
Target test_acc: 0.9337
Epoch to reach the target acc: 4399
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
