Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.867 seconds.
Building the CSC structure...
        It takes 2.009 seconds.
Building the CSC structure...
        It takes 2.018 seconds.
Building the CSC structure...
        It takes 2.061 seconds.
Building the CSC structure...
        It takes 2.285 seconds.
Building the CSC structure...
        It takes 2.303 seconds.
Building the CSC structure...
        It takes 2.420 seconds.
Building the CSC structure...
        It takes 2.619 seconds.
Building the CSC structure...
        It takes 1.831 seconds.
        It takes 1.839 seconds.
        It takes 1.869 seconds.
        It takes 1.860 seconds.
        It takes 2.169 seconds.
        It takes 2.366 seconds.
Building the Feature Vector...
        It takes 2.289 seconds.
        It takes 2.307 seconds.
        It takes 0.251 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.040 seconds.
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.316 seconds.
Building the Label Vector...
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.039 seconds.
        It takes 0.248 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.253 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.171 Gbps (per GPU), 473.370 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.869 Gbps (per GPU), 470.951 Gbps (aggregated)
The layer-level communication performance: 58.874 Gbps (per GPU), 470.992 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.578 Gbps (per GPU), 468.621 Gbps (aggregated)
The layer-level communication performance: 58.614 Gbps (per GPU), 468.908 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.359 Gbps (per GPU), 466.871 Gbps (aggregated)
The layer-level communication performance: 58.317 Gbps (per GPU), 466.534 Gbps (aggregated)
The layer-level communication performance: 58.283 Gbps (per GPU), 466.267 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.381 Gbps (per GPU), 1251.051 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.390 Gbps (per GPU), 1251.121 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.381 Gbps (per GPU), 1251.051 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.367 Gbps (per GPU), 1250.934 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.382 Gbps (per GPU), 1251.056 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.393 Gbps (per GPU), 1251.144 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.384 Gbps (per GPU), 1251.074 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.367 Gbps (per GPU), 1250.934 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.357 Gbps (per GPU), 802.859 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.358 Gbps (per GPU), 802.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.357 Gbps (per GPU), 802.853 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.357 Gbps (per GPU), 802.859 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.354 Gbps (per GPU), 802.834 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.358 Gbps (per GPU), 802.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.357 Gbps (per GPU), 802.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.359 Gbps (per GPU), 802.872 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.627 Gbps (per GPU), 285.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.627 Gbps (per GPU), 285.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.626 Gbps (per GPU), 285.008 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.625 Gbps (per GPU), 285.001 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.627 Gbps (per GPU), 285.012 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.627 Gbps (per GPU), 285.013 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.626 Gbps (per GPU), 285.011 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.625 Gbps (per GPU), 284.999 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.52ms  9.50ms  9.79ms  3.89 29.12K 14.23M
 chk_1  2.53ms  5.15ms  5.51ms  2.18 29.12K  6.56M
 chk_2  2.53ms 16.50ms 16.92ms  6.69 29.12K 24.68M
 chk_3  2.53ms 16.66ms 17.02ms  6.72 29.12K 22.95M
 chk_4  2.53ms  4.97ms  5.34ms  2.11 29.12K  6.33M
 chk_5  2.54ms  8.90ms  9.23ms  3.64 29.12K 12.05M
 chk_6  2.54ms 10.04ms 10.35ms  4.08 29.12K 14.60M
 chk_7  2.53ms  9.18ms  9.64ms  3.81 29.12K 13.21M
   Avg  2.53 10.11 10.47
   Max  2.54 16.66 17.02
   Min  2.52  4.97  5.34
 Ratio  1.01  3.35  3.19
   Var  0.00 17.18 17.26
Profiling takes 2.156 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 343.866 ms
Partition 0 [0, 5) has cost: 343.866 ms
Partition 1 [5, 9) has cost: 323.624 ms
Partition 2 [9, 13) has cost: 323.624 ms
Partition 3 [13, 17) has cost: 323.624 ms
Partition 4 [17, 21) has cost: 323.624 ms
Partition 5 [21, 25) has cost: 323.624 ms
Partition 6 [25, 29) has cost: 323.624 ms
Partition 7 [29, 33) has cost: 326.503 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 300.336 ms
GPU 0, Compute+Comm Time: 123.888 ms, Bubble Time: 129.105 ms, Imbalance Overhead: 47.342 ms
GPU 1, Compute+Comm Time: 118.800 ms, Bubble Time: 119.931 ms, Imbalance Overhead: 61.605 ms
GPU 2, Compute+Comm Time: 118.800 ms, Bubble Time: 110.129 ms, Imbalance Overhead: 71.406 ms
GPU 3, Compute+Comm Time: 118.800 ms, Bubble Time: 110.435 ms, Imbalance Overhead: 71.100 ms
GPU 4, Compute+Comm Time: 118.800 ms, Bubble Time: 119.921 ms, Imbalance Overhead: 61.614 ms
GPU 5, Compute+Comm Time: 118.800 ms, Bubble Time: 128.772 ms, Imbalance Overhead: 52.764 ms
GPU 6, Compute+Comm Time: 118.800 ms, Bubble Time: 137.559 ms, Imbalance Overhead: 43.976 ms
GPU 7, Compute+Comm Time: 119.454 ms, Bubble Time: 147.582 ms, Imbalance Overhead: 33.300 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 587.921 ms
GPU 0, Compute+Comm Time: 232.247 ms, Bubble Time: 290.594 ms, Imbalance Overhead: 65.080 ms
GPU 1, Compute+Comm Time: 230.021 ms, Bubble Time: 271.168 ms, Imbalance Overhead: 86.731 ms
GPU 2, Compute+Comm Time: 230.021 ms, Bubble Time: 253.782 ms, Imbalance Overhead: 104.118 ms
GPU 3, Compute+Comm Time: 230.021 ms, Bubble Time: 236.149 ms, Imbalance Overhead: 121.750 ms
GPU 4, Compute+Comm Time: 230.021 ms, Bubble Time: 216.619 ms, Imbalance Overhead: 141.281 ms
GPU 5, Compute+Comm Time: 230.021 ms, Bubble Time: 215.396 ms, Imbalance Overhead: 142.504 ms
GPU 6, Compute+Comm Time: 230.021 ms, Bubble Time: 234.236 ms, Imbalance Overhead: 123.664 ms
GPU 7, Compute+Comm Time: 245.175 ms, Bubble Time: 251.186 ms, Imbalance Overhead: 91.559 ms
The estimated cost of the whole pipeline: 932.670 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 667.490 ms
Partition 0 [0, 9) has cost: 667.490 ms
Partition 1 [9, 17) has cost: 647.248 ms
Partition 2 [17, 25) has cost: 647.248 ms
Partition 3 [25, 33) has cost: 650.127 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 310.055 ms
GPU 0, Compute+Comm Time: 160.269 ms, Bubble Time: 149.119 ms, Imbalance Overhead: 0.667 ms
GPU 1, Compute+Comm Time: 157.712 ms, Bubble Time: 130.151 ms, Imbalance Overhead: 22.192 ms
GPU 2, Compute+Comm Time: 157.712 ms, Bubble Time: 110.548 ms, Imbalance Overhead: 41.796 ms
GPU 3, Compute+Comm Time: 158.046 ms, Bubble Time: 110.621 ms, Imbalance Overhead: 41.389 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 592.476 ms
GPU 0, Compute+Comm Time: 300.100 ms, Bubble Time: 211.640 ms, Imbalance Overhead: 80.735 ms
GPU 1, Compute+Comm Time: 299.038 ms, Bubble Time: 210.810 ms, Imbalance Overhead: 82.628 ms
GPU 2, Compute+Comm Time: 299.038 ms, Bubble Time: 248.490 ms, Imbalance Overhead: 44.948 ms
GPU 3, Compute+Comm Time: 306.625 ms, Bubble Time: 284.273 ms, Imbalance Overhead: 1.578 ms
    The estimated cost with 2 DP ways is 947.658 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1314.738 ms
Partition 0 [0, 17) has cost: 1314.738 ms
Partition 1 [17, 33) has cost: 1297.375 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 351.006 ms
GPU 0, Compute+Comm Time: 234.049 ms, Bubble Time: 116.957 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 232.953 ms, Bubble Time: 116.534 ms, Imbalance Overhead: 1.519 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 623.049 ms
GPU 0, Compute+Comm Time: 412.710 ms, Bubble Time: 207.175 ms, Imbalance Overhead: 3.164 ms
GPU 1, Compute+Comm Time: 415.923 ms, Bubble Time: 207.126 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1022.757 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2612.113 ms
Partition 0 [0, 33) has cost: 2612.113 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 470.269 ms
GPU 0, Compute+Comm Time: 470.269 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 651.657 ms
GPU 0, Compute+Comm Time: 651.657 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1178.022 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 4, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.9492	TrainAcc 0.0523	ValidAcc 0.0493	TestAcc 0.0504	BestValid 0.0493
	Epoch 50:	Loss 1.6909	TrainAcc 0.6860	ValidAcc 0.7080	TestAcc 0.7034	BestValid 0.7080
	Epoch 100:	Loss 1.1790	TrainAcc 0.7865	ValidAcc 0.7998	TestAcc 0.7960	BestValid 0.7998
	Epoch 150:	Loss 0.9540	TrainAcc 0.8347	ValidAcc 0.8458	TestAcc 0.8401	BestValid 0.8458
	Epoch 200:	Loss 0.8218	TrainAcc 0.8648	ValidAcc 0.8731	TestAcc 0.8677	BestValid 0.8731
	Epoch 250:	Loss 0.7395	TrainAcc 0.8796	ValidAcc 0.8868	TestAcc 0.8814	BestValid 0.8868
	Epoch 300:	Loss 0.6805	TrainAcc 0.8870	ValidAcc 0.8934	TestAcc 0.8881	BestValid 0.8934
	Epoch 350:	Loss 0.6423	TrainAcc 0.8941	ValidAcc 0.9006	TestAcc 0.8949	BestValid 0.9006
	Epoch 400:	Loss 0.6069	TrainAcc 0.8979	ValidAcc 0.9045	TestAcc 0.8995	BestValid 0.9045
	Epoch 450:	Loss 0.5867	TrainAcc 0.8986	ValidAcc 0.9045	TestAcc 0.8997	BestValid 0.9045
	Epoch 500:	Loss 0.5617	TrainAcc 0.9056	ValidAcc 0.9116	TestAcc 0.9065	BestValid 0.9116
	Epoch 550:	Loss 0.5511	TrainAcc 0.9116	ValidAcc 0.9183	TestAcc 0.9129	BestValid 0.9183
	Epoch 600:	Loss 0.5344	TrainAcc 0.9136	ValidAcc 0.9208	TestAcc 0.9148	BestValid 0.9208
	Epoch 650:	Loss 0.5218	TrainAcc 0.9105	ValidAcc 0.9159	TestAcc 0.9110	BestValid 0.9208
	Epoch 700:	Loss 0.5103	TrainAcc 0.9103	ValidAcc 0.9155	TestAcc 0.9108	BestValid 0.9208
	Epoch 750:	Loss 0.5020	TrainAcc 0.9169	ValidAcc 0.9227	TestAcc 0.9177	BestValid 0.9227
	Epoch 800:	Loss 0.4920	TrainAcc 0.9137	ValidAcc 0.9189	TestAcc 0.9144	BestValid 0.9227
	Epoch 850:	Loss 0.4895	TrainAcc 0.9179	ValidAcc 0.9233	TestAcc 0.9181	BestValid 0.9233
	Epoch 900:	Loss 0.4740	TrainAcc 0.9172	ValidAcc 0.9224	TestAcc 0.9174	BestValid 0.9233
	Epoch 950:	Loss 0.4767	TrainAcc 0.9129	ValidAcc 0.9170	TestAcc 0.9131	BestValid 0.9233
	Epoch 1000:	Loss 0.4690	TrainAcc 0.9164	ValidAcc 0.9213	TestAcc 0.9170	BestValid 0.9233
	Epoch 1050:	Loss 0.4674	TrainAcc 0.9188	ValidAcc 0.9228	TestAcc 0.9188	BestValid 0.9233
	Epoch 1100:	Loss 0.4541	TrainAcc 0.9093	ValidAcc 0.9129	TestAcc 0.9082	BestValid 0.9233
	Epoch 1150:	Loss 0.4501	TrainAcc 0.9143	ValidAcc 0.9177	TestAcc 0.9142	BestValid 0.9233
	Epoch 1200:	Loss 0.4435	TrainAcc 0.9170	ValidAcc 0.9201	TestAcc 0.9166	BestValid 0.9233
	Epoch 1250:	Loss 0.4442	TrainAcc 0.9209	ValidAcc 0.9240	TestAcc 0.9205	BestValid 0.9240
	Epoch 1300:	Loss 0.4371	TrainAcc 0.9218	ValidAcc 0.9250	TestAcc 0.9210	BestValid 0.9250
	Epoch 1350:	Loss 0.4323	TrainAcc 0.9252	ValidAcc 0.9280	TestAcc 0.9242	BestValid 0.9280
	Epoch 1400:	Loss 0.4316	TrainAcc 0.9264	ValidAcc 0.9290	TestAcc 0.9254	BestValid 0.9290
	Epoch 1450:	Loss 0.4267	TrainAcc 0.9212	ValidAcc 0.9233	TestAcc 0.9203	BestValid 0.9290
	Epoch 1500:	Loss 0.4243	TrainAcc 0.9228	ValidAcc 0.9248	TestAcc 0.9224	BestValid 0.9290
	Epoch 1550:	Loss 0.4198	TrainAcc 0.9216	ValidAcc 0.9235	TestAcc 0.9208	BestValid 0.9290
	Epoch 1600:	Loss 0.4176	TrainAcc 0.9173	ValidAcc 0.9186	TestAcc 0.9168	BestValid 0.9290
	Epoch 1650:	Loss 0.4177	TrainAcc 0.9231	ValidAcc 0.9248	TestAcc 0.9221	BestValid 0.9290
	Epoch 1700:	Loss 0.4135	TrainAcc 0.9128	ValidAcc 0.9139	TestAcc 0.9113	BestValid 0.9290
	Epoch 1750:	Loss 0.4128	TrainAcc 0.9179	ValidAcc 0.9187	TestAcc 0.9165	BestValid 0.9290
	Epoch 1800:	Loss 0.4191	TrainAcc 0.9222	ValidAcc 0.9228	TestAcc 0.9210	BestValid 0.9290
	Epoch 1850:	Loss 0.4086	TrainAcc 0.9262	ValidAcc 0.9273	TestAcc 0.9250	BestValid 0.9290
	Epoch 1900:	Loss 0.4059	TrainAcc 0.9274	ValidAcc 0.9284	TestAcc 0.9262	BestValid 0.9290
	Epoch 1950:	Loss 0.4012	TrainAcc 0.9205	ValidAcc 0.9214	TestAcc 0.9195	BestValid 0.9290
	Epoch 2000:	Loss 0.4037	TrainAcc 0.9242	ValidAcc 0.9249	TestAcc 0.9225	BestValid 0.9290
	Epoch 2050:	Loss 0.3944	TrainAcc 0.9241	ValidAcc 0.9241	TestAcc 0.9225	BestValid 0.9290
	Epoch 2100:	Loss 0.3945	TrainAcc 0.9292	ValidAcc 0.9300	TestAcc 0.9276	BestValid 0.9300
	Epoch 2150:	Loss 0.3968	TrainAcc 0.9232	ValidAcc 0.9235	TestAcc 0.9215	BestValid 0.9300
	Epoch 2200:	Loss 0.3969	TrainAcc 0.9258	ValidAcc 0.9255	TestAcc 0.9242	BestValid 0.9300
	Epoch 2250:	Loss 0.3884	TrainAcc 0.9271	ValidAcc 0.9267	TestAcc 0.9252	BestValid 0.9300
	Epoch 2300:	Loss 0.3915	TrainAcc 0.9230	ValidAcc 0.9227	TestAcc 0.9213	BestValid 0.9300
	Epoch 2350:	Loss 0.3933	TrainAcc 0.9240	ValidAcc 0.9239	TestAcc 0.9223	BestValid 0.9300
	Epoch 2400:	Loss 0.3834	TrainAcc 0.9211	ValidAcc 0.9210	TestAcc 0.9185	BestValid 0.9300
	Epoch 2450:	Loss 0.3818	TrainAcc 0.9231	ValidAcc 0.9229	TestAcc 0.9209	BestValid 0.9300
	Epoch 2500:	Loss 0.3915	TrainAcc 0.9142	ValidAcc 0.9141	TestAcc 0.9111	BestValid 0.9300
	Epoch 2550:	Loss 0.3806	TrainAcc 0.9248	ValidAcc 0.9249	TestAcc 0.9229	BestValid 0.9300
	Epoch 2600:	Loss 0.3859	TrainAcc 0.9286	ValidAcc 0.9278	TestAcc 0.9263	BestValid 0.9300
	Epoch 2650:	Loss 0.3794	TrainAcc 0.9197	ValidAcc 0.9200	TestAcc 0.9176	BestValid 0.9300
	Epoch 2700:	Loss 0.3785	TrainAcc 0.9229	ValidAcc 0.9228	TestAcc 0.9210	BestValid 0.9300
	Epoch 2750:	Loss 0.3765	TrainAcc 0.9192	ValidAcc 0.9192	TestAcc 0.9167	BestValid 0.9300
	Epoch 2800:	Loss 0.3741	TrainAcc 0.9239	ValidAcc 0.9233	TestAcc 0.9217	BestValid 0.9300
	Epoch 2850:	Loss 0.3808	TrainAcc 0.9220	ValidAcc 0.9216	TestAcc 0.9192	BestValid 0.9300
	Epoch 2900:	Loss 0.3703	TrainAcc 0.9239	ValidAcc 0.9238	TestAcc 0.9218	BestValid 0.9300
	Epoch 2950:	Loss 0.3702	TrainAcc 0.9276	ValidAcc 0.9269	TestAcc 0.9255	BestValid 0.9300
	Epoch 3000:	Loss 0.3713	TrainAcc 0.9311	ValidAcc 0.9303	TestAcc 0.9287	BestValid 0.9303
	Epoch 3050:	Loss 0.3727	TrainAcc 0.9315	ValidAcc 0.9305	TestAcc 0.9293	BestValid 0.9305
	Epoch 3100:	Loss 0.3681	TrainAcc 0.9298	ValidAcc 0.9295	TestAcc 0.9278	BestValid 0.9305
	Epoch 3150:	Loss 0.3704	TrainAcc 0.9216	ValidAcc 0.9206	TestAcc 0.9187	BestValid 0.9305
	Epoch 3200:	Loss 0.3655	TrainAcc 0.9154	ValidAcc 0.9147	TestAcc 0.9121	BestValid 0.9305
	Epoch 3250:	Loss 0.3666	TrainAcc 0.9281	ValidAcc 0.9273	TestAcc 0.9260	BestValid 0.9305
	Epoch 3300:	Loss 0.3624	TrainAcc 0.9163	ValidAcc 0.9152	TestAcc 0.9131	BestValid 0.9305
	Epoch 3350:	Loss 0.3596	TrainAcc 0.9219	ValidAcc 0.9211	TestAcc 0.9191	BestValid 0.9305
	Epoch 3400:	Loss 0.3735	TrainAcc 0.9118	ValidAcc 0.9104	TestAcc 0.9084	BestValid 0.9305
	Epoch 3450:	Loss 0.3675	TrainAcc 0.9197	ValidAcc 0.9180	TestAcc 0.9160	BestValid 0.9305
	Epoch 3500:	Loss 0.3613	TrainAcc 0.9200	ValidAcc 0.9192	TestAcc 0.9172	BestValid 0.9305
	Epoch 3550:	Loss 0.3620	TrainAcc 0.9217	ValidAcc 0.9206	TestAcc 0.9189	BestValid 0.9305
	Epoch 3600:	Loss 0.3596	TrainAcc 0.9343	ValidAcc 0.9335	TestAcc 0.9312	BestValid 0.9335
	Epoch 3650:	Loss 0.3563	TrainAcc 0.9210	ValidAcc 0.9198	TestAcc 0.9176	BestValid 0.9335
	Epoch 3700:	Loss 0.3551	TrainAcc 0.9264	ValidAcc 0.9254	TestAcc 0.9240	BestValid 0.9335
	Epoch 3750:	Loss 0.3597	TrainAcc 0.9178	ValidAcc 0.9162	TestAcc 0.9142	BestValid 0.9335
	Epoch 3800:	Loss 0.3567	TrainAcc 0.9285	ValidAcc 0.9272	TestAcc 0.9253	BestValid 0.9335
	Epoch 3850:	Loss 0.3675	TrainAcc 0.9102	ValidAcc 0.9089	TestAcc 0.9062	BestValid 0.9335
	Epoch 3900:	Loss 0.3511	TrainAcc 0.9262	ValidAcc 0.9245	TestAcc 0.9229	BestValid 0.9335
	Epoch 3950:	Loss 0.3598	TrainAcc 0.9152	ValidAcc 0.9139	TestAcc 0.9108	BestValid 0.9335
	Epoch 4000:	Loss 0.3509	TrainAcc 0.9208	ValidAcc 0.9201	TestAcc 0.9179	BestValid 0.9335
	Epoch 4050:	Loss 0.3567	TrainAcc 0.9326	ValidAcc 0.9319	TestAcc 0.9292	BestValid 0.9335
	Epoch 4100:	Loss 0.3565	TrainAcc 0.9278	ValidAcc 0.9265	TestAcc 0.9249	BestValid 0.9335
	Epoch 4150:	Loss 0.3511	TrainAcc 0.9168	ValidAcc 0.9145	TestAcc 0.9129	BestValid 0.9335
	Epoch 4200:	Loss 0.3476	TrainAcc 0.9197	ValidAcc 0.9183	TestAcc 0.9163	BestValid 0.9335
	Epoch 4250:	Loss 0.3510	TrainAcc 0.9231	ValidAcc 0.9215	TestAcc 0.9193	BestValid 0.9335
	Epoch 4300:	Loss 0.3457	TrainAcc 0.9165	ValidAcc 0.9145	TestAcc 0.9125	BestValid 0.9335
	Epoch 4350:	Loss 0.3520	TrainAcc 0.9214	ValidAcc 0.9196	TestAcc 0.9180	BestValid 0.9335
	Epoch 4400:	Loss 0.3444	TrainAcc 0.9368	ValidAcc 0.9364	TestAcc 0.9337	BestValid 0.9364
	Epoch 4450:	Loss 0.3437	TrainAcc 0.9319	ValidAcc 0.9312	TestAcc 0.9289	BestValid 0.9364
	Epoch 4500:	Loss 0.3474	TrainAcc 0.9332	ValidAcc 0.9321	TestAcc 0.9303	BestValid 0.9364
	Epoch 4550:	Loss 0.3482	TrainAcc 0.9316	ValidAcc 0.9308	TestAcc 0.9291	BestValid 0.9364
	Epoch 4600:	Loss 0.3427	TrainAcc 0.9235	ValidAcc 0.9224	TestAcc 0.9206	BestValid 0.9364
	Epoch 4650:	Loss 0.3465	TrainAcc 0.9239	ValidAcc 0.9225	TestAcc 0.9202	BestValid 0.9364
	Epoch 4700:	Loss 0.3400	TrainAcc 0.9310	ValidAcc 0.9301	TestAcc 0.9279	BestValid 0.9364
	Epoch 4750:	Loss 0.3445	TrainAcc 0.9289	ValidAcc 0.9280	TestAcc 0.9259	BestValid 0.9364
	Epoch 4800:	Loss 0.3457	TrainAcc 0.9213	ValidAcc 0.9195	TestAcc 0.9170	BestValid 0.9364
	Epoch 4850:	Loss 0.3485	TrainAcc 0.9312	ValidAcc 0.9302	TestAcc 0.9279	BestValid 0.9364
	Epoch 4900:	Loss 0.3383	TrainAcc 0.9314	ValidAcc 0.9304	TestAcc 0.9285	BestValid 0.9364
	Epoch 4950:	Loss 0.3388	TrainAcc 0.9283	ValidAcc 0.9271	TestAcc 0.9249	BestValid 0.9364
	Epoch 5000:	Loss 0.3438	TrainAcc 0.9282	ValidAcc 0.9272	TestAcc 0.9247	BestValid 0.9364
****** Epoch Time (Excluding Evaluation Cost): 0.901 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.907 ms (Max: 1.280, Min: 0.104, Sum: 7.258)
Cluster-Wide Average, Compute: 247.539 ms (Max: 385.201, Min: 140.238, Sum: 1980.316)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.012, Min: 0.008, Sum: 0.072)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.019, Min: 0.015, Sum: 0.133)
Cluster-Wide Average, Communication-Graph: 635.810 ms (Max: 742.811, Min: 498.918, Sum: 5086.478)
Cluster-Wide Average, Optimization: 3.201 ms (Max: 3.238, Min: 3.175, Sum: 25.606)
Cluster-Wide Average, Others: 13.841 ms (Max: 13.900, Min: 13.806, Sum: 110.730)
****** Breakdown Sum: 901.324 ms ******
Cluster-Wide Average, GPU Memory Consumption: 16.524 GB (Max: 17.239, Min: 16.405, Sum: 132.192)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.667 Gbps (Max: 49.505, Min: 11.514, Sum: 213.335)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.020 GB
Total communication (cluster-wide, per-epoch): 14.502 GB
****** Accuracy Results ******
Highest valid_acc: 0.9364
Target test_acc: 0.9337
Epoch to reach the target acc: 4399
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
