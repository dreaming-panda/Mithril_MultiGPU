Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.860 seconds.
Building the CSC structure...
        It takes 1.879 seconds.
Building the CSC structure...
        It takes 2.015 seconds.
Building the CSC structure...
        It takes 2.038 seconds.
Building the CSC structure...
        It takes 2.357 seconds.
Building the CSC structure...
        It takes 2.386 seconds.
Building the CSC structure...
        It takes 2.395 seconds.
Building the CSC structure...
        It takes 2.639 seconds.
Building the CSC structure...
        It takes 1.815 seconds.
        It takes 1.867 seconds.
        It takes 1.871 seconds.
        It takes 1.876 seconds.
        It takes 2.181 seconds.
Building the Feature Vector...
        It takes 2.294 seconds.
Building the Feature Vector...
        It takes 2.310 seconds.
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 2.366 seconds.
        It takes 0.034 seconds.
Building the Feature Vector...
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.259 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.042 seconds.
        It takes 0.036 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.772 Gbps (per GPU), 470.178 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.503 Gbps (per GPU), 468.022 Gbps (aggregated)
The layer-level communication performance: 58.495 Gbps (per GPU), 467.957 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.263 Gbps (per GPU), 466.108 Gbps (aggregated)
The layer-level communication performance: 58.224 Gbps (per GPU), 465.791 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.025 Gbps (per GPU), 464.202 Gbps (aggregated)
The layer-level communication performance: 57.952 Gbps (per GPU), 463.617 Gbps (aggregated)
The layer-level communication performance: 57.990 Gbps (per GPU), 463.920 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.368 Gbps (per GPU), 1266.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.362 Gbps (per GPU), 1266.898 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.374 Gbps (per GPU), 1266.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.404 Gbps (per GPU), 1267.233 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.371 Gbps (per GPU), 1266.970 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.359 Gbps (per GPU), 1266.874 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.368 Gbps (per GPU), 1266.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.425 Gbps (per GPU), 1267.404 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 95.686 Gbps (per GPU), 765.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.685 Gbps (per GPU), 765.484 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.710 Gbps (per GPU), 765.681 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.685 Gbps (per GPU), 765.483 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.685 Gbps (per GPU), 765.477 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.687 Gbps (per GPU), 765.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.709 Gbps (per GPU), 765.675 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.684 Gbps (per GPU), 765.471 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.993 Gbps (per GPU), 303.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.992 Gbps (per GPU), 303.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.992 Gbps (per GPU), 303.940 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.992 Gbps (per GPU), 303.939 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.992 Gbps (per GPU), 303.939 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.994 Gbps (per GPU), 303.949 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.991 Gbps (per GPU), 303.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.993 Gbps (per GPU), 303.941 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.45ms  8.96ms  8.65ms  1.09 29.12K 14.23M
 chk_1  5.18ms  4.56ms  4.30ms  1.21 29.12K  6.56M
 chk_2 16.65ms 16.01ms 15.66ms  1.06 29.12K 24.68M
 chk_3 16.68ms 16.12ms 15.85ms  1.05 29.12K 22.95M
 chk_4  4.98ms  4.37ms  4.09ms  1.22 29.12K  6.33M
 chk_5  8.94ms  8.31ms  8.12ms  1.10 29.12K 12.05M
 chk_6 10.17ms  9.51ms  9.24ms  1.10 29.12K 14.60M
 chk_7  9.42ms  8.62ms  8.28ms  1.14 29.12K 13.21M
   Avg 10.19  9.56  9.27
   Max 16.68 16.12 15.85
   Min  4.98  4.37  4.09
 Ratio  3.35  3.69  3.88
   Var 17.36 17.42 17.30
Profiling takes 2.661 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 310.877 ms
Partition 0 [0, 4) has cost: 310.877 ms
Partition 1 [4, 8) has cost: 305.855 ms
Partition 2 [8, 12) has cost: 305.855 ms
Partition 3 [12, 16) has cost: 305.855 ms
Partition 4 [16, 20) has cost: 305.855 ms
Partition 5 [20, 24) has cost: 305.855 ms
Partition 6 [24, 28) has cost: 305.855 ms
Partition 7 [28, 32) has cost: 303.581 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 289.154 ms
GPU 0, Compute+Comm Time: 115.147 ms, Bubble Time: 124.325 ms, Imbalance Overhead: 49.683 ms
GPU 1, Compute+Comm Time: 113.463 ms, Bubble Time: 114.650 ms, Imbalance Overhead: 61.041 ms
GPU 2, Compute+Comm Time: 113.463 ms, Bubble Time: 104.906 ms, Imbalance Overhead: 70.786 ms
GPU 3, Compute+Comm Time: 113.463 ms, Bubble Time: 105.041 ms, Imbalance Overhead: 70.650 ms
GPU 4, Compute+Comm Time: 113.463 ms, Bubble Time: 114.026 ms, Imbalance Overhead: 61.666 ms
GPU 5, Compute+Comm Time: 113.463 ms, Bubble Time: 122.796 ms, Imbalance Overhead: 52.896 ms
GPU 6, Compute+Comm Time: 113.463 ms, Bubble Time: 131.631 ms, Imbalance Overhead: 44.060 ms
GPU 7, Compute+Comm Time: 112.809 ms, Bubble Time: 141.832 ms, Imbalance Overhead: 34.513 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 559.516 ms
GPU 0, Compute+Comm Time: 216.141 ms, Bubble Time: 275.766 ms, Imbalance Overhead: 67.610 ms
GPU 1, Compute+Comm Time: 217.761 ms, Bubble Time: 255.607 ms, Imbalance Overhead: 86.149 ms
GPU 2, Compute+Comm Time: 217.761 ms, Bubble Time: 237.725 ms, Imbalance Overhead: 104.030 ms
GPU 3, Compute+Comm Time: 217.761 ms, Bubble Time: 220.051 ms, Imbalance Overhead: 121.705 ms
GPU 4, Compute+Comm Time: 217.761 ms, Bubble Time: 202.030 ms, Imbalance Overhead: 139.726 ms
GPU 5, Compute+Comm Time: 217.761 ms, Bubble Time: 201.713 ms, Imbalance Overhead: 140.042 ms
GPU 6, Compute+Comm Time: 217.761 ms, Bubble Time: 220.607 ms, Imbalance Overhead: 121.148 ms
GPU 7, Compute+Comm Time: 221.099 ms, Bubble Time: 239.084 ms, Imbalance Overhead: 99.333 ms
The estimated cost of the whole pipeline: 891.104 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 616.733 ms
Partition 0 [0, 8) has cost: 616.733 ms
Partition 1 [8, 16) has cost: 611.711 ms
Partition 2 [16, 24) has cost: 611.711 ms
Partition 3 [24, 32) has cost: 609.437 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 299.524 ms
GPU 0, Compute+Comm Time: 153.092 ms, Bubble Time: 144.965 ms, Imbalance Overhead: 1.466 ms
GPU 1, Compute+Comm Time: 152.299 ms, Bubble Time: 125.546 ms, Imbalance Overhead: 21.678 ms
GPU 2, Compute+Comm Time: 152.299 ms, Bubble Time: 106.057 ms, Imbalance Overhead: 41.167 ms
GPU 3, Compute+Comm Time: 151.971 ms, Bubble Time: 105.995 ms, Imbalance Overhead: 41.557 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 566.654 ms
GPU 0, Compute+Comm Time: 286.356 ms, Bubble Time: 199.753 ms, Imbalance Overhead: 80.545 ms
GPU 1, Compute+Comm Time: 287.154 ms, Bubble Time: 199.710 ms, Imbalance Overhead: 79.791 ms
GPU 2, Compute+Comm Time: 287.154 ms, Bubble Time: 237.498 ms, Imbalance Overhead: 42.003 ms
GPU 3, Compute+Comm Time: 288.685 ms, Bubble Time: 274.868 ms, Imbalance Overhead: 3.101 ms
    The estimated cost with 2 DP ways is 909.487 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1228.444 ms
Partition 0 [0, 16) has cost: 1228.444 ms
Partition 1 [16, 32) has cost: 1221.148 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 345.685 ms
GPU 0, Compute+Comm Time: 230.403 ms, Bubble Time: 115.283 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 229.762 ms, Bubble Time: 114.825 ms, Imbalance Overhead: 1.098 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 605.576 ms
GPU 0, Compute+Comm Time: 402.545 ms, Bubble Time: 201.343 ms, Imbalance Overhead: 1.688 ms
GPU 1, Compute+Comm Time: 403.729 ms, Bubble Time: 201.847 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 998.824 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2449.592 ms
Partition 0 [0, 32) has cost: 2449.592 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 446.136 ms
GPU 0, Compute+Comm Time: 446.136 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 619.509 ms
GPU 0, Compute+Comm Time: 619.509 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1118.928 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7136	TrainAcc 0.0241	ValidAcc 0.0201	TestAcc 0.0207	BestValid 0.0201
	Epoch 50:	Loss 3.3045	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.1223	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.1121	TrainAcc 0.0717	ValidAcc 0.0614	TestAcc 0.0596	BestValid 0.0614
	Epoch 200:	Loss 2.8354	TrainAcc 0.1974	ValidAcc 0.1791	TestAcc 0.1813	BestValid 0.1791
	Epoch 250:	Loss 2.7925	TrainAcc 0.3280	ValidAcc 0.3730	TestAcc 0.3750	BestValid 0.3730
	Epoch 300:	Loss 2.5281	TrainAcc 0.3537	ValidAcc 0.3921	TestAcc 0.3952	BestValid 0.3921
	Epoch 350:	Loss 3.0754	TrainAcc 0.2441	ValidAcc 0.2750	TestAcc 0.2745	BestValid 0.3921
	Epoch 400:	Loss 1.8439	TrainAcc 0.4579	ValidAcc 0.5019	TestAcc 0.4982	BestValid 0.5019
	Epoch 450:	Loss 2.0258	TrainAcc 0.4350	ValidAcc 0.4768	TestAcc 0.4776	BestValid 0.5019
	Epoch 500:	Loss 1.8970	TrainAcc 0.4227	ValidAcc 0.4599	TestAcc 0.4618	BestValid 0.5019
	Epoch 550:	Loss 1.7410	TrainAcc 0.5086	ValidAcc 0.5396	TestAcc 0.5402	BestValid 0.5396
	Epoch 600:	Loss 1.7293	TrainAcc 0.4294	ValidAcc 0.4477	TestAcc 0.4520	BestValid 0.5396
	Epoch 650:	Loss 1.5458	TrainAcc 0.5785	ValidAcc 0.6044	TestAcc 0.6030	BestValid 0.6044
	Epoch 700:	Loss 2.4389	TrainAcc 0.5066	ValidAcc 0.5389	TestAcc 0.5384	BestValid 0.6044
	Epoch 750:	Loss 1.3931	TrainAcc 0.5403	ValidAcc 0.5700	TestAcc 0.5710	BestValid 0.6044
	Epoch 800:	Loss 1.4427	TrainAcc 0.5881	ValidAcc 0.6099	TestAcc 0.6117	BestValid 0.6099
	Epoch 850:	Loss 1.3693	TrainAcc 0.5791	ValidAcc 0.6057	TestAcc 0.6068	BestValid 0.6099
	Epoch 900:	Loss 1.5774	TrainAcc 0.5958	ValidAcc 0.6205	TestAcc 0.6172	BestValid 0.6205
	Epoch 950:	Loss 1.3652	TrainAcc 0.5962	ValidAcc 0.6174	TestAcc 0.6196	BestValid 0.6205
	Epoch 1000:	Loss 1.3123	TrainAcc 0.6438	ValidAcc 0.6661	TestAcc 0.6676	BestValid 0.6661
	Epoch 1050:	Loss 1.8762	TrainAcc 0.5161	ValidAcc 0.5461	TestAcc 0.5444	BestValid 0.6661
	Epoch 1100:	Loss 1.2410	TrainAcc 0.6528	ValidAcc 0.6697	TestAcc 0.6699	BestValid 0.6697
	Epoch 1150:	Loss 1.4318	TrainAcc 0.5890	ValidAcc 0.6137	TestAcc 0.6122	BestValid 0.6697
	Epoch 1200:	Loss 1.1416	TrainAcc 0.6736	ValidAcc 0.6876	TestAcc 0.6895	BestValid 0.6876
	Epoch 1250:	Loss 1.3526	TrainAcc 0.5996	ValidAcc 0.6226	TestAcc 0.6224	BestValid 0.6876
	Epoch 1300:	Loss 1.0714	TrainAcc 0.6400	ValidAcc 0.6564	TestAcc 0.6586	BestValid 0.6876
	Epoch 1350:	Loss 1.0741	TrainAcc 0.6494	ValidAcc 0.6690	TestAcc 0.6701	BestValid 0.6876
	Epoch 1400:	Loss 2.1933	TrainAcc 0.5776	ValidAcc 0.5998	TestAcc 0.5997	BestValid 0.6876
	Epoch 1450:	Loss 1.1313	TrainAcc 0.6692	ValidAcc 0.6859	TestAcc 0.6876	BestValid 0.6876
	Epoch 1500:	Loss 1.1015	TrainAcc 0.7181	ValidAcc 0.7309	TestAcc 0.7326	BestValid 0.7309
	Epoch 1550:	Loss 2.2992	TrainAcc 0.5860	ValidAcc 0.5977	TestAcc 0.6013	BestValid 0.7309
	Epoch 1600:	Loss 1.0685	TrainAcc 0.7348	ValidAcc 0.7460	TestAcc 0.7482	BestValid 0.7460
	Epoch 1650:	Loss 1.1075	TrainAcc 0.5986	ValidAcc 0.6178	TestAcc 0.6201	BestValid 0.7460
	Epoch 1700:	Loss 1.0463	TrainAcc 0.7141	ValidAcc 0.7272	TestAcc 0.7274	BestValid 0.7460
	Epoch 1750:	Loss 0.9438	TrainAcc 0.6326	ValidAcc 0.6548	TestAcc 0.6550	BestValid 0.7460
	Epoch 1800:	Loss 0.9110	TrainAcc 0.6452	ValidAcc 0.6654	TestAcc 0.6682	BestValid 0.7460
	Epoch 1850:	Loss 3.3228	TrainAcc 0.4355	ValidAcc 0.4581	TestAcc 0.4627	BestValid 0.7460
	Epoch 1900:	Loss 1.0220	TrainAcc 0.6500	ValidAcc 0.6660	TestAcc 0.6711	BestValid 0.7460
	Epoch 1950:	Loss 0.9406	TrainAcc 0.7354	ValidAcc 0.7454	TestAcc 0.7495	BestValid 0.7460
	Epoch 2000:	Loss 0.9416	TrainAcc 0.6749	ValidAcc 0.6850	TestAcc 0.6886	BestValid 0.7460
	Epoch 2050:	Loss 0.9767	TrainAcc 0.7253	ValidAcc 0.7393	TestAcc 0.7415	BestValid 0.7460
	Epoch 2100:	Loss 0.8552	TrainAcc 0.7479	ValidAcc 0.7587	TestAcc 0.7627	BestValid 0.7587
	Epoch 2150:	Loss 0.8509	TrainAcc 0.6719	ValidAcc 0.6922	TestAcc 0.6928	BestValid 0.7587
	Epoch 2200:	Loss 0.7981	TrainAcc 0.6225	ValidAcc 0.6409	TestAcc 0.6414	BestValid 0.7587
	Epoch 2250:	Loss 1.4099	TrainAcc 0.6317	ValidAcc 0.6440	TestAcc 0.6464	BestValid 0.7587
	Epoch 2300:	Loss 1.1339	TrainAcc 0.6025	ValidAcc 0.6263	TestAcc 0.6296	BestValid 0.7587
	Epoch 2350:	Loss 0.8513	TrainAcc 0.6467	ValidAcc 0.6685	TestAcc 0.6709	BestValid 0.7587
	Epoch 2400:	Loss 1.0077	TrainAcc 0.7593	ValidAcc 0.7697	TestAcc 0.7746	BestValid 0.7697
	Epoch 2450:	Loss 0.7836	TrainAcc 0.7120	ValidAcc 0.7287	TestAcc 0.7339	BestValid 0.7697
	Epoch 2500:	Loss 0.7585	TrainAcc 0.6627	ValidAcc 0.6836	TestAcc 0.6848	BestValid 0.7697
	Epoch 2550:	Loss 0.7706	TrainAcc 0.6915	ValidAcc 0.7091	TestAcc 0.7095	BestValid 0.7697
	Epoch 2600:	Loss 0.7785	TrainAcc 0.7598	ValidAcc 0.7720	TestAcc 0.7761	BestValid 0.7720
	Epoch 2650:	Loss 0.7485	TrainAcc 0.7297	ValidAcc 0.7450	TestAcc 0.7486	BestValid 0.7720
	Epoch 2700:	Loss 0.7069	TrainAcc 0.6398	ValidAcc 0.6654	TestAcc 0.6666	BestValid 0.7720
	Epoch 2750:	Loss 0.7246	TrainAcc 0.7282	ValidAcc 0.7433	TestAcc 0.7455	BestValid 0.7720
	Epoch 2800:	Loss 0.7170	TrainAcc 0.6415	ValidAcc 0.6641	TestAcc 0.6641	BestValid 0.7720
	Epoch 2850:	Loss 0.6900	TrainAcc 0.6568	ValidAcc 0.6790	TestAcc 0.6787	BestValid 0.7720
	Epoch 2900:	Loss 0.7964	TrainAcc 0.6436	ValidAcc 0.6679	TestAcc 0.6669	BestValid 0.7720
	Epoch 2950:	Loss 1.1690	TrainAcc 0.5935	ValidAcc 0.6207	TestAcc 0.6213	BestValid 0.7720
	Epoch 3000:	Loss 0.7773	TrainAcc 0.7862	ValidAcc 0.7931	TestAcc 0.7965	BestValid 0.7931
	Epoch 3050:	Loss 0.7308	TrainAcc 0.7460	ValidAcc 0.7604	TestAcc 0.7642	BestValid 0.7931
	Epoch 3100:	Loss 0.8389	TrainAcc 0.7039	ValidAcc 0.7194	TestAcc 0.7226	BestValid 0.7931
	Epoch 3150:	Loss 0.8869	TrainAcc 0.7579	ValidAcc 0.7681	TestAcc 0.7713	BestValid 0.7931
	Epoch 3200:	Loss 0.6668	TrainAcc 0.7521	ValidAcc 0.7648	TestAcc 0.7690	BestValid 0.7931
	Epoch 3250:	Loss 0.7776	TrainAcc 0.7311	ValidAcc 0.7486	TestAcc 0.7483	BestValid 0.7931
	Epoch 3300:	Loss 0.7298	TrainAcc 0.7002	ValidAcc 0.7191	TestAcc 0.7213	BestValid 0.7931
	Epoch 3350:	Loss 0.6655	TrainAcc 0.7047	ValidAcc 0.7241	TestAcc 0.7253	BestValid 0.7931
	Epoch 3400:	Loss 0.7751	TrainAcc 0.6326	ValidAcc 0.6601	TestAcc 0.6598	BestValid 0.7931
	Epoch 3450:	Loss 0.6305	TrainAcc 0.6926	ValidAcc 0.7133	TestAcc 0.7146	BestValid 0.7931
	Epoch 3500:	Loss 0.7189	TrainAcc 0.7740	ValidAcc 0.7878	TestAcc 0.7911	BestValid 0.7931
	Epoch 3550:	Loss 0.6221	TrainAcc 0.7478	ValidAcc 0.7631	TestAcc 0.7654	BestValid 0.7931
	Epoch 3600:	Loss 0.7001	TrainAcc 0.7378	ValidAcc 0.7543	TestAcc 0.7555	BestValid 0.7931
	Epoch 3650:	Loss 0.6241	TrainAcc 0.7301	ValidAcc 0.7482	TestAcc 0.7499	BestValid 0.7931
	Epoch 3700:	Loss 2.1357	TrainAcc 0.4285	ValidAcc 0.4508	TestAcc 0.4478	BestValid 0.7931
	Epoch 3750:	Loss 0.6724	TrainAcc 0.7241	ValidAcc 0.7421	TestAcc 0.7445	BestValid 0.7931
	Epoch 3800:	Loss 0.6935	TrainAcc 0.7098	ValidAcc 0.7300	TestAcc 0.7311	BestValid 0.7931
	Epoch 3850:	Loss 0.6114	TrainAcc 0.7350	ValidAcc 0.7517	TestAcc 0.7543	BestValid 0.7931
	Epoch 3900:	Loss 0.6039	TrainAcc 0.7293	ValidAcc 0.7457	TestAcc 0.7469	BestValid 0.7931
	Epoch 3950:	Loss 0.6071	TrainAcc 0.7694	ValidAcc 0.7832	TestAcc 0.7865	BestValid 0.7931
	Epoch 4000:	Loss 0.6668	TrainAcc 0.6612	ValidAcc 0.6803	TestAcc 0.6807	BestValid 0.7931
	Epoch 4050:	Loss 0.6288	TrainAcc 0.6944	ValidAcc 0.7148	TestAcc 0.7153	BestValid 0.7931
	Epoch 4100:	Loss 0.6129	TrainAcc 0.7518	ValidAcc 0.7654	TestAcc 0.7658	BestValid 0.7931
	Epoch 4150:	Loss 0.5882	TrainAcc 0.7746	ValidAcc 0.7899	TestAcc 0.7913	BestValid 0.7931
	Epoch 4200:	Loss 0.6266	TrainAcc 0.6310	ValidAcc 0.6573	TestAcc 0.6579	BestValid 0.7931
	Epoch 4250:	Loss 0.6012	TrainAcc 0.7121	ValidAcc 0.7353	TestAcc 0.7367	BestValid 0.7931
	Epoch 4300:	Loss 0.6140	TrainAcc 0.6601	ValidAcc 0.6842	TestAcc 0.6836	BestValid 0.7931
	Epoch 4350:	Loss 0.5794	TrainAcc 0.6723	ValidAcc 0.6970	TestAcc 0.6974	BestValid 0.7931
	Epoch 4400:	Loss 0.6561	TrainAcc 0.7351	ValidAcc 0.7502	TestAcc 0.7534	BestValid 0.7931
	Epoch 4450:	Loss 0.6123	TrainAcc 0.7330	ValidAcc 0.7491	TestAcc 0.7523	BestValid 0.7931
	Epoch 4500:	Loss 0.5712	TrainAcc 0.7427	ValidAcc 0.7581	TestAcc 0.7617	BestValid 0.7931
	Epoch 4550:	Loss 0.5743	TrainAcc 0.7386	ValidAcc 0.7552	TestAcc 0.7582	BestValid 0.7931
	Epoch 4600:	Loss 1.0263	TrainAcc 0.6987	ValidAcc 0.7219	TestAcc 0.7257	BestValid 0.7931
	Epoch 4650:	Loss 0.5608	TrainAcc 0.7733	ValidAcc 0.7874	TestAcc 0.7889	BestValid 0.7931
	Epoch 4700:	Loss 0.5916	TrainAcc 0.7333	ValidAcc 0.7511	TestAcc 0.7529	BestValid 0.7931
	Epoch 4750:	Loss 0.5985	TrainAcc 0.6699	ValidAcc 0.6938	TestAcc 0.6938	BestValid 0.7931
	Epoch 4800:	Loss 0.5590	TrainAcc 0.6749	ValidAcc 0.6926	TestAcc 0.6923	BestValid 0.7931
	Epoch 4850:	Loss 0.5688	TrainAcc 0.7684	ValidAcc 0.7837	TestAcc 0.7857	BestValid 0.7931
	Epoch 4900:	Loss 0.5473	TrainAcc 0.7510	ValidAcc 0.7677	TestAcc 0.7698	BestValid 0.7931
	Epoch 4950:	Loss 0.5246	TrainAcc 0.7266	ValidAcc 0.7459	TestAcc 0.7453	BestValid 0.7931
	Epoch 5000:	Loss 0.5469	TrainAcc 0.7438	ValidAcc 0.7592	TestAcc 0.7627	BestValid 0.7931
****** Epoch Time (Excluding Evaluation Cost): 0.875 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.910 ms (Max: 1.318, Min: 0.067, Sum: 7.277)
Cluster-Wide Average, Compute: 222.041 ms (Max: 359.617, Min: 117.182, Sum: 1776.330)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.011, Min: 0.008, Sum: 0.075)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.018, Min: 0.015, Sum: 0.136)
Cluster-Wide Average, Communication-Graph: 634.739 ms (Max: 739.372, Min: 497.948, Sum: 5077.913)
Cluster-Wide Average, Optimization: 3.062 ms (Max: 3.082, Min: 3.033, Sum: 24.499)
Cluster-Wide Average, Others: 13.837 ms (Max: 13.873, Min: 13.804, Sum: 110.694)
****** Breakdown Sum: 874.616 ms ******
Cluster-Wide Average, GPU Memory Consumption: 14.566 GB (Max: 15.104, Min: 14.471, Sum: 116.530)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.703 Gbps (Max: 49.618, Min: 11.570, Sum: 213.626)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 14.501 GB
****** Accuracy Results ******
Highest valid_acc: 0.7931
Target test_acc: 0.7965
Epoch to reach the target acc: 2999
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
