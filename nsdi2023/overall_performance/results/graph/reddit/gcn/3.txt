Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.870 seconds.
Building the CSC structure...
        It takes 2.008 seconds.
Building the CSC structure...
        It takes 2.052 seconds.
Building the CSC structure...
        It takes 2.357 seconds.
Building the CSC structure...
        It takes 2.367 seconds.
Building the CSC structure...
        It takes 2.400 seconds.
Building the CSC structure...
        It takes 2.406 seconds.
Building the CSC structure...
        It takes 2.626 seconds.
Building the CSC structure...
        It takes 1.787 seconds.
        It takes 1.866 seconds.
        It takes 1.860 seconds.
        It takes 2.173 seconds.
Building the Feature Vector...
        It takes 2.284 seconds.
        It takes 2.351 seconds.
        It takes 2.330 seconds.
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 2.307 seconds.
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.249 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.298 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
        It takes 0.041 seconds.
        It takes 0.304 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.259 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.030 seconds.
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.676 Gbps (per GPU), 445.407 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.438 Gbps (per GPU), 443.507 Gbps (aggregated)
The layer-level communication performance: 55.430 Gbps (per GPU), 443.442 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.194 Gbps (per GPU), 441.551 Gbps (aggregated)
The layer-level communication performance: 55.225 Gbps (per GPU), 441.803 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.014 Gbps (per GPU), 440.113 Gbps (aggregated)
The layer-level communication performance: 54.979 Gbps (per GPU), 439.829 Gbps (aggregated)
The layer-level communication performance: 54.945 Gbps (per GPU), 439.564 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.776 Gbps (per GPU), 1270.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.769 Gbps (per GPU), 1270.156 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.779 Gbps (per GPU), 1270.231 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.878 Gbps (per GPU), 1271.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.776 Gbps (per GPU), 1270.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.773 Gbps (per GPU), 1270.183 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.776 Gbps (per GPU), 1270.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.878 Gbps (per GPU), 1271.026 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.821 Gbps (per GPU), 814.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.822 Gbps (per GPU), 814.579 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.821 Gbps (per GPU), 814.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.817 Gbps (per GPU), 814.533 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.822 Gbps (per GPU), 814.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.822 Gbps (per GPU), 814.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.820 Gbps (per GPU), 814.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.822 Gbps (per GPU), 814.580 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.839 Gbps (per GPU), 286.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.839 Gbps (per GPU), 286.712 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.839 Gbps (per GPU), 286.710 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.839 Gbps (per GPU), 286.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.840 Gbps (per GPU), 286.716 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.838 Gbps (per GPU), 286.707 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.840 Gbps (per GPU), 286.717 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.839 Gbps (per GPU), 286.710 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.51ms  8.96ms  8.73ms  1.09 29.12K 14.23M
 chk_1  5.21ms  4.59ms  4.30ms  1.21 29.12K  6.56M
 chk_2 16.66ms 16.03ms 15.82ms  1.05 29.12K 24.68M
 chk_3 16.79ms 16.22ms 15.92ms  1.05 29.12K 22.95M
 chk_4  5.01ms  4.42ms  4.10ms  1.22 29.12K  6.33M
 chk_5  9.10ms  8.40ms  8.12ms  1.12 29.12K 12.05M
 chk_6 10.27ms  9.54ms  9.25ms  1.11 29.12K 14.60M
 chk_7  9.44ms  8.63ms  8.36ms  1.13 29.12K 13.21M
   Avg 10.25  9.60  9.32
   Max 16.79 16.22 15.92
   Min  5.01  4.42  4.10
 Ratio  3.35  3.67  3.88
   Var 17.40 17.49 17.61
Profiling takes 2.658 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 312.320 ms
Partition 0 [0, 4) has cost: 312.320 ms
Partition 1 [4, 8) has cost: 307.124 ms
Partition 2 [8, 12) has cost: 307.124 ms
Partition 3 [12, 16) has cost: 307.124 ms
Partition 4 [16, 20) has cost: 307.124 ms
Partition 5 [20, 24) has cost: 307.124 ms
Partition 6 [24, 28) has cost: 307.124 ms
Partition 7 [28, 32) has cost: 304.942 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 291.521 ms
GPU 0, Compute+Comm Time: 116.386 ms, Bubble Time: 125.407 ms, Imbalance Overhead: 49.727 ms
GPU 1, Compute+Comm Time: 114.642 ms, Bubble Time: 115.585 ms, Imbalance Overhead: 61.293 ms
GPU 2, Compute+Comm Time: 114.642 ms, Bubble Time: 105.690 ms, Imbalance Overhead: 71.188 ms
GPU 3, Compute+Comm Time: 114.642 ms, Bubble Time: 105.772 ms, Imbalance Overhead: 71.106 ms
GPU 4, Compute+Comm Time: 114.642 ms, Bubble Time: 114.848 ms, Imbalance Overhead: 62.031 ms
GPU 5, Compute+Comm Time: 114.642 ms, Bubble Time: 123.701 ms, Imbalance Overhead: 53.177 ms
GPU 6, Compute+Comm Time: 114.642 ms, Bubble Time: 132.634 ms, Imbalance Overhead: 44.244 ms
GPU 7, Compute+Comm Time: 114.021 ms, Bubble Time: 142.913 ms, Imbalance Overhead: 34.586 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 563.406 ms
GPU 0, Compute+Comm Time: 217.700 ms, Bubble Time: 277.539 ms, Imbalance Overhead: 68.166 ms
GPU 1, Compute+Comm Time: 219.261 ms, Bubble Time: 257.183 ms, Imbalance Overhead: 86.961 ms
GPU 2, Compute+Comm Time: 219.261 ms, Bubble Time: 239.124 ms, Imbalance Overhead: 105.020 ms
GPU 3, Compute+Comm Time: 219.261 ms, Bubble Time: 221.276 ms, Imbalance Overhead: 122.869 ms
GPU 4, Compute+Comm Time: 219.261 ms, Bubble Time: 203.078 ms, Imbalance Overhead: 141.066 ms
GPU 5, Compute+Comm Time: 219.261 ms, Bubble Time: 202.993 ms, Imbalance Overhead: 141.151 ms
GPU 6, Compute+Comm Time: 219.261 ms, Bubble Time: 222.155 ms, Imbalance Overhead: 121.989 ms
GPU 7, Compute+Comm Time: 222.713 ms, Bubble Time: 240.840 ms, Imbalance Overhead: 99.852 ms
The estimated cost of the whole pipeline: 897.673 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 619.444 ms
Partition 0 [0, 8) has cost: 619.444 ms
Partition 1 [8, 16) has cost: 614.248 ms
Partition 2 [16, 24) has cost: 614.248 ms
Partition 3 [24, 32) has cost: 612.066 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 301.660 ms
GPU 0, Compute+Comm Time: 154.443 ms, Bubble Time: 146.169 ms, Imbalance Overhead: 1.048 ms
GPU 1, Compute+Comm Time: 153.646 ms, Bubble Time: 126.452 ms, Imbalance Overhead: 21.562 ms
GPU 2, Compute+Comm Time: 153.646 ms, Bubble Time: 106.662 ms, Imbalance Overhead: 41.352 ms
GPU 3, Compute+Comm Time: 153.355 ms, Bubble Time: 106.521 ms, Imbalance Overhead: 41.785 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 570.061 ms
GPU 0, Compute+Comm Time: 288.001 ms, Bubble Time: 200.343 ms, Imbalance Overhead: 81.717 ms
GPU 1, Compute+Comm Time: 288.728 ms, Bubble Time: 200.798 ms, Imbalance Overhead: 80.535 ms
GPU 2, Compute+Comm Time: 288.728 ms, Bubble Time: 239.122 ms, Imbalance Overhead: 42.211 ms
GPU 3, Compute+Comm Time: 290.380 ms, Bubble Time: 276.970 ms, Imbalance Overhead: 2.712 ms
    The estimated cost with 2 DP ways is 915.307 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1233.693 ms
Partition 0 [0, 16) has cost: 1233.693 ms
Partition 1 [16, 32) has cost: 1226.315 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 343.778 ms
GPU 0, Compute+Comm Time: 229.089 ms, Bubble Time: 114.690 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 228.493 ms, Bubble Time: 114.101 ms, Imbalance Overhead: 1.184 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 604.561 ms
GPU 0, Compute+Comm Time: 401.604 ms, Bubble Time: 200.304 ms, Imbalance Overhead: 2.652 ms
GPU 1, Compute+Comm Time: 402.721 ms, Bubble Time: 201.840 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 995.756 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2460.007 ms
Partition 0 [0, 32) has cost: 2460.007 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 463.576 ms
GPU 0, Compute+Comm Time: 463.576 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 638.155 ms
GPU 0, Compute+Comm Time: 638.155 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1156.818 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 3.3300	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.1470	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 2.9997	TrainAcc 0.1170	ValidAcc 0.1246	TestAcc 0.1239	BestValid 0.1246
	Epoch 200:	Loss 3.6719	TrainAcc 0.1007	ValidAcc 0.0944	TestAcc 0.0993	BestValid 0.1246
	Epoch 250:	Loss 2.3310	TrainAcc 0.3098	ValidAcc 0.3382	TestAcc 0.3406	BestValid 0.3382
	Epoch 300:	Loss 2.5230	TrainAcc 0.2930	ValidAcc 0.3336	TestAcc 0.3358	BestValid 0.3382
	Epoch 350:	Loss 3.2562	TrainAcc 0.3131	ValidAcc 0.3271	TestAcc 0.3261	BestValid 0.3382
	Epoch 400:	Loss 1.9898	TrainAcc 0.4459	ValidAcc 0.4698	TestAcc 0.4719	BestValid 0.4698
	Epoch 450:	Loss 2.1324	TrainAcc 0.4117	ValidAcc 0.4478	TestAcc 0.4493	BestValid 0.4698
	Epoch 500:	Loss 1.8378	TrainAcc 0.5173	ValidAcc 0.5383	TestAcc 0.5396	BestValid 0.5383
	Epoch 550:	Loss 1.7598	TrainAcc 0.5684	ValidAcc 0.5867	TestAcc 0.5869	BestValid 0.5867
	Epoch 600:	Loss 1.8632	TrainAcc 0.4915	ValidAcc 0.5097	TestAcc 0.5088	BestValid 0.5867
	Epoch 650:	Loss 2.1923	TrainAcc 0.3830	ValidAcc 0.4049	TestAcc 0.4005	BestValid 0.5867
	Epoch 700:	Loss 1.3422	TrainAcc 0.5989	ValidAcc 0.6129	TestAcc 0.6145	BestValid 0.6129
	Epoch 750:	Loss 1.3312	TrainAcc 0.6503	ValidAcc 0.6626	TestAcc 0.6651	BestValid 0.6626
	Epoch 800:	Loss 1.1456	TrainAcc 0.5876	ValidAcc 0.6009	TestAcc 0.6014	BestValid 0.6626
	Epoch 850:	Loss 1.1385	TrainAcc 0.6248	ValidAcc 0.6399	TestAcc 0.6439	BestValid 0.6626
	Epoch 900:	Loss 1.5310	TrainAcc 0.5908	ValidAcc 0.6040	TestAcc 0.6010	BestValid 0.6626
	Epoch 950:	Loss 1.1453	TrainAcc 0.6695	ValidAcc 0.6805	TestAcc 0.6831	BestValid 0.6805
	Epoch 1000:	Loss 1.3357	TrainAcc 0.7039	ValidAcc 0.7168	TestAcc 0.7182	BestValid 0.7168
	Epoch 1050:	Loss 0.9975	TrainAcc 0.6024	ValidAcc 0.6159	TestAcc 0.6198	BestValid 0.7168
	Epoch 1100:	Loss 0.9641	TrainAcc 0.6948	ValidAcc 0.7053	TestAcc 0.7064	BestValid 0.7168
	Epoch 1150:	Loss 0.9833	TrainAcc 0.6902	ValidAcc 0.7009	TestAcc 0.7029	BestValid 0.7168
	Epoch 1200:	Loss 1.8100	TrainAcc 0.6534	ValidAcc 0.6585	TestAcc 0.6585	BestValid 0.7168
	Epoch 1250:	Loss 0.9773	TrainAcc 0.6953	ValidAcc 0.7074	TestAcc 0.7105	BestValid 0.7168
	Epoch 1300:	Loss 0.9474	TrainAcc 0.7002	ValidAcc 0.7116	TestAcc 0.7151	BestValid 0.7168
	Epoch 1350:	Loss 0.9092	TrainAcc 0.6469	ValidAcc 0.6635	TestAcc 0.6650	BestValid 0.7168
	Epoch 1400:	Loss 0.9042	TrainAcc 0.7252	ValidAcc 0.7351	TestAcc 0.7390	BestValid 0.7351
	Epoch 1450:	Loss 0.8990	TrainAcc 0.7326	ValidAcc 0.7435	TestAcc 0.7464	BestValid 0.7435
	Epoch 1500:	Loss 0.8201	TrainAcc 0.6710	ValidAcc 0.6874	TestAcc 0.6874	BestValid 0.7435
	Epoch 1550:	Loss 0.7788	TrainAcc 0.6844	ValidAcc 0.6974	TestAcc 0.7003	BestValid 0.7435
	Epoch 1600:	Loss 0.8946	TrainAcc 0.6481	ValidAcc 0.6642	TestAcc 0.6686	BestValid 0.7435
	Epoch 1650:	Loss 0.7891	TrainAcc 0.6725	ValidAcc 0.6875	TestAcc 0.6897	BestValid 0.7435
	Epoch 1700:	Loss 0.8498	TrainAcc 0.6419	ValidAcc 0.6571	TestAcc 0.6587	BestValid 0.7435
	Epoch 1750:	Loss 1.3913	TrainAcc 0.6837	ValidAcc 0.6946	TestAcc 0.6957	BestValid 0.7435
	Epoch 1800:	Loss 0.8381	TrainAcc 0.7096	ValidAcc 0.7239	TestAcc 0.7255	BestValid 0.7435
	Epoch 1850:	Loss 0.8450	TrainAcc 0.6952	ValidAcc 0.7097	TestAcc 0.7110	BestValid 0.7435
	Epoch 1900:	Loss 1.0016	TrainAcc 0.6878	ValidAcc 0.6990	TestAcc 0.7069	BestValid 0.7435
	Epoch 1950:	Loss 0.7427	TrainAcc 0.6946	ValidAcc 0.7083	TestAcc 0.7107	BestValid 0.7435
	Epoch 2000:	Loss 0.7806	TrainAcc 0.6921	ValidAcc 0.7074	TestAcc 0.7072	BestValid 0.7435
	Epoch 2050:	Loss 0.8635	TrainAcc 0.6764	ValidAcc 0.6907	TestAcc 0.6928	BestValid 0.7435
	Epoch 2100:	Loss 0.7670	TrainAcc 0.6946	ValidAcc 0.7092	TestAcc 0.7096	BestValid 0.7435
	Epoch 2150:	Loss 0.8136	TrainAcc 0.7354	ValidAcc 0.7462	TestAcc 0.7501	BestValid 0.7462
	Epoch 2200:	Loss 0.6790	TrainAcc 0.7272	ValidAcc 0.7395	TestAcc 0.7439	BestValid 0.7462
	Epoch 2250:	Loss 1.0369	TrainAcc 0.6733	ValidAcc 0.6758	TestAcc 0.6800	BestValid 0.7462
	Epoch 2300:	Loss 0.6979	TrainAcc 0.7296	ValidAcc 0.7444	TestAcc 0.7446	BestValid 0.7462
	Epoch 2350:	Loss 0.6654	TrainAcc 0.7133	ValidAcc 0.7279	TestAcc 0.7281	BestValid 0.7462
	Epoch 2400:	Loss 0.6938	TrainAcc 0.7259	ValidAcc 0.7349	TestAcc 0.7392	BestValid 0.7462
	Epoch 2450:	Loss 0.6542	TrainAcc 0.7518	ValidAcc 0.7605	TestAcc 0.7658	BestValid 0.7605
	Epoch 2500:	Loss 0.6567	TrainAcc 0.6574	ValidAcc 0.6755	TestAcc 0.6734	BestValid 0.7605
	Epoch 2550:	Loss 0.6604	TrainAcc 0.6905	ValidAcc 0.7050	TestAcc 0.7053	BestValid 0.7605
	Epoch 2600:	Loss 0.6502	TrainAcc 0.7061	ValidAcc 0.7197	TestAcc 0.7202	BestValid 0.7605
	Epoch 2650:	Loss 0.6294	TrainAcc 0.6677	ValidAcc 0.6832	TestAcc 0.6823	BestValid 0.7605
	Epoch 2700:	Loss 0.6120	TrainAcc 0.6828	ValidAcc 0.6983	TestAcc 0.6994	BestValid 0.7605
	Epoch 2750:	Loss 0.7532	TrainAcc 0.6971	ValidAcc 0.7098	TestAcc 0.7146	BestValid 0.7605
	Epoch 2800:	Loss 0.8632	TrainAcc 0.6871	ValidAcc 0.6997	TestAcc 0.6997	BestValid 0.7605
	Epoch 2850:	Loss 0.6264	TrainAcc 0.6965	ValidAcc 0.7110	TestAcc 0.7116	BestValid 0.7605
	Epoch 2900:	Loss 1.3193	TrainAcc 0.7754	ValidAcc 0.7825	TestAcc 0.7841	BestValid 0.7825
	Epoch 2950:	Loss 0.6489	TrainAcc 0.7151	ValidAcc 0.7290	TestAcc 0.7288	BestValid 0.7825
	Epoch 3000:	Loss 0.6540	TrainAcc 0.7152	ValidAcc 0.7286	TestAcc 0.7292	BestValid 0.7825
	Epoch 3050:	Loss 0.6012	TrainAcc 0.7247	ValidAcc 0.7377	TestAcc 0.7385	BestValid 0.7825
	Epoch 3100:	Loss 0.6091	TrainAcc 0.7108	ValidAcc 0.7258	TestAcc 0.7250	BestValid 0.7825
	Epoch 3150:	Loss 0.6022	TrainAcc 0.7031	ValidAcc 0.7186	TestAcc 0.7193	BestValid 0.7825
	Epoch 3200:	Loss 0.5987	TrainAcc 0.7713	ValidAcc 0.7802	TestAcc 0.7832	BestValid 0.7825
	Epoch 3250:	Loss 0.6039	TrainAcc 0.6829	ValidAcc 0.7001	TestAcc 0.6999	BestValid 0.7825
	Epoch 3300:	Loss 0.5717	TrainAcc 0.7296	ValidAcc 0.7398	TestAcc 0.7430	BestValid 0.7825
	Epoch 3350:	Loss 0.5714	TrainAcc 0.7414	ValidAcc 0.7525	TestAcc 0.7545	BestValid 0.7825
	Epoch 3400:	Loss 0.7319	TrainAcc 0.6208	ValidAcc 0.6448	TestAcc 0.6441	BestValid 0.7825
	Epoch 3450:	Loss 1.2748	TrainAcc 0.6103	ValidAcc 0.6327	TestAcc 0.6356	BestValid 0.7825
	Epoch 3500:	Loss 0.7701	TrainAcc 0.7490	ValidAcc 0.7595	TestAcc 0.7629	BestValid 0.7825
	Epoch 3550:	Loss 0.6828	TrainAcc 0.7332	ValidAcc 0.7443	TestAcc 0.7458	BestValid 0.7825
	Epoch 3600:	Loss 0.8142	TrainAcc 0.7338	ValidAcc 0.7435	TestAcc 0.7458	BestValid 0.7825
	Epoch 3650:	Loss 0.5928	TrainAcc 0.7248	ValidAcc 0.7367	TestAcc 0.7379	BestValid 0.7825
	Epoch 3700:	Loss 0.5639	TrainAcc 0.7025	ValidAcc 0.7189	TestAcc 0.7184	BestValid 0.7825
	Epoch 3750:	Loss 0.5690	TrainAcc 0.7122	ValidAcc 0.7279	TestAcc 0.7284	BestValid 0.7825
	Epoch 3800:	Loss 0.5665	TrainAcc 0.7149	ValidAcc 0.7305	TestAcc 0.7289	BestValid 0.7825
	Epoch 3850:	Loss 0.5576	TrainAcc 0.6825	ValidAcc 0.6960	TestAcc 0.6968	BestValid 0.7825
	Epoch 3900:	Loss 0.5931	TrainAcc 0.7405	ValidAcc 0.7515	TestAcc 0.7530	BestValid 0.7825
	Epoch 3950:	Loss 0.5358	TrainAcc 0.7155	ValidAcc 0.7312	TestAcc 0.7303	BestValid 0.7825
	Epoch 4000:	Loss 0.5415	TrainAcc 0.7283	ValidAcc 0.7420	TestAcc 0.7422	BestValid 0.7825
	Epoch 4050:	Loss 1.1279	TrainAcc 0.7463	ValidAcc 0.7552	TestAcc 0.7532	BestValid 0.7825
	Epoch 4100:	Loss 0.6818	TrainAcc 0.7906	ValidAcc 0.7955	TestAcc 0.7958	BestValid 0.7955
	Epoch 4150:	Loss 0.6119	TrainAcc 0.7956	ValidAcc 0.8013	TestAcc 0.8025	BestValid 0.8013
	Epoch 4200:	Loss 0.6873	TrainAcc 0.7944	ValidAcc 0.8002	TestAcc 0.8019	BestValid 0.8013
	Epoch 4250:	Loss 0.6500	TrainAcc 0.7646	ValidAcc 0.7730	TestAcc 0.7721	BestValid 0.8013
	Epoch 4300:	Loss 0.5880	TrainAcc 0.8035	ValidAcc 0.8101	TestAcc 0.8106	BestValid 0.8101
	Epoch 4350:	Loss 0.5779	TrainAcc 0.7969	ValidAcc 0.8019	TestAcc 0.8023	BestValid 0.8101
	Epoch 4400:	Loss 0.5701	TrainAcc 0.8022	ValidAcc 0.8086	TestAcc 0.8091	BestValid 0.8101
	Epoch 4450:	Loss 0.5462	TrainAcc 0.8211	ValidAcc 0.8263	TestAcc 0.8270	BestValid 0.8263
	Epoch 4500:	Loss 0.5330	TrainAcc 0.8121	ValidAcc 0.8191	TestAcc 0.8179	BestValid 0.8263
	Epoch 4550:	Loss 0.5372	TrainAcc 0.7861	ValidAcc 0.7925	TestAcc 0.7917	BestValid 0.8263
	Epoch 4600:	Loss 0.5510	TrainAcc 0.8217	ValidAcc 0.8267	TestAcc 0.8251	BestValid 0.8267
	Epoch 4650:	Loss 0.5169	TrainAcc 0.8063	ValidAcc 0.8119	TestAcc 0.8120	BestValid 0.8267
	Epoch 4700:	Loss 0.5186	TrainAcc 0.8222	ValidAcc 0.8286	TestAcc 0.8282	BestValid 0.8286
	Epoch 4750:	Loss 9.1609	TrainAcc 0.3418	ValidAcc 0.2904	TestAcc 0.2943	BestValid 0.8286
	Epoch 4800:	Loss 0.5783	TrainAcc 0.7990	ValidAcc 0.8052	TestAcc 0.8065	BestValid 0.8286
	Epoch 4850:	Loss 0.5551	TrainAcc 0.7894	ValidAcc 0.7977	TestAcc 0.7980	BestValid 0.8286
	Epoch 4900:	Loss 0.5378	TrainAcc 0.8079	ValidAcc 0.8153	TestAcc 0.8143	BestValid 0.8286
	Epoch 4950:	Loss 0.5079	TrainAcc 0.7741	ValidAcc 0.7818	TestAcc 0.7828	BestValid 0.8286
	Epoch 5000:	Loss 0.5597	TrainAcc 0.7979	ValidAcc 0.8040	TestAcc 0.8033	BestValid 0.8286
****** Epoch Time (Excluding Evaluation Cost): 0.875 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.998 ms (Max: 1.425, Min: 0.056, Sum: 7.983)
Cluster-Wide Average, Compute: 223.069 ms (Max: 361.335, Min: 116.716, Sum: 1784.556)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.011, Min: 0.008, Sum: 0.073)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.018, Min: 0.014, Sum: 0.135)
Cluster-Wide Average, Communication-Graph: 634.488 ms (Max: 740.471, Min: 497.087, Sum: 5075.907)
Cluster-Wide Average, Optimization: 3.019 ms (Max: 3.046, Min: 2.992, Sum: 24.153)
Cluster-Wide Average, Others: 13.834 ms (Max: 13.881, Min: 13.800, Sum: 110.674)
****** Breakdown Sum: 875.435 ms ******
Cluster-Wide Average, GPU Memory Consumption: 14.566 GB (Max: 15.104, Min: 14.471, Sum: 116.530)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.726 Gbps (Max: 49.725, Min: 11.551, Sum: 213.806)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 14.501 GB
****** Accuracy Results ******
Highest valid_acc: 0.8286
Target test_acc: 0.8282
Epoch to reach the target acc: 4699
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
