Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.891 seconds.
Building the CSC structure...
        It takes 1.992 seconds.
Building the CSC structure...
        It takes 2.308 seconds.
Building the CSC structure...
        It takes 2.374 seconds.
Building the CSC structure...
        It takes 2.380 seconds.
Building the CSC structure...
        It takes 2.401 seconds.
Building the CSC structure...
        It takes 2.425 seconds.
Building the CSC structure...
        It takes 2.481 seconds.
Building the CSC structure...
        It takes 1.814 seconds.
        It takes 1.884 seconds.
        It takes 2.192 seconds.
        It takes 2.329 seconds.
        It takes 2.291 seconds.
Building the Feature Vector...
        It takes 2.306 seconds.
        It takes 2.280 seconds.
        It takes 2.345 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.308 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.034 seconds.
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.289 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.041 seconds.
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.035 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.836 Gbps (per GPU), 470.686 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.564 Gbps (per GPU), 468.513 Gbps (aggregated)
The layer-level communication performance: 58.556 Gbps (per GPU), 468.450 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.316 Gbps (per GPU), 466.527 Gbps (aggregated)
The layer-level communication performance: 58.282 Gbps (per GPU), 466.253 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.082 Gbps (per GPU), 464.659 Gbps (aggregated)
The layer-level communication performance: 58.044 Gbps (per GPU), 464.353 Gbps (aggregated)
The layer-level communication performance: 58.007 Gbps (per GPU), 464.059 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.823 Gbps (per GPU), 1254.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.814 Gbps (per GPU), 1254.512 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.788 Gbps (per GPU), 1254.301 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.808 Gbps (per GPU), 1254.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.823 Gbps (per GPU), 1254.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.811 Gbps (per GPU), 1254.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.791 Gbps (per GPU), 1254.324 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.814 Gbps (per GPU), 1254.513 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.099 Gbps (per GPU), 808.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.100 Gbps (per GPU), 808.800 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.099 Gbps (per GPU), 808.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.100 Gbps (per GPU), 808.800 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.100 Gbps (per GPU), 808.800 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.085 Gbps (per GPU), 808.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.100 Gbps (per GPU), 808.800 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.086 Gbps (per GPU), 808.690 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.816 Gbps (per GPU), 310.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.814 Gbps (per GPU), 310.516 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.815 Gbps (per GPU), 310.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.816 Gbps (per GPU), 310.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.816 Gbps (per GPU), 310.526 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.816 Gbps (per GPU), 310.524 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.815 Gbps (per GPU), 310.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.814 Gbps (per GPU), 310.510 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.52ms  8.86ms  8.66ms  1.10 29.12K 14.23M
 chk_1  5.21ms  4.54ms  4.26ms  1.22 29.12K  6.56M
 chk_2 16.64ms 15.89ms 15.67ms  1.06 29.12K 24.68M
 chk_3 16.70ms 16.02ms 15.76ms  1.06 29.12K 22.95M
 chk_4  4.99ms  4.37ms  4.06ms  1.23 29.12K  6.33M
 chk_5  8.88ms  8.38ms  8.01ms  1.11 29.12K 12.05M
 chk_6 10.09ms  9.41ms  9.19ms  1.10 29.12K 14.60M
 chk_7  9.40ms  8.57ms  8.26ms  1.14 29.12K 13.21M
   Avg 10.18  9.51  9.23
   Max 16.70 16.02 15.76
   Min  4.99  4.37  4.06
 Ratio  3.35  3.66  3.88
   Var 17.33 17.09 17.29
Profiling takes 2.637 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 309.580 ms
Partition 0 [0, 4) has cost: 309.580 ms
Partition 1 [4, 8) has cost: 304.215 ms
Partition 2 [8, 12) has cost: 304.215 ms
Partition 3 [12, 16) has cost: 304.215 ms
Partition 4 [16, 20) has cost: 304.215 ms
Partition 5 [20, 24) has cost: 304.215 ms
Partition 6 [24, 28) has cost: 304.215 ms
Partition 7 [28, 32) has cost: 302.032 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 287.379 ms
GPU 0, Compute+Comm Time: 114.712 ms, Bubble Time: 123.245 ms, Imbalance Overhead: 49.422 ms
GPU 1, Compute+Comm Time: 112.874 ms, Bubble Time: 113.745 ms, Imbalance Overhead: 60.759 ms
GPU 2, Compute+Comm Time: 112.874 ms, Bubble Time: 104.121 ms, Imbalance Overhead: 70.383 ms
GPU 3, Compute+Comm Time: 112.874 ms, Bubble Time: 104.308 ms, Imbalance Overhead: 70.196 ms
GPU 4, Compute+Comm Time: 112.874 ms, Bubble Time: 113.417 ms, Imbalance Overhead: 61.088 ms
GPU 5, Compute+Comm Time: 112.874 ms, Bubble Time: 122.270 ms, Imbalance Overhead: 52.235 ms
GPU 6, Compute+Comm Time: 112.874 ms, Bubble Time: 131.158 ms, Imbalance Overhead: 43.347 ms
GPU 7, Compute+Comm Time: 112.273 ms, Bubble Time: 141.273 ms, Imbalance Overhead: 33.833 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 555.674 ms
GPU 0, Compute+Comm Time: 215.100 ms, Bubble Time: 273.760 ms, Imbalance Overhead: 66.813 ms
GPU 1, Compute+Comm Time: 216.682 ms, Bubble Time: 253.780 ms, Imbalance Overhead: 85.211 ms
GPU 2, Compute+Comm Time: 216.682 ms, Bubble Time: 236.002 ms, Imbalance Overhead: 102.989 ms
GPU 3, Compute+Comm Time: 216.682 ms, Bubble Time: 218.405 ms, Imbalance Overhead: 120.586 ms
GPU 4, Compute+Comm Time: 216.682 ms, Bubble Time: 200.386 ms, Imbalance Overhead: 138.605 ms
GPU 5, Compute+Comm Time: 216.682 ms, Bubble Time: 200.070 ms, Imbalance Overhead: 138.921 ms
GPU 6, Compute+Comm Time: 216.682 ms, Bubble Time: 219.066 ms, Imbalance Overhead: 119.925 ms
GPU 7, Compute+Comm Time: 220.209 ms, Bubble Time: 237.532 ms, Imbalance Overhead: 97.932 ms
The estimated cost of the whole pipeline: 885.205 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 613.795 ms
Partition 0 [0, 8) has cost: 613.795 ms
Partition 1 [8, 16) has cost: 608.430 ms
Partition 2 [16, 24) has cost: 608.430 ms
Partition 3 [24, 32) has cost: 606.247 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 298.446 ms
GPU 0, Compute+Comm Time: 152.722 ms, Bubble Time: 144.222 ms, Imbalance Overhead: 1.502 ms
GPU 1, Compute+Comm Time: 151.835 ms, Bubble Time: 125.099 ms, Imbalance Overhead: 21.512 ms
GPU 2, Compute+Comm Time: 151.835 ms, Bubble Time: 105.851 ms, Imbalance Overhead: 40.760 ms
GPU 3, Compute+Comm Time: 151.541 ms, Bubble Time: 105.869 ms, Imbalance Overhead: 41.037 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 562.555 ms
GPU 0, Compute+Comm Time: 285.023 ms, Bubble Time: 197.761 ms, Imbalance Overhead: 79.772 ms
GPU 1, Compute+Comm Time: 285.785 ms, Bubble Time: 197.778 ms, Imbalance Overhead: 78.992 ms
GPU 2, Compute+Comm Time: 285.785 ms, Bubble Time: 235.771 ms, Imbalance Overhead: 40.999 ms
GPU 3, Compute+Comm Time: 287.471 ms, Bubble Time: 273.233 ms, Imbalance Overhead: 1.852 ms
    The estimated cost with 2 DP ways is 904.052 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1222.225 ms
Partition 0 [0, 16) has cost: 1222.225 ms
Partition 1 [16, 32) has cost: 1214.677 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 340.483 ms
GPU 0, Compute+Comm Time: 226.993 ms, Bubble Time: 113.490 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 226.318 ms, Bubble Time: 113.169 ms, Imbalance Overhead: 0.996 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 598.308 ms
GPU 0, Compute+Comm Time: 397.615 ms, Bubble Time: 198.777 ms, Imbalance Overhead: 1.916 ms
GPU 1, Compute+Comm Time: 398.845 ms, Bubble Time: 199.462 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 985.730 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2436.903 ms
Partition 0 [0, 32) has cost: 2436.903 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 439.284 ms
GPU 0, Compute+Comm Time: 439.284 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 611.501 ms
GPU 0, Compute+Comm Time: 611.501 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1103.324 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7136	TrainAcc 0.0454	ValidAcc 0.0390	TestAcc 0.0428	BestValid 0.0390
	Epoch 50:	Loss 3.2805	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.2575	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.2499	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 3.0020	TrainAcc 0.1748	ValidAcc 0.2028	TestAcc 0.2035	BestValid 0.2028
	Epoch 250:	Loss 2.7868	TrainAcc 0.1827	ValidAcc 0.2145	TestAcc 0.2153	BestValid 0.2145
	Epoch 300:	Loss 2.6226	TrainAcc 0.2086	ValidAcc 0.2377	TestAcc 0.2400	BestValid 0.2377
	Epoch 350:	Loss 2.3747	TrainAcc 0.3477	ValidAcc 0.4000	TestAcc 0.4040	BestValid 0.4000
	Epoch 400:	Loss 2.1401	TrainAcc 0.3031	ValidAcc 0.3423	TestAcc 0.3443	BestValid 0.4000
	Epoch 450:	Loss 2.1302	TrainAcc 0.2694	ValidAcc 0.3234	TestAcc 0.3250	BestValid 0.4000
	Epoch 500:	Loss 1.9548	TrainAcc 0.4508	ValidAcc 0.4783	TestAcc 0.4804	BestValid 0.4783
	Epoch 550:	Loss 1.8004	TrainAcc 0.4931	ValidAcc 0.5205	TestAcc 0.5212	BestValid 0.5205
	Epoch 600:	Loss 1.6705	TrainAcc 0.4287	ValidAcc 0.4684	TestAcc 0.4701	BestValid 0.5205
	Epoch 650:	Loss 1.6161	TrainAcc 0.4444	ValidAcc 0.4840	TestAcc 0.4870	BestValid 0.5205
	Epoch 700:	Loss 2.0289	TrainAcc 0.4782	ValidAcc 0.5001	TestAcc 0.5022	BestValid 0.5205
	Epoch 750:	Loss 1.4690	TrainAcc 0.5497	ValidAcc 0.5689	TestAcc 0.5702	BestValid 0.5689
	Epoch 800:	Loss 1.3793	TrainAcc 0.5489	ValidAcc 0.5651	TestAcc 0.5657	BestValid 0.5689
	Epoch 850:	Loss 1.3650	TrainAcc 0.5891	ValidAcc 0.6094	TestAcc 0.6105	BestValid 0.6094
	Epoch 900:	Loss 1.5234	TrainAcc 0.5827	ValidAcc 0.6064	TestAcc 0.6039	BestValid 0.6094
	Epoch 950:	Loss 1.2919	TrainAcc 0.5525	ValidAcc 0.5809	TestAcc 0.5815	BestValid 0.6094
	Epoch 1000:	Loss 1.3718	TrainAcc 0.6039	ValidAcc 0.6194	TestAcc 0.6214	BestValid 0.6194
	Epoch 1050:	Loss 1.3486	TrainAcc 0.6194	ValidAcc 0.6417	TestAcc 0.6402	BestValid 0.6417
	Epoch 1100:	Loss 1.4184	TrainAcc 0.4977	ValidAcc 0.5199	TestAcc 0.5189	BestValid 0.6417
	Epoch 1150:	Loss 1.2333	TrainAcc 0.6391	ValidAcc 0.6573	TestAcc 0.6575	BestValid 0.6573
	Epoch 1200:	Loss 1.1710	TrainAcc 0.6295	ValidAcc 0.6508	TestAcc 0.6477	BestValid 0.6573
	Epoch 1250:	Loss 1.4701	TrainAcc 0.6674	ValidAcc 0.6855	TestAcc 0.6851	BestValid 0.6855
	Epoch 1300:	Loss 1.2325	TrainAcc 0.6181	ValidAcc 0.6363	TestAcc 0.6353	BestValid 0.6855
	Epoch 1350:	Loss 2.1157	TrainAcc 0.6464	ValidAcc 0.6643	TestAcc 0.6639	BestValid 0.6855
	Epoch 1400:	Loss 1.1402	TrainAcc 0.6517	ValidAcc 0.6682	TestAcc 0.6701	BestValid 0.6855
	Epoch 1450:	Loss 1.1193	TrainAcc 0.6624	ValidAcc 0.6786	TestAcc 0.6813	BestValid 0.6855
	Epoch 1500:	Loss 1.1355	TrainAcc 0.5503	ValidAcc 0.5766	TestAcc 0.5782	BestValid 0.6855
	Epoch 1550:	Loss 1.0716	TrainAcc 0.6579	ValidAcc 0.6761	TestAcc 0.6780	BestValid 0.6855
	Epoch 1600:	Loss 1.0730	TrainAcc 0.6952	ValidAcc 0.7091	TestAcc 0.7150	BestValid 0.7091
	Epoch 1650:	Loss 1.2646	TrainAcc 0.5746	ValidAcc 0.5942	TestAcc 0.5940	BestValid 0.7091
	Epoch 1700:	Loss 1.0642	TrainAcc 0.6929	ValidAcc 0.7119	TestAcc 0.7149	BestValid 0.7119
	Epoch 1750:	Loss 1.0388	TrainAcc 0.6913	ValidAcc 0.7066	TestAcc 0.7093	BestValid 0.7119
	Epoch 1800:	Loss 0.9772	TrainAcc 0.7091	ValidAcc 0.7267	TestAcc 0.7280	BestValid 0.7267
	Epoch 1850:	Loss 1.0040	TrainAcc 0.7119	ValidAcc 0.7275	TestAcc 0.7299	BestValid 0.7275
	Epoch 1900:	Loss 1.0625	TrainAcc 0.6620	ValidAcc 0.6750	TestAcc 0.6788	BestValid 0.7275
	Epoch 1950:	Loss 0.9905	TrainAcc 0.6351	ValidAcc 0.6584	TestAcc 0.6629	BestValid 0.7275
	Epoch 2000:	Loss 1.0969	TrainAcc 0.6603	ValidAcc 0.6733	TestAcc 0.6757	BestValid 0.7275
	Epoch 2050:	Loss 1.6611	TrainAcc 0.6557	ValidAcc 0.6734	TestAcc 0.6730	BestValid 0.7275
	Epoch 2100:	Loss 0.9302	TrainAcc 0.7008	ValidAcc 0.7194	TestAcc 0.7235	BestValid 0.7275
	Epoch 2150:	Loss 0.9222	TrainAcc 0.6651	ValidAcc 0.6780	TestAcc 0.6821	BestValid 0.7275
	Epoch 2200:	Loss 0.9649	TrainAcc 0.7360	ValidAcc 0.7574	TestAcc 0.7577	BestValid 0.7574
	Epoch 2250:	Loss 0.9555	TrainAcc 0.7383	ValidAcc 0.7572	TestAcc 0.7584	BestValid 0.7574
	Epoch 2300:	Loss 0.8766	TrainAcc 0.7249	ValidAcc 0.7416	TestAcc 0.7435	BestValid 0.7574
	Epoch 2350:	Loss 1.0601	TrainAcc 0.6692	ValidAcc 0.6836	TestAcc 0.6868	BestValid 0.7574
	Epoch 2400:	Loss 0.9370	TrainAcc 0.7314	ValidAcc 0.7507	TestAcc 0.7518	BestValid 0.7574
	Epoch 2450:	Loss 0.8851	TrainAcc 0.7294	ValidAcc 0.7476	TestAcc 0.7501	BestValid 0.7574
	Epoch 2500:	Loss 0.9128	TrainAcc 0.7361	ValidAcc 0.7541	TestAcc 0.7556	BestValid 0.7574
	Epoch 2550:	Loss 0.8915	TrainAcc 0.6895	ValidAcc 0.7076	TestAcc 0.7037	BestValid 0.7574
	Epoch 2600:	Loss 0.9064	TrainAcc 0.7291	ValidAcc 0.7509	TestAcc 0.7532	BestValid 0.7574
	Epoch 2650:	Loss 1.1833	TrainAcc 0.6622	ValidAcc 0.6886	TestAcc 0.6898	BestValid 0.7574
	Epoch 2700:	Loss 0.8882	TrainAcc 0.7487	ValidAcc 0.7653	TestAcc 0.7677	BestValid 0.7653
	Epoch 2750:	Loss 0.8272	TrainAcc 0.7610	ValidAcc 0.7763	TestAcc 0.7783	BestValid 0.7763
	Epoch 2800:	Loss 0.9028	TrainAcc 0.7679	ValidAcc 0.7851	TestAcc 0.7858	BestValid 0.7851
	Epoch 2850:	Loss 0.8030	TrainAcc 0.7426	ValidAcc 0.7586	TestAcc 0.7597	BestValid 0.7851
	Epoch 2900:	Loss 1.1735	TrainAcc 0.6649	ValidAcc 0.6798	TestAcc 0.6832	BestValid 0.7851
	Epoch 2950:	Loss 0.7933	TrainAcc 0.7285	ValidAcc 0.7436	TestAcc 0.7459	BestValid 0.7851
	Epoch 3000:	Loss 0.8112	TrainAcc 0.6499	ValidAcc 0.6726	TestAcc 0.6776	BestValid 0.7851
	Epoch 3050:	Loss 0.9103	TrainAcc 0.7031	ValidAcc 0.7199	TestAcc 0.7173	BestValid 0.7851
	Epoch 3100:	Loss 0.8263	TrainAcc 0.7419	ValidAcc 0.7576	TestAcc 0.7596	BestValid 0.7851
	Epoch 3150:	Loss 0.8021	TrainAcc 0.7400	ValidAcc 0.7585	TestAcc 0.7588	BestValid 0.7851
	Epoch 3200:	Loss 0.7844	TrainAcc 0.7296	ValidAcc 0.7439	TestAcc 0.7460	BestValid 0.7851
	Epoch 3250:	Loss 0.8567	TrainAcc 0.7718	ValidAcc 0.7878	TestAcc 0.7890	BestValid 0.7878
	Epoch 3300:	Loss 0.7870	TrainAcc 0.7174	ValidAcc 0.7374	TestAcc 0.7373	BestValid 0.7878
	Epoch 3350:	Loss 0.7785	TrainAcc 0.7244	ValidAcc 0.7445	TestAcc 0.7461	BestValid 0.7878
	Epoch 3400:	Loss 0.7861	TrainAcc 0.7100	ValidAcc 0.7212	TestAcc 0.7239	BestValid 0.7878
	Epoch 3450:	Loss 0.6997	TrainAcc 0.7745	ValidAcc 0.7926	TestAcc 0.7925	BestValid 0.7926
	Epoch 3500:	Loss 0.9102	TrainAcc 0.6812	ValidAcc 0.7000	TestAcc 0.7050	BestValid 0.7926
	Epoch 3550:	Loss 0.6943	TrainAcc 0.7799	ValidAcc 0.7949	TestAcc 0.7966	BestValid 0.7949
	Epoch 3600:	Loss 0.7170	TrainAcc 0.7716	ValidAcc 0.7878	TestAcc 0.7902	BestValid 0.7949
	Epoch 3650:	Loss 0.6762	TrainAcc 0.7635	ValidAcc 0.7774	TestAcc 0.7795	BestValid 0.7949
	Epoch 3700:	Loss 0.6845	TrainAcc 0.7683	ValidAcc 0.7860	TestAcc 0.7872	BestValid 0.7949
	Epoch 3750:	Loss 0.8072	TrainAcc 0.7784	ValidAcc 0.7937	TestAcc 0.7946	BestValid 0.7949
	Epoch 3800:	Loss 0.7629	TrainAcc 0.7667	ValidAcc 0.7857	TestAcc 0.7872	BestValid 0.7949
	Epoch 3850:	Loss 0.6968	TrainAcc 0.7547	ValidAcc 0.7696	TestAcc 0.7737	BestValid 0.7949
	Epoch 3900:	Loss 0.7218	TrainAcc 0.7213	ValidAcc 0.7308	TestAcc 0.7354	BestValid 0.7949
	Epoch 3950:	Loss 0.7075	TrainAcc 0.7796	ValidAcc 0.7960	TestAcc 0.7960	BestValid 0.7960
	Epoch 4000:	Loss 0.7360	TrainAcc 0.7584	ValidAcc 0.7734	TestAcc 0.7751	BestValid 0.7960
	Epoch 4050:	Loss 0.6416	TrainAcc 0.7454	ValidAcc 0.7589	TestAcc 0.7633	BestValid 0.7960
	Epoch 4100:	Loss 0.6690	TrainAcc 0.7665	ValidAcc 0.7807	TestAcc 0.7808	BestValid 0.7960
	Epoch 4150:	Loss 0.7446	TrainAcc 0.7263	ValidAcc 0.7378	TestAcc 0.7418	BestValid 0.7960
	Epoch 4200:	Loss 0.8623	TrainAcc 0.7838	ValidAcc 0.7973	TestAcc 0.7987	BestValid 0.7973
	Epoch 4250:	Loss 0.6863	TrainAcc 0.7750	ValidAcc 0.7905	TestAcc 0.7916	BestValid 0.7973
	Epoch 4300:	Loss 0.7002	TrainAcc 0.7827	ValidAcc 0.7957	TestAcc 0.7974	BestValid 0.7973
	Epoch 4350:	Loss 0.6837	TrainAcc 0.7929	ValidAcc 0.8075	TestAcc 0.8087	BestValid 0.8075
	Epoch 4400:	Loss 0.6119	TrainAcc 0.7756	ValidAcc 0.7901	TestAcc 0.7929	BestValid 0.8075
	Epoch 4450:	Loss 0.6113	TrainAcc 0.7932	ValidAcc 0.8081	TestAcc 0.8092	BestValid 0.8081
	Epoch 4500:	Loss 0.6168	TrainAcc 0.7633	ValidAcc 0.7764	TestAcc 0.7786	BestValid 0.8081
	Epoch 4550:	Loss 0.8965	TrainAcc 0.7512	ValidAcc 0.7681	TestAcc 0.7689	BestValid 0.8081
	Epoch 4600:	Loss 0.6798	TrainAcc 0.7759	ValidAcc 0.7929	TestAcc 0.7941	BestValid 0.8081
	Epoch 4650:	Loss 0.7969	TrainAcc 0.7846	ValidAcc 0.7994	TestAcc 0.8002	BestValid 0.8081
	Epoch 4700:	Loss 0.6302	TrainAcc 0.7737	ValidAcc 0.7904	TestAcc 0.7909	BestValid 0.8081
	Epoch 4750:	Loss 0.6661	TrainAcc 0.7767	ValidAcc 0.7920	TestAcc 0.7937	BestValid 0.8081
	Epoch 4800:	Loss 0.5966	TrainAcc 0.7898	ValidAcc 0.8053	TestAcc 0.8055	BestValid 0.8081
	Epoch 4850:	Loss 0.6327	TrainAcc 0.7936	ValidAcc 0.8091	TestAcc 0.8093	BestValid 0.8091
	Epoch 4900:	Loss 0.6245	TrainAcc 0.7849	ValidAcc 0.7996	TestAcc 0.8006	BestValid 0.8091
	Epoch 4950:	Loss 0.6892	TrainAcc 0.7083	ValidAcc 0.7266	TestAcc 0.7296	BestValid 0.8091
	Epoch 5000:	Loss 0.6070	TrainAcc 0.7985	ValidAcc 0.8127	TestAcc 0.8149	BestValid 0.8127
****** Epoch Time (Excluding Evaluation Cost): 0.874 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.882 ms (Max: 1.285, Min: 0.068, Sum: 7.060)
Cluster-Wide Average, Compute: 221.852 ms (Max: 359.295, Min: 117.043, Sum: 1774.813)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.011, Min: 0.008, Sum: 0.070)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.019, Min: 0.016, Sum: 0.134)
Cluster-Wide Average, Communication-Graph: 633.895 ms (Max: 738.476, Min: 497.245, Sum: 5071.159)
Cluster-Wide Average, Optimization: 3.036 ms (Max: 3.051, Min: 3.013, Sum: 24.287)
Cluster-Wide Average, Others: 13.828 ms (Max: 13.851, Min: 13.805, Sum: 110.624)
****** Breakdown Sum: 873.518 ms ******
Cluster-Wide Average, GPU Memory Consumption: 14.566 GB (Max: 15.104, Min: 14.471, Sum: 116.530)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.736 Gbps (Max: 49.670, Min: 11.585, Sum: 213.890)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 14.501 GB
****** Accuracy Results ******
Highest valid_acc: 0.8127
Target test_acc: 0.8149
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
