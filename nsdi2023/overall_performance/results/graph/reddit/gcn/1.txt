Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.880 seconds.
Building the CSC structure...
        It takes 2.018 seconds.
Building the CSC structure...
        It takes 2.042 seconds.
Building the CSC structure...
        It takes 2.395 seconds.
Building the CSC structure...
        It takes 2.396 seconds.
Building the CSC structure...
        It takes 2.401 seconds.
Building the CSC structure...
        It takes 2.436 seconds.
Building the CSC structure...
        It takes 2.621 seconds.
Building the CSC structure...
        It takes 1.807 seconds.
        It takes 1.851 seconds.
        It takes 1.879 seconds.
Building the Feature Vector...
        It takes 2.296 seconds.
        It takes 2.354 seconds.
        It takes 2.360 seconds.
        It takes 2.445 seconds.
        It takes 2.294 seconds.
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.278 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.324 seconds.
Building the Label Vector...
        It takes 0.250 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.036 seconds.
        It takes 0.039 seconds.
        It takes 0.308 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
        It takes 0.248 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.030 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.248 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
csr in-out ready !Start Cost Model Initialization...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 56.896 Gbps (per GPU), 455.168 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.646 Gbps (per GPU), 453.167 Gbps (aggregated)
The layer-level communication performance: 56.635 Gbps (per GPU), 453.082 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.416 Gbps (per GPU), 451.326 Gbps (aggregated)
The layer-level communication performance: 56.386 Gbps (per GPU), 451.087 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.191 Gbps (per GPU), 449.531 Gbps (aggregated)
The layer-level communication performance: 56.155 Gbps (per GPU), 449.237 Gbps (aggregated)
The layer-level communication performance: 56.123 Gbps (per GPU), 448.987 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.513 Gbps (per GPU), 1252.101 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.516 Gbps (per GPU), 1252.124 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.559 Gbps (per GPU), 1252.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.507 Gbps (per GPU), 1252.054 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.513 Gbps (per GPU), 1252.102 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.509 Gbps (per GPU), 1252.076 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.583 Gbps (per GPU), 1252.662 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.507 Gbps (per GPU), 1252.057 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.097 Gbps (per GPU), 800.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.097 Gbps (per GPU), 800.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.097 Gbps (per GPU), 800.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.096 Gbps (per GPU), 800.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.097 Gbps (per GPU), 800.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.097 Gbps (per GPU), 800.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.098 Gbps (per GPU), 800.783 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.098 Gbps (per GPU), 800.783 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.101 Gbps (per GPU), 288.810 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.101 Gbps (per GPU), 288.810 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.101 Gbps (per GPU), 288.804 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.100 Gbps (per GPU), 288.801 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.099 Gbps (per GPU), 288.792 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.102 Gbps (per GPU), 288.813 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.099 Gbps (per GPU), 288.795 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.099 Gbps (per GPU), 288.790 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.64ms  8.94ms  8.69ms  1.11 29.12K 14.23M
 chk_1  5.23ms  4.55ms  4.27ms  1.23 29.12K  6.56M
 chk_2 16.47ms 15.89ms 15.68ms  1.05 29.12K 24.68M
 chk_3 16.70ms 16.03ms 15.77ms  1.06 29.12K 22.95M
 chk_4  4.98ms  4.34ms  4.07ms  1.22 29.12K  6.33M
 chk_5  8.99ms  8.33ms  8.05ms  1.12 29.12K 12.05M
 chk_6 10.18ms  9.49ms  9.17ms  1.11 29.12K 14.60M
 chk_7  9.37ms  8.55ms  8.29ms  1.13 29.12K 13.21M
   Avg 10.20  9.51  9.25
   Max 16.70 16.03 15.77
   Min  4.98  4.34  4.07
 Ratio  3.35  3.70  3.87
   Var 16.99 17.15 17.26
Profiling takes 2.640 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 309.912 ms
Partition 0 [0, 4) has cost: 309.912 ms
Partition 1 [4, 8) has cost: 304.445 ms
Partition 2 [8, 12) has cost: 304.445 ms
Partition 3 [12, 16) has cost: 304.445 ms
Partition 4 [16, 20) has cost: 304.445 ms
Partition 5 [20, 24) has cost: 304.445 ms
Partition 6 [24, 28) has cost: 304.445 ms
Partition 7 [28, 32) has cost: 302.323 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 288.867 ms
GPU 0, Compute+Comm Time: 115.431 ms, Bubble Time: 123.993 ms, Imbalance Overhead: 49.442 ms
GPU 1, Compute+Comm Time: 113.552 ms, Bubble Time: 114.630 ms, Imbalance Overhead: 60.684 ms
GPU 2, Compute+Comm Time: 113.552 ms, Bubble Time: 105.087 ms, Imbalance Overhead: 70.227 ms
GPU 3, Compute+Comm Time: 113.552 ms, Bubble Time: 105.168 ms, Imbalance Overhead: 70.147 ms
GPU 4, Compute+Comm Time: 113.552 ms, Bubble Time: 114.179 ms, Imbalance Overhead: 61.135 ms
GPU 5, Compute+Comm Time: 113.552 ms, Bubble Time: 122.941 ms, Imbalance Overhead: 52.373 ms
GPU 6, Compute+Comm Time: 113.552 ms, Bubble Time: 131.776 ms, Imbalance Overhead: 43.538 ms
GPU 7, Compute+Comm Time: 112.907 ms, Bubble Time: 141.932 ms, Imbalance Overhead: 34.028 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 557.548 ms
GPU 0, Compute+Comm Time: 215.621 ms, Bubble Time: 274.525 ms, Imbalance Overhead: 67.402 ms
GPU 1, Compute+Comm Time: 217.098 ms, Bubble Time: 254.492 ms, Imbalance Overhead: 85.959 ms
GPU 2, Compute+Comm Time: 217.098 ms, Bubble Time: 236.840 ms, Imbalance Overhead: 103.611 ms
GPU 3, Compute+Comm Time: 217.098 ms, Bubble Time: 219.428 ms, Imbalance Overhead: 121.022 ms
GPU 4, Compute+Comm Time: 217.098 ms, Bubble Time: 201.597 ms, Imbalance Overhead: 138.854 ms
GPU 5, Compute+Comm Time: 217.098 ms, Bubble Time: 201.402 ms, Imbalance Overhead: 139.048 ms
GPU 6, Compute+Comm Time: 217.098 ms, Bubble Time: 220.243 ms, Imbalance Overhead: 120.207 ms
GPU 7, Compute+Comm Time: 220.685 ms, Bubble Time: 238.559 ms, Imbalance Overhead: 98.304 ms
The estimated cost of the whole pipeline: 888.736 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 614.356 ms
Partition 0 [0, 8) has cost: 614.356 ms
Partition 1 [8, 16) has cost: 608.890 ms
Partition 2 [16, 24) has cost: 608.890 ms
Partition 3 [24, 32) has cost: 606.768 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 299.819 ms
GPU 0, Compute+Comm Time: 153.537 ms, Bubble Time: 144.710 ms, Imbalance Overhead: 1.572 ms
GPU 1, Compute+Comm Time: 152.659 ms, Bubble Time: 125.803 ms, Imbalance Overhead: 21.357 ms
GPU 2, Compute+Comm Time: 152.659 ms, Bubble Time: 106.716 ms, Imbalance Overhead: 40.444 ms
GPU 3, Compute+Comm Time: 152.322 ms, Bubble Time: 106.594 ms, Imbalance Overhead: 40.903 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 564.210 ms
GPU 0, Compute+Comm Time: 285.412 ms, Bubble Time: 198.733 ms, Imbalance Overhead: 80.065 ms
GPU 1, Compute+Comm Time: 286.079 ms, Bubble Time: 198.910 ms, Imbalance Overhead: 79.222 ms
GPU 2, Compute+Comm Time: 286.079 ms, Bubble Time: 236.592 ms, Imbalance Overhead: 41.540 ms
GPU 3, Compute+Comm Time: 287.828 ms, Bubble Time: 273.748 ms, Imbalance Overhead: 2.634 ms
    The estimated cost with 2 DP ways is 907.230 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1223.246 ms
Partition 0 [0, 16) has cost: 1223.246 ms
Partition 1 [16, 32) has cost: 1215.657 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 341.881 ms
GPU 0, Compute+Comm Time: 227.856 ms, Bubble Time: 114.025 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 227.265 ms, Bubble Time: 113.496 ms, Imbalance Overhead: 1.121 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 599.735 ms
GPU 0, Compute+Comm Time: 398.574 ms, Bubble Time: 199.098 ms, Imbalance Overhead: 2.063 ms
GPU 1, Compute+Comm Time: 399.717 ms, Bubble Time: 200.018 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 988.698 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2438.904 ms
Partition 0 [0, 32) has cost: 2438.904 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 459.699 ms
GPU 0, Compute+Comm Time: 459.699 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 631.951 ms
GPU 0, Compute+Comm Time: 631.951 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1146.233 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 5 initializing the weights for op[0, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 3.2764	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.2467	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.2190	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 3.0615	TrainAcc 0.1762	ValidAcc 0.2042	TestAcc 0.2049	BestValid 0.2042
	Epoch 250:	Loss 2.6975	TrainAcc 0.2632	ValidAcc 0.3028	TestAcc 0.3074	BestValid 0.3028
	Epoch 300:	Loss 2.5197	TrainAcc 0.2656	ValidAcc 0.3211	TestAcc 0.3239	BestValid 0.3211
	Epoch 350:	Loss 2.1536	TrainAcc 0.3784	ValidAcc 0.4267	TestAcc 0.4305	BestValid 0.4267
	Epoch 400:	Loss 2.0859	TrainAcc 0.3495	ValidAcc 0.3838	TestAcc 0.3883	BestValid 0.4267
	Epoch 450:	Loss 2.6568	TrainAcc 0.3076	ValidAcc 0.3683	TestAcc 0.3711	BestValid 0.4267
	Epoch 500:	Loss 2.0214	TrainAcc 0.4111	ValidAcc 0.4555	TestAcc 0.4570	BestValid 0.4555
	Epoch 550:	Loss 1.9580	TrainAcc 0.4411	ValidAcc 0.4818	TestAcc 0.4789	BestValid 0.4818
	Epoch 600:	Loss 1.7764	TrainAcc 0.4133	ValidAcc 0.4557	TestAcc 0.4588	BestValid 0.4818
	Epoch 650:	Loss 2.0718	TrainAcc 0.4098	ValidAcc 0.4546	TestAcc 0.4590	BestValid 0.4818
	Epoch 700:	Loss 1.7060	TrainAcc 0.5097	ValidAcc 0.5461	TestAcc 0.5424	BestValid 0.5461
	Epoch 750:	Loss 1.5498	TrainAcc 0.5268	ValidAcc 0.5603	TestAcc 0.5612	BestValid 0.5603
	Epoch 800:	Loss 1.8858	TrainAcc 0.5538	ValidAcc 0.5936	TestAcc 0.5912	BestValid 0.5936
	Epoch 850:	Loss 1.5220	TrainAcc 0.5547	ValidAcc 0.5846	TestAcc 0.5853	BestValid 0.5936
	Epoch 900:	Loss 1.6676	TrainAcc 0.3300	ValidAcc 0.3662	TestAcc 0.3690	BestValid 0.5936
	Epoch 950:	Loss 1.7514	TrainAcc 0.4528	ValidAcc 0.4933	TestAcc 0.4970	BestValid 0.5936
	Epoch 1000:	Loss 1.6822	TrainAcc 0.5456	ValidAcc 0.5770	TestAcc 0.5801	BestValid 0.5936
	Epoch 1050:	Loss 1.4915	TrainAcc 0.5504	ValidAcc 0.5803	TestAcc 0.5836	BestValid 0.5936
	Epoch 1100:	Loss 1.5979	TrainAcc 0.4419	ValidAcc 0.4699	TestAcc 0.4638	BestValid 0.5936
	Epoch 1150:	Loss 1.4911	TrainAcc 0.5684	ValidAcc 0.5985	TestAcc 0.6016	BestValid 0.5985
	Epoch 1200:	Loss 1.4250	TrainAcc 0.5543	ValidAcc 0.5845	TestAcc 0.5850	BestValid 0.5985
	Epoch 1250:	Loss 1.7515	TrainAcc 0.5590	ValidAcc 0.5842	TestAcc 0.5846	BestValid 0.5985
	Epoch 1300:	Loss 1.2947	TrainAcc 0.5792	ValidAcc 0.6053	TestAcc 0.6052	BestValid 0.6053
	Epoch 1350:	Loss 1.3963	TrainAcc 0.6148	ValidAcc 0.6427	TestAcc 0.6431	BestValid 0.6427
	Epoch 1400:	Loss 1.2723	TrainAcc 0.5717	ValidAcc 0.6040	TestAcc 0.6002	BestValid 0.6427
	Epoch 1450:	Loss 1.3201	TrainAcc 0.5524	ValidAcc 0.5799	TestAcc 0.5777	BestValid 0.6427
	Epoch 1500:	Loss 1.4220	TrainAcc 0.5984	ValidAcc 0.6262	TestAcc 0.6296	BestValid 0.6427
	Epoch 1550:	Loss 1.2224	TrainAcc 0.5947	ValidAcc 0.6229	TestAcc 0.6274	BestValid 0.6427
	Epoch 1600:	Loss 1.2128	TrainAcc 0.5623	ValidAcc 0.5920	TestAcc 0.5944	BestValid 0.6427
	Epoch 1650:	Loss 1.1454	TrainAcc 0.6186	ValidAcc 0.6438	TestAcc 0.6463	BestValid 0.6438
	Epoch 1700:	Loss 1.5851	TrainAcc 0.5275	ValidAcc 0.5587	TestAcc 0.5602	BestValid 0.6438
	Epoch 1750:	Loss 1.0920	TrainAcc 0.6371	ValidAcc 0.6602	TestAcc 0.6601	BestValid 0.6602
	Epoch 1800:	Loss 1.2143	TrainAcc 0.6497	ValidAcc 0.6726	TestAcc 0.6698	BestValid 0.6726
	Epoch 1850:	Loss 1.1489	TrainAcc 0.6487	ValidAcc 0.6699	TestAcc 0.6689	BestValid 0.6726
	Epoch 1900:	Loss 1.1147	TrainAcc 0.6418	ValidAcc 0.6631	TestAcc 0.6622	BestValid 0.6726
	Epoch 1950:	Loss 1.1593	TrainAcc 0.4414	ValidAcc 0.4618	TestAcc 0.4655	BestValid 0.6726
	Epoch 2000:	Loss 1.1631	TrainAcc 0.6094	ValidAcc 0.6322	TestAcc 0.6350	BestValid 0.6726
	Epoch 2050:	Loss 1.1378	TrainAcc 0.6061	ValidAcc 0.6281	TestAcc 0.6289	BestValid 0.6726
	Epoch 2100:	Loss 1.0858	TrainAcc 0.6325	ValidAcc 0.6510	TestAcc 0.6552	BestValid 0.6726
	Epoch 2150:	Loss 0.9972	TrainAcc 0.6397	ValidAcc 0.6583	TestAcc 0.6573	BestValid 0.6726
	Epoch 2200:	Loss 0.9465	TrainAcc 0.6837	ValidAcc 0.6991	TestAcc 0.7000	BestValid 0.6991
	Epoch 2250:	Loss 1.3615	TrainAcc 0.7113	ValidAcc 0.7285	TestAcc 0.7308	BestValid 0.7285
	Epoch 2300:	Loss 1.3030	TrainAcc 0.6264	ValidAcc 0.6582	TestAcc 0.6557	BestValid 0.7285
	Epoch 2350:	Loss 1.1834	TrainAcc 0.5033	ValidAcc 0.5315	TestAcc 0.5248	BestValid 0.7285
	Epoch 2400:	Loss 0.9568	TrainAcc 0.6199	ValidAcc 0.6417	TestAcc 0.6383	BestValid 0.7285
	Epoch 2450:	Loss 1.2238	TrainAcc 0.5441	ValidAcc 0.5578	TestAcc 0.5542	BestValid 0.7285
	Epoch 2500:	Loss 0.9462	TrainAcc 0.6951	ValidAcc 0.7118	TestAcc 0.7159	BestValid 0.7285
	Epoch 2550:	Loss 1.2217	TrainAcc 0.5394	ValidAcc 0.5604	TestAcc 0.5580	BestValid 0.7285
	Epoch 2600:	Loss 0.9646	TrainAcc 0.6533	ValidAcc 0.6690	TestAcc 0.6726	BestValid 0.7285
	Epoch 2650:	Loss 1.0640	TrainAcc 0.5967	ValidAcc 0.6207	TestAcc 0.6222	BestValid 0.7285
	Epoch 2700:	Loss 0.9203	TrainAcc 0.6326	ValidAcc 0.6530	TestAcc 0.6527	BestValid 0.7285
	Epoch 2750:	Loss 0.9243	TrainAcc 0.7126	ValidAcc 0.7289	TestAcc 0.7294	BestValid 0.7289
	Epoch 2800:	Loss 0.8446	TrainAcc 0.6648	ValidAcc 0.6867	TestAcc 0.6873	BestValid 0.7289
	Epoch 2850:	Loss 0.8075	TrainAcc 0.6802	ValidAcc 0.6950	TestAcc 0.6941	BestValid 0.7289
	Epoch 2900:	Loss 0.9029	TrainAcc 0.6995	ValidAcc 0.7173	TestAcc 0.7195	BestValid 0.7289
	Epoch 2950:	Loss 0.8237	TrainAcc 0.6333	ValidAcc 0.6528	TestAcc 0.6512	BestValid 0.7289
	Epoch 3000:	Loss 1.0628	TrainAcc 0.4629	ValidAcc 0.4985	TestAcc 0.4973	BestValid 0.7289
	Epoch 3050:	Loss 0.8202	TrainAcc 0.6547	ValidAcc 0.6733	TestAcc 0.6703	BestValid 0.7289
	Epoch 3100:	Loss 0.7709	TrainAcc 0.7357	ValidAcc 0.7512	TestAcc 0.7500	BestValid 0.7512
	Epoch 3150:	Loss 0.9833	TrainAcc 0.6647	ValidAcc 0.6856	TestAcc 0.6861	BestValid 0.7512
	Epoch 3200:	Loss 0.7540	TrainAcc 0.7456	ValidAcc 0.7597	TestAcc 0.7589	BestValid 0.7597
	Epoch 3250:	Loss 0.8010	TrainAcc 0.7558	ValidAcc 0.7706	TestAcc 0.7704	BestValid 0.7706
	Epoch 3300:	Loss 0.7975	TrainAcc 0.7481	ValidAcc 0.7635	TestAcc 0.7641	BestValid 0.7706
	Epoch 3350:	Loss 0.7259	TrainAcc 0.7071	ValidAcc 0.7242	TestAcc 0.7256	BestValid 0.7706
	Epoch 3400:	Loss 0.7836	TrainAcc 0.7283	ValidAcc 0.7418	TestAcc 0.7427	BestValid 0.7706
	Epoch 3450:	Loss 0.9999	TrainAcc 0.5364	ValidAcc 0.5345	TestAcc 0.5333	BestValid 0.7706
	Epoch 3500:	Loss 0.7374	TrainAcc 0.7026	ValidAcc 0.7215	TestAcc 0.7180	BestValid 0.7706
	Epoch 3550:	Loss 0.6986	TrainAcc 0.7357	ValidAcc 0.7499	TestAcc 0.7497	BestValid 0.7706
	Epoch 3600:	Loss 0.6948	TrainAcc 0.7471	ValidAcc 0.7617	TestAcc 0.7611	BestValid 0.7706
	Epoch 3650:	Loss 0.7236	TrainAcc 0.6910	ValidAcc 0.7067	TestAcc 0.7050	BestValid 0.7706
	Epoch 3700:	Loss 0.8420	TrainAcc 0.6996	ValidAcc 0.7190	TestAcc 0.7153	BestValid 0.7706
	Epoch 3750:	Loss 0.7165	TrainAcc 0.7402	ValidAcc 0.7568	TestAcc 0.7578	BestValid 0.7706
	Epoch 3800:	Loss 0.7176	TrainAcc 0.7617	ValidAcc 0.7772	TestAcc 0.7779	BestValid 0.7772
	Epoch 3850:	Loss 0.6593	TrainAcc 0.7018	ValidAcc 0.7202	TestAcc 0.7199	BestValid 0.7772
	Epoch 3900:	Loss 0.7078	TrainAcc 0.6981	ValidAcc 0.7111	TestAcc 0.7104	BestValid 0.7772
	Epoch 3950:	Loss 0.6801	TrainAcc 0.7522	ValidAcc 0.7684	TestAcc 0.7663	BestValid 0.7772
	Epoch 4000:	Loss 0.6650	TrainAcc 0.7504	ValidAcc 0.7651	TestAcc 0.7638	BestValid 0.7772
	Epoch 4050:	Loss 0.6750	TrainAcc 0.7203	ValidAcc 0.7375	TestAcc 0.7374	BestValid 0.7772
	Epoch 4100:	Loss 0.7321	TrainAcc 0.7433	ValidAcc 0.7601	TestAcc 0.7601	BestValid 0.7772
	Epoch 4150:	Loss 0.6595	TrainAcc 0.7516	ValidAcc 0.7669	TestAcc 0.7665	BestValid 0.7772
	Epoch 4200:	Loss 2.3027	TrainAcc 0.5566	ValidAcc 0.5809	TestAcc 0.5757	BestValid 0.7772
	Epoch 4250:	Loss 0.6946	TrainAcc 0.5076	ValidAcc 0.5432	TestAcc 0.5422	BestValid 0.7772
	Epoch 4300:	Loss 0.7199	TrainAcc 0.7689	ValidAcc 0.7828	TestAcc 0.7814	BestValid 0.7828
	Epoch 4350:	Loss 0.6531	TrainAcc 0.7552	ValidAcc 0.7687	TestAcc 0.7681	BestValid 0.7828
	Epoch 4400:	Loss 0.7272	TrainAcc 0.7547	ValidAcc 0.7683	TestAcc 0.7673	BestValid 0.7828
	Epoch 4450:	Loss 0.6400	TrainAcc 0.7173	ValidAcc 0.7336	TestAcc 0.7312	BestValid 0.7828
	Epoch 4500:	Loss 0.6328	TrainAcc 0.7402	ValidAcc 0.7541	TestAcc 0.7528	BestValid 0.7828
	Epoch 4550:	Loss 0.6626	TrainAcc 0.7347	ValidAcc 0.7519	TestAcc 0.7493	BestValid 0.7828
	Epoch 4600:	Loss 0.6766	TrainAcc 0.6985	ValidAcc 0.7165	TestAcc 0.7138	BestValid 0.7828
	Epoch 4650:	Loss 0.6203	TrainAcc 0.7401	ValidAcc 0.7563	TestAcc 0.7595	BestValid 0.7828
	Epoch 4700:	Loss 0.6929	TrainAcc 0.6984	ValidAcc 0.7169	TestAcc 0.7165	BestValid 0.7828
	Epoch 4750:	Loss 0.6106	TrainAcc 0.7076	ValidAcc 0.7263	TestAcc 0.7259	BestValid 0.7828
	Epoch 4800:	Loss 0.5799	TrainAcc 0.7398	ValidAcc 0.7557	TestAcc 0.7539	BestValid 0.7828
	Epoch 4850:	Loss 0.5828	TrainAcc 0.7513	ValidAcc 0.7666	TestAcc 0.7643	BestValid 0.7828
	Epoch 4900:	Loss 0.6186	TrainAcc 0.7591	ValidAcc 0.7728	TestAcc 0.7716	BestValid 0.7828
	Epoch 4950:	Loss 0.5823	TrainAcc 0.7816	ValidAcc 0.7933	TestAcc 0.7912	BestValid 0.7933
	Epoch 5000:	Loss 0.5909	TrainAcc 0.7825	ValidAcc 0.7938	TestAcc 0.7927	BestValid 0.7938
****** Epoch Time (Excluding Evaluation Cost): 0.876 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.969 ms (Max: 1.376, Min: 0.044, Sum: 7.754)
Cluster-Wide Average, Compute: 222.773 ms (Max: 360.717, Min: 116.927, Sum: 1782.181)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.010, Min: 0.008, Sum: 0.070)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.018, Min: 0.015, Sum: 0.132)
Cluster-Wide Average, Communication-Graph: 635.194 ms (Max: 740.649, Min: 498.130, Sum: 5081.552)
Cluster-Wide Average, Optimization: 3.047 ms (Max: 3.061, Min: 3.022, Sum: 24.373)
Cluster-Wide Average, Others: 13.834 ms (Max: 13.869, Min: 13.815, Sum: 110.674)
****** Breakdown Sum: 875.842 ms ******
Cluster-Wide Average, GPU Memory Consumption: 14.566 GB (Max: 15.104, Min: 14.471, Sum: 116.530)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.680 Gbps (Max: 49.592, Min: 11.549, Sum: 213.441)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 14.501 GB
****** Accuracy Results ******
Highest valid_acc: 0.7938
Target test_acc: 0.7927
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
