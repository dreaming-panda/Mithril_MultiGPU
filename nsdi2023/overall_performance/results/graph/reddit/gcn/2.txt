Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.889 seconds.
Building the CSC structure...
        It takes 1.900 seconds.
Building the CSC structure...
        It takes 2.019 seconds.
Building the CSC structure...
        It takes 2.028 seconds.
Building the CSC structure...
        It takes 2.054 seconds.
Building the CSC structure...
        It takes 2.231 seconds.
Building the CSC structure...
        It takes 2.364 seconds.
Building the CSC structure...
        It takes 2.416 seconds.
Building the CSC structure...
        It takes 1.830 seconds.
        It takes 1.836 seconds.
        It takes 1.866 seconds.
        It takes 1.863 seconds.
        It takes 1.873 seconds.
        It takes 2.406 seconds.
        It takes 2.299 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.371 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.038 seconds.
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.255 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.045 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
        It takes 0.033 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.074 Gbps (per GPU), 464.594 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.781 Gbps (per GPU), 462.248 Gbps (aggregated)
The layer-level communication performance: 57.791 Gbps (per GPU), 462.331 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.513 Gbps (per GPU), 460.105 Gbps (aggregated)
The layer-level communication performance: 57.548 Gbps (per GPU), 460.384 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.342 Gbps (per GPU), 458.736 Gbps (aggregated)
The layer-level communication performance: 57.283 Gbps (per GPU), 458.262 Gbps (aggregated)
The layer-level communication performance: 57.245 Gbps (per GPU), 457.964 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.711 Gbps (per GPU), 1253.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.720 Gbps (per GPU), 1253.762 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.711 Gbps (per GPU), 1253.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.630 Gbps (per GPU), 1253.036 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.709 Gbps (per GPU), 1253.668 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.726 Gbps (per GPU), 1253.809 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.711 Gbps (per GPU), 1253.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.635 Gbps (per GPU), 1253.083 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.551 Gbps (per GPU), 804.405 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.550 Gbps (per GPU), 804.398 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.551 Gbps (per GPU), 804.405 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.540 Gbps (per GPU), 804.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.551 Gbps (per GPU), 804.405 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.549 Gbps (per GPU), 804.392 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.551 Gbps (per GPU), 804.405 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.547 Gbps (per GPU), 804.373 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.637 Gbps (per GPU), 309.095 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.637 Gbps (per GPU), 309.100 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.637 Gbps (per GPU), 309.100 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.636 Gbps (per GPU), 309.091 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.636 Gbps (per GPU), 309.091 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.636 Gbps (per GPU), 309.089 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.637 Gbps (per GPU), 309.092 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.637 Gbps (per GPU), 309.098 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.59ms  9.00ms  8.78ms  1.09 29.12K 14.23M
 chk_1  5.19ms  4.59ms  4.28ms  1.21 29.12K  6.56M
 chk_2 16.63ms 16.06ms 15.84ms  1.05 29.12K 24.68M
 chk_3 16.78ms 16.11ms 15.85ms  1.06 29.12K 22.95M
 chk_4  5.03ms  4.40ms  4.08ms  1.23 29.12K  6.33M
 chk_5  9.01ms  8.38ms  8.10ms  1.11 29.12K 12.05M
 chk_6 10.23ms  9.61ms  9.30ms  1.10 29.12K 14.60M
 chk_7  9.41ms  8.66ms  8.35ms  1.13 29.12K 13.21M
   Avg 10.24  9.60  9.32
   Max 16.78 16.11 15.85
   Min  5.03  4.40  4.08
 Ratio  3.33  3.66  3.88
   Var 17.36 17.38 17.60
Profiling takes 2.659 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 312.336 ms
Partition 0 [0, 4) has cost: 312.336 ms
Partition 1 [4, 8) has cost: 307.265 ms
Partition 2 [8, 12) has cost: 307.265 ms
Partition 3 [12, 16) has cost: 307.265 ms
Partition 4 [16, 20) has cost: 307.265 ms
Partition 5 [20, 24) has cost: 307.265 ms
Partition 6 [24, 28) has cost: 307.265 ms
Partition 7 [28, 32) has cost: 305.036 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 290.254 ms
GPU 0, Compute+Comm Time: 115.995 ms, Bubble Time: 124.716 ms, Imbalance Overhead: 49.543 ms
GPU 1, Compute+Comm Time: 114.229 ms, Bubble Time: 115.332 ms, Imbalance Overhead: 60.693 ms
GPU 2, Compute+Comm Time: 114.229 ms, Bubble Time: 105.785 ms, Imbalance Overhead: 70.240 ms
GPU 3, Compute+Comm Time: 114.229 ms, Bubble Time: 106.007 ms, Imbalance Overhead: 70.019 ms
GPU 4, Compute+Comm Time: 114.229 ms, Bubble Time: 114.946 ms, Imbalance Overhead: 61.079 ms
GPU 5, Compute+Comm Time: 114.229 ms, Bubble Time: 123.644 ms, Imbalance Overhead: 52.381 ms
GPU 6, Compute+Comm Time: 114.229 ms, Bubble Time: 132.411 ms, Imbalance Overhead: 43.614 ms
GPU 7, Compute+Comm Time: 113.603 ms, Bubble Time: 142.528 ms, Imbalance Overhead: 34.124 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 560.659 ms
GPU 0, Compute+Comm Time: 217.107 ms, Bubble Time: 275.853 ms, Imbalance Overhead: 67.699 ms
GPU 1, Compute+Comm Time: 218.709 ms, Bubble Time: 255.840 ms, Imbalance Overhead: 86.111 ms
GPU 2, Compute+Comm Time: 218.709 ms, Bubble Time: 238.264 ms, Imbalance Overhead: 103.686 ms
GPU 3, Compute+Comm Time: 218.709 ms, Bubble Time: 220.926 ms, Imbalance Overhead: 121.024 ms
GPU 4, Compute+Comm Time: 218.709 ms, Bubble Time: 203.164 ms, Imbalance Overhead: 138.786 ms
GPU 5, Compute+Comm Time: 218.709 ms, Bubble Time: 202.775 ms, Imbalance Overhead: 139.175 ms
GPU 6, Compute+Comm Time: 218.709 ms, Bubble Time: 221.677 ms, Imbalance Overhead: 120.273 ms
GPU 7, Compute+Comm Time: 222.014 ms, Bubble Time: 240.150 ms, Imbalance Overhead: 98.496 ms
The estimated cost of the whole pipeline: 893.459 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 619.601 ms
Partition 0 [0, 8) has cost: 619.601 ms
Partition 1 [8, 16) has cost: 614.529 ms
Partition 2 [16, 24) has cost: 614.529 ms
Partition 3 [24, 32) has cost: 612.300 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 301.007 ms
GPU 0, Compute+Comm Time: 154.264 ms, Bubble Time: 145.120 ms, Imbalance Overhead: 1.624 ms
GPU 1, Compute+Comm Time: 153.411 ms, Bubble Time: 126.189 ms, Imbalance Overhead: 21.407 ms
GPU 2, Compute+Comm Time: 153.411 ms, Bubble Time: 107.095 ms, Imbalance Overhead: 40.500 ms
GPU 3, Compute+Comm Time: 153.140 ms, Bubble Time: 107.246 ms, Imbalance Overhead: 40.622 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 566.961 ms
GPU 0, Compute+Comm Time: 287.057 ms, Bubble Time: 199.952 ms, Imbalance Overhead: 79.951 ms
GPU 1, Compute+Comm Time: 287.769 ms, Bubble Time: 199.716 ms, Imbalance Overhead: 79.476 ms
GPU 2, Compute+Comm Time: 287.769 ms, Bubble Time: 237.519 ms, Imbalance Overhead: 41.673 ms
GPU 3, Compute+Comm Time: 289.374 ms, Bubble Time: 274.893 ms, Imbalance Overhead: 2.695 ms
    The estimated cost with 2 DP ways is 911.367 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1234.130 ms
Partition 0 [0, 16) has cost: 1234.130 ms
Partition 1 [16, 32) has cost: 1226.829 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 342.948 ms
GPU 0, Compute+Comm Time: 228.741 ms, Bubble Time: 114.207 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 228.143 ms, Bubble Time: 114.215 ms, Imbalance Overhead: 0.590 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 602.601 ms
GPU 0, Compute+Comm Time: 400.796 ms, Bubble Time: 200.620 ms, Imbalance Overhead: 1.184 ms
GPU 1, Compute+Comm Time: 401.914 ms, Bubble Time: 200.687 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 992.826 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2460.959 ms
Partition 0 [0, 32) has cost: 2460.959 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 441.537 ms
GPU 0, Compute+Comm Time: 441.537 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 614.784 ms
GPU 0, Compute+Comm Time: 614.784 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1109.137 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7136	TrainAcc 0.0426	ValidAcc 0.0420	TestAcc 0.0412	BestValid 0.0420
	Epoch 50:	Loss 3.3367	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.1732	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.0240	TrainAcc 0.0868	ValidAcc 0.0748	TestAcc 0.0728	BestValid 0.0748
	Epoch 200:	Loss 2.8466	TrainAcc 0.1454	ValidAcc 0.1229	TestAcc 0.1241	BestValid 0.1229
	Epoch 250:	Loss 2.7149	TrainAcc 0.1437	ValidAcc 0.1812	TestAcc 0.1853	BestValid 0.1812
	Epoch 300:	Loss 2.0583	TrainAcc 0.4229	ValidAcc 0.4273	TestAcc 0.4318	BestValid 0.4273
	Epoch 350:	Loss 1.7878	TrainAcc 0.5243	ValidAcc 0.5397	TestAcc 0.5388	BestValid 0.5397
	Epoch 400:	Loss 2.3546	TrainAcc 0.3569	ValidAcc 0.3920	TestAcc 0.3915	BestValid 0.5397
	Epoch 450:	Loss 1.6065	TrainAcc 0.5674	ValidAcc 0.5919	TestAcc 0.5901	BestValid 0.5919
	Epoch 500:	Loss 1.5126	TrainAcc 0.5999	ValidAcc 0.6196	TestAcc 0.6192	BestValid 0.6196
	Epoch 550:	Loss 1.4165	TrainAcc 0.6161	ValidAcc 0.6347	TestAcc 0.6327	BestValid 0.6347
	Epoch 600:	Loss 1.3470	TrainAcc 0.6376	ValidAcc 0.6547	TestAcc 0.6542	BestValid 0.6547
	Epoch 650:	Loss 1.6590	TrainAcc 0.5769	ValidAcc 0.5966	TestAcc 0.5977	BestValid 0.6547
	Epoch 700:	Loss 1.2348	TrainAcc 0.6359	ValidAcc 0.6505	TestAcc 0.6517	BestValid 0.6547
	Epoch 750:	Loss 1.3536	TrainAcc 0.6574	ValidAcc 0.6751	TestAcc 0.6751	BestValid 0.6751
	Epoch 800:	Loss 1.1502	TrainAcc 0.6773	ValidAcc 0.6942	TestAcc 0.6951	BestValid 0.6942
	Epoch 850:	Loss 1.0847	TrainAcc 0.6705	ValidAcc 0.6864	TestAcc 0.6874	BestValid 0.6942
	Epoch 900:	Loss 1.0692	TrainAcc 0.6777	ValidAcc 0.6940	TestAcc 0.6961	BestValid 0.6942
	Epoch 950:	Loss 0.9942	TrainAcc 0.6945	ValidAcc 0.7108	TestAcc 0.7113	BestValid 0.7108
	Epoch 1000:	Loss 1.0001	TrainAcc 0.7102	ValidAcc 0.7257	TestAcc 0.7261	BestValid 0.7257
	Epoch 1050:	Loss 0.9592	TrainAcc 0.6785	ValidAcc 0.6940	TestAcc 0.6942	BestValid 0.7257
	Epoch 1100:	Loss 0.8983	TrainAcc 0.6918	ValidAcc 0.7066	TestAcc 0.7072	BestValid 0.7257
	Epoch 1150:	Loss 0.8878	TrainAcc 0.6773	ValidAcc 0.6900	TestAcc 0.6930	BestValid 0.7257
	Epoch 1200:	Loss 1.9961	TrainAcc 0.5542	ValidAcc 0.5729	TestAcc 0.5785	BestValid 0.7257
	Epoch 1250:	Loss 1.0658	TrainAcc 0.7016	ValidAcc 0.7151	TestAcc 0.7176	BestValid 0.7257
	Epoch 1300:	Loss 1.0264	TrainAcc 0.6935	ValidAcc 0.7080	TestAcc 0.7082	BestValid 0.7257
	Epoch 1350:	Loss 0.9469	TrainAcc 0.6119	ValidAcc 0.6311	TestAcc 0.6315	BestValid 0.7257
	Epoch 1400:	Loss 1.1143	TrainAcc 0.6154	ValidAcc 0.6289	TestAcc 0.6329	BestValid 0.7257
	Epoch 1450:	Loss 0.9681	TrainAcc 0.6665	ValidAcc 0.6826	TestAcc 0.6813	BestValid 0.7257
	Epoch 1500:	Loss 0.8735	TrainAcc 0.6866	ValidAcc 0.7009	TestAcc 0.7031	BestValid 0.7257
	Epoch 1550:	Loss 0.8157	TrainAcc 0.6948	ValidAcc 0.7067	TestAcc 0.7094	BestValid 0.7257
	Epoch 1600:	Loss 0.8369	TrainAcc 0.6706	ValidAcc 0.6842	TestAcc 0.6878	BestValid 0.7257
	Epoch 1650:	Loss 0.8186	TrainAcc 0.6857	ValidAcc 0.7003	TestAcc 0.6997	BestValid 0.7257
	Epoch 1700:	Loss 0.7642	TrainAcc 0.6845	ValidAcc 0.6977	TestAcc 0.6992	BestValid 0.7257
	Epoch 1750:	Loss 1.2648	TrainAcc 0.6018	ValidAcc 0.6189	TestAcc 0.6197	BestValid 0.7257
	Epoch 1800:	Loss 0.7989	TrainAcc 0.6824	ValidAcc 0.6955	TestAcc 0.6978	BestValid 0.7257
	Epoch 1850:	Loss 0.7502	TrainAcc 0.7062	ValidAcc 0.7188	TestAcc 0.7210	BestValid 0.7257
	Epoch 1900:	Loss 0.7640	TrainAcc 0.7307	ValidAcc 0.7414	TestAcc 0.7436	BestValid 0.7414
	Epoch 1950:	Loss 0.7278	TrainAcc 0.6855	ValidAcc 0.6977	TestAcc 0.7003	BestValid 0.7414
	Epoch 2000:	Loss 0.7301	TrainAcc 0.7039	ValidAcc 0.7162	TestAcc 0.7176	BestValid 0.7414
	Epoch 2050:	Loss 0.7545	TrainAcc 0.6839	ValidAcc 0.6969	TestAcc 0.6968	BestValid 0.7414
	Epoch 2100:	Loss 0.7078	TrainAcc 0.7004	ValidAcc 0.7117	TestAcc 0.7144	BestValid 0.7414
	Epoch 2150:	Loss 0.8551	TrainAcc 0.7219	ValidAcc 0.7335	TestAcc 0.7366	BestValid 0.7414
	Epoch 2200:	Loss 0.7317	TrainAcc 0.7044	ValidAcc 0.7157	TestAcc 0.7182	BestValid 0.7414
	Epoch 2250:	Loss 0.6904	TrainAcc 0.7194	ValidAcc 0.7309	TestAcc 0.7336	BestValid 0.7414
	Epoch 2300:	Loss 0.6963	TrainAcc 0.7403	ValidAcc 0.7533	TestAcc 0.7553	BestValid 0.7533
	Epoch 2350:	Loss 0.6760	TrainAcc 0.7194	ValidAcc 0.7301	TestAcc 0.7331	BestValid 0.7533
	Epoch 2400:	Loss 0.6865	TrainAcc 0.7058	ValidAcc 0.7163	TestAcc 0.7211	BestValid 0.7533
	Epoch 2450:	Loss 0.6738	TrainAcc 0.6938	ValidAcc 0.7049	TestAcc 0.7091	BestValid 0.7533
	Epoch 2500:	Loss 0.6835	TrainAcc 0.7223	ValidAcc 0.7330	TestAcc 0.7372	BestValid 0.7533
	Epoch 2550:	Loss 0.9815	TrainAcc 0.6769	ValidAcc 0.6899	TestAcc 0.6917	BestValid 0.7533
	Epoch 2600:	Loss 1.8928	TrainAcc 0.4245	ValidAcc 0.4607	TestAcc 0.4647	BestValid 0.7533
	Epoch 2650:	Loss 0.7800	TrainAcc 0.6990	ValidAcc 0.7114	TestAcc 0.7153	BestValid 0.7533
	Epoch 2700:	Loss 0.7927	TrainAcc 0.6894	ValidAcc 0.6993	TestAcc 0.7011	BestValid 0.7533
	Epoch 2750:	Loss 0.6643	TrainAcc 0.7382	ValidAcc 0.7483	TestAcc 0.7527	BestValid 0.7533
	Epoch 2800:	Loss 0.6545	TrainAcc 0.7457	ValidAcc 0.7567	TestAcc 0.7602	BestValid 0.7567
	Epoch 2850:	Loss 0.6324	TrainAcc 0.8072	ValidAcc 0.8156	TestAcc 0.8195	BestValid 0.8156
	Epoch 2900:	Loss 0.6323	TrainAcc 0.7465	ValidAcc 0.7566	TestAcc 0.7593	BestValid 0.8156
	Epoch 2950:	Loss 2.5282	TrainAcc 0.5787	ValidAcc 0.5949	TestAcc 0.5957	BestValid 0.8156
	Epoch 3000:	Loss 0.6390	TrainAcc 0.7716	ValidAcc 0.7799	TestAcc 0.7838	BestValid 0.8156
	Epoch 3050:	Loss 0.5880	TrainAcc 0.7268	ValidAcc 0.7365	TestAcc 0.7405	BestValid 0.8156
	Epoch 3100:	Loss 0.5922	TrainAcc 0.7568	ValidAcc 0.7655	TestAcc 0.7698	BestValid 0.8156
	Epoch 3150:	Loss 0.5727	TrainAcc 0.7762	ValidAcc 0.7844	TestAcc 0.7877	BestValid 0.8156
	Epoch 3200:	Loss 0.5670	TrainAcc 0.7604	ValidAcc 0.7689	TestAcc 0.7718	BestValid 0.8156
	Epoch 3250:	Loss 0.5601	TrainAcc 0.7483	ValidAcc 0.7574	TestAcc 0.7606	BestValid 0.8156
	Epoch 3300:	Loss 0.5472	TrainAcc 0.7941	ValidAcc 0.8024	TestAcc 0.8048	BestValid 0.8156
	Epoch 3350:	Loss 0.5629	TrainAcc 0.7955	ValidAcc 0.8044	TestAcc 0.8073	BestValid 0.8156
	Epoch 3400:	Loss 0.5546	TrainAcc 0.7582	ValidAcc 0.7664	TestAcc 0.7701	BestValid 0.8156
	Epoch 3450:	Loss 0.5392	TrainAcc 0.7857	ValidAcc 0.7948	TestAcc 0.7977	BestValid 0.8156
	Epoch 3500:	Loss 0.5770	TrainAcc 0.8112	ValidAcc 0.8198	TestAcc 0.8222	BestValid 0.8198
	Epoch 3550:	Loss 0.5404	TrainAcc 0.8043	ValidAcc 0.8112	TestAcc 0.8135	BestValid 0.8198
	Epoch 3600:	Loss 0.5410	TrainAcc 0.8528	ValidAcc 0.8590	TestAcc 0.8593	BestValid 0.8590
	Epoch 3650:	Loss 0.5185	TrainAcc 0.8523	ValidAcc 0.8576	TestAcc 0.8585	BestValid 0.8590
	Epoch 3700:	Loss 0.5210	TrainAcc 0.8114	ValidAcc 0.8197	TestAcc 0.8202	BestValid 0.8590
	Epoch 3750:	Loss 0.7189	TrainAcc 0.7934	ValidAcc 0.8061	TestAcc 0.8060	BestValid 0.8590
	Epoch 3800:	Loss 0.5716	TrainAcc 0.7995	ValidAcc 0.8069	TestAcc 0.8091	BestValid 0.8590
	Epoch 3850:	Loss 0.5418	TrainAcc 0.8084	ValidAcc 0.8183	TestAcc 0.8182	BestValid 0.8590
	Epoch 3900:	Loss 2.4338	TrainAcc 0.6216	ValidAcc 0.6186	TestAcc 0.6195	BestValid 0.8590
	Epoch 3950:	Loss 0.6212	TrainAcc 0.7842	ValidAcc 0.7935	TestAcc 0.7966	BestValid 0.8590
	Epoch 4000:	Loss 0.5349	TrainAcc 0.7902	ValidAcc 0.7995	TestAcc 0.8012	BestValid 0.8590
	Epoch 4050:	Loss 0.5141	TrainAcc 0.8436	ValidAcc 0.8494	TestAcc 0.8509	BestValid 0.8590
	Epoch 4100:	Loss 0.5079	TrainAcc 0.7950	ValidAcc 0.8029	TestAcc 0.8049	BestValid 0.8590
	Epoch 4150:	Loss 0.5228	TrainAcc 0.8187	ValidAcc 0.8265	TestAcc 0.8286	BestValid 0.8590
	Epoch 4200:	Loss 0.5026	TrainAcc 0.7905	ValidAcc 0.7979	TestAcc 0.8001	BestValid 0.8590
	Epoch 4250:	Loss 0.4994	TrainAcc 0.8094	ValidAcc 0.8163	TestAcc 0.8201	BestValid 0.8590
	Epoch 4300:	Loss 0.5028	TrainAcc 0.8199	ValidAcc 0.8284	TestAcc 0.8303	BestValid 0.8590
	Epoch 4350:	Loss 0.4922	TrainAcc 0.8217	ValidAcc 0.8309	TestAcc 0.8307	BestValid 0.8590
	Epoch 4400:	Loss 0.4892	TrainAcc 0.8572	ValidAcc 0.8635	TestAcc 0.8634	BestValid 0.8635
	Epoch 4450:	Loss 0.4847	TrainAcc 0.8203	ValidAcc 0.8288	TestAcc 0.8305	BestValid 0.8635
	Epoch 4500:	Loss 1.3851	TrainAcc 0.6573	ValidAcc 0.6627	TestAcc 0.6606	BestValid 0.8635
	Epoch 4550:	Loss 0.5734	TrainAcc 0.8053	ValidAcc 0.8146	TestAcc 0.8180	BestValid 0.8635
	Epoch 4600:	Loss 0.5409	TrainAcc 0.7851	ValidAcc 0.7940	TestAcc 0.7962	BestValid 0.8635
	Epoch 4650:	Loss 0.5558	TrainAcc 0.8445	ValidAcc 0.8533	TestAcc 0.8526	BestValid 0.8635
	Epoch 4700:	Loss 0.5242	TrainAcc 0.8311	ValidAcc 0.8395	TestAcc 0.8402	BestValid 0.8635
	Epoch 4750:	Loss 0.5240	TrainAcc 0.8231	ValidAcc 0.8319	TestAcc 0.8330	BestValid 0.8635
	Epoch 4800:	Loss 0.5177	TrainAcc 0.7884	ValidAcc 0.7967	TestAcc 0.7989	BestValid 0.8635
	Epoch 4850:	Loss 0.4952	TrainAcc 0.8530	ValidAcc 0.8610	TestAcc 0.8611	BestValid 0.8635
	Epoch 4900:	Loss 0.4663	TrainAcc 0.8147	ValidAcc 0.8201	TestAcc 0.8228	BestValid 0.8635
	Epoch 4950:	Loss 0.4795	TrainAcc 0.8197	ValidAcc 0.8280	TestAcc 0.8292	BestValid 0.8635
	Epoch 5000:	Loss 0.4665	TrainAcc 0.7997	ValidAcc 0.8081	TestAcc 0.8107	BestValid 0.8635
****** Epoch Time (Excluding Evaluation Cost): 0.873 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.887 ms (Max: 1.320, Min: 0.065, Sum: 7.099)
Cluster-Wide Average, Compute: 222.115 ms (Max: 359.610, Min: 117.428, Sum: 1776.921)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.010, Min: 0.007, Sum: 0.067)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.017, Min: 0.015, Sum: 0.128)
Cluster-Wide Average, Communication-Graph: 633.381 ms (Max: 737.869, Min: 496.681, Sum: 5067.049)
Cluster-Wide Average, Optimization: 3.046 ms (Max: 3.056, Min: 3.032, Sum: 24.367)
Cluster-Wide Average, Others: 13.828 ms (Max: 13.854, Min: 13.809, Sum: 110.622)
****** Breakdown Sum: 873.282 ms ******
Cluster-Wide Average, GPU Memory Consumption: 14.566 GB (Max: 15.104, Min: 14.471, Sum: 116.530)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.761 Gbps (Max: 49.733, Min: 11.596, Sum: 214.089)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 14.501 GB
****** Accuracy Results ******
Highest valid_acc: 0.8635
Target test_acc: 0.8634
Epoch to reach the target acc: 4399
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
