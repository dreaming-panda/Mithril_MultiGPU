Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.869 seconds.
Building the CSC structure...
        It takes 1.889 seconds.
Building the CSC structure...
        It takes 2.018 seconds.
Building the CSC structure...
        It takes 2.380 seconds.
Building the CSC structure...
        It takes 2.404 seconds.
Building the CSC structure...
        It takes 2.416 seconds.
Building the CSC structure...
        It takes 2.620 seconds.
Building the CSC structure...
        It takes 2.618 seconds.
Building the CSC structure...
        It takes 1.833 seconds.
        It takes 1.839 seconds.
        It takes 1.828 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.333 seconds.
        It takes 2.321 seconds.
        It takes 2.430 seconds.
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 2.324 seconds.
Building the Feature Vector...
        It takes 0.038 seconds.
        It takes 2.361 seconds.
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.048 seconds.
        It takes 0.242 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.285 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.035 seconds.
        It takes 0.040 seconds.
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.273 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.571 Gbps (per GPU), 468.566 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.291 Gbps (per GPU), 466.331 Gbps (aggregated)
The layer-level communication performance: 58.291 Gbps (per GPU), 466.329 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.045 Gbps (per GPU), 464.360 Gbps (aggregated)
The layer-level communication performance: 58.009 Gbps (per GPU), 464.070 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.826 Gbps (per GPU), 462.605 Gbps (aggregated)
The layer-level communication performance: 57.748 Gbps (per GPU), 461.986 Gbps (aggregated)
The layer-level communication performance: 57.778 Gbps (per GPU), 462.224 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.656 Gbps (per GPU), 1253.247 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.650 Gbps (per GPU), 1253.200 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.659 Gbps (per GPU), 1253.270 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.685 Gbps (per GPU), 1253.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.656 Gbps (per GPU), 1253.247 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.665 Gbps (per GPU), 1253.317 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.654 Gbps (per GPU), 1253.229 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.688 Gbps (per GPU), 1253.507 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.445 Gbps (per GPU), 795.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.446 Gbps (per GPU), 795.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.445 Gbps (per GPU), 795.556 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.446 Gbps (per GPU), 795.569 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.445 Gbps (per GPU), 795.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.445 Gbps (per GPU), 795.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.446 Gbps (per GPU), 795.569 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.419 Gbps (per GPU), 795.355 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.312 Gbps (per GPU), 290.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.310 Gbps (per GPU), 290.482 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.311 Gbps (per GPU), 290.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.311 Gbps (per GPU), 290.485 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.309 Gbps (per GPU), 290.469 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.311 Gbps (per GPU), 290.491 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.309 Gbps (per GPU), 290.471 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.311 Gbps (per GPU), 290.491 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.61ms  8.94ms  8.76ms  1.10 29.12K 14.23M
 chk_1  5.20ms  4.57ms  4.28ms  1.21 29.12K  6.56M
 chk_2 16.50ms 16.02ms 15.80ms  1.04 29.12K 24.68M
 chk_3 16.70ms 16.12ms 15.85ms  1.05 29.12K 22.95M
 chk_4  4.99ms  4.41ms  4.08ms  1.22 29.12K  6.33M
 chk_5  8.95ms  8.33ms  8.05ms  1.11 29.12K 12.05M
 chk_6 10.18ms  9.55ms  9.24ms  1.10 29.12K 14.60M
 chk_7  9.49ms  8.77ms  8.31ms  1.14 29.12K 13.21M
   Avg 10.20  9.59  9.30
   Max 16.70 16.12 15.85
   Min  4.99  4.41  4.08
 Ratio  3.35  3.65  3.88
   Var 17.08 17.34 17.55
Profiling takes 2.652 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 311.793 ms
Partition 0 [0, 4) has cost: 311.793 ms
Partition 1 [4, 8) has cost: 306.914 ms
Partition 2 [8, 12) has cost: 306.914 ms
Partition 3 [12, 16) has cost: 306.914 ms
Partition 4 [16, 20) has cost: 306.914 ms
Partition 5 [20, 24) has cost: 306.914 ms
Partition 6 [24, 28) has cost: 306.914 ms
Partition 7 [28, 32) has cost: 304.563 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 289.976 ms
GPU 0, Compute+Comm Time: 116.036 ms, Bubble Time: 124.842 ms, Imbalance Overhead: 49.099 ms
GPU 1, Compute+Comm Time: 114.460 ms, Bubble Time: 115.394 ms, Imbalance Overhead: 60.122 ms
GPU 2, Compute+Comm Time: 114.460 ms, Bubble Time: 105.742 ms, Imbalance Overhead: 69.775 ms
GPU 3, Compute+Comm Time: 114.460 ms, Bubble Time: 105.888 ms, Imbalance Overhead: 69.628 ms
GPU 4, Compute+Comm Time: 114.460 ms, Bubble Time: 114.866 ms, Imbalance Overhead: 60.650 ms
GPU 5, Compute+Comm Time: 114.460 ms, Bubble Time: 123.620 ms, Imbalance Overhead: 51.896 ms
GPU 6, Compute+Comm Time: 114.460 ms, Bubble Time: 132.457 ms, Imbalance Overhead: 43.059 ms
GPU 7, Compute+Comm Time: 113.647 ms, Bubble Time: 142.142 ms, Imbalance Overhead: 34.187 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 559.658 ms
GPU 0, Compute+Comm Time: 216.372 ms, Bubble Time: 275.430 ms, Imbalance Overhead: 67.856 ms
GPU 1, Compute+Comm Time: 217.909 ms, Bubble Time: 255.253 ms, Imbalance Overhead: 86.495 ms
GPU 2, Compute+Comm Time: 217.909 ms, Bubble Time: 237.504 ms, Imbalance Overhead: 104.245 ms
GPU 3, Compute+Comm Time: 217.909 ms, Bubble Time: 219.979 ms, Imbalance Overhead: 121.769 ms
GPU 4, Compute+Comm Time: 217.909 ms, Bubble Time: 202.101 ms, Imbalance Overhead: 139.647 ms
GPU 5, Compute+Comm Time: 217.909 ms, Bubble Time: 201.886 ms, Imbalance Overhead: 139.862 ms
GPU 6, Compute+Comm Time: 217.909 ms, Bubble Time: 220.950 ms, Imbalance Overhead: 120.798 ms
GPU 7, Compute+Comm Time: 221.213 ms, Bubble Time: 239.553 ms, Imbalance Overhead: 98.891 ms
The estimated cost of the whole pipeline: 892.116 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 618.707 ms
Partition 0 [0, 8) has cost: 618.707 ms
Partition 1 [8, 16) has cost: 613.828 ms
Partition 2 [16, 24) has cost: 613.828 ms
Partition 3 [24, 32) has cost: 611.477 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 300.253 ms
GPU 0, Compute+Comm Time: 153.760 ms, Bubble Time: 145.018 ms, Imbalance Overhead: 1.475 ms
GPU 1, Compute+Comm Time: 152.919 ms, Bubble Time: 125.918 ms, Imbalance Overhead: 21.417 ms
GPU 2, Compute+Comm Time: 152.919 ms, Bubble Time: 106.612 ms, Imbalance Overhead: 40.722 ms
GPU 3, Compute+Comm Time: 152.618 ms, Bubble Time: 106.646 ms, Imbalance Overhead: 40.989 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 566.214 ms
GPU 0, Compute+Comm Time: 286.276 ms, Bubble Time: 199.140 ms, Imbalance Overhead: 80.799 ms
GPU 1, Compute+Comm Time: 286.935 ms, Bubble Time: 199.199 ms, Imbalance Overhead: 80.081 ms
GPU 2, Compute+Comm Time: 286.935 ms, Bubble Time: 237.326 ms, Imbalance Overhead: 41.954 ms
GPU 3, Compute+Comm Time: 288.433 ms, Bubble Time: 274.993 ms, Imbalance Overhead: 2.788 ms
    The estimated cost with 2 DP ways is 909.791 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1232.534 ms
Partition 0 [0, 16) has cost: 1232.534 ms
Partition 1 [16, 32) has cost: 1225.304 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 343.261 ms
GPU 0, Compute+Comm Time: 228.877 ms, Bubble Time: 114.384 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 228.309 ms, Bubble Time: 114.194 ms, Imbalance Overhead: 0.759 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 602.926 ms
GPU 0, Compute+Comm Time: 400.947 ms, Bubble Time: 200.391 ms, Imbalance Overhead: 1.588 ms
GPU 1, Compute+Comm Time: 401.930 ms, Bubble Time: 200.996 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 993.496 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2457.839 ms
Partition 0 [0, 32) has cost: 2457.839 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 458.799 ms
GPU 0, Compute+Comm Time: 458.799 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 632.267 ms
GPU 0, Compute+Comm Time: 632.267 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1145.619 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 3.3368	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.1954	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.0227	TrainAcc 0.0870	ValidAcc 0.0750	TestAcc 0.0730	BestValid 0.0750
	Epoch 200:	Loss 2.8147	TrainAcc 0.1026	ValidAcc 0.0864	TestAcc 0.0845	BestValid 0.0864
	Epoch 250:	Loss 2.4606	TrainAcc 0.1924	ValidAcc 0.1629	TestAcc 0.1610	BestValid 0.1629
	Epoch 300:	Loss 3.4623	TrainAcc 0.1439	ValidAcc 0.1224	TestAcc 0.1205	BestValid 0.1629
	Epoch 350:	Loss 3.0291	TrainAcc 0.2171	ValidAcc 0.2180	TestAcc 0.2198	BestValid 0.2180
	Epoch 400:	Loss 2.5210	TrainAcc 0.3298	ValidAcc 0.3164	TestAcc 0.3193	BestValid 0.3164
	Epoch 450:	Loss 1.6930	TrainAcc 0.5326	ValidAcc 0.5471	TestAcc 0.5491	BestValid 0.5471
	Epoch 500:	Loss 1.6949	TrainAcc 0.5440	ValidAcc 0.5651	TestAcc 0.5659	BestValid 0.5651
	Epoch 550:	Loss 1.5997	TrainAcc 0.5602	ValidAcc 0.5743	TestAcc 0.5792	BestValid 0.5743
	Epoch 600:	Loss 1.3388	TrainAcc 0.6141	ValidAcc 0.6290	TestAcc 0.6313	BestValid 0.6290
	Epoch 650:	Loss 1.5526	TrainAcc 0.1462	ValidAcc 0.1325	TestAcc 0.1319	BestValid 0.6290
	Epoch 700:	Loss 1.2515	TrainAcc 0.6533	ValidAcc 0.6680	TestAcc 0.6680	BestValid 0.6680
	Epoch 750:	Loss 1.1919	TrainAcc 0.6476	ValidAcc 0.6648	TestAcc 0.6665	BestValid 0.6680
	Epoch 800:	Loss 1.0727	TrainAcc 0.6278	ValidAcc 0.6456	TestAcc 0.6473	BestValid 0.6680
	Epoch 850:	Loss 1.0412	TrainAcc 0.6169	ValidAcc 0.6386	TestAcc 0.6372	BestValid 0.6680
	Epoch 900:	Loss 1.1367	TrainAcc 0.6763	ValidAcc 0.6907	TestAcc 0.6932	BestValid 0.6907
	Epoch 950:	Loss 0.9698	TrainAcc 0.6849	ValidAcc 0.6985	TestAcc 0.7018	BestValid 0.6985
	Epoch 1000:	Loss 0.9525	TrainAcc 0.7229	ValidAcc 0.7357	TestAcc 0.7391	BestValid 0.7357
	Epoch 1050:	Loss 0.9625	TrainAcc 0.6475	ValidAcc 0.6659	TestAcc 0.6666	BestValid 0.7357
	Epoch 1100:	Loss 0.9644	TrainAcc 0.6943	ValidAcc 0.7105	TestAcc 0.7126	BestValid 0.7357
	Epoch 1150:	Loss 0.8662	TrainAcc 0.6522	ValidAcc 0.6661	TestAcc 0.6707	BestValid 0.7357
	Epoch 1200:	Loss 1.0367	TrainAcc 0.6825	ValidAcc 0.6917	TestAcc 0.6949	BestValid 0.7357
	Epoch 1250:	Loss 0.8589	TrainAcc 0.6786	ValidAcc 0.6973	TestAcc 0.6989	BestValid 0.7357
	Epoch 1300:	Loss 0.8967	TrainAcc 0.6540	ValidAcc 0.6746	TestAcc 0.6738	BestValid 0.7357
	Epoch 1350:	Loss 0.8300	TrainAcc 0.6562	ValidAcc 0.6724	TestAcc 0.6754	BestValid 0.7357
	Epoch 1400:	Loss 0.7619	TrainAcc 0.6787	ValidAcc 0.6969	TestAcc 0.6995	BestValid 0.7357
	Epoch 1450:	Loss 0.7750	TrainAcc 0.7190	ValidAcc 0.7330	TestAcc 0.7356	BestValid 0.7357
	Epoch 1500:	Loss 0.8121	TrainAcc 0.7017	ValidAcc 0.7183	TestAcc 0.7207	BestValid 0.7357
	Epoch 1550:	Loss 0.7464	TrainAcc 0.6734	ValidAcc 0.6894	TestAcc 0.6898	BestValid 0.7357
	Epoch 1600:	Loss 0.7177	TrainAcc 0.6792	ValidAcc 0.6951	TestAcc 0.6987	BestValid 0.7357
	Epoch 1650:	Loss 0.7342	TrainAcc 0.7118	ValidAcc 0.7269	TestAcc 0.7299	BestValid 0.7357
	Epoch 1700:	Loss 1.2050	TrainAcc 0.5953	ValidAcc 0.6141	TestAcc 0.6172	BestValid 0.7357
	Epoch 1750:	Loss 0.8267	TrainAcc 0.6994	ValidAcc 0.7131	TestAcc 0.7177	BestValid 0.7357
	Epoch 1800:	Loss 0.7312	TrainAcc 0.6870	ValidAcc 0.7004	TestAcc 0.7046	BestValid 0.7357
	Epoch 1850:	Loss 0.7074	TrainAcc 0.6821	ValidAcc 0.6992	TestAcc 0.7027	BestValid 0.7357
	Epoch 1900:	Loss 0.8976	TrainAcc 0.7004	ValidAcc 0.7144	TestAcc 0.7169	BestValid 0.7357
	Epoch 1950:	Loss 0.6679	TrainAcc 0.6867	ValidAcc 0.7019	TestAcc 0.7057	BestValid 0.7357
	Epoch 2000:	Loss 0.7211	TrainAcc 0.7514	ValidAcc 0.7623	TestAcc 0.7653	BestValid 0.7623
	Epoch 2050:	Loss 0.6778	TrainAcc 0.7092	ValidAcc 0.7226	TestAcc 0.7264	BestValid 0.7623
	Epoch 2100:	Loss 0.6399	TrainAcc 0.7084	ValidAcc 0.7201	TestAcc 0.7238	BestValid 0.7623
	Epoch 2150:	Loss 0.6849	TrainAcc 0.6960	ValidAcc 0.7089	TestAcc 0.7128	BestValid 0.7623
	Epoch 2200:	Loss 0.6610	TrainAcc 0.7356	ValidAcc 0.7490	TestAcc 0.7505	BestValid 0.7623
	Epoch 2250:	Loss 1.3292	TrainAcc 0.6570	ValidAcc 0.6774	TestAcc 0.6797	BestValid 0.7623
	Epoch 2300:	Loss 0.6359	TrainAcc 0.7187	ValidAcc 0.7311	TestAcc 0.7347	BestValid 0.7623
	Epoch 2350:	Loss 0.6339	TrainAcc 0.6999	ValidAcc 0.7112	TestAcc 0.7156	BestValid 0.7623
	Epoch 2400:	Loss 0.6079	TrainAcc 0.6995	ValidAcc 0.7119	TestAcc 0.7157	BestValid 0.7623
	Epoch 2450:	Loss 0.5939	TrainAcc 0.7047	ValidAcc 0.7165	TestAcc 0.7195	BestValid 0.7623
	Epoch 2500:	Loss 0.5783	TrainAcc 0.7032	ValidAcc 0.7158	TestAcc 0.7190	BestValid 0.7623
	Epoch 2550:	Loss 0.6605	TrainAcc 0.6931	ValidAcc 0.7050	TestAcc 0.7083	BestValid 0.7623
	Epoch 2600:	Loss 0.5744	TrainAcc 0.6951	ValidAcc 0.7079	TestAcc 0.7106	BestValid 0.7623
	Epoch 2650:	Loss 0.6514	TrainAcc 0.7647	ValidAcc 0.7759	TestAcc 0.7784	BestValid 0.7759
	Epoch 2700:	Loss 0.5531	TrainAcc 0.7389	ValidAcc 0.7496	TestAcc 0.7527	BestValid 0.7759
	Epoch 2750:	Loss 0.5786	TrainAcc 0.7232	ValidAcc 0.7327	TestAcc 0.7369	BestValid 0.7759
	Epoch 2800:	Loss 0.7553	TrainAcc 0.7077	ValidAcc 0.7183	TestAcc 0.7216	BestValid 0.7759
	Epoch 2850:	Loss 0.8622	TrainAcc 0.6457	ValidAcc 0.6616	TestAcc 0.6655	BestValid 0.7759
	Epoch 2900:	Loss 0.6004	TrainAcc 0.7178	ValidAcc 0.7293	TestAcc 0.7323	BestValid 0.7759
	Epoch 2950:	Loss 0.5690	TrainAcc 0.7206	ValidAcc 0.7303	TestAcc 0.7355	BestValid 0.7759
	Epoch 3000:	Loss 0.5651	TrainAcc 0.7322	ValidAcc 0.7411	TestAcc 0.7467	BestValid 0.7759
	Epoch 3050:	Loss 0.5561	TrainAcc 0.7023	ValidAcc 0.7146	TestAcc 0.7176	BestValid 0.7759
	Epoch 3100:	Loss 0.5481	TrainAcc 0.7375	ValidAcc 0.7460	TestAcc 0.7521	BestValid 0.7759
	Epoch 3150:	Loss 0.5894	TrainAcc 0.7370	ValidAcc 0.7479	TestAcc 0.7513	BestValid 0.7759
	Epoch 3200:	Loss 0.5375	TrainAcc 0.7515	ValidAcc 0.7617	TestAcc 0.7657	BestValid 0.7759
	Epoch 3250:	Loss 0.5391	TrainAcc 0.7350	ValidAcc 0.7475	TestAcc 0.7500	BestValid 0.7759
	Epoch 3300:	Loss 0.5021	TrainAcc 0.7333	ValidAcc 0.7425	TestAcc 0.7478	BestValid 0.7759
	Epoch 3350:	Loss 0.5137	TrainAcc 0.7236	ValidAcc 0.7345	TestAcc 0.7379	BestValid 0.7759
	Epoch 3400:	Loss 0.5182	TrainAcc 0.7254	ValidAcc 0.7366	TestAcc 0.7393	BestValid 0.7759
	Epoch 3450:	Loss 0.5078	TrainAcc 0.7229	ValidAcc 0.7326	TestAcc 0.7370	BestValid 0.7759
	Epoch 3500:	Loss 0.5041	TrainAcc 0.7347	ValidAcc 0.7451	TestAcc 0.7481	BestValid 0.7759
	Epoch 3550:	Loss 0.5052	TrainAcc 0.7385	ValidAcc 0.7487	TestAcc 0.7527	BestValid 0.7759
	Epoch 3600:	Loss 0.7867	TrainAcc 0.7415	ValidAcc 0.7512	TestAcc 0.7552	BestValid 0.7759
	Epoch 3650:	Loss 0.5209	TrainAcc 0.7102	ValidAcc 0.7204	TestAcc 0.7234	BestValid 0.7759
	Epoch 3700:	Loss 0.5009	TrainAcc 0.7629	ValidAcc 0.7724	TestAcc 0.7762	BestValid 0.7759
	Epoch 3750:	Loss 0.4932	TrainAcc 0.7438	ValidAcc 0.7527	TestAcc 0.7572	BestValid 0.7759
	Epoch 3800:	Loss 0.4868	TrainAcc 0.7188	ValidAcc 0.7293	TestAcc 0.7325	BestValid 0.7759
	Epoch 3850:	Loss 0.4992	TrainAcc 0.7413	ValidAcc 0.7516	TestAcc 0.7551	BestValid 0.7759
	Epoch 3900:	Loss 1.6535	TrainAcc 0.5802	ValidAcc 0.6264	TestAcc 0.6305	BestValid 0.7759
	Epoch 3950:	Loss 0.5456	TrainAcc 0.7052	ValidAcc 0.7197	TestAcc 0.7211	BestValid 0.7759
	Epoch 4000:	Loss 0.5220	TrainAcc 0.7208	ValidAcc 0.7323	TestAcc 0.7353	BestValid 0.7759
	Epoch 4050:	Loss 0.4922	TrainAcc 0.7307	ValidAcc 0.7416	TestAcc 0.7443	BestValid 0.7759
	Epoch 4100:	Loss 0.4999	TrainAcc 0.7394	ValidAcc 0.7512	TestAcc 0.7538	BestValid 0.7759
	Epoch 4150:	Loss 0.4769	TrainAcc 0.7228	ValidAcc 0.7340	TestAcc 0.7367	BestValid 0.7759
	Epoch 4200:	Loss 0.5524	TrainAcc 0.7241	ValidAcc 0.7354	TestAcc 0.7371	BestValid 0.7759
	Epoch 4250:	Loss 0.4829	TrainAcc 0.7272	ValidAcc 0.7375	TestAcc 0.7409	BestValid 0.7759
	Epoch 4300:	Loss 0.4764	TrainAcc 0.7486	ValidAcc 0.7593	TestAcc 0.7624	BestValid 0.7759
	Epoch 4350:	Loss 0.4662	TrainAcc 0.7343	ValidAcc 0.7451	TestAcc 0.7473	BestValid 0.7759
	Epoch 4400:	Loss 0.4875	TrainAcc 0.7808	ValidAcc 0.7920	TestAcc 0.7940	BestValid 0.7920
	Epoch 4450:	Loss 0.4701	TrainAcc 0.7397	ValidAcc 0.7494	TestAcc 0.7520	BestValid 0.7920
	Epoch 4500:	Loss 0.4684	TrainAcc 0.7371	ValidAcc 0.7482	TestAcc 0.7507	BestValid 0.7920
	Epoch 4550:	Loss 0.4542	TrainAcc 0.7428	ValidAcc 0.7518	TestAcc 0.7548	BestValid 0.7920
	Epoch 4600:	Loss 0.4548	TrainAcc 0.7381	ValidAcc 0.7505	TestAcc 0.7510	BestValid 0.7920
	Epoch 4650:	Loss 0.5471	TrainAcc 0.7350	ValidAcc 0.7457	TestAcc 0.7475	BestValid 0.7920
	Epoch 4700:	Loss 0.4647	TrainAcc 0.7531	ValidAcc 0.7612	TestAcc 0.7631	BestValid 0.7920
	Epoch 4750:	Loss 0.4477	TrainAcc 0.7545	ValidAcc 0.7637	TestAcc 0.7664	BestValid 0.7920
	Epoch 4800:	Loss 0.4479	TrainAcc 0.7177	ValidAcc 0.7281	TestAcc 0.7310	BestValid 0.7920
	Epoch 4850:	Loss 0.4624	TrainAcc 0.7280	ValidAcc 0.7386	TestAcc 0.7416	BestValid 0.7920
	Epoch 4900:	Loss 0.4464	TrainAcc 0.7404	ValidAcc 0.7508	TestAcc 0.7528	BestValid 0.7920
	Epoch 4950:	Loss 0.4925	TrainAcc 0.7375	ValidAcc 0.7458	TestAcc 0.7497	BestValid 0.7920
	Epoch 5000:	Loss 0.4373	TrainAcc 0.7419	ValidAcc 0.7505	TestAcc 0.7550	BestValid 0.7920
****** Epoch Time (Excluding Evaluation Cost): 0.878 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 1.036 ms (Max: 1.474, Min: 0.054, Sum: 8.290)
Cluster-Wide Average, Compute: 223.083 ms (Max: 361.826, Min: 116.727, Sum: 1784.665)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.010, Min: 0.008, Sum: 0.068)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.019, Min: 0.015, Sum: 0.127)
Cluster-Wide Average, Communication-Graph: 637.006 ms (Max: 742.976, Min: 499.149, Sum: 5096.051)
Cluster-Wide Average, Optimization: 3.031 ms (Max: 3.062, Min: 3.009, Sum: 24.250)
Cluster-Wide Average, Others: 13.832 ms (Max: 13.890, Min: 13.798, Sum: 110.659)
****** Breakdown Sum: 878.014 ms ******
Cluster-Wide Average, GPU Memory Consumption: 14.566 GB (Max: 15.104, Min: 14.471, Sum: 116.530)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.609 Gbps (Max: 49.506, Min: 11.511, Sum: 212.874)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 14.501 GB
****** Accuracy Results ******
Highest valid_acc: 0.7920
Target test_acc: 0.7940
Epoch to reach the target acc: 4399
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
