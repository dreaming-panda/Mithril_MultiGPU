Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INITInitialized node 0 on machine gnerv7

Initialized node 3 on machine gnerv7
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.385 seconds.
Building the CSC structure...
        It takes 2.433 seconds.
Building the CSC structure...
        It takes 2.479 seconds.
Building the CSC structure...
        It takes 2.484 seconds.
Building the CSC structure...
        It takes 2.509 seconds.
Building the CSC structure...
        It takes 2.518 seconds.
Building the CSC structure...
        It takes 2.698 seconds.
Building the CSC structure...
        It takes 2.701 seconds.
Building the CSC structure...
        It takes 2.353 seconds.
        It takes 2.344 seconds.
        It takes 2.377 seconds.
        It takes 2.374 seconds.
        It takes 2.416 seconds.
        It takes 2.402 seconds.
        It takes 2.406 seconds.
        It takes 2.413 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.353 seconds.
Building the Label Vector...
        It takes 0.043 seconds.
        It takes 0.341 seconds.
Building the Label Vector...
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.333 seconds.
Building the Label Vector...
        It takes 0.045 seconds.
        It takes 0.033 seconds.
        It takes 0.046 seconds.
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.311 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.316 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.643 Gbps (per GPU), 477.141 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.339 Gbps (per GPU), 474.711 Gbps (aggregated)
The layer-level communication performance: 59.330 Gbps (per GPU), 474.641 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.076 Gbps (per GPU), 472.606 Gbps (aggregated)
The layer-level communication performance: 59.049 Gbps (per GPU), 472.396 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.822 Gbps (per GPU), 470.579 Gbps (aggregated)
The layer-level communication performance: 58.771 Gbps (per GPU), 470.170 Gbps (aggregated)
The layer-level communication performance: 58.741 Gbps (per GPU), 469.932 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.921 Gbps (per GPU), 1263.366 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.912 Gbps (per GPU), 1263.297 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.921 Gbps (per GPU), 1263.368 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.915 Gbps (per GPU), 1263.321 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.791 Gbps (per GPU), 1262.326 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.912 Gbps (per GPU), 1263.297 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.793 Gbps (per GPU), 1262.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.912 Gbps (per GPU), 1263.297 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 98.076 Gbps (per GPU), 784.604 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.073 Gbps (per GPU), 784.587 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.077 Gbps (per GPU), 784.617 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.076 Gbps (per GPU), 784.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.076 Gbps (per GPU), 784.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.076 Gbps (per GPU), 784.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.074 Gbps (per GPU), 784.593 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.075 Gbps (per GPU), 784.599 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.960 Gbps (per GPU), 319.680 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.960 Gbps (per GPU), 319.681 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.961 Gbps (per GPU), 319.684 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.960 Gbps (per GPU), 319.682 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.960 Gbps (per GPU), 319.682 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.960 Gbps (per GPU), 319.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.960 Gbps (per GPU), 319.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.960 Gbps (per GPU), 319.682 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 10.31ms 11.52ms 13.97ms  1.36 29.12K 14.23M
 chk_1  5.87ms  7.14ms  9.55ms  1.63 29.12K  6.56M
 chk_2 17.29ms 18.73ms 21.40ms  1.24 29.12K 24.68M
 chk_3 17.45ms 18.75ms 21.21ms  1.22 29.12K 22.95M
 chk_4  5.72ms  7.02ms  9.38ms  1.64 29.12K  6.33M
 chk_5  9.70ms 11.02ms 13.32ms  1.37 29.12K 12.05M
 chk_6 10.93ms 12.15ms 14.39ms  1.32 29.12K 14.60M
 chk_7 10.18ms 11.24ms 13.48ms  1.32 29.12K 13.21M
   Avg 10.93 12.19 14.59
   Max 17.45 18.75 21.40
   Min  5.72  7.02  9.38
 Ratio  3.05  2.67  2.28
   Var 17.28 17.61 18.26
Profiling takes 3.439 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 4, starting model training...
*** Node 3, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 421)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 421)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 421)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 421)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 421)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 421)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[0, 421)...
+++++++++ Node 4 initializing the weights for op[0, 421)...
+++++++++ Node 1 initializing the weights for op[0, 421)...
+++++++++ Node 5 initializing the weights for op[0, 421)...
+++++++++ Node 3 initializing the weights for op[0, 421)...
+++++++++ Node 7 initializing the weights for op[0, 421)...
+++++++++ Node 0 initializing the weights for op[0, 421)...
+++++++++ Node 6 initializing the weights for op[0, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.3203	TrainAcc 0.1656	ValidAcc 0.1969	TestAcc 0.1969	BestValid 0.1969
	Epoch 50:	Loss 0.7541	TrainAcc 0.8955	ValidAcc 0.9075	TestAcc 0.9056	BestValid 0.9075
	Epoch 100:	Loss 0.3986	TrainAcc 0.9402	ValidAcc 0.9447	TestAcc 0.9441	BestValid 0.9447
	Epoch 150:	Loss 0.3205	TrainAcc 0.9494	ValidAcc 0.9520	TestAcc 0.9513	BestValid 0.9520
	Epoch 200:	Loss 0.2787	TrainAcc 0.9549	ValidAcc 0.9558	TestAcc 0.9547	BestValid 0.9558
	Epoch 250:	Loss 0.2555	TrainAcc 0.9584	ValidAcc 0.9578	TestAcc 0.9564	BestValid 0.9578
	Epoch 300:	Loss 0.2339	TrainAcc 0.9616	ValidAcc 0.9596	TestAcc 0.9582	BestValid 0.9596
	Epoch 350:	Loss 0.2170	TrainAcc 0.9642	ValidAcc 0.9608	TestAcc 0.9599	BestValid 0.9608
	Epoch 400:	Loss 0.2029	TrainAcc 0.9665	ValidAcc 0.9616	TestAcc 0.9610	BestValid 0.9616
	Epoch 450:	Loss 0.1943	TrainAcc 0.9686	ValidAcc 0.9626	TestAcc 0.9614	BestValid 0.9626
	Epoch 500:	Loss 0.1861	TrainAcc 0.9701	ValidAcc 0.9631	TestAcc 0.9616	BestValid 0.9631
	Epoch 550:	Loss 0.1759	TrainAcc 0.9717	ValidAcc 0.9634	TestAcc 0.9615	BestValid 0.9634
	Epoch 600:	Loss 0.1686	TrainAcc 0.9733	ValidAcc 0.9635	TestAcc 0.9620	BestValid 0.9635
	Epoch 650:	Loss 0.1627	TrainAcc 0.9745	ValidAcc 0.9636	TestAcc 0.9626	BestValid 0.9636
	Epoch 700:	Loss 0.1545	TrainAcc 0.9758	ValidAcc 0.9640	TestAcc 0.9626	BestValid 0.9640
	Epoch 750:	Loss 0.1495	TrainAcc 0.9769	ValidAcc 0.9645	TestAcc 0.9629	BestValid 0.9645
