Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITInitialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT

Initialized node 5 on machine gnerv8
Initialized node 7 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
DONE MPI INIT
Initialized node 1 on machine gnerv4
Initialized node 0 on machine gnerv4
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.883 seconds.
Building the CSC structure...
        It takes 2.049 seconds.
Building the CSC structure...
        It takes 2.434 seconds.
Building the CSC structure...
        It takes 2.450 seconds.
Building the CSC structure...
        It takes 2.606 seconds.
Building the CSC structure...
        It takes 2.615 seconds.
Building the CSC structure...
        It takes 2.638 seconds.
Building the CSC structure...
        It takes 2.640 seconds.
Building the CSC structure...
        It takes 1.858 seconds.
        It takes 1.847 seconds.
Building the Feature Vector...
        It takes 2.357 seconds.
        It takes 2.392 seconds.
        It takes 2.354 seconds.
        It takes 2.355 seconds.
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 2.362 seconds.
        It takes 2.363 seconds.
        It takes 0.040 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.240 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.284 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.040 seconds.
        It takes 0.315 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.302 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.265 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.032 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.598 Gbps (per GPU), 476.781 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.304 Gbps (per GPU), 474.434 Gbps (aggregated)
The layer-level communication performance: 59.286 Gbps (per GPU), 474.287 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.047 Gbps (per GPU), 472.378 Gbps (aggregated)
The layer-level communication performance: 59.018 Gbps (per GPU), 472.140 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.811 Gbps (per GPU), 470.484 Gbps (aggregated)
The layer-level communication performance: 58.775 Gbps (per GPU), 470.196 Gbps (aggregated)
The layer-level communication performance: 58.729 Gbps (per GPU), 469.834 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.582 Gbps (per GPU), 1276.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.570 Gbps (per GPU), 1276.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.579 Gbps (per GPU), 1276.635 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.573 Gbps (per GPU), 1276.586 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.582 Gbps (per GPU), 1276.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.570 Gbps (per GPU), 1276.564 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.585 Gbps (per GPU), 1276.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.579 Gbps (per GPU), 1276.635 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.740 Gbps (per GPU), 837.918 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.739 Gbps (per GPU), 837.911 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.741 Gbps (per GPU), 837.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.740 Gbps (per GPU), 837.918 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.740 Gbps (per GPU), 837.917 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.739 Gbps (per GPU), 837.911 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.740 Gbps (per GPU), 837.919 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.739 Gbps (per GPU), 837.911 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 40.129 Gbps (per GPU), 321.034 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.128 Gbps (per GPU), 321.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.129 Gbps (per GPU), 321.028 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.129 Gbps (per GPU), 321.032 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.129 Gbps (per GPU), 321.033 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.129 Gbps (per GPU), 321.033 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.129 Gbps (per GPU), 321.033 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.129 Gbps (per GPU), 321.035 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 10.00ms 10.59ms 12.55ms  1.25 29.12K 14.23M
 chk_1  5.36ms  6.18ms  8.12ms  1.51 29.12K  6.56M
 chk_2 16.98ms 17.92ms 19.76ms  1.16 29.12K 24.68M
 chk_3 17.02ms 17.81ms 19.95ms  1.17 29.12K 22.95M
 chk_4  5.13ms  6.02ms  7.93ms  1.55 29.12K  6.33M
 chk_5  9.17ms 10.00ms 11.98ms  1.31 29.12K 12.05M
 chk_6 10.45ms 11.13ms 13.03ms  1.25 29.12K 14.60M
 chk_7  9.76ms 10.54ms 12.18ms  1.25 29.12K 13.21M
   Avg 10.48 11.28 13.19
   Max 17.02 17.92 19.95
   Min  5.13  6.02  7.93
 Ratio  3.32  2.98  2.52
   Var 17.79 17.89 18.16
Profiling takes 3.176 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 4, starting model training...
*** Node 3, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 229)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 229)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 229)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 229)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 229)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 229)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 3 initializing the weights for op[0, 229)...
+++++++++ Node 1 initializing the weights for op[0, 229)...
+++++++++ Node 2 initializing the weights for op[0, 229)...
+++++++++ Node 0 initializing the weights for op[0, 229)...
+++++++++ Node 5 initializing the weights for op[0, 229)...
+++++++++ Node 6 initializing the weights for op[0, 229)...
+++++++++ Node 4 initializing the weights for op[0, 229)...
+++++++++ Node 7 initializing the weights for op[0, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.2788	TrainAcc 0.1848	ValidAcc 0.2250	TestAcc 0.2251	BestValid 0.2250
	Epoch 50:	Loss 0.4977	TrainAcc 0.9292	ValidAcc 0.9363	TestAcc 0.9362	BestValid 0.9363
	Epoch 100:	Loss 0.3834	TrainAcc 0.9393	ValidAcc 0.9441	TestAcc 0.9434	BestValid 0.9441
	Epoch 150:	Loss 0.3458	TrainAcc 0.9434	ValidAcc 0.9459	TestAcc 0.9462	BestValid 0.9459
	Epoch 200:	Loss 0.3242	TrainAcc 0.9456	ValidAcc 0.9477	TestAcc 0.9474	BestValid 0.9477
	Epoch 250:	Loss 0.3120	TrainAcc 0.9471	ValidAcc 0.9491	TestAcc 0.9481	BestValid 0.9491
	Epoch 300:	Loss 0.3006	TrainAcc 0.9484	ValidAcc 0.9495	TestAcc 0.9487	BestValid 0.9495
	Epoch 350:	Loss 0.2915	TrainAcc 0.9496	ValidAcc 0.9502	TestAcc 0.9490	BestValid 0.9502
	Epoch 400:	Loss 0.2829	TrainAcc 0.9506	ValidAcc 0.9503	TestAcc 0.9495	BestValid 0.9503
	Epoch 450:	Loss 0.2773	TrainAcc 0.9512	ValidAcc 0.9503	TestAcc 0.9500	BestValid 0.9503
	Epoch 500:	Loss 0.2712	TrainAcc 0.9521	ValidAcc 0.9507	TestAcc 0.9501	BestValid 0.9507
	Epoch 550:	Loss 0.2659	TrainAcc 0.9527	ValidAcc 0.9509	TestAcc 0.9502	BestValid 0.9509
	Epoch 600:	Loss 0.2622	TrainAcc 0.9536	ValidAcc 0.9507	TestAcc 0.9508	BestValid 0.9509
	Epoch 650:	Loss 0.2578	TrainAcc 0.9541	ValidAcc 0.9515	TestAcc 0.9509	BestValid 0.9515
	Epoch 700:	Loss 0.2551	TrainAcc 0.9547	ValidAcc 0.9515	TestAcc 0.9510	BestValid 0.9515
	Epoch 750:	Loss 0.2496	TrainAcc 0.9552	ValidAcc 0.9515	TestAcc 0.9511	BestValid 0.9515
	Epoch 800:	Loss 0.2484	TrainAcc 0.9557	ValidAcc 0.9520	TestAcc 0.9512	BestValid 0.9520
	Epoch 850:	Loss 0.2448	TrainAcc 0.9561	ValidAcc 0.9520	TestAcc 0.9510	BestValid 0.9520
	Epoch 900:	Loss 0.2433	TrainAcc 0.9565	ValidAcc 0.9526	TestAcc 0.9515	BestValid 0.9526
	Epoch 950:	Loss 0.2400	TrainAcc 0.9569	ValidAcc 0.9526	TestAcc 0.9516	BestValid 0.9526
	Epoch 1000:	Loss 0.2358	TrainAcc 0.9576	ValidAcc 0.9526	TestAcc 0.9519	BestValid 0.9526
	Epoch 1050:	Loss 0.2332	TrainAcc 0.9581	ValidAcc 0.9524	TestAcc 0.9513	BestValid 0.9526
	Epoch 1100:	Loss 0.2295	TrainAcc 0.9582	ValidAcc 0.9526	TestAcc 0.9522	BestValid 0.9526
	Epoch 1150:	Loss 0.2287	TrainAcc 0.9587	ValidAcc 0.9523	TestAcc 0.9519	BestValid 0.9526
	Epoch 1200:	Loss 0.2270	TrainAcc 0.9590	ValidAcc 0.9524	TestAcc 0.9521	BestValid 0.9526
	Epoch 1250:	Loss 0.2247	TrainAcc 0.9592	ValidAcc 0.9528	TestAcc 0.9526	BestValid 0.9528
	Epoch 1300:	Loss 0.2217	TrainAcc 0.9595	ValidAcc 0.9525	TestAcc 0.9522	BestValid 0.9528
	Epoch 1350:	Loss 0.2210	TrainAcc 0.9602	ValidAcc 0.9526	TestAcc 0.9526	BestValid 0.9528
	Epoch 1400:	Loss 0.2190	TrainAcc 0.9605	ValidAcc 0.9522	TestAcc 0.9528	BestValid 0.9528
	Epoch 1450:	Loss 0.2153	TrainAcc 0.9606	ValidAcc 0.9524	TestAcc 0.9525	BestValid 0.9528
	Epoch 1500:	Loss 0.2145	TrainAcc 0.9609	ValidAcc 0.9527	TestAcc 0.9528	BestValid 0.9528
	Epoch 1550:	Loss 0.2130	TrainAcc 0.9610	ValidAcc 0.9522	TestAcc 0.9530	BestValid 0.9528
	Epoch 1600:	Loss 0.2126	TrainAcc 0.9615	ValidAcc 0.9525	TestAcc 0.9528	BestValid 0.9528
	Epoch 1650:	Loss 0.2091	TrainAcc 0.9616	ValidAcc 0.9524	TestAcc 0.9530	BestValid 0.9528
	Epoch 1700:	Loss 0.2089	TrainAcc 0.9621	ValidAcc 0.9527	TestAcc 0.9527	BestValid 0.9528
	Epoch 1750:	Loss 0.2074	TrainAcc 0.9621	ValidAcc 0.9520	TestAcc 0.9526	BestValid 0.9528
	Epoch 1800:	Loss 0.2064	TrainAcc 0.9625	ValidAcc 0.9525	TestAcc 0.9528	BestValid 0.9528
	Epoch 1850:	Loss 0.2051	TrainAcc 0.9628	ValidAcc 0.9521	TestAcc 0.9528	BestValid 0.9528
	Epoch 1900:	Loss 0.2038	TrainAcc 0.9628	ValidAcc 0.9522	TestAcc 0.9526	BestValid 0.9528
	Epoch 1950:	Loss 0.2015	TrainAcc 0.9633	ValidAcc 0.9523	TestAcc 0.9529	BestValid 0.9528
	Epoch 2000:	Loss 0.1998	TrainAcc 0.9636	ValidAcc 0.9522	TestAcc 0.9529	BestValid 0.9528
	Epoch 2050:	Loss 0.1991	TrainAcc 0.9637	ValidAcc 0.9528	TestAcc 0.9532	BestValid 0.9528
	Epoch 2100:	Loss 0.1983	TrainAcc 0.9641	ValidAcc 0.9530	TestAcc 0.9526	BestValid 0.9530
	Epoch 2150:	Loss 0.1960	TrainAcc 0.9643	ValidAcc 0.9522	TestAcc 0.9526	BestValid 0.9530
	Epoch 2200:	Loss 0.1957	TrainAcc 0.9644	ValidAcc 0.9525	TestAcc 0.9530	BestValid 0.9530
	Epoch 2250:	Loss 0.1928	TrainAcc 0.9647	ValidAcc 0.9528	TestAcc 0.9534	BestValid 0.9530
	Epoch 2300:	Loss 0.1940	TrainAcc 0.9649	ValidAcc 0.9527	TestAcc 0.9535	BestValid 0.9530
	Epoch 2350:	Loss 0.1896	TrainAcc 0.9650	ValidAcc 0.9523	TestAcc 0.9528	BestValid 0.9530
	Epoch 2400:	Loss 0.1900	TrainAcc 0.9654	ValidAcc 0.9520	TestAcc 0.9531	BestValid 0.9530
	Epoch 2450:	Loss 0.1879	TrainAcc 0.9655	ValidAcc 0.9523	TestAcc 0.9533	BestValid 0.9530
	Epoch 2500:	Loss 0.1888	TrainAcc 0.9659	ValidAcc 0.9522	TestAcc 0.9533	BestValid 0.9530
	Epoch 2550:	Loss 0.1868	TrainAcc 0.9657	ValidAcc 0.9516	TestAcc 0.9529	BestValid 0.9530
	Epoch 2600:	Loss 0.1854	TrainAcc 0.9661	ValidAcc 0.9519	TestAcc 0.9529	BestValid 0.9530
	Epoch 2650:	Loss 0.1855	TrainAcc 0.9663	ValidAcc 0.9520	TestAcc 0.9530	BestValid 0.9530
	Epoch 2700:	Loss 0.1849	TrainAcc 0.9661	ValidAcc 0.9523	TestAcc 0.9528	BestValid 0.9530
	Epoch 2750:	Loss 0.1822	TrainAcc 0.9668	ValidAcc 0.9516	TestAcc 0.9531	BestValid 0.9530
	Epoch 2800:	Loss 0.1826	TrainAcc 0.9666	ValidAcc 0.9523	TestAcc 0.9530	BestValid 0.9530
	Epoch 2850:	Loss 0.1800	TrainAcc 0.9669	ValidAcc 0.9519	TestAcc 0.9531	BestValid 0.9530
	Epoch 2900:	Loss 0.1803	TrainAcc 0.9670	ValidAcc 0.9524	TestAcc 0.9529	BestValid 0.9530
	Epoch 2950:	Loss 0.1788	TrainAcc 0.9672	ValidAcc 0.9519	TestAcc 0.9527	BestValid 0.9530
	Epoch 3000:	Loss 0.1781	TrainAcc 0.9675	ValidAcc 0.9518	TestAcc 0.9526	BestValid 0.9530
	Epoch 3050:	Loss 0.1773	TrainAcc 0.9677	ValidAcc 0.9515	TestAcc 0.9526	BestValid 0.9530
	Epoch 3100:	Loss 0.1751	TrainAcc 0.9677	ValidAcc 0.9515	TestAcc 0.9530	BestValid 0.9530
	Epoch 3150:	Loss 0.1740	TrainAcc 0.9680	ValidAcc 0.9519	TestAcc 0.9535	BestValid 0.9530
	Epoch 3200:	Loss 0.1755	TrainAcc 0.9679	ValidAcc 0.9513	TestAcc 0.9527	BestValid 0.9530
	Epoch 3250:	Loss 0.1735	TrainAcc 0.9683	ValidAcc 0.9519	TestAcc 0.9530	BestValid 0.9530
	Epoch 3300:	Loss 0.1741	TrainAcc 0.9681	ValidAcc 0.9521	TestAcc 0.9530	BestValid 0.9530
	Epoch 3350:	Loss 0.1719	TrainAcc 0.9685	ValidAcc 0.9520	TestAcc 0.9532	BestValid 0.9530
	Epoch 3400:	Loss 0.1711	TrainAcc 0.9686	ValidAcc 0.9520	TestAcc 0.9530	BestValid 0.9530
	Epoch 3450:	Loss 0.1692	TrainAcc 0.9688	ValidAcc 0.9522	TestAcc 0.9531	BestValid 0.9530
	Epoch 3500:	Loss 0.1699	TrainAcc 0.9691	ValidAcc 0.9516	TestAcc 0.9529	BestValid 0.9530
	Epoch 3550:	Loss 0.1698	TrainAcc 0.9694	ValidAcc 0.9512	TestAcc 0.9530	BestValid 0.9530
	Epoch 3600:	Loss 0.1671	TrainAcc 0.9694	ValidAcc 0.9516	TestAcc 0.9530	BestValid 0.9530
	Epoch 3650:	Loss 0.1659	TrainAcc 0.9692	ValidAcc 0.9518	TestAcc 0.9532	BestValid 0.9530
	Epoch 3700:	Loss 0.1664	TrainAcc 0.9694	ValidAcc 0.9521	TestAcc 0.9529	BestValid 0.9530
	Epoch 3750:	Loss 0.1656	TrainAcc 0.9696	ValidAcc 0.9519	TestAcc 0.9529	BestValid 0.9530
	Epoch 3800:	Loss 0.1658	TrainAcc 0.9697	ValidAcc 0.9524	TestAcc 0.9531	BestValid 0.9530
	Epoch 3850:	Loss 0.1647	TrainAcc 0.9701	ValidAcc 0.9519	TestAcc 0.9531	BestValid 0.9530
	Epoch 3900:	Loss 0.1647	TrainAcc 0.9699	ValidAcc 0.9513	TestAcc 0.9529	BestValid 0.9530
	Epoch 3950:	Loss 0.1626	TrainAcc 0.9703	ValidAcc 0.9513	TestAcc 0.9531	BestValid 0.9530
	Epoch 4000:	Loss 0.1628	TrainAcc 0.9703	ValidAcc 0.9513	TestAcc 0.9528	BestValid 0.9530
	Epoch 4050:	Loss 0.1637	TrainAcc 0.9704	ValidAcc 0.9516	TestAcc 0.9530	BestValid 0.9530
	Epoch 4100:	Loss 0.1604	TrainAcc 0.9707	ValidAcc 0.9524	TestAcc 0.9533	BestValid 0.9530
	Epoch 4150:	Loss 0.1597	TrainAcc 0.9708	ValidAcc 0.9515	TestAcc 0.9531	BestValid 0.9530
	Epoch 4200:	Loss 0.1607	TrainAcc 0.9709	ValidAcc 0.9514	TestAcc 0.9528	BestValid 0.9530
	Epoch 4250:	Loss 0.1585	TrainAcc 0.9711	ValidAcc 0.9516	TestAcc 0.9530	BestValid 0.9530
	Epoch 4300:	Loss 0.1579	TrainAcc 0.9710	ValidAcc 0.9516	TestAcc 0.9531	BestValid 0.9530
	Epoch 4350:	Loss 0.1565	TrainAcc 0.9713	ValidAcc 0.9517	TestAcc 0.9529	BestValid 0.9530
	Epoch 4400:	Loss 0.1555	TrainAcc 0.9713	ValidAcc 0.9514	TestAcc 0.9530	BestValid 0.9530
	Epoch 4450:	Loss 0.1572	TrainAcc 0.9714	ValidAcc 0.9518	TestAcc 0.9527	BestValid 0.9530
	Epoch 4500:	Loss 0.1571	TrainAcc 0.9715	ValidAcc 0.9517	TestAcc 0.9529	BestValid 0.9530
	Epoch 4550:	Loss 0.1559	TrainAcc 0.9717	ValidAcc 0.9521	TestAcc 0.9530	BestValid 0.9530
	Epoch 4600:	Loss 0.1532	TrainAcc 0.9719	ValidAcc 0.9515	TestAcc 0.9528	BestValid 0.9530
	Epoch 4650:	Loss 0.1548	TrainAcc 0.9721	ValidAcc 0.9520	TestAcc 0.9532	BestValid 0.9530
	Epoch 4700:	Loss 0.1549	TrainAcc 0.9722	ValidAcc 0.9514	TestAcc 0.9526	BestValid 0.9530
	Epoch 4750:	Loss 0.1543	TrainAcc 0.9720	ValidAcc 0.9522	TestAcc 0.9529	BestValid 0.9530
	Epoch 4800:	Loss 0.1536	TrainAcc 0.9722	ValidAcc 0.9519	TestAcc 0.9534	BestValid 0.9530
	Epoch 4850:	Loss 0.1524	TrainAcc 0.9725	ValidAcc 0.9518	TestAcc 0.9525	BestValid 0.9530
	Epoch 4900:	Loss 0.1514	TrainAcc 0.9725	ValidAcc 0.9520	TestAcc 0.9529	BestValid 0.9530
	Epoch 4950:	Loss 0.1519	TrainAcc 0.9726	ValidAcc 0.9513	TestAcc 0.9524	BestValid 0.9530
	Epoch 5000:	Loss 0.1504	TrainAcc 0.9727	ValidAcc 0.9514	TestAcc 0.9527	BestValid 0.9530
****** Epoch Time (Excluding Evaluation Cost): 0.900 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 3.136 ms (Max: 4.995, Min: 0.040, Sum: 25.085)
Cluster-Wide Average, Compute: 252.882 ms (Max: 393.495, Min: 143.912, Sum: 2023.056)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.009, Min: 0.008, Sum: 0.069)
Cluster-Wide Average, Bubble-Imbalance: 0.017 ms (Max: 0.017, Min: 0.016, Sum: 0.134)
Cluster-Wide Average, Communication-Graph: 639.729 ms (Max: 746.895, Min: 502.122, Sum: 5117.833)
Cluster-Wide Average, Optimization: 2.994 ms (Max: 3.060, Min: 2.953, Sum: 23.955)
Cluster-Wide Average, Others: 0.838 ms (Max: 0.863, Min: 0.815, Sum: 6.702)
****** Breakdown Sum: 899.604 ms ******
Cluster-Wide Average, GPU Memory Consumption: 8.599 GB (Max: 9.342, Min: 8.475, Sum: 68.788)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.479 Gbps (Max: 49.185, Min: 11.454, Sum: 211.830)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.020 GB
Total communication (cluster-wide, per-epoch): 14.502 GB
****** Accuracy Results ******
Highest valid_acc: 0.9530
Target test_acc: 0.9526
Epoch to reach the target acc: 2099
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
