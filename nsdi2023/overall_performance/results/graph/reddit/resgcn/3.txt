Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 1 on machine gnerv4
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.881 seconds.
Building the CSC structure...
        It takes 1.902 seconds.
Building the CSC structure...
        It takes 2.351 seconds.
Building the CSC structure...
        It takes 2.463 seconds.
Building the CSC structure...
        It takes 2.526 seconds.
Building the CSC structure...
        It takes 2.564 seconds.
Building the CSC structure...
        It takes 2.630 seconds.
Building the CSC structure...
        It takes 2.651 seconds.
Building the CSC structure...
        It takes 1.854 seconds.
        It takes 1.848 seconds.
        It takes 2.250 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.357 seconds.
        It takes 2.482 seconds.
        It takes 2.303 seconds.
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 2.378 seconds.
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 2.507 seconds.
        It takes 0.037 seconds.
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.250 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.290 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
        It takes 0.296 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
        It takes 0.232 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
        It takes 0.292 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.554 Gbps (per GPU), 476.436 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.268 Gbps (per GPU), 474.145 Gbps (aggregated)
The layer-level communication performance: 59.260 Gbps (per GPU), 474.082 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.028 Gbps (per GPU), 472.226 Gbps (aggregated)
The layer-level communication performance: 58.975 Gbps (per GPU), 471.799 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.796 Gbps (per GPU), 470.364 Gbps (aggregated)
The layer-level communication performance: 58.753 Gbps (per GPU), 470.022 Gbps (aggregated)
The layer-level communication performance: 58.702 Gbps (per GPU), 469.619 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.734 Gbps (per GPU), 1269.871 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.695 Gbps (per GPU), 1269.559 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.650 Gbps (per GPU), 1269.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.647 Gbps (per GPU), 1269.174 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.644 Gbps (per GPU), 1269.150 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.692 Gbps (per GPU), 1269.535 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.647 Gbps (per GPU), 1269.174 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.674 Gbps (per GPU), 1269.395 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.059 Gbps (per GPU), 832.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.059 Gbps (per GPU), 832.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.059 Gbps (per GPU), 832.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.061 Gbps (per GPU), 832.485 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.058 Gbps (per GPU), 832.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.059 Gbps (per GPU), 832.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.049 Gbps (per GPU), 832.396 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 4): 102.523 Gbps (per GPU), 820.188 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 40.150 Gbps (per GPU), 321.199 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.150 Gbps (per GPU), 321.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.150 Gbps (per GPU), 321.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.151 Gbps (per GPU), 321.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.150 Gbps (per GPU), 321.202 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.150 Gbps (per GPU), 321.199 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.149 Gbps (per GPU), 321.188 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.151 Gbps (per GPU), 321.207 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.95ms 10.51ms 12.63ms  1.27 29.12K 14.23M
 chk_1  5.34ms  6.18ms  8.17ms  1.53 29.12K  6.56M
 chk_2 16.96ms 17.83ms 19.81ms  1.17 29.12K 24.68M
 chk_3 17.03ms 17.85ms 19.82ms  1.16 29.12K 22.95M
 chk_4  5.13ms  6.01ms  7.98ms  1.55 29.12K  6.33M
 chk_5  9.20ms  9.96ms 11.95ms  1.30 29.12K 12.05M
 chk_6 10.39ms 11.10ms 13.07ms  1.26 29.12K 14.60M
 chk_7  9.76ms 10.35ms 12.20ms  1.25 29.12K 13.21M
   Avg 10.47 11.22 13.20
   Max 17.03 17.85 19.82
   Min  5.13  6.01  7.98
 Ratio  3.32  2.97  2.48
   Var 17.80 17.87 17.88
Profiling takes 3.185 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 229)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 229)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 229)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 229)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 229)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 229)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 229)...
+++++++++ Node 0 initializing the weights for op[0, 229)...
+++++++++ Node 2 initializing the weights for op[0, 229)...
+++++++++ Node 3 initializing the weights for op[0, 229)...
+++++++++ Node 6 initializing the weights for op[0, 229)...
+++++++++ Node 7 initializing the weights for op[0, 229)...
+++++++++ Node 4 initializing the weights for op[0, 229)...
+++++++++ Node 5 initializing the weights for op[0, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.2991	TrainAcc 0.1430	ValidAcc 0.1343	TestAcc 0.1330	BestValid 0.1343
	Epoch 50:	Loss 0.4980	TrainAcc 0.9268	ValidAcc 0.9334	TestAcc 0.9329	BestValid 0.9334
	Epoch 100:	Loss 0.3790	TrainAcc 0.9398	ValidAcc 0.9433	TestAcc 0.9444	BestValid 0.9433
	Epoch 150:	Loss 0.3415	TrainAcc 0.9435	ValidAcc 0.9459	TestAcc 0.9472	BestValid 0.9459
	Epoch 200:	Loss 0.3202	TrainAcc 0.9457	ValidAcc 0.9480	TestAcc 0.9479	BestValid 0.9480
	Epoch 250:	Loss 0.3063	TrainAcc 0.9473	ValidAcc 0.9486	TestAcc 0.9488	BestValid 0.9486
	Epoch 300:	Loss 0.2943	TrainAcc 0.9488	ValidAcc 0.9491	TestAcc 0.9492	BestValid 0.9491
	Epoch 350:	Loss 0.2846	TrainAcc 0.9499	ValidAcc 0.9499	TestAcc 0.9501	BestValid 0.9499
	Epoch 400:	Loss 0.2790	TrainAcc 0.9509	ValidAcc 0.9502	TestAcc 0.9508	BestValid 0.9502
	Epoch 450:	Loss 0.2734	TrainAcc 0.9516	ValidAcc 0.9507	TestAcc 0.9509	BestValid 0.9507
	Epoch 500:	Loss 0.2685	TrainAcc 0.9523	ValidAcc 0.9509	TestAcc 0.9513	BestValid 0.9509
	Epoch 550:	Loss 0.2615	TrainAcc 0.9532	ValidAcc 0.9516	TestAcc 0.9517	BestValid 0.9516
	Epoch 600:	Loss 0.2595	TrainAcc 0.9537	ValidAcc 0.9513	TestAcc 0.9515	BestValid 0.9516
	Epoch 650:	Loss 0.2561	TrainAcc 0.9543	ValidAcc 0.9520	TestAcc 0.9519	BestValid 0.9520
	Epoch 700:	Loss 0.2503	TrainAcc 0.9551	ValidAcc 0.9518	TestAcc 0.9519	BestValid 0.9520
	Epoch 750:	Loss 0.2480	TrainAcc 0.9555	ValidAcc 0.9520	TestAcc 0.9520	BestValid 0.9520
	Epoch 800:	Loss 0.2452	TrainAcc 0.9562	ValidAcc 0.9521	TestAcc 0.9521	BestValid 0.9521
	Epoch 850:	Loss 0.2421	TrainAcc 0.9564	ValidAcc 0.9521	TestAcc 0.9523	BestValid 0.9521
	Epoch 900:	Loss 0.2395	TrainAcc 0.9568	ValidAcc 0.9521	TestAcc 0.9520	BestValid 0.9521
	Epoch 950:	Loss 0.2368	TrainAcc 0.9571	ValidAcc 0.9520	TestAcc 0.9526	BestValid 0.9521
	Epoch 1000:	Loss 0.2325	TrainAcc 0.9577	ValidAcc 0.9523	TestAcc 0.9527	BestValid 0.9523
	Epoch 1050:	Loss 0.2313	TrainAcc 0.9581	ValidAcc 0.9529	TestAcc 0.9527	BestValid 0.9529
	Epoch 1100:	Loss 0.2309	TrainAcc 0.9584	ValidAcc 0.9525	TestAcc 0.9529	BestValid 0.9529
	Epoch 1150:	Loss 0.2265	TrainAcc 0.9586	ValidAcc 0.9528	TestAcc 0.9529	BestValid 0.9529
	Epoch 1200:	Loss 0.2251	TrainAcc 0.9590	ValidAcc 0.9525	TestAcc 0.9529	BestValid 0.9529
	Epoch 1250:	Loss 0.2220	TrainAcc 0.9594	ValidAcc 0.9526	TestAcc 0.9529	BestValid 0.9529
	Epoch 1300:	Loss 0.2214	TrainAcc 0.9597	ValidAcc 0.9524	TestAcc 0.9529	BestValid 0.9529
	Epoch 1350:	Loss 0.2187	TrainAcc 0.9600	ValidAcc 0.9523	TestAcc 0.9528	BestValid 0.9529
	Epoch 1400:	Loss 0.2163	TrainAcc 0.9606	ValidAcc 0.9525	TestAcc 0.9528	BestValid 0.9529
	Epoch 1450:	Loss 0.2139	TrainAcc 0.9605	ValidAcc 0.9524	TestAcc 0.9529	BestValid 0.9529
	Epoch 1500:	Loss 0.2119	TrainAcc 0.9610	ValidAcc 0.9528	TestAcc 0.9531	BestValid 0.9529
	Epoch 1550:	Loss 0.2123	TrainAcc 0.9612	ValidAcc 0.9528	TestAcc 0.9529	BestValid 0.9529
	Epoch 1600:	Loss 0.2096	TrainAcc 0.9614	ValidAcc 0.9528	TestAcc 0.9531	BestValid 0.9529
	Epoch 1650:	Loss 0.2092	TrainAcc 0.9618	ValidAcc 0.9528	TestAcc 0.9530	BestValid 0.9529
	Epoch 1700:	Loss 0.2088	TrainAcc 0.9621	ValidAcc 0.9525	TestAcc 0.9530	BestValid 0.9529
	Epoch 1750:	Loss 0.2055	TrainAcc 0.9623	ValidAcc 0.9525	TestAcc 0.9529	BestValid 0.9529
	Epoch 1800:	Loss 0.2045	TrainAcc 0.9625	ValidAcc 0.9529	TestAcc 0.9531	BestValid 0.9529
	Epoch 1850:	Loss 0.2042	TrainAcc 0.9627	ValidAcc 0.9527	TestAcc 0.9533	BestValid 0.9529
	Epoch 1900:	Loss 0.2013	TrainAcc 0.9631	ValidAcc 0.9524	TestAcc 0.9533	BestValid 0.9529
	Epoch 1950:	Loss 0.1991	TrainAcc 0.9635	ValidAcc 0.9523	TestAcc 0.9531	BestValid 0.9529
	Epoch 2000:	Loss 0.2002	TrainAcc 0.9636	ValidAcc 0.9522	TestAcc 0.9531	BestValid 0.9529
	Epoch 2050:	Loss 0.1974	TrainAcc 0.9638	ValidAcc 0.9525	TestAcc 0.9531	BestValid 0.9529
	Epoch 2100:	Loss 0.1957	TrainAcc 0.9644	ValidAcc 0.9526	TestAcc 0.9533	BestValid 0.9529
	Epoch 2150:	Loss 0.1952	TrainAcc 0.9643	ValidAcc 0.9523	TestAcc 0.9529	BestValid 0.9529
	Epoch 2200:	Loss 0.1926	TrainAcc 0.9646	ValidAcc 0.9522	TestAcc 0.9531	BestValid 0.9529
	Epoch 2250:	Loss 0.1906	TrainAcc 0.9648	ValidAcc 0.9527	TestAcc 0.9534	BestValid 0.9529
	Epoch 2300:	Loss 0.1903	TrainAcc 0.9650	ValidAcc 0.9525	TestAcc 0.9530	BestValid 0.9529
	Epoch 2350:	Loss 0.1914	TrainAcc 0.9652	ValidAcc 0.9529	TestAcc 0.9530	BestValid 0.9529
	Epoch 2400:	Loss 0.1900	TrainAcc 0.9655	ValidAcc 0.9523	TestAcc 0.9528	BestValid 0.9529
	Epoch 2450:	Loss 0.1877	TrainAcc 0.9657	ValidAcc 0.9528	TestAcc 0.9532	BestValid 0.9529
	Epoch 2500:	Loss 0.1849	TrainAcc 0.9659	ValidAcc 0.9528	TestAcc 0.9524	BestValid 0.9529
	Epoch 2550:	Loss 0.1858	TrainAcc 0.9661	ValidAcc 0.9530	TestAcc 0.9529	BestValid 0.9530
	Epoch 2600:	Loss 0.1843	TrainAcc 0.9662	ValidAcc 0.9525	TestAcc 0.9526	BestValid 0.9530
	Epoch 2650:	Loss 0.1837	TrainAcc 0.9663	ValidAcc 0.9527	TestAcc 0.9528	BestValid 0.9530
	Epoch 2700:	Loss 0.1829	TrainAcc 0.9667	ValidAcc 0.9528	TestAcc 0.9528	BestValid 0.9530
	Epoch 2750:	Loss 0.1826	TrainAcc 0.9668	ValidAcc 0.9535	TestAcc 0.9532	BestValid 0.9535
	Epoch 2800:	Loss 0.1793	TrainAcc 0.9669	ValidAcc 0.9524	TestAcc 0.9531	BestValid 0.9535
	Epoch 2850:	Loss 0.1808	TrainAcc 0.9668	ValidAcc 0.9525	TestAcc 0.9529	BestValid 0.9535
	Epoch 2900:	Loss 0.1789	TrainAcc 0.9672	ValidAcc 0.9527	TestAcc 0.9527	BestValid 0.9535
	Epoch 2950:	Loss 0.1763	TrainAcc 0.9675	ValidAcc 0.9523	TestAcc 0.9527	BestValid 0.9535
	Epoch 3000:	Loss 0.1763	TrainAcc 0.9677	ValidAcc 0.9527	TestAcc 0.9526	BestValid 0.9535
	Epoch 3050:	Loss 0.1761	TrainAcc 0.9677	ValidAcc 0.9529	TestAcc 0.9527	BestValid 0.9535
	Epoch 3100:	Loss 0.1754	TrainAcc 0.9680	ValidAcc 0.9528	TestAcc 0.9530	BestValid 0.9535
	Epoch 3150:	Loss 0.1721	TrainAcc 0.9681	ValidAcc 0.9525	TestAcc 0.9530	BestValid 0.9535
	Epoch 3200:	Loss 0.1730	TrainAcc 0.9682	ValidAcc 0.9529	TestAcc 0.9532	BestValid 0.9535
	Epoch 3250:	Loss 0.1717	TrainAcc 0.9681	ValidAcc 0.9524	TestAcc 0.9529	BestValid 0.9535
	Epoch 3300:	Loss 0.1717	TrainAcc 0.9685	ValidAcc 0.9527	TestAcc 0.9527	BestValid 0.9535
	Epoch 3350:	Loss 0.1717	TrainAcc 0.9688	ValidAcc 0.9529	TestAcc 0.9530	BestValid 0.9535
	Epoch 3400:	Loss 0.1693	TrainAcc 0.9687	ValidAcc 0.9525	TestAcc 0.9528	BestValid 0.9535
	Epoch 3450:	Loss 0.1690	TrainAcc 0.9690	ValidAcc 0.9523	TestAcc 0.9530	BestValid 0.9535
	Epoch 3500:	Loss 0.1688	TrainAcc 0.9692	ValidAcc 0.9525	TestAcc 0.9532	BestValid 0.9535
	Epoch 3550:	Loss 0.1675	TrainAcc 0.9692	ValidAcc 0.9529	TestAcc 0.9531	BestValid 0.9535
	Epoch 3600:	Loss 0.1682	TrainAcc 0.9693	ValidAcc 0.9520	TestAcc 0.9525	BestValid 0.9535
	Epoch 3650:	Loss 0.1647	TrainAcc 0.9697	ValidAcc 0.9520	TestAcc 0.9526	BestValid 0.9535
	Epoch 3700:	Loss 0.1650	TrainAcc 0.9696	ValidAcc 0.9520	TestAcc 0.9530	BestValid 0.9535
	Epoch 3750:	Loss 0.1628	TrainAcc 0.9698	ValidAcc 0.9521	TestAcc 0.9533	BestValid 0.9535
	Epoch 3800:	Loss 0.1624	TrainAcc 0.9701	ValidAcc 0.9523	TestAcc 0.9530	BestValid 0.9535
	Epoch 3850:	Loss 0.1629	TrainAcc 0.9701	ValidAcc 0.9523	TestAcc 0.9531	BestValid 0.9535
	Epoch 3900:	Loss 0.1615	TrainAcc 0.9703	ValidAcc 0.9523	TestAcc 0.9531	BestValid 0.9535
	Epoch 3950:	Loss 0.1597	TrainAcc 0.9702	ValidAcc 0.9520	TestAcc 0.9529	BestValid 0.9535
	Epoch 4000:	Loss 0.1628	TrainAcc 0.9704	ValidAcc 0.9519	TestAcc 0.9528	BestValid 0.9535
	Epoch 4050:	Loss 0.1597	TrainAcc 0.9708	ValidAcc 0.9521	TestAcc 0.9527	BestValid 0.9535
	Epoch 4100:	Loss 0.1590	TrainAcc 0.9706	ValidAcc 0.9520	TestAcc 0.9526	BestValid 0.9535
	Epoch 4150:	Loss 0.1587	TrainAcc 0.9708	ValidAcc 0.9519	TestAcc 0.9527	BestValid 0.9535
	Epoch 4200:	Loss 0.1583	TrainAcc 0.9708	ValidAcc 0.9513	TestAcc 0.9526	BestValid 0.9535
	Epoch 4250:	Loss 0.1590	TrainAcc 0.9710	ValidAcc 0.9519	TestAcc 0.9523	BestValid 0.9535
	Epoch 4300:	Loss 0.1560	TrainAcc 0.9711	ValidAcc 0.9516	TestAcc 0.9523	BestValid 0.9535
	Epoch 4350:	Loss 0.1547	TrainAcc 0.9713	ValidAcc 0.9510	TestAcc 0.9520	BestValid 0.9535
	Epoch 4400:	Loss 0.1537	TrainAcc 0.9713	ValidAcc 0.9520	TestAcc 0.9524	BestValid 0.9535
	Epoch 4450:	Loss 0.1545	TrainAcc 0.9716	ValidAcc 0.9512	TestAcc 0.9524	BestValid 0.9535
	Epoch 4500:	Loss 0.1540	TrainAcc 0.9716	ValidAcc 0.9513	TestAcc 0.9523	BestValid 0.9535
	Epoch 4550:	Loss 0.1534	TrainAcc 0.9719	ValidAcc 0.9517	TestAcc 0.9529	BestValid 0.9535
	Epoch 4600:	Loss 0.1535	TrainAcc 0.9716	ValidAcc 0.9517	TestAcc 0.9527	BestValid 0.9535
	Epoch 4650:	Loss 0.1525	TrainAcc 0.9718	ValidAcc 0.9516	TestAcc 0.9526	BestValid 0.9535
	Epoch 4700:	Loss 0.1521	TrainAcc 0.9719	ValidAcc 0.9513	TestAcc 0.9527	BestValid 0.9535
	Epoch 4750:	Loss 0.1516	TrainAcc 0.9723	ValidAcc 0.9512	TestAcc 0.9522	BestValid 0.9535
	Epoch 4800:	Loss 0.1513	TrainAcc 0.9722	ValidAcc 0.9514	TestAcc 0.9527	BestValid 0.9535
	Epoch 4850:	Loss 0.1514	TrainAcc 0.9725	ValidAcc 0.9506	TestAcc 0.9522	BestValid 0.9535
	Epoch 4900:	Loss 0.1503	TrainAcc 0.9723	ValidAcc 0.9513	TestAcc 0.9522	BestValid 0.9535
	Epoch 4950:	Loss 0.1501	TrainAcc 0.9722	ValidAcc 0.9515	TestAcc 0.9524	BestValid 0.9535
	Epoch 5000:	Loss 0.1484	TrainAcc 0.9726	ValidAcc 0.9510	TestAcc 0.9523	BestValid 0.9535
****** Epoch Time (Excluding Evaluation Cost): 0.909 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 3.102 ms (Max: 4.936, Min: 0.039, Sum: 24.816)
Cluster-Wide Average, Compute: 252.680 ms (Max: 392.922, Min: 144.409, Sum: 2021.439)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.008, Sum: 0.068)
Cluster-Wide Average, Bubble-Imbalance: 0.018 ms (Max: 0.021, Min: 0.016, Sum: 0.145)
Cluster-Wide Average, Communication-Graph: 649.574 ms (Max: 755.964, Min: 512.348, Sum: 5196.591)
Cluster-Wide Average, Optimization: 2.945 ms (Max: 2.977, Min: 2.930, Sum: 23.563)
Cluster-Wide Average, Others: 0.850 ms (Max: 0.873, Min: 0.824, Sum: 6.800)
****** Breakdown Sum: 909.178 ms ******
Cluster-Wide Average, GPU Memory Consumption: 8.599 GB (Max: 9.342, Min: 8.475, Sum: 68.788)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.025 Gbps (Max: 48.134, Min: 11.316, Sum: 208.203)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 14.482 GB
Weight-sync communication (cluster-wide, per-epoch): 0.020 GB
Total communication (cluster-wide, per-epoch): 14.502 GB
****** Accuracy Results ******
Highest valid_acc: 0.9535
Target test_acc: 0.9532
Epoch to reach the target acc: 2749
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 97630 RUNNING AT gnerv8
=   EXIT CODE: 9
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
[proxy:0:1@gnerv8] HYD_pmcd_pmip_control_cmd_cb (proxy/pmip_cb.c:480): assert (!closed) failed
[proxy:0:1@gnerv8] HYDT_dmxu_poll_wait_for_event (lib/tools/demux/demux_poll.c:76): callback returned error status
[proxy:0:1@gnerv8] main (proxy/pmip.c:190): demux engine error waiting for event
srun: error: gnerv8: task 1: Exited with exit code 7
[mpiexec@gnerv4] HYDT_bscu_wait_for_completion (lib/tools/bootstrap/utils/bscu_wait.c:109): one of the processes terminated badly; aborting
[mpiexec@gnerv4] HYDT_bsci_wait_for_completion (lib/tools/bootstrap/src/bsci_wait.c:21): launcher returned error waiting for completion
[mpiexec@gnerv4] HYD_pmci_wait_for_completion (mpiexec/pmiserv_pmci.c:197): launcher returned error waiting for completion
[mpiexec@gnerv4] main (mpiexec/mpiexec.c:252): process manager error waiting for completion
