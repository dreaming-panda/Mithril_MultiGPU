srun: Job 3743 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 3743
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
DONE MPI INIT
Initialized node 7 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INITDONE MPI INIT
Initialized node 5 on machine gnerv8

Initialized node 6 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.384 seconds.
Building the CSC structure...
        It takes 2.461 seconds.
Building the CSC structure...
        It takes 2.469 seconds.
Building the CSC structure...
        It takes 2.486 seconds.
Building the CSC structure...
        It takes 2.631 seconds.
Building the CSC structure...
        It takes 2.693 seconds.
Building the CSC structure...
        It takes 2.735 seconds.
Building the CSC structure...
        It takes 2.811 seconds.
Building the CSC structure...
        It takes 2.363 seconds.
        It takes 2.417 seconds.
        It takes 2.401 seconds.
        It takes 2.485 seconds.
        It takes 2.354 seconds.
        It takes 2.362 seconds.
        It takes 2.323 seconds.
        It takes 2.314 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.347 seconds.
Building the Label Vector...
        It takes 0.359 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.045 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.033 seconds.
        It takes 0.295 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.309 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.332 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 59.063 Gbps (per GPU), 472.500 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.771 Gbps (per GPU), 470.165 Gbps (aggregated)
The layer-level communication performance: 58.770 Gbps (per GPU), 470.157 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.512 Gbps (per GPU), 468.096 Gbps (aggregated)
The layer-level communication performance: 58.500 Gbps (per GPU), 468.000 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.276 Gbps (per GPU), 466.208 Gbps (aggregated)
The layer-level communication performance: 58.231 Gbps (per GPU), 465.849 Gbps (aggregated)
The layer-level communication performance: 58.193 Gbps (per GPU), 465.547 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.476 Gbps (per GPU), 1267.805 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.452 Gbps (per GPU), 1267.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.470 Gbps (per GPU), 1267.760 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.452 Gbps (per GPU), 1267.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.473 Gbps (per GPU), 1267.784 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.455 Gbps (per GPU), 1267.641 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.473 Gbps (per GPU), 1267.782 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.455 Gbps (per GPU), 1267.639 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.737 Gbps (per GPU), 837.897 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.736 Gbps (per GPU), 837.890 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.737 Gbps (per GPU), 837.897 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.738 Gbps (per GPU), 837.903 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.737 Gbps (per GPU), 837.897 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.737 Gbps (per GPU), 837.897 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.736 Gbps (per GPU), 837.890 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.738 Gbps (per GPU), 837.904 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.674 Gbps (per GPU), 317.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.673 Gbps (per GPU), 317.384 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.673 Gbps (per GPU), 317.388 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.674 Gbps (per GPU), 317.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.675 Gbps (per GPU), 317.399 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.674 Gbps (per GPU), 317.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.673 Gbps (per GPU), 317.385 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.674 Gbps (per GPU), 317.396 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 10.47ms 11.49ms 14.29ms  1.36 29.12K 14.23M
 chk_1  5.93ms  7.15ms  9.59ms  1.62 29.12K  6.56M
 chk_2 17.49ms 18.75ms 21.24ms  1.21 29.12K 24.68M
 chk_3 17.55ms 19.07ms 21.22ms  1.21 29.12K 22.95M
 chk_4  5.73ms  7.02ms  9.40ms  1.64 29.12K  6.33M
 chk_5  9.74ms 11.39ms 13.36ms  1.37 29.12K 12.05M
 chk_6 10.95ms 12.33ms 14.47ms  1.32 29.12K 14.60M
 chk_7 10.25ms 11.57ms 13.62ms  1.33 29.12K 13.21M
   Avg 11.01 12.35 14.65
   Max 17.55 19.07 21.24
   Min  5.73  7.02  9.40
 Ratio  3.07  2.72  2.26
   Var 17.63 17.98 17.83
Profiling takes 3.461 s

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 1142027 RUNNING AT gnerv8
=   EXIT CODE: 9
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
[proxy:0:0@gnerv7] HYD_pmcd_pmip_control_cmd_cb (proxy/pmip_cb.c:480): assert (!closed) failed
[proxy:0:0@gnerv7] HYDT_dmxu_poll_wait_for_event (lib/tools/demux/demux_poll.c:76): callback returned error status
[proxy:0:0@gnerv7] main (proxy/pmip.c:127): demux engine error waiting for event
srun: error: gnerv7: task 0: Exited with exit code 7
[mpiexec@gnerv7] HYDT_bscu_wait_for_completion (lib/tools/bootstrap/utils/bscu_wait.c:109): one of the processes terminated badly; aborting
[mpiexec@gnerv7] HYDT_bsci_wait_for_completion (lib/tools/bootstrap/src/bsci_wait.c:21): launcher returned error waiting for completion
[mpiexec@gnerv7] HYD_pmci_wait_for_completion (mpiexec/pmiserv_pmci.c:197): launcher returned error waiting for completion
[mpiexec@gnerv7] main (mpiexec/mpiexec.c:252): process manager error waiting for completion
