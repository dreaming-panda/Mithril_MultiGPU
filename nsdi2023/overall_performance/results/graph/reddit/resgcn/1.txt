Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INIT

Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 4 on machine gnerv8
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.360 seconds.
Building the CSC structure...
        It takes 2.452 seconds.
Building the CSC structure...
        It takes 2.597 seconds.
Building the CSC structure...
        It takes 2.641 seconds.
Building the CSC structure...
        It takes 2.909 seconds.
Building the CSC structure...
        It takes 2.909 seconds.
Building the CSC structure...
        It takes 2.909 seconds.
Building the CSC structure...
        It takes 2.908 seconds.
Building the CSC structure...
        It takes 2.344 seconds.
        It takes 2.357 seconds.
        It takes 2.235 seconds.
        It takes 2.378 seconds.
Building the Feature Vector...
        It takes 2.862 seconds.
        It takes 2.862 seconds.
        It takes 2.861 seconds.
        It takes 2.862 seconds.
Building the Feature Vector...
        It takes 0.270 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.035 seconds.
Building the Feature Vector...
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.309 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.794 seconds.
        It takes 0.667 seconds.
        It takes 0.687 seconds.
        It takes 0.356 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.077 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
        It takes 0.077 seconds.
        It takes 0.077 seconds.
        It takes 0.077 seconds.
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.907 Gbps (per GPU), 471.256 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.636 Gbps (per GPU), 469.090 Gbps (aggregated)
The layer-level communication performance: 58.636 Gbps (per GPU), 469.086 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.403 Gbps (per GPU), 467.224 Gbps (aggregated)
The layer-level communication performance: 58.382 Gbps (per GPU), 467.052 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.168 Gbps (per GPU), 465.347 Gbps (aggregated)
The layer-level communication performance: 58.129 Gbps (per GPU), 465.035 Gbps (aggregated)
The layer-level communication performance: 58.101 Gbps (per GPU), 464.807 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.818 Gbps (per GPU), 1270.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.821 Gbps (per GPU), 1270.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.863 Gbps (per GPU), 1270.905 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.821 Gbps (per GPU), 1270.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.818 Gbps (per GPU), 1270.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.809 Gbps (per GPU), 1270.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.848 Gbps (per GPU), 1270.785 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.776 Gbps (per GPU), 1270.207 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.590 Gbps (per GPU), 828.717 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.588 Gbps (per GPU), 828.702 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.582 Gbps (per GPU), 828.655 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.588 Gbps (per GPU), 828.702 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.590 Gbps (per GPU), 828.723 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.588 Gbps (per GPU), 828.702 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.586 Gbps (per GPU), 828.690 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.589 Gbps (per GPU), 828.709 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.662 Gbps (per GPU), 317.294 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.662 Gbps (per GPU), 317.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.662 Gbps (per GPU), 317.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.662 Gbps (per GPU), 317.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.661 Gbps (per GPU), 317.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.661 Gbps (per GPU), 317.287 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.659 Gbps (per GPU), 317.271 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.659 Gbps (per GPU), 317.276 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 11.94ms 10.55ms 10.89ms  1.13 29.12K 14.23M
 chk_1  7.48ms  6.18ms  6.49ms  1.21 29.12K  6.56M
 chk_2 19.14ms 17.80ms 18.10ms  1.08 29.12K 24.68M
 chk_3 19.26ms 18.06ms 18.15ms  1.07 29.12K 22.95M
 chk_4  7.37ms  6.04ms  6.32ms  1.22 29.12K  6.33M
 chk_5 11.36ms 10.04ms 10.30ms  1.13 29.12K 12.05M
 chk_6 12.51ms 11.17ms 11.49ms  1.12 29.12K 14.60M
 chk_7 11.66ms 10.26ms 10.68ms  1.14 29.12K 13.21M
   Avg 12.59 11.26 11.55
   Max 19.26 18.06 18.15
   Min  7.37  6.04  6.32
 Ratio  2.61  2.99  2.87
   Var 17.96 18.13 17.77
Profiling takes 3.219 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 4, starting model training...
*** Node 2, starting model training...
*** Node 6, starting model training...
*** Node 3, starting model training...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 230)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 230)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 230)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 230)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 230)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 230)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 230)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 230)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 230)...
+++++++++ Node 2 initializing the weights for op[0, 230)...
+++++++++ Node 6 initializing the weights for op[0, 230)...
+++++++++ Node 4 initializing the weights for op[0, 230)...
+++++++++ Node 5 initializing the weights for op[0, 230)...
+++++++++ Node 7 initializing the weights for op[0, 230)...
+++++++++ Node 1 initializing the weights for op[0, 230)...
+++++++++ Node 3 initializing the weights for op[0, 230)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 6.9954	TrainAcc 0.1076	ValidAcc 0.1010	TestAcc 0.1010	BestValid 0.1010
	Epoch 50:	Loss 0.3831	TrainAcc 0.9150	ValidAcc 0.9201	TestAcc 0.9170	BestValid 0.9201
	Epoch 100:	Loss 0.2645	TrainAcc 0.9414	ValidAcc 0.9417	TestAcc 0.9409	BestValid 0.9417
	Epoch 150:	Loss 0.2283	TrainAcc 0.9493	ValidAcc 0.9473	TestAcc 0.9464	BestValid 0.9473
	Epoch 200:	Loss 0.2082	TrainAcc 0.9538	ValidAcc 0.9498	TestAcc 0.9494	BestValid 0.9498
	Epoch 250:	Loss 0.1914	TrainAcc 0.9572	ValidAcc 0.9519	TestAcc 0.9516	BestValid 0.9519
	Epoch 300:	Loss 0.1800	TrainAcc 0.9597	ValidAcc 0.9530	TestAcc 0.9530	BestValid 0.9530
	Epoch 350:	Loss 0.1710	TrainAcc 0.9618	ValidAcc 0.9541	TestAcc 0.9538	BestValid 0.9541
	Epoch 400:	Loss 0.1627	TrainAcc 0.9637	ValidAcc 0.9552	TestAcc 0.9547	BestValid 0.9552
	Epoch 450:	Loss 0.1561	TrainAcc 0.9654	ValidAcc 0.9555	TestAcc 0.9552	BestValid 0.9555
	Epoch 500:	Loss 0.1499	TrainAcc 0.9668	ValidAcc 0.9560	TestAcc 0.9558	BestValid 0.9560
	Epoch 550:	Loss 0.1438	TrainAcc 0.9681	ValidAcc 0.9569	TestAcc 0.9563	BestValid 0.9569
	Epoch 600:	Loss 0.1404	TrainAcc 0.9691	ValidAcc 0.9570	TestAcc 0.9563	BestValid 0.9570
	Epoch 650:	Loss 0.1358	TrainAcc 0.9701	ValidAcc 0.9576	TestAcc 0.9567	BestValid 0.9576
	Epoch 700:	Loss 0.1308	TrainAcc 0.9709	ValidAcc 0.9580	TestAcc 0.9569	BestValid 0.9580
	Epoch 750:	Loss 0.1281	TrainAcc 0.9718	ValidAcc 0.9581	TestAcc 0.9572	BestValid 0.9581
	Epoch 800:	Loss 0.1234	TrainAcc 0.9726	ValidAcc 0.9584	TestAcc 0.9573	BestValid 0.9584
	Epoch 850:	Loss 0.1205	TrainAcc 0.9734	ValidAcc 0.9583	TestAcc 0.9574	BestValid 0.9584
	Epoch 900:	Loss 0.1162	TrainAcc 0.9744	ValidAcc 0.9587	TestAcc 0.9576	BestValid 0.9587
	Epoch 950:	Loss 0.1140	TrainAcc 0.9750	ValidAcc 0.9583	TestAcc 0.9578	BestValid 0.9587
	Epoch 1000:	Loss 0.1107	TrainAcc 0.9757	ValidAcc 0.9586	TestAcc 0.9580	BestValid 0.9587
	Epoch 1050:	Loss 0.1086	TrainAcc 0.9764	ValidAcc 0.9586	TestAcc 0.9580	BestValid 0.9587
	Epoch 1100:	Loss 0.1054	TrainAcc 0.9769	ValidAcc 0.9590	TestAcc 0.9580	BestValid 0.9590
	Epoch 1150:	Loss 0.1029	TrainAcc 0.9774	ValidAcc 0.9592	TestAcc 0.9582	BestValid 0.9592
	Epoch 1200:	Loss 0.1009	TrainAcc 0.9779	ValidAcc 0.9592	TestAcc 0.9582	BestValid 0.9592
	Epoch 1250:	Loss 0.0988	TrainAcc 0.9786	ValidAcc 0.9587	TestAcc 0.9582	BestValid 0.9592
	Epoch 1300:	Loss 0.0959	TrainAcc 0.9789	ValidAcc 0.9587	TestAcc 0.9582	BestValid 0.9592
	Epoch 1350:	Loss 0.0957	TrainAcc 0.9795	ValidAcc 0.9588	TestAcc 0.9583	BestValid 0.9592
	Epoch 1400:	Loss 0.0917	TrainAcc 0.9799	ValidAcc 0.9586	TestAcc 0.9579	BestValid 0.9592
	Epoch 1450:	Loss 0.0895	TrainAcc 0.9803	ValidAcc 0.9590	TestAcc 0.9582	BestValid 0.9592
	Epoch 1500:	Loss 0.0885	TrainAcc 0.9806	ValidAcc 0.9585	TestAcc 0.9579	BestValid 0.9592
	Epoch 1550:	Loss 0.0869	TrainAcc 0.9812	ValidAcc 0.9582	TestAcc 0.9580	BestValid 0.9592
	Epoch 1600:	Loss 0.0847	TrainAcc 0.9817	ValidAcc 0.9581	TestAcc 0.9582	BestValid 0.9592
	Epoch 1650:	Loss 0.0830	TrainAcc 0.9820	ValidAcc 0.9581	TestAcc 0.9581	BestValid 0.9592
	Epoch 1700:	Loss 0.0815	TrainAcc 0.9823	ValidAcc 0.9582	TestAcc 0.9581	BestValid 0.9592
	Epoch 1750:	Loss 0.0798	TrainAcc 0.9828	ValidAcc 0.9583	TestAcc 0.9581	BestValid 0.9592
	Epoch 1800:	Loss 0.0787	TrainAcc 0.9828	ValidAcc 0.9573	TestAcc 0.9574	BestValid 0.9592
	Epoch 1850:	Loss 0.0773	TrainAcc 0.9837	ValidAcc 0.9578	TestAcc 0.9580	BestValid 0.9592
	Epoch 1900:	Loss 0.0761	TrainAcc 0.9837	ValidAcc 0.9577	TestAcc 0.9577	BestValid 0.9592
	Epoch 1950:	Loss 0.0738	TrainAcc 0.9844	ValidAcc 0.9580	TestAcc 0.9577	BestValid 0.9592
	Epoch 2000:	Loss 0.0724	TrainAcc 0.9847	ValidAcc 0.9573	TestAcc 0.9575	BestValid 0.9592
	Epoch 2050:	Loss 0.0721	TrainAcc 0.9850	ValidAcc 0.9577	TestAcc 0.9573	BestValid 0.9592
	Epoch 2100:	Loss 0.0699	TrainAcc 0.9853	ValidAcc 0.9582	TestAcc 0.9574	BestValid 0.9592
	Epoch 2150:	Loss 0.0685	TrainAcc 0.9857	ValidAcc 0.9571	TestAcc 0.9574	BestValid 0.9592
	Epoch 2200:	Loss 0.0680	TrainAcc 0.9860	ValidAcc 0.9575	TestAcc 0.9573	BestValid 0.9592
	Epoch 2250:	Loss 0.0665	TrainAcc 0.9863	ValidAcc 0.9575	TestAcc 0.9574	BestValid 0.9592
slurmstepd-gnerv4: error: *** STEP 3721.8 ON gnerv4 CANCELLED AT 2023-09-18T22:28:54 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
