Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
DONE MPI INIT

Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
DONE MPI INITDONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2

Initialized node 0 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.286 seconds.
Building the CSC structure...
        It takes 0.289 seconds.
Building the CSC structure...
        It takes 0.293 seconds.
Building the CSC structure...
        It takes 0.297 seconds.
Building the CSC structure...
        It takes 0.301 seconds.
Building the CSC structure...
        It takes 0.313 seconds.
Building the CSC structure...
        It takes 0.313 seconds.
Building the CSC structure...
        It takes 0.324 seconds.
Building the CSC structure...
        It takes 0.275 seconds.
        It takes 0.281 seconds.
        It takes 0.282 seconds.
        It takes 0.291 seconds.
        It takes 0.284 seconds.
        It takes 0.299 seconds.
        It takes 0.315 seconds.
        It takes 0.319 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.475 seconds.
Building the Label Vector...
        It takes 0.470 seconds.
Building the Label Vector...
        It takes 0.486 seconds.
Building the Label Vector...
        It takes 0.496 seconds.
Building the Label Vector...
        It takes 0.490 seconds.
Building the Label Vector...
        It takes 0.482 seconds.
Building the Label Vector...
        It takes 0.503 seconds.
Building the Label Vector...
        It takes 0.491 seconds.
Building the Label Vector...
        It takes 0.184 seconds.
        It takes 0.188 seconds.
        It takes 0.193 seconds.
        It takes 0.192 seconds.
        It takes 0.185 seconds.
        It takes 0.179 seconds.
        It takes 0.189 seconds.
        It takes 0.186 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/yelp/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 100
Number of feature dimensions: 300
Number of vertices: 716847
Number of GPUs: 8
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 89606
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 89606
716847, 13954819, 13954819
Number of vertices per chunk: 89606
train nodes 537635, valid nodes 107527, test nodes 71685
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 89605) 1-[89605, 179211) 2-[179211, 268818) 3-[268818, 358423) 4-[358423, 448029) 5-[448029, 537635) 6-[537635, 627242) 7-[627242, 716847)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 89606
716847, 13954819, 13954819
Number of vertices per chunk: 89606
716847, 13954819, 13954819
Number of vertices per chunk: 89606
716847, 13954819, 13954819
716847, 13954819, 13954819
Number of vertices per chunk: 89606
Number of vertices per chunk: 89606
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 58.879 Gbps (per GPU), 471.032 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.584 Gbps (per GPU), 468.671 Gbps (aggregated)
The layer-level communication performance: 58.575 Gbps (per GPU), 468.603 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.339 Gbps (per GPU), 466.715 Gbps (aggregated)
The layer-level communication performance: 58.304 Gbps (per GPU), 466.432 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.111 Gbps (per GPU), 464.887 Gbps (aggregated)
The layer-level communication performance: 58.066 Gbps (per GPU), 464.528 Gbps (aggregated)
The layer-level communication performance: 58.035 Gbps (per GPU), 464.284 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.983 Gbps (per GPU), 1247.864 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.986 Gbps (per GPU), 1247.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.006 Gbps (per GPU), 1248.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.971 Gbps (per GPU), 1247.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.893 Gbps (per GPU), 1247.145 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.971 Gbps (per GPU), 1247.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.968 Gbps (per GPU), 1247.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.968 Gbps (per GPU), 1247.748 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.912 Gbps (per GPU), 807.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.912 Gbps (per GPU), 807.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.911 Gbps (per GPU), 807.289 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.913 Gbps (per GPU), 807.302 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.912 Gbps (per GPU), 807.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.912 Gbps (per GPU), 807.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.912 Gbps (per GPU), 807.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.913 Gbps (per GPU), 807.302 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.378 Gbps (per GPU), 275.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.378 Gbps (per GPU), 275.021 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.378 Gbps (per GPU), 275.024 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.377 Gbps (per GPU), 275.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.377 Gbps (per GPU), 275.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.377 Gbps (per GPU), 275.016 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.378 Gbps (per GPU), 275.023 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.378 Gbps (per GPU), 275.020 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.86ms  3.22ms  2.37ms  1.63 89.61K  0.73M
 chk_1  3.79ms  3.15ms  2.28ms  1.66 89.61K  0.74M
 chk_2  6.47ms  5.84ms  4.95ms  1.31 89.61K  3.52M
 chk_3  5.88ms  5.23ms  4.35ms  1.35 89.61K  2.76M
 chk_4  5.09ms  4.41ms  3.54ms  1.44 89.61K  1.50M
 chk_5  4.36ms  3.70ms  2.83ms  1.54 89.61K  1.19M
 chk_6  5.99ms  5.31ms  4.45ms  1.35 89.61K  2.34M
 chk_7  3.59ms  2.93ms  2.07ms  1.74 89.61K  0.47M
   Avg  4.88  4.22  3.36
   Max  6.47  5.84  4.95
   Min  3.59  2.93  2.07
 Ratio  1.80  1.99  2.39
   Var  1.12  1.12  1.10
Profiling takes 1.221 s
*** Node 0, starting model training...
*** Node 2, starting model training...
*** Node 4, starting model training...
*** Node 3, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 159)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 537635, Num Local Vertices: 89607
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 159)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 358423, Num Local Vertices: 89606
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 159)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 627242, Num Local Vertices: 89605
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 159)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 448029, Num Local Vertices: 89606
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 159)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89605
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 159)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 179211, Num Local Vertices: 89607
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 159)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 268818, Num Local Vertices: 89605
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 159)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 89605, Num Local Vertices: 89606
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 159)...
+++++++++ Node 5 initializing the weights for op[0, 159)...
+++++++++ Node 3 initializing the weights for op[0, 159)...
+++++++++ Node 7 initializing the weights for op[0, 159)...
+++++++++ Node 0 initializing the weights for op[0, 159)...
+++++++++ Node 6 initializing the weights for op[0, 159)...
+++++++++ Node 4 initializing the weights for op[0, 159)...
+++++++++ Node 2 initializing the weights for op[0, 159)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 898218
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 0.6931	TrainAcc 0.1780	ValidAcc 0.1771	TestAcc 0.1771	BestValid 0.1771
	Epoch 50:	Loss 0.3683	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.1771
	Epoch 100:	Loss 0.3291	TrainAcc 0.3215	ValidAcc 0.3212	TestAcc 0.3210	BestValid 0.3212
	Epoch 150:	Loss 0.3180	TrainAcc 0.3174	ValidAcc 0.3171	TestAcc 0.3167	BestValid 0.3212
	Epoch 200:	Loss 0.3116	TrainAcc 0.3313	ValidAcc 0.3312	TestAcc 0.3306	BestValid 0.3312
	Epoch 250:	Loss 0.3086	TrainAcc 0.3106	ValidAcc 0.3102	TestAcc 0.3101	BestValid 0.3312
	Epoch 300:	Loss 0.3054	TrainAcc 0.3006	ValidAcc 0.3000	TestAcc 0.2997	BestValid 0.3312
	Epoch 350:	Loss 0.3059	TrainAcc 0.3019	ValidAcc 0.3013	TestAcc 0.3012	BestValid 0.3312
	Epoch 400:	Loss 0.3034	TrainAcc 0.2889	ValidAcc 0.2881	TestAcc 0.2880	BestValid 0.3312
	Epoch 450:	Loss 0.3015	TrainAcc 0.2916	ValidAcc 0.2908	TestAcc 0.2908	BestValid 0.3312
	Epoch 500:	Loss 0.3011	TrainAcc 0.2880	ValidAcc 0.2871	TestAcc 0.2871	BestValid 0.3312
	Epoch 550:	Loss 0.3020	TrainAcc 0.2839	ValidAcc 0.2830	TestAcc 0.2830	BestValid 0.3312
	Epoch 600:	Loss 0.3069	TrainAcc 0.2838	ValidAcc 0.2828	TestAcc 0.2828	BestValid 0.3312
	Epoch 650:	Loss 0.3013	TrainAcc 0.2857	ValidAcc 0.2847	TestAcc 0.2846	BestValid 0.3312
	Epoch 700:	Loss 0.2980	TrainAcc 0.2935	ValidAcc 0.2927	TestAcc 0.2926	BestValid 0.3312
	Epoch 750:	Loss 0.3014	TrainAcc 0.2918	ValidAcc 0.2910	TestAcc 0.2908	BestValid 0.3312
	Epoch 800:	Loss 0.2928	TrainAcc 0.2941	ValidAcc 0.2933	TestAcc 0.2932	BestValid 0.3312
	Epoch 850:	Loss 0.2920	TrainAcc 0.2860	ValidAcc 0.2851	TestAcc 0.2851	BestValid 0.3312
	Epoch 900:	Loss 0.2905	TrainAcc 0.2935	ValidAcc 0.2927	TestAcc 0.2926	BestValid 0.3312
	Epoch 950:	Loss 0.2896	TrainAcc 0.3136	ValidAcc 0.3131	TestAcc 0.3128	BestValid 0.3312
	Epoch 1000:	Loss 0.2908	TrainAcc 0.3316	ValidAcc 0.3314	TestAcc 0.3307	BestValid 0.3314
	Epoch 1050:	Loss 0.3021	TrainAcc 0.2658	ValidAcc 0.2649	TestAcc 0.2647	BestValid 0.3314
	Epoch 1100:	Loss 0.2927	TrainAcc 0.2497	ValidAcc 0.2487	TestAcc 0.2487	BestValid 0.3314
	Epoch 1150:	Loss 0.2878	TrainAcc 0.2785	ValidAcc 0.2775	TestAcc 0.2773	BestValid 0.3314
	Epoch 1200:	Loss 0.2927	TrainAcc 0.2527	ValidAcc 0.2517	TestAcc 0.2517	BestValid 0.3314
	Epoch 1250:	Loss 0.2885	TrainAcc 0.2590	ValidAcc 0.2580	TestAcc 0.2579	BestValid 0.3314
	Epoch 1300:	Loss 0.2883	TrainAcc 0.2912	ValidAcc 0.2915	TestAcc 0.2911	BestValid 0.3314
	Epoch 1350:	Loss 0.2969	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 1400:	Loss 0.2885	TrainAcc 0.3050	ValidAcc 0.3042	TestAcc 0.3042	BestValid 0.3314
	Epoch 1450:	Loss 0.2879	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 1500:	Loss 0.2863	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 1550:	Loss 0.2851	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 1600:	Loss 0.2847	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 1650:	Loss 0.3012	TrainAcc 0.2408	ValidAcc 0.2396	TestAcc 0.2398	BestValid 0.3314
	Epoch 1700:	Loss 0.2995	TrainAcc 0.2874	ValidAcc 0.2864	TestAcc 0.2865	BestValid 0.3314
	Epoch 1750:	Loss 0.2947	TrainAcc 0.3272	ValidAcc 0.3271	TestAcc 0.3266	BestValid 0.3314
	Epoch 1800:	Loss 0.2919	TrainAcc 0.3272	ValidAcc 0.3271	TestAcc 0.3266	BestValid 0.3314
	Epoch 1850:	Loss 0.2905	TrainAcc 0.2848	ValidAcc 0.2849	TestAcc 0.2846	BestValid 0.3314
	Epoch 1900:	Loss 0.2896	TrainAcc 0.2848	ValidAcc 0.2849	TestAcc 0.2846	BestValid 0.3314
	Epoch 1950:	Loss 0.2880	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2000:	Loss 0.2894	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2050:	Loss 0.2884	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2100:	Loss 0.2859	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2150:	Loss 0.2858	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2200:	Loss 0.2862	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2250:	Loss 0.2836	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2300:	Loss 0.2864	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2350:	Loss 0.2853	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2400:	Loss 0.2836	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2450:	Loss 0.2821	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2500:	Loss 0.3117	TrainAcc 0.2345	ValidAcc 0.2352	TestAcc 0.2347	BestValid 0.3314
	Epoch 2550:	Loss 0.2988	TrainAcc 0.2629	ValidAcc 0.2630	TestAcc 0.2624	BestValid 0.3314
	Epoch 2600:	Loss 0.2946	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 2650:	Loss 0.2922	TrainAcc 0.1643	ValidAcc 0.1649	TestAcc 0.1647	BestValid 0.3314
	Epoch 2700:	Loss 0.2923	TrainAcc 0.1502	ValidAcc 0.1509	TestAcc 0.1506	BestValid 0.3314
	Epoch 2750:	Loss 0.2887	TrainAcc 0.1643	ValidAcc 0.1649	TestAcc 0.1647	BestValid 0.3314
	Epoch 2800:	Loss 0.2960	TrainAcc 0.1877	ValidAcc 0.1882	TestAcc 0.1877	BestValid 0.3314
	Epoch 2850:	Loss 0.2862	TrainAcc 0.2788	ValidAcc 0.2792	TestAcc 0.2786	BestValid 0.3314
	Epoch 2900:	Loss 0.2846	TrainAcc 0.1993	ValidAcc 0.2002	TestAcc 0.1995	BestValid 0.3314
	Epoch 2950:	Loss 0.2850	TrainAcc 0.1502	ValidAcc 0.1509	TestAcc 0.1506	BestValid 0.3314
	Epoch 3000:	Loss 0.2911	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3050:	Loss 0.2909	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3100:	Loss 0.2900	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3150:	Loss 0.2871	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3200:	Loss 0.2828	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3250:	Loss 0.2864	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3300:	Loss 0.2818	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3350:	Loss 0.2821	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3400:	Loss 0.2817	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3450:	Loss 0.2845	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3500:	Loss 0.2816	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3550:	Loss 0.3048	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3600:	Loss 0.2927	TrainAcc 0.1992	ValidAcc 0.2001	TestAcc 0.1994	BestValid 0.3314
	Epoch 3650:	Loss 0.2932	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3700:	Loss 0.2942	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3750:	Loss 0.2900	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3800:	Loss 0.2890	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3850:	Loss 0.2871	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3900:	Loss 0.3008	TrainAcc 0.1416	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 3950:	Loss 0.2923	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4000:	Loss 0.2860	TrainAcc 0.1416	ValidAcc 0.1423	TestAcc 0.1419	BestValid 0.3314
	Epoch 4050:	Loss 0.2826	TrainAcc 0.1417	ValidAcc 0.1424	TestAcc 0.1420	BestValid 0.3314
	Epoch 4100:	Loss 0.2830	TrainAcc 0.1416	ValidAcc 0.1423	TestAcc 0.1420	BestValid 0.3314
	Epoch 4150:	Loss 0.2827	TrainAcc 0.1417	ValidAcc 0.1424	TestAcc 0.1420	BestValid 0.3314
	Epoch 4200:	Loss 0.2823	TrainAcc 0.1417	ValidAcc 0.1424	TestAcc 0.1421	BestValid 0.3314
	Epoch 4250:	Loss 0.2809	TrainAcc 0.1417	ValidAcc 0.1424	TestAcc 0.1421	BestValid 0.3314
	Epoch 4300:	Loss 0.2811	TrainAcc 0.1417	ValidAcc 0.1424	TestAcc 0.1421	BestValid 0.3314
	Epoch 4350:	Loss 0.2809	TrainAcc 0.1417	ValidAcc 0.1424	TestAcc 0.1421	BestValid 0.3314
	Epoch 4400:	Loss 0.2807	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4450:	Loss 0.2804	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4500:	Loss 0.2828	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4550:	Loss 0.2803	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4600:	Loss 0.2803	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4650:	Loss 0.2801	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4700:	Loss 0.2804	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4750:	Loss 0.2797	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4800:	Loss 0.2794	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4850:	Loss 0.2796	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4900:	Loss 0.2809	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
	Epoch 4950:	Loss 0.2926	TrainAcc 0.3272	ValidAcc 0.3270	TestAcc 0.3266	BestValid 0.3314
	Epoch 5000:	Loss 0.2885	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3314
****** Epoch Time (Excluding Evaluation Cost): 0.745 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 1.366 ms (Max: 2.239, Min: 0.036, Sum: 10.924)
Cluster-Wide Average, Compute: 118.521 ms (Max: 151.967, Min: 90.820, Sum: 948.168)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.065)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.013, Sum: 0.121)
Cluster-Wide Average, Communication-Graph: 621.827 ms (Max: 649.000, Min: 589.642, Sum: 4974.612)
Cluster-Wide Average, Optimization: 2.661 ms (Max: 2.675, Min: 2.647, Sum: 21.286)
Cluster-Wide Average, Others: 0.772 ms (Max: 0.798, Min: 0.734, Sum: 6.176)
****** Breakdown Sum: 745.169 ms ******
Cluster-Wide Average, GPU Memory Consumption: 12.851 GB (Max: 13.542, Min: 12.737, Sum: 102.808)
Cluster-Wide Average, Graph-Level Communication Throughput: 39.057 Gbps (Max: 52.590, Min: 25.036, Sum: 312.458)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 21.415 GB
Weight-sync communication (cluster-wide, per-epoch): 0.018 GB
Total communication (cluster-wide, per-epoch): 21.433 GB
****** Accuracy Results ******
Highest valid_acc: 0.3314
Target test_acc: 0.3307
Epoch to reach the target acc: 999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
