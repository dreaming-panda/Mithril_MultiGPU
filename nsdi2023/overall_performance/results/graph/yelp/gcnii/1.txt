Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITInitialized node 0 on machine gnerv2

Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.226 seconds.
Building the CSC structure...
        It takes 0.245 seconds.
Building the CSC structure...
        It takes 0.277 seconds.
Building the CSC structure...
        It takes 0.297 seconds.
Building the CSC structure...
        It takes 0.295 seconds.
Building the CSC structure...
        It takes 0.299 seconds.
Building the CSC structure...
        It takes 0.314 seconds.
Building the CSC structure...
        It takes 0.321 seconds.
Building the CSC structure...
        It takes 0.217 seconds.
        It takes 0.240 seconds.
        It takes 0.271 seconds.
Building the Feature Vector...
        It takes 0.276 seconds.
        It takes 0.286 seconds.
        It takes 0.304 seconds.
        It takes 0.317 seconds.
        It takes 0.314 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.460 seconds.
Building the Label Vector...
        It takes 0.480 seconds.
Building the Label Vector...
        It takes 0.497 seconds.
Building the Label Vector...
        It takes 0.168 seconds.
        It takes 0.479 seconds.
Building the Label Vector...
        It takes 0.496 seconds.
Building the Label Vector...
        It takes 0.496 seconds.
Building the Label Vector...
        It takes 0.499 seconds.
Building the Label Vector...
        It takes 0.506 seconds.
Building the Label Vector...
        It takes 0.187 seconds.
        It takes 0.192 seconds.
        It takes 0.190 seconds.
        It takes 0.190 seconds.
        It takes 0.188 seconds.
        It takes 0.196 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/yelp/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 100
Number of feature dimensions: 300
Number of vertices: 716847
Number of GPUs: 8
        It takes 0.189 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 89606
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 89606
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 89606
716847, 13954819, 13954819
Number of vertices per chunk: 89606
716847, 13954819, 13954819
Number of vertices per chunk: 89606
train nodes 537635, valid nodes 107527, test nodes 71685
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 89605) 1-[89605, 179211) 2-[179211, 268818) 3-[268818, 358423) 4-[358423, 448029) 5-[448029, 537635) 6-[537635, 627242) 7-[627242, 716847)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 89606
716847, 13954819, 13954819
Number of vertices per chunk: 89606
716847, 13954819, 13954819
Number of vertices per chunk: 89606
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.606 Gbps (per GPU), 452.852 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.357 Gbps (per GPU), 450.856 Gbps (aggregated)
The layer-level communication performance: 56.348 Gbps (per GPU), 450.786 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.141 Gbps (per GPU), 449.128 Gbps (aggregated)
The layer-level communication performance: 56.110 Gbps (per GPU), 448.880 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.932 Gbps (per GPU), 447.453 Gbps (aggregated)
The layer-level communication performance: 55.886 Gbps (per GPU), 447.087 Gbps (aggregated)
The layer-level communication performance: 55.858 Gbps (per GPU), 446.862 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.265 Gbps (per GPU), 1250.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.256 Gbps (per GPU), 1250.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.227 Gbps (per GPU), 1249.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.259 Gbps (per GPU), 1250.072 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.227 Gbps (per GPU), 1249.814 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.262 Gbps (per GPU), 1250.095 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.178 Gbps (per GPU), 1249.420 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.256 Gbps (per GPU), 1250.049 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.523 Gbps (per GPU), 812.187 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.514 Gbps (per GPU), 812.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.522 Gbps (per GPU), 812.174 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.523 Gbps (per GPU), 812.187 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.519 Gbps (per GPU), 812.154 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.520 Gbps (per GPU), 812.161 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.523 Gbps (per GPU), 812.181 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.523 Gbps (per GPU), 812.180 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.052 Gbps (per GPU), 280.413 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.051 Gbps (per GPU), 280.407 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.052 Gbps (per GPU), 280.415 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.051 Gbps (per GPU), 280.411 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.051 Gbps (per GPU), 280.411 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.051 Gbps (per GPU), 280.405 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.049 Gbps (per GPU), 280.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.050 Gbps (per GPU), 280.397 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  4.11ms  4.98ms  5.79ms  1.41 89.61K  0.73M
 chk_1  4.13ms  4.90ms  5.71ms  1.38 89.61K  0.74M
 chk_2  4.14ms  7.57ms  8.38ms  2.03 89.61K  3.52M
 chk_3  4.13ms  7.00ms  7.83ms  1.89 89.61K  2.76M
 chk_4  4.14ms  6.17ms  6.98ms  1.69 89.61K  1.50M
 chk_5  4.14ms  5.45ms  6.28ms  1.52 89.61K  1.19M
 chk_6  4.14ms  7.08ms  7.89ms  1.91 89.61K  2.34M
 chk_7  4.14ms  4.70ms  5.94ms  1.43 89.61K  0.47M
   Avg  4.13  5.98  6.85
   Max  4.14  7.57  8.38
   Min  4.11  4.70  5.71
 Ratio  1.01  1.61  1.47
   Var  0.00  1.11  1.00
Profiling takes 1.651 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 232)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89605
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 232)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 179211, Num Local Vertices: 89607
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 232)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 89605, Num Local Vertices: 89606
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 232)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 448029, Num Local Vertices: 89606
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 232)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 537635, Num Local Vertices: 89607
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 232)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 268818, Num Local Vertices: 89605
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 232)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 358423, Num Local Vertices: 89606
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 232)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 627242, Num Local Vertices: 89605
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 232)...
+++++++++ Node 4 initializing the weights for op[0, 232)...
+++++++++ Node 0 initializing the weights for op[0, 232)...
+++++++++ Node 5 initializing the weights for op[0, 232)...
+++++++++ Node 6 initializing the weights for op[0, 232)...
+++++++++ Node 7 initializing the weights for op[0, 232)...
+++++++++ Node 3 initializing the weights for op[0, 232)...
+++++++++ Node 2 initializing the weights for op[0, 232)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 898218
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 0.7895	TrainAcc 0.1689	ValidAcc 0.1679	TestAcc 0.1680	BestValid 0.1679
	Epoch 50:	Loss 0.3405	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.1679
	Epoch 100:	Loss 0.3191	TrainAcc 0.1416	ValidAcc 0.1423	TestAcc 0.1420	BestValid 0.1679
	Epoch 150:	Loss 0.3009	TrainAcc 0.1428	ValidAcc 0.1434	TestAcc 0.1432	BestValid 0.1679
	Epoch 200:	Loss 0.2813	TrainAcc 0.1489	ValidAcc 0.1495	TestAcc 0.1494	BestValid 0.1679
	Epoch 250:	Loss 0.2736	TrainAcc 0.1533	ValidAcc 0.1537	TestAcc 0.1537	BestValid 0.1679
	Epoch 300:	Loss 0.2688	TrainAcc 0.1564	ValidAcc 0.1569	TestAcc 0.1570	BestValid 0.1679
	Epoch 350:	Loss 0.2655	TrainAcc 0.1630	ValidAcc 0.1636	TestAcc 0.1638	BestValid 0.1679
	Epoch 400:	Loss 0.2628	TrainAcc 0.1717	ValidAcc 0.1724	TestAcc 0.1726	BestValid 0.1724
	Epoch 450:	Loss 0.2607	TrainAcc 0.1770	ValidAcc 0.1778	TestAcc 0.1782	BestValid 0.1778
	Epoch 500:	Loss 0.2590	TrainAcc 0.1856	ValidAcc 0.1864	TestAcc 0.1866	BestValid 0.1864
	Epoch 550:	Loss 0.2572	TrainAcc 0.1903	ValidAcc 0.1910	TestAcc 0.1913	BestValid 0.1910
	Epoch 600:	Loss 0.2559	TrainAcc 0.1989	ValidAcc 0.1995	TestAcc 0.2000	BestValid 0.1995
	Epoch 650:	Loss 0.2546	TrainAcc 0.2042	ValidAcc 0.2046	TestAcc 0.2054	BestValid 0.2046
	Epoch 700:	Loss 0.2535	TrainAcc 0.2100	ValidAcc 0.2103	TestAcc 0.2113	BestValid 0.2103
	Epoch 750:	Loss 0.2523	TrainAcc 0.2116	ValidAcc 0.2118	TestAcc 0.2129	BestValid 0.2118
	Epoch 800:	Loss 0.2511	TrainAcc 0.2179	ValidAcc 0.2183	TestAcc 0.2194	BestValid 0.2183
	Epoch 850:	Loss 0.2503	TrainAcc 0.2225	ValidAcc 0.2227	TestAcc 0.2240	BestValid 0.2227
	Epoch 900:	Loss 0.2491	TrainAcc 0.2279	ValidAcc 0.2279	TestAcc 0.2292	BestValid 0.2279
	Epoch 950:	Loss 0.2481	TrainAcc 0.2348	ValidAcc 0.2347	TestAcc 0.2359	BestValid 0.2347
	Epoch 1000:	Loss 0.2471	TrainAcc 0.2401	ValidAcc 0.2401	TestAcc 0.2410	BestValid 0.2401
	Epoch 1050:	Loss 0.2464	TrainAcc 0.2459	ValidAcc 0.2458	TestAcc 0.2468	BestValid 0.2458
	Epoch 1100:	Loss 0.2453	TrainAcc 0.2481	ValidAcc 0.2480	TestAcc 0.2489	BestValid 0.2480
	Epoch 1150:	Loss 0.2442	TrainAcc 0.2555	ValidAcc 0.2553	TestAcc 0.2565	BestValid 0.2553
	Epoch 1200:	Loss 0.2434	TrainAcc 0.2592	ValidAcc 0.2588	TestAcc 0.2602	BestValid 0.2588
	Epoch 1250:	Loss 0.2426	TrainAcc 0.2663	ValidAcc 0.2659	TestAcc 0.2672	BestValid 0.2659
	Epoch 1300:	Loss 0.2417	TrainAcc 0.2727	ValidAcc 0.2722	TestAcc 0.2735	BestValid 0.2722
	Epoch 1350:	Loss 0.2409	TrainAcc 0.2758	ValidAcc 0.2752	TestAcc 0.2766	BestValid 0.2752
	Epoch 1400:	Loss 0.2401	TrainAcc 0.2786	ValidAcc 0.2782	TestAcc 0.2794	BestValid 0.2782
	Epoch 1450:	Loss 0.2394	TrainAcc 0.2917	ValidAcc 0.2913	TestAcc 0.2924	BestValid 0.2913
	Epoch 1500:	Loss 0.2387	TrainAcc 0.2914	ValidAcc 0.2911	TestAcc 0.2922	BestValid 0.2913
	Epoch 1550:	Loss 0.2380	TrainAcc 0.2935	ValidAcc 0.2932	TestAcc 0.2942	BestValid 0.2932
	Epoch 1600:	Loss 0.2372	TrainAcc 0.3026	ValidAcc 0.3022	TestAcc 0.3031	BestValid 0.3022
	Epoch 1650:	Loss 0.2367	TrainAcc 0.3099	ValidAcc 0.3096	TestAcc 0.3102	BestValid 0.3096
	Epoch 1700:	Loss 0.2361	TrainAcc 0.3116	ValidAcc 0.3114	TestAcc 0.3120	BestValid 0.3114
	Epoch 1750:	Loss 0.2353	TrainAcc 0.3158	ValidAcc 0.3155	TestAcc 0.3164	BestValid 0.3155
	Epoch 1800:	Loss 0.2348	TrainAcc 0.3226	ValidAcc 0.3222	TestAcc 0.3230	BestValid 0.3222
	Epoch 1850:	Loss 0.2343	TrainAcc 0.3247	ValidAcc 0.3244	TestAcc 0.3252	BestValid 0.3244
	Epoch 1900:	Loss 0.2335	TrainAcc 0.3251	ValidAcc 0.3248	TestAcc 0.3254	BestValid 0.3248
	Epoch 1950:	Loss 0.2330	TrainAcc 0.3349	ValidAcc 0.3347	TestAcc 0.3356	BestValid 0.3347
	Epoch 2000:	Loss 0.2326	TrainAcc 0.3341	ValidAcc 0.3339	TestAcc 0.3349	BestValid 0.3347
	Epoch 2050:	Loss 0.2321	TrainAcc 0.3399	ValidAcc 0.3398	TestAcc 0.3408	BestValid 0.3398
	Epoch 2100:	Loss 0.2316	TrainAcc 0.3411	ValidAcc 0.3410	TestAcc 0.3420	BestValid 0.3410
	Epoch 2150:	Loss 0.2309	TrainAcc 0.3458	ValidAcc 0.3456	TestAcc 0.3467	BestValid 0.3456
	Epoch 2200:	Loss 0.2305	TrainAcc 0.3515	ValidAcc 0.3513	TestAcc 0.3524	BestValid 0.3513
	Epoch 2250:	Loss 0.2300	TrainAcc 0.3577	ValidAcc 0.3577	TestAcc 0.3588	BestValid 0.3577
	Epoch 2300:	Loss 0.2297	TrainAcc 0.3546	ValidAcc 0.3545	TestAcc 0.3556	BestValid 0.3577
	Epoch 2350:	Loss 0.2292	TrainAcc 0.3611	ValidAcc 0.3610	TestAcc 0.3621	BestValid 0.3610
	Epoch 2400:	Loss 0.2289	TrainAcc 0.3607	ValidAcc 0.3606	TestAcc 0.3617	BestValid 0.3610
	Epoch 2450:	Loss 0.2285	TrainAcc 0.3666	ValidAcc 0.3664	TestAcc 0.3675	BestValid 0.3664
	Epoch 2500:	Loss 0.2280	TrainAcc 0.3692	ValidAcc 0.3689	TestAcc 0.3702	BestValid 0.3689
	Epoch 2550:	Loss 0.2279	TrainAcc 0.3719	ValidAcc 0.3717	TestAcc 0.3728	BestValid 0.3717
	Epoch 2600:	Loss 0.2274	TrainAcc 0.3737	ValidAcc 0.3735	TestAcc 0.3746	BestValid 0.3735
	Epoch 2650:	Loss 0.2271	TrainAcc 0.3759	ValidAcc 0.3756	TestAcc 0.3767	BestValid 0.3756
	Epoch 2700:	Loss 0.2267	TrainAcc 0.3758	ValidAcc 0.3756	TestAcc 0.3765	BestValid 0.3756
	Epoch 2750:	Loss 0.2263	TrainAcc 0.3797	ValidAcc 0.3795	TestAcc 0.3805	BestValid 0.3795
	Epoch 2800:	Loss 0.2261	TrainAcc 0.3857	ValidAcc 0.3853	TestAcc 0.3864	BestValid 0.3853
	Epoch 2850:	Loss 0.2256	TrainAcc 0.3836	ValidAcc 0.3832	TestAcc 0.3843	BestValid 0.3853
	Epoch 2900:	Loss 0.2254	TrainAcc 0.3839	ValidAcc 0.3834	TestAcc 0.3844	BestValid 0.3853
	Epoch 2950:	Loss 0.2253	TrainAcc 0.3905	ValidAcc 0.3900	TestAcc 0.3912	BestValid 0.3900
	Epoch 3000:	Loss 0.2249	TrainAcc 0.3921	ValidAcc 0.3916	TestAcc 0.3926	BestValid 0.3916
	Epoch 3050:	Loss 0.2247	TrainAcc 0.3961	ValidAcc 0.3954	TestAcc 0.3965	BestValid 0.3954
	Epoch 3100:	Loss 0.2242	TrainAcc 0.3956	ValidAcc 0.3949	TestAcc 0.3960	BestValid 0.3954
	Epoch 3150:	Loss 0.2238	TrainAcc 0.3988	ValidAcc 0.3982	TestAcc 0.3992	BestValid 0.3982
	Epoch 3200:	Loss 0.2235	TrainAcc 0.3990	ValidAcc 0.3985	TestAcc 0.3994	BestValid 0.3985
	Epoch 3250:	Loss 0.2234	TrainAcc 0.4039	ValidAcc 0.4033	TestAcc 0.4043	BestValid 0.4033
	Epoch 3300:	Loss 0.2231	TrainAcc 0.4039	ValidAcc 0.4035	TestAcc 0.4044	BestValid 0.4035
	Epoch 3350:	Loss 0.2226	TrainAcc 0.4077	ValidAcc 0.4070	TestAcc 0.4080	BestValid 0.4070
	Epoch 3400:	Loss 0.2224	TrainAcc 0.4137	ValidAcc 0.4131	TestAcc 0.4142	BestValid 0.4131
	Epoch 3450:	Loss 0.2222	TrainAcc 0.4127	ValidAcc 0.4122	TestAcc 0.4131	BestValid 0.4131
	Epoch 3500:	Loss 0.2220	TrainAcc 0.4161	ValidAcc 0.4153	TestAcc 0.4165	BestValid 0.4153
	Epoch 3550:	Loss 0.2218	TrainAcc 0.4167	ValidAcc 0.4161	TestAcc 0.4172	BestValid 0.4161
	Epoch 3600:	Loss 0.2215	TrainAcc 0.4203	ValidAcc 0.4196	TestAcc 0.4210	BestValid 0.4196
	Epoch 3650:	Loss 0.2214	TrainAcc 0.4237	ValidAcc 0.4229	TestAcc 0.4241	BestValid 0.4229
	Epoch 3700:	Loss 0.2211	TrainAcc 0.4240	ValidAcc 0.4233	TestAcc 0.4244	BestValid 0.4233
	Epoch 3750:	Loss 0.2208	TrainAcc 0.4257	ValidAcc 0.4249	TestAcc 0.4261	BestValid 0.4249
	Epoch 3800:	Loss 0.2206	TrainAcc 0.4270	ValidAcc 0.4261	TestAcc 0.4273	BestValid 0.4261
	Epoch 3850:	Loss 0.2203	TrainAcc 0.4226	ValidAcc 0.4218	TestAcc 0.4229	BestValid 0.4261
	Epoch 3900:	Loss 0.2201	TrainAcc 0.4338	ValidAcc 0.4331	TestAcc 0.4342	BestValid 0.4331
	Epoch 3950:	Loss 0.2199	TrainAcc 0.4358	ValidAcc 0.4351	TestAcc 0.4362	BestValid 0.4351
	Epoch 4000:	Loss 0.2197	TrainAcc 0.4341	ValidAcc 0.4333	TestAcc 0.4344	BestValid 0.4351
	Epoch 4050:	Loss 0.2196	TrainAcc 0.4363	ValidAcc 0.4356	TestAcc 0.4367	BestValid 0.4356
	Epoch 4100:	Loss 0.2194	TrainAcc 0.4380	ValidAcc 0.4372	TestAcc 0.4383	BestValid 0.4372
	Epoch 4150:	Loss 0.2191	TrainAcc 0.4400	ValidAcc 0.4392	TestAcc 0.4403	BestValid 0.4392
	Epoch 4200:	Loss 0.2189	TrainAcc 0.4402	ValidAcc 0.4393	TestAcc 0.4404	BestValid 0.4393
	Epoch 4250:	Loss 0.2187	TrainAcc 0.4455	ValidAcc 0.4447	TestAcc 0.4459	BestValid 0.4447
	Epoch 4300:	Loss 0.2187	TrainAcc 0.4462	ValidAcc 0.4454	TestAcc 0.4467	BestValid 0.4454
	Epoch 4350:	Loss 0.2184	TrainAcc 0.4459	ValidAcc 0.4450	TestAcc 0.4463	BestValid 0.4454
	Epoch 4400:	Loss 0.2182	TrainAcc 0.4456	ValidAcc 0.4448	TestAcc 0.4460	BestValid 0.4454
	Epoch 4450:	Loss 0.2180	TrainAcc 0.4483	ValidAcc 0.4475	TestAcc 0.4487	BestValid 0.4475
	Epoch 4500:	Loss 0.2178	TrainAcc 0.4533	ValidAcc 0.4525	TestAcc 0.4539	BestValid 0.4525
	Epoch 4550:	Loss 0.2176	TrainAcc 0.4526	ValidAcc 0.4517	TestAcc 0.4531	BestValid 0.4525
	Epoch 4600:	Loss 0.2175	TrainAcc 0.4574	ValidAcc 0.4568	TestAcc 0.4581	BestValid 0.4568
	Epoch 4650:	Loss 0.2174	TrainAcc 0.4574	ValidAcc 0.4565	TestAcc 0.4582	BestValid 0.4568
	Epoch 4700:	Loss 0.2171	TrainAcc 0.4602	ValidAcc 0.4593	TestAcc 0.4608	BestValid 0.4593
	Epoch 4750:	Loss 0.2170	TrainAcc 0.4575	ValidAcc 0.4566	TestAcc 0.4583	BestValid 0.4593
	Epoch 4800:	Loss 0.2168	TrainAcc 0.4656	ValidAcc 0.4649	TestAcc 0.4663	BestValid 0.4649
	Epoch 4850:	Loss 0.2166	TrainAcc 0.4643	ValidAcc 0.4634	TestAcc 0.4651	BestValid 0.4649
	Epoch 4900:	Loss 0.2166	TrainAcc 0.4637	ValidAcc 0.4629	TestAcc 0.4644	BestValid 0.4649
	Epoch 4950:	Loss 0.2164	TrainAcc 0.4640	ValidAcc 0.4632	TestAcc 0.4648	BestValid 0.4649
	Epoch 5000:	Loss 0.2162	TrainAcc 0.4669	ValidAcc 0.4661	TestAcc 0.4676	BestValid 0.4661
****** Epoch Time (Excluding Evaluation Cost): 0.812 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 1.379 ms (Max: 2.247, Min: 0.035, Sum: 11.034)
Cluster-Wide Average, Compute: 183.049 ms (Max: 216.300, Min: 155.258, Sum: 1464.393)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.067)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.122)
Cluster-Wide Average, Communication-Graph: 623.750 ms (Max: 650.916, Min: 591.834, Sum: 4990.001)
Cluster-Wide Average, Optimization: 2.872 ms (Max: 2.885, Min: 2.859, Sum: 22.980)
Cluster-Wide Average, Others: 0.804 ms (Max: 0.826, Min: 0.764, Sum: 6.432)
****** Breakdown Sum: 811.879 ms ******
Cluster-Wide Average, GPU Memory Consumption: 18.104 GB (Max: 18.995, Min: 17.962, Sum: 144.831)
Cluster-Wide Average, Graph-Level Communication Throughput: 38.926 Gbps (Max: 52.370, Min: 24.964, Sum: 311.411)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 21.415 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 21.434 GB
****** Accuracy Results ******
Highest valid_acc: 0.4661
Target test_acc: 0.4676
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
