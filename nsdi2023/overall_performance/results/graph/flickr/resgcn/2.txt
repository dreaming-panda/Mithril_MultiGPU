Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
DONE MPI INIT
Initialized node 0 on machine gnerv4

Initialized node 1 on machine gnerv4
DONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.018 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.102 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.634 Gbps (per GPU), 485.073 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.318 Gbps (per GPU), 482.541 Gbps (aggregated)
The layer-level communication performance: 60.317 Gbps (per GPU), 482.532 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.055 Gbps (per GPU), 480.441 Gbps (aggregated)
The layer-level communication performance: 60.029 Gbps (per GPU), 480.231 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.809 Gbps (per GPU), 478.474 Gbps (aggregated)
The layer-level communication performance: 59.762 Gbps (per GPU), 478.093 Gbps (aggregated)
The layer-level communication performance: 59.728 Gbps (per GPU), 477.825 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.452 Gbps (per GPU), 1267.618 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.437 Gbps (per GPU), 1267.496 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.405 Gbps (per GPU), 1267.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.431 Gbps (per GPU), 1267.448 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.431 Gbps (per GPU), 1267.451 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.410 Gbps (per GPU), 1267.281 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.395 Gbps (per GPU), 1267.161 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.431 Gbps (per GPU), 1267.452 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.613 Gbps (per GPU), 836.901 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.608 Gbps (per GPU), 836.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.616 Gbps (per GPU), 836.928 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.600 Gbps (per GPU), 836.797 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.611 Gbps (per GPU), 836.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.612 Gbps (per GPU), 836.894 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.572 Gbps (per GPU), 836.576 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.609 Gbps (per GPU), 836.873 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.318 Gbps (per GPU), 306.542 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.316 Gbps (per GPU), 306.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.314 Gbps (per GPU), 306.515 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.311 Gbps (per GPU), 306.484 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.313 Gbps (per GPU), 306.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.313 Gbps (per GPU), 306.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.311 Gbps (per GPU), 306.490 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.314 Gbps (per GPU), 306.508 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.74ms  1.30ms  2.24ms  3.01 11.16K  0.12M
 chk_1  0.73ms  1.30ms  2.25ms  3.06 11.16K  0.11M
 chk_2  0.70ms  1.27ms  2.22ms  3.17 11.16K  0.11M
 chk_3  0.67ms  1.23ms  2.18ms  3.27 11.16K  0.12M
 chk_4  0.73ms  1.30ms  2.25ms  3.07 11.16K  0.11M
 chk_5  0.72ms  1.29ms  2.24ms  3.09 11.16K  0.10M
 chk_6  0.73ms  1.29ms  2.24ms  3.08 11.16K  0.12M
 chk_7  0.73ms  1.30ms  2.25ms  3.07 11.16K  0.11M
   Avg  0.72  1.28  2.23
   Max  0.74  1.30  2.25
   Min  0.67  1.23  2.18
 Ratio  1.12  1.06  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.459 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 229)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 229)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 229)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 229)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 229)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 229)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 229)...
+++++++++ Node 1 initializing the weights for op[0, 229)...
+++++++++ Node 5 initializing the weights for op[0, 229)...
+++++++++ Node 3 initializing the weights for op[0, 229)...
+++++++++ Node 7 initializing the weights for op[0, 229)...
+++++++++ Node 2 initializing the weights for op[0, 229)...
+++++++++ Node 4 initializing the weights for op[0, 229)...
+++++++++ Node 6 initializing the weights for op[0, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2529	TrainAcc 0.3503	ValidAcc 0.3529	TestAcc 0.3538	BestValid 0.3529
	Epoch 50:	Loss 1.5560	TrainAcc 0.4644	ValidAcc 0.4677	TestAcc 0.4661	BestValid 0.4677
	Epoch 100:	Loss 1.4807	TrainAcc 0.4764	ValidAcc 0.4783	TestAcc 0.4732	BestValid 0.4783
	Epoch 150:	Loss 1.4522	TrainAcc 0.4922	ValidAcc 0.4906	TestAcc 0.4891	BestValid 0.4906
	Epoch 200:	Loss 1.4383	TrainAcc 0.4988	ValidAcc 0.4949	TestAcc 0.4947	BestValid 0.4949
	Epoch 250:	Loss 1.4267	TrainAcc 0.4988	ValidAcc 0.4942	TestAcc 0.4953	BestValid 0.4949
	Epoch 300:	Loss 1.4184	TrainAcc 0.5075	ValidAcc 0.5000	TestAcc 0.5006	BestValid 0.5000
	Epoch 350:	Loss 1.4117	TrainAcc 0.5088	ValidAcc 0.5010	TestAcc 0.5026	BestValid 0.5010
	Epoch 400:	Loss 1.4136	TrainAcc 0.5089	ValidAcc 0.5015	TestAcc 0.5010	BestValid 0.5015
	Epoch 450:	Loss 1.3992	TrainAcc 0.5159	ValidAcc 0.5079	TestAcc 0.5080	BestValid 0.5079
	Epoch 500:	Loss 1.3920	TrainAcc 0.5161	ValidAcc 0.5059	TestAcc 0.5063	BestValid 0.5079
	Epoch 550:	Loss 1.3911	TrainAcc 0.5187	ValidAcc 0.5099	TestAcc 0.5102	BestValid 0.5099
	Epoch 600:	Loss 1.3876	TrainAcc 0.5197	ValidAcc 0.5108	TestAcc 0.5103	BestValid 0.5108
	Epoch 650:	Loss 1.3781	TrainAcc 0.5235	ValidAcc 0.5111	TestAcc 0.5116	BestValid 0.5111
	Epoch 700:	Loss 1.3811	TrainAcc 0.5209	ValidAcc 0.5105	TestAcc 0.5117	BestValid 0.5111
	Epoch 750:	Loss 1.3769	TrainAcc 0.5113	ValidAcc 0.4992	TestAcc 0.4973	BestValid 0.5111
	Epoch 800:	Loss 1.3645	TrainAcc 0.5265	ValidAcc 0.5145	TestAcc 0.5147	BestValid 0.5145
	Epoch 850:	Loss 1.3638	TrainAcc 0.5270	ValidAcc 0.5141	TestAcc 0.5164	BestValid 0.5145
	Epoch 900:	Loss 1.3621	TrainAcc 0.5320	ValidAcc 0.5195	TestAcc 0.5174	BestValid 0.5195
	Epoch 950:	Loss 1.3610	TrainAcc 0.5319	ValidAcc 0.5185	TestAcc 0.5182	BestValid 0.5195
	Epoch 1000:	Loss 1.3537	TrainAcc 0.5323	ValidAcc 0.5178	TestAcc 0.5167	BestValid 0.5195
	Epoch 1050:	Loss 1.3506	TrainAcc 0.5348	ValidAcc 0.5204	TestAcc 0.5189	BestValid 0.5204
	Epoch 1100:	Loss 1.3497	TrainAcc 0.5360	ValidAcc 0.5177	TestAcc 0.5185	BestValid 0.5204
	Epoch 1150:	Loss 1.3497	TrainAcc 0.5364	ValidAcc 0.5176	TestAcc 0.5200	BestValid 0.5204
	Epoch 1200:	Loss 1.3415	TrainAcc 0.5325	ValidAcc 0.5156	TestAcc 0.5179	BestValid 0.5204
	Epoch 1250:	Loss 1.3465	TrainAcc 0.5353	ValidAcc 0.5166	TestAcc 0.5192	BestValid 0.5204
	Epoch 1300:	Loss 1.3397	TrainAcc 0.5429	ValidAcc 0.5220	TestAcc 0.5220	BestValid 0.5220
	Epoch 1350:	Loss 1.3347	TrainAcc 0.5434	ValidAcc 0.5208	TestAcc 0.5225	BestValid 0.5220
	Epoch 1400:	Loss 1.3358	TrainAcc 0.5407	ValidAcc 0.5186	TestAcc 0.5212	BestValid 0.5220
	Epoch 1450:	Loss 1.3323	TrainAcc 0.5454	ValidAcc 0.5220	TestAcc 0.5223	BestValid 0.5220
	Epoch 1500:	Loss 1.3525	TrainAcc 0.5342	ValidAcc 0.5149	TestAcc 0.5187	BestValid 0.5220
	Epoch 1550:	Loss 1.3237	TrainAcc 0.5485	ValidAcc 0.5253	TestAcc 0.5234	BestValid 0.5253
	Epoch 1600:	Loss 1.3280	TrainAcc 0.5468	ValidAcc 0.5245	TestAcc 0.5241	BestValid 0.5253
	Epoch 1650:	Loss 1.3237	TrainAcc 0.5490	ValidAcc 0.5224	TestAcc 0.5235	BestValid 0.5253
	Epoch 1700:	Loss 1.3178	TrainAcc 0.5503	ValidAcc 0.5244	TestAcc 0.5239	BestValid 0.5253
	Epoch 1750:	Loss 1.3213	TrainAcc 0.5443	ValidAcc 0.5167	TestAcc 0.5180	BestValid 0.5253
	Epoch 1800:	Loss 1.3168	TrainAcc 0.5507	ValidAcc 0.5229	TestAcc 0.5233	BestValid 0.5253
	Epoch 1850:	Loss 1.3115	TrainAcc 0.5529	ValidAcc 0.5263	TestAcc 0.5251	BestValid 0.5263
	Epoch 1900:	Loss 1.3125	TrainAcc 0.5537	ValidAcc 0.5221	TestAcc 0.5258	BestValid 0.5263
	Epoch 1950:	Loss 1.3281	TrainAcc 0.5288	ValidAcc 0.5076	TestAcc 0.5078	BestValid 0.5263
	Epoch 2000:	Loss 1.3101	TrainAcc 0.5575	ValidAcc 0.5278	TestAcc 0.5255	BestValid 0.5278
	Epoch 2050:	Loss 1.3044	TrainAcc 0.5572	ValidAcc 0.5250	TestAcc 0.5283	BestValid 0.5278
	Epoch 2100:	Loss 1.3079	TrainAcc 0.5559	ValidAcc 0.5235	TestAcc 0.5234	BestValid 0.5278
	Epoch 2150:	Loss 1.3065	TrainAcc 0.5533	ValidAcc 0.5237	TestAcc 0.5254	BestValid 0.5278
	Epoch 2200:	Loss 1.2992	TrainAcc 0.5578	ValidAcc 0.5244	TestAcc 0.5268	BestValid 0.5278
	Epoch 2250:	Loss 1.3029	TrainAcc 0.5568	ValidAcc 0.5244	TestAcc 0.5239	BestValid 0.5278
	Epoch 2300:	Loss 1.3039	TrainAcc 0.5602	ValidAcc 0.5228	TestAcc 0.5232	BestValid 0.5278
	Epoch 2350:	Loss 1.2991	TrainAcc 0.5614	ValidAcc 0.5272	TestAcc 0.5273	BestValid 0.5278
	Epoch 2400:	Loss 1.2942	TrainAcc 0.5601	ValidAcc 0.5226	TestAcc 0.5227	BestValid 0.5278
	Epoch 2450:	Loss 1.2999	TrainAcc 0.5582	ValidAcc 0.5218	TestAcc 0.5226	BestValid 0.5278
	Epoch 2500:	Loss 1.2954	TrainAcc 0.5629	ValidAcc 0.5234	TestAcc 0.5261	BestValid 0.5278
	Epoch 2550:	Loss 1.2924	TrainAcc 0.5660	ValidAcc 0.5309	TestAcc 0.5287	BestValid 0.5309
	Epoch 2600:	Loss 1.2887	TrainAcc 0.5665	ValidAcc 0.5272	TestAcc 0.5283	BestValid 0.5309
	Epoch 2650:	Loss 1.2890	TrainAcc 0.5652	ValidAcc 0.5305	TestAcc 0.5301	BestValid 0.5309
	Epoch 2700:	Loss 1.2875	TrainAcc 0.5668	ValidAcc 0.5321	TestAcc 0.5308	BestValid 0.5321
	Epoch 2750:	Loss 1.2823	TrainAcc 0.5671	ValidAcc 0.5275	TestAcc 0.5286	BestValid 0.5321
	Epoch 2800:	Loss 1.2811	TrainAcc 0.5670	ValidAcc 0.5241	TestAcc 0.5279	BestValid 0.5321
	Epoch 2850:	Loss 1.2833	TrainAcc 0.5691	ValidAcc 0.5298	TestAcc 0.5308	BestValid 0.5321
	Epoch 2900:	Loss 1.2770	TrainAcc 0.5689	ValidAcc 0.5232	TestAcc 0.5286	BestValid 0.5321
	Epoch 2950:	Loss 1.2820	TrainAcc 0.5621	ValidAcc 0.5202	TestAcc 0.5189	BestValid 0.5321
	Epoch 3000:	Loss 1.2755	TrainAcc 0.5713	ValidAcc 0.5301	TestAcc 0.5291	BestValid 0.5321
	Epoch 3050:	Loss 1.2725	TrainAcc 0.5708	ValidAcc 0.5261	TestAcc 0.5270	BestValid 0.5321
	Epoch 3100:	Loss 1.2809	TrainAcc 0.5678	ValidAcc 0.5282	TestAcc 0.5278	BestValid 0.5321
	Epoch 3150:	Loss 1.2724	TrainAcc 0.5715	ValidAcc 0.5256	TestAcc 0.5268	BestValid 0.5321
	Epoch 3200:	Loss 1.2736	TrainAcc 0.5743	ValidAcc 0.5310	TestAcc 0.5320	BestValid 0.5321
	Epoch 3250:	Loss 1.2708	TrainAcc 0.5760	ValidAcc 0.5312	TestAcc 0.5319	BestValid 0.5321
	Epoch 3300:	Loss 1.2702	TrainAcc 0.5762	ValidAcc 0.5300	TestAcc 0.5321	BestValid 0.5321
	Epoch 3350:	Loss 1.2673	TrainAcc 0.5774	ValidAcc 0.5315	TestAcc 0.5340	BestValid 0.5321
	Epoch 3400:	Loss 1.2813	TrainAcc 0.5724	ValidAcc 0.5260	TestAcc 0.5269	BestValid 0.5321
	Epoch 3450:	Loss 1.2684	TrainAcc 0.5748	ValidAcc 0.5276	TestAcc 0.5282	BestValid 0.5321
	Epoch 3500:	Loss 1.2661	TrainAcc 0.5769	ValidAcc 0.5290	TestAcc 0.5320	BestValid 0.5321
	Epoch 3550:	Loss 1.2632	TrainAcc 0.5788	ValidAcc 0.5286	TestAcc 0.5344	BestValid 0.5321
	Epoch 3600:	Loss 1.2637	TrainAcc 0.5801	ValidAcc 0.5328	TestAcc 0.5330	BestValid 0.5328
	Epoch 3650:	Loss 1.2596	TrainAcc 0.5799	ValidAcc 0.5281	TestAcc 0.5322	BestValid 0.5328
	Epoch 3700:	Loss 1.2601	TrainAcc 0.5799	ValidAcc 0.5257	TestAcc 0.5297	BestValid 0.5328
	Epoch 3750:	Loss 1.2546	TrainAcc 0.5827	ValidAcc 0.5309	TestAcc 0.5327	BestValid 0.5328
	Epoch 3800:	Loss 1.2616	TrainAcc 0.5788	ValidAcc 0.5309	TestAcc 0.5337	BestValid 0.5328
	Epoch 3850:	Loss 1.2526	TrainAcc 0.5834	ValidAcc 0.5301	TestAcc 0.5319	BestValid 0.5328
	Epoch 3900:	Loss 1.2600	TrainAcc 0.5796	ValidAcc 0.5272	TestAcc 0.5330	BestValid 0.5328
	Epoch 3950:	Loss 1.2509	TrainAcc 0.5835	ValidAcc 0.5315	TestAcc 0.5344	BestValid 0.5328
	Epoch 4000:	Loss 1.2546	TrainAcc 0.5793	ValidAcc 0.5263	TestAcc 0.5291	BestValid 0.5328
	Epoch 4050:	Loss 1.2485	TrainAcc 0.5846	ValidAcc 0.5318	TestAcc 0.5357	BestValid 0.5328
	Epoch 4100:	Loss 1.2472	TrainAcc 0.5861	ValidAcc 0.5275	TestAcc 0.5330	BestValid 0.5328
	Epoch 4150:	Loss 1.2491	TrainAcc 0.5859	ValidAcc 0.5332	TestAcc 0.5359	BestValid 0.5332
	Epoch 4200:	Loss 1.2495	TrainAcc 0.5861	ValidAcc 0.5290	TestAcc 0.5318	BestValid 0.5332
	Epoch 4250:	Loss 1.2435	TrainAcc 0.5871	ValidAcc 0.5324	TestAcc 0.5344	BestValid 0.5332
	Epoch 4300:	Loss 1.2424	TrainAcc 0.5894	ValidAcc 0.5332	TestAcc 0.5354	BestValid 0.5332
	Epoch 4350:	Loss 1.2488	TrainAcc 0.5867	ValidAcc 0.5324	TestAcc 0.5326	BestValid 0.5332
	Epoch 4400:	Loss 1.2456	TrainAcc 0.5867	ValidAcc 0.5298	TestAcc 0.5335	BestValid 0.5332
	Epoch 4450:	Loss 1.2485	TrainAcc 0.5714	ValidAcc 0.5104	TestAcc 0.5188	BestValid 0.5332
	Epoch 4500:	Loss 1.2376	TrainAcc 0.5891	ValidAcc 0.5295	TestAcc 0.5326	BestValid 0.5332
	Epoch 4550:	Loss 1.2405	TrainAcc 0.5872	ValidAcc 0.5275	TestAcc 0.5309	BestValid 0.5332
	Epoch 4600:	Loss 1.2419	TrainAcc 0.5920	ValidAcc 0.5310	TestAcc 0.5355	BestValid 0.5332
	Epoch 4650:	Loss 1.2383	TrainAcc 0.5925	ValidAcc 0.5316	TestAcc 0.5357	BestValid 0.5332
	Epoch 4700:	Loss 1.2345	TrainAcc 0.5897	ValidAcc 0.5265	TestAcc 0.5296	BestValid 0.5332
	Epoch 4750:	Loss 1.2345	TrainAcc 0.5922	ValidAcc 0.5346	TestAcc 0.5363	BestValid 0.5346
	Epoch 4800:	Loss 1.2365	TrainAcc 0.5933	ValidAcc 0.5321	TestAcc 0.5349	BestValid 0.5346
	Epoch 4850:	Loss 1.2352	TrainAcc 0.5908	ValidAcc 0.5296	TestAcc 0.5332	BestValid 0.5346
	Epoch 4900:	Loss 1.2399	TrainAcc 0.5873	ValidAcc 0.5291	TestAcc 0.5355	BestValid 0.5346
	Epoch 4950:	Loss 1.2291	TrainAcc 0.5938	ValidAcc 0.5325	TestAcc 0.5376	BestValid 0.5346
	Epoch 5000:	Loss 1.2308	TrainAcc 0.5959	ValidAcc 0.5323	TestAcc 0.5350	BestValid 0.5346
****** Epoch Time (Excluding Evaluation Cost): 0.165 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.177 ms (Max: 0.282, Min: 0.034, Sum: 1.417)
Cluster-Wide Average, Compute: 38.194 ms (Max: 38.951, Min: 37.497, Sum: 305.548)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.061)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.118)
Cluster-Wide Average, Communication-Graph: 123.608 ms (Max: 124.160, Min: 122.733, Sum: 988.862)
Cluster-Wide Average, Optimization: 2.710 ms (Max: 2.724, Min: 2.698, Sum: 21.683)
Cluster-Wide Average, Others: 0.436 ms (Max: 0.463, Min: 0.414, Sum: 3.491)
****** Breakdown Sum: 165.148 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.924 GB (Max: 4.399, Min: 3.842, Sum: 31.390)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.703 Gbps (Max: 51.753, Min: 16.959, Sum: 341.628)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 4.603 GB
****** Accuracy Results ******
Highest valid_acc: 0.5346
Target test_acc: 0.5363
Epoch to reach the target acc: 4749
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
