Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
DONE MPI INIT
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
        It takes 0.025 seconds.
        It takes 0.019 seconds.
        It takes 0.028 seconds.
        It takes 0.019 seconds.
        It takes 0.023 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.111 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.011 seconds.
        It takes 0.119 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.120 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.012 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 11157
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 11157
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.258 Gbps (per GPU), 482.062 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.943 Gbps (per GPU), 479.541 Gbps (aggregated)
The layer-level communication performance: 59.935 Gbps (per GPU), 479.478 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.697 Gbps (per GPU), 477.575 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.446 Gbps (per GPU), 475.567 Gbps (aggregated)
The layer-level communication performance: 59.401 Gbps (per GPU), 475.205 Gbps (aggregated)
The layer-level communication performance: 59.661 Gbps (per GPU), 477.287 Gbps (aggregated)
The layer-level communication performance: 59.366 Gbps (per GPU), 474.931 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.586 Gbps (per GPU), 1260.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.562 Gbps (per GPU), 1260.497 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.509 Gbps (per GPU), 1260.071 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.589 Gbps (per GPU), 1260.712 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.586 Gbps (per GPU), 1260.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.580 Gbps (per GPU), 1260.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.515 Gbps (per GPU), 1260.118 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.577 Gbps (per GPU), 1260.615 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.438 Gbps (per GPU), 811.506 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.441 Gbps (per GPU), 811.526 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.442 Gbps (per GPU), 811.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.437 Gbps (per GPU), 811.493 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.442 Gbps (per GPU), 811.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.441 Gbps (per GPU), 811.526 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.440 Gbps (per GPU), 811.519 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.439 Gbps (per GPU), 811.513 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.922 Gbps (per GPU), 295.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.927 Gbps (per GPU), 295.419 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.923 Gbps (per GPU), 295.388 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.924 Gbps (per GPU), 295.394 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.922 Gbps (per GPU), 295.379 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.923 Gbps (per GPU), 295.387 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.916 Gbps (per GPU), 295.327 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.920 Gbps (per GPU), 295.362 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.06ms  1.74ms  2.86ms  2.69 11.16K  0.12M
 chk_1  1.03ms  1.75ms  2.87ms  2.79 11.16K  0.11M
 chk_2  0.98ms  1.72ms  2.83ms  2.88 11.16K  0.11M
 chk_3  0.94ms  1.67ms  2.79ms  2.97 11.16K  0.12M
 chk_4  1.01ms  1.74ms  2.86ms  2.84 11.16K  0.11M
 chk_5  1.00ms  1.73ms  2.85ms  2.86 11.16K  0.10M
 chk_6  1.04ms  1.73ms  2.85ms  2.76 11.16K  0.12M
 chk_7  1.01ms  1.74ms  2.86ms  2.84 11.16K  0.11M
   Avg  1.01  1.73  2.85
   Max  1.06  1.75  2.87
   Min  0.94  1.67  2.79
 Ratio  1.13  1.04  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.591 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 421)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 421)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 421)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 421)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 421)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 421)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 421)...
+++++++++ Node 4 initializing the weights for op[0, 421)...
+++++++++ Node 5 initializing the weights for op[0, 421)...
+++++++++ Node 6 initializing the weights for op[0, 421)...
+++++++++ Node 7 initializing the weights for op[0, 421)...
+++++++++ Node 1 initializing the weights for op[0, 421)...
+++++++++ Node 2 initializing the weights for op[0, 421)...
+++++++++ Node 3 initializing the weights for op[0, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5553	TrainAcc 0.4212	ValidAcc 0.4232	TestAcc 0.4228	BestValid 0.4232
	Epoch 50:	Loss 1.5676	TrainAcc 0.4497	ValidAcc 0.4490	TestAcc 0.4500	BestValid 0.4490
	Epoch 100:	Loss 1.4675	TrainAcc 0.5015	ValidAcc 0.4993	TestAcc 0.5015	BestValid 0.4993
	Epoch 150:	Loss 1.4133	TrainAcc 0.5208	ValidAcc 0.5146	TestAcc 0.5154	BestValid 0.5146
	Epoch 200:	Loss 1.3884	TrainAcc 0.5266	ValidAcc 0.5159	TestAcc 0.5214	BestValid 0.5159
	Epoch 250:	Loss 1.3811	TrainAcc 0.5288	ValidAcc 0.5182	TestAcc 0.5228	BestValid 0.5182
	Epoch 300:	Loss 1.3660	TrainAcc 0.5324	ValidAcc 0.5212	TestAcc 0.5249	BestValid 0.5212
	Epoch 350:	Loss 1.3615	TrainAcc 0.5336	ValidAcc 0.5209	TestAcc 0.5268	BestValid 0.5212
	Epoch 400:	Loss 1.3587	TrainAcc 0.5324	ValidAcc 0.5180	TestAcc 0.5208	BestValid 0.5212
	Epoch 450:	Loss 1.3469	TrainAcc 0.5380	ValidAcc 0.5252	TestAcc 0.5286	BestValid 0.5252
	Epoch 500:	Loss 1.3407	TrainAcc 0.5412	ValidAcc 0.5258	TestAcc 0.5284	BestValid 0.5258
	Epoch 550:	Loss 1.3328	TrainAcc 0.5438	ValidAcc 0.5258	TestAcc 0.5290	BestValid 0.5258
	Epoch 600:	Loss 1.3281	TrainAcc 0.5431	ValidAcc 0.5240	TestAcc 0.5271	BestValid 0.5258
	Epoch 650:	Loss 1.3269	TrainAcc 0.5398	ValidAcc 0.5209	TestAcc 0.5227	BestValid 0.5258
	Epoch 700:	Loss 1.3186	TrainAcc 0.5492	ValidAcc 0.5276	TestAcc 0.5304	BestValid 0.5276
	Epoch 750:	Loss 1.3122	TrainAcc 0.5501	ValidAcc 0.5250	TestAcc 0.5276	BestValid 0.5276
	Epoch 800:	Loss 1.3252	TrainAcc 0.5493	ValidAcc 0.5271	TestAcc 0.5286	BestValid 0.5276
	Epoch 850:	Loss 1.3031	TrainAcc 0.5527	ValidAcc 0.5268	TestAcc 0.5298	BestValid 0.5276
	Epoch 900:	Loss 1.3024	TrainAcc 0.5455	ValidAcc 0.5211	TestAcc 0.5249	BestValid 0.5276
	Epoch 950:	Loss 1.3020	TrainAcc 0.5553	ValidAcc 0.5295	TestAcc 0.5329	BestValid 0.5295
	Epoch 1000:	Loss 1.2917	TrainAcc 0.5584	ValidAcc 0.5268	TestAcc 0.5318	BestValid 0.5295
	Epoch 1050:	Loss 1.2870	TrainAcc 0.5578	ValidAcc 0.5239	TestAcc 0.5295	BestValid 0.5295
	Epoch 1100:	Loss 1.2840	TrainAcc 0.5524	ValidAcc 0.5212	TestAcc 0.5249	BestValid 0.5295
	Epoch 1150:	Loss 1.2843	TrainAcc 0.5644	ValidAcc 0.5303	TestAcc 0.5334	BestValid 0.5303
	Epoch 1200:	Loss 1.2751	TrainAcc 0.5653	ValidAcc 0.5278	TestAcc 0.5335	BestValid 0.5303
	Epoch 1250:	Loss 1.2686	TrainAcc 0.5668	ValidAcc 0.5292	TestAcc 0.5346	BestValid 0.5303
	Epoch 1300:	Loss 1.2670	TrainAcc 0.5692	ValidAcc 0.5292	TestAcc 0.5341	BestValid 0.5303
	Epoch 1350:	Loss 1.2714	TrainAcc 0.5684	ValidAcc 0.5270	TestAcc 0.5337	BestValid 0.5303
	Epoch 1400:	Loss 1.2630	TrainAcc 0.5715	ValidAcc 0.5267	TestAcc 0.5336	BestValid 0.5303
	Epoch 1450:	Loss 1.2572	TrainAcc 0.5687	ValidAcc 0.5218	TestAcc 0.5276	BestValid 0.5303
	Epoch 1500:	Loss 1.2692	TrainAcc 0.5733	ValidAcc 0.5314	TestAcc 0.5325	BestValid 0.5314
	Epoch 1550:	Loss 1.2482	TrainAcc 0.5773	ValidAcc 0.5290	TestAcc 0.5339	BestValid 0.5314
	Epoch 1600:	Loss 1.2526	TrainAcc 0.5775	ValidAcc 0.5308	TestAcc 0.5326	BestValid 0.5314
	Epoch 1650:	Loss 1.2433	TrainAcc 0.5798	ValidAcc 0.5286	TestAcc 0.5360	BestValid 0.5314
	Epoch 1700:	Loss 1.2408	TrainAcc 0.5824	ValidAcc 0.5294	TestAcc 0.5351	BestValid 0.5314
	Epoch 1750:	Loss 1.2435	TrainAcc 0.5830	ValidAcc 0.5281	TestAcc 0.5345	BestValid 0.5314
	Epoch 1800:	Loss 1.2360	TrainAcc 0.5752	ValidAcc 0.5209	TestAcc 0.5294	BestValid 0.5314
	Epoch 1850:	Loss 1.2308	TrainAcc 0.5843	ValidAcc 0.5278	TestAcc 0.5346	BestValid 0.5314
	Epoch 1900:	Loss 1.2248	TrainAcc 0.5874	ValidAcc 0.5281	TestAcc 0.5360	BestValid 0.5314
	Epoch 1950:	Loss 1.2173	TrainAcc 0.5866	ValidAcc 0.5245	TestAcc 0.5327	BestValid 0.5314
	Epoch 2000:	Loss 1.2242	TrainAcc 0.5887	ValidAcc 0.5271	TestAcc 0.5345	BestValid 0.5314
	Epoch 2050:	Loss 1.2137	TrainAcc 0.5813	ValidAcc 0.5152	TestAcc 0.5212	BestValid 0.5314
	Epoch 2100:	Loss 1.2169	TrainAcc 0.5922	ValidAcc 0.5262	TestAcc 0.5337	BestValid 0.5314
	Epoch 2150:	Loss 1.2191	TrainAcc 0.5956	ValidAcc 0.5296	TestAcc 0.5352	BestValid 0.5314
	Epoch 2200:	Loss 1.2056	TrainAcc 0.5963	ValidAcc 0.5284	TestAcc 0.5347	BestValid 0.5314
	Epoch 2250:	Loss 1.2065	TrainAcc 0.5991	ValidAcc 0.5307	TestAcc 0.5352	BestValid 0.5314
	Epoch 2300:	Loss 1.1956	TrainAcc 0.5974	ValidAcc 0.5208	TestAcc 0.5293	BestValid 0.5314
	Epoch 2350:	Loss 1.1986	TrainAcc 0.5963	ValidAcc 0.5232	TestAcc 0.5316	BestValid 0.5314
	Epoch 2400:	Loss 1.1864	TrainAcc 0.6028	ValidAcc 0.5288	TestAcc 0.5328	BestValid 0.5314
	Epoch 2450:	Loss 1.1915	TrainAcc 0.6046	ValidAcc 0.5248	TestAcc 0.5307	BestValid 0.5314
	Epoch 2500:	Loss 1.1784	TrainAcc 0.5987	ValidAcc 0.5212	TestAcc 0.5288	BestValid 0.5314
	Epoch 2550:	Loss 1.1800	TrainAcc 0.5992	ValidAcc 0.5218	TestAcc 0.5274	BestValid 0.5314
	Epoch 2600:	Loss 1.1852	TrainAcc 0.6011	ValidAcc 0.5254	TestAcc 0.5292	BestValid 0.5314
	Epoch 2650:	Loss 1.1704	TrainAcc 0.6070	ValidAcc 0.5203	TestAcc 0.5256	BestValid 0.5314
	Epoch 2700:	Loss 1.1633	TrainAcc 0.6133	ValidAcc 0.5224	TestAcc 0.5287	BestValid 0.5314
	Epoch 2750:	Loss 1.1628	TrainAcc 0.6139	ValidAcc 0.5286	TestAcc 0.5321	BestValid 0.5314
	Epoch 2800:	Loss 1.1642	TrainAcc 0.6162	ValidAcc 0.5218	TestAcc 0.5282	BestValid 0.5314
	Epoch 2850:	Loss 1.1599	TrainAcc 0.6143	ValidAcc 0.5192	TestAcc 0.5227	BestValid 0.5314
	Epoch 2900:	Loss 1.1504	TrainAcc 0.6197	ValidAcc 0.5246	TestAcc 0.5297	BestValid 0.5314
	Epoch 2950:	Loss 1.1615	TrainAcc 0.6215	ValidAcc 0.5281	TestAcc 0.5320	BestValid 0.5314
	Epoch 3000:	Loss 1.1466	TrainAcc 0.6221	ValidAcc 0.5292	TestAcc 0.5326	BestValid 0.5314
	Epoch 3050:	Loss 1.1436	TrainAcc 0.6173	ValidAcc 0.5091	TestAcc 0.5144	BestValid 0.5314
	Epoch 3100:	Loss 1.1363	TrainAcc 0.6257	ValidAcc 0.5265	TestAcc 0.5316	BestValid 0.5314
	Epoch 3150:	Loss 1.1316	TrainAcc 0.6283	ValidAcc 0.5281	TestAcc 0.5311	BestValid 0.5314
	Epoch 3200:	Loss 1.1288	TrainAcc 0.6221	ValidAcc 0.5158	TestAcc 0.5209	BestValid 0.5314
	Epoch 3250:	Loss 1.1252	TrainAcc 0.6318	ValidAcc 0.5250	TestAcc 0.5286	BestValid 0.5314
	Epoch 3300:	Loss 1.1308	TrainAcc 0.6343	ValidAcc 0.5218	TestAcc 0.5260	BestValid 0.5314
	Epoch 3350:	Loss 1.1231	TrainAcc 0.6326	ValidAcc 0.5137	TestAcc 0.5197	BestValid 0.5314
	Epoch 3400:	Loss 1.1173	TrainAcc 0.6305	ValidAcc 0.5234	TestAcc 0.5277	BestValid 0.5314
	Epoch 3450:	Loss 1.1108	TrainAcc 0.6415	ValidAcc 0.5217	TestAcc 0.5264	BestValid 0.5314
	Epoch 3500:	Loss 1.1260	TrainAcc 0.6370	ValidAcc 0.5275	TestAcc 0.5310	BestValid 0.5314
	Epoch 3550:	Loss 1.1051	TrainAcc 0.6394	ValidAcc 0.5203	TestAcc 0.5248	BestValid 0.5314
	Epoch 3600:	Loss 1.0996	TrainAcc 0.6363	ValidAcc 0.5100	TestAcc 0.5153	BestValid 0.5314
	Epoch 3650:	Loss 1.0919	TrainAcc 0.6464	ValidAcc 0.5151	TestAcc 0.5197	BestValid 0.5314
	Epoch 3700:	Loss 1.1002	TrainAcc 0.6451	ValidAcc 0.5221	TestAcc 0.5253	BestValid 0.5314
	Epoch 3750:	Loss 1.0891	TrainAcc 0.6488	ValidAcc 0.5169	TestAcc 0.5219	BestValid 0.5314
	Epoch 3800:	Loss 1.0832	TrainAcc 0.6525	ValidAcc 0.5203	TestAcc 0.5244	BestValid 0.5314
	Epoch 3850:	Loss 1.0841	TrainAcc 0.6527	ValidAcc 0.5141	TestAcc 0.5188	BestValid 0.5314
	Epoch 3900:	Loss 1.0890	TrainAcc 0.6527	ValidAcc 0.5203	TestAcc 0.5250	BestValid 0.5314
	Epoch 3950:	Loss 1.0798	TrainAcc 0.6566	ValidAcc 0.5221	TestAcc 0.5257	BestValid 0.5314
	Epoch 4000:	Loss 1.0802	TrainAcc 0.6523	ValidAcc 0.5199	TestAcc 0.5246	BestValid 0.5314
	Epoch 4050:	Loss 1.0700	TrainAcc 0.6559	ValidAcc 0.5125	TestAcc 0.5176	BestValid 0.5314
	Epoch 4100:	Loss 1.0642	TrainAcc 0.6627	ValidAcc 0.5182	TestAcc 0.5227	BestValid 0.5314
	Epoch 4150:	Loss 1.0585	TrainAcc 0.6638	ValidAcc 0.5148	TestAcc 0.5187	BestValid 0.5314
	Epoch 4200:	Loss 1.0605	TrainAcc 0.6423	ValidAcc 0.5147	TestAcc 0.5170	BestValid 0.5314
	Epoch 4250:	Loss 1.0484	TrainAcc 0.6639	ValidAcc 0.5197	TestAcc 0.5242	BestValid 0.5314
	Epoch 4300:	Loss 1.0473	TrainAcc 0.6695	ValidAcc 0.5156	TestAcc 0.5196	BestValid 0.5314
	Epoch 4350:	Loss 1.0442	TrainAcc 0.6685	ValidAcc 0.5117	TestAcc 0.5172	BestValid 0.5314
	Epoch 4400:	Loss 1.0474	TrainAcc 0.6719	ValidAcc 0.5160	TestAcc 0.5202	BestValid 0.5314
	Epoch 4450:	Loss 1.0414	TrainAcc 0.6734	ValidAcc 0.5164	TestAcc 0.5196	BestValid 0.5314
	Epoch 4500:	Loss 1.0330	TrainAcc 0.6757	ValidAcc 0.5125	TestAcc 0.5166	BestValid 0.5314
	Epoch 4550:	Loss 1.0269	TrainAcc 0.6739	ValidAcc 0.5149	TestAcc 0.5184	BestValid 0.5314
	Epoch 4600:	Loss 1.0281	TrainAcc 0.6792	ValidAcc 0.5124	TestAcc 0.5168	BestValid 0.5314
	Epoch 4650:	Loss 1.0273	TrainAcc 0.6807	ValidAcc 0.5075	TestAcc 0.5124	BestValid 0.5314
	Epoch 4700:	Loss 1.0194	TrainAcc 0.6831	ValidAcc 0.5114	TestAcc 0.5166	BestValid 0.5314
	Epoch 4750:	Loss 1.0324	TrainAcc 0.6824	ValidAcc 0.5140	TestAcc 0.5174	BestValid 0.5314
	Epoch 4800:	Loss 1.0135	TrainAcc 0.6827	ValidAcc 0.5043	TestAcc 0.5104	BestValid 0.5314
	Epoch 4850:	Loss 1.0091	TrainAcc 0.6874	ValidAcc 0.5044	TestAcc 0.5099	BestValid 0.5314
	Epoch 4900:	Loss 1.0005	TrainAcc 0.6901	ValidAcc 0.5114	TestAcc 0.5143	BestValid 0.5314
	Epoch 4950:	Loss 1.0011	TrainAcc 0.6878	ValidAcc 0.5109	TestAcc 0.5168	BestValid 0.5314
	Epoch 5000:	Loss 0.9991	TrainAcc 0.6891	ValidAcc 0.5095	TestAcc 0.5128	BestValid 0.5314
****** Epoch Time (Excluding Evaluation Cost): 0.186 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.150 ms (Max: 0.241, Min: 0.035, Sum: 1.202)
Cluster-Wide Average, Compute: 52.421 ms (Max: 52.920, Min: 51.428, Sum: 419.369)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.065)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.017, Min: 0.015, Sum: 0.127)
Cluster-Wide Average, Communication-Graph: 123.960 ms (Max: 124.804, Min: 123.406, Sum: 991.682)
Cluster-Wide Average, Optimization: 7.692 ms (Max: 7.742, Min: 7.663, Sum: 61.532)
Cluster-Wide Average, Others: 1.338 ms (Max: 1.381, Min: 1.233, Sum: 10.706)
****** Breakdown Sum: 185.585 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.057 GB (Max: 5.540, Min: 4.975, Sum: 40.460)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.587 Gbps (Max: 51.441, Min: 16.865, Sum: 340.693)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.036 GB
Total communication (cluster-wide, per-epoch): 4.620 GB
****** Accuracy Results ******
Highest valid_acc: 0.5314
Target test_acc: 0.5325
Epoch to reach the target acc: 1499
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
