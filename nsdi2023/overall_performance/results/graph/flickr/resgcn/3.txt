Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4

Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8

Initialized node 4 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.026 seconds.
        It takes 0.027 seconds.
        It takes 0.020 seconds.
        It takes 0.018 seconds.
        It takes 0.029 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.114 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.613 Gbps (per GPU), 484.901 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.337 Gbps (per GPU), 482.699 Gbps (aggregated)
The layer-level communication performance: 60.328 Gbps (per GPU), 482.623 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.080 Gbps (per GPU), 480.637 Gbps (aggregated)
The layer-level communication performance: 60.049 Gbps (per GPU), 480.395 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.816 Gbps (per GPU), 478.526 Gbps (aggregated)
The layer-level communication performance: 59.769 Gbps (per GPU), 478.149 Gbps (aggregated)
The layer-level communication performance: 59.731 Gbps (per GPU), 477.847 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.267 Gbps (per GPU), 1274.138 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.255 Gbps (per GPU), 1274.042 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.202 Gbps (per GPU), 1273.612 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.237 Gbps (per GPU), 1273.896 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.249 Gbps (per GPU), 1273.993 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.258 Gbps (per GPU), 1274.068 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.195 Gbps (per GPU), 1273.558 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.252 Gbps (per GPU), 1274.018 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.041 Gbps (per GPU), 832.327 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.040 Gbps (per GPU), 832.320 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.043 Gbps (per GPU), 832.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.041 Gbps (per GPU), 832.327 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.043 Gbps (per GPU), 832.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.036 Gbps (per GPU), 832.286 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.043 Gbps (per GPU), 832.341 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.037 Gbps (per GPU), 832.293 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.962 Gbps (per GPU), 311.696 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.962 Gbps (per GPU), 311.699 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.958 Gbps (per GPU), 311.665 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.956 Gbps (per GPU), 311.651 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.959 Gbps (per GPU), 311.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.952 Gbps (per GPU), 311.612 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.958 Gbps (per GPU), 311.660 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.952 Gbps (per GPU), 311.613 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.74ms  1.33ms  2.30ms  3.12 11.16K  0.12M
 chk_1  0.75ms  1.33ms  2.31ms  3.09 11.16K  0.11M
 chk_2  0.72ms  1.30ms  2.27ms  3.16 11.16K  0.11M
 chk_3  0.68ms  1.26ms  2.23ms  3.28 11.16K  0.12M
 chk_4  0.74ms  1.33ms  2.30ms  3.12 11.16K  0.11M
 chk_5  0.73ms  1.32ms  2.29ms  3.14 11.16K  0.10M
 chk_6  0.81ms  1.32ms  2.29ms  2.82 11.16K  0.12M
 chk_7  0.74ms  1.33ms  2.30ms  3.11 11.16K  0.11M
   Avg  0.74  1.31  2.29
   Max  0.81  1.33  2.31
   Min  0.68  1.26  2.23
 Ratio  1.19  1.06  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.470 s
*** Node 0, starting model training...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 229)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 229)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 229)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 229)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
Num Stages: 1 / 1
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 229)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 229)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 229)...
+++++++++ Node 0 initializing the weights for op[0, 229)...
+++++++++ Node 4 initializing the weights for op[0, 229)...
+++++++++ Node 5 initializing the weights for op[0, 229)...
+++++++++ Node 2 initializing the weights for op[0, 229)...
+++++++++ Node 7 initializing the weights for op[0, 229)...
+++++++++ Node 6 initializing the weights for op[0, 229)...
+++++++++ Node 3 initializing the weights for op[0, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5119	TrainAcc 0.4209	ValidAcc 0.4233	TestAcc 0.4230	BestValid 0.4233
	Epoch 50:	Loss 1.5505	TrainAcc 0.4726	ValidAcc 0.4745	TestAcc 0.4700	BestValid 0.4745
	Epoch 100:	Loss 1.4802	TrainAcc 0.4890	ValidAcc 0.4890	TestAcc 0.4858	BestValid 0.4890
	Epoch 150:	Loss 1.4523	TrainAcc 0.4931	ValidAcc 0.4931	TestAcc 0.4888	BestValid 0.4931
	Epoch 200:	Loss 1.4398	TrainAcc 0.4980	ValidAcc 0.4948	TestAcc 0.4930	BestValid 0.4948
	Epoch 250:	Loss 1.4471	TrainAcc 0.4992	ValidAcc 0.4959	TestAcc 0.4938	BestValid 0.4959
	Epoch 300:	Loss 1.4299	TrainAcc 0.5003	ValidAcc 0.4971	TestAcc 0.4933	BestValid 0.4971
	Epoch 350:	Loss 1.4137	TrainAcc 0.5094	ValidAcc 0.5007	TestAcc 0.5028	BestValid 0.5007
	Epoch 400:	Loss 1.4136	TrainAcc 0.5093	ValidAcc 0.5035	TestAcc 0.5024	BestValid 0.5035
	Epoch 450:	Loss 1.4050	TrainAcc 0.5099	ValidAcc 0.5038	TestAcc 0.5020	BestValid 0.5038
	Epoch 500:	Loss 1.3987	TrainAcc 0.5141	ValidAcc 0.5064	TestAcc 0.5058	BestValid 0.5064
	Epoch 550:	Loss 1.3984	TrainAcc 0.5154	ValidAcc 0.5068	TestAcc 0.5075	BestValid 0.5068
	Epoch 600:	Loss 1.3932	TrainAcc 0.5157	ValidAcc 0.5068	TestAcc 0.5063	BestValid 0.5068
	Epoch 650:	Loss 1.3850	TrainAcc 0.5199	ValidAcc 0.5095	TestAcc 0.5090	BestValid 0.5095
	Epoch 700:	Loss 1.3839	TrainAcc 0.5209	ValidAcc 0.5093	TestAcc 0.5097	BestValid 0.5095
	Epoch 750:	Loss 1.3797	TrainAcc 0.5211	ValidAcc 0.5086	TestAcc 0.5105	BestValid 0.5095
	Epoch 800:	Loss 1.3770	TrainAcc 0.5243	ValidAcc 0.5130	TestAcc 0.5113	BestValid 0.5130
	Epoch 850:	Loss 1.3806	TrainAcc 0.5232	ValidAcc 0.5106	TestAcc 0.5071	BestValid 0.5130
	Epoch 900:	Loss 1.3702	TrainAcc 0.5279	ValidAcc 0.5144	TestAcc 0.5135	BestValid 0.5144
	Epoch 950:	Loss 1.3682	TrainAcc 0.5263	ValidAcc 0.5112	TestAcc 0.5101	BestValid 0.5144
	Epoch 1000:	Loss 1.3621	TrainAcc 0.5307	ValidAcc 0.5159	TestAcc 0.5157	BestValid 0.5159
	Epoch 1050:	Loss 1.3631	TrainAcc 0.5290	ValidAcc 0.5147	TestAcc 0.5131	BestValid 0.5159
	Epoch 1100:	Loss 1.3594	TrainAcc 0.5319	ValidAcc 0.5158	TestAcc 0.5149	BestValid 0.5159
	Epoch 1150:	Loss 1.3569	TrainAcc 0.5341	ValidAcc 0.5173	TestAcc 0.5163	BestValid 0.5173
	Epoch 1200:	Loss 1.3566	TrainAcc 0.5357	ValidAcc 0.5186	TestAcc 0.5192	BestValid 0.5186
	Epoch 1250:	Loss 1.3500	TrainAcc 0.5364	ValidAcc 0.5174	TestAcc 0.5171	BestValid 0.5186
	Epoch 1300:	Loss 1.3473	TrainAcc 0.5351	ValidAcc 0.5152	TestAcc 0.5139	BestValid 0.5186
	Epoch 1350:	Loss 1.3459	TrainAcc 0.5398	ValidAcc 0.5177	TestAcc 0.5200	BestValid 0.5186
	Epoch 1400:	Loss 1.3584	TrainAcc 0.5347	ValidAcc 0.5112	TestAcc 0.5117	BestValid 0.5186
	Epoch 1450:	Loss 1.3397	TrainAcc 0.5413	ValidAcc 0.5186	TestAcc 0.5198	BestValid 0.5186
	Epoch 1500:	Loss 1.3401	TrainAcc 0.5397	ValidAcc 0.5165	TestAcc 0.5177	BestValid 0.5186
	Epoch 1550:	Loss 1.3438	TrainAcc 0.5271	ValidAcc 0.5051	TestAcc 0.5051	BestValid 0.5186
	Epoch 1600:	Loss 1.3359	TrainAcc 0.5402	ValidAcc 0.5162	TestAcc 0.5132	BestValid 0.5186
	Epoch 1650:	Loss 1.3293	TrainAcc 0.5468	ValidAcc 0.5221	TestAcc 0.5222	BestValid 0.5221
	Epoch 1700:	Loss 1.3291	TrainAcc 0.5446	ValidAcc 0.5214	TestAcc 0.5194	BestValid 0.5221
	Epoch 1750:	Loss 1.3311	TrainAcc 0.5463	ValidAcc 0.5209	TestAcc 0.5243	BestValid 0.5221
	Epoch 1800:	Loss 1.3274	TrainAcc 0.5501	ValidAcc 0.5216	TestAcc 0.5250	BestValid 0.5221
	Epoch 1850:	Loss 1.3219	TrainAcc 0.5514	ValidAcc 0.5239	TestAcc 0.5255	BestValid 0.5239
	Epoch 1900:	Loss 1.3250	TrainAcc 0.5505	ValidAcc 0.5229	TestAcc 0.5240	BestValid 0.5239
	Epoch 1950:	Loss 1.3191	TrainAcc 0.5516	ValidAcc 0.5226	TestAcc 0.5240	BestValid 0.5239
	Epoch 2000:	Loss 1.3154	TrainAcc 0.5541	ValidAcc 0.5243	TestAcc 0.5255	BestValid 0.5243
	Epoch 2050:	Loss 1.3185	TrainAcc 0.5525	ValidAcc 0.5212	TestAcc 0.5233	BestValid 0.5243
	Epoch 2100:	Loss 1.3195	TrainAcc 0.5512	ValidAcc 0.5202	TestAcc 0.5208	BestValid 0.5243
	Epoch 2150:	Loss 1.3135	TrainAcc 0.5538	ValidAcc 0.5216	TestAcc 0.5227	BestValid 0.5243
	Epoch 2200:	Loss 1.3095	TrainAcc 0.5567	ValidAcc 0.5241	TestAcc 0.5280	BestValid 0.5243
	Epoch 2250:	Loss 1.3103	TrainAcc 0.5546	ValidAcc 0.5223	TestAcc 0.5238	BestValid 0.5243
	Epoch 2300:	Loss 1.3069	TrainAcc 0.5578	ValidAcc 0.5226	TestAcc 0.5262	BestValid 0.5243
	Epoch 2350:	Loss 1.3081	TrainAcc 0.5557	ValidAcc 0.5226	TestAcc 0.5242	BestValid 0.5243
	Epoch 2400:	Loss 1.3139	TrainAcc 0.5534	ValidAcc 0.5219	TestAcc 0.5253	BestValid 0.5243
	Epoch 2450:	Loss 1.3066	TrainAcc 0.5520	ValidAcc 0.5180	TestAcc 0.5191	BestValid 0.5243
	Epoch 2500:	Loss 1.3005	TrainAcc 0.5600	ValidAcc 0.5228	TestAcc 0.5298	BestValid 0.5243
	Epoch 2550:	Loss 1.3078	TrainAcc 0.5618	ValidAcc 0.5245	TestAcc 0.5291	BestValid 0.5245
	Epoch 2600:	Loss 1.2973	TrainAcc 0.5635	ValidAcc 0.5251	TestAcc 0.5287	BestValid 0.5251
	Epoch 2650:	Loss 1.2953	TrainAcc 0.5623	ValidAcc 0.5261	TestAcc 0.5279	BestValid 0.5261
	Epoch 2700:	Loss 1.3000	TrainAcc 0.5627	ValidAcc 0.5252	TestAcc 0.5284	BestValid 0.5261
	Epoch 2750:	Loss 1.2968	TrainAcc 0.5637	ValidAcc 0.5271	TestAcc 0.5314	BestValid 0.5271
	Epoch 2800:	Loss 1.3025	TrainAcc 0.5526	ValidAcc 0.5125	TestAcc 0.5154	BestValid 0.5271
	Epoch 2850:	Loss 1.2977	TrainAcc 0.5652	ValidAcc 0.5244	TestAcc 0.5301	BestValid 0.5271
	Epoch 2900:	Loss 1.2883	TrainAcc 0.5663	ValidAcc 0.5253	TestAcc 0.5293	BestValid 0.5271
	Epoch 2950:	Loss 1.2924	TrainAcc 0.5677	ValidAcc 0.5254	TestAcc 0.5299	BestValid 0.5271
	Epoch 3000:	Loss 1.2833	TrainAcc 0.5658	ValidAcc 0.5260	TestAcc 0.5276	BestValid 0.5271
	Epoch 3050:	Loss 1.2872	TrainAcc 0.5678	ValidAcc 0.5234	TestAcc 0.5307	BestValid 0.5271
	Epoch 3100:	Loss 1.2850	TrainAcc 0.5686	ValidAcc 0.5270	TestAcc 0.5317	BestValid 0.5271
	Epoch 3150:	Loss 1.2818	TrainAcc 0.5699	ValidAcc 0.5277	TestAcc 0.5306	BestValid 0.5277
	Epoch 3200:	Loss 1.2803	TrainAcc 0.5686	ValidAcc 0.5247	TestAcc 0.5274	BestValid 0.5277
	Epoch 3250:	Loss 1.2868	TrainAcc 0.5707	ValidAcc 0.5233	TestAcc 0.5268	BestValid 0.5277
	Epoch 3300:	Loss 1.2804	TrainAcc 0.5709	ValidAcc 0.5251	TestAcc 0.5287	BestValid 0.5277
	Epoch 3350:	Loss 1.2834	TrainAcc 0.5744	ValidAcc 0.5274	TestAcc 0.5315	BestValid 0.5277
	Epoch 3400:	Loss 1.2886	TrainAcc 0.5718	ValidAcc 0.5261	TestAcc 0.5310	BestValid 0.5277
	Epoch 3450:	Loss 1.2759	TrainAcc 0.5724	ValidAcc 0.5269	TestAcc 0.5293	BestValid 0.5277
	Epoch 3500:	Loss 1.2757	TrainAcc 0.5748	ValidAcc 0.5266	TestAcc 0.5305	BestValid 0.5277
	Epoch 3550:	Loss 1.2726	TrainAcc 0.5754	ValidAcc 0.5252	TestAcc 0.5288	BestValid 0.5277
	Epoch 3600:	Loss 1.2684	TrainAcc 0.5774	ValidAcc 0.5279	TestAcc 0.5323	BestValid 0.5279
	Epoch 3650:	Loss 1.2731	TrainAcc 0.5743	ValidAcc 0.5263	TestAcc 0.5277	BestValid 0.5279
	Epoch 3700:	Loss 1.2754	TrainAcc 0.5711	ValidAcc 0.5225	TestAcc 0.5249	BestValid 0.5279
	Epoch 3750:	Loss 1.2744	TrainAcc 0.5676	ValidAcc 0.5190	TestAcc 0.5208	BestValid 0.5279
	Epoch 3800:	Loss 1.2712	TrainAcc 0.5775	ValidAcc 0.5248	TestAcc 0.5280	BestValid 0.5279
	Epoch 3850:	Loss 1.2610	TrainAcc 0.5811	ValidAcc 0.5277	TestAcc 0.5309	BestValid 0.5279
	Epoch 3900:	Loss 1.2663	TrainAcc 0.5781	ValidAcc 0.5234	TestAcc 0.5273	BestValid 0.5279
	Epoch 3950:	Loss 1.2624	TrainAcc 0.5802	ValidAcc 0.5270	TestAcc 0.5310	BestValid 0.5279
	Epoch 4000:	Loss 1.2612	TrainAcc 0.5822	ValidAcc 0.5288	TestAcc 0.5309	BestValid 0.5288
	Epoch 4050:	Loss 1.2613	TrainAcc 0.5815	ValidAcc 0.5274	TestAcc 0.5287	BestValid 0.5288
	Epoch 4100:	Loss 1.2671	TrainAcc 0.5781	ValidAcc 0.5267	TestAcc 0.5299	BestValid 0.5288
	Epoch 4150:	Loss 1.2544	TrainAcc 0.5823	ValidAcc 0.5269	TestAcc 0.5306	BestValid 0.5288
	Epoch 4200:	Loss 1.2579	TrainAcc 0.5814	ValidAcc 0.5234	TestAcc 0.5268	BestValid 0.5288
	Epoch 4250:	Loss 1.2714	TrainAcc 0.5828	ValidAcc 0.5294	TestAcc 0.5334	BestValid 0.5294
	Epoch 4300:	Loss 1.2705	TrainAcc 0.5634	ValidAcc 0.5242	TestAcc 0.5253	BestValid 0.5294
	Epoch 4350:	Loss 1.2525	TrainAcc 0.5837	ValidAcc 0.5289	TestAcc 0.5318	BestValid 0.5294
	Epoch 4400:	Loss 1.2493	TrainAcc 0.5871	ValidAcc 0.5272	TestAcc 0.5309	BestValid 0.5294
	Epoch 4450:	Loss 1.2465	TrainAcc 0.5878	ValidAcc 0.5288	TestAcc 0.5322	BestValid 0.5294
	Epoch 4500:	Loss 1.2528	TrainAcc 0.5878	ValidAcc 0.5291	TestAcc 0.5329	BestValid 0.5294
	Epoch 4550:	Loss 1.2501	TrainAcc 0.5848	ValidAcc 0.5320	TestAcc 0.5348	BestValid 0.5320
	Epoch 4600:	Loss 1.2488	TrainAcc 0.5800	ValidAcc 0.5211	TestAcc 0.5238	BestValid 0.5320
	Epoch 4650:	Loss 1.2432	TrainAcc 0.5900	ValidAcc 0.5299	TestAcc 0.5339	BestValid 0.5320
	Epoch 4700:	Loss 1.2438	TrainAcc 0.5909	ValidAcc 0.5303	TestAcc 0.5335	BestValid 0.5320
	Epoch 4750:	Loss 1.2432	TrainAcc 0.5866	ValidAcc 0.5266	TestAcc 0.5295	BestValid 0.5320
	Epoch 4800:	Loss 1.2404	TrainAcc 0.5861	ValidAcc 0.5267	TestAcc 0.5297	BestValid 0.5320
	Epoch 4850:	Loss 1.2496	TrainAcc 0.5900	ValidAcc 0.5252	TestAcc 0.5303	BestValid 0.5320
	Epoch 4900:	Loss 1.2387	TrainAcc 0.5926	ValidAcc 0.5282	TestAcc 0.5319	BestValid 0.5320
	Epoch 4950:	Loss 1.2413	TrainAcc 0.5900	ValidAcc 0.5324	TestAcc 0.5378	BestValid 0.5324
	Epoch 5000:	Loss 1.2490	TrainAcc 0.5890	ValidAcc 0.5263	TestAcc 0.5292	BestValid 0.5324
****** Epoch Time (Excluding Evaluation Cost): 0.165 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.181 ms (Max: 0.315, Min: 0.033, Sum: 1.450)
Cluster-Wide Average, Compute: 38.270 ms (Max: 39.194, Min: 36.586, Sum: 306.156)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.061)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.119)
Cluster-Wide Average, Communication-Graph: 123.608 ms (Max: 125.207, Min: 122.548, Sum: 988.865)
Cluster-Wide Average, Optimization: 2.728 ms (Max: 2.746, Min: 2.710, Sum: 21.824)
Cluster-Wide Average, Others: 0.443 ms (Max: 0.474, Min: 0.415, Sum: 3.542)
****** Breakdown Sum: 165.252 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.924 GB (Max: 4.399, Min: 3.842, Sum: 31.390)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.740 Gbps (Max: 51.858, Min: 16.786, Sum: 341.919)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 4.603 GB
****** Accuracy Results ******
Highest valid_acc: 0.5324
Target test_acc: 0.5378
Epoch to reach the target acc: 4949
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
