Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
DONE MPI INIT
Initialized node 0 on machine gnerv2
DONE MPI INITDONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2

Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.030 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
        It takes 0.022 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.019 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.028 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.115 seconds.
Building the Label Vector...
        It takes 0.114 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.114 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.276 Gbps (per GPU), 482.207 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.985 Gbps (per GPU), 479.883 Gbps (aggregated)
The layer-level communication performance: 59.980 Gbps (per GPU), 479.839 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.727 Gbps (per GPU), 477.815 Gbps (aggregated)
The layer-level communication performance: 59.699 Gbps (per GPU), 477.590 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.482 Gbps (per GPU), 475.856 Gbps (aggregated)
The layer-level communication performance: 59.436 Gbps (per GPU), 475.488 Gbps (aggregated)
The layer-level communication performance: 59.404 Gbps (per GPU), 475.229 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.285 Gbps (per GPU), 1250.282 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.282 Gbps (per GPU), 1250.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.239 Gbps (per GPU), 1249.909 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.280 Gbps (per GPU), 1250.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.291 Gbps (per GPU), 1250.329 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.282 Gbps (per GPU), 1250.256 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.244 Gbps (per GPU), 1249.956 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.279 Gbps (per GPU), 1250.235 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.746 Gbps (per GPU), 805.970 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.744 Gbps (per GPU), 805.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.748 Gbps (per GPU), 805.983 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.746 Gbps (per GPU), 805.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.750 Gbps (per GPU), 805.996 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.685 Gbps (per GPU), 805.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.747 Gbps (per GPU), 805.977 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.630 Gbps (per GPU), 805.042 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.649 Gbps (per GPU), 269.192 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.651 Gbps (per GPU), 269.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.649 Gbps (per GPU), 269.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.648 Gbps (per GPU), 269.184 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.649 Gbps (per GPU), 269.192 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.644 Gbps (per GPU), 269.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.643 Gbps (per GPU), 269.142 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.641 Gbps (per GPU), 269.132 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.05ms  1.74ms  2.87ms  2.72 11.16K  0.12M
 chk_1  1.04ms  1.75ms  2.88ms  2.76 11.16K  0.11M
 chk_2  1.01ms  1.72ms  2.84ms  2.81 11.16K  0.11M
 chk_3  0.94ms  1.68ms  2.80ms  2.97 11.16K  0.12M
 chk_4  1.01ms  1.74ms  2.87ms  2.84 11.16K  0.11M
 chk_5  1.00ms  1.73ms  2.86ms  2.86 11.16K  0.10M
 chk_6  1.01ms  1.74ms  2.86ms  2.85 11.16K  0.12M
 chk_7  1.01ms  1.75ms  2.87ms  2.84 11.16K  0.11M
   Avg  1.01  1.73  2.86
   Max  1.05  1.75  2.88
   Min  0.94  1.68  2.80
 Ratio  1.12  1.04  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.603 s
*** Node 0, starting model training...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 421)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 421)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 421)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 421)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 421)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 421)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 421)...
+++++++++ Node 7 initializing the weights for op[0, 421)...
+++++++++ Node 4 initializing the weights for op[0, 421)...
+++++++++ Node 5 initializing the weights for op[0, 421)...
+++++++++ Node 1 initializing the weights for op[0, 421)...
+++++++++ Node 2 initializing the weights for op[0, 421)...
+++++++++ Node 6 initializing the weights for op[0, 421)...
+++++++++ Node 3 initializing the weights for op[0, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2862	TrainAcc 0.4064	ValidAcc 0.4070	TestAcc 0.4038	BestValid 0.4070
	Epoch 50:	Loss 1.5535	TrainAcc 0.4654	ValidAcc 0.4617	TestAcc 0.4643	BestValid 0.4617
	Epoch 100:	Loss 1.4506	TrainAcc 0.5072	ValidAcc 0.5061	TestAcc 0.5075	BestValid 0.5061
	Epoch 150:	Loss 1.4090	TrainAcc 0.5183	ValidAcc 0.5125	TestAcc 0.5147	BestValid 0.5125
	Epoch 200:	Loss 1.3910	TrainAcc 0.5236	ValidAcc 0.5170	TestAcc 0.5185	BestValid 0.5170
	Epoch 250:	Loss 1.3780	TrainAcc 0.5283	ValidAcc 0.5184	TestAcc 0.5233	BestValid 0.5184
	Epoch 300:	Loss 1.3712	TrainAcc 0.5291	ValidAcc 0.5183	TestAcc 0.5223	BestValid 0.5184
	Epoch 350:	Loss 1.3543	TrainAcc 0.5329	ValidAcc 0.5198	TestAcc 0.5236	BestValid 0.5198
	Epoch 400:	Loss 1.3477	TrainAcc 0.5375	ValidAcc 0.5216	TestAcc 0.5250	BestValid 0.5216
	Epoch 450:	Loss 1.3374	TrainAcc 0.5403	ValidAcc 0.5256	TestAcc 0.5275	BestValid 0.5256
	Epoch 500:	Loss 1.3350	TrainAcc 0.5426	ValidAcc 0.5270	TestAcc 0.5299	BestValid 0.5270
	Epoch 550:	Loss 1.3277	TrainAcc 0.5432	ValidAcc 0.5294	TestAcc 0.5305	BestValid 0.5294
	Epoch 600:	Loss 1.3347	TrainAcc 0.5473	ValidAcc 0.5284	TestAcc 0.5291	BestValid 0.5294
	Epoch 650:	Loss 1.3196	TrainAcc 0.5466	ValidAcc 0.5279	TestAcc 0.5299	BestValid 0.5294
	Epoch 700:	Loss 1.3105	TrainAcc 0.5493	ValidAcc 0.5293	TestAcc 0.5316	BestValid 0.5294
	Epoch 750:	Loss 1.3112	TrainAcc 0.5504	ValidAcc 0.5300	TestAcc 0.5331	BestValid 0.5300
	Epoch 800:	Loss 1.2985	TrainAcc 0.5554	ValidAcc 0.5283	TestAcc 0.5306	BestValid 0.5300
	Epoch 850:	Loss 1.2974	TrainAcc 0.5544	ValidAcc 0.5302	TestAcc 0.5343	BestValid 0.5302
	Epoch 900:	Loss 1.2912	TrainAcc 0.5599	ValidAcc 0.5302	TestAcc 0.5326	BestValid 0.5302
	Epoch 950:	Loss 1.2877	TrainAcc 0.5632	ValidAcc 0.5309	TestAcc 0.5325	BestValid 0.5309
	Epoch 1000:	Loss 1.2827	TrainAcc 0.5646	ValidAcc 0.5345	TestAcc 0.5361	BestValid 0.5345
	Epoch 1050:	Loss 1.2782	TrainAcc 0.5599	ValidAcc 0.5265	TestAcc 0.5287	BestValid 0.5345
	Epoch 1100:	Loss 1.2776	TrainAcc 0.5692	ValidAcc 0.5337	TestAcc 0.5367	BestValid 0.5345
