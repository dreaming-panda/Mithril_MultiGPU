Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
DONE MPI INIT
Initialized node 0 on machine gnerv2
DONE MPI INITDONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2

Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.030 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
        It takes 0.022 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.019 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.028 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.115 seconds.
Building the Label Vector...
        It takes 0.114 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.114 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.276 Gbps (per GPU), 482.207 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.985 Gbps (per GPU), 479.883 Gbps (aggregated)
The layer-level communication performance: 59.980 Gbps (per GPU), 479.839 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.727 Gbps (per GPU), 477.815 Gbps (aggregated)
The layer-level communication performance: 59.699 Gbps (per GPU), 477.590 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.482 Gbps (per GPU), 475.856 Gbps (aggregated)
The layer-level communication performance: 59.436 Gbps (per GPU), 475.488 Gbps (aggregated)
The layer-level communication performance: 59.404 Gbps (per GPU), 475.229 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.285 Gbps (per GPU), 1250.282 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.282 Gbps (per GPU), 1250.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.239 Gbps (per GPU), 1249.909 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.280 Gbps (per GPU), 1250.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.291 Gbps (per GPU), 1250.329 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.282 Gbps (per GPU), 1250.256 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.244 Gbps (per GPU), 1249.956 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.279 Gbps (per GPU), 1250.235 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.746 Gbps (per GPU), 805.970 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.744 Gbps (per GPU), 805.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.748 Gbps (per GPU), 805.983 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.746 Gbps (per GPU), 805.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.750 Gbps (per GPU), 805.996 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.685 Gbps (per GPU), 805.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.747 Gbps (per GPU), 805.977 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.630 Gbps (per GPU), 805.042 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.649 Gbps (per GPU), 269.192 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.651 Gbps (per GPU), 269.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.649 Gbps (per GPU), 269.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.648 Gbps (per GPU), 269.184 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.649 Gbps (per GPU), 269.192 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.644 Gbps (per GPU), 269.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.643 Gbps (per GPU), 269.142 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.641 Gbps (per GPU), 269.132 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.05ms  1.74ms  2.87ms  2.72 11.16K  0.12M
 chk_1  1.04ms  1.75ms  2.88ms  2.76 11.16K  0.11M
 chk_2  1.01ms  1.72ms  2.84ms  2.81 11.16K  0.11M
 chk_3  0.94ms  1.68ms  2.80ms  2.97 11.16K  0.12M
 chk_4  1.01ms  1.74ms  2.87ms  2.84 11.16K  0.11M
 chk_5  1.00ms  1.73ms  2.86ms  2.86 11.16K  0.10M
 chk_6  1.01ms  1.74ms  2.86ms  2.85 11.16K  0.12M
 chk_7  1.01ms  1.75ms  2.87ms  2.84 11.16K  0.11M
   Avg  1.01  1.73  2.86
   Max  1.05  1.75  2.88
   Min  0.94  1.68  2.80
 Ratio  1.12  1.04  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.603 s
*** Node 0, starting model training...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 421)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 421)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 421)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 421)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 421)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 421)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 421)...
+++++++++ Node 7 initializing the weights for op[0, 421)...
+++++++++ Node 4 initializing the weights for op[0, 421)...
+++++++++ Node 5 initializing the weights for op[0, 421)...
+++++++++ Node 1 initializing the weights for op[0, 421)...
+++++++++ Node 2 initializing the weights for op[0, 421)...
+++++++++ Node 6 initializing the weights for op[0, 421)...
+++++++++ Node 3 initializing the weights for op[0, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2862	TrainAcc 0.4064	ValidAcc 0.4070	TestAcc 0.4038	BestValid 0.4070
	Epoch 50:	Loss 1.5535	TrainAcc 0.4654	ValidAcc 0.4617	TestAcc 0.4643	BestValid 0.4617
	Epoch 100:	Loss 1.4506	TrainAcc 0.5072	ValidAcc 0.5061	TestAcc 0.5075	BestValid 0.5061
	Epoch 150:	Loss 1.4090	TrainAcc 0.5183	ValidAcc 0.5125	TestAcc 0.5147	BestValid 0.5125
	Epoch 200:	Loss 1.3910	TrainAcc 0.5236	ValidAcc 0.5170	TestAcc 0.5185	BestValid 0.5170
	Epoch 250:	Loss 1.3780	TrainAcc 0.5283	ValidAcc 0.5184	TestAcc 0.5233	BestValid 0.5184
	Epoch 300:	Loss 1.3712	TrainAcc 0.5291	ValidAcc 0.5183	TestAcc 0.5223	BestValid 0.5184
	Epoch 350:	Loss 1.3543	TrainAcc 0.5329	ValidAcc 0.5198	TestAcc 0.5236	BestValid 0.5198
	Epoch 400:	Loss 1.3477	TrainAcc 0.5375	ValidAcc 0.5216	TestAcc 0.5250	BestValid 0.5216
	Epoch 450:	Loss 1.3374	TrainAcc 0.5403	ValidAcc 0.5256	TestAcc 0.5275	BestValid 0.5256
	Epoch 500:	Loss 1.3350	TrainAcc 0.5426	ValidAcc 0.5270	TestAcc 0.5299	BestValid 0.5270
	Epoch 550:	Loss 1.3277	TrainAcc 0.5432	ValidAcc 0.5294	TestAcc 0.5305	BestValid 0.5294
	Epoch 600:	Loss 1.3347	TrainAcc 0.5473	ValidAcc 0.5284	TestAcc 0.5291	BestValid 0.5294
	Epoch 650:	Loss 1.3196	TrainAcc 0.5466	ValidAcc 0.5279	TestAcc 0.5299	BestValid 0.5294
	Epoch 700:	Loss 1.3105	TrainAcc 0.5493	ValidAcc 0.5293	TestAcc 0.5316	BestValid 0.5294
	Epoch 750:	Loss 1.3112	TrainAcc 0.5504	ValidAcc 0.5300	TestAcc 0.5331	BestValid 0.5300
	Epoch 800:	Loss 1.2985	TrainAcc 0.5554	ValidAcc 0.5283	TestAcc 0.5306	BestValid 0.5300
	Epoch 850:	Loss 1.2974	TrainAcc 0.5544	ValidAcc 0.5302	TestAcc 0.5343	BestValid 0.5302
	Epoch 900:	Loss 1.2912	TrainAcc 0.5599	ValidAcc 0.5302	TestAcc 0.5326	BestValid 0.5302
	Epoch 950:	Loss 1.2877	TrainAcc 0.5632	ValidAcc 0.5309	TestAcc 0.5325	BestValid 0.5309
	Epoch 1000:	Loss 1.2827	TrainAcc 0.5646	ValidAcc 0.5345	TestAcc 0.5361	BestValid 0.5345
	Epoch 1050:	Loss 1.2782	TrainAcc 0.5599	ValidAcc 0.5265	TestAcc 0.5287	BestValid 0.5345
	Epoch 1100:	Loss 1.2776	TrainAcc 0.5692	ValidAcc 0.5337	TestAcc 0.5367	BestValid 0.5345
	Epoch 1150:	Loss 1.2678	TrainAcc 0.5699	ValidAcc 0.5317	TestAcc 0.5350	BestValid 0.5345
	Epoch 1200:	Loss 1.2709	TrainAcc 0.5708	ValidAcc 0.5303	TestAcc 0.5337	BestValid 0.5345
	Epoch 1250:	Loss 1.2668	TrainAcc 0.5718	ValidAcc 0.5331	TestAcc 0.5344	BestValid 0.5345
	Epoch 1300:	Loss 1.2591	TrainAcc 0.5647	ValidAcc 0.5205	TestAcc 0.5241	BestValid 0.5345
	Epoch 1350:	Loss 1.2531	TrainAcc 0.5737	ValidAcc 0.5299	TestAcc 0.5313	BestValid 0.5345
	Epoch 1400:	Loss 1.2536	TrainAcc 0.5778	ValidAcc 0.5314	TestAcc 0.5340	BestValid 0.5345
	Epoch 1450:	Loss 1.2471	TrainAcc 0.5812	ValidAcc 0.5354	TestAcc 0.5377	BestValid 0.5354
	Epoch 1500:	Loss 1.2540	TrainAcc 0.5766	ValidAcc 0.5298	TestAcc 0.5328	BestValid 0.5354
	Epoch 1550:	Loss 1.2390	TrainAcc 0.5827	ValidAcc 0.5322	TestAcc 0.5359	BestValid 0.5354
	Epoch 1600:	Loss 1.2358	TrainAcc 0.5829	ValidAcc 0.5339	TestAcc 0.5366	BestValid 0.5354
	Epoch 1650:	Loss 1.2366	TrainAcc 0.5816	ValidAcc 0.5294	TestAcc 0.5316	BestValid 0.5354
	Epoch 1700:	Loss 1.2319	TrainAcc 0.5828	ValidAcc 0.5277	TestAcc 0.5313	BestValid 0.5354
	Epoch 1750:	Loss 1.2263	TrainAcc 0.5897	ValidAcc 0.5342	TestAcc 0.5370	BestValid 0.5354
	Epoch 1800:	Loss 1.2242	TrainAcc 0.5877	ValidAcc 0.5333	TestAcc 0.5344	BestValid 0.5354
	Epoch 1850:	Loss 1.2197	TrainAcc 0.5787	ValidAcc 0.5174	TestAcc 0.5203	BestValid 0.5354
	Epoch 1900:	Loss 1.2130	TrainAcc 0.5952	ValidAcc 0.5312	TestAcc 0.5343	BestValid 0.5354
	Epoch 1950:	Loss 1.2141	TrainAcc 0.5902	ValidAcc 0.5242	TestAcc 0.5278	BestValid 0.5354
	Epoch 2000:	Loss 1.2095	TrainAcc 0.5968	ValidAcc 0.5309	TestAcc 0.5357	BestValid 0.5354
	Epoch 2050:	Loss 1.2102	TrainAcc 0.5985	ValidAcc 0.5327	TestAcc 0.5348	BestValid 0.5354
	Epoch 2100:	Loss 1.2032	TrainAcc 0.6007	ValidAcc 0.5312	TestAcc 0.5360	BestValid 0.5354
	Epoch 2150:	Loss 1.1972	TrainAcc 0.5984	ValidAcc 0.5261	TestAcc 0.5311	BestValid 0.5354
	Epoch 2200:	Loss 1.1962	TrainAcc 0.5905	ValidAcc 0.5197	TestAcc 0.5267	BestValid 0.5354
	Epoch 2250:	Loss 1.1950	TrainAcc 0.6032	ValidAcc 0.5330	TestAcc 0.5364	BestValid 0.5354
	Epoch 2300:	Loss 1.2248	TrainAcc 0.5859	ValidAcc 0.5221	TestAcc 0.5216	BestValid 0.5354
	Epoch 2350:	Loss 1.1869	TrainAcc 0.6055	ValidAcc 0.5289	TestAcc 0.5315	BestValid 0.5354
	Epoch 2400:	Loss 1.1844	TrainAcc 0.6080	ValidAcc 0.5281	TestAcc 0.5355	BestValid 0.5354
	Epoch 2450:	Loss 1.1796	TrainAcc 0.6083	ValidAcc 0.5284	TestAcc 0.5306	BestValid 0.5354
	Epoch 2500:	Loss 1.1744	TrainAcc 0.6077	ValidAcc 0.5281	TestAcc 0.5337	BestValid 0.5354
	Epoch 2550:	Loss 1.1709	TrainAcc 0.6138	ValidAcc 0.5321	TestAcc 0.5357	BestValid 0.5354
	Epoch 2600:	Loss 1.1683	TrainAcc 0.6092	ValidAcc 0.5240	TestAcc 0.5288	BestValid 0.5354
	Epoch 2650:	Loss 1.1693	TrainAcc 0.6058	ValidAcc 0.5213	TestAcc 0.5249	BestValid 0.5354
	Epoch 2700:	Loss 1.1709	TrainAcc 0.6173	ValidAcc 0.5266	TestAcc 0.5331	BestValid 0.5354
	Epoch 2750:	Loss 1.1624	TrainAcc 0.6164	ValidAcc 0.5265	TestAcc 0.5308	BestValid 0.5354
	Epoch 2800:	Loss 1.1654	TrainAcc 0.6203	ValidAcc 0.5310	TestAcc 0.5374	BestValid 0.5354
	Epoch 2850:	Loss 1.1513	TrainAcc 0.6221	ValidAcc 0.5264	TestAcc 0.5324	BestValid 0.5354
	Epoch 2900:	Loss 1.1494	TrainAcc 0.6208	ValidAcc 0.5298	TestAcc 0.5343	BestValid 0.5354
	Epoch 2950:	Loss 1.1480	TrainAcc 0.6183	ValidAcc 0.5204	TestAcc 0.5268	BestValid 0.5354
	Epoch 3000:	Loss 1.1392	TrainAcc 0.6192	ValidAcc 0.5221	TestAcc 0.5271	BestValid 0.5354
	Epoch 3050:	Loss 1.1472	TrainAcc 0.6275	ValidAcc 0.5297	TestAcc 0.5358	BestValid 0.5354
	Epoch 3100:	Loss 1.1507	TrainAcc 0.6270	ValidAcc 0.5232	TestAcc 0.5298	BestValid 0.5354
	Epoch 3150:	Loss 1.1445	TrainAcc 0.6315	ValidAcc 0.5294	TestAcc 0.5341	BestValid 0.5354
	Epoch 3200:	Loss 1.1306	TrainAcc 0.6320	ValidAcc 0.5259	TestAcc 0.5299	BestValid 0.5354
	Epoch 3250:	Loss 1.1290	TrainAcc 0.6312	ValidAcc 0.5212	TestAcc 0.5261	BestValid 0.5354
	Epoch 3300:	Loss 1.1263	TrainAcc 0.6355	ValidAcc 0.5284	TestAcc 0.5336	BestValid 0.5354
	Epoch 3350:	Loss 1.1361	TrainAcc 0.6298	ValidAcc 0.5156	TestAcc 0.5259	BestValid 0.5354
	Epoch 3400:	Loss 1.1188	TrainAcc 0.6374	ValidAcc 0.5212	TestAcc 0.5286	BestValid 0.5354
	Epoch 3450:	Loss 1.1211	TrainAcc 0.6400	ValidAcc 0.5228	TestAcc 0.5283	BestValid 0.5354
	Epoch 3500:	Loss 1.1125	TrainAcc 0.6400	ValidAcc 0.5224	TestAcc 0.5285	BestValid 0.5354
	Epoch 3550:	Loss 1.1062	TrainAcc 0.6423	ValidAcc 0.5222	TestAcc 0.5277	BestValid 0.5354
	Epoch 3600:	Loss 1.1079	TrainAcc 0.6416	ValidAcc 0.5222	TestAcc 0.5267	BestValid 0.5354
	Epoch 3650:	Loss 1.0999	TrainAcc 0.6463	ValidAcc 0.5264	TestAcc 0.5327	BestValid 0.5354
	Epoch 3700:	Loss 1.0948	TrainAcc 0.6461	ValidAcc 0.5216	TestAcc 0.5299	BestValid 0.5354
	Epoch 3750:	Loss 1.0946	TrainAcc 0.6466	ValidAcc 0.5269	TestAcc 0.5332	BestValid 0.5354
	Epoch 3800:	Loss 1.0924	TrainAcc 0.6503	ValidAcc 0.5203	TestAcc 0.5275	BestValid 0.5354
	Epoch 3850:	Loss 1.0954	TrainAcc 0.6493	ValidAcc 0.5214	TestAcc 0.5279	BestValid 0.5354
	Epoch 3900:	Loss 1.0834	TrainAcc 0.6487	ValidAcc 0.5195	TestAcc 0.5221	BestValid 0.5354
	Epoch 3950:	Loss 1.0759	TrainAcc 0.6547	ValidAcc 0.5183	TestAcc 0.5240	BestValid 0.5354
	Epoch 4000:	Loss 1.0828	TrainAcc 0.6445	ValidAcc 0.5087	TestAcc 0.5130	BestValid 0.5354
	Epoch 4050:	Loss 1.0730	TrainAcc 0.6592	ValidAcc 0.5204	TestAcc 0.5267	BestValid 0.5354
	Epoch 4100:	Loss 1.0701	TrainAcc 0.6555	ValidAcc 0.5167	TestAcc 0.5201	BestValid 0.5354
	Epoch 4150:	Loss 1.0628	TrainAcc 0.6592	ValidAcc 0.5211	TestAcc 0.5260	BestValid 0.5354
	Epoch 4200:	Loss 1.0639	TrainAcc 0.6587	ValidAcc 0.5176	TestAcc 0.5270	BestValid 0.5354
	Epoch 4250:	Loss 1.0698	TrainAcc 0.6628	ValidAcc 0.5188	TestAcc 0.5254	BestValid 0.5354
	Epoch 4300:	Loss 1.0591	TrainAcc 0.6663	ValidAcc 0.5177	TestAcc 0.5225	BestValid 0.5354
	Epoch 4350:	Loss 1.0491	TrainAcc 0.6677	ValidAcc 0.5176	TestAcc 0.5244	BestValid 0.5354
	Epoch 4400:	Loss 1.0462	TrainAcc 0.6683	ValidAcc 0.5160	TestAcc 0.5216	BestValid 0.5354
	Epoch 4450:	Loss 1.0536	TrainAcc 0.6634	ValidAcc 0.5198	TestAcc 0.5269	BestValid 0.5354
	Epoch 4500:	Loss 1.0434	TrainAcc 0.6686	ValidAcc 0.5130	TestAcc 0.5200	BestValid 0.5354
	Epoch 4550:	Loss 1.0363	TrainAcc 0.6709	ValidAcc 0.5142	TestAcc 0.5199	BestValid 0.5354
	Epoch 4600:	Loss 1.0357	TrainAcc 0.6743	ValidAcc 0.5155	TestAcc 0.5236	BestValid 0.5354
	Epoch 4650:	Loss 1.0366	TrainAcc 0.6734	ValidAcc 0.5134	TestAcc 0.5190	BestValid 0.5354
	Epoch 4700:	Loss 1.0256	TrainAcc 0.6781	ValidAcc 0.5137	TestAcc 0.5203	BestValid 0.5354
	Epoch 4750:	Loss 1.0305	TrainAcc 0.6703	ValidAcc 0.5181	TestAcc 0.5231	BestValid 0.5354
	Epoch 4800:	Loss 1.0226	TrainAcc 0.6815	ValidAcc 0.5156	TestAcc 0.5227	BestValid 0.5354
	Epoch 4850:	Loss 1.0219	TrainAcc 0.6836	ValidAcc 0.5149	TestAcc 0.5205	BestValid 0.5354
	Epoch 4900:	Loss 1.0147	TrainAcc 0.6807	ValidAcc 0.5063	TestAcc 0.5112	BestValid 0.5354
	Epoch 4950:	Loss 1.0155	TrainAcc 0.6796	ValidAcc 0.5125	TestAcc 0.5178	BestValid 0.5354
	Epoch 5000:	Loss 1.0025	TrainAcc 0.6850	ValidAcc 0.5157	TestAcc 0.5221	BestValid 0.5354
****** Epoch Time (Excluding Evaluation Cost): 0.186 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.150 ms (Max: 0.292, Min: 0.035, Sum: 1.198)
Cluster-Wide Average, Compute: 52.361 ms (Max: 52.953, Min: 50.775, Sum: 418.891)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.123)
Cluster-Wide Average, Communication-Graph: 124.176 ms (Max: 125.717, Min: 123.403, Sum: 993.406)
Cluster-Wide Average, Optimization: 7.717 ms (Max: 7.771, Min: 7.674, Sum: 61.739)
Cluster-Wide Average, Others: 1.319 ms (Max: 1.397, Min: 1.199, Sum: 10.554)
****** Breakdown Sum: 185.747 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.057 GB (Max: 5.540, Min: 4.975, Sum: 40.460)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.529 Gbps (Max: 51.453, Min: 16.721, Sum: 340.234)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.036 GB
Total communication (cluster-wide, per-epoch): 4.620 GB
****** Accuracy Results ******
Highest valid_acc: 0.5354
Target test_acc: 0.5377
Epoch to reach the target acc: 1449
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
