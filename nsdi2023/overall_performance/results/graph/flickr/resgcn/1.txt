Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 3 on machine gnerv4
Initialized node 0 on machine gnerv4
DONE MPI INIT

Initialized node 1 on machine gnerv4
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.103 seconds.
Building the CSC structure...
        It takes 0.103 seconds.
Building the CSC structure...
        It takes 0.103 seconds.
Building the CSC structure...
        It takes 0.104 seconds.
Building the CSC structure...
        It takes 0.105 seconds.
Building the CSC structure...
        It takes 0.105 seconds.
Building the CSC structure...
        It takes 0.105 seconds.
Building the CSC structure...
        It takes 0.105 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
        It takes 0.022 seconds.
        It takes 0.022 seconds.
        It takes 0.022 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.297 seconds.
        It takes 0.297 seconds.
        It takes 0.297 seconds.
        It takes 0.297 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.018 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.018 seconds.
        It takes 0.018 seconds.
        It takes 0.019 seconds.
        It takes 0.318 seconds.
        It takes 0.318 seconds.
        It takes 0.318 seconds.
        It takes 0.318 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.009 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.579 Gbps (per GPU), 468.635 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.282 Gbps (per GPU), 466.259 Gbps (aggregated)
The layer-level communication performance: 58.273 Gbps (per GPU), 466.183 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.033 Gbps (per GPU), 464.267 Gbps (aggregated)
The layer-level communication performance: 58.003 Gbps (per GPU), 464.026 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.795 Gbps (per GPU), 462.364 Gbps (aggregated)
The layer-level communication performance: 57.753 Gbps (per GPU), 462.022 Gbps (aggregated)
The layer-level communication performance: 57.717 Gbps (per GPU), 461.733 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.859 Gbps (per GPU), 1262.875 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.891 Gbps (per GPU), 1263.131 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.921 Gbps (per GPU), 1263.368 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.868 Gbps (per GPU), 1262.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.868 Gbps (per GPU), 1262.940 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.885 Gbps (per GPU), 1263.084 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.936 Gbps (per GPU), 1263.487 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.755 Gbps (per GPU), 1262.038 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.177 Gbps (per GPU), 801.415 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.180 Gbps (per GPU), 801.440 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.172 Gbps (per GPU), 801.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.181 Gbps (per GPU), 801.447 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.180 Gbps (per GPU), 801.440 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.169 Gbps (per GPU), 801.350 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.170 Gbps (per GPU), 801.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.172 Gbps (per GPU), 801.376 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 40.223 Gbps (per GPU), 321.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.220 Gbps (per GPU), 321.762 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.222 Gbps (per GPU), 321.780 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.219 Gbps (per GPU), 321.753 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.220 Gbps (per GPU), 321.761 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.219 Gbps (per GPU), 321.750 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.214 Gbps (per GPU), 321.710 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.214 Gbps (per GPU), 321.713 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.91ms  1.43ms  1.57ms  1.33 11.16K  0.12M
 chk_1  1.91ms  1.44ms  1.58ms  1.32 11.16K  0.11M
 chk_2  1.91ms  1.41ms  1.55ms  1.36 11.16K  0.11M
 chk_3  1.84ms  1.37ms  1.51ms  1.34 11.16K  0.12M
 chk_4  1.91ms  1.44ms  1.58ms  1.33 11.16K  0.11M
 chk_5  1.89ms  1.43ms  1.57ms  1.33 11.16K  0.10M
 chk_6  1.89ms  1.43ms  1.57ms  1.33 11.16K  0.12M
 chk_7  1.90ms  1.44ms  1.58ms  1.32 11.16K  0.11M
   Avg  1.90  1.42  1.57
   Max  1.91  1.44  1.58
   Min  1.84  1.37  1.51
 Ratio  1.04  1.05  1.05
   Var  0.00  0.00  0.00
Profiling takes 0.519 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 6, starting model training...
*** Node 4, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 262)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 262)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 262)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 262)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 262)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 262)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 262)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 262)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 262)...
+++++++++ Node 2 initializing the weights for op[0, 262)...
+++++++++ Node 3 initializing the weights for op[0, 262)...
+++++++++ Node 1 initializing the weights for op[0, 262)...
+++++++++ Node 4 initializing the weights for op[0, 262)...
+++++++++ Node 5 initializing the weights for op[0, 262)...
+++++++++ Node 6 initializing the weights for op[0, 262)...
+++++++++ Node 7 initializing the weights for op[0, 262)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5405	TrainAcc 0.3941	ValidAcc 0.3960	TestAcc 0.3961	BestValid 0.3960
	Epoch 50:	Loss 1.6004	TrainAcc 0.4220	ValidAcc 0.4246	TestAcc 0.4239	BestValid 0.4246
	Epoch 100:	Loss 1.5790	TrainAcc 0.4218	ValidAcc 0.4243	TestAcc 0.4238	BestValid 0.4246
	Epoch 150:	Loss 1.5735	TrainAcc 0.4212	ValidAcc 0.4238	TestAcc 0.4233	BestValid 0.4246
	Epoch 200:	Loss 1.5702	TrainAcc 0.4216	ValidAcc 0.4237	TestAcc 0.4234	BestValid 0.4246
	Epoch 250:	Loss 1.5595	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4246
	Epoch 300:	Loss 1.5625	TrainAcc 0.4223	ValidAcc 0.4247	TestAcc 0.4243	BestValid 0.4247
	Epoch 350:	Loss 1.5593	TrainAcc 0.4251	ValidAcc 0.4279	TestAcc 0.4283	BestValid 0.4279
	Epoch 400:	Loss 1.5418	TrainAcc 0.3869	ValidAcc 0.3861	TestAcc 0.3839	BestValid 0.4279
	Epoch 450:	Loss 1.5378	TrainAcc 0.4568	ValidAcc 0.4593	TestAcc 0.4605	BestValid 0.4593
	Epoch 500:	Loss 1.5185	TrainAcc 0.4737	ValidAcc 0.4768	TestAcc 0.4721	BestValid 0.4768
	Epoch 550:	Loss 1.5085	TrainAcc 0.4684	ValidAcc 0.4717	TestAcc 0.4677	BestValid 0.4768
	Epoch 600:	Loss 1.4999	TrainAcc 0.4835	ValidAcc 0.4847	TestAcc 0.4814	BestValid 0.4847
	Epoch 650:	Loss 1.4969	TrainAcc 0.4860	ValidAcc 0.4890	TestAcc 0.4851	BestValid 0.4890
	Epoch 700:	Loss 1.4956	TrainAcc 0.4879	ValidAcc 0.4893	TestAcc 0.4870	BestValid 0.4893
	Epoch 750:	Loss 1.4924	TrainAcc 0.4858	ValidAcc 0.4882	TestAcc 0.4838	BestValid 0.4893
	Epoch 800:	Loss 1.4934	TrainAcc 0.4877	ValidAcc 0.4906	TestAcc 0.4864	BestValid 0.4906
	Epoch 850:	Loss 1.4872	TrainAcc 0.4905	ValidAcc 0.4914	TestAcc 0.4878	BestValid 0.4914
	Epoch 900:	Loss 1.4846	TrainAcc 0.4912	ValidAcc 0.4928	TestAcc 0.4890	BestValid 0.4928
	Epoch 950:	Loss 1.4858	TrainAcc 0.4914	ValidAcc 0.4935	TestAcc 0.4907	BestValid 0.4935
	Epoch 1000:	Loss 1.4831	TrainAcc 0.4915	ValidAcc 0.4938	TestAcc 0.4910	BestValid 0.4938
	Epoch 1050:	Loss 1.4839	TrainAcc 0.4912	ValidAcc 0.4939	TestAcc 0.4893	BestValid 0.4939
	Epoch 1100:	Loss 1.4919	TrainAcc 0.4930	ValidAcc 0.4958	TestAcc 0.4926	BestValid 0.4958
	Epoch 1150:	Loss 1.4805	TrainAcc 0.4952	ValidAcc 0.4965	TestAcc 0.4945	BestValid 0.4965
	Epoch 1200:	Loss 1.4762	TrainAcc 0.4937	ValidAcc 0.4961	TestAcc 0.4918	BestValid 0.4965
	Epoch 1250:	Loss 1.4743	TrainAcc 0.4963	ValidAcc 0.4992	TestAcc 0.4951	BestValid 0.4992
	Epoch 1300:	Loss 1.4714	TrainAcc 0.4967	ValidAcc 0.4986	TestAcc 0.4955	BestValid 0.4992
	Epoch 1350:	Loss 1.4735	TrainAcc 0.4980	ValidAcc 0.4997	TestAcc 0.4981	BestValid 0.4997
	Epoch 1400:	Loss 1.4664	TrainAcc 0.4997	ValidAcc 0.5013	TestAcc 0.4985	BestValid 0.5013
	Epoch 1450:	Loss 1.4659	TrainAcc 0.5018	ValidAcc 0.5016	TestAcc 0.5009	BestValid 0.5016
	Epoch 1500:	Loss 1.4595	TrainAcc 0.5037	ValidAcc 0.5031	TestAcc 0.5026	BestValid 0.5031
	Epoch 1550:	Loss 1.4552	TrainAcc 0.5032	ValidAcc 0.5026	TestAcc 0.5026	BestValid 0.5031
	Epoch 1600:	Loss 1.4547	TrainAcc 0.5052	ValidAcc 0.5020	TestAcc 0.5043	BestValid 0.5031
	Epoch 1650:	Loss 1.4493	TrainAcc 0.5050	ValidAcc 0.5032	TestAcc 0.5031	BestValid 0.5032
	Epoch 1700:	Loss 1.4850	TrainAcc 0.5056	ValidAcc 0.5031	TestAcc 0.5031	BestValid 0.5032
	Epoch 1750:	Loss 1.4430	TrainAcc 0.5075	ValidAcc 0.5049	TestAcc 0.5040	BestValid 0.5049
	Epoch 1800:	Loss 1.4401	TrainAcc 0.5097	ValidAcc 0.5078	TestAcc 0.5057	BestValid 0.5078
	Epoch 1850:	Loss 1.4354	TrainAcc 0.5085	ValidAcc 0.5056	TestAcc 0.5028	BestValid 0.5078
	Epoch 1900:	Loss 1.4319	TrainAcc 0.5121	ValidAcc 0.5096	TestAcc 0.5069	BestValid 0.5096
	Epoch 1950:	Loss 1.4279	TrainAcc 0.5122	ValidAcc 0.5078	TestAcc 0.5077	BestValid 0.5096
	Epoch 2000:	Loss 1.4257	TrainAcc 0.5129	ValidAcc 0.5111	TestAcc 0.5086	BestValid 0.5111
	Epoch 2050:	Loss 1.4234	TrainAcc 0.5141	ValidAcc 0.5110	TestAcc 0.5086	BestValid 0.5111
	Epoch 2100:	Loss 1.4217	TrainAcc 0.5145	ValidAcc 0.5114	TestAcc 0.5092	BestValid 0.5114
	Epoch 2150:	Loss 1.4228	TrainAcc 0.5149	ValidAcc 0.5090	TestAcc 0.5094	BestValid 0.5114
	Epoch 2200:	Loss 1.4209	TrainAcc 0.5156	ValidAcc 0.5122	TestAcc 0.5097	BestValid 0.5122
	Epoch 2250:	Loss 1.4185	TrainAcc 0.5161	ValidAcc 0.5094	TestAcc 0.5102	BestValid 0.5122
	Epoch 2300:	Loss 1.4183	TrainAcc 0.5158	ValidAcc 0.5086	TestAcc 0.5093	BestValid 0.5122
	Epoch 2350:	Loss 1.4155	TrainAcc 0.5184	ValidAcc 0.5135	TestAcc 0.5104	BestValid 0.5135
	Epoch 2400:	Loss 1.4215	TrainAcc 0.5165	ValidAcc 0.5139	TestAcc 0.5108	BestValid 0.5139
	Epoch 2450:	Loss 1.4118	TrainAcc 0.5186	ValidAcc 0.5141	TestAcc 0.5102	BestValid 0.5141
	Epoch 2500:	Loss 1.4134	TrainAcc 0.5192	ValidAcc 0.5143	TestAcc 0.5108	BestValid 0.5143
	Epoch 2550:	Loss 1.4109	TrainAcc 0.5200	ValidAcc 0.5141	TestAcc 0.5119	BestValid 0.5143
	Epoch 2600:	Loss 1.4110	TrainAcc 0.5181	ValidAcc 0.5117	TestAcc 0.5105	BestValid 0.5143
	Epoch 2650:	Loss 1.4096	TrainAcc 0.5159	ValidAcc 0.5111	TestAcc 0.5101	BestValid 0.5143
	Epoch 2700:	Loss 1.4136	TrainAcc 0.5181	ValidAcc 0.5131	TestAcc 0.5093	BestValid 0.5143
	Epoch 2750:	Loss 1.4065	TrainAcc 0.5214	ValidAcc 0.5154	TestAcc 0.5138	BestValid 0.5154
	Epoch 2800:	Loss 1.4063	TrainAcc 0.5202	ValidAcc 0.5147	TestAcc 0.5136	BestValid 0.5154
	Epoch 2850:	Loss 1.4049	TrainAcc 0.5186	ValidAcc 0.5125	TestAcc 0.5119	BestValid 0.5154
	Epoch 2900:	Loss 1.4054	TrainAcc 0.5210	ValidAcc 0.5148	TestAcc 0.5136	BestValid 0.5154
	Epoch 2950:	Loss 1.4062	TrainAcc 0.5222	ValidAcc 0.5151	TestAcc 0.5146	BestValid 0.5154
	Epoch 3000:	Loss 1.4024	TrainAcc 0.5159	ValidAcc 0.5118	TestAcc 0.5094	BestValid 0.5154
	Epoch 3050:	Loss 1.4053	TrainAcc 0.5195	ValidAcc 0.5138	TestAcc 0.5119	BestValid 0.5154
	Epoch 3100:	Loss 1.4014	TrainAcc 0.5196	ValidAcc 0.5135	TestAcc 0.5122	BestValid 0.5154
	Epoch 3150:	Loss 1.3992	TrainAcc 0.5194	ValidAcc 0.5133	TestAcc 0.5108	BestValid 0.5154
	Epoch 3200:	Loss 1.3967	TrainAcc 0.5222	ValidAcc 0.5162	TestAcc 0.5155	BestValid 0.5162
	Epoch 3250:	Loss 1.4003	TrainAcc 0.5240	ValidAcc 0.5167	TestAcc 0.5162	BestValid 0.5167
	Epoch 3300:	Loss 1.3967	TrainAcc 0.5222	ValidAcc 0.5158	TestAcc 0.5143	BestValid 0.5167
	Epoch 3350:	Loss 1.3974	TrainAcc 0.5205	ValidAcc 0.5148	TestAcc 0.5120	BestValid 0.5167
	Epoch 3400:	Loss 1.3969	TrainAcc 0.5268	ValidAcc 0.5169	TestAcc 0.5183	BestValid 0.5169
	Epoch 3450:	Loss 1.3940	TrainAcc 0.5213	ValidAcc 0.5135	TestAcc 0.5126	BestValid 0.5169
	Epoch 3500:	Loss 1.3920	TrainAcc 0.5239	ValidAcc 0.5160	TestAcc 0.5149	BestValid 0.5169
	Epoch 3550:	Loss 1.3948	TrainAcc 0.5123	ValidAcc 0.5055	TestAcc 0.5035	BestValid 0.5169
	Epoch 3600:	Loss 1.3917	TrainAcc 0.5248	ValidAcc 0.5174	TestAcc 0.5166	BestValid 0.5174
	Epoch 3650:	Loss 1.3950	TrainAcc 0.5222	ValidAcc 0.5144	TestAcc 0.5130	BestValid 0.5174
	Epoch 3700:	Loss 1.3951	TrainAcc 0.5261	ValidAcc 0.5186	TestAcc 0.5166	BestValid 0.5186
	Epoch 3750:	Loss 1.3897	TrainAcc 0.5273	ValidAcc 0.5194	TestAcc 0.5177	BestValid 0.5194
	Epoch 3800:	Loss 1.3902	TrainAcc 0.5244	ValidAcc 0.5171	TestAcc 0.5146	BestValid 0.5194
	Epoch 3850:	Loss 1.3903	TrainAcc 0.5189	ValidAcc 0.5104	TestAcc 0.5096	BestValid 0.5194
	Epoch 3900:	Loss 1.3895	TrainAcc 0.5254	ValidAcc 0.5160	TestAcc 0.5156	BestValid 0.5194
	Epoch 3950:	Loss 1.3912	TrainAcc 0.5282	ValidAcc 0.5199	TestAcc 0.5183	BestValid 0.5199
	Epoch 4000:	Loss 1.3864	TrainAcc 0.5290	ValidAcc 0.5211	TestAcc 0.5201	BestValid 0.5211
	Epoch 4050:	Loss 1.3868	TrainAcc 0.5307	ValidAcc 0.5210	TestAcc 0.5202	BestValid 0.5211
	Epoch 4100:	Loss 1.3899	TrainAcc 0.5233	ValidAcc 0.5146	TestAcc 0.5125	BestValid 0.5211
	Epoch 4150:	Loss 1.3870	TrainAcc 0.5254	ValidAcc 0.5169	TestAcc 0.5152	BestValid 0.5211
	Epoch 4200:	Loss 1.3841	TrainAcc 0.5265	ValidAcc 0.5175	TestAcc 0.5161	BestValid 0.5211
	Epoch 4250:	Loss 1.3860	TrainAcc 0.5263	ValidAcc 0.5169	TestAcc 0.5174	BestValid 0.5211
	Epoch 4300:	Loss 1.3815	TrainAcc 0.5243	ValidAcc 0.5145	TestAcc 0.5145	BestValid 0.5211
	Epoch 4350:	Loss 1.3838	TrainAcc 0.5271	ValidAcc 0.5182	TestAcc 0.5171	BestValid 0.5211
	Epoch 4400:	Loss 1.3844	TrainAcc 0.5303	ValidAcc 0.5203	TestAcc 0.5197	BestValid 0.5211
	Epoch 4450:	Loss 1.3832	TrainAcc 0.5309	ValidAcc 0.5201	TestAcc 0.5199	BestValid 0.5211
	Epoch 4500:	Loss 1.3809	TrainAcc 0.5299	ValidAcc 0.5200	TestAcc 0.5194	BestValid 0.5211
	Epoch 4550:	Loss 1.3793	TrainAcc 0.5254	ValidAcc 0.5168	TestAcc 0.5165	BestValid 0.5211
	Epoch 4600:	Loss 1.3817	TrainAcc 0.5330	ValidAcc 0.5239	TestAcc 0.5220	BestValid 0.5239
	Epoch 4650:	Loss 1.3793	TrainAcc 0.5269	ValidAcc 0.5174	TestAcc 0.5170	BestValid 0.5239
	Epoch 4700:	Loss 1.3857	TrainAcc 0.5345	ValidAcc 0.5243	TestAcc 0.5232	BestValid 0.5243
	Epoch 4750:	Loss 1.3796	TrainAcc 0.5272	ValidAcc 0.5180	TestAcc 0.5173	BestValid 0.5243
	Epoch 4800:	Loss 1.3782	TrainAcc 0.5309	ValidAcc 0.5212	TestAcc 0.5206	BestValid 0.5243
	Epoch 4850:	Loss 1.3776	TrainAcc 0.5320	ValidAcc 0.5224	TestAcc 0.5211	BestValid 0.5243
	Epoch 4900:	Loss 1.3791	TrainAcc 0.5270	ValidAcc 0.5167	TestAcc 0.5173	BestValid 0.5243
	Epoch 4950:	Loss 1.3750	TrainAcc 0.5306	ValidAcc 0.5201	TestAcc 0.5196	BestValid 0.5243
	Epoch 5000:	Loss 1.3777	TrainAcc 0.5337	ValidAcc 0.5248	TestAcc 0.5239	BestValid 0.5248
****** Epoch Time (Excluding Evaluation Cost): 0.172 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.195 ms (Max: 0.314, Min: 0.039, Sum: 1.558)
Cluster-Wide Average, Compute: 44.540 ms (Max: 45.092, Min: 43.401, Sum: 356.323)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.010, Min: 0.008, Sum: 0.073)
Cluster-Wide Average, Bubble-Imbalance: 0.018 ms (Max: 0.021, Min: 0.016, Sum: 0.141)
Cluster-Wide Average, Communication-Graph: 123.592 ms (Max: 124.617, Min: 123.002, Sum: 988.732)
Cluster-Wide Average, Optimization: 2.779 ms (Max: 2.784, Min: 2.772, Sum: 22.234)
Cluster-Wide Average, Others: 0.485 ms (Max: 0.504, Min: 0.458, Sum: 3.876)
****** Breakdown Sum: 171.617 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.311 GB (Max: 4.788, Min: 4.229, Sum: 34.485)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.758 Gbps (Max: 51.622, Min: 16.893, Sum: 342.061)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 4.603 GB
****** Accuracy Results ******
Highest valid_acc: 0.5248
Target test_acc: 0.5239
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
