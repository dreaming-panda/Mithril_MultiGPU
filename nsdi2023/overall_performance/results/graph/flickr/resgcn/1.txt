Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 3 on machine gnerv4
DONE MPI INIT
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4

Initialized node 1 on machine gnerv4
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.020 seconds.
        It takes 0.017 seconds.
        It takes 0.019 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.808 Gbps (per GPU), 486.463 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.507 Gbps (per GPU), 484.059 Gbps (aggregated)
The layer-level communication performance: 60.506 Gbps (per GPU), 484.047 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.250 Gbps (per GPU), 482.002 Gbps (aggregated)
The layer-level communication performance: 60.222 Gbps (per GPU), 481.778 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.007 Gbps (per GPU), 480.057 Gbps (aggregated)
The layer-level communication performance: 59.962 Gbps (per GPU), 479.693 Gbps (aggregated)
The layer-level communication performance: 59.928 Gbps (per GPU), 479.423 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.805 Gbps (per GPU), 1262.441 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.859 Gbps (per GPU), 1262.869 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.793 Gbps (per GPU), 1262.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.805 Gbps (per GPU), 1262.441 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.793 Gbps (per GPU), 1262.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.820 Gbps (per GPU), 1262.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.787 Gbps (per GPU), 1262.299 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.793 Gbps (per GPU), 1262.346 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.272 Gbps (per GPU), 834.176 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.273 Gbps (per GPU), 834.182 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.275 Gbps (per GPU), 834.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.274 Gbps (per GPU), 834.189 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.268 Gbps (per GPU), 834.148 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.274 Gbps (per GPU), 834.189 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.240 Gbps (per GPU), 833.920 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.275 Gbps (per GPU), 834.196 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.783 Gbps (per GPU), 294.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.783 Gbps (per GPU), 294.260 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.784 Gbps (per GPU), 294.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.784 Gbps (per GPU), 294.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.784 Gbps (per GPU), 294.272 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.784 Gbps (per GPU), 294.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.780 Gbps (per GPU), 294.237 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.781 Gbps (per GPU), 294.252 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.77ms  1.32ms  2.28ms  2.98 11.16K  0.12M
 chk_1  0.75ms  1.33ms  2.29ms  3.06 11.16K  0.11M
 chk_2  0.72ms  1.30ms  2.26ms  3.15 11.16K  0.11M
 chk_3  0.67ms  1.25ms  2.23ms  3.32 11.16K  0.12M
 chk_4  0.74ms  1.32ms  2.29ms  3.11 11.16K  0.11M
 chk_5  0.73ms  1.31ms  2.28ms  3.14 11.16K  0.10M
 chk_6  0.73ms  1.32ms  2.28ms  3.12 11.16K  0.12M
 chk_7  0.74ms  1.32ms  2.30ms  3.11 11.16K  0.11M
   Avg  0.73  1.31  2.28
   Max  0.77  1.33  2.30
   Min  0.67  1.25  2.23
 Ratio  1.14  1.06  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.473 s
*** Node 0, starting model training...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 229)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 4, starting model training...
*** Node 2, starting model training...
*** Node 5, starting model training...
*** Node 3, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 229)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 229)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 229)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 229)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 229)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 229)...
+++++++++ Node 5 initializing the weights for op[0, 229)...
+++++++++ Node 0 initializing the weights for op[0, 229)...
+++++++++ Node 6 initializing the weights for op[0, 229)...
+++++++++ Node 2 initializing the weights for op[0, 229)...
+++++++++ Node 7 initializing the weights for op[0, 229)...
+++++++++ Node 4 initializing the weights for op[0, 229)...
+++++++++ Node 3 initializing the weights for op[0, 229)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.4358	TrainAcc 0.4215	ValidAcc 0.4238	TestAcc 0.4233	BestValid 0.4238
	Epoch 50:	Loss 1.5439	TrainAcc 0.4702	ValidAcc 0.4713	TestAcc 0.4687	BestValid 0.4713
	Epoch 100:	Loss 1.4849	TrainAcc 0.4820	ValidAcc 0.4854	TestAcc 0.4819	BestValid 0.4854
	Epoch 150:	Loss 1.4575	TrainAcc 0.4919	ValidAcc 0.4925	TestAcc 0.4888	BestValid 0.4925
	Epoch 200:	Loss 1.4464	TrainAcc 0.4972	ValidAcc 0.4955	TestAcc 0.4922	BestValid 0.4955
	Epoch 250:	Loss 1.4347	TrainAcc 0.4994	ValidAcc 0.4946	TestAcc 0.4956	BestValid 0.4955
	Epoch 300:	Loss 1.4257	TrainAcc 0.5018	ValidAcc 0.4962	TestAcc 0.4978	BestValid 0.4962
	Epoch 350:	Loss 1.4255	TrainAcc 0.5026	ValidAcc 0.4990	TestAcc 0.4978	BestValid 0.4990
	Epoch 400:	Loss 1.4181	TrainAcc 0.5064	ValidAcc 0.5005	TestAcc 0.5016	BestValid 0.5005
	Epoch 450:	Loss 1.4215	TrainAcc 0.5065	ValidAcc 0.5002	TestAcc 0.4992	BestValid 0.5005
	Epoch 500:	Loss 1.4041	TrainAcc 0.5098	ValidAcc 0.5020	TestAcc 0.5037	BestValid 0.5020
	Epoch 550:	Loss 1.4015	TrainAcc 0.5119	ValidAcc 0.5034	TestAcc 0.5047	BestValid 0.5034
	Epoch 600:	Loss 1.3946	TrainAcc 0.5141	ValidAcc 0.5052	TestAcc 0.5077	BestValid 0.5052
	Epoch 650:	Loss 1.4198	TrainAcc 0.5119	ValidAcc 0.5021	TestAcc 0.5047	BestValid 0.5052
	Epoch 700:	Loss 1.3883	TrainAcc 0.5182	ValidAcc 0.5076	TestAcc 0.5088	BestValid 0.5076
	Epoch 750:	Loss 1.3866	TrainAcc 0.5196	ValidAcc 0.5093	TestAcc 0.5108	BestValid 0.5093
	Epoch 800:	Loss 1.3816	TrainAcc 0.5215	ValidAcc 0.5094	TestAcc 0.5115	BestValid 0.5094
	Epoch 850:	Loss 1.3778	TrainAcc 0.5227	ValidAcc 0.5108	TestAcc 0.5112	BestValid 0.5108
	Epoch 900:	Loss 1.3765	TrainAcc 0.5236	ValidAcc 0.5116	TestAcc 0.5128	BestValid 0.5116
	Epoch 950:	Loss 1.3759	TrainAcc 0.5252	ValidAcc 0.5137	TestAcc 0.5144	BestValid 0.5137
	Epoch 1000:	Loss 1.3686	TrainAcc 0.5278	ValidAcc 0.5130	TestAcc 0.5154	BestValid 0.5137
	Epoch 1050:	Loss 1.3675	TrainAcc 0.5274	ValidAcc 0.5126	TestAcc 0.5136	BestValid 0.5137
	Epoch 1100:	Loss 1.3641	TrainAcc 0.5284	ValidAcc 0.5140	TestAcc 0.5144	BestValid 0.5140
	Epoch 1150:	Loss 1.3624	TrainAcc 0.5305	ValidAcc 0.5170	TestAcc 0.5177	BestValid 0.5170
	Epoch 1200:	Loss 1.3592	TrainAcc 0.5313	ValidAcc 0.5188	TestAcc 0.5173	BestValid 0.5188
	Epoch 1250:	Loss 1.3589	TrainAcc 0.5302	ValidAcc 0.5151	TestAcc 0.5197	BestValid 0.5188
	Epoch 1300:	Loss 1.3521	TrainAcc 0.5332	ValidAcc 0.5171	TestAcc 0.5159	BestValid 0.5188
	Epoch 1350:	Loss 1.3497	TrainAcc 0.5369	ValidAcc 0.5176	TestAcc 0.5185	BestValid 0.5188
	Epoch 1400:	Loss 1.3442	TrainAcc 0.5364	ValidAcc 0.5188	TestAcc 0.5195	BestValid 0.5188
	Epoch 1450:	Loss 1.3458	TrainAcc 0.5381	ValidAcc 0.5199	TestAcc 0.5195	BestValid 0.5199
	Epoch 1500:	Loss 1.3440	TrainAcc 0.5408	ValidAcc 0.5211	TestAcc 0.5210	BestValid 0.5211
	Epoch 1550:	Loss 1.3395	TrainAcc 0.5416	ValidAcc 0.5214	TestAcc 0.5204	BestValid 0.5214
	Epoch 1600:	Loss 1.3403	TrainAcc 0.5374	ValidAcc 0.5158	TestAcc 0.5136	BestValid 0.5214
	Epoch 1650:	Loss 1.3347	TrainAcc 0.5414	ValidAcc 0.5202	TestAcc 0.5177	BestValid 0.5214
	Epoch 1700:	Loss 1.3421	TrainAcc 0.5432	ValidAcc 0.5222	TestAcc 0.5204	BestValid 0.5222
	Epoch 1750:	Loss 1.3271	TrainAcc 0.5472	ValidAcc 0.5242	TestAcc 0.5220	BestValid 0.5242
	Epoch 1800:	Loss 1.3258	TrainAcc 0.5466	ValidAcc 0.5212	TestAcc 0.5192	BestValid 0.5242
	Epoch 1850:	Loss 1.3256	TrainAcc 0.5455	ValidAcc 0.5201	TestAcc 0.5180	BestValid 0.5242
	Epoch 1900:	Loss 1.3218	TrainAcc 0.5495	ValidAcc 0.5237	TestAcc 0.5238	BestValid 0.5242
	Epoch 1950:	Loss 1.3216	TrainAcc 0.5504	ValidAcc 0.5210	TestAcc 0.5185	BestValid 0.5242
	Epoch 2000:	Loss 1.3200	TrainAcc 0.5528	ValidAcc 0.5256	TestAcc 0.5227	BestValid 0.5256
	Epoch 2050:	Loss 1.3194	TrainAcc 0.5538	ValidAcc 0.5252	TestAcc 0.5246	BestValid 0.5256
	Epoch 2100:	Loss 1.3118	TrainAcc 0.5534	ValidAcc 0.5232	TestAcc 0.5234	BestValid 0.5256
	Epoch 2150:	Loss 1.3151	TrainAcc 0.5526	ValidAcc 0.5211	TestAcc 0.5223	BestValid 0.5256
	Epoch 2200:	Loss 1.3137	TrainAcc 0.5530	ValidAcc 0.5243	TestAcc 0.5220	BestValid 0.5256
	Epoch 2250:	Loss 1.3047	TrainAcc 0.5582	ValidAcc 0.5247	TestAcc 0.5252	BestValid 0.5256
	Epoch 2300:	Loss 1.3134	TrainAcc 0.5554	ValidAcc 0.5226	TestAcc 0.5233	BestValid 0.5256
	Epoch 2350:	Loss 1.3033	TrainAcc 0.5570	ValidAcc 0.5255	TestAcc 0.5234	BestValid 0.5256
	Epoch 2400:	Loss 1.3031	TrainAcc 0.5611	ValidAcc 0.5268	TestAcc 0.5262	BestValid 0.5268
	Epoch 2450:	Loss 1.3031	TrainAcc 0.5601	ValidAcc 0.5251	TestAcc 0.5270	BestValid 0.5268
	Epoch 2500:	Loss 1.3002	TrainAcc 0.5606	ValidAcc 0.5277	TestAcc 0.5267	BestValid 0.5277
	Epoch 2550:	Loss 1.3055	TrainAcc 0.5587	ValidAcc 0.5246	TestAcc 0.5262	BestValid 0.5277
	Epoch 2600:	Loss 1.3155	TrainAcc 0.5577	ValidAcc 0.5188	TestAcc 0.5155	BestValid 0.5277
	Epoch 2650:	Loss 1.2935	TrainAcc 0.5632	ValidAcc 0.5255	TestAcc 0.5223	BestValid 0.5277
	Epoch 2700:	Loss 1.2916	TrainAcc 0.5634	ValidAcc 0.5254	TestAcc 0.5261	BestValid 0.5277
	Epoch 2750:	Loss 1.2914	TrainAcc 0.5657	ValidAcc 0.5273	TestAcc 0.5241	BestValid 0.5277
	Epoch 2800:	Loss 1.2897	TrainAcc 0.5664	ValidAcc 0.5282	TestAcc 0.5281	BestValid 0.5282
	Epoch 2850:	Loss 1.2868	TrainAcc 0.5663	ValidAcc 0.5285	TestAcc 0.5265	BestValid 0.5285
	Epoch 2900:	Loss 1.2874	TrainAcc 0.5608	ValidAcc 0.5165	TestAcc 0.5174	BestValid 0.5285
	Epoch 2950:	Loss 1.2815	TrainAcc 0.5649	ValidAcc 0.5217	TestAcc 0.5240	BestValid 0.5285
	Epoch 3000:	Loss 1.2864	TrainAcc 0.5680	ValidAcc 0.5242	TestAcc 0.5232	BestValid 0.5285
	Epoch 3050:	Loss 1.2850	TrainAcc 0.5678	ValidAcc 0.5251	TestAcc 0.5260	BestValid 0.5285
	Epoch 3100:	Loss 1.2835	TrainAcc 0.5703	ValidAcc 0.5284	TestAcc 0.5267	BestValid 0.5285
	Epoch 3150:	Loss 1.2758	TrainAcc 0.5712	ValidAcc 0.5264	TestAcc 0.5246	BestValid 0.5285
	Epoch 3200:	Loss 1.2770	TrainAcc 0.5668	ValidAcc 0.5213	TestAcc 0.5188	BestValid 0.5285
	Epoch 3250:	Loss 1.2723	TrainAcc 0.5696	ValidAcc 0.5239	TestAcc 0.5247	BestValid 0.5285
	Epoch 3300:	Loss 1.2792	TrainAcc 0.5736	ValidAcc 0.5294	TestAcc 0.5264	BestValid 0.5294
	Epoch 3350:	Loss 1.2711	TrainAcc 0.5714	ValidAcc 0.5254	TestAcc 0.5237	BestValid 0.5294
	Epoch 3400:	Loss 1.2703	TrainAcc 0.5738	ValidAcc 0.5271	TestAcc 0.5273	BestValid 0.5294
	Epoch 3450:	Loss 1.2709	TrainAcc 0.5759	ValidAcc 0.5288	TestAcc 0.5253	BestValid 0.5294
	Epoch 3500:	Loss 1.2774	TrainAcc 0.5721	ValidAcc 0.5279	TestAcc 0.5267	BestValid 0.5294
	Epoch 3550:	Loss 1.2674	TrainAcc 0.5765	ValidAcc 0.5290	TestAcc 0.5271	BestValid 0.5294
	Epoch 3600:	Loss 1.2678	TrainAcc 0.5778	ValidAcc 0.5291	TestAcc 0.5288	BestValid 0.5294
	Epoch 3650:	Loss 1.2659	TrainAcc 0.5733	ValidAcc 0.5215	TestAcc 0.5214	BestValid 0.5294
	Epoch 3700:	Loss 1.2649	TrainAcc 0.5768	ValidAcc 0.5305	TestAcc 0.5313	BestValid 0.5305
	Epoch 3750:	Loss 1.2612	TrainAcc 0.5781	ValidAcc 0.5268	TestAcc 0.5266	BestValid 0.5305
	Epoch 3800:	Loss 1.2548	TrainAcc 0.5811	ValidAcc 0.5301	TestAcc 0.5293	BestValid 0.5305
	Epoch 3850:	Loss 1.2592	TrainAcc 0.5808	ValidAcc 0.5281	TestAcc 0.5293	BestValid 0.5305
	Epoch 3900:	Loss 1.2631	TrainAcc 0.5804	ValidAcc 0.5266	TestAcc 0.5262	BestValid 0.5305
	Epoch 3950:	Loss 1.2523	TrainAcc 0.5819	ValidAcc 0.5289	TestAcc 0.5301	BestValid 0.5305
	Epoch 4000:	Loss 1.2579	TrainAcc 0.5763	ValidAcc 0.5220	TestAcc 0.5223	BestValid 0.5305
	Epoch 4050:	Loss 1.2521	TrainAcc 0.5781	ValidAcc 0.5220	TestAcc 0.5226	BestValid 0.5305
	Epoch 4100:	Loss 1.2531	TrainAcc 0.5807	ValidAcc 0.5268	TestAcc 0.5262	BestValid 0.5305
	Epoch 4150:	Loss 1.2503	TrainAcc 0.5819	ValidAcc 0.5298	TestAcc 0.5300	BestValid 0.5305
	Epoch 4200:	Loss 1.2630	TrainAcc 0.5795	ValidAcc 0.5297	TestAcc 0.5292	BestValid 0.5305
	Epoch 4250:	Loss 1.2501	TrainAcc 0.5815	ValidAcc 0.5254	TestAcc 0.5265	BestValid 0.5305
	Epoch 4300:	Loss 1.2420	TrainAcc 0.5825	ValidAcc 0.5262	TestAcc 0.5254	BestValid 0.5305
	Epoch 4350:	Loss 1.2490	TrainAcc 0.5864	ValidAcc 0.5298	TestAcc 0.5292	BestValid 0.5305
	Epoch 4400:	Loss 1.2475	TrainAcc 0.5860	ValidAcc 0.5290	TestAcc 0.5297	BestValid 0.5305
	Epoch 4450:	Loss 1.2692	TrainAcc 0.5686	ValidAcc 0.5133	TestAcc 0.5146	BestValid 0.5305
	Epoch 4500:	Loss 1.2420	TrainAcc 0.5873	ValidAcc 0.5268	TestAcc 0.5292	BestValid 0.5305
	Epoch 4550:	Loss 1.2444	TrainAcc 0.5795	ValidAcc 0.5289	TestAcc 0.5308	BestValid 0.5305
	Epoch 4600:	Loss 1.2425	TrainAcc 0.5892	ValidAcc 0.5279	TestAcc 0.5299	BestValid 0.5305
	Epoch 4650:	Loss 1.2371	TrainAcc 0.5886	ValidAcc 0.5291	TestAcc 0.5311	BestValid 0.5305
	Epoch 4700:	Loss 1.2358	TrainAcc 0.5864	ValidAcc 0.5254	TestAcc 0.5296	BestValid 0.5305
	Epoch 4750:	Loss 1.2430	TrainAcc 0.5863	ValidAcc 0.5307	TestAcc 0.5312	BestValid 0.5307
	Epoch 4800:	Loss 1.2447	TrainAcc 0.5832	ValidAcc 0.5263	TestAcc 0.5280	BestValid 0.5307
	Epoch 4850:	Loss 1.2405	TrainAcc 0.5905	ValidAcc 0.5297	TestAcc 0.5310	BestValid 0.5307
	Epoch 4900:	Loss 1.2338	TrainAcc 0.5878	ValidAcc 0.5303	TestAcc 0.5323	BestValid 0.5307
	Epoch 4950:	Loss 1.2928	TrainAcc 0.5806	ValidAcc 0.5290	TestAcc 0.5268	BestValid 0.5307
	Epoch 5000:	Loss 1.2293	TrainAcc 0.5938	ValidAcc 0.5308	TestAcc 0.5312	BestValid 0.5308
****** Epoch Time (Excluding Evaluation Cost): 0.165 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.175 ms (Max: 0.308, Min: 0.034, Sum: 1.399)
Cluster-Wide Average, Compute: 38.488 ms (Max: 39.085, Min: 36.886, Sum: 307.901)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.065)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.017, Min: 0.016, Sum: 0.131)
Cluster-Wide Average, Communication-Graph: 123.482 ms (Max: 124.992, Min: 122.793, Sum: 987.859)
Cluster-Wide Average, Optimization: 2.782 ms (Max: 2.795, Min: 2.764, Sum: 22.256)
Cluster-Wide Average, Others: 0.460 ms (Max: 0.483, Min: 0.436, Sum: 3.676)
****** Breakdown Sum: 165.411 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.924 GB (Max: 4.399, Min: 3.842, Sum: 31.390)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.802 Gbps (Max: 51.731, Min: 16.820, Sum: 342.417)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 4.603 GB
****** Accuracy Results ******
Highest valid_acc: 0.5308
Target test_acc: 0.5312
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
