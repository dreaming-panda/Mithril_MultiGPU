Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.032 seconds.
        It takes 0.032 seconds.
Building the CSC structure...
Building the CSC structure...
        It takes 0.032 seconds.
Building the CSC structure...
        It takes 0.033 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.351 seconds.
        It takes 0.350 seconds.
        It takes 0.351 seconds.
        It takes 0.347 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.009 seconds.
        It takes 0.012 seconds.
        It takes 0.406 seconds.
        It takes 0.413 seconds.
        It takes 0.413 seconds.
        It takes 0.412 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.008 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.009 seconds.
        It takes 0.011 seconds.
        It takes 0.011 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 11157
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.282 Gbps (per GPU), 442.253 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.030 Gbps (per GPU), 440.239 Gbps (aggregated)
The layer-level communication performance: 55.024 Gbps (per GPU), 440.190 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.805 Gbps (per GPU), 438.440 Gbps (aggregated)
The layer-level communication performance: 54.775 Gbps (per GPU), 438.197 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.589 Gbps (per GPU), 436.708 Gbps (aggregated)
The layer-level communication performance: 54.550 Gbps (per GPU), 436.398 Gbps (aggregated)
The layer-level communication performance: 54.523 Gbps (per GPU), 436.181 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.437 Gbps (per GPU), 1251.497 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.422 Gbps (per GPU), 1251.377 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.440 Gbps (per GPU), 1251.517 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.422 Gbps (per GPU), 1251.377 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.428 Gbps (per GPU), 1251.426 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.422 Gbps (per GPU), 1251.377 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.440 Gbps (per GPU), 1251.517 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.423 Gbps (per GPU), 1251.380 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.939 Gbps (per GPU), 807.516 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.941 Gbps (per GPU), 807.529 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.945 Gbps (per GPU), 807.561 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.930 Gbps (per GPU), 807.438 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.947 Gbps (per GPU), 807.574 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.927 Gbps (per GPU), 807.412 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.944 Gbps (per GPU), 807.554 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.903 Gbps (per GPU), 807.225 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.136 Gbps (per GPU), 273.090 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.134 Gbps (per GPU), 273.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.127 Gbps (per GPU), 273.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.127 Gbps (per GPU), 273.015 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.135 Gbps (per GPU), 273.077 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.136 Gbps (per GPU), 273.086 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.134 Gbps (per GPU), 273.070 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.136 Gbps (per GPU), 273.086 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.06ms  1.86ms  2.88ms  2.72 11.16K  0.12M
 chk_1  1.05ms  1.84ms  3.21ms  3.07 11.16K  0.11M
 chk_2  1.01ms  1.72ms  2.86ms  2.81 11.16K  0.11M
 chk_3  0.94ms  1.68ms  2.81ms  2.99 11.16K  0.12M
 chk_4  1.01ms  1.75ms  2.88ms  2.86 11.16K  0.11M
 chk_5  1.00ms  1.75ms  2.87ms  2.87 11.16K  0.10M
 chk_6  1.00ms  1.75ms  2.88ms  2.88 11.16K  0.12M
 chk_7  1.01ms  1.76ms  2.88ms  2.86 11.16K  0.11M
   Avg  1.01  1.76  2.91
   Max  1.06  1.86  3.21
   Min  0.94  1.68  2.81
 Ratio  1.13  1.11  1.14
   Var  0.00  0.00  0.01
Profiling takes 0.600 s
*** Node 0, starting model training...
*** Node 2, starting model training...
*** Node 1, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 421)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 421)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 421)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 421)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 421)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 421)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 421)...
+++++++++ Node 4 initializing the weights for op[0, 421)...
+++++++++ Node 3 initializing the weights for op[0, 421)...
+++++++++ Node 7 initializing the weights for op[0, 421)...
+++++++++ Node 0 initializing the weights for op[0, 421)...
+++++++++ Node 2 initializing the weights for op[0, 421)...
+++++++++ Node 5 initializing the weights for op[0, 421)...
+++++++++ Node 6 initializing the weights for op[0, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...



*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5581	TrainAcc 0.4193	ValidAcc 0.4218	TestAcc 0.4215	BestValid 0.4218
	Epoch 50:	Loss 1.5226	TrainAcc 0.4816	ValidAcc 0.4801	TestAcc 0.4795	BestValid 0.4801
	Epoch 100:	Loss 1.4268	TrainAcc 0.5108	ValidAcc 0.5111	TestAcc 0.5119	BestValid 0.5111
	Epoch 150:	Loss 1.3974	TrainAcc 0.5185	ValidAcc 0.5087	TestAcc 0.5112	BestValid 0.5111
	Epoch 200:	Loss 1.3838	TrainAcc 0.5259	ValidAcc 0.5203	TestAcc 0.5209	BestValid 0.5203
	Epoch 250:	Loss 1.3829	TrainAcc 0.5313	ValidAcc 0.5225	TestAcc 0.5233	BestValid 0.5225
	Epoch 300:	Loss 1.3595	TrainAcc 0.5315	ValidAcc 0.5201	TestAcc 0.5198	BestValid 0.5225
	Epoch 350:	Loss 1.3522	TrainAcc 0.5347	ValidAcc 0.5222	TestAcc 0.5229	BestValid 0.5225
	Epoch 400:	Loss 1.3410	TrainAcc 0.5406	ValidAcc 0.5253	TestAcc 0.5285	BestValid 0.5253
	Epoch 450:	Loss 1.3368	TrainAcc 0.5425	ValidAcc 0.5249	TestAcc 0.5291	BestValid 0.5253
	Epoch 500:	Loss 1.3279	TrainAcc 0.5467	ValidAcc 0.5279	TestAcc 0.5319	BestValid 0.5279
	Epoch 550:	Loss 1.3236	TrainAcc 0.5462	ValidAcc 0.5268	TestAcc 0.5308	BestValid 0.5279
	Epoch 600:	Loss 1.3183	TrainAcc 0.5498	ValidAcc 0.5287	TestAcc 0.5313	BestValid 0.5287
	Epoch 650:	Loss 1.3113	TrainAcc 0.5525	ValidAcc 0.5310	TestAcc 0.5317	BestValid 0.5310
	Epoch 700:	Loss 1.3068	TrainAcc 0.5554	ValidAcc 0.5312	TestAcc 0.5325	BestValid 0.5312
	Epoch 750:	Loss 1.3049	TrainAcc 0.5565	ValidAcc 0.5303	TestAcc 0.5307	BestValid 0.5312
	Epoch 800:	Loss 1.2984	TrainAcc 0.5531	ValidAcc 0.5274	TestAcc 0.5273	BestValid 0.5312
	Epoch 850:	Loss 1.2916	TrainAcc 0.5607	ValidAcc 0.5305	TestAcc 0.5323	BestValid 0.5312
	Epoch 900:	Loss 1.3019	TrainAcc 0.5591	ValidAcc 0.5323	TestAcc 0.5311	BestValid 0.5323
	Epoch 950:	Loss 1.2803	TrainAcc 0.5656	ValidAcc 0.5319	TestAcc 0.5316	BestValid 0.5323
	Epoch 1000:	Loss 1.2797	TrainAcc 0.5647	ValidAcc 0.5312	TestAcc 0.5320	BestValid 0.5323
	Epoch 1050:	Loss 1.2749	TrainAcc 0.5684	ValidAcc 0.5339	TestAcc 0.5353	BestValid 0.5339
	Epoch 1100:	Loss 1.2698	TrainAcc 0.5699	ValidAcc 0.5323	TestAcc 0.5309	BestValid 0.5339
	Epoch 1150:	Loss 1.2672	TrainAcc 0.5736	ValidAcc 0.5343	TestAcc 0.5341	BestValid 0.5343
	Epoch 1200:	Loss 1.2636	TrainAcc 0.5754	ValidAcc 0.5346	TestAcc 0.5343	BestValid 0.5346
	Epoch 1250:	Loss 1.2598	TrainAcc 0.5746	ValidAcc 0.5331	TestAcc 0.5337	BestValid 0.5346
	Epoch 1300:	Loss 1.2548	TrainAcc 0.5742	ValidAcc 0.5322	TestAcc 0.5331	BestValid 0.5346
	Epoch 1350:	Loss 1.2478	TrainAcc 0.5800	ValidAcc 0.5322	TestAcc 0.5331	BestValid 0.5346
	Epoch 1400:	Loss 1.2418	TrainAcc 0.5717	ValidAcc 0.5218	TestAcc 0.5218	BestValid 0.5346
	Epoch 1450:	Loss 1.2423	TrainAcc 0.5806	ValidAcc 0.5336	TestAcc 0.5335	BestValid 0.5346
	Epoch 1500:	Loss 1.2346	TrainAcc 0.5752	ValidAcc 0.5212	TestAcc 0.5223	BestValid 0.5346
	Epoch 1550:	Loss 1.2355	TrainAcc 0.5868	ValidAcc 0.5363	TestAcc 0.5351	BestValid 0.5363
	Epoch 1600:	Loss 1.2347	TrainAcc 0.5877	ValidAcc 0.5321	TestAcc 0.5341	BestValid 0.5363
	Epoch 1650:	Loss 1.2310	TrainAcc 0.5901	ValidAcc 0.5302	TestAcc 0.5331	BestValid 0.5363
	Epoch 1700:	Loss 1.2184	TrainAcc 0.5930	ValidAcc 0.5343	TestAcc 0.5348	BestValid 0.5363
	Epoch 1750:	Loss 1.2144	TrainAcc 0.5911	ValidAcc 0.5313	TestAcc 0.5335	BestValid 0.5363
	Epoch 1800:	Loss 1.2120	TrainAcc 0.5942	ValidAcc 0.5305	TestAcc 0.5305	BestValid 0.5363
	Epoch 1850:	Loss 1.2036	TrainAcc 0.5975	ValidAcc 0.5311	TestAcc 0.5322	BestValid 0.5363
	Epoch 1900:	Loss 1.2069	TrainAcc 0.5965	ValidAcc 0.5277	TestAcc 0.5309	BestValid 0.5363
	Epoch 1950:	Loss 1.1967	TrainAcc 0.6002	ValidAcc 0.5296	TestAcc 0.5321	BestValid 0.5363
	Epoch 2000:	Loss 1.2012	TrainAcc 0.6028	ValidAcc 0.5330	TestAcc 0.5352	BestValid 0.5363
	Epoch 2050:	Loss 1.1908	TrainAcc 0.6059	ValidAcc 0.5315	TestAcc 0.5323	BestValid 0.5363
	Epoch 2100:	Loss 1.1970	TrainAcc 0.6062	ValidAcc 0.5329	TestAcc 0.5338	BestValid 0.5363
	Epoch 2150:	Loss 1.1841	TrainAcc 0.6046	ValidAcc 0.5264	TestAcc 0.5287	BestValid 0.5363
	Epoch 2200:	Loss 1.1792	TrainAcc 0.6120	ValidAcc 0.5328	TestAcc 0.5338	BestValid 0.5363
	Epoch 2250:	Loss 1.1782	TrainAcc 0.6115	ValidAcc 0.5267	TestAcc 0.5305	BestValid 0.5363
	Epoch 2300:	Loss 1.1772	TrainAcc 0.6104	ValidAcc 0.5304	TestAcc 0.5296	BestValid 0.5363
	Epoch 2350:	Loss 1.1664	TrainAcc 0.6174	ValidAcc 0.5303	TestAcc 0.5337	BestValid 0.5363
	Epoch 2400:	Loss 1.1715	TrainAcc 0.6180	ValidAcc 0.5299	TestAcc 0.5291	BestValid 0.5363
	Epoch 2450:	Loss 1.1610	TrainAcc 0.6209	ValidAcc 0.5292	TestAcc 0.5295	BestValid 0.5363
	Epoch 2500:	Loss 1.1665	TrainAcc 0.6129	ValidAcc 0.5286	TestAcc 0.5306	BestValid 0.5363
	Epoch 2550:	Loss 1.1486	TrainAcc 0.6261	ValidAcc 0.5264	TestAcc 0.5286	BestValid 0.5363
	Epoch 2600:	Loss 1.1457	TrainAcc 0.6277	ValidAcc 0.5259	TestAcc 0.5283	BestValid 0.5363
	Epoch 2650:	Loss 1.1564	TrainAcc 0.6254	ValidAcc 0.5247	TestAcc 0.5242	BestValid 0.5363
	Epoch 2700:	Loss 1.1382	TrainAcc 0.6258	ValidAcc 0.5276	TestAcc 0.5306	BestValid 0.5363
	Epoch 2750:	Loss 1.1348	TrainAcc 0.6235	ValidAcc 0.5244	TestAcc 0.5260	BestValid 0.5363
	Epoch 2800:	Loss 1.1299	TrainAcc 0.6349	ValidAcc 0.5259	TestAcc 0.5265	BestValid 0.5363
	Epoch 2850:	Loss 1.1276	TrainAcc 0.6343	ValidAcc 0.5206	TestAcc 0.5243	BestValid 0.5363
	Epoch 2900:	Loss 1.1291	TrainAcc 0.6356	ValidAcc 0.5298	TestAcc 0.5323	BestValid 0.5363
	Epoch 2950:	Loss 1.1226	TrainAcc 0.6395	ValidAcc 0.5276	TestAcc 0.5289	BestValid 0.5363
	Epoch 3000:	Loss 1.1157	TrainAcc 0.6403	ValidAcc 0.5253	TestAcc 0.5277	BestValid 0.5363
	Epoch 3050:	Loss 1.1062	TrainAcc 0.6443	ValidAcc 0.5270	TestAcc 0.5298	BestValid 0.5363
	Epoch 3100:	Loss 1.1179	TrainAcc 0.6335	ValidAcc 0.5119	TestAcc 0.5159	BestValid 0.5363
	Epoch 3150:	Loss 1.1025	TrainAcc 0.6459	ValidAcc 0.5195	TestAcc 0.5218	BestValid 0.5363
	Epoch 3200:	Loss 1.1000	TrainAcc 0.6483	ValidAcc 0.5187	TestAcc 0.5209	BestValid 0.5363
	Epoch 3250:	Loss 1.1000	TrainAcc 0.6516	ValidAcc 0.5207	TestAcc 0.5229	BestValid 0.5363
	Epoch 3300:	Loss 1.1040	TrainAcc 0.6429	ValidAcc 0.5129	TestAcc 0.5166	BestValid 0.5363
	Epoch 3350:	Loss 1.0848	TrainAcc 0.6531	ValidAcc 0.5160	TestAcc 0.5185	BestValid 0.5363
	Epoch 3400:	Loss 1.0867	TrainAcc 0.6523	ValidAcc 0.5131	TestAcc 0.5161	BestValid 0.5363
	Epoch 3450:	Loss 1.0808	TrainAcc 0.6571	ValidAcc 0.5226	TestAcc 0.5267	BestValid 0.5363
	Epoch 3500:	Loss 1.0756	TrainAcc 0.6599	ValidAcc 0.5185	TestAcc 0.5197	BestValid 0.5363
	Epoch 3550:	Loss 1.0685	TrainAcc 0.6603	ValidAcc 0.5219	TestAcc 0.5226	BestValid 0.5363
	Epoch 3600:	Loss 1.0653	TrainAcc 0.6554	ValidAcc 0.5137	TestAcc 0.5154	BestValid 0.5363
	Epoch 3650:	Loss 1.0615	TrainAcc 0.6642	ValidAcc 0.5191	TestAcc 0.5218	BestValid 0.5363
	Epoch 3700:	Loss 1.0549	TrainAcc 0.6659	ValidAcc 0.5124	TestAcc 0.5149	BestValid 0.5363
	Epoch 3750:	Loss 1.0542	TrainAcc 0.6619	ValidAcc 0.5186	TestAcc 0.5224	BestValid 0.5363
	Epoch 3800:	Loss 1.0451	TrainAcc 0.6702	ValidAcc 0.5206	TestAcc 0.5233	BestValid 0.5363
	Epoch 3850:	Loss 1.0452	TrainAcc 0.6699	ValidAcc 0.5128	TestAcc 0.5147	BestValid 0.5363
	Epoch 3900:	Loss 1.0434	TrainAcc 0.6712	ValidAcc 0.5141	TestAcc 0.5164	BestValid 0.5363
	Epoch 3950:	Loss 1.0428	TrainAcc 0.6774	ValidAcc 0.5185	TestAcc 0.5184	BestValid 0.5363
	Epoch 4000:	Loss 1.0330	TrainAcc 0.6775	ValidAcc 0.5180	TestAcc 0.5199	BestValid 0.5363
	Epoch 4050:	Loss 1.0296	TrainAcc 0.6811	ValidAcc 0.5153	TestAcc 0.5153	BestValid 0.5363
	Epoch 4100:	Loss 1.0334	TrainAcc 0.6799	ValidAcc 0.5200	TestAcc 0.5231	BestValid 0.5363
	Epoch 4150:	Loss 1.0203	TrainAcc 0.6848	ValidAcc 0.5160	TestAcc 0.5178	BestValid 0.5363
	Epoch 4200:	Loss 1.0172	TrainAcc 0.6800	ValidAcc 0.5107	TestAcc 0.5132	BestValid 0.5363
	Epoch 4250:	Loss 1.0122	TrainAcc 0.6844	ValidAcc 0.5076	TestAcc 0.5097	BestValid 0.5363
	Epoch 4300:	Loss 1.0218	TrainAcc 0.6879	ValidAcc 0.5110	TestAcc 0.5133	BestValid 0.5363
	Epoch 4350:	Loss 1.0016	TrainAcc 0.6928	ValidAcc 0.5138	TestAcc 0.5155	BestValid 0.5363
	Epoch 4400:	Loss 1.0005	TrainAcc 0.6960	ValidAcc 0.5115	TestAcc 0.5141	BestValid 0.5363
	Epoch 4450:	Loss 0.9963	TrainAcc 0.6956	ValidAcc 0.5132	TestAcc 0.5160	BestValid 0.5363
	Epoch 4500:	Loss 0.9906	TrainAcc 0.6978	ValidAcc 0.5115	TestAcc 0.5133	BestValid 0.5363
	Epoch 4550:	Loss 0.9827	TrainAcc 0.6940	ValidAcc 0.5039	TestAcc 0.5058	BestValid 0.5363
