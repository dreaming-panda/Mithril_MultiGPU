Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.030 seconds.
Building the CSC structure...
        It takes 0.032 seconds.
Building the CSC structure...
        It takes 0.030 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
        It takes 0.022 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.024 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.090 Gbps (per GPU), 480.724 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.778 Gbps (per GPU), 478.225 Gbps (aggregated)
The layer-level communication performance: 59.772 Gbps (per GPU), 478.173 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.538 Gbps (per GPU), 476.303 Gbps (aggregated)
The layer-level communication performance: 59.503 Gbps (per GPU), 476.023 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.300 Gbps (per GPU), 474.396 Gbps (aggregated)
The layer-level communication performance: 59.253 Gbps (per GPU), 474.021 Gbps (aggregated)
The layer-level communication performance: 59.219 Gbps (per GPU), 473.753 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.597 Gbps (per GPU), 1252.779 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.589 Gbps (per GPU), 1252.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.533 Gbps (per GPU), 1252.264 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.586 Gbps (per GPU), 1252.685 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.609 Gbps (per GPU), 1252.873 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.589 Gbps (per GPU), 1252.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.527 Gbps (per GPU), 1252.218 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.592 Gbps (per GPU), 1252.734 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.502 Gbps (per GPU), 812.017 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.500 Gbps (per GPU), 811.997 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.498 Gbps (per GPU), 811.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.500 Gbps (per GPU), 811.997 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.498 Gbps (per GPU), 811.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.500 Gbps (per GPU), 811.998 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.503 Gbps (per GPU), 812.022 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.500 Gbps (per GPU), 812.004 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.300 Gbps (per GPU), 290.399 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.297 Gbps (per GPU), 290.375 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.300 Gbps (per GPU), 290.397 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.299 Gbps (per GPU), 290.389 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.293 Gbps (per GPU), 290.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.296 Gbps (per GPU), 290.369 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.295 Gbps (per GPU), 290.359 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.293 Gbps (per GPU), 290.347 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.69ms  0.84ms  0.49ms  3.45 11.16K  0.12M
 chk_1  1.70ms  0.84ms  0.50ms  3.42 11.16K  0.11M
 chk_2  1.65ms  0.81ms  0.46ms  3.56 11.16K  0.11M
 chk_3  1.61ms  0.77ms  0.42ms  3.81 11.16K  0.12M
 chk_4  1.68ms  0.84ms  0.49ms  3.40 11.16K  0.11M
 chk_5  1.67ms  0.83ms  0.48ms  3.46 11.16K  0.10M
 chk_6  1.82ms  0.83ms  0.48ms  3.75 11.16K  0.12M
 chk_7  1.68ms  0.84ms  0.49ms  3.39 11.16K  0.11M
   Avg  1.68  0.82  0.48
   Max  1.82  0.84  0.50
   Min  1.61  0.77  0.42
 Ratio  1.13  1.10  1.17
   Var  0.00  0.00  0.00
Profiling takes 0.350 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 30.222 ms
Partition 0 [0, 3) has cost: 26.676 ms
Partition 1 [3, 7) has cost: 26.398 ms
Partition 2 [7, 11) has cost: 26.398 ms
Partition 3 [11, 15) has cost: 26.398 ms
Partition 4 [15, 19) has cost: 26.398 ms
Partition 5 [19, 23) has cost: 26.398 ms
Partition 6 [23, 27) has cost: 26.398 ms
Partition 7 [27, 32) has cost: 30.222 ms
The optimal partitioning:
[0, 3)
[3, 7)
[7, 11)
[11, 15)
[15, 19)
[19, 23)
[23, 27)
[27, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.813 ms
GPU 0, Compute+Comm Time: 13.017 ms, Bubble Time: 12.399 ms, Imbalance Overhead: 0.398 ms
GPU 1, Compute+Comm Time: 12.986 ms, Bubble Time: 12.212 ms, Imbalance Overhead: 0.615 ms
GPU 2, Compute+Comm Time: 12.986 ms, Bubble Time: 12.100 ms, Imbalance Overhead: 0.728 ms
GPU 3, Compute+Comm Time: 12.986 ms, Bubble Time: 12.068 ms, Imbalance Overhead: 0.759 ms
GPU 4, Compute+Comm Time: 12.986 ms, Bubble Time: 11.921 ms, Imbalance Overhead: 0.906 ms
GPU 5, Compute+Comm Time: 12.986 ms, Bubble Time: 11.786 ms, Imbalance Overhead: 1.041 ms
GPU 6, Compute+Comm Time: 12.986 ms, Bubble Time: 11.647 ms, Imbalance Overhead: 1.181 ms
GPU 7, Compute+Comm Time: 14.193 ms, Bubble Time: 11.620 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 45.979 ms
GPU 0, Compute+Comm Time: 25.535 ms, Bubble Time: 20.444 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 22.918 ms, Bubble Time: 20.774 ms, Imbalance Overhead: 2.287 ms
GPU 2, Compute+Comm Time: 22.918 ms, Bubble Time: 21.079 ms, Imbalance Overhead: 1.982 ms
GPU 3, Compute+Comm Time: 22.918 ms, Bubble Time: 21.364 ms, Imbalance Overhead: 1.698 ms
GPU 4, Compute+Comm Time: 22.918 ms, Bubble Time: 21.686 ms, Imbalance Overhead: 1.375 ms
GPU 5, Compute+Comm Time: 22.918 ms, Bubble Time: 21.779 ms, Imbalance Overhead: 1.282 ms
GPU 6, Compute+Comm Time: 22.918 ms, Bubble Time: 21.983 ms, Imbalance Overhead: 1.079 ms
GPU 7, Compute+Comm Time: 23.165 ms, Bubble Time: 22.301 ms, Imbalance Overhead: 0.513 ms
The estimated cost of the whole pipeline: 75.382 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 56.620 ms
Partition 0 [0, 7) has cost: 53.075 ms
Partition 1 [7, 15) has cost: 52.796 ms
Partition 2 [15, 23) has cost: 52.796 ms
Partition 3 [23, 32) has cost: 56.620 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.309 ms
GPU 0, Compute+Comm Time: 16.387 ms, Bubble Time: 13.386 ms, Imbalance Overhead: 0.536 ms
GPU 1, Compute+Comm Time: 16.770 ms, Bubble Time: 12.965 ms, Imbalance Overhead: 0.575 ms
GPU 2, Compute+Comm Time: 16.770 ms, Bubble Time: 12.708 ms, Imbalance Overhead: 0.832 ms
GPU 3, Compute+Comm Time: 17.848 ms, Bubble Time: 12.461 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.727 ms
GPU 0, Compute+Comm Time: 28.652 ms, Bubble Time: 20.075 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 26.851 ms, Bubble Time: 20.511 ms, Imbalance Overhead: 1.365 ms
GPU 2, Compute+Comm Time: 26.851 ms, Bubble Time: 20.931 ms, Imbalance Overhead: 0.945 ms
GPU 3, Compute+Comm Time: 26.497 ms, Bubble Time: 21.492 ms, Imbalance Overhead: 0.738 ms
    The estimated cost with 2 DP ways is 82.988 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 109.417 ms
Partition 0 [0, 15) has cost: 105.871 ms
Partition 1 [15, 32) has cost: 109.417 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.010 ms
GPU 0, Compute+Comm Time: 29.091 ms, Bubble Time: 15.722 ms, Imbalance Overhead: 1.197 ms
GPU 1, Compute+Comm Time: 31.407 ms, Bubble Time: 14.603 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.400 ms
GPU 0, Compute+Comm Time: 41.858 ms, Bubble Time: 19.542 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 39.138 ms, Bubble Time: 20.958 ms, Imbalance Overhead: 1.303 ms
    The estimated cost with 4 DP ways is 112.780 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 215.288 ms
Partition 0 [0, 32) has cost: 215.288 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 118.898 ms
GPU 0, Compute+Comm Time: 118.898 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 129.096 ms
GPU 0, Compute+Comm Time: 129.096 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 260.394 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 20.7089	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 50:	Loss 1.6074	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6015	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5945	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5967	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5915	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5892	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5929	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5882	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5883	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5873	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5814	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5736	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5703	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5695	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5620	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5588	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5503	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5463	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5513	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5442	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5433	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1100:	Loss 1.5426	TrainAcc 0.2604	ValidAcc 0.2599	TestAcc 0.2563	BestValid 0.4239
	Epoch 1150:	Loss 1.5422	TrainAcc 0.2907	ValidAcc 0.2898	TestAcc 0.2851	BestValid 0.4239
	Epoch 1200:	Loss 1.5415	TrainAcc 0.2586	ValidAcc 0.2587	TestAcc 0.2551	BestValid 0.4239
	Epoch 1250:	Loss 1.5427	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1300:	Loss 1.5422	TrainAcc 0.2631	ValidAcc 0.2631	TestAcc 0.2598	BestValid 0.4239
	Epoch 1350:	Loss 1.5416	TrainAcc 0.3064	ValidAcc 0.3059	TestAcc 0.3035	BestValid 0.4239
	Epoch 1400:	Loss 1.5382	TrainAcc 0.3390	ValidAcc 0.3369	TestAcc 0.3349	BestValid 0.4239
	Epoch 1450:	Loss 1.5404	TrainAcc 0.3577	ValidAcc 0.3618	TestAcc 0.3604	BestValid 0.4239
	Epoch 1500:	Loss 1.5371	TrainAcc 0.2885	ValidAcc 0.2895	TestAcc 0.2864	BestValid 0.4239
	Epoch 1550:	Loss 1.5372	TrainAcc 0.3610	ValidAcc 0.3649	TestAcc 0.3636	BestValid 0.4239
	Epoch 1600:	Loss 1.5362	TrainAcc 0.3962	ValidAcc 0.3996	TestAcc 0.3971	BestValid 0.4239
	Epoch 1650:	Loss 1.5373	TrainAcc 0.3936	ValidAcc 0.3976	TestAcc 0.3942	BestValid 0.4239
	Epoch 1700:	Loss 1.5362	TrainAcc 0.4123	ValidAcc 0.4147	TestAcc 0.4146	BestValid 0.4239
	Epoch 1750:	Loss 1.5357	TrainAcc 0.3937	ValidAcc 0.3979	TestAcc 0.3951	BestValid 0.4239
	Epoch 1800:	Loss 1.5356	TrainAcc 0.4037	ValidAcc 0.4082	TestAcc 0.4050	BestValid 0.4239
	Epoch 1850:	Loss 1.5334	TrainAcc 0.4086	ValidAcc 0.4099	TestAcc 0.4102	BestValid 0.4239
	Epoch 1900:	Loss 1.5341	TrainAcc 0.4002	ValidAcc 0.4010	TestAcc 0.4000	BestValid 0.4239
	Epoch 1950:	Loss 1.5333	TrainAcc 0.4092	ValidAcc 0.4106	TestAcc 0.4102	BestValid 0.4239
	Epoch 2000:	Loss 1.5303	TrainAcc 0.3833	ValidAcc 0.3857	TestAcc 0.3866	BestValid 0.4239
	Epoch 2050:	Loss 1.5335	TrainAcc 0.4088	ValidAcc 0.4117	TestAcc 0.4121	BestValid 0.4239
	Epoch 2100:	Loss 1.5350	TrainAcc 0.3715	ValidAcc 0.3719	TestAcc 0.3739	BestValid 0.4239
	Epoch 2150:	Loss 1.5307	TrainAcc 0.4086	ValidAcc 0.4093	TestAcc 0.4094	BestValid 0.4239
	Epoch 2200:	Loss 1.5303	TrainAcc 0.4092	ValidAcc 0.4124	TestAcc 0.4107	BestValid 0.4239
	Epoch 2250:	Loss 1.5320	TrainAcc 0.4070	ValidAcc 0.4090	TestAcc 0.4085	BestValid 0.4239
	Epoch 2300:	Loss 1.5316	TrainAcc 0.3865	ValidAcc 0.3892	TestAcc 0.3888	BestValid 0.4239
	Epoch 2350:	Loss 1.5299	TrainAcc 0.4122	ValidAcc 0.4136	TestAcc 0.4141	BestValid 0.4239
	Epoch 2400:	Loss 1.5321	TrainAcc 0.3972	ValidAcc 0.4009	TestAcc 0.3986	BestValid 0.4239
	Epoch 2450:	Loss 1.5267	TrainAcc 0.4134	ValidAcc 0.4145	TestAcc 0.4158	BestValid 0.4239
	Epoch 2500:	Loss 1.5281	TrainAcc 0.4116	ValidAcc 0.4136	TestAcc 0.4140	BestValid 0.4239
	Epoch 2550:	Loss 1.5262	TrainAcc 0.4175	ValidAcc 0.4202	TestAcc 0.4201	BestValid 0.4239
	Epoch 2600:	Loss 1.5283	TrainAcc 0.4151	ValidAcc 0.4174	TestAcc 0.4181	BestValid 0.4239
	Epoch 2650:	Loss 1.5248	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5262	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5242	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5189	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5200	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5204	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5159	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5191	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5170	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5146	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5158	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5154	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5165	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5132	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5123	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5268	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5154	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5093	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5148	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5107	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5127	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5132	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5069	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5096	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5073	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5083	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5073	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5093	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5085	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5066	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5078	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5040	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5046	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5030	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5086	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5116	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5128	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.4993	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5011	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5042	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5036	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5018	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.4968	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5092	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.4963	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.4991	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5053	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.4972	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.173 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.204 ms (Max: 0.271, Min: 0.043, Sum: 1.628)
Cluster-Wide Average, Compute: 37.324 ms (Max: 38.019, Min: 35.265, Sum: 298.591)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.017, Min: 0.015, Sum: 0.130)
Cluster-Wide Average, Communication-Graph: 123.505 ms (Max: 125.567, Min: 122.767, Sum: 988.042)
Cluster-Wide Average, Optimization: 5.862 ms (Max: 5.941, Min: 5.753, Sum: 46.899)
Cluster-Wide Average, Others: 5.860 ms (Max: 5.977, Min: 5.778, Sum: 46.883)
****** Breakdown Sum: 172.779 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.880 GB (Max: 7.311, Min: 6.805, Sum: 55.042)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.787 Gbps (Max: 51.700, Min: 16.740, Sum: 342.295)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.037 GB
Total communication (cluster-wide, per-epoch): 4.620 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
