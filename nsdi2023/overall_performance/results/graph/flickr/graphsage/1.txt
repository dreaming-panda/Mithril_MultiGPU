Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.026 seconds.
        It takes 0.023 seconds.
        It takes 0.027 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.091 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.110 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.904 Gbps (per GPU), 463.235 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.639 Gbps (per GPU), 461.112 Gbps (aggregated)
The layer-level communication performance: 57.632 Gbps (per GPU), 461.057 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.391 Gbps (per GPU), 459.125 Gbps (aggregated)
The layer-level communication performance: 57.361 Gbps (per GPU), 458.891 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.162 Gbps (per GPU), 457.299 Gbps (aggregated)
The layer-level communication performance: 57.119 Gbps (per GPU), 456.949 Gbps (aggregated)
The layer-level communication performance: 57.093 Gbps (per GPU), 456.742 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.019 Gbps (per GPU), 1256.156 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.022 Gbps (per GPU), 1256.179 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.940 Gbps (per GPU), 1255.519 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.064 Gbps (per GPU), 1256.509 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.014 Gbps (per GPU), 1256.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.011 Gbps (per GPU), 1256.085 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.928 Gbps (per GPU), 1255.428 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.049 Gbps (per GPU), 1256.391 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.668 Gbps (per GPU), 805.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.667 Gbps (per GPU), 805.332 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.671 Gbps (per GPU), 805.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.671 Gbps (per GPU), 805.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.671 Gbps (per GPU), 805.365 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.671 Gbps (per GPU), 805.370 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.671 Gbps (per GPU), 805.371 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.672 Gbps (per GPU), 805.377 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.232 Gbps (per GPU), 241.857 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.230 Gbps (per GPU), 241.841 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.228 Gbps (per GPU), 241.826 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.231 Gbps (per GPU), 241.849 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.229 Gbps (per GPU), 241.829 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.230 Gbps (per GPU), 241.842 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.228 Gbps (per GPU), 241.825 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.227 Gbps (per GPU), 241.819 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.71ms  0.83ms  0.49ms  3.45 11.16K  0.12M
 chk_1  1.67ms  0.84ms  0.49ms  3.40 11.16K  0.11M
 chk_2  1.64ms  0.80ms  0.46ms  3.57 11.16K  0.11M
 chk_3  1.60ms  0.76ms  0.42ms  3.84 11.16K  0.12M
 chk_4  1.66ms  0.83ms  0.49ms  3.40 11.16K  0.11M
 chk_5  1.65ms  0.82ms  0.48ms  3.47 11.16K  0.10M
 chk_6  1.66ms  0.82ms  0.48ms  3.44 11.16K  0.12M
 chk_7  1.66ms  0.83ms  0.49ms  3.40 11.16K  0.11M
   Avg  1.66  0.82  0.47
   Max  1.71  0.84  0.49
   Min  1.60  0.76  0.42
 Ratio  1.07  1.10  1.19
   Var  0.00  0.00  0.00
Profiling takes 0.346 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 29.904 ms
Partition 0 [0, 3) has cost: 26.313 ms
Partition 1 [3, 7) has cost: 26.105 ms
Partition 2 [7, 11) has cost: 26.105 ms
Partition 3 [11, 15) has cost: 26.105 ms
Partition 4 [15, 19) has cost: 26.105 ms
Partition 5 [19, 23) has cost: 26.105 ms
Partition 6 [23, 27) has cost: 26.105 ms
Partition 7 [27, 32) has cost: 29.904 ms
The optimal partitioning:
[0, 3)
[3, 7)
[7, 11)
[11, 15)
[15, 19)
[19, 23)
[23, 27)
[27, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.821 ms
GPU 0, Compute+Comm Time: 12.959 ms, Bubble Time: 12.435 ms, Imbalance Overhead: 0.427 ms
GPU 1, Compute+Comm Time: 13.043 ms, Bubble Time: 12.251 ms, Imbalance Overhead: 0.527 ms
GPU 2, Compute+Comm Time: 13.043 ms, Bubble Time: 12.148 ms, Imbalance Overhead: 0.630 ms
GPU 3, Compute+Comm Time: 13.043 ms, Bubble Time: 12.124 ms, Imbalance Overhead: 0.654 ms
GPU 4, Compute+Comm Time: 13.043 ms, Bubble Time: 11.976 ms, Imbalance Overhead: 0.802 ms
GPU 5, Compute+Comm Time: 13.043 ms, Bubble Time: 11.854 ms, Imbalance Overhead: 0.924 ms
GPU 6, Compute+Comm Time: 13.043 ms, Bubble Time: 11.724 ms, Imbalance Overhead: 1.053 ms
GPU 7, Compute+Comm Time: 14.238 ms, Bubble Time: 11.583 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 46.024 ms
GPU 0, Compute+Comm Time: 25.531 ms, Bubble Time: 20.493 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 22.927 ms, Bubble Time: 20.813 ms, Imbalance Overhead: 2.284 ms
GPU 2, Compute+Comm Time: 22.927 ms, Bubble Time: 21.113 ms, Imbalance Overhead: 1.985 ms
GPU 3, Compute+Comm Time: 22.927 ms, Bubble Time: 21.394 ms, Imbalance Overhead: 1.703 ms
GPU 4, Compute+Comm Time: 22.927 ms, Bubble Time: 21.709 ms, Imbalance Overhead: 1.388 ms
GPU 5, Compute+Comm Time: 22.927 ms, Bubble Time: 21.801 ms, Imbalance Overhead: 1.297 ms
GPU 6, Compute+Comm Time: 22.927 ms, Bubble Time: 22.007 ms, Imbalance Overhead: 1.090 ms
GPU 7, Compute+Comm Time: 23.218 ms, Bubble Time: 22.292 ms, Imbalance Overhead: 0.514 ms
The estimated cost of the whole pipeline: 75.437 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 56.009 ms
Partition 0 [0, 7) has cost: 52.418 ms
Partition 1 [7, 15) has cost: 52.210 ms
Partition 2 [15, 23) has cost: 52.210 ms
Partition 3 [23, 32) has cost: 56.009 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.444 ms
GPU 0, Compute+Comm Time: 16.320 ms, Bubble Time: 13.423 ms, Imbalance Overhead: 0.701 ms
GPU 1, Compute+Comm Time: 16.839 ms, Bubble Time: 13.010 ms, Imbalance Overhead: 0.595 ms
GPU 2, Compute+Comm Time: 16.839 ms, Bubble Time: 12.778 ms, Imbalance Overhead: 0.828 ms
GPU 3, Compute+Comm Time: 17.909 ms, Bubble Time: 12.535 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.758 ms
GPU 0, Compute+Comm Time: 28.645 ms, Bubble Time: 20.113 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 26.850 ms, Bubble Time: 20.533 ms, Imbalance Overhead: 1.376 ms
GPU 2, Compute+Comm Time: 26.850 ms, Bubble Time: 20.953 ms, Imbalance Overhead: 0.955 ms
GPU 3, Compute+Comm Time: 26.524 ms, Bubble Time: 21.484 ms, Imbalance Overhead: 0.750 ms
    The estimated cost with 2 DP ways is 83.163 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 108.219 ms
Partition 0 [0, 15) has cost: 104.627 ms
Partition 1 [15, 32) has cost: 108.219 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.234 ms
GPU 0, Compute+Comm Time: 29.180 ms, Bubble Time: 15.841 ms, Imbalance Overhead: 1.213 ms
GPU 1, Compute+Comm Time: 31.656 ms, Bubble Time: 14.578 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.669 ms
GPU 0, Compute+Comm Time: 42.026 ms, Bubble Time: 19.643 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 39.311 ms, Bubble Time: 21.049 ms, Imbalance Overhead: 1.310 ms
    The estimated cost with 4 DP ways is 113.298 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 212.846 ms
Partition 0 [0, 32) has cost: 212.846 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 140.753 ms
GPU 0, Compute+Comm Time: 140.753 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 151.074 ms
GPU 0, Compute+Comm Time: 151.074 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 306.419 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 7 initializing the weights for op[0, 257)...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.6083	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6017	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5943	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5968	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5915	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5892	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5930	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5885	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5885	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5869	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5831	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5754	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5707	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5712	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5631	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5611	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5512	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5474	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5556	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5446	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5437	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5430	TrainAcc 0.3222	ValidAcc 0.3202	TestAcc 0.3159	BestValid 0.4239
	Epoch 1150:	Loss 1.5421	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1200:	Loss 1.5420	TrainAcc 0.3569	ValidAcc 0.3582	TestAcc 0.3552	BestValid 0.4239
	Epoch 1250:	Loss 1.5435	TrainAcc 0.3637	ValidAcc 0.3634	TestAcc 0.3659	BestValid 0.4239
	Epoch 1300:	Loss 1.5418	TrainAcc 0.3252	ValidAcc 0.3233	TestAcc 0.3193	BestValid 0.4239
	Epoch 1350:	Loss 1.5416	TrainAcc 0.3246	ValidAcc 0.3214	TestAcc 0.3190	BestValid 0.4239
	Epoch 1400:	Loss 1.5395	TrainAcc 0.3426	ValidAcc 0.3412	TestAcc 0.3376	BestValid 0.4239
	Epoch 1450:	Loss 1.5415	TrainAcc 0.3693	ValidAcc 0.3727	TestAcc 0.3670	BestValid 0.4239
	Epoch 1500:	Loss 1.5369	TrainAcc 0.3485	ValidAcc 0.3498	TestAcc 0.3479	BestValid 0.4239
	Epoch 1550:	Loss 1.5378	TrainAcc 0.3166	ValidAcc 0.3147	TestAcc 0.3120	BestValid 0.4239
	Epoch 1600:	Loss 1.5371	TrainAcc 0.3905	ValidAcc 0.3918	TestAcc 0.3909	BestValid 0.4239
	Epoch 1650:	Loss 1.5367	TrainAcc 0.3684	ValidAcc 0.3698	TestAcc 0.3693	BestValid 0.4239
	Epoch 1700:	Loss 1.5361	TrainAcc 0.4030	ValidAcc 0.4048	TestAcc 0.4038	BestValid 0.4239
	Epoch 1750:	Loss 1.5357	TrainAcc 0.4001	ValidAcc 0.4003	TestAcc 0.3985	BestValid 0.4239
	Epoch 1800:	Loss 1.5363	TrainAcc 0.4099	ValidAcc 0.4090	TestAcc 0.4119	BestValid 0.4239
	Epoch 1850:	Loss 1.5329	TrainAcc 0.3927	ValidAcc 0.3933	TestAcc 0.3920	BestValid 0.4239
	Epoch 1900:	Loss 1.5345	TrainAcc 0.3938	ValidAcc 0.3953	TestAcc 0.3923	BestValid 0.4239
	Epoch 1950:	Loss 1.5332	TrainAcc 0.3852	ValidAcc 0.3884	TestAcc 0.3851	BestValid 0.4239
	Epoch 2000:	Loss 1.5298	TrainAcc 0.3981	ValidAcc 0.3984	TestAcc 0.3978	BestValid 0.4239
	Epoch 2050:	Loss 1.5336	TrainAcc 0.3982	ValidAcc 0.4017	TestAcc 0.3995	BestValid 0.4239
	Epoch 2100:	Loss 1.5330	TrainAcc 0.4071	ValidAcc 0.4102	TestAcc 0.4088	BestValid 0.4239
	Epoch 2150:	Loss 1.5308	TrainAcc 0.3994	ValidAcc 0.4009	TestAcc 0.3995	BestValid 0.4239
	Epoch 2200:	Loss 1.5298	TrainAcc 0.3984	ValidAcc 0.3992	TestAcc 0.3997	BestValid 0.4239
	Epoch 2250:	Loss 1.5315	TrainAcc 0.3857	ValidAcc 0.3886	TestAcc 0.3871	BestValid 0.4239
	Epoch 2300:	Loss 1.5317	TrainAcc 0.4152	ValidAcc 0.4168	TestAcc 0.4171	BestValid 0.4239
	Epoch 2350:	Loss 1.5303	TrainAcc 0.4100	ValidAcc 0.4116	TestAcc 0.4127	BestValid 0.4239
	Epoch 2400:	Loss 1.5339	TrainAcc 0.4171	ValidAcc 0.4191	TestAcc 0.4193	BestValid 0.4239
	Epoch 2450:	Loss 1.5268	TrainAcc 0.3554	ValidAcc 0.3591	TestAcc 0.3584	BestValid 0.4239
	Epoch 2500:	Loss 1.5289	TrainAcc 0.3777	ValidAcc 0.3822	TestAcc 0.3794	BestValid 0.4239
	Epoch 2550:	Loss 1.5281	TrainAcc 0.4131	ValidAcc 0.4153	TestAcc 0.4154	BestValid 0.4239
	Epoch 2600:	Loss 1.5306	TrainAcc 0.4150	ValidAcc 0.4173	TestAcc 0.4171	BestValid 0.4239
	Epoch 2650:	Loss 1.5253	TrainAcc 0.4174	ValidAcc 0.4193	TestAcc 0.4193	BestValid 0.4239
	Epoch 2700:	Loss 1.5276	TrainAcc 0.4196	ValidAcc 0.4214	TestAcc 0.4212	BestValid 0.4239
	Epoch 2750:	Loss 1.5268	TrainAcc 0.4208	ValidAcc 0.4231	TestAcc 0.4228	BestValid 0.4239
	Epoch 2800:	Loss 1.5220	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5247	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5230	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5198	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5195	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5192	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5171	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5178	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5173	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5183	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5151	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5141	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5144	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5151	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5138	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5155	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5120	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5185	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5130	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5113	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5125	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5094	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5080	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5114	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5099	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5136	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5083	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5084	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5071	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5097	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5080	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5091	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5050	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5068	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5011	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5012	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5051	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5028	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5033	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.4999	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5011	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5023	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.4997	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5026	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5062	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.172 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.227 ms (Max: 0.293, Min: 0.041, Sum: 1.815)
Cluster-Wide Average, Compute: 37.216 ms (Max: 37.630, Min: 35.498, Sum: 297.725)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.060)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.018, Min: 0.014, Sum: 0.125)
Cluster-Wide Average, Communication-Graph: 123.139 ms (Max: 124.833, Min: 122.717, Sum: 985.115)
Cluster-Wide Average, Optimization: 5.713 ms (Max: 5.784, Min: 5.618, Sum: 45.705)
Cluster-Wide Average, Others: 5.863 ms (Max: 5.999, Min: 5.783, Sum: 46.907)
****** Breakdown Sum: 172.182 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.880 GB (Max: 7.309, Min: 6.805, Sum: 55.040)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.895 Gbps (Max: 51.630, Min: 16.848, Sum: 343.158)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.037 GB
Total communication (cluster-wide, per-epoch): 4.620 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
