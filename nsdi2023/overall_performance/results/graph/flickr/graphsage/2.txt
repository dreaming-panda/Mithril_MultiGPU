Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the Feature Vector...
        It takes 0.017 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.018 seconds.
Building the Feature Vector...
        It takes 0.029 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
        It takes 0.008 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.614 Gbps (per GPU), 484.908 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.320 Gbps (per GPU), 482.557 Gbps (aggregated)
The layer-level communication performance: 60.312 Gbps (per GPU), 482.494 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.075 Gbps (per GPU), 480.604 Gbps (aggregated)
The layer-level communication performance: 60.043 Gbps (per GPU), 480.345 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.833 Gbps (per GPU), 478.668 Gbps (aggregated)
The layer-level communication performance: 59.749 Gbps (per GPU), 477.994 Gbps (aggregated)
The layer-level communication performance: 59.785 Gbps (per GPU), 478.282 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 154.248 Gbps (per GPU), 1233.982 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.268 Gbps (per GPU), 1234.141 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.253 Gbps (per GPU), 1234.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.870 Gbps (per GPU), 1254.958 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.256 Gbps (per GPU), 1234.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.273 Gbps (per GPU), 1234.183 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.256 Gbps (per GPU), 1234.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.197 Gbps (per GPU), 1233.573 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.046 Gbps (per GPU), 816.370 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.044 Gbps (per GPU), 816.350 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.045 Gbps (per GPU), 816.363 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.043 Gbps (per GPU), 816.343 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.040 Gbps (per GPU), 816.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.039 Gbps (per GPU), 816.310 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.041 Gbps (per GPU), 816.330 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.042 Gbps (per GPU), 816.337 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.697 Gbps (per GPU), 253.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.695 Gbps (per GPU), 253.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.695 Gbps (per GPU), 253.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.690 Gbps (per GPU), 253.521 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.692 Gbps (per GPU), 253.538 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.693 Gbps (per GPU), 253.546 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.692 Gbps (per GPU), 253.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.691 Gbps (per GPU), 253.529 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.70ms  0.84ms  0.50ms  3.42 11.16K  0.12M
 chk_1  1.70ms  0.84ms  0.50ms  3.39 11.16K  0.11M
 chk_2  1.65ms  0.81ms  0.47ms  3.52 11.16K  0.11M
 chk_3  1.62ms  0.77ms  0.43ms  3.78 11.16K  0.12M
 chk_4  1.68ms  0.84ms  0.50ms  3.37 11.16K  0.11M
 chk_5  1.67ms  0.83ms  0.49ms  3.43 11.16K  0.10M
 chk_6  1.67ms  0.84ms  0.49ms  3.41 11.16K  0.12M
 chk_7  1.68ms  0.84ms  0.50ms  3.36 11.16K  0.11M
   Avg  1.67  0.83  0.48
   Max  1.70  0.84  0.50
   Min  1.62  0.77  0.43
 Ratio  1.05  1.09  1.18
   Var  0.00  0.00  0.00
Profiling takes 0.350 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 30.328 ms
Partition 0 [0, 3) has cost: 26.608 ms
Partition 1 [3, 7) has cost: 26.453 ms
Partition 2 [7, 11) has cost: 26.453 ms
Partition 3 [11, 15) has cost: 26.453 ms
Partition 4 [15, 19) has cost: 26.453 ms
Partition 5 [19, 23) has cost: 26.453 ms
Partition 6 [23, 27) has cost: 26.453 ms
Partition 7 [27, 32) has cost: 30.328 ms
The optimal partitioning:
[0, 3)
[3, 7)
[7, 11)
[11, 15)
[15, 19)
[19, 23)
[23, 27)
[27, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.644 ms
GPU 0, Compute+Comm Time: 12.849 ms, Bubble Time: 12.381 ms, Imbalance Overhead: 0.414 ms
GPU 1, Compute+Comm Time: 12.948 ms, Bubble Time: 12.195 ms, Imbalance Overhead: 0.501 ms
GPU 2, Compute+Comm Time: 12.948 ms, Bubble Time: 12.080 ms, Imbalance Overhead: 0.616 ms
GPU 3, Compute+Comm Time: 12.948 ms, Bubble Time: 12.046 ms, Imbalance Overhead: 0.650 ms
GPU 4, Compute+Comm Time: 12.948 ms, Bubble Time: 11.893 ms, Imbalance Overhead: 0.803 ms
GPU 5, Compute+Comm Time: 12.948 ms, Bubble Time: 11.764 ms, Imbalance Overhead: 0.932 ms
GPU 6, Compute+Comm Time: 12.948 ms, Bubble Time: 11.623 ms, Imbalance Overhead: 1.073 ms
GPU 7, Compute+Comm Time: 14.173 ms, Bubble Time: 11.471 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 46.034 ms
GPU 0, Compute+Comm Time: 25.579 ms, Bubble Time: 20.455 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 22.929 ms, Bubble Time: 20.788 ms, Imbalance Overhead: 2.317 ms
GPU 2, Compute+Comm Time: 22.929 ms, Bubble Time: 21.101 ms, Imbalance Overhead: 2.004 ms
GPU 3, Compute+Comm Time: 22.929 ms, Bubble Time: 21.388 ms, Imbalance Overhead: 1.717 ms
GPU 4, Compute+Comm Time: 22.929 ms, Bubble Time: 21.719 ms, Imbalance Overhead: 1.386 ms
GPU 5, Compute+Comm Time: 22.929 ms, Bubble Time: 21.820 ms, Imbalance Overhead: 1.285 ms
GPU 6, Compute+Comm Time: 22.929 ms, Bubble Time: 22.022 ms, Imbalance Overhead: 1.083 ms
GPU 7, Compute+Comm Time: 23.182 ms, Bubble Time: 22.342 ms, Imbalance Overhead: 0.510 ms
The estimated cost of the whole pipeline: 75.262 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 56.782 ms
Partition 0 [0, 7) has cost: 53.061 ms
Partition 1 [7, 15) has cost: 52.907 ms
Partition 2 [15, 23) has cost: 52.907 ms
Partition 3 [23, 32) has cost: 56.782 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.379 ms
GPU 0, Compute+Comm Time: 16.267 ms, Bubble Time: 13.417 ms, Imbalance Overhead: 0.695 ms
GPU 1, Compute+Comm Time: 16.799 ms, Bubble Time: 12.999 ms, Imbalance Overhead: 0.582 ms
GPU 2, Compute+Comm Time: 16.799 ms, Bubble Time: 12.748 ms, Imbalance Overhead: 0.833 ms
GPU 3, Compute+Comm Time: 17.894 ms, Bubble Time: 12.486 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.875 ms
GPU 0, Compute+Comm Time: 28.743 ms, Bubble Time: 20.132 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 26.918 ms, Bubble Time: 20.571 ms, Imbalance Overhead: 1.386 ms
GPU 2, Compute+Comm Time: 26.918 ms, Bubble Time: 20.998 ms, Imbalance Overhead: 0.959 ms
GPU 3, Compute+Comm Time: 26.562 ms, Bubble Time: 21.557 ms, Imbalance Overhead: 0.756 ms
    The estimated cost with 2 DP ways is 83.217 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 109.689 ms
Partition 0 [0, 15) has cost: 105.968 ms
Partition 1 [15, 32) has cost: 109.689 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 45.698 ms
GPU 0, Compute+Comm Time: 28.836 ms, Bubble Time: 15.656 ms, Imbalance Overhead: 1.206 ms
GPU 1, Compute+Comm Time: 31.289 ms, Bubble Time: 14.408 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.303 ms
GPU 0, Compute+Comm Time: 41.786 ms, Bubble Time: 19.517 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 39.076 ms, Bubble Time: 20.916 ms, Imbalance Overhead: 1.312 ms
    The estimated cost with 4 DP ways is 112.351 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 215.657 ms
Partition 0 [0, 32) has cost: 215.657 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 134.757 ms
GPU 0, Compute+Comm Time: 134.757 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 145.148 ms
GPU 0, Compute+Comm Time: 145.148 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 293.901 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 17.1528	TrainAcc 0.1427	ValidAcc 0.1428	TestAcc 0.1444	BestValid 0.1428
	Epoch 50:	Loss 1.6121	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6053	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5984	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5942	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5934	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5928	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5877	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5943	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5867	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5858	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5862	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5872	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5866	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5862	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5879	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5852	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5860	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5851	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5874	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5863	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5830	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5840	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5846	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5844	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5839	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5990	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5790	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5809	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5783	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5781	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5768	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5773	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5725	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5750	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5758	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5730	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5678	TrainAcc 0.3774	ValidAcc 0.3806	TestAcc 0.3796	BestValid 0.4239
	Epoch 1900:	Loss 1.5584	TrainAcc 0.4074	ValidAcc 0.4091	TestAcc 0.4090	BestValid 0.4239
	Epoch 1950:	Loss 1.5546	TrainAcc 0.3869	ValidAcc 0.3889	TestAcc 0.3887	BestValid 0.4239
	Epoch 2000:	Loss 1.5548	TrainAcc 0.3743	ValidAcc 0.3767	TestAcc 0.3755	BestValid 0.4239
	Epoch 2050:	Loss 1.5489	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5465	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5441	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5438	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5425	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5428	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5430	TrainAcc 0.4237	ValidAcc 0.4288	TestAcc 0.4240	BestValid 0.4288
	Epoch 2400:	Loss 1.5425	TrainAcc 0.4078	ValidAcc 0.4082	TestAcc 0.4070	BestValid 0.4288
	Epoch 2450:	Loss 1.5391	TrainAcc 0.4113	ValidAcc 0.4101	TestAcc 0.4108	BestValid 0.4288
	Epoch 2500:	Loss 1.5413	TrainAcc 0.3979	ValidAcc 0.4002	TestAcc 0.3987	BestValid 0.4288
	Epoch 2550:	Loss 1.5410	TrainAcc 0.4062	ValidAcc 0.4050	TestAcc 0.4042	BestValid 0.4288
	Epoch 2600:	Loss 1.5393	TrainAcc 0.4097	ValidAcc 0.4077	TestAcc 0.4092	BestValid 0.4288
	Epoch 2650:	Loss 1.5388	TrainAcc 0.3937	ValidAcc 0.3958	TestAcc 0.3959	BestValid 0.4288
	Epoch 2700:	Loss 1.5368	TrainAcc 0.4064	ValidAcc 0.4047	TestAcc 0.4047	BestValid 0.4288
	Epoch 2750:	Loss 1.5344	TrainAcc 0.4047	ValidAcc 0.4044	TestAcc 0.4019	BestValid 0.4288
	Epoch 2800:	Loss 1.5354	TrainAcc 0.4061	ValidAcc 0.4064	TestAcc 0.4055	BestValid 0.4288
	Epoch 2850:	Loss 1.5374	TrainAcc 0.4221	ValidAcc 0.4243	TestAcc 0.4244	BestValid 0.4288
	Epoch 2900:	Loss 1.5363	TrainAcc 0.4191	ValidAcc 0.4218	TestAcc 0.4215	BestValid 0.4288
	Epoch 2950:	Loss 1.5349	TrainAcc 0.4260	ValidAcc 0.4296	TestAcc 0.4330	BestValid 0.4296
	Epoch 3000:	Loss 1.5346	TrainAcc 0.4236	ValidAcc 0.4235	TestAcc 0.4264	BestValid 0.4296
	Epoch 3050:	Loss 1.5340	TrainAcc 0.4271	ValidAcc 0.4258	TestAcc 0.4299	BestValid 0.4296
	Epoch 3100:	Loss 1.5336	TrainAcc 0.4225	ValidAcc 0.4258	TestAcc 0.4280	BestValid 0.4296
	Epoch 3150:	Loss 1.5333	TrainAcc 0.4259	ValidAcc 0.4279	TestAcc 0.4295	BestValid 0.4296
	Epoch 3200:	Loss 1.5316	TrainAcc 0.4283	ValidAcc 0.4325	TestAcc 0.4350	BestValid 0.4325
	Epoch 3250:	Loss 1.5302	TrainAcc 0.4257	ValidAcc 0.4306	TestAcc 0.4274	BestValid 0.4325
	Epoch 3300:	Loss 1.5298	TrainAcc 0.4413	ValidAcc 0.4436	TestAcc 0.4444	BestValid 0.4436
	Epoch 3350:	Loss 1.5299	TrainAcc 0.4196	ValidAcc 0.4219	TestAcc 0.4210	BestValid 0.4436
	Epoch 3400:	Loss 1.5358	TrainAcc 0.3945	ValidAcc 0.3982	TestAcc 0.3982	BestValid 0.4436
	Epoch 3450:	Loss 1.5327	TrainAcc 0.4138	ValidAcc 0.4161	TestAcc 0.4150	BestValid 0.4436
	Epoch 3500:	Loss 1.5266	TrainAcc 0.4200	ValidAcc 0.4221	TestAcc 0.4217	BestValid 0.4436
	Epoch 3550:	Loss 1.5278	TrainAcc 0.4099	ValidAcc 0.4118	TestAcc 0.4120	BestValid 0.4436
	Epoch 3600:	Loss 1.5253	TrainAcc 0.4245	ValidAcc 0.4239	TestAcc 0.4242	BestValid 0.4436
	Epoch 3650:	Loss 1.5252	TrainAcc 0.4125	ValidAcc 0.4138	TestAcc 0.4098	BestValid 0.4436
	Epoch 3700:	Loss 1.5253	TrainAcc 0.4281	ValidAcc 0.4297	TestAcc 0.4308	BestValid 0.4436
	Epoch 3750:	Loss 1.5261	TrainAcc 0.3777	ValidAcc 0.3767	TestAcc 0.3783	BestValid 0.4436
	Epoch 3800:	Loss 1.5241	TrainAcc 0.3945	ValidAcc 0.3936	TestAcc 0.3914	BestValid 0.4436
	Epoch 3850:	Loss 1.5259	TrainAcc 0.3664	ValidAcc 0.3630	TestAcc 0.3613	BestValid 0.4436
	Epoch 3900:	Loss 1.5223	TrainAcc 0.4164	ValidAcc 0.4213	TestAcc 0.4177	BestValid 0.4436
	Epoch 3950:	Loss 1.5244	TrainAcc 0.3991	ValidAcc 0.4006	TestAcc 0.4032	BestValid 0.4436
	Epoch 4000:	Loss 1.5191	TrainAcc 0.3952	ValidAcc 0.4004	TestAcc 0.3949	BestValid 0.4436
	Epoch 4050:	Loss 1.5243	TrainAcc 0.4026	ValidAcc 0.4061	TestAcc 0.4052	BestValid 0.4436
	Epoch 4100:	Loss 1.5183	TrainAcc 0.4081	ValidAcc 0.4069	TestAcc 0.4101	BestValid 0.4436
	Epoch 4150:	Loss 1.5048	TrainAcc 0.4326	ValidAcc 0.4308	TestAcc 0.4380	BestValid 0.4436
	Epoch 4200:	Loss 1.4938	TrainAcc 0.4203	ValidAcc 0.4230	TestAcc 0.4225	BestValid 0.4436
	Epoch 4250:	Loss 1.4939	TrainAcc 0.4202	ValidAcc 0.4229	TestAcc 0.4223	BestValid 0.4436
	Epoch 4300:	Loss 1.4892	TrainAcc 0.4203	ValidAcc 0.4230	TestAcc 0.4224	BestValid 0.4436
	Epoch 4350:	Loss 1.4804	TrainAcc 0.4228	ValidAcc 0.4247	TestAcc 0.4225	BestValid 0.4436
	Epoch 4400:	Loss 1.4700	TrainAcc 0.4215	ValidAcc 0.4238	TestAcc 0.4233	BestValid 0.4436
	Epoch 4450:	Loss 1.4678	TrainAcc 0.4257	ValidAcc 0.4255	TestAcc 0.4242	BestValid 0.4436
	Epoch 4500:	Loss 1.4655	TrainAcc 0.4255	ValidAcc 0.4273	TestAcc 0.4242	BestValid 0.4436
	Epoch 4550:	Loss 1.4638	TrainAcc 0.4220	ValidAcc 0.4243	TestAcc 0.4237	BestValid 0.4436
	Epoch 4600:	Loss 1.4592	TrainAcc 0.4248	ValidAcc 0.4269	TestAcc 0.4237	BestValid 0.4436
	Epoch 4650:	Loss 1.4566	TrainAcc 0.4235	ValidAcc 0.4249	TestAcc 0.4236	BestValid 0.4436
	Epoch 4700:	Loss 1.4606	TrainAcc 0.4217	ValidAcc 0.4242	TestAcc 0.4235	BestValid 0.4436
	Epoch 4750:	Loss 1.4532	TrainAcc 0.4219	ValidAcc 0.4239	TestAcc 0.4235	BestValid 0.4436
	Epoch 4800:	Loss 1.4606	TrainAcc 0.4237	ValidAcc 0.4260	TestAcc 0.4236	BestValid 0.4436
	Epoch 4850:	Loss 1.4531	TrainAcc 0.4219	ValidAcc 0.4243	TestAcc 0.4235	BestValid 0.4436
	Epoch 4900:	Loss 1.4517	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4436
	Epoch 4950:	Loss 1.4524	TrainAcc 0.4216	ValidAcc 0.4238	TestAcc 0.4234	BestValid 0.4436
	Epoch 5000:	Loss 1.4537	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4233	BestValid 0.4436
****** Epoch Time (Excluding Evaluation Cost): 0.173 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.201 ms (Max: 0.308, Min: 0.041, Sum: 1.609)
Cluster-Wide Average, Compute: 37.293 ms (Max: 37.994, Min: 35.384, Sum: 298.342)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.060)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.018, Min: 0.014, Sum: 0.127)
Cluster-Wide Average, Communication-Graph: 123.787 ms (Max: 125.693, Min: 123.059, Sum: 990.295)
Cluster-Wide Average, Optimization: 5.762 ms (Max: 5.843, Min: 5.661, Sum: 46.099)
Cluster-Wide Average, Others: 5.873 ms (Max: 5.998, Min: 5.759, Sum: 46.986)
****** Breakdown Sum: 172.940 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.880 GB (Max: 7.311, Min: 6.805, Sum: 55.042)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.667 Gbps (Max: 51.290, Min: 16.719, Sum: 341.338)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.037 GB
Total communication (cluster-wide, per-epoch): 4.620 GB
****** Accuracy Results ******
Highest valid_acc: 0.4436
Target test_acc: 0.4444
Epoch to reach the target acc: 3299
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
