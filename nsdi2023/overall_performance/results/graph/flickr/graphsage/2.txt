Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.030 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.018 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
        It takes 0.018 seconds.
        It takes 0.020 seconds.
        It takes 0.022 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.

GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.650 Gbps (per GPU), 477.200 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.346 Gbps (per GPU), 474.772 Gbps (aggregated)
The layer-level communication performance: 59.406 Gbps (per GPU), 475.245 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.099 Gbps (per GPU), 472.795 Gbps (aggregated)
The layer-level communication performance: 59.138 Gbps (per GPU), 473.100 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.846 Gbps (per GPU), 470.771 Gbps (aggregated)
The layer-level communication performance: 58.803 Gbps (per GPU), 470.425 Gbps (aggregated)
The layer-level communication performance: 58.769 Gbps (per GPU), 470.153 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.267 Gbps (per GPU), 1266.133 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.273 Gbps (per GPU), 1266.181 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.264 Gbps (per GPU), 1266.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.270 Gbps (per GPU), 1266.157 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.255 Gbps (per GPU), 1266.038 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.240 Gbps (per GPU), 1265.918 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.267 Gbps (per GPU), 1266.133 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.201 Gbps (per GPU), 1265.606 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.384 Gbps (per GPU), 811.075 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.379 Gbps (per GPU), 811.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.379 Gbps (per GPU), 811.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.384 Gbps (per GPU), 811.076 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.379 Gbps (per GPU), 811.036 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.384 Gbps (per GPU), 811.075 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.382 Gbps (per GPU), 811.055 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.133 Gbps (per GPU), 809.067 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.587 Gbps (per GPU), 276.699 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.584 Gbps (per GPU), 276.670 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.587 Gbps (per GPU), 276.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.585 Gbps (per GPU), 276.677 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.581 Gbps (per GPU), 276.648 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.579 Gbps (per GPU), 276.633 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.583 Gbps (per GPU), 276.665 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.580 Gbps (per GPU), 276.642 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.69ms  0.83ms  0.48ms  3.49 11.16K  0.12M
 chk_1  1.69ms  0.83ms  0.59ms  2.87 11.16K  0.11M
 chk_2  1.64ms  0.80ms  0.51ms  3.22 11.16K  0.11M
 chk_3  1.60ms  0.76ms  0.42ms  3.83 11.16K  0.12M
 chk_4  1.66ms  0.83ms  0.49ms  3.42 11.16K  0.11M
 chk_5  1.65ms  0.82ms  0.47ms  3.48 11.16K  0.10M
 chk_6  1.65ms  0.82ms  0.48ms  3.46 11.16K  0.12M
 chk_7  1.66ms  0.83ms  0.49ms  3.42 11.16K  0.11M
   Avg  1.66  0.81  0.49
   Max  1.69  0.83  0.59
   Min  1.60  0.76  0.42
 Ratio  1.06  1.10  1.41
   Var  0.00  0.00  0.00
Profiling takes 0.346 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 29.964 ms
Partition 0 [0, 3) has cost: 26.269 ms
Partition 1 [3, 7) has cost: 26.041 ms
Partition 2 [7, 11) has cost: 26.041 ms
Partition 3 [11, 15) has cost: 26.041 ms
Partition 4 [15, 19) has cost: 26.041 ms
Partition 5 [19, 23) has cost: 26.041 ms
Partition 6 [23, 27) has cost: 26.041 ms
Partition 7 [27, 32) has cost: 29.964 ms
The optimal partitioning:
[0, 3)
[3, 7)
[7, 11)
[11, 15)
[15, 19)
[19, 23)
[23, 27)
[27, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.549 ms
GPU 0, Compute+Comm Time: 12.798 ms, Bubble Time: 12.349 ms, Imbalance Overhead: 0.402 ms
GPU 1, Compute+Comm Time: 12.879 ms, Bubble Time: 12.138 ms, Imbalance Overhead: 0.533 ms
GPU 2, Compute+Comm Time: 12.879 ms, Bubble Time: 12.000 ms, Imbalance Overhead: 0.671 ms
GPU 3, Compute+Comm Time: 12.879 ms, Bubble Time: 11.974 ms, Imbalance Overhead: 0.697 ms
GPU 4, Compute+Comm Time: 12.879 ms, Bubble Time: 11.825 ms, Imbalance Overhead: 0.846 ms
GPU 5, Compute+Comm Time: 12.879 ms, Bubble Time: 11.701 ms, Imbalance Overhead: 0.970 ms
GPU 6, Compute+Comm Time: 12.879 ms, Bubble Time: 11.566 ms, Imbalance Overhead: 1.105 ms
GPU 7, Compute+Comm Time: 14.131 ms, Bubble Time: 11.419 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 45.737 ms
GPU 0, Compute+Comm Time: 25.409 ms, Bubble Time: 20.329 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 22.739 ms, Bubble Time: 20.651 ms, Imbalance Overhead: 2.348 ms
GPU 2, Compute+Comm Time: 22.739 ms, Bubble Time: 20.954 ms, Imbalance Overhead: 2.045 ms
GPU 3, Compute+Comm Time: 22.739 ms, Bubble Time: 21.227 ms, Imbalance Overhead: 1.771 ms
GPU 4, Compute+Comm Time: 22.739 ms, Bubble Time: 21.544 ms, Imbalance Overhead: 1.455 ms
GPU 5, Compute+Comm Time: 22.739 ms, Bubble Time: 21.638 ms, Imbalance Overhead: 1.361 ms
GPU 6, Compute+Comm Time: 22.739 ms, Bubble Time: 21.834 ms, Imbalance Overhead: 1.165 ms
GPU 7, Compute+Comm Time: 23.046 ms, Bubble Time: 22.196 ms, Imbalance Overhead: 0.495 ms
The estimated cost of the whole pipeline: 74.851 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 56.005 ms
Partition 0 [0, 7) has cost: 52.310 ms
Partition 1 [7, 15) has cost: 52.083 ms
Partition 2 [15, 23) has cost: 52.083 ms
Partition 3 [23, 32) has cost: 56.005 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.133 ms
GPU 0, Compute+Comm Time: 16.130 ms, Bubble Time: 13.329 ms, Imbalance Overhead: 0.674 ms
GPU 1, Compute+Comm Time: 16.640 ms, Bubble Time: 12.892 ms, Imbalance Overhead: 0.600 ms
GPU 2, Compute+Comm Time: 16.640 ms, Bubble Time: 12.627 ms, Imbalance Overhead: 0.866 ms
GPU 3, Compute+Comm Time: 17.760 ms, Bubble Time: 12.373 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.447 ms
GPU 0, Compute+Comm Time: 28.496 ms, Bubble Time: 19.950 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 26.644 ms, Bubble Time: 20.375 ms, Imbalance Overhead: 1.428 ms
GPU 2, Compute+Comm Time: 26.644 ms, Bubble Time: 20.790 ms, Imbalance Overhead: 1.013 ms
GPU 3, Compute+Comm Time: 26.335 ms, Bubble Time: 21.386 ms, Imbalance Overhead: 0.726 ms
    The estimated cost with 2 DP ways is 82.508 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 108.088 ms
Partition 0 [0, 15) has cost: 104.393 ms
Partition 1 [15, 32) has cost: 108.088 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 45.817 ms
GPU 0, Compute+Comm Time: 28.881 ms, Bubble Time: 15.709 ms, Imbalance Overhead: 1.227 ms
GPU 1, Compute+Comm Time: 31.387 ms, Bubble Time: 14.431 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.284 ms
GPU 0, Compute+Comm Time: 41.784 ms, Bubble Time: 19.500 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 39.033 ms, Bubble Time: 20.943 ms, Imbalance Overhead: 1.308 ms
    The estimated cost with 4 DP ways is 112.456 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 212.481 ms
Partition 0 [0, 32) has cost: 212.481 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 124.090 ms
GPU 0, Compute+Comm Time: 124.090 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 134.407 ms
GPU 0, Compute+Comm Time: 134.407 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 271.422 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.6111	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6052	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5984	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5943	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5934	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5928	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5878	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5944	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5866	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5858	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5863	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5873	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5866	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5861	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5879	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5849	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5860	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5848	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5875	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5862	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5833	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5836	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5843	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5840	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5830	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.6080	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5792	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5817	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5773	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5781	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5757	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5775	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5726	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5750	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5774	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5760	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5663	TrainAcc 0.3857	ValidAcc 0.3871	TestAcc 0.3867	BestValid 0.4239
	Epoch 1900:	Loss 1.5583	TrainAcc 0.3836	ValidAcc 0.3859	TestAcc 0.3854	BestValid 0.4239
	Epoch 1950:	Loss 1.5546	TrainAcc 0.3914	ValidAcc 0.3932	TestAcc 0.3926	BestValid 0.4239
	Epoch 2000:	Loss 1.5564	TrainAcc 0.3978	ValidAcc 0.3985	TestAcc 0.3983	BestValid 0.4239
	Epoch 2050:	Loss 1.5487	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5464	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5444	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5437	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5431	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5441	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5425	TrainAcc 0.4152	ValidAcc 0.4195	TestAcc 0.4152	BestValid 0.4239
	Epoch 2400:	Loss 1.5423	TrainAcc 0.4206	ValidAcc 0.4237	TestAcc 0.4183	BestValid 0.4239
	Epoch 2450:	Loss 1.5387	TrainAcc 0.3997	ValidAcc 0.4010	TestAcc 0.4018	BestValid 0.4239
	Epoch 2500:	Loss 1.5411	TrainAcc 0.4138	ValidAcc 0.4125	TestAcc 0.4123	BestValid 0.4239
	Epoch 2550:	Loss 1.5469	TrainAcc 0.4014	ValidAcc 0.4018	TestAcc 0.4032	BestValid 0.4239
	Epoch 2600:	Loss 1.5383	TrainAcc 0.4224	ValidAcc 0.4219	TestAcc 0.4217	BestValid 0.4239
	Epoch 2650:	Loss 1.5391	TrainAcc 0.4206	ValidAcc 0.4224	TestAcc 0.4219	BestValid 0.4239
	Epoch 2700:	Loss 1.5375	TrainAcc 0.3996	ValidAcc 0.3967	TestAcc 0.3992	BestValid 0.4239
	Epoch 2750:	Loss 1.5346	TrainAcc 0.4312	ValidAcc 0.4363	TestAcc 0.4336	BestValid 0.4363
	Epoch 2800:	Loss 1.5351	TrainAcc 0.4227	ValidAcc 0.4185	TestAcc 0.4203	BestValid 0.4363
	Epoch 2850:	Loss 1.5400	TrainAcc 0.4215	ValidAcc 0.4217	TestAcc 0.4209	BestValid 0.4363
	Epoch 2900:	Loss 1.5362	TrainAcc 0.4176	ValidAcc 0.4206	TestAcc 0.4167	BestValid 0.4363
	Epoch 2950:	Loss 1.5349	TrainAcc 0.4157	ValidAcc 0.4188	TestAcc 0.4172	BestValid 0.4363
	Epoch 3000:	Loss 1.5344	TrainAcc 0.3945	ValidAcc 0.3920	TestAcc 0.3940	BestValid 0.4363
	Epoch 3050:	Loss 1.5332	TrainAcc 0.4298	ValidAcc 0.4322	TestAcc 0.4329	BestValid 0.4363
	Epoch 3100:	Loss 1.5342	TrainAcc 0.4188	ValidAcc 0.4239	TestAcc 0.4215	BestValid 0.4363
	Epoch 3150:	Loss 1.5338	TrainAcc 0.4344	ValidAcc 0.4366	TestAcc 0.4384	BestValid 0.4366
	Epoch 3200:	Loss 1.5318	TrainAcc 0.4390	ValidAcc 0.4399	TestAcc 0.4416	BestValid 0.4399
	Epoch 3250:	Loss 1.5302	TrainAcc 0.4208	ValidAcc 0.4248	TestAcc 0.4200	BestValid 0.4399
	Epoch 3300:	Loss 1.5301	TrainAcc 0.3822	ValidAcc 0.3836	TestAcc 0.3817	BestValid 0.4399
	Epoch 3350:	Loss 1.5306	TrainAcc 0.3927	ValidAcc 0.3911	TestAcc 0.3913	BestValid 0.4399
	Epoch 3400:	Loss 1.5385	TrainAcc 0.4063	ValidAcc 0.4088	TestAcc 0.4082	BestValid 0.4399
	Epoch 3450:	Loss 1.5332	TrainAcc 0.4108	ValidAcc 0.4088	TestAcc 0.4075	BestValid 0.4399
	Epoch 3500:	Loss 1.5282	TrainAcc 0.4384	ValidAcc 0.4368	TestAcc 0.4389	BestValid 0.4399
	Epoch 3550:	Loss 1.5291	TrainAcc 0.4160	ValidAcc 0.4127	TestAcc 0.4146	BestValid 0.4399
	Epoch 3600:	Loss 1.5258	TrainAcc 0.4198	ValidAcc 0.4194	TestAcc 0.4185	BestValid 0.4399
	Epoch 3650:	Loss 1.5256	TrainAcc 0.4225	ValidAcc 0.4260	TestAcc 0.4260	BestValid 0.4399
	Epoch 3700:	Loss 1.5260	TrainAcc 0.4294	ValidAcc 0.4306	TestAcc 0.4299	BestValid 0.4399
	Epoch 3750:	Loss 1.5263	TrainAcc 0.4207	ValidAcc 0.4226	TestAcc 0.4237	BestValid 0.4399
	Epoch 3800:	Loss 1.5262	TrainAcc 0.4129	ValidAcc 0.4152	TestAcc 0.4146	BestValid 0.4399
	Epoch 3850:	Loss 1.5265	TrainAcc 0.3967	ValidAcc 0.3998	TestAcc 0.3982	BestValid 0.4399
	Epoch 3900:	Loss 1.5227	TrainAcc 0.4212	ValidAcc 0.4235	TestAcc 0.4233	BestValid 0.4399
	Epoch 3950:	Loss 1.5256	TrainAcc 0.4181	ValidAcc 0.4227	TestAcc 0.4197	BestValid 0.4399
	Epoch 4000:	Loss 1.5235	TrainAcc 0.4208	ValidAcc 0.4233	TestAcc 0.4227	BestValid 0.4399
	Epoch 4050:	Loss 1.5244	TrainAcc 0.4166	ValidAcc 0.4182	TestAcc 0.4187	BestValid 0.4399
	Epoch 4100:	Loss 1.5246	TrainAcc 0.4212	ValidAcc 0.4234	TestAcc 0.4231	BestValid 0.4399
	Epoch 4150:	Loss 1.5207	TrainAcc 0.4215	ValidAcc 0.4238	TestAcc 0.4234	BestValid 0.4399
	Epoch 4200:	Loss 1.5222	TrainAcc 0.4215	ValidAcc 0.4238	TestAcc 0.4234	BestValid 0.4399
	Epoch 4250:	Loss 1.5201	TrainAcc 0.4215	ValidAcc 0.4237	TestAcc 0.4233	BestValid 0.4399
	Epoch 4300:	Loss 1.5205	TrainAcc 0.3715	ValidAcc 0.3680	TestAcc 0.3661	BestValid 0.4399
	Epoch 4350:	Loss 1.5217	TrainAcc 0.4254	ValidAcc 0.4273	TestAcc 0.4280	BestValid 0.4399
	Epoch 4400:	Loss 1.5210	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4399
	Epoch 4450:	Loss 1.5166	TrainAcc 0.3964	ValidAcc 0.4005	TestAcc 0.3980	BestValid 0.4399
	Epoch 4500:	Loss 1.5136	TrainAcc 0.4152	ValidAcc 0.4160	TestAcc 0.4154	BestValid 0.4399
	Epoch 4550:	Loss 1.5127	TrainAcc 0.3641	ValidAcc 0.3616	TestAcc 0.3578	BestValid 0.4399
	Epoch 4600:	Loss 1.5157	TrainAcc 0.4126	ValidAcc 0.4137	TestAcc 0.4112	BestValid 0.4399
	Epoch 4650:	Loss 1.5065	TrainAcc 0.3900	ValidAcc 0.3897	TestAcc 0.3901	BestValid 0.4399
	Epoch 4700:	Loss 1.5019	TrainAcc 0.3848	ValidAcc 0.3839	TestAcc 0.3841	BestValid 0.4399
	Epoch 4750:	Loss 1.5314	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4399
	Epoch 4800:	Loss 1.4936	TrainAcc 0.4204	ValidAcc 0.4223	TestAcc 0.4226	BestValid 0.4399
	Epoch 4850:	Loss 1.4727	TrainAcc 0.4332	ValidAcc 0.4338	TestAcc 0.4337	BestValid 0.4399
	Epoch 4900:	Loss 1.4841	TrainAcc 0.4221	ValidAcc 0.4244	TestAcc 0.4234	BestValid 0.4399
	Epoch 4950:	Loss 1.4731	TrainAcc 0.4391	ValidAcc 0.4410	TestAcc 0.4381	BestValid 0.4410
	Epoch 5000:	Loss 1.4725	TrainAcc 0.4209	ValidAcc 0.4213	TestAcc 0.4203	BestValid 0.4410
****** Epoch Time (Excluding Evaluation Cost): 0.173 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.219 ms (Max: 0.312, Min: 0.041, Sum: 1.755)
Cluster-Wide Average, Compute: 37.250 ms (Max: 37.741, Min: 35.949, Sum: 298.000)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.061)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.018, Min: 0.014, Sum: 0.124)
Cluster-Wide Average, Communication-Graph: 123.458 ms (Max: 124.706, Min: 122.964, Sum: 987.662)
Cluster-Wide Average, Optimization: 5.757 ms (Max: 5.826, Min: 5.675, Sum: 46.060)
Cluster-Wide Average, Others: 5.869 ms (Max: 6.002, Min: 5.773, Sum: 46.952)
****** Breakdown Sum: 172.577 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.880 GB (Max: 7.311, Min: 6.805, Sum: 55.042)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.756 Gbps (Max: 51.458, Min: 16.881, Sum: 342.052)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.037 GB
Total communication (cluster-wide, per-epoch): 4.620 GB
****** Accuracy Results ******
Highest valid_acc: 0.4410
Target test_acc: 0.4381
Epoch to reach the target acc: 4949
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
