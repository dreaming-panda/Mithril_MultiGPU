Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.016 seconds.
        It takes 0.018 seconds.
        It takes 0.017 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.025 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.095 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.113 seconds.
        It takes 0.113 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.006 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.509 Gbps (per GPU), 476.073 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.222 Gbps (per GPU), 473.774 Gbps (aggregated)
The layer-level communication performance: 59.219 Gbps (per GPU), 473.751 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.966 Gbps (per GPU), 471.732 Gbps (aggregated)
The layer-level communication performance: 58.935 Gbps (per GPU), 471.480 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.731 Gbps (per GPU), 469.851 Gbps (aggregated)
The layer-level communication performance: 58.683 Gbps (per GPU), 469.468 Gbps (aggregated)
The layer-level communication performance: 58.654 Gbps (per GPU), 469.230 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.924 Gbps (per GPU), 1263.392 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.894 Gbps (per GPU), 1263.154 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.879 Gbps (per GPU), 1263.035 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.290 Gbps (per GPU), 1282.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.933 Gbps (per GPU), 1263.464 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.915 Gbps (per GPU), 1263.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.886 Gbps (per GPU), 1263.086 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.305 Gbps (per GPU), 1282.441 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.914 Gbps (per GPU), 807.308 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.855 Gbps (per GPU), 806.843 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.914 Gbps (per GPU), 807.315 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.862 Gbps (per GPU), 806.894 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.911 Gbps (per GPU), 807.289 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.863 Gbps (per GPU), 806.901 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.913 Gbps (per GPU), 807.301 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.776 Gbps (per GPU), 806.209 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.197 Gbps (per GPU), 257.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.199 Gbps (per GPU), 257.594 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.199 Gbps (per GPU), 257.590 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.200 Gbps (per GPU), 257.603 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.198 Gbps (per GPU), 257.581 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.193 Gbps (per GPU), 257.543 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.195 Gbps (per GPU), 257.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.193 Gbps (per GPU), 257.542 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.69ms  0.83ms  0.48ms  3.49 11.16K  0.12M
 chk_1  1.69ms  0.83ms  0.49ms  3.46 11.16K  0.11M
 chk_2  1.64ms  0.80ms  0.46ms  3.59 11.16K  0.11M
 chk_3  1.60ms  0.76ms  0.42ms  3.86 11.16K  0.12M
 chk_4  1.67ms  0.83ms  0.49ms  3.42 11.16K  0.11M
 chk_5  1.65ms  0.82ms  0.48ms  3.48 11.16K  0.10M
 chk_6  1.66ms  0.82ms  0.48ms  3.47 11.16K  0.12M
 chk_7  1.67ms  0.83ms  0.49ms  3.42 11.16K  0.11M
   Avg  1.66  0.81  0.47
   Max  1.69  0.83  0.49
   Min  1.60  0.76  0.42
 Ratio  1.06  1.10  1.17
   Var  0.00  0.00  0.00
Profiling takes 0.346 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 29.838 ms
Partition 0 [0, 3) has cost: 26.299 ms
Partition 1 [3, 7) has cost: 26.065 ms
Partition 2 [7, 11) has cost: 26.065 ms
Partition 3 [11, 15) has cost: 26.065 ms
Partition 4 [15, 19) has cost: 26.065 ms
Partition 5 [19, 23) has cost: 26.065 ms
Partition 6 [23, 27) has cost: 26.065 ms
Partition 7 [27, 32) has cost: 29.838 ms
The optimal partitioning:
[0, 3)
[3, 7)
[7, 11)
[11, 15)
[15, 19)
[19, 23)
[23, 27)
[27, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.517 ms
GPU 0, Compute+Comm Time: 12.826 ms, Bubble Time: 12.304 ms, Imbalance Overhead: 0.387 ms
GPU 1, Compute+Comm Time: 12.896 ms, Bubble Time: 12.126 ms, Imbalance Overhead: 0.495 ms
GPU 2, Compute+Comm Time: 12.896 ms, Bubble Time: 12.018 ms, Imbalance Overhead: 0.604 ms
GPU 3, Compute+Comm Time: 12.896 ms, Bubble Time: 11.990 ms, Imbalance Overhead: 0.631 ms
GPU 4, Compute+Comm Time: 12.896 ms, Bubble Time: 11.842 ms, Imbalance Overhead: 0.779 ms
GPU 5, Compute+Comm Time: 12.896 ms, Bubble Time: 11.717 ms, Imbalance Overhead: 0.905 ms
GPU 6, Compute+Comm Time: 12.896 ms, Bubble Time: 11.579 ms, Imbalance Overhead: 1.043 ms
GPU 7, Compute+Comm Time: 14.085 ms, Bubble Time: 11.432 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 45.709 ms
GPU 0, Compute+Comm Time: 25.351 ms, Bubble Time: 20.359 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 22.768 ms, Bubble Time: 20.678 ms, Imbalance Overhead: 2.263 ms
GPU 2, Compute+Comm Time: 22.768 ms, Bubble Time: 20.972 ms, Imbalance Overhead: 1.969 ms
GPU 3, Compute+Comm Time: 22.768 ms, Bubble Time: 21.245 ms, Imbalance Overhead: 1.696 ms
GPU 4, Compute+Comm Time: 22.768 ms, Bubble Time: 21.562 ms, Imbalance Overhead: 1.378 ms
GPU 5, Compute+Comm Time: 22.768 ms, Bubble Time: 21.652 ms, Imbalance Overhead: 1.289 ms
GPU 6, Compute+Comm Time: 22.768 ms, Bubble Time: 21.835 ms, Imbalance Overhead: 1.106 ms
GPU 7, Compute+Comm Time: 23.071 ms, Bubble Time: 22.141 ms, Imbalance Overhead: 0.497 ms
The estimated cost of the whole pipeline: 74.788 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 55.903 ms
Partition 0 [0, 7) has cost: 52.364 ms
Partition 1 [7, 15) has cost: 52.131 ms
Partition 2 [15, 23) has cost: 52.131 ms
Partition 3 [23, 32) has cost: 55.903 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.120 ms
GPU 0, Compute+Comm Time: 16.159 ms, Bubble Time: 13.297 ms, Imbalance Overhead: 0.664 ms
GPU 1, Compute+Comm Time: 16.666 ms, Bubble Time: 12.891 ms, Imbalance Overhead: 0.563 ms
GPU 2, Compute+Comm Time: 16.666 ms, Bubble Time: 12.641 ms, Imbalance Overhead: 0.813 ms
GPU 3, Compute+Comm Time: 17.732 ms, Bubble Time: 12.388 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.425 ms
GPU 0, Compute+Comm Time: 28.454 ms, Bubble Time: 19.970 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 26.675 ms, Bubble Time: 20.390 ms, Imbalance Overhead: 1.359 ms
GPU 2, Compute+Comm Time: 26.675 ms, Bubble Time: 20.794 ms, Imbalance Overhead: 0.956 ms
GPU 3, Compute+Comm Time: 26.357 ms, Bubble Time: 21.339 ms, Imbalance Overhead: 0.728 ms
    The estimated cost with 2 DP ways is 82.472 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 108.034 ms
Partition 0 [0, 15) has cost: 104.495 ms
Partition 1 [15, 32) has cost: 108.034 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 45.901 ms
GPU 0, Compute+Comm Time: 28.969 ms, Bubble Time: 15.727 ms, Imbalance Overhead: 1.205 ms
GPU 1, Compute+Comm Time: 31.428 ms, Bubble Time: 14.473 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.365 ms
GPU 0, Compute+Comm Time: 41.830 ms, Bubble Time: 19.534 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 39.124 ms, Bubble Time: 20.947 ms, Imbalance Overhead: 1.294 ms
    The estimated cost with 4 DP ways is 112.629 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 212.529 ms
Partition 0 [0, 32) has cost: 212.529 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 132.638 ms
GPU 0, Compute+Comm Time: 132.638 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 142.950 ms
GPU 0, Compute+Comm Time: 142.950 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 289.368 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 6, starting model training...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 21.8497	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 50:	Loss 1.6062	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.5990	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5950	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5920	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5898	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5889	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5893	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5892	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5894	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5846	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5771	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5760	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5664	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5629	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5597	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5523	TrainAcc 0.3456	ValidAcc 0.3498	TestAcc 0.3495	BestValid 0.4239
	Epoch 850:	Loss 1.5486	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 900:	Loss 1.5502	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 950:	Loss 1.5447	TrainAcc 0.3821	ValidAcc 0.3846	TestAcc 0.3865	BestValid 0.4239
	Epoch 1000:	Loss 1.5457	TrainAcc 0.4215	ValidAcc 0.4240	TestAcc 0.4235	BestValid 0.4240
	Epoch 1050:	Loss 1.5459	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 1100:	Loss 1.5438	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 1150:	Loss 1.5418	TrainAcc 0.2901	ValidAcc 0.2903	TestAcc 0.2853	BestValid 0.4240
	Epoch 1200:	Loss 1.5445	TrainAcc 0.3070	ValidAcc 0.3075	TestAcc 0.3013	BestValid 0.4240
	Epoch 1250:	Loss 1.5402	TrainAcc 0.3898	ValidAcc 0.3934	TestAcc 0.3941	BestValid 0.4240
	Epoch 1300:	Loss 1.5418	TrainAcc 0.2584	ValidAcc 0.2580	TestAcc 0.2548	BestValid 0.4240
	Epoch 1350:	Loss 1.5408	TrainAcc 0.3230	ValidAcc 0.3211	TestAcc 0.3225	BestValid 0.4240
	Epoch 1400:	Loss 1.5424	TrainAcc 0.3790	ValidAcc 0.3820	TestAcc 0.3829	BestValid 0.4240
	Epoch 1450:	Loss 1.5396	TrainAcc 0.3707	ValidAcc 0.3723	TestAcc 0.3731	BestValid 0.4240
	Epoch 1500:	Loss 1.5385	TrainAcc 0.3954	ValidAcc 0.3951	TestAcc 0.3969	BestValid 0.4240
	Epoch 1550:	Loss 1.5402	TrainAcc 0.4142	ValidAcc 0.4158	TestAcc 0.4156	BestValid 0.4240
	Epoch 1600:	Loss 1.5394	TrainAcc 0.3910	ValidAcc 0.3927	TestAcc 0.3916	BestValid 0.4240
	Epoch 1650:	Loss 1.5371	TrainAcc 0.3998	ValidAcc 0.4018	TestAcc 0.4009	BestValid 0.4240
	Epoch 1700:	Loss 1.5395	TrainAcc 0.3815	ValidAcc 0.3818	TestAcc 0.3814	BestValid 0.4240
	Epoch 1750:	Loss 1.5368	TrainAcc 0.4089	ValidAcc 0.4103	TestAcc 0.4092	BestValid 0.4240
	Epoch 1800:	Loss 1.5419	TrainAcc 0.3922	ValidAcc 0.3949	TestAcc 0.3933	BestValid 0.4240
	Epoch 1850:	Loss 1.5374	TrainAcc 0.3872	ValidAcc 0.3903	TestAcc 0.3876	BestValid 0.4240
	Epoch 1900:	Loss 1.5376	TrainAcc 0.4020	ValidAcc 0.4047	TestAcc 0.4039	BestValid 0.4240
	Epoch 1950:	Loss 1.5341	TrainAcc 0.3858	ValidAcc 0.3901	TestAcc 0.3870	BestValid 0.4240
	Epoch 2000:	Loss 1.5346	TrainAcc 0.3920	ValidAcc 0.3939	TestAcc 0.3950	BestValid 0.4240
	Epoch 2050:	Loss 1.5357	TrainAcc 0.3981	ValidAcc 0.4004	TestAcc 0.4005	BestValid 0.4240
	Epoch 2100:	Loss 1.5332	TrainAcc 0.4009	ValidAcc 0.4048	TestAcc 0.4058	BestValid 0.4240
	Epoch 2150:	Loss 1.5330	TrainAcc 0.4019	ValidAcc 0.4036	TestAcc 0.4069	BestValid 0.4240
	Epoch 2200:	Loss 1.5333	TrainAcc 0.4078	ValidAcc 0.4088	TestAcc 0.4124	BestValid 0.4240
	Epoch 2250:	Loss 1.5381	TrainAcc 0.3996	ValidAcc 0.3963	TestAcc 0.3991	BestValid 0.4240
	Epoch 2300:	Loss 1.5303	TrainAcc 0.4118	ValidAcc 0.4114	TestAcc 0.4101	BestValid 0.4240
	Epoch 2350:	Loss 1.5317	TrainAcc 0.3859	ValidAcc 0.3869	TestAcc 0.3852	BestValid 0.4240
	Epoch 2400:	Loss 1.5323	TrainAcc 0.4090	ValidAcc 0.4124	TestAcc 0.4105	BestValid 0.4240
	Epoch 2450:	Loss 1.5309	TrainAcc 0.4086	ValidAcc 0.4104	TestAcc 0.4090	BestValid 0.4240
	Epoch 2500:	Loss 1.5291	TrainAcc 0.3890	ValidAcc 0.3906	TestAcc 0.3896	BestValid 0.4240
	Epoch 2550:	Loss 1.5285	TrainAcc 0.4161	ValidAcc 0.4167	TestAcc 0.4167	BestValid 0.4240
	Epoch 2600:	Loss 1.5265	TrainAcc 0.4171	ValidAcc 0.4196	TestAcc 0.4176	BestValid 0.4240
	Epoch 2650:	Loss 1.5307	TrainAcc 0.4203	ValidAcc 0.4222	TestAcc 0.4223	BestValid 0.4240
	Epoch 2700:	Loss 1.5270	TrainAcc 0.4141	ValidAcc 0.4174	TestAcc 0.4157	BestValid 0.4240
	Epoch 2750:	Loss 1.5277	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4235	BestValid 0.4240
	Epoch 2800:	Loss 1.5261	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 2850:	Loss 1.5228	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 2900:	Loss 1.5256	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 2950:	Loss 1.5234	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3000:	Loss 1.5225	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3050:	Loss 1.5220	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3100:	Loss 1.5212	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3150:	Loss 1.5190	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3200:	Loss 1.5173	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3250:	Loss 1.5198	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3300:	Loss 1.5178	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3350:	Loss 1.5179	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3400:	Loss 1.5113	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3450:	Loss 1.5266	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3500:	Loss 1.5210	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3550:	Loss 1.5176	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3600:	Loss 1.5130	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3650:	Loss 1.5086	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3700:	Loss 1.5129	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3750:	Loss 1.5130	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3800:	Loss 1.5153	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3850:	Loss 1.5104	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3900:	Loss 1.5100	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 3950:	Loss 1.5100	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4000:	Loss 1.5080	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4050:	Loss 1.5061	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4100:	Loss 1.5148	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4150:	Loss 1.5110	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4200:	Loss 1.5033	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4250:	Loss 1.5032	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4300:	Loss 1.5041	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4350:	Loss 1.4994	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4400:	Loss 1.5014	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4450:	Loss 1.4972	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4500:	Loss 1.5001	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4550:	Loss 1.5051	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4600:	Loss 1.5108	TrainAcc 0.4208	ValidAcc 0.4232	TestAcc 0.4228	BestValid 0.4240
	Epoch 4650:	Loss 1.5056	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4240
	Epoch 4700:	Loss 1.4955	TrainAcc 0.3715	ValidAcc 0.3707	TestAcc 0.3714	BestValid 0.4240
	Epoch 4750:	Loss 1.4913	TrainAcc 0.3746	ValidAcc 0.3744	TestAcc 0.3711	BestValid 0.4240
	Epoch 4800:	Loss 1.4866	TrainAcc 0.3656	ValidAcc 0.3644	TestAcc 0.3620	BestValid 0.4240
	Epoch 4850:	Loss 1.4847	TrainAcc 0.3673	ValidAcc 0.3653	TestAcc 0.3641	BestValid 0.4240
	Epoch 4900:	Loss 1.4907	TrainAcc 0.3566	ValidAcc 0.3560	TestAcc 0.3535	BestValid 0.4240
	Epoch 4950:	Loss 1.4924	TrainAcc 0.3616	ValidAcc 0.3601	TestAcc 0.3587	BestValid 0.4240
	Epoch 5000:	Loss 1.4847	TrainAcc 0.3469	ValidAcc 0.3442	TestAcc 0.3424	BestValid 0.4240
****** Epoch Time (Excluding Evaluation Cost): 0.172 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.262 ms (Max: 0.336, Min: 0.041, Sum: 2.094)
Cluster-Wide Average, Compute: 37.217 ms (Max: 38.075, Min: 35.242, Sum: 297.735)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.061)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.017, Min: 0.015, Sum: 0.125)
Cluster-Wide Average, Communication-Graph: 123.247 ms (Max: 125.223, Min: 122.339, Sum: 985.980)
Cluster-Wide Average, Optimization: 5.790 ms (Max: 5.856, Min: 5.727, Sum: 46.317)
Cluster-Wide Average, Others: 5.855 ms (Max: 5.991, Min: 5.760, Sum: 46.843)
****** Breakdown Sum: 172.394 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.880 GB (Max: 7.311, Min: 6.805, Sum: 55.042)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.870 Gbps (Max: 51.903, Min: 16.785, Sum: 342.963)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.037 GB
Total communication (cluster-wide, per-epoch): 4.620 GB
****** Accuracy Results ******
Highest valid_acc: 0.4240
Target test_acc: 0.4235
Epoch to reach the target acc: 999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
