Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.031 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.016 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.031 seconds.
        It takes 0.028 seconds.
        It takes 0.020 seconds.
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.982 Gbps (per GPU), 479.855 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.691 Gbps (per GPU), 477.531 Gbps (aggregated)
The layer-level communication performance: 59.683 Gbps (per GPU), 477.461 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.431 Gbps (per GPU), 475.446 Gbps (aggregated)
The layer-level communication performance: 59.398 Gbps (per GPU), 475.183 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.178 Gbps (per GPU), 473.421 Gbps (aggregated)
The layer-level communication performance: 59.128 Gbps (per GPU), 473.027 Gbps (aggregated)
The layer-level communication performance: 59.095 Gbps (per GPU), 472.760 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.215 Gbps (per GPU), 1249.723 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.192 Gbps (per GPU), 1249.537 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.146 Gbps (per GPU), 1249.164 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.204 Gbps (per GPU), 1249.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.230 Gbps (per GPU), 1249.839 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.198 Gbps (per GPU), 1249.583 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.146 Gbps (per GPU), 1249.164 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.189 Gbps (per GPU), 1249.513 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.979 Gbps (per GPU), 815.834 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.979 Gbps (per GPU), 815.834 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.983 Gbps (per GPU), 815.861 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.977 Gbps (per GPU), 815.814 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.959 Gbps (per GPU), 815.675 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.976 Gbps (per GPU), 815.808 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.972 Gbps (per GPU), 815.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.981 Gbps (per GPU), 815.848 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.670 Gbps (per GPU), 293.357 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.673 Gbps (per GPU), 293.384 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.669 Gbps (per GPU), 293.350 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.670 Gbps (per GPU), 293.361 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.667 Gbps (per GPU), 293.337 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.672 Gbps (per GPU), 293.375 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.667 Gbps (per GPU), 293.335 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.660 Gbps (per GPU), 293.282 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.71ms  0.84ms  0.49ms  3.45 11.16K  0.12M
 chk_1  1.71ms  0.84ms  0.50ms  3.41 11.16K  0.11M
 chk_2  1.65ms  0.81ms  0.47ms  3.52 11.16K  0.11M
 chk_3  1.61ms  0.77ms  0.43ms  3.79 11.16K  0.12M
 chk_4  1.68ms  0.84ms  0.50ms  3.37 11.16K  0.11M
 chk_5  1.67ms  0.83ms  0.49ms  3.43 11.16K  0.10M
 chk_6  1.67ms  0.83ms  0.49ms  3.41 11.16K  0.12M
 chk_7  1.68ms  0.84ms  0.50ms  3.36 11.16K  0.11M
   Avg  1.67  0.82  0.48
   Max  1.71  0.84  0.50
   Min  1.61  0.77  0.43
 Ratio  1.06  1.09  1.17
   Var  0.00  0.00  0.00
Profiling takes 0.352 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 30.242 ms
Partition 0 [0, 3) has cost: 26.553 ms
Partition 1 [3, 7) has cost: 26.381 ms
Partition 2 [7, 11) has cost: 26.381 ms
Partition 3 [11, 15) has cost: 26.381 ms
Partition 4 [15, 19) has cost: 26.381 ms
Partition 5 [19, 23) has cost: 26.381 ms
Partition 6 [23, 27) has cost: 26.381 ms
Partition 7 [27, 32) has cost: 30.242 ms
The optimal partitioning:
[0, 3)
[3, 7)
[7, 11)
[11, 15)
[15, 19)
[19, 23)
[23, 27)
[27, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.691 ms
GPU 0, Compute+Comm Time: 12.881 ms, Bubble Time: 12.405 ms, Imbalance Overhead: 0.406 ms
GPU 1, Compute+Comm Time: 12.980 ms, Bubble Time: 12.220 ms, Imbalance Overhead: 0.491 ms
GPU 2, Compute+Comm Time: 12.980 ms, Bubble Time: 12.110 ms, Imbalance Overhead: 0.601 ms
GPU 3, Compute+Comm Time: 12.980 ms, Bubble Time: 12.069 ms, Imbalance Overhead: 0.642 ms
GPU 4, Compute+Comm Time: 12.980 ms, Bubble Time: 11.915 ms, Imbalance Overhead: 0.796 ms
GPU 5, Compute+Comm Time: 12.980 ms, Bubble Time: 11.782 ms, Imbalance Overhead: 0.929 ms
GPU 6, Compute+Comm Time: 12.980 ms, Bubble Time: 11.644 ms, Imbalance Overhead: 1.067 ms
GPU 7, Compute+Comm Time: 14.202 ms, Bubble Time: 11.489 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 46.044 ms
GPU 0, Compute+Comm Time: 25.563 ms, Bubble Time: 20.481 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 22.924 ms, Bubble Time: 20.811 ms, Imbalance Overhead: 2.310 ms
GPU 2, Compute+Comm Time: 22.924 ms, Bubble Time: 21.116 ms, Imbalance Overhead: 2.005 ms
GPU 3, Compute+Comm Time: 22.924 ms, Bubble Time: 21.404 ms, Imbalance Overhead: 1.717 ms
GPU 4, Compute+Comm Time: 22.924 ms, Bubble Time: 21.731 ms, Imbalance Overhead: 1.390 ms
GPU 5, Compute+Comm Time: 22.924 ms, Bubble Time: 21.829 ms, Imbalance Overhead: 1.291 ms
GPU 6, Compute+Comm Time: 22.924 ms, Bubble Time: 22.020 ms, Imbalance Overhead: 1.101 ms
GPU 7, Compute+Comm Time: 23.195 ms, Bubble Time: 22.322 ms, Imbalance Overhead: 0.527 ms
The estimated cost of the whole pipeline: 75.322 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 56.623 ms
Partition 0 [0, 7) has cost: 52.934 ms
Partition 1 [7, 15) has cost: 52.762 ms
Partition 2 [15, 23) has cost: 52.762 ms
Partition 3 [23, 32) has cost: 56.623 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.336 ms
GPU 0, Compute+Comm Time: 16.253 ms, Bubble Time: 13.392 ms, Imbalance Overhead: 0.692 ms
GPU 1, Compute+Comm Time: 16.777 ms, Bubble Time: 12.981 ms, Imbalance Overhead: 0.578 ms
GPU 2, Compute+Comm Time: 16.777 ms, Bubble Time: 12.738 ms, Imbalance Overhead: 0.821 ms
GPU 3, Compute+Comm Time: 17.863 ms, Bubble Time: 12.473 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.799 ms
GPU 0, Compute+Comm Time: 28.680 ms, Bubble Time: 20.120 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 26.865 ms, Bubble Time: 20.553 ms, Imbalance Overhead: 1.381 ms
GPU 2, Compute+Comm Time: 26.865 ms, Bubble Time: 20.962 ms, Imbalance Overhead: 0.972 ms
GPU 3, Compute+Comm Time: 26.535 ms, Bubble Time: 21.503 ms, Imbalance Overhead: 0.761 ms
    The estimated cost with 2 DP ways is 83.093 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 109.384 ms
Partition 0 [0, 15) has cost: 105.695 ms
Partition 1 [15, 32) has cost: 109.384 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 45.754 ms
GPU 0, Compute+Comm Time: 28.872 ms, Bubble Time: 15.669 ms, Imbalance Overhead: 1.213 ms
GPU 1, Compute+Comm Time: 31.324 ms, Bubble Time: 14.430 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.347 ms
GPU 0, Compute+Comm Time: 41.806 ms, Bubble Time: 19.541 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 39.114 ms, Bubble Time: 20.924 ms, Imbalance Overhead: 1.309 ms
    The estimated cost with 4 DP ways is 112.456 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 215.080 ms
Partition 0 [0, 32) has cost: 215.080 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 117.591 ms
GPU 0, Compute+Comm Time: 117.591 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 127.979 ms
GPU 0, Compute+Comm Time: 127.979 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 257.848 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 257)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 257)
*** Node 4, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 257)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 257)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 257)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 6, starting model training...
Num Stages: 1 / 1
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 257)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 257)...
+++++++++ Node 0 initializing the weights for op[0, 257)...
+++++++++ Node 2 initializing the weights for op[0, 257)...
+++++++++ Node 3 initializing the weights for op[0, 257)...
+++++++++ Node 5 initializing the weights for op[0, 257)...
+++++++++ Node 4 initializing the weights for op[0, 257)...
+++++++++ Node 6 initializing the weights for op[0, 257)...
+++++++++ Node 7 initializing the weights for op[0, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.6056	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.5991	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5951	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5920	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5898	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5890	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5894	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5891	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5894	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5847	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5772	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5762	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5663	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5632	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5589	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5519	TrainAcc 0.2869	ValidAcc 0.2869	TestAcc 0.2862	BestValid 0.4239
	Epoch 850:	Loss 1.5482	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 900:	Loss 1.5497	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 950:	Loss 1.5444	TrainAcc 0.3328	ValidAcc 0.3326	TestAcc 0.3286	BestValid 0.4239
	Epoch 1000:	Loss 1.5456	TrainAcc 0.2930	ValidAcc 0.2916	TestAcc 0.2901	BestValid 0.4239
	Epoch 1050:	Loss 1.5460	TrainAcc 0.3031	ValidAcc 0.3023	TestAcc 0.2980	BestValid 0.4239
	Epoch 1100:	Loss 1.5433	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5416	TrainAcc 0.2935	ValidAcc 0.2930	TestAcc 0.2884	BestValid 0.4239
	Epoch 1200:	Loss 1.5456	TrainAcc 0.2587	ValidAcc 0.2588	TestAcc 0.2550	BestValid 0.4239
	Epoch 1250:	Loss 1.5403	TrainAcc 0.3817	ValidAcc 0.3840	TestAcc 0.3859	BestValid 0.4239
	Epoch 1300:	Loss 1.5418	TrainAcc 0.2585	ValidAcc 0.2586	TestAcc 0.2549	BestValid 0.4239
	Epoch 1350:	Loss 1.5405	TrainAcc 0.3604	ValidAcc 0.3634	TestAcc 0.3653	BestValid 0.4239
	Epoch 1400:	Loss 1.5427	TrainAcc 0.3482	ValidAcc 0.3514	TestAcc 0.3519	BestValid 0.4239
	Epoch 1450:	Loss 1.5396	TrainAcc 0.3618	ValidAcc 0.3626	TestAcc 0.3635	BestValid 0.4239
	Epoch 1500:	Loss 1.5388	TrainAcc 0.4000	ValidAcc 0.4034	TestAcc 0.4024	BestValid 0.4239
	Epoch 1550:	Loss 1.5399	TrainAcc 0.3890	ValidAcc 0.3927	TestAcc 0.3903	BestValid 0.4239
	Epoch 1600:	Loss 1.5395	TrainAcc 0.3511	ValidAcc 0.3555	TestAcc 0.3541	BestValid 0.4239
	Epoch 1650:	Loss 1.5364	TrainAcc 0.4078	ValidAcc 0.4101	TestAcc 0.4096	BestValid 0.4239
	Epoch 1700:	Loss 1.5393	TrainAcc 0.3969	ValidAcc 0.4008	TestAcc 0.4008	BestValid 0.4239
	Epoch 1750:	Loss 1.5372	TrainAcc 0.4106	ValidAcc 0.4118	TestAcc 0.4122	BestValid 0.4239
	Epoch 1800:	Loss 1.5429	TrainAcc 0.4031	ValidAcc 0.4035	TestAcc 0.4041	BestValid 0.4239
	Epoch 1850:	Loss 1.5379	TrainAcc 0.3866	ValidAcc 0.3907	TestAcc 0.3895	BestValid 0.4239
	Epoch 1900:	Loss 1.5389	TrainAcc 0.3966	ValidAcc 0.3992	TestAcc 0.3981	BestValid 0.4239
	Epoch 1950:	Loss 1.5344	TrainAcc 0.4083	ValidAcc 0.4079	TestAcc 0.4094	BestValid 0.4239
	Epoch 2000:	Loss 1.5348	TrainAcc 0.4053	ValidAcc 0.4040	TestAcc 0.4068	BestValid 0.4239
	Epoch 2050:	Loss 1.5361	TrainAcc 0.4055	ValidAcc 0.4043	TestAcc 0.4054	BestValid 0.4239
	Epoch 2100:	Loss 1.5335	TrainAcc 0.3980	ValidAcc 0.3977	TestAcc 0.3987	BestValid 0.4239
	Epoch 2150:	Loss 1.5341	TrainAcc 0.4121	ValidAcc 0.4124	TestAcc 0.4133	BestValid 0.4239
	Epoch 2200:	Loss 1.5321	TrainAcc 0.4107	ValidAcc 0.4112	TestAcc 0.4116	BestValid 0.4239
	Epoch 2250:	Loss 1.5377	TrainAcc 0.4080	ValidAcc 0.4052	TestAcc 0.4077	BestValid 0.4239
	Epoch 2300:	Loss 1.5303	TrainAcc 0.4133	ValidAcc 0.4134	TestAcc 0.4149	BestValid 0.4239
	Epoch 2350:	Loss 1.5322	TrainAcc 0.4124	ValidAcc 0.4139	TestAcc 0.4123	BestValid 0.4239
	Epoch 2400:	Loss 1.5318	TrainAcc 0.3913	ValidAcc 0.3929	TestAcc 0.3920	BestValid 0.4239
	Epoch 2450:	Loss 1.5310	TrainAcc 0.4097	ValidAcc 0.4109	TestAcc 0.4119	BestValid 0.4239
	Epoch 2500:	Loss 1.5293	TrainAcc 0.4140	ValidAcc 0.4152	TestAcc 0.4155	BestValid 0.4239
	Epoch 2550:	Loss 1.5285	TrainAcc 0.3682	ValidAcc 0.3692	TestAcc 0.3700	BestValid 0.4239
	Epoch 2600:	Loss 1.5269	TrainAcc 0.4133	ValidAcc 0.4142	TestAcc 0.4144	BestValid 0.4239
	Epoch 2650:	Loss 1.5306	TrainAcc 0.4195	ValidAcc 0.4214	TestAcc 0.4212	BestValid 0.4239
	Epoch 2700:	Loss 1.5281	TrainAcc 0.4195	ValidAcc 0.4219	TestAcc 0.4211	BestValid 0.4239
	Epoch 2750:	Loss 1.5278	TrainAcc 0.4203	ValidAcc 0.4228	TestAcc 0.4223	BestValid 0.4239
	Epoch 2800:	Loss 1.5263	TrainAcc 0.4215	ValidAcc 0.4238	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5228	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5261	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5260	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5240	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5226	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5242	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5203	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5201	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5203	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5193	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5228	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5133	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5160	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5209	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5181	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5134	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5163	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5139	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5110	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5122	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5081	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5102	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5119	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5125	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5136	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5149	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5068	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5053	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5014	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.4993	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.4992	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5035	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.4968	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5073	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.4960	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5043	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.4998	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.4887	TrainAcc 0.4167	ValidAcc 0.4193	TestAcc 0.4175	BestValid 0.4239
	Epoch 4750:	Loss 1.4908	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.4837	TrainAcc 0.3646	ValidAcc 0.3637	TestAcc 0.3608	BestValid 0.4239
	Epoch 4850:	Loss 1.4855	TrainAcc 0.3583	ValidAcc 0.3580	TestAcc 0.3546	BestValid 0.4239
	Epoch 4900:	Loss 1.4843	TrainAcc 0.3739	ValidAcc 0.3712	TestAcc 0.3702	BestValid 0.4239
	Epoch 4950:	Loss 1.4872	TrainAcc 0.3583	ValidAcc 0.3569	TestAcc 0.3541	BestValid 0.4239
	Epoch 5000:	Loss 1.4841	TrainAcc 0.3458	ValidAcc 0.3434	TestAcc 0.3415	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.174 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.144 ms (Max: 0.204, Min: 0.045, Sum: 1.153)
Cluster-Wide Average, Compute: 37.443 ms (Max: 38.071, Min: 35.801, Sum: 299.545)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.017, Min: 0.016, Sum: 0.129)
Cluster-Wide Average, Communication-Graph: 124.275 ms (Max: 125.898, Min: 123.618, Sum: 994.197)
Cluster-Wide Average, Optimization: 5.811 ms (Max: 5.914, Min: 5.702, Sum: 46.489)
Cluster-Wide Average, Others: 5.887 ms (Max: 5.994, Min: 5.802, Sum: 47.099)
****** Breakdown Sum: 173.584 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.880 GB (Max: 7.311, Min: 6.805, Sum: 55.042)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.500 Gbps (Max: 51.276, Min: 16.711, Sum: 340.002)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.037 GB
Total communication (cluster-wide, per-epoch): 4.620 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
