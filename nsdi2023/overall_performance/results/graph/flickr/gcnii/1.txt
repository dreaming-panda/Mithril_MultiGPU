Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
        It takes 0.018 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.022 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.113 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.106 seconds.
        It takes 0.008 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.419 Gbps (per GPU), 483.350 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.116 Gbps (per GPU), 480.929 Gbps (aggregated)
The layer-level communication performance: 60.118 Gbps (per GPU), 480.941 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.865 Gbps (per GPU), 478.923 Gbps (aggregated)
The layer-level communication performance: 59.833 Gbps (per GPU), 478.666 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.618 Gbps (per GPU), 476.946 Gbps (aggregated)
The layer-level communication performance: 59.567 Gbps (per GPU), 476.538 Gbps (aggregated)
The layer-level communication performance: 59.535 Gbps (per GPU), 476.282 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.093 Gbps (per GPU), 1256.744 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.078 Gbps (per GPU), 1256.626 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.028 Gbps (per GPU), 1256.226 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.067 Gbps (per GPU), 1256.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.055 Gbps (per GPU), 1256.438 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.081 Gbps (per GPU), 1256.650 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.049 Gbps (per GPU), 1256.393 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.069 Gbps (per GPU), 1256.556 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.315 Gbps (per GPU), 794.519 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.309 Gbps (per GPU), 794.470 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.309 Gbps (per GPU), 794.475 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.312 Gbps (per GPU), 794.495 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.310 Gbps (per GPU), 794.476 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.308 Gbps (per GPU), 794.464 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.281 Gbps (per GPU), 794.251 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.313 Gbps (per GPU), 794.501 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.627 Gbps (per GPU), 269.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.626 Gbps (per GPU), 269.004 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.627 Gbps (per GPU), 269.019 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.626 Gbps (per GPU), 269.010 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.623 Gbps (per GPU), 268.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.623 Gbps (per GPU), 268.981 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.621 Gbps (per GPU), 268.969 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.619 Gbps (per GPU), 268.955 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.99ms  0.80ms  0.96ms  1.23 11.16K  0.12M
 chk_1  0.99ms  0.81ms  0.96ms  1.22 11.16K  0.11M
 chk_2  0.99ms  0.77ms  0.93ms  1.28 11.16K  0.11M
 chk_3  0.96ms  0.74ms  0.89ms  1.31 11.16K  0.12M
 chk_4  0.96ms  0.80ms  0.96ms  1.19 11.16K  0.11M
 chk_5  0.96ms  0.79ms  0.95ms  1.21 11.16K  0.10M
 chk_6  0.96ms  0.80ms  0.95ms  1.21 11.16K  0.12M
 chk_7  0.96ms  0.81ms  0.96ms  1.19 11.16K  0.11M
   Avg  0.97  0.79  0.95
   Max  0.99  0.81  0.96
   Min  0.96  0.74  0.89
 Ratio  1.03  1.10  1.08
   Var  0.00  0.00  0.00
Profiling takes 0.330 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 31.601 ms
Partition 0 [0, 4) has cost: 26.732 ms
Partition 1 [4, 8) has cost: 25.281 ms
Partition 2 [8, 12) has cost: 25.281 ms
Partition 3 [12, 16) has cost: 25.281 ms
Partition 4 [16, 20) has cost: 25.281 ms
Partition 5 [20, 24) has cost: 25.281 ms
Partition 6 [24, 29) has cost: 31.601 ms
Partition 7 [29, 33) has cost: 26.521 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 26.137 ms
GPU 0, Compute+Comm Time: 12.853 ms, Bubble Time: 12.670 ms, Imbalance Overhead: 0.614 ms
GPU 1, Compute+Comm Time: 12.761 ms, Bubble Time: 12.466 ms, Imbalance Overhead: 0.910 ms
GPU 2, Compute+Comm Time: 12.761 ms, Bubble Time: 12.335 ms, Imbalance Overhead: 1.041 ms
GPU 3, Compute+Comm Time: 12.761 ms, Bubble Time: 12.083 ms, Imbalance Overhead: 1.293 ms
GPU 4, Compute+Comm Time: 12.761 ms, Bubble Time: 11.852 ms, Imbalance Overhead: 1.525 ms
GPU 5, Compute+Comm Time: 12.761 ms, Bubble Time: 11.617 ms, Imbalance Overhead: 1.759 ms
GPU 6, Compute+Comm Time: 14.769 ms, Bubble Time: 11.368 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 13.020 ms, Bubble Time: 11.591 ms, Imbalance Overhead: 1.526 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 46.677 ms
GPU 0, Compute+Comm Time: 22.955 ms, Bubble Time: 20.812 ms, Imbalance Overhead: 2.911 ms
GPU 1, Compute+Comm Time: 26.286 ms, Bubble Time: 20.391 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 21.974 ms, Bubble Time: 20.820 ms, Imbalance Overhead: 3.883 ms
GPU 3, Compute+Comm Time: 21.974 ms, Bubble Time: 21.191 ms, Imbalance Overhead: 3.513 ms
GPU 4, Compute+Comm Time: 21.974 ms, Bubble Time: 21.684 ms, Imbalance Overhead: 3.019 ms
GPU 5, Compute+Comm Time: 21.974 ms, Bubble Time: 22.115 ms, Imbalance Overhead: 2.588 ms
GPU 6, Compute+Comm Time: 21.974 ms, Bubble Time: 22.247 ms, Imbalance Overhead: 2.456 ms
GPU 7, Compute+Comm Time: 23.334 ms, Bubble Time: 22.515 ms, Imbalance Overhead: 0.828 ms
The estimated cost of the whole pipeline: 76.455 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 56.882 ms
Partition 0 [0, 8) has cost: 52.013 ms
Partition 1 [8, 16) has cost: 50.562 ms
Partition 2 [16, 25) has cost: 56.882 ms
Partition 3 [25, 33) has cost: 51.802 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.352 ms
GPU 0, Compute+Comm Time: 16.113 ms, Bubble Time: 13.153 ms, Imbalance Overhead: 1.087 ms
GPU 1, Compute+Comm Time: 16.537 ms, Bubble Time: 12.704 ms, Imbalance Overhead: 1.112 ms
GPU 2, Compute+Comm Time: 18.013 ms, Bubble Time: 12.340 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 16.667 ms, Bubble Time: 12.682 ms, Imbalance Overhead: 1.003 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.270 ms
GPU 0, Compute+Comm Time: 26.394 ms, Bubble Time: 20.250 ms, Imbalance Overhead: 1.625 ms
GPU 1, Compute+Comm Time: 28.551 ms, Bubble Time: 19.719 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 25.904 ms, Bubble Time: 20.304 ms, Imbalance Overhead: 2.062 ms
GPU 3, Compute+Comm Time: 26.140 ms, Bubble Time: 20.848 ms, Imbalance Overhead: 1.283 ms
    The estimated cost with 2 DP ways is 82.554 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 108.685 ms
Partition 0 [0, 16) has cost: 102.575 ms
Partition 1 [16, 33) has cost: 108.685 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.405 ms
GPU 0, Compute+Comm Time: 29.127 ms, Bubble Time: 15.932 ms, Imbalance Overhead: 1.346 ms
GPU 1, Compute+Comm Time: 31.849 ms, Bubble Time: 14.556 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.419 ms
GPU 0, Compute+Comm Time: 42.012 ms, Bubble Time: 19.406 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.862 ms, Bubble Time: 21.036 ms, Imbalance Overhead: 1.521 ms
    The estimated cost with 4 DP ways is 113.215 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 211.260 ms
Partition 0 [0, 33) has cost: 211.260 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 127.447 ms
GPU 0, Compute+Comm Time: 127.447 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 137.439 ms
GPU 0, Compute+Comm Time: 137.439 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 278.130 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.4448	TrainAcc 0.4214	ValidAcc 0.4234	TestAcc 0.4232	BestValid 0.4234
	Epoch 50:	Loss 1.6287	TrainAcc 0.4462	ValidAcc 0.4464	TestAcc 0.4479	BestValid 0.4464
	Epoch 100:	Loss 1.6083	TrainAcc 0.4717	ValidAcc 0.4754	TestAcc 0.4721	BestValid 0.4754
	Epoch 150:	Loss 1.5903	TrainAcc 0.4813	ValidAcc 0.4845	TestAcc 0.4795	BestValid 0.4845
	Epoch 200:	Loss 1.5660	TrainAcc 0.4845	ValidAcc 0.4842	TestAcc 0.4822	BestValid 0.4845
	Epoch 250:	Loss 1.5404	TrainAcc 0.4872	ValidAcc 0.4832	TestAcc 0.4843	BestValid 0.4845
	Epoch 300:	Loss 1.5248	TrainAcc 0.4906	ValidAcc 0.4866	TestAcc 0.4872	BestValid 0.4866
	Epoch 350:	Loss 1.5124	TrainAcc 0.4918	ValidAcc 0.4880	TestAcc 0.4881	BestValid 0.4880
	Epoch 400:	Loss 1.5047	TrainAcc 0.4939	ValidAcc 0.4889	TestAcc 0.4892	BestValid 0.4889
	Epoch 450:	Loss 1.4985	TrainAcc 0.4919	ValidAcc 0.4866	TestAcc 0.4863	BestValid 0.4889
	Epoch 500:	Loss 1.4934	TrainAcc 0.4984	ValidAcc 0.4928	TestAcc 0.4931	BestValid 0.4928
	Epoch 550:	Loss 1.4865	TrainAcc 0.4983	ValidAcc 0.4922	TestAcc 0.4927	BestValid 0.4928
	Epoch 600:	Loss 1.4868	TrainAcc 0.5015	ValidAcc 0.4941	TestAcc 0.4957	BestValid 0.4941
	Epoch 650:	Loss 1.4766	TrainAcc 0.5027	ValidAcc 0.4947	TestAcc 0.4966	BestValid 0.4947
	Epoch 700:	Loss 1.4755	TrainAcc 0.5010	ValidAcc 0.4915	TestAcc 0.4936	BestValid 0.4947
	Epoch 750:	Loss 1.4790	TrainAcc 0.4889	ValidAcc 0.4811	TestAcc 0.4814	BestValid 0.4947
	Epoch 800:	Loss 1.4702	TrainAcc 0.5041	ValidAcc 0.4953	TestAcc 0.4972	BestValid 0.4953
	Epoch 850:	Loss 1.4650	TrainAcc 0.5035	ValidAcc 0.4939	TestAcc 0.4961	BestValid 0.4953
	Epoch 900:	Loss 1.4667	TrainAcc 0.5101	ValidAcc 0.5022	TestAcc 0.5031	BestValid 0.5022
	Epoch 950:	Loss 1.4635	TrainAcc 0.5062	ValidAcc 0.4974	TestAcc 0.4993	BestValid 0.5022
	Epoch 1000:	Loss 1.4628	TrainAcc 0.5021	ValidAcc 0.4930	TestAcc 0.4942	BestValid 0.5022
	Epoch 1050:	Loss 1.4649	TrainAcc 0.5009	ValidAcc 0.4917	TestAcc 0.4940	BestValid 0.5022
	Epoch 1100:	Loss 1.4590	TrainAcc 0.5063	ValidAcc 0.4961	TestAcc 0.4979	BestValid 0.5022
	Epoch 1150:	Loss 1.4588	TrainAcc 0.5069	ValidAcc 0.4963	TestAcc 0.4976	BestValid 0.5022
	Epoch 1200:	Loss 1.4560	TrainAcc 0.5033	ValidAcc 0.4944	TestAcc 0.4949	BestValid 0.5022
	Epoch 1250:	Loss 1.4526	TrainAcc 0.5111	ValidAcc 0.5025	TestAcc 0.5032	BestValid 0.5025
	Epoch 1300:	Loss 1.4528	TrainAcc 0.5083	ValidAcc 0.4975	TestAcc 0.5005	BestValid 0.5025
	Epoch 1350:	Loss 1.4492	TrainAcc 0.5089	ValidAcc 0.4984	TestAcc 0.5003	BestValid 0.5025
	Epoch 1400:	Loss 1.4494	TrainAcc 0.5066	ValidAcc 0.4957	TestAcc 0.4980	BestValid 0.5025
	Epoch 1450:	Loss 1.4481	TrainAcc 0.5018	ValidAcc 0.4914	TestAcc 0.4929	BestValid 0.5025
	Epoch 1500:	Loss 1.4419	TrainAcc 0.5114	ValidAcc 0.5017	TestAcc 0.5024	BestValid 0.5025
	Epoch 1550:	Loss 1.4426	TrainAcc 0.5135	ValidAcc 0.5050	TestAcc 0.5051	BestValid 0.5050
	Epoch 1600:	Loss 1.4459	TrainAcc 0.5084	ValidAcc 0.4976	TestAcc 0.4998	BestValid 0.5050
	Epoch 1650:	Loss 1.4412	TrainAcc 0.5125	ValidAcc 0.5024	TestAcc 0.5027	BestValid 0.5050
	Epoch 1700:	Loss 1.4406	TrainAcc 0.5127	ValidAcc 0.5035	TestAcc 0.5039	BestValid 0.5050
	Epoch 1750:	Loss 1.4379	TrainAcc 0.5113	ValidAcc 0.5006	TestAcc 0.5025	BestValid 0.5050
	Epoch 1800:	Loss 1.4387	TrainAcc 0.5140	ValidAcc 0.5046	TestAcc 0.5041	BestValid 0.5050
	Epoch 1850:	Loss 1.4382	TrainAcc 0.5115	ValidAcc 0.4997	TestAcc 0.5017	BestValid 0.5050
	Epoch 1900:	Loss 1.4351	TrainAcc 0.5123	ValidAcc 0.5022	TestAcc 0.5032	BestValid 0.5050
	Epoch 1950:	Loss 1.4314	TrainAcc 0.5136	ValidAcc 0.5047	TestAcc 0.5045	BestValid 0.5050
	Epoch 2000:	Loss 1.4368	TrainAcc 0.5172	ValidAcc 0.5073	TestAcc 0.5066	BestValid 0.5073
	Epoch 2050:	Loss 1.4313	TrainAcc 0.5079	ValidAcc 0.4944	TestAcc 0.4960	BestValid 0.5073
	Epoch 2100:	Loss 1.4331	TrainAcc 0.5144	ValidAcc 0.5041	TestAcc 0.5047	BestValid 0.5073
	Epoch 2150:	Loss 1.4287	TrainAcc 0.5065	ValidAcc 0.4935	TestAcc 0.4959	BestValid 0.5073
	Epoch 2200:	Loss 1.4262	TrainAcc 0.5146	ValidAcc 0.5021	TestAcc 0.5036	BestValid 0.5073
	Epoch 2250:	Loss 1.4248	TrainAcc 0.5170	ValidAcc 0.5054	TestAcc 0.5066	BestValid 0.5073
	Epoch 2300:	Loss 1.4289	TrainAcc 0.5192	ValidAcc 0.5082	TestAcc 0.5074	BestValid 0.5082
	Epoch 2350:	Loss 1.4280	TrainAcc 0.5132	ValidAcc 0.5004	TestAcc 0.5023	BestValid 0.5082
	Epoch 2400:	Loss 1.4258	TrainAcc 0.5125	ValidAcc 0.5002	TestAcc 0.5020	BestValid 0.5082
	Epoch 2450:	Loss 1.4275	TrainAcc 0.5219	ValidAcc 0.5109	TestAcc 0.5114	BestValid 0.5109
	Epoch 2500:	Loss 1.4242	TrainAcc 0.5217	ValidAcc 0.5100	TestAcc 0.5106	BestValid 0.5109
	Epoch 2550:	Loss 1.4276	TrainAcc 0.5178	ValidAcc 0.5075	TestAcc 0.5070	BestValid 0.5109
	Epoch 2600:	Loss 1.4289	TrainAcc 0.5150	ValidAcc 0.5014	TestAcc 0.5037	BestValid 0.5109
	Epoch 2650:	Loss 1.4215	TrainAcc 0.5181	ValidAcc 0.5070	TestAcc 0.5073	BestValid 0.5109
	Epoch 2700:	Loss 1.4201	TrainAcc 0.5225	ValidAcc 0.5092	TestAcc 0.5105	BestValid 0.5109
	Epoch 2750:	Loss 1.4251	TrainAcc 0.5222	ValidAcc 0.5119	TestAcc 0.5125	BestValid 0.5119
	Epoch 2800:	Loss 1.4236	TrainAcc 0.5239	ValidAcc 0.5111	TestAcc 0.5117	BestValid 0.5119
	Epoch 2850:	Loss 1.4233	TrainAcc 0.5234	ValidAcc 0.5105	TestAcc 0.5114	BestValid 0.5119
	Epoch 2900:	Loss 1.4230	TrainAcc 0.5115	ValidAcc 0.4962	TestAcc 0.4977	BestValid 0.5119
	Epoch 2950:	Loss 1.4164	TrainAcc 0.5205	ValidAcc 0.5087	TestAcc 0.5082	BestValid 0.5119
	Epoch 3000:	Loss 1.4206	TrainAcc 0.5237	ValidAcc 0.5104	TestAcc 0.5110	BestValid 0.5119
	Epoch 3050:	Loss 1.4143	TrainAcc 0.5237	ValidAcc 0.5088	TestAcc 0.5096	BestValid 0.5119
	Epoch 3100:	Loss 1.4146	TrainAcc 0.5190	ValidAcc 0.5056	TestAcc 0.5073	BestValid 0.5119
	Epoch 3150:	Loss 1.4151	TrainAcc 0.5202	ValidAcc 0.5065	TestAcc 0.5076	BestValid 0.5119
	Epoch 3200:	Loss 1.4146	TrainAcc 0.5204	ValidAcc 0.5071	TestAcc 0.5078	BestValid 0.5119
	Epoch 3250:	Loss 1.4132	TrainAcc 0.5265	ValidAcc 0.5134	TestAcc 0.5127	BestValid 0.5134
	Epoch 3300:	Loss 1.4124	TrainAcc 0.5202	ValidAcc 0.5066	TestAcc 0.5081	BestValid 0.5134
	Epoch 3350:	Loss 1.4132	TrainAcc 0.5250	ValidAcc 0.5091	TestAcc 0.5098	BestValid 0.5134
	Epoch 3400:	Loss 1.4090	TrainAcc 0.5254	ValidAcc 0.5132	TestAcc 0.5149	BestValid 0.5134
	Epoch 3450:	Loss 1.4141	TrainAcc 0.5258	ValidAcc 0.5093	TestAcc 0.5097	BestValid 0.5134
	Epoch 3500:	Loss 1.4121	TrainAcc 0.5268	ValidAcc 0.5131	TestAcc 0.5123	BestValid 0.5134
	Epoch 3550:	Loss 1.4139	TrainAcc 0.5279	ValidAcc 0.5127	TestAcc 0.5125	BestValid 0.5134
	Epoch 3600:	Loss 1.4066	TrainAcc 0.5281	ValidAcc 0.5126	TestAcc 0.5149	BestValid 0.5134
	Epoch 3650:	Loss 1.4026	TrainAcc 0.5283	ValidAcc 0.5123	TestAcc 0.5131	BestValid 0.5134
	Epoch 3700:	Loss 1.4058	TrainAcc 0.5287	ValidAcc 0.5131	TestAcc 0.5128	BestValid 0.5134
	Epoch 3750:	Loss 1.4081	TrainAcc 0.5237	ValidAcc 0.5081	TestAcc 0.5093	BestValid 0.5134
	Epoch 3800:	Loss 1.4079	TrainAcc 0.5267	ValidAcc 0.5099	TestAcc 0.5103	BestValid 0.5134
	Epoch 3850:	Loss 1.4044	TrainAcc 0.5281	ValidAcc 0.5112	TestAcc 0.5109	BestValid 0.5134
	Epoch 3900:	Loss 1.4037	TrainAcc 0.5271	ValidAcc 0.5110	TestAcc 0.5108	BestValid 0.5134
	Epoch 3950:	Loss 1.4012	TrainAcc 0.5301	ValidAcc 0.5122	TestAcc 0.5126	BestValid 0.5134
	Epoch 4000:	Loss 1.4029	TrainAcc 0.5317	ValidAcc 0.5151	TestAcc 0.5149	BestValid 0.5151
	Epoch 4050:	Loss 1.4036	TrainAcc 0.5293	ValidAcc 0.5137	TestAcc 0.5145	BestValid 0.5151
	Epoch 4100:	Loss 1.4010	TrainAcc 0.5309	ValidAcc 0.5139	TestAcc 0.5137	BestValid 0.5151
	Epoch 4150:	Loss 1.4023	TrainAcc 0.5320	ValidAcc 0.5137	TestAcc 0.5141	BestValid 0.5151
	Epoch 4200:	Loss 1.4058	TrainAcc 0.5284	ValidAcc 0.5119	TestAcc 0.5115	BestValid 0.5151
	Epoch 4250:	Loss 1.4013	TrainAcc 0.5326	ValidAcc 0.5156	TestAcc 0.5151	BestValid 0.5156
	Epoch 4300:	Loss 1.4004	TrainAcc 0.5327	ValidAcc 0.5160	TestAcc 0.5161	BestValid 0.5160
	Epoch 4350:	Loss 1.3968	TrainAcc 0.5317	ValidAcc 0.5141	TestAcc 0.5156	BestValid 0.5160
	Epoch 4400:	Loss 1.4031	TrainAcc 0.5323	ValidAcc 0.5145	TestAcc 0.5134	BestValid 0.5160
	Epoch 4450:	Loss 1.3977	TrainAcc 0.5331	ValidAcc 0.5156	TestAcc 0.5164	BestValid 0.5160
	Epoch 4500:	Loss 1.3937	TrainAcc 0.5335	ValidAcc 0.5151	TestAcc 0.5139	BestValid 0.5160
	Epoch 4550:	Loss 1.3964	TrainAcc 0.5325	ValidAcc 0.5139	TestAcc 0.5151	BestValid 0.5160
	Epoch 4600:	Loss 1.3941	TrainAcc 0.5342	ValidAcc 0.5158	TestAcc 0.5144	BestValid 0.5160
	Epoch 4650:	Loss 1.3946	TrainAcc 0.5348	ValidAcc 0.5155	TestAcc 0.5171	BestValid 0.5160
	Epoch 4700:	Loss 1.3924	TrainAcc 0.5354	ValidAcc 0.5156	TestAcc 0.5165	BestValid 0.5160
	Epoch 4750:	Loss 1.3925	TrainAcc 0.5359	ValidAcc 0.5161	TestAcc 0.5161	BestValid 0.5161
	Epoch 4800:	Loss 1.3936	TrainAcc 0.5358	ValidAcc 0.5155	TestAcc 0.5161	BestValid 0.5161
	Epoch 4850:	Loss 1.3911	TrainAcc 0.5368	ValidAcc 0.5166	TestAcc 0.5162	BestValid 0.5166
	Epoch 4900:	Loss 1.3936	TrainAcc 0.5358	ValidAcc 0.5145	TestAcc 0.5159	BestValid 0.5166
	Epoch 4950:	Loss 1.3914	TrainAcc 0.5368	ValidAcc 0.5160	TestAcc 0.5169	BestValid 0.5166
	Epoch 5000:	Loss 1.3872	TrainAcc 0.5379	ValidAcc 0.5162	TestAcc 0.5145	BestValid 0.5166
****** Epoch Time (Excluding Evaluation Cost): 0.166 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.180 ms (Max: 0.249, Min: 0.036, Sum: 1.443)
Cluster-Wide Average, Compute: 32.987 ms (Max: 33.439, Min: 31.920, Sum: 263.899)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.015, Sum: 0.123)
Cluster-Wide Average, Communication-Graph: 123.992 ms (Max: 124.992, Min: 123.494, Sum: 991.935)
Cluster-Wide Average, Optimization: 3.314 ms (Max: 3.355, Min: 3.256, Sum: 26.512)
Cluster-Wide Average, Others: 5.607 ms (Max: 5.692, Min: 5.538, Sum: 44.855)
****** Breakdown Sum: 166.104 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.018 GB (Max: 7.514, Min: 6.934, Sum: 56.147)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.578 Gbps (Max: 51.368, Min: 16.823, Sum: 340.627)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 4.603 GB
****** Accuracy Results ******
Highest valid_acc: 0.5166
Target test_acc: 0.5162
Epoch to reach the target acc: 4849
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
