Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
        It takes 0.032 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
        It takes 0.017 seconds.
        It takes 0.020 seconds.
        It takes 0.017 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.018 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.147 Gbps (per GPU), 457.177 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.879 Gbps (per GPU), 455.030 Gbps (aggregated)
The layer-level communication performance: 56.889 Gbps (per GPU), 455.111 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.650 Gbps (per GPU), 453.196 Gbps (aggregated)
The layer-level communication performance: 56.622 Gbps (per GPU), 452.980 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.413 Gbps (per GPU), 451.301 Gbps (aggregated)
The layer-level communication performance: 56.372 Gbps (per GPU), 450.973 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.340 Gbps (per GPU), 450.722 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.961 Gbps (per GPU), 1255.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.955 Gbps (per GPU), 1255.636 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.958 Gbps (per GPU), 1255.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.049 Gbps (per GPU), 1256.391 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.961 Gbps (per GPU), 1255.687 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.955 Gbps (per GPU), 1255.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.963 Gbps (per GPU), 1255.706 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.987 Gbps (per GPU), 1255.894 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.655 Gbps (per GPU), 805.242 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.654 Gbps (per GPU), 805.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.658 Gbps (per GPU), 805.268 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.652 Gbps (per GPU), 805.216 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.654 Gbps (per GPU), 805.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.658 Gbps (per GPU), 805.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.654 Gbps (per GPU), 805.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.657 Gbps (per GPU), 805.255 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.973 Gbps (per GPU), 295.786 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.979 Gbps (per GPU), 295.834 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.973 Gbps (per GPU), 295.783 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.975 Gbps (per GPU), 295.801 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.975 Gbps (per GPU), 295.798 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.969 Gbps (per GPU), 295.754 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.969 Gbps (per GPU), 295.755 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.971 Gbps (per GPU), 295.771 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.98ms  0.82ms  0.98ms  1.20 11.16K  0.12M
 chk_1  0.98ms  0.82ms  0.98ms  1.19 11.16K  0.11M
 chk_2  0.98ms  0.79ms  0.95ms  1.24 11.16K  0.11M
 chk_3  0.98ms  0.75ms  0.91ms  1.31 11.16K  0.12M
 chk_4  0.98ms  0.82ms  0.98ms  1.20 11.16K  0.11M
 chk_5  0.97ms  0.81ms  0.97ms  1.21 11.16K  0.10M
 chk_6  0.97ms  0.81ms  0.97ms  1.20 11.16K  0.12M
 chk_7  0.98ms  0.82ms  0.98ms  1.20 11.16K  0.11M
   Avg  0.98  0.80  0.97
   Max  0.98  0.82  0.98
   Min  0.97  0.75  0.91
 Ratio  1.01  1.10  1.08
   Var  0.00  0.00  0.00
Profiling takes 0.335 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 32.199 ms
Partition 0 [0, 4) has cost: 27.149 ms
Partition 1 [4, 8) has cost: 25.759 ms
Partition 2 [8, 12) has cost: 25.759 ms
Partition 3 [12, 16) has cost: 25.759 ms
Partition 4 [16, 20) has cost: 25.759 ms
Partition 5 [20, 24) has cost: 25.759 ms
Partition 6 [24, 29) has cost: 32.199 ms
Partition 7 [29, 33) has cost: 27.057 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 27.003 ms
GPU 0, Compute+Comm Time: 13.289 ms, Bubble Time: 13.091 ms, Imbalance Overhead: 0.623 ms
GPU 1, Compute+Comm Time: 13.200 ms, Bubble Time: 12.880 ms, Imbalance Overhead: 0.924 ms
GPU 2, Compute+Comm Time: 13.200 ms, Bubble Time: 12.744 ms, Imbalance Overhead: 1.060 ms
GPU 3, Compute+Comm Time: 13.200 ms, Bubble Time: 12.480 ms, Imbalance Overhead: 1.324 ms
GPU 4, Compute+Comm Time: 13.200 ms, Bubble Time: 12.244 ms, Imbalance Overhead: 1.559 ms
GPU 5, Compute+Comm Time: 13.200 ms, Bubble Time: 12.008 ms, Imbalance Overhead: 1.796 ms
GPU 6, Compute+Comm Time: 15.250 ms, Bubble Time: 11.753 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 13.475 ms, Bubble Time: 11.977 ms, Imbalance Overhead: 1.551 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 47.792 ms
GPU 0, Compute+Comm Time: 23.576 ms, Bubble Time: 21.265 ms, Imbalance Overhead: 2.951 ms
GPU 1, Compute+Comm Time: 26.944 ms, Bubble Time: 20.848 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 22.555 ms, Bubble Time: 21.273 ms, Imbalance Overhead: 3.964 ms
GPU 3, Compute+Comm Time: 22.555 ms, Bubble Time: 21.640 ms, Imbalance Overhead: 3.596 ms
GPU 4, Compute+Comm Time: 22.555 ms, Bubble Time: 22.137 ms, Imbalance Overhead: 3.100 ms
GPU 5, Compute+Comm Time: 22.555 ms, Bubble Time: 22.608 ms, Imbalance Overhead: 2.629 ms
GPU 6, Compute+Comm Time: 22.555 ms, Bubble Time: 22.776 ms, Imbalance Overhead: 2.461 ms
GPU 7, Compute+Comm Time: 23.855 ms, Bubble Time: 23.089 ms, Imbalance Overhead: 0.847 ms
The estimated cost of the whole pipeline: 78.535 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 57.958 ms
Partition 0 [0, 8) has cost: 52.908 ms
Partition 1 [8, 16) has cost: 51.518 ms
Partition 2 [16, 25) has cost: 57.958 ms
Partition 3 [25, 33) has cost: 52.816 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.172 ms
GPU 0, Compute+Comm Time: 16.558 ms, Bubble Time: 13.498 ms, Imbalance Overhead: 1.116 ms
GPU 1, Compute+Comm Time: 16.985 ms, Bubble Time: 13.052 ms, Imbalance Overhead: 1.136 ms
GPU 2, Compute+Comm Time: 18.483 ms, Bubble Time: 12.689 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 17.121 ms, Bubble Time: 13.043 ms, Imbalance Overhead: 1.008 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 49.293 ms
GPU 0, Compute+Comm Time: 26.998 ms, Bubble Time: 20.686 ms, Imbalance Overhead: 1.608 ms
GPU 1, Compute+Comm Time: 29.174 ms, Bubble Time: 20.120 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 26.487 ms, Bubble Time: 20.707 ms, Imbalance Overhead: 2.099 ms
GPU 3, Compute+Comm Time: 26.659 ms, Bubble Time: 21.271 ms, Imbalance Overhead: 1.364 ms
    The estimated cost with 2 DP ways is 84.488 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 110.774 ms
Partition 0 [0, 16) has cost: 104.426 ms
Partition 1 [16, 33) has cost: 110.774 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.753 ms
GPU 0, Compute+Comm Time: 29.361 ms, Bubble Time: 16.032 ms, Imbalance Overhead: 1.359 ms
GPU 1, Compute+Comm Time: 32.070 ms, Bubble Time: 14.683 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 62.008 ms
GPU 0, Compute+Comm Time: 42.419 ms, Bubble Time: 19.589 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 39.222 ms, Bubble Time: 21.234 ms, Imbalance Overhead: 1.552 ms
    The estimated cost with 4 DP ways is 114.198 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 215.201 ms
Partition 0 [0, 33) has cost: 215.201 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 116.861 ms
GPU 0, Compute+Comm Time: 116.861 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 127.006 ms
GPU 0, Compute+Comm Time: 127.006 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 256.060 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.6287	TrainAcc 0.4462	ValidAcc 0.4464	TestAcc 0.4479	BestValid 0.4464
	Epoch 100:	Loss 1.6083	TrainAcc 0.4717	ValidAcc 0.4755	TestAcc 0.4721	BestValid 0.4755
	Epoch 150:	Loss 1.5903	TrainAcc 0.4813	ValidAcc 0.4845	TestAcc 0.4795	BestValid 0.4845
	Epoch 200:	Loss 1.5660	TrainAcc 0.4845	ValidAcc 0.4843	TestAcc 0.4821	BestValid 0.4845
	Epoch 250:	Loss 1.5404	TrainAcc 0.4872	ValidAcc 0.4832	TestAcc 0.4843	BestValid 0.4845
	Epoch 300:	Loss 1.5248	TrainAcc 0.4906	ValidAcc 0.4866	TestAcc 0.4872	BestValid 0.4866
	Epoch 350:	Loss 1.5124	TrainAcc 0.4918	ValidAcc 0.4880	TestAcc 0.4880	BestValid 0.4880
	Epoch 400:	Loss 1.5047	TrainAcc 0.4939	ValidAcc 0.4889	TestAcc 0.4891	BestValid 0.4889
	Epoch 450:	Loss 1.4985	TrainAcc 0.4919	ValidAcc 0.4866	TestAcc 0.4862	BestValid 0.4889
	Epoch 500:	Loss 1.4934	TrainAcc 0.4984	ValidAcc 0.4929	TestAcc 0.4931	BestValid 0.4929
	Epoch 550:	Loss 1.4865	TrainAcc 0.4983	ValidAcc 0.4922	TestAcc 0.4928	BestValid 0.4929
	Epoch 600:	Loss 1.4868	TrainAcc 0.5015	ValidAcc 0.4941	TestAcc 0.4958	BestValid 0.4941
	Epoch 650:	Loss 1.4766	TrainAcc 0.5027	ValidAcc 0.4946	TestAcc 0.4967	BestValid 0.4946
	Epoch 700:	Loss 1.4755	TrainAcc 0.5010	ValidAcc 0.4916	TestAcc 0.4937	BestValid 0.4946
	Epoch 750:	Loss 1.4790	TrainAcc 0.4889	ValidAcc 0.4811	TestAcc 0.4814	BestValid 0.4946
	Epoch 800:	Loss 1.4702	TrainAcc 0.5041	ValidAcc 0.4954	TestAcc 0.4972	BestValid 0.4954
	Epoch 850:	Loss 1.4650	TrainAcc 0.5035	ValidAcc 0.4939	TestAcc 0.4961	BestValid 0.4954
	Epoch 900:	Loss 1.4667	TrainAcc 0.5101	ValidAcc 0.5022	TestAcc 0.5029	BestValid 0.5022
	Epoch 950:	Loss 1.4635	TrainAcc 0.5062	ValidAcc 0.4974	TestAcc 0.4993	BestValid 0.5022
	Epoch 1000:	Loss 1.4628	TrainAcc 0.5020	ValidAcc 0.4930	TestAcc 0.4944	BestValid 0.5022
	Epoch 1050:	Loss 1.4649	TrainAcc 0.5009	ValidAcc 0.4917	TestAcc 0.4941	BestValid 0.5022
	Epoch 1100:	Loss 1.4590	TrainAcc 0.5063	ValidAcc 0.4960	TestAcc 0.4979	BestValid 0.5022
	Epoch 1150:	Loss 1.4588	TrainAcc 0.5069	ValidAcc 0.4963	TestAcc 0.4975	BestValid 0.5022
	Epoch 1200:	Loss 1.4560	TrainAcc 0.5033	ValidAcc 0.4944	TestAcc 0.4949	BestValid 0.5022
	Epoch 1250:	Loss 1.4527	TrainAcc 0.5110	ValidAcc 0.5024	TestAcc 0.5033	BestValid 0.5024
	Epoch 1300:	Loss 1.4528	TrainAcc 0.5084	ValidAcc 0.4976	TestAcc 0.5006	BestValid 0.5024
	Epoch 1350:	Loss 1.4492	TrainAcc 0.5089	ValidAcc 0.4984	TestAcc 0.5003	BestValid 0.5024
	Epoch 1400:	Loss 1.4494	TrainAcc 0.5066	ValidAcc 0.4957	TestAcc 0.4981	BestValid 0.5024
	Epoch 1450:	Loss 1.4481	TrainAcc 0.5018	ValidAcc 0.4915	TestAcc 0.4929	BestValid 0.5024
	Epoch 1500:	Loss 1.4418	TrainAcc 0.5115	ValidAcc 0.5018	TestAcc 0.5024	BestValid 0.5024
	Epoch 1550:	Loss 1.4426	TrainAcc 0.5137	ValidAcc 0.5052	TestAcc 0.5052	BestValid 0.5052
	Epoch 1600:	Loss 1.4459	TrainAcc 0.5085	ValidAcc 0.4977	TestAcc 0.5000	BestValid 0.5052
	Epoch 1650:	Loss 1.4411	TrainAcc 0.5124	ValidAcc 0.5025	TestAcc 0.5027	BestValid 0.5052
	Epoch 1700:	Loss 1.4406	TrainAcc 0.5127	ValidAcc 0.5037	TestAcc 0.5038	BestValid 0.5052
	Epoch 1750:	Loss 1.4378	TrainAcc 0.5113	ValidAcc 0.5008	TestAcc 0.5026	BestValid 0.5052
	Epoch 1800:	Loss 1.4386	TrainAcc 0.5139	ValidAcc 0.5046	TestAcc 0.5044	BestValid 0.5052
	Epoch 1850:	Loss 1.4382	TrainAcc 0.5114	ValidAcc 0.4997	TestAcc 0.5018	BestValid 0.5052
	Epoch 1900:	Loss 1.4350	TrainAcc 0.5122	ValidAcc 0.5020	TestAcc 0.5033	BestValid 0.5052
	Epoch 1950:	Loss 1.4314	TrainAcc 0.5135	ValidAcc 0.5046	TestAcc 0.5044	BestValid 0.5052
	Epoch 2000:	Loss 1.4366	TrainAcc 0.5173	ValidAcc 0.5073	TestAcc 0.5065	BestValid 0.5073
	Epoch 2050:	Loss 1.4312	TrainAcc 0.5080	ValidAcc 0.4945	TestAcc 0.4958	BestValid 0.5073
	Epoch 2100:	Loss 1.4330	TrainAcc 0.5142	ValidAcc 0.5042	TestAcc 0.5047	BestValid 0.5073
	Epoch 2150:	Loss 1.4287	TrainAcc 0.5064	ValidAcc 0.4936	TestAcc 0.4959	BestValid 0.5073
	Epoch 2200:	Loss 1.4261	TrainAcc 0.5145	ValidAcc 0.5021	TestAcc 0.5034	BestValid 0.5073
	Epoch 2250:	Loss 1.4248	TrainAcc 0.5170	ValidAcc 0.5058	TestAcc 0.5066	BestValid 0.5073
	Epoch 2300:	Loss 1.4288	TrainAcc 0.5194	ValidAcc 0.5082	TestAcc 0.5076	BestValid 0.5082
	Epoch 2350:	Loss 1.4279	TrainAcc 0.5133	ValidAcc 0.5005	TestAcc 0.5024	BestValid 0.5082
	Epoch 2400:	Loss 1.4258	TrainAcc 0.5125	ValidAcc 0.5003	TestAcc 0.5018	BestValid 0.5082
	Epoch 2450:	Loss 1.4273	TrainAcc 0.5218	ValidAcc 0.5110	TestAcc 0.5115	BestValid 0.5110
	Epoch 2500:	Loss 1.4242	TrainAcc 0.5217	ValidAcc 0.5099	TestAcc 0.5107	BestValid 0.5110
	Epoch 2550:	Loss 1.4275	TrainAcc 0.5177	ValidAcc 0.5074	TestAcc 0.5067	BestValid 0.5110
	Epoch 2600:	Loss 1.4287	TrainAcc 0.5151	ValidAcc 0.5017	TestAcc 0.5040	BestValid 0.5110
	Epoch 2650:	Loss 1.4214	TrainAcc 0.5182	ValidAcc 0.5069	TestAcc 0.5072	BestValid 0.5110
	Epoch 2700:	Loss 1.4200	TrainAcc 0.5227	ValidAcc 0.5092	TestAcc 0.5103	BestValid 0.5110
	Epoch 2750:	Loss 1.4251	TrainAcc 0.5221	ValidAcc 0.5118	TestAcc 0.5127	BestValid 0.5118
	Epoch 2800:	Loss 1.4235	TrainAcc 0.5238	ValidAcc 0.5113	TestAcc 0.5115	BestValid 0.5118
	Epoch 2850:	Loss 1.4231	TrainAcc 0.5236	ValidAcc 0.5110	TestAcc 0.5115	BestValid 0.5118
	Epoch 2900:	Loss 1.4230	TrainAcc 0.5114	ValidAcc 0.4965	TestAcc 0.4978	BestValid 0.5118
	Epoch 2950:	Loss 1.4164	TrainAcc 0.5208	ValidAcc 0.5086	TestAcc 0.5084	BestValid 0.5118
	Epoch 3000:	Loss 1.4205	TrainAcc 0.5238	ValidAcc 0.5104	TestAcc 0.5114	BestValid 0.5118
	Epoch 3050:	Loss 1.4142	TrainAcc 0.5240	ValidAcc 0.5086	TestAcc 0.5094	BestValid 0.5118
	Epoch 3100:	Loss 1.4145	TrainAcc 0.5191	ValidAcc 0.5058	TestAcc 0.5074	BestValid 0.5118
	Epoch 3150:	Loss 1.4150	TrainAcc 0.5203	ValidAcc 0.5064	TestAcc 0.5074	BestValid 0.5118
	Epoch 3200:	Loss 1.4144	TrainAcc 0.5204	ValidAcc 0.5069	TestAcc 0.5080	BestValid 0.5118
	Epoch 3250:	Loss 1.4131	TrainAcc 0.5267	ValidAcc 0.5133	TestAcc 0.5128	BestValid 0.5133
	Epoch 3300:	Loss 1.4123	TrainAcc 0.5202	ValidAcc 0.5066	TestAcc 0.5080	BestValid 0.5133
	Epoch 3350:	Loss 1.4130	TrainAcc 0.5249	ValidAcc 0.5090	TestAcc 0.5100	BestValid 0.5133
	Epoch 3400:	Loss 1.4089	TrainAcc 0.5253	ValidAcc 0.5130	TestAcc 0.5149	BestValid 0.5133
	Epoch 3450:	Loss 1.4140	TrainAcc 0.5260	ValidAcc 0.5097	TestAcc 0.5096	BestValid 0.5133
	Epoch 3500:	Loss 1.4120	TrainAcc 0.5269	ValidAcc 0.5132	TestAcc 0.5124	BestValid 0.5133
	Epoch 3550:	Loss 1.4138	TrainAcc 0.5280	ValidAcc 0.5130	TestAcc 0.5126	BestValid 0.5133
	Epoch 3600:	Loss 1.4064	TrainAcc 0.5283	ValidAcc 0.5127	TestAcc 0.5148	BestValid 0.5133
	Epoch 3650:	Loss 1.4026	TrainAcc 0.5284	ValidAcc 0.5125	TestAcc 0.5128	BestValid 0.5133
	Epoch 3700:	Loss 1.4056	TrainAcc 0.5287	ValidAcc 0.5130	TestAcc 0.5128	BestValid 0.5133
	Epoch 3750:	Loss 1.4080	TrainAcc 0.5236	ValidAcc 0.5081	TestAcc 0.5091	BestValid 0.5133
	Epoch 3800:	Loss 1.4078	TrainAcc 0.5267	ValidAcc 0.5101	TestAcc 0.5105	BestValid 0.5133
	Epoch 3850:	Loss 1.4043	TrainAcc 0.5281	ValidAcc 0.5110	TestAcc 0.5109	BestValid 0.5133
	Epoch 3900:	Loss 1.4037	TrainAcc 0.5272	ValidAcc 0.5110	TestAcc 0.5109	BestValid 0.5133
	Epoch 3950:	Loss 1.4012	TrainAcc 0.5302	ValidAcc 0.5124	TestAcc 0.5126	BestValid 0.5133
	Epoch 4000:	Loss 1.4027	TrainAcc 0.5317	ValidAcc 0.5150	TestAcc 0.5153	BestValid 0.5150
	Epoch 4050:	Loss 1.4036	TrainAcc 0.5295	ValidAcc 0.5140	TestAcc 0.5146	BestValid 0.5150
	Epoch 4100:	Loss 1.4009	TrainAcc 0.5308	ValidAcc 0.5139	TestAcc 0.5135	BestValid 0.5150
	Epoch 4150:	Loss 1.4022	TrainAcc 0.5319	ValidAcc 0.5140	TestAcc 0.5139	BestValid 0.5150
	Epoch 4200:	Loss 1.4057	TrainAcc 0.5284	ValidAcc 0.5120	TestAcc 0.5117	BestValid 0.5150
	Epoch 4250:	Loss 1.4012	TrainAcc 0.5327	ValidAcc 0.5156	TestAcc 0.5149	BestValid 0.5156
	Epoch 4300:	Loss 1.4003	TrainAcc 0.5327	ValidAcc 0.5160	TestAcc 0.5162	BestValid 0.5160
	Epoch 4350:	Loss 1.3967	TrainAcc 0.5317	ValidAcc 0.5143	TestAcc 0.5156	BestValid 0.5160
	Epoch 4400:	Loss 1.4030	TrainAcc 0.5324	ValidAcc 0.5147	TestAcc 0.5135	BestValid 0.5160
	Epoch 4450:	Loss 1.3976	TrainAcc 0.5329	ValidAcc 0.5155	TestAcc 0.5164	BestValid 0.5160
	Epoch 4500:	Loss 1.3936	TrainAcc 0.5334	ValidAcc 0.5151	TestAcc 0.5139	BestValid 0.5160
	Epoch 4550:	Loss 1.3964	TrainAcc 0.5325	ValidAcc 0.5141	TestAcc 0.5151	BestValid 0.5160
	Epoch 4600:	Loss 1.3941	TrainAcc 0.5343	ValidAcc 0.5157	TestAcc 0.5145	BestValid 0.5160
	Epoch 4650:	Loss 1.3945	TrainAcc 0.5346	ValidAcc 0.5155	TestAcc 0.5167	BestValid 0.5160
	Epoch 4700:	Loss 1.3923	TrainAcc 0.5354	ValidAcc 0.5153	TestAcc 0.5169	BestValid 0.5160
	Epoch 4750:	Loss 1.3925	TrainAcc 0.5359	ValidAcc 0.5159	TestAcc 0.5162	BestValid 0.5160
	Epoch 4800:	Loss 1.3935	TrainAcc 0.5358	ValidAcc 0.5153	TestAcc 0.5162	BestValid 0.5160
	Epoch 4850:	Loss 1.3910	TrainAcc 0.5368	ValidAcc 0.5166	TestAcc 0.5164	BestValid 0.5166
	Epoch 4900:	Loss 1.3935	TrainAcc 0.5356	ValidAcc 0.5144	TestAcc 0.5158	BestValid 0.5166
	Epoch 4950:	Loss 1.3912	TrainAcc 0.5368	ValidAcc 0.5162	TestAcc 0.5170	BestValid 0.5166
	Epoch 5000:	Loss 1.3870	TrainAcc 0.5380	ValidAcc 0.5161	TestAcc 0.5151	BestValid 0.5166
****** Epoch Time (Excluding Evaluation Cost): 0.167 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.151 ms (Max: 0.217, Min: 0.036, Sum: 1.206)
Cluster-Wide Average, Compute: 33.005 ms (Max: 33.520, Min: 32.038, Sum: 264.036)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.064)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.013, Sum: 0.119)
Cluster-Wide Average, Communication-Graph: 124.526 ms (Max: 125.430, Min: 124.044, Sum: 996.210)
Cluster-Wide Average, Optimization: 3.312 ms (Max: 3.356, Min: 3.250, Sum: 26.492)
Cluster-Wide Average, Others: 5.614 ms (Max: 5.704, Min: 5.538, Sum: 44.914)
****** Breakdown Sum: 166.630 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.018 GB (Max: 7.514, Min: 6.934, Sum: 56.147)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.377 Gbps (Max: 50.955, Min: 16.773, Sum: 339.018)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 4.603 GB
****** Accuracy Results ******
Highest valid_acc: 0.5166
Target test_acc: 0.5164
Epoch to reach the target acc: 4849
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
