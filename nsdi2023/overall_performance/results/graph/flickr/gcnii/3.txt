Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.028 seconds.
Building the Feature Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
        It takes 0.100 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.767 Gbps (per GPU), 478.132 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.466 Gbps (per GPU), 475.725 Gbps (aggregated)
The layer-level communication performance: 59.455 Gbps (per GPU), 475.638 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.177 Gbps (per GPU), 473.418 Gbps (aggregated)
The layer-level communication performance: 59.211 Gbps (per GPU), 473.686 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.965 Gbps (per GPU), 471.721 Gbps (aggregated)
The layer-level communication performance: 58.920 Gbps (per GPU), 471.363 Gbps (aggregated)
The layer-level communication performance: 58.890 Gbps (per GPU), 471.124 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.416 Gbps (per GPU), 1251.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.443 Gbps (per GPU), 1251.541 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.416 Gbps (per GPU), 1251.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.384 Gbps (per GPU), 1251.074 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.416 Gbps (per GPU), 1251.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.457 Gbps (per GPU), 1251.657 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.414 Gbps (per GPU), 1251.310 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.373 Gbps (per GPU), 1250.981 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.597 Gbps (per GPU), 804.772 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.597 Gbps (per GPU), 804.778 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.597 Gbps (per GPU), 804.772 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.598 Gbps (per GPU), 804.785 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.596 Gbps (per GPU), 804.766 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.599 Gbps (per GPU), 804.791 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.597 Gbps (per GPU), 804.778 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.584 Gbps (per GPU), 804.670 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.659 Gbps (per GPU), 245.270 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.658 Gbps (per GPU), 245.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.658 Gbps (per GPU), 245.267 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.659 Gbps (per GPU), 245.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.655 Gbps (per GPU), 245.242 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.655 Gbps (per GPU), 245.238 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.656 Gbps (per GPU), 245.247 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.656 Gbps (per GPU), 245.244 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.99ms  0.81ms  0.98ms  1.22 11.16K  0.12M
 chk_1  0.99ms  0.82ms  0.98ms  1.20 11.16K  0.11M
 chk_2  0.99ms  0.79ms  0.95ms  1.25 11.16K  0.11M
 chk_3  0.99ms  0.75ms  0.91ms  1.31 11.16K  0.12M
 chk_4  0.98ms  0.82ms  0.98ms  1.20 11.16K  0.11M
 chk_5  0.97ms  0.81ms  0.97ms  1.21 11.16K  0.10M
 chk_6  0.98ms  0.81ms  0.98ms  1.20 11.16K  0.12M
 chk_7  0.97ms  0.82ms  0.98ms  1.20 11.16K  0.11M
   Avg  0.98  0.80  0.97
   Max  0.99  0.82  0.98
   Min  0.97  0.75  0.91
 Ratio  1.01  1.09  1.08
   Var  0.00  0.00  0.00
Profiling takes 0.336 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 32.161 ms
Partition 0 [0, 4) has cost: 27.156 ms
Partition 1 [4, 8) has cost: 25.729 ms
Partition 2 [8, 12) has cost: 25.729 ms
Partition 3 [12, 16) has cost: 25.729 ms
Partition 4 [16, 20) has cost: 25.729 ms
Partition 5 [20, 24) has cost: 25.729 ms
Partition 6 [24, 29) has cost: 32.161 ms
Partition 7 [29, 33) has cost: 27.026 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 26.569 ms
GPU 0, Compute+Comm Time: 13.064 ms, Bubble Time: 12.896 ms, Imbalance Overhead: 0.609 ms
GPU 1, Compute+Comm Time: 12.971 ms, Bubble Time: 12.681 ms, Imbalance Overhead: 0.917 ms
GPU 2, Compute+Comm Time: 12.971 ms, Bubble Time: 12.544 ms, Imbalance Overhead: 1.054 ms
GPU 3, Compute+Comm Time: 12.971 ms, Bubble Time: 12.276 ms, Imbalance Overhead: 1.322 ms
GPU 4, Compute+Comm Time: 12.971 ms, Bubble Time: 12.038 ms, Imbalance Overhead: 1.560 ms
GPU 5, Compute+Comm Time: 12.971 ms, Bubble Time: 11.801 ms, Imbalance Overhead: 1.797 ms
GPU 6, Compute+Comm Time: 15.019 ms, Bubble Time: 11.550 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 13.247 ms, Bubble Time: 11.770 ms, Imbalance Overhead: 1.553 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 47.364 ms
GPU 0, Compute+Comm Time: 23.336 ms, Bubble Time: 21.071 ms, Imbalance Overhead: 2.957 ms
GPU 1, Compute+Comm Time: 26.699 ms, Bubble Time: 20.665 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 22.315 ms, Bubble Time: 21.100 ms, Imbalance Overhead: 3.950 ms
GPU 3, Compute+Comm Time: 22.315 ms, Bubble Time: 21.470 ms, Imbalance Overhead: 3.579 ms
GPU 4, Compute+Comm Time: 22.315 ms, Bubble Time: 21.958 ms, Imbalance Overhead: 3.091 ms
GPU 5, Compute+Comm Time: 22.315 ms, Bubble Time: 22.417 ms, Imbalance Overhead: 2.632 ms
GPU 6, Compute+Comm Time: 22.315 ms, Bubble Time: 22.591 ms, Imbalance Overhead: 2.458 ms
GPU 7, Compute+Comm Time: 23.650 ms, Bubble Time: 22.897 ms, Imbalance Overhead: 0.817 ms
The estimated cost of the whole pipeline: 77.630 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 57.890 ms
Partition 0 [0, 8) has cost: 52.886 ms
Partition 1 [8, 16) has cost: 51.458 ms
Partition 2 [16, 25) has cost: 57.890 ms
Partition 3 [25, 33) has cost: 52.755 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.804 ms
GPU 0, Compute+Comm Time: 16.345 ms, Bubble Time: 13.336 ms, Imbalance Overhead: 1.123 ms
GPU 1, Compute+Comm Time: 16.770 ms, Bubble Time: 12.891 ms, Imbalance Overhead: 1.143 ms
GPU 2, Compute+Comm Time: 18.269 ms, Bubble Time: 12.535 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 16.909 ms, Bubble Time: 12.896 ms, Imbalance Overhead: 1.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.895 ms
GPU 0, Compute+Comm Time: 26.771 ms, Bubble Time: 20.494 ms, Imbalance Overhead: 1.631 ms
GPU 1, Compute+Comm Time: 28.944 ms, Bubble Time: 19.952 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 26.259 ms, Bubble Time: 20.568 ms, Imbalance Overhead: 2.069 ms
GPU 3, Compute+Comm Time: 26.452 ms, Bubble Time: 21.143 ms, Imbalance Overhead: 1.300 ms
    The estimated cost with 2 DP ways is 83.685 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 110.646 ms
Partition 0 [0, 16) has cost: 104.344 ms
Partition 1 [16, 33) has cost: 110.646 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.440 ms
GPU 0, Compute+Comm Time: 29.147 ms, Bubble Time: 15.916 ms, Imbalance Overhead: 1.377 ms
GPU 1, Compute+Comm Time: 31.855 ms, Bubble Time: 14.584 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.602 ms
GPU 0, Compute+Comm Time: 42.136 ms, Bubble Time: 19.466 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.956 ms, Bubble Time: 21.082 ms, Imbalance Overhead: 1.564 ms
    The estimated cost with 4 DP ways is 113.444 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 214.989 ms
Partition 0 [0, 33) has cost: 214.989 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 139.139 ms
GPU 0, Compute+Comm Time: 139.139 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 149.186 ms
GPU 0, Compute+Comm Time: 149.186 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 302.741 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.6500	TrainAcc 0.4300	ValidAcc 0.4333	TestAcc 0.4334	BestValid 0.4333
	Epoch 100:	Loss 1.6325	TrainAcc 0.4530	ValidAcc 0.4542	TestAcc 0.4552	BestValid 0.4542
	Epoch 150:	Loss 1.6193	TrainAcc 0.4669	ValidAcc 0.4664	TestAcc 0.4679	BestValid 0.4664
	Epoch 200:	Loss 1.6061	TrainAcc 0.4755	ValidAcc 0.4753	TestAcc 0.4755	BestValid 0.4753
	Epoch 250:	Loss 1.5943	TrainAcc 0.4823	ValidAcc 0.4833	TestAcc 0.4833	BestValid 0.4833
	Epoch 300:	Loss 1.5826	TrainAcc 0.4854	ValidAcc 0.4867	TestAcc 0.4857	BestValid 0.4867
	Epoch 350:	Loss 1.5717	TrainAcc 0.4903	ValidAcc 0.4914	TestAcc 0.4887	BestValid 0.4914
	Epoch 400:	Loss 1.5642	TrainAcc 0.4918	ValidAcc 0.4921	TestAcc 0.4894	BestValid 0.4921
	Epoch 450:	Loss 1.5590	TrainAcc 0.4950	ValidAcc 0.4938	TestAcc 0.4931	BestValid 0.4938
	Epoch 500:	Loss 1.5488	TrainAcc 0.4963	ValidAcc 0.4945	TestAcc 0.4937	BestValid 0.4945
	Epoch 550:	Loss 1.5417	TrainAcc 0.4970	ValidAcc 0.4962	TestAcc 0.4952	BestValid 0.4962
	Epoch 600:	Loss 1.5350	TrainAcc 0.4963	ValidAcc 0.4969	TestAcc 0.4960	BestValid 0.4969
	Epoch 650:	Loss 1.5284	TrainAcc 0.4993	ValidAcc 0.4977	TestAcc 0.4972	BestValid 0.4977
	Epoch 700:	Loss 1.5226	TrainAcc 0.4992	ValidAcc 0.4966	TestAcc 0.4963	BestValid 0.4977
	Epoch 750:	Loss 1.5107	TrainAcc 0.5006	ValidAcc 0.4972	TestAcc 0.4970	BestValid 0.4977
	Epoch 800:	Loss 1.5051	TrainAcc 0.4961	ValidAcc 0.4917	TestAcc 0.4930	BestValid 0.4977
	Epoch 850:	Loss 1.5017	TrainAcc 0.5005	ValidAcc 0.4965	TestAcc 0.4965	BestValid 0.4977
	Epoch 900:	Loss 1.4970	TrainAcc 0.4991	ValidAcc 0.4945	TestAcc 0.4952	BestValid 0.4977
	Epoch 950:	Loss 1.4946	TrainAcc 0.5032	ValidAcc 0.4991	TestAcc 0.5006	BestValid 0.4991
	Epoch 1000:	Loss 1.4899	TrainAcc 0.4989	ValidAcc 0.4929	TestAcc 0.4946	BestValid 0.4991
	Epoch 1050:	Loss 1.4865	TrainAcc 0.5005	ValidAcc 0.4944	TestAcc 0.4961	BestValid 0.4991
	Epoch 1100:	Loss 1.4801	TrainAcc 0.5034	ValidAcc 0.4989	TestAcc 0.5007	BestValid 0.4991
	Epoch 1150:	Loss 1.4823	TrainAcc 0.5007	ValidAcc 0.4938	TestAcc 0.4942	BestValid 0.4991
	Epoch 1200:	Loss 1.4783	TrainAcc 0.5034	ValidAcc 0.4984	TestAcc 0.4994	BestValid 0.4991
	Epoch 1250:	Loss 1.4734	TrainAcc 0.5061	ValidAcc 0.5006	TestAcc 0.5034	BestValid 0.5006
	Epoch 1300:	Loss 1.4691	TrainAcc 0.5015	ValidAcc 0.4939	TestAcc 0.4941	BestValid 0.5006
	Epoch 1350:	Loss 1.4693	TrainAcc 0.4932	ValidAcc 0.4843	TestAcc 0.4851	BestValid 0.5006
	Epoch 1400:	Loss 1.4680	TrainAcc 0.5006	ValidAcc 0.4934	TestAcc 0.4925	BestValid 0.5006
	Epoch 1450:	Loss 1.4702	TrainAcc 0.5033	ValidAcc 0.4965	TestAcc 0.4972	BestValid 0.5006
	Epoch 1500:	Loss 1.4702	TrainAcc 0.5034	ValidAcc 0.4972	TestAcc 0.4985	BestValid 0.5006
	Epoch 1550:	Loss 1.4669	TrainAcc 0.5034	ValidAcc 0.4957	TestAcc 0.4964	BestValid 0.5006
	Epoch 1600:	Loss 1.4632	TrainAcc 0.5043	ValidAcc 0.4972	TestAcc 0.4971	BestValid 0.5006
	Epoch 1650:	Loss 1.4602	TrainAcc 0.5078	ValidAcc 0.5007	TestAcc 0.5028	BestValid 0.5007
	Epoch 1700:	Loss 1.4657	TrainAcc 0.5120	ValidAcc 0.5061	TestAcc 0.5058	BestValid 0.5061
	Epoch 1750:	Loss 1.4556	TrainAcc 0.5008	ValidAcc 0.4915	TestAcc 0.4924	BestValid 0.5061
	Epoch 1800:	Loss 1.4516	TrainAcc 0.5107	ValidAcc 0.5045	TestAcc 0.5044	BestValid 0.5061
	Epoch 1850:	Loss 1.4568	TrainAcc 0.5100	ValidAcc 0.5026	TestAcc 0.5031	BestValid 0.5061
	Epoch 1900:	Loss 1.4589	TrainAcc 0.5038	ValidAcc 0.4951	TestAcc 0.4968	BestValid 0.5061
	Epoch 1950:	Loss 1.4510	TrainAcc 0.5121	ValidAcc 0.5053	TestAcc 0.5063	BestValid 0.5061
	Epoch 2000:	Loss 1.4495	TrainAcc 0.5070	ValidAcc 0.4990	TestAcc 0.5007	BestValid 0.5061
	Epoch 2050:	Loss 1.4539	TrainAcc 0.5141	ValidAcc 0.5075	TestAcc 0.5071	BestValid 0.5075
	Epoch 2100:	Loss 1.4522	TrainAcc 0.5109	ValidAcc 0.5026	TestAcc 0.5048	BestValid 0.5075
	Epoch 2150:	Loss 1.4454	TrainAcc 0.5068	ValidAcc 0.4966	TestAcc 0.4995	BestValid 0.5075
	Epoch 2200:	Loss 1.4453	TrainAcc 0.5163	ValidAcc 0.5105	TestAcc 0.5093	BestValid 0.5105
	Epoch 2250:	Loss 1.4449	TrainAcc 0.5145	ValidAcc 0.5076	TestAcc 0.5077	BestValid 0.5105
	Epoch 2300:	Loss 1.4427	TrainAcc 0.5154	ValidAcc 0.5086	TestAcc 0.5094	BestValid 0.5105
	Epoch 2350:	Loss 1.4447	TrainAcc 0.5087	ValidAcc 0.4981	TestAcc 0.5000	BestValid 0.5105
	Epoch 2400:	Loss 1.4441	TrainAcc 0.5098	ValidAcc 0.4998	TestAcc 0.5038	BestValid 0.5105
	Epoch 2450:	Loss 1.4444	TrainAcc 0.5172	ValidAcc 0.5104	TestAcc 0.5106	BestValid 0.5105
	Epoch 2500:	Loss 1.4418	TrainAcc 0.5164	ValidAcc 0.5078	TestAcc 0.5081	BestValid 0.5105
	Epoch 2550:	Loss 1.4417	TrainAcc 0.5046	ValidAcc 0.4940	TestAcc 0.4958	BestValid 0.5105
	Epoch 2600:	Loss 1.4370	TrainAcc 0.5114	ValidAcc 0.5017	TestAcc 0.5047	BestValid 0.5105
	Epoch 2650:	Loss 1.4391	TrainAcc 0.5141	ValidAcc 0.5044	TestAcc 0.5070	BestValid 0.5105
	Epoch 2700:	Loss 1.4353	TrainAcc 0.5182	ValidAcc 0.5099	TestAcc 0.5110	BestValid 0.5105
	Epoch 2750:	Loss 1.4374	TrainAcc 0.5191	ValidAcc 0.5102	TestAcc 0.5109	BestValid 0.5105
	Epoch 2800:	Loss 1.4347	TrainAcc 0.5154	ValidAcc 0.5052	TestAcc 0.5074	BestValid 0.5105
	Epoch 2850:	Loss 1.4356	TrainAcc 0.5176	ValidAcc 0.5082	TestAcc 0.5084	BestValid 0.5105
	Epoch 2900:	Loss 1.4404	TrainAcc 0.5187	ValidAcc 0.5095	TestAcc 0.5114	BestValid 0.5105
	Epoch 2950:	Loss 1.4303	TrainAcc 0.5194	ValidAcc 0.5105	TestAcc 0.5104	BestValid 0.5105
	Epoch 3000:	Loss 1.4390	TrainAcc 0.5180	ValidAcc 0.5073	TestAcc 0.5084	BestValid 0.5105
	Epoch 3050:	Loss 1.4308	TrainAcc 0.5182	ValidAcc 0.5078	TestAcc 0.5086	BestValid 0.5105
	Epoch 3100:	Loss 1.4318	TrainAcc 0.5151	ValidAcc 0.5033	TestAcc 0.5059	BestValid 0.5105
	Epoch 3150:	Loss 1.4323	TrainAcc 0.5223	ValidAcc 0.5131	TestAcc 0.5136	BestValid 0.5131
	Epoch 3200:	Loss 1.4298	TrainAcc 0.5157	ValidAcc 0.5036	TestAcc 0.5062	BestValid 0.5131
	Epoch 3250:	Loss 1.4244	TrainAcc 0.5177	ValidAcc 0.5064	TestAcc 0.5089	BestValid 0.5131
	Epoch 3300:	Loss 1.4314	TrainAcc 0.5223	ValidAcc 0.5101	TestAcc 0.5120	BestValid 0.5131
	Epoch 3350:	Loss 1.4354	TrainAcc 0.5174	ValidAcc 0.5050	TestAcc 0.5081	BestValid 0.5131
	Epoch 3400:	Loss 1.4296	TrainAcc 0.5232	ValidAcc 0.5105	TestAcc 0.5124	BestValid 0.5131
	Epoch 3450:	Loss 1.4210	TrainAcc 0.5236	ValidAcc 0.5104	TestAcc 0.5132	BestValid 0.5131
	Epoch 3500:	Loss 1.4361	TrainAcc 0.5255	ValidAcc 0.5124	TestAcc 0.5135	BestValid 0.5131
	Epoch 3550:	Loss 1.4223	TrainAcc 0.5247	ValidAcc 0.5120	TestAcc 0.5128	BestValid 0.5131
	Epoch 3600:	Loss 1.4180	TrainAcc 0.5254	ValidAcc 0.5130	TestAcc 0.5130	BestValid 0.5131
	Epoch 3650:	Loss 1.4253	TrainAcc 0.5210	ValidAcc 0.5091	TestAcc 0.5112	BestValid 0.5131
	Epoch 3700:	Loss 1.4218	TrainAcc 0.5200	ValidAcc 0.5068	TestAcc 0.5098	BestValid 0.5131
	Epoch 3750:	Loss 1.4190	TrainAcc 0.5261	ValidAcc 0.5125	TestAcc 0.5141	BestValid 0.5131
	Epoch 3800:	Loss 1.4206	TrainAcc 0.5273	ValidAcc 0.5143	TestAcc 0.5149	BestValid 0.5143
	Epoch 3850:	Loss 1.4155	TrainAcc 0.5252	ValidAcc 0.5118	TestAcc 0.5129	BestValid 0.5143
	Epoch 3900:	Loss 1.4143	TrainAcc 0.5232	ValidAcc 0.5092	TestAcc 0.5117	BestValid 0.5143
	Epoch 3950:	Loss 1.4156	TrainAcc 0.5266	ValidAcc 0.5130	TestAcc 0.5143	BestValid 0.5143
	Epoch 4000:	Loss 1.4192	TrainAcc 0.5285	ValidAcc 0.5143	TestAcc 0.5144	BestValid 0.5143
	Epoch 4050:	Loss 1.4138	TrainAcc 0.5289	ValidAcc 0.5138	TestAcc 0.5147	BestValid 0.5143
	Epoch 4100:	Loss 1.4145	TrainAcc 0.5296	ValidAcc 0.5151	TestAcc 0.5155	BestValid 0.5151
	Epoch 4150:	Loss 1.4091	TrainAcc 0.5303	ValidAcc 0.5152	TestAcc 0.5151	BestValid 0.5152
	Epoch 4200:	Loss 1.4138	TrainAcc 0.5261	ValidAcc 0.5123	TestAcc 0.5138	BestValid 0.5152
	Epoch 4250:	Loss 1.4080	TrainAcc 0.5288	ValidAcc 0.5146	TestAcc 0.5153	BestValid 0.5152
	Epoch 4300:	Loss 1.4110	TrainAcc 0.5321	ValidAcc 0.5161	TestAcc 0.5160	BestValid 0.5161
	Epoch 4350:	Loss 1.4087	TrainAcc 0.5297	ValidAcc 0.5139	TestAcc 0.5155	BestValid 0.5161
	Epoch 4400:	Loss 1.4062	TrainAcc 0.5315	ValidAcc 0.5159	TestAcc 0.5158	BestValid 0.5161
	Epoch 4450:	Loss 1.4106	TrainAcc 0.5323	ValidAcc 0.5153	TestAcc 0.5163	BestValid 0.5161
	Epoch 4500:	Loss 1.4085	TrainAcc 0.5324	ValidAcc 0.5144	TestAcc 0.5156	BestValid 0.5161
	Epoch 4550:	Loss 1.4166	TrainAcc 0.5319	ValidAcc 0.5144	TestAcc 0.5161	BestValid 0.5161
	Epoch 4600:	Loss 1.4025	TrainAcc 0.5333	ValidAcc 0.5156	TestAcc 0.5168	BestValid 0.5161
	Epoch 4650:	Loss 1.4058	TrainAcc 0.5332	ValidAcc 0.5147	TestAcc 0.5165	BestValid 0.5161
	Epoch 4700:	Loss 1.4034	TrainAcc 0.5336	ValidAcc 0.5153	TestAcc 0.5166	BestValid 0.5161
	Epoch 4750:	Loss 1.4025	TrainAcc 0.5338	ValidAcc 0.5147	TestAcc 0.5166	BestValid 0.5161
	Epoch 4800:	Loss 1.4034	TrainAcc 0.5318	ValidAcc 0.5156	TestAcc 0.5159	BestValid 0.5161
	Epoch 4850:	Loss 1.3998	TrainAcc 0.5333	ValidAcc 0.5150	TestAcc 0.5166	BestValid 0.5161
	Epoch 4900:	Loss 1.4007	TrainAcc 0.5354	ValidAcc 0.5153	TestAcc 0.5171	BestValid 0.5161
	Epoch 4950:	Loss 1.4022	TrainAcc 0.5350	ValidAcc 0.5147	TestAcc 0.5162	BestValid 0.5161
	Epoch 5000:	Loss 1.4099	TrainAcc 0.5360	ValidAcc 0.5157	TestAcc 0.5168	BestValid 0.5161
****** Epoch Time (Excluding Evaluation Cost): 0.166 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.147 ms (Max: 0.194, Min: 0.036, Sum: 1.172)
Cluster-Wide Average, Compute: 33.013 ms (Max: 33.498, Min: 31.816, Sum: 264.103)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.121)
Cluster-Wide Average, Communication-Graph: 123.857 ms (Max: 125.016, Min: 123.326, Sum: 990.858)
Cluster-Wide Average, Optimization: 3.323 ms (Max: 3.369, Min: 3.269, Sum: 26.582)
Cluster-Wide Average, Others: 5.609 ms (Max: 5.702, Min: 5.533, Sum: 44.869)
****** Breakdown Sum: 165.971 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.018 GB (Max: 7.514, Min: 6.934, Sum: 56.147)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.644 Gbps (Max: 51.411, Min: 16.822, Sum: 341.153)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 4.603 GB
****** Accuracy Results ******
Highest valid_acc: 0.5161
Target test_acc: 0.5160
Epoch to reach the target acc: 4299
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
