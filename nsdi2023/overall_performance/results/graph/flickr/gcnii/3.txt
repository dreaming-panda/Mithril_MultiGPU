Initialized node 0 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 1 on machine gnerv2
Initialized node 7 on machine gnerv3
Initialized node 2 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 3 on machine gnerv2
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.031 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.019 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.028 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.437 Gbps (per GPU), 475.499 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.147 Gbps (per GPU), 473.178 Gbps (aggregated)
The layer-level communication performance: 59.142 Gbps (per GPU), 473.140 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.907 Gbps (per GPU), 471.253 Gbps (aggregated)
The layer-level communication performance: 58.880 Gbps (per GPU), 471.036 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.661 Gbps (per GPU), 469.291 Gbps (aggregated)
The layer-level communication performance: 58.617 Gbps (per GPU), 468.939 Gbps (aggregated)
The layer-level communication performance: 58.588 Gbps (per GPU), 468.703 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.195 Gbps (per GPU), 1265.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.195 Gbps (per GPU), 1265.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.186 Gbps (per GPU), 1265.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.174 Gbps (per GPU), 1265.393 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.189 Gbps (per GPU), 1265.512 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.186 Gbps (per GPU), 1265.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.183 Gbps (per GPU), 1265.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.180 Gbps (per GPU), 1265.441 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.409 Gbps (per GPU), 803.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.403 Gbps (per GPU), 803.224 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.409 Gbps (per GPU), 803.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.406 Gbps (per GPU), 803.250 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.407 Gbps (per GPU), 803.256 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.405 Gbps (per GPU), 803.237 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.409 Gbps (per GPU), 803.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.405 Gbps (per GPU), 803.237 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.091 Gbps (per GPU), 240.726 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.092 Gbps (per GPU), 240.733 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.087 Gbps (per GPU), 240.693 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.091 Gbps (per GPU), 240.727 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.088 Gbps (per GPU), 240.708 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.089 Gbps (per GPU), 240.710 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.084 Gbps (per GPU), 240.675 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.090 Gbps (per GPU), 240.721 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.97ms  0.81ms  0.98ms  1.21 11.16K  0.12M
 chk_1  0.97ms  0.81ms  0.97ms  1.20 11.16K  0.11M
 chk_2  0.97ms  0.78ms  0.93ms  1.25 11.16K  0.11M
 chk_3  0.97ms  0.74ms  0.90ms  1.31 11.16K  0.12M
 chk_4  0.97ms  0.80ms  0.96ms  1.21 11.16K  0.11M
 chk_5  0.97ms  0.81ms  0.95ms  1.19 11.16K  0.10M
 chk_6  0.97ms  0.81ms  0.96ms  1.19 11.16K  0.12M
 chk_7  0.97ms  0.81ms  0.96ms  1.20 11.16K  0.11M
   Avg  0.97  0.80  0.95
   Max  0.97  0.81  0.98
   Min  0.97  0.74  0.90
 Ratio  1.01  1.10  1.09
   Var  0.00  0.00  0.00
Profiling takes 0.352 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 31.877 ms
Partition 0 [0, 4) has cost: 26.882 ms
Partition 1 [4, 8) has cost: 25.502 ms
Partition 2 [8, 12) has cost: 25.502 ms
Partition 3 [12, 16) has cost: 25.502 ms
Partition 4 [16, 20) has cost: 25.502 ms
Partition 5 [20, 24) has cost: 25.502 ms
Partition 6 [24, 29) has cost: 31.877 ms
Partition 7 [29, 33) has cost: 26.728 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 26.406 ms
GPU 0, Compute+Comm Time: 12.998 ms, Bubble Time: 12.819 ms, Imbalance Overhead: 0.589 ms
GPU 1, Compute+Comm Time: 12.913 ms, Bubble Time: 12.617 ms, Imbalance Overhead: 0.876 ms
GPU 2, Compute+Comm Time: 12.913 ms, Bubble Time: 12.484 ms, Imbalance Overhead: 1.009 ms
GPU 3, Compute+Comm Time: 12.913 ms, Bubble Time: 12.240 ms, Imbalance Overhead: 1.254 ms
GPU 4, Compute+Comm Time: 12.913 ms, Bubble Time: 11.998 ms, Imbalance Overhead: 1.496 ms
GPU 5, Compute+Comm Time: 12.913 ms, Bubble Time: 11.715 ms, Imbalance Overhead: 1.778 ms
GPU 6, Compute+Comm Time: 14.940 ms, Bubble Time: 11.466 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 13.174 ms, Bubble Time: 11.696 ms, Imbalance Overhead: 1.536 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 47.057 ms
GPU 0, Compute+Comm Time: 23.164 ms, Bubble Time: 20.932 ms, Imbalance Overhead: 2.960 ms
GPU 1, Compute+Comm Time: 26.547 ms, Bubble Time: 20.509 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 22.199 ms, Bubble Time: 20.884 ms, Imbalance Overhead: 3.974 ms
GPU 3, Compute+Comm Time: 22.199 ms, Bubble Time: 21.296 ms, Imbalance Overhead: 3.562 ms
GPU 4, Compute+Comm Time: 22.199 ms, Bubble Time: 21.851 ms, Imbalance Overhead: 3.007 ms
GPU 5, Compute+Comm Time: 22.199 ms, Bubble Time: 22.290 ms, Imbalance Overhead: 2.568 ms
GPU 6, Compute+Comm Time: 22.199 ms, Bubble Time: 22.461 ms, Imbalance Overhead: 2.397 ms
GPU 7, Compute+Comm Time: 23.494 ms, Bubble Time: 22.753 ms, Imbalance Overhead: 0.809 ms
The estimated cost of the whole pipeline: 77.136 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 57.379 ms
Partition 0 [0, 8) has cost: 52.384 ms
Partition 1 [8, 16) has cost: 51.003 ms
Partition 2 [16, 25) has cost: 57.379 ms
Partition 3 [25, 33) has cost: 52.230 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.593 ms
GPU 0, Compute+Comm Time: 16.267 ms, Bubble Time: 13.293 ms, Imbalance Overhead: 1.032 ms
GPU 1, Compute+Comm Time: 16.695 ms, Bubble Time: 12.769 ms, Imbalance Overhead: 1.128 ms
GPU 2, Compute+Comm Time: 18.181 ms, Bubble Time: 12.411 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 16.828 ms, Bubble Time: 12.760 ms, Imbalance Overhead: 1.005 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.522 ms
GPU 0, Compute+Comm Time: 26.568 ms, Bubble Time: 20.310 ms, Imbalance Overhead: 1.644 ms
GPU 1, Compute+Comm Time: 28.745 ms, Bubble Time: 19.777 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 26.085 ms, Bubble Time: 20.336 ms, Imbalance Overhead: 2.101 ms
GPU 3, Compute+Comm Time: 26.262 ms, Bubble Time: 20.973 ms, Imbalance Overhead: 1.287 ms
    The estimated cost with 2 DP ways is 83.070 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 109.609 ms
Partition 0 [0, 16) has cost: 103.388 ms
Partition 1 [16, 33) has cost: 109.609 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.476 ms
GPU 0, Compute+Comm Time: 29.149 ms, Bubble Time: 15.888 ms, Imbalance Overhead: 1.438 ms
GPU 1, Compute+Comm Time: 31.865 ms, Bubble Time: 14.611 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.434 ms
GPU 0, Compute+Comm Time: 42.044 ms, Bubble Time: 19.390 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.871 ms, Bubble Time: 21.073 ms, Imbalance Overhead: 1.490 ms
    The estimated cost with 4 DP ways is 113.306 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 212.996 ms
Partition 0 [0, 33) has cost: 212.996 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 141.622 ms
GPU 0, Compute+Comm Time: 141.622 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 151.588 ms
GPU 0, Compute+Comm Time: 151.588 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 307.871 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.2980	TrainAcc 0.3028	ValidAcc 0.3053	TestAcc 0.3043	BestValid 0.3053
	Epoch 50:	Loss 1.6500	TrainAcc 0.4300	ValidAcc 0.4333	TestAcc 0.4334	BestValid 0.4333
	Epoch 100:	Loss 1.6325	TrainAcc 0.4530	ValidAcc 0.4542	TestAcc 0.4552	BestValid 0.4542
	Epoch 150:	Loss 1.6193	TrainAcc 0.4669	ValidAcc 0.4664	TestAcc 0.4679	BestValid 0.4664
	Epoch 200:	Loss 1.6061	TrainAcc 0.4755	ValidAcc 0.4753	TestAcc 0.4755	BestValid 0.4753
	Epoch 250:	Loss 1.5943	TrainAcc 0.4823	ValidAcc 0.4833	TestAcc 0.4833	BestValid 0.4833
	Epoch 300:	Loss 1.5826	TrainAcc 0.4854	ValidAcc 0.4867	TestAcc 0.4857	BestValid 0.4867
	Epoch 350:	Loss 1.5717	TrainAcc 0.4903	ValidAcc 0.4914	TestAcc 0.4886	BestValid 0.4914
	Epoch 400:	Loss 1.5642	TrainAcc 0.4919	ValidAcc 0.4922	TestAcc 0.4894	BestValid 0.4922
	Epoch 450:	Loss 1.5590	TrainAcc 0.4950	ValidAcc 0.4938	TestAcc 0.4931	BestValid 0.4938
	Epoch 500:	Loss 1.5488	TrainAcc 0.4963	ValidAcc 0.4945	TestAcc 0.4937	BestValid 0.4945
	Epoch 550:	Loss 1.5417	TrainAcc 0.4970	ValidAcc 0.4962	TestAcc 0.4953	BestValid 0.4962
	Epoch 600:	Loss 1.5350	TrainAcc 0.4963	ValidAcc 0.4969	TestAcc 0.4960	BestValid 0.4969
	Epoch 650:	Loss 1.5284	TrainAcc 0.4994	ValidAcc 0.4977	TestAcc 0.4972	BestValid 0.4977
	Epoch 700:	Loss 1.5226	TrainAcc 0.4993	ValidAcc 0.4966	TestAcc 0.4962	BestValid 0.4977
	Epoch 750:	Loss 1.5107	TrainAcc 0.5006	ValidAcc 0.4971	TestAcc 0.4970	BestValid 0.4977
	Epoch 800:	Loss 1.5051	TrainAcc 0.4961	ValidAcc 0.4918	TestAcc 0.4930	BestValid 0.4977
	Epoch 850:	Loss 1.5017	TrainAcc 0.5005	ValidAcc 0.4965	TestAcc 0.4965	BestValid 0.4977
	Epoch 900:	Loss 1.4970	TrainAcc 0.4990	ValidAcc 0.4945	TestAcc 0.4951	BestValid 0.4977
	Epoch 950:	Loss 1.4946	TrainAcc 0.5032	ValidAcc 0.4991	TestAcc 0.5006	BestValid 0.4991
	Epoch 1000:	Loss 1.4899	TrainAcc 0.4989	ValidAcc 0.4929	TestAcc 0.4946	BestValid 0.4991
	Epoch 1050:	Loss 1.4865	TrainAcc 0.5006	ValidAcc 0.4944	TestAcc 0.4961	BestValid 0.4991
	Epoch 1100:	Loss 1.4801	TrainAcc 0.5034	ValidAcc 0.4989	TestAcc 0.5008	BestValid 0.4991
	Epoch 1150:	Loss 1.4823	TrainAcc 0.5007	ValidAcc 0.4939	TestAcc 0.4942	BestValid 0.4991
	Epoch 1200:	Loss 1.4783	TrainAcc 0.5034	ValidAcc 0.4983	TestAcc 0.4994	BestValid 0.4991
	Epoch 1250:	Loss 1.4734	TrainAcc 0.5061	ValidAcc 0.5006	TestAcc 0.5034	BestValid 0.5006
	Epoch 1300:	Loss 1.4691	TrainAcc 0.5014	ValidAcc 0.4938	TestAcc 0.4941	BestValid 0.5006
	Epoch 1350:	Loss 1.4693	TrainAcc 0.4932	ValidAcc 0.4842	TestAcc 0.4851	BestValid 0.5006
	Epoch 1400:	Loss 1.4680	TrainAcc 0.5005	ValidAcc 0.4934	TestAcc 0.4925	BestValid 0.5006
	Epoch 1450:	Loss 1.4702	TrainAcc 0.5033	ValidAcc 0.4965	TestAcc 0.4972	BestValid 0.5006
	Epoch 1500:	Loss 1.4702	TrainAcc 0.5034	ValidAcc 0.4971	TestAcc 0.4985	BestValid 0.5006
	Epoch 1550:	Loss 1.4669	TrainAcc 0.5035	ValidAcc 0.4957	TestAcc 0.4965	BestValid 0.5006
	Epoch 1600:	Loss 1.4632	TrainAcc 0.5042	ValidAcc 0.4973	TestAcc 0.4970	BestValid 0.5006
	Epoch 1650:	Loss 1.4603	TrainAcc 0.5078	ValidAcc 0.5007	TestAcc 0.5030	BestValid 0.5007
	Epoch 1700:	Loss 1.4657	TrainAcc 0.5121	ValidAcc 0.5060	TestAcc 0.5059	BestValid 0.5060
	Epoch 1750:	Loss 1.4556	TrainAcc 0.5008	ValidAcc 0.4915	TestAcc 0.4924	BestValid 0.5060
	Epoch 1800:	Loss 1.4516	TrainAcc 0.5106	ValidAcc 0.5045	TestAcc 0.5045	BestValid 0.5060
	Epoch 1850:	Loss 1.4568	TrainAcc 0.5099	ValidAcc 0.5026	TestAcc 0.5031	BestValid 0.5060
	Epoch 1900:	Loss 1.4589	TrainAcc 0.5038	ValidAcc 0.4951	TestAcc 0.4968	BestValid 0.5060
	Epoch 1950:	Loss 1.4510	TrainAcc 0.5121	ValidAcc 0.5053	TestAcc 0.5063	BestValid 0.5060
	Epoch 2000:	Loss 1.4496	TrainAcc 0.5070	ValidAcc 0.4989	TestAcc 0.5008	BestValid 0.5060
	Epoch 2050:	Loss 1.4539	TrainAcc 0.5141	ValidAcc 0.5076	TestAcc 0.5071	BestValid 0.5076
	Epoch 2100:	Loss 1.4522	TrainAcc 0.5108	ValidAcc 0.5026	TestAcc 0.5048	BestValid 0.5076
	Epoch 2150:	Loss 1.4454	TrainAcc 0.5067	ValidAcc 0.4965	TestAcc 0.4994	BestValid 0.5076
	Epoch 2200:	Loss 1.4453	TrainAcc 0.5162	ValidAcc 0.5105	TestAcc 0.5094	BestValid 0.5105
	Epoch 2250:	Loss 1.4449	TrainAcc 0.5145	ValidAcc 0.5075	TestAcc 0.5075	BestValid 0.5105
	Epoch 2300:	Loss 1.4427	TrainAcc 0.5154	ValidAcc 0.5087	TestAcc 0.5093	BestValid 0.5105
	Epoch 2350:	Loss 1.4447	TrainAcc 0.5086	ValidAcc 0.4982	TestAcc 0.5000	BestValid 0.5105
	Epoch 2400:	Loss 1.4442	TrainAcc 0.5098	ValidAcc 0.4998	TestAcc 0.5038	BestValid 0.5105
	Epoch 2450:	Loss 1.4444	TrainAcc 0.5172	ValidAcc 0.5105	TestAcc 0.5106	BestValid 0.5105
	Epoch 2500:	Loss 1.4418	TrainAcc 0.5163	ValidAcc 0.5077	TestAcc 0.5082	BestValid 0.5105
	Epoch 2550:	Loss 1.4416	TrainAcc 0.5046	ValidAcc 0.4941	TestAcc 0.4956	BestValid 0.5105
	Epoch 2600:	Loss 1.4370	TrainAcc 0.5114	ValidAcc 0.5017	TestAcc 0.5047	BestValid 0.5105
	Epoch 2650:	Loss 1.4391	TrainAcc 0.5139	ValidAcc 0.5045	TestAcc 0.5071	BestValid 0.5105
	Epoch 2700:	Loss 1.4353	TrainAcc 0.5183	ValidAcc 0.5096	TestAcc 0.5110	BestValid 0.5105
	Epoch 2750:	Loss 1.4374	TrainAcc 0.5192	ValidAcc 0.5102	TestAcc 0.5108	BestValid 0.5105
	Epoch 2800:	Loss 1.4347	TrainAcc 0.5153	ValidAcc 0.5052	TestAcc 0.5073	BestValid 0.5105
	Epoch 2850:	Loss 1.4356	TrainAcc 0.5175	ValidAcc 0.5083	TestAcc 0.5084	BestValid 0.5105
	Epoch 2900:	Loss 1.4404	TrainAcc 0.5187	ValidAcc 0.5095	TestAcc 0.5113	BestValid 0.5105
	Epoch 2950:	Loss 1.4303	TrainAcc 0.5194	ValidAcc 0.5104	TestAcc 0.5103	BestValid 0.5105
	Epoch 3000:	Loss 1.4390	TrainAcc 0.5180	ValidAcc 0.5073	TestAcc 0.5086	BestValid 0.5105
	Epoch 3050:	Loss 1.4308	TrainAcc 0.5183	ValidAcc 0.5078	TestAcc 0.5086	BestValid 0.5105
	Epoch 3100:	Loss 1.4318	TrainAcc 0.5150	ValidAcc 0.5032	TestAcc 0.5060	BestValid 0.5105
	Epoch 3150:	Loss 1.4324	TrainAcc 0.5222	ValidAcc 0.5132	TestAcc 0.5135	BestValid 0.5132
	Epoch 3200:	Loss 1.4298	TrainAcc 0.5157	ValidAcc 0.5036	TestAcc 0.5062	BestValid 0.5132
	Epoch 3250:	Loss 1.4244	TrainAcc 0.5177	ValidAcc 0.5063	TestAcc 0.5089	BestValid 0.5132
	Epoch 3300:	Loss 1.4314	TrainAcc 0.5222	ValidAcc 0.5102	TestAcc 0.5121	BestValid 0.5132
	Epoch 3350:	Loss 1.4354	TrainAcc 0.5174	ValidAcc 0.5049	TestAcc 0.5080	BestValid 0.5132
	Epoch 3400:	Loss 1.4296	TrainAcc 0.5233	ValidAcc 0.5105	TestAcc 0.5124	BestValid 0.5132
	Epoch 3450:	Loss 1.4210	TrainAcc 0.5235	ValidAcc 0.5104	TestAcc 0.5132	BestValid 0.5132
	Epoch 3500:	Loss 1.4362	TrainAcc 0.5253	ValidAcc 0.5125	TestAcc 0.5136	BestValid 0.5132
	Epoch 3550:	Loss 1.4223	TrainAcc 0.5247	ValidAcc 0.5121	TestAcc 0.5127	BestValid 0.5132
	Epoch 3600:	Loss 1.4180	TrainAcc 0.5254	ValidAcc 0.5130	TestAcc 0.5131	BestValid 0.5132
	Epoch 3650:	Loss 1.4253	TrainAcc 0.5208	ValidAcc 0.5090	TestAcc 0.5112	BestValid 0.5132
	Epoch 3700:	Loss 1.4218	TrainAcc 0.5199	ValidAcc 0.5066	TestAcc 0.5097	BestValid 0.5132
	Epoch 3750:	Loss 1.4190	TrainAcc 0.5262	ValidAcc 0.5126	TestAcc 0.5140	BestValid 0.5132
	Epoch 3800:	Loss 1.4207	TrainAcc 0.5274	ValidAcc 0.5143	TestAcc 0.5150	BestValid 0.5143
	Epoch 3850:	Loss 1.4156	TrainAcc 0.5252	ValidAcc 0.5117	TestAcc 0.5129	BestValid 0.5143
	Epoch 3900:	Loss 1.4143	TrainAcc 0.5233	ValidAcc 0.5092	TestAcc 0.5117	BestValid 0.5143
	Epoch 3950:	Loss 1.4156	TrainAcc 0.5267	ValidAcc 0.5130	TestAcc 0.5145	BestValid 0.5143
	Epoch 4000:	Loss 1.4192	TrainAcc 0.5285	ValidAcc 0.5143	TestAcc 0.5145	BestValid 0.5143
	Epoch 4050:	Loss 1.4138	TrainAcc 0.5289	ValidAcc 0.5139	TestAcc 0.5149	BestValid 0.5143
	Epoch 4100:	Loss 1.4145	TrainAcc 0.5295	ValidAcc 0.5153	TestAcc 0.5157	BestValid 0.5153
	Epoch 4150:	Loss 1.4091	TrainAcc 0.5303	ValidAcc 0.5153	TestAcc 0.5150	BestValid 0.5153
	Epoch 4200:	Loss 1.4138	TrainAcc 0.5260	ValidAcc 0.5125	TestAcc 0.5137	BestValid 0.5153
	Epoch 4250:	Loss 1.4080	TrainAcc 0.5289	ValidAcc 0.5147	TestAcc 0.5152	BestValid 0.5153
	Epoch 4300:	Loss 1.4110	TrainAcc 0.5321	ValidAcc 0.5160	TestAcc 0.5160	BestValid 0.5160
	Epoch 4350:	Loss 1.4087	TrainAcc 0.5297	ValidAcc 0.5138	TestAcc 0.5156	BestValid 0.5160
	Epoch 4400:	Loss 1.4062	TrainAcc 0.5314	ValidAcc 0.5158	TestAcc 0.5159	BestValid 0.5160
	Epoch 4450:	Loss 1.4106	TrainAcc 0.5322	ValidAcc 0.5154	TestAcc 0.5164	BestValid 0.5160
	Epoch 4500:	Loss 1.4085	TrainAcc 0.5325	ValidAcc 0.5143	TestAcc 0.5156	BestValid 0.5160
	Epoch 4550:	Loss 1.4166	TrainAcc 0.5320	ValidAcc 0.5143	TestAcc 0.5160	BestValid 0.5160
	Epoch 4600:	Loss 1.4026	TrainAcc 0.5333	ValidAcc 0.5156	TestAcc 0.5168	BestValid 0.5160
	Epoch 4650:	Loss 1.4058	TrainAcc 0.5332	ValidAcc 0.5149	TestAcc 0.5164	BestValid 0.5160
	Epoch 4700:	Loss 1.4034	TrainAcc 0.5336	ValidAcc 0.5152	TestAcc 0.5168	BestValid 0.5160
	Epoch 4750:	Loss 1.4025	TrainAcc 0.5338	ValidAcc 0.5147	TestAcc 0.5164	BestValid 0.5160
	Epoch 4800:	Loss 1.4034	TrainAcc 0.5317	ValidAcc 0.5155	TestAcc 0.5159	BestValid 0.5160
	Epoch 4850:	Loss 1.3998	TrainAcc 0.5332	ValidAcc 0.5151	TestAcc 0.5165	BestValid 0.5160
	Epoch 4900:	Loss 1.4007	TrainAcc 0.5355	ValidAcc 0.5150	TestAcc 0.5167	BestValid 0.5160
	Epoch 4950:	Loss 1.4022	TrainAcc 0.5350	ValidAcc 0.5146	TestAcc 0.5164	BestValid 0.5160
	Epoch 5000:	Loss 1.4099	TrainAcc 0.5359	ValidAcc 0.5156	TestAcc 0.5169	BestValid 0.5160
****** Epoch Time (Excluding Evaluation Cost): 0.166 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.198 ms (Max: 0.270, Min: 0.039, Sum: 1.582)
Cluster-Wide Average, Compute: 33.036 ms (Max: 33.569, Min: 32.234, Sum: 264.285)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.013, Sum: 0.121)
Cluster-Wide Average, Communication-Graph: 123.924 ms (Max: 124.662, Min: 123.361, Sum: 991.390)
Cluster-Wide Average, Optimization: 3.277 ms (Max: 3.328, Min: 3.219, Sum: 26.213)
Cluster-Wide Average, Others: 5.615 ms (Max: 5.711, Min: 5.544, Sum: 44.923)
****** Breakdown Sum: 166.072 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.018 GB (Max: 7.514, Min: 6.934, Sum: 56.147)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.594 Gbps (Max: 51.450, Min: 16.885, Sum: 340.749)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 4.603 GB
****** Accuracy Results ******
Highest valid_acc: 0.5160
Target test_acc: 0.5160
Epoch to reach the target acc: 4299
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
