Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.018 seconds.
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
Building the Feature Vector...
        It takes 0.017 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.019 seconds.
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.006 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
csr in-out ready !Number of vertices per chunk: 11157
Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.951 Gbps (per GPU), 479.606 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.669 Gbps (per GPU), 477.355 Gbps (aggregated)
The layer-level communication performance: 59.655 Gbps (per GPU), 477.244 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.427 Gbps (per GPU), 475.418 Gbps (aggregated)
The layer-level communication performance: 59.387 Gbps (per GPU), 475.097 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.210 Gbps (per GPU), 473.678 Gbps (aggregated)
The layer-level communication performance: 59.164 Gbps (per GPU), 473.310 Gbps (aggregated)
The layer-level communication performance: 59.128 Gbps (per GPU), 473.026 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 154.069 Gbps (per GPU), 1232.554 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.075 Gbps (per GPU), 1232.599 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.075 Gbps (per GPU), 1232.599 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.066 Gbps (per GPU), 1232.531 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.072 Gbps (per GPU), 1232.578 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.064 Gbps (per GPU), 1232.509 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.072 Gbps (per GPU), 1232.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.064 Gbps (per GPU), 1232.509 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.340 Gbps (per GPU), 818.720 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.337 Gbps (per GPU), 818.694 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.331 Gbps (per GPU), 818.647 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.340 Gbps (per GPU), 818.720 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.340 Gbps (per GPU), 818.720 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.341 Gbps (per GPU), 818.727 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.305 Gbps (per GPU), 818.441 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.342 Gbps (per GPU), 818.740 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.332 Gbps (per GPU), 242.657 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.334 Gbps (per GPU), 242.675 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.332 Gbps (per GPU), 242.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.333 Gbps (per GPU), 242.662 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.330 Gbps (per GPU), 242.644 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.329 Gbps (per GPU), 242.633 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.330 Gbps (per GPU), 242.641 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.329 Gbps (per GPU), 242.631 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.99ms  0.80ms  0.96ms  1.23 11.16K  0.12M
 chk_1  0.99ms  0.81ms  0.97ms  1.22 11.16K  0.11M
 chk_2  0.99ms  0.78ms  0.93ms  1.28 11.16K  0.11M
 chk_3  0.97ms  0.74ms  0.90ms  1.31 11.16K  0.12M
 chk_4  0.97ms  0.81ms  0.97ms  1.20 11.16K  0.11M
 chk_5  0.96ms  0.79ms  0.95ms  1.21 11.16K  0.10M
 chk_6  0.96ms  0.80ms  0.96ms  1.20 11.16K  0.12M
 chk_7  0.96ms  0.81ms  0.96ms  1.20 11.16K  0.11M
   Avg  0.97  0.79  0.95
   Max  0.99  0.81  0.97
   Min  0.96  0.74  0.90
 Ratio  1.03  1.10  1.08
   Var  0.00  0.00  0.00
Profiling takes 0.329 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 31.686 ms
Partition 0 [0, 4) has cost: 26.811 ms
Partition 1 [4, 8) has cost: 25.349 ms
Partition 2 [8, 12) has cost: 25.349 ms
Partition 3 [12, 16) has cost: 25.349 ms
Partition 4 [16, 20) has cost: 25.349 ms
Partition 5 [20, 24) has cost: 25.349 ms
Partition 6 [24, 29) has cost: 31.686 ms
Partition 7 [29, 33) has cost: 26.602 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 26.260 ms
GPU 0, Compute+Comm Time: 12.914 ms, Bubble Time: 12.725 ms, Imbalance Overhead: 0.621 ms
GPU 1, Compute+Comm Time: 12.822 ms, Bubble Time: 12.522 ms, Imbalance Overhead: 0.916 ms
GPU 2, Compute+Comm Time: 12.822 ms, Bubble Time: 12.390 ms, Imbalance Overhead: 1.048 ms
GPU 3, Compute+Comm Time: 12.822 ms, Bubble Time: 12.139 ms, Imbalance Overhead: 1.299 ms
GPU 4, Compute+Comm Time: 12.822 ms, Bubble Time: 11.912 ms, Imbalance Overhead: 1.527 ms
GPU 5, Compute+Comm Time: 12.822 ms, Bubble Time: 11.672 ms, Imbalance Overhead: 1.766 ms
GPU 6, Compute+Comm Time: 14.836 ms, Bubble Time: 11.424 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 13.085 ms, Bubble Time: 11.650 ms, Imbalance Overhead: 1.525 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 46.850 ms
GPU 0, Compute+Comm Time: 23.045 ms, Bubble Time: 20.886 ms, Imbalance Overhead: 2.918 ms
GPU 1, Compute+Comm Time: 26.378 ms, Bubble Time: 20.472 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 22.055 ms, Bubble Time: 20.897 ms, Imbalance Overhead: 3.897 ms
GPU 3, Compute+Comm Time: 22.055 ms, Bubble Time: 21.267 ms, Imbalance Overhead: 3.528 ms
GPU 4, Compute+Comm Time: 22.055 ms, Bubble Time: 21.758 ms, Imbalance Overhead: 3.037 ms
GPU 5, Compute+Comm Time: 22.055 ms, Bubble Time: 22.191 ms, Imbalance Overhead: 2.604 ms
GPU 6, Compute+Comm Time: 22.055 ms, Bubble Time: 22.325 ms, Imbalance Overhead: 2.469 ms
GPU 7, Compute+Comm Time: 23.425 ms, Bubble Time: 22.601 ms, Imbalance Overhead: 0.824 ms
The estimated cost of the whole pipeline: 76.765 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 57.035 ms
Partition 0 [0, 8) has cost: 52.160 ms
Partition 1 [8, 16) has cost: 50.698 ms
Partition 2 [16, 25) has cost: 57.035 ms
Partition 3 [25, 33) has cost: 51.951 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.597 ms
GPU 0, Compute+Comm Time: 16.241 ms, Bubble Time: 13.262 ms, Imbalance Overhead: 1.094 ms
GPU 1, Compute+Comm Time: 16.674 ms, Bubble Time: 12.800 ms, Imbalance Overhead: 1.123 ms
GPU 2, Compute+Comm Time: 18.162 ms, Bubble Time: 12.435 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 16.805 ms, Bubble Time: 12.780 ms, Imbalance Overhead: 1.012 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.551 ms
GPU 0, Compute+Comm Time: 26.556 ms, Bubble Time: 20.369 ms, Imbalance Overhead: 1.625 ms
GPU 1, Compute+Comm Time: 28.722 ms, Bubble Time: 19.829 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 26.060 ms, Bubble Time: 20.413 ms, Imbalance Overhead: 2.078 ms
GPU 3, Compute+Comm Time: 26.289 ms, Bubble Time: 20.968 ms, Imbalance Overhead: 1.294 ms
    The estimated cost with 2 DP ways is 83.105 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 108.987 ms
Partition 0 [0, 16) has cost: 102.858 ms
Partition 1 [16, 33) has cost: 108.987 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 45.716 ms
GPU 0, Compute+Comm Time: 28.710 ms, Bubble Time: 15.694 ms, Imbalance Overhead: 1.311 ms
GPU 1, Compute+Comm Time: 31.370 ms, Bubble Time: 14.346 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 60.774 ms
GPU 0, Compute+Comm Time: 41.563 ms, Bubble Time: 19.212 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.471 ms, Bubble Time: 20.808 ms, Imbalance Overhead: 1.496 ms
    The estimated cost with 4 DP ways is 111.815 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 211.844 ms
Partition 0 [0, 33) has cost: 211.844 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 140.379 ms
GPU 0, Compute+Comm Time: 140.379 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 150.389 ms
GPU 0, Compute+Comm Time: 150.389 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 305.306 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.6279	TrainAcc 0.4606	ValidAcc 0.4605	TestAcc 0.4622	BestValid 0.4605
	Epoch 100:	Loss 1.6020	TrainAcc 0.4830	ValidAcc 0.4844	TestAcc 0.4805	BestValid 0.4844
	Epoch 150:	Loss 1.5796	TrainAcc 0.4857	ValidAcc 0.4862	TestAcc 0.4837	BestValid 0.4862
	Epoch 200:	Loss 1.5671	TrainAcc 0.4844	ValidAcc 0.4859	TestAcc 0.4850	BestValid 0.4862
	Epoch 250:	Loss 1.5486	TrainAcc 0.4843	ValidAcc 0.4846	TestAcc 0.4849	BestValid 0.4862
	Epoch 300:	Loss 1.5322	TrainAcc 0.4890	ValidAcc 0.4876	TestAcc 0.4877	BestValid 0.4876
	Epoch 350:	Loss 1.5198	TrainAcc 0.4848	ValidAcc 0.4811	TestAcc 0.4805	BestValid 0.4876
	Epoch 400:	Loss 1.5084	TrainAcc 0.4832	ValidAcc 0.4780	TestAcc 0.4772	BestValid 0.4876
	Epoch 450:	Loss 1.4986	TrainAcc 0.4771	ValidAcc 0.4712	TestAcc 0.4703	BestValid 0.4876
	Epoch 500:	Loss 1.4952	TrainAcc 0.4855	ValidAcc 0.4784	TestAcc 0.4801	BestValid 0.4876
	Epoch 550:	Loss 1.4885	TrainAcc 0.4724	ValidAcc 0.4638	TestAcc 0.4636	BestValid 0.4876
	Epoch 600:	Loss 1.4843	TrainAcc 0.4837	ValidAcc 0.4762	TestAcc 0.4775	BestValid 0.4876
	Epoch 650:	Loss 1.4803	TrainAcc 0.4844	ValidAcc 0.4777	TestAcc 0.4782	BestValid 0.4876
	Epoch 700:	Loss 1.4756	TrainAcc 0.4890	ValidAcc 0.4815	TestAcc 0.4835	BestValid 0.4876
	Epoch 750:	Loss 1.4722	TrainAcc 0.4869	ValidAcc 0.4796	TestAcc 0.4813	BestValid 0.4876
	Epoch 800:	Loss 1.4687	TrainAcc 0.4908	ValidAcc 0.4816	TestAcc 0.4841	BestValid 0.4876
	Epoch 850:	Loss 1.4665	TrainAcc 0.4926	ValidAcc 0.4833	TestAcc 0.4842	BestValid 0.4876
	Epoch 900:	Loss 1.4613	TrainAcc 0.4841	ValidAcc 0.4728	TestAcc 0.4751	BestValid 0.4876
	Epoch 950:	Loss 1.4644	TrainAcc 0.4917	ValidAcc 0.4824	TestAcc 0.4846	BestValid 0.4876
	Epoch 1000:	Loss 1.4625	TrainAcc 0.4974	ValidAcc 0.4878	TestAcc 0.4898	BestValid 0.4878
	Epoch 1050:	Loss 1.4603	TrainAcc 0.4863	ValidAcc 0.4758	TestAcc 0.4770	BestValid 0.4878
	Epoch 1100:	Loss 1.4645	TrainAcc 0.4882	ValidAcc 0.4763	TestAcc 0.4790	BestValid 0.4878
	Epoch 1150:	Loss 1.4523	TrainAcc 0.4949	ValidAcc 0.4849	TestAcc 0.4871	BestValid 0.4878
	Epoch 1200:	Loss 1.4590	TrainAcc 0.5051	ValidAcc 0.4949	TestAcc 0.4979	BestValid 0.4949
	Epoch 1250:	Loss 1.4554	TrainAcc 0.4998	ValidAcc 0.4900	TestAcc 0.4927	BestValid 0.4949
	Epoch 1300:	Loss 1.4492	TrainAcc 0.4981	ValidAcc 0.4861	TestAcc 0.4895	BestValid 0.4949
	Epoch 1350:	Loss 1.4553	TrainAcc 0.4807	ValidAcc 0.4688	TestAcc 0.4697	BestValid 0.4949
	Epoch 1400:	Loss 1.4510	TrainAcc 0.4784	ValidAcc 0.4670	TestAcc 0.4669	BestValid 0.4949
	Epoch 1450:	Loss 1.4473	TrainAcc 0.5017	ValidAcc 0.4898	TestAcc 0.4929	BestValid 0.4949
	Epoch 1500:	Loss 1.4487	TrainAcc 0.4928	ValidAcc 0.4810	TestAcc 0.4842	BestValid 0.4949
	Epoch 1550:	Loss 1.4465	TrainAcc 0.5019	ValidAcc 0.4889	TestAcc 0.4931	BestValid 0.4949
	Epoch 1600:	Loss 1.4483	TrainAcc 0.4917	ValidAcc 0.4795	TestAcc 0.4831	BestValid 0.4949
	Epoch 1650:	Loss 1.4429	TrainAcc 0.5024	ValidAcc 0.4895	TestAcc 0.4929	BestValid 0.4949
	Epoch 1700:	Loss 1.4418	TrainAcc 0.5061	ValidAcc 0.4937	TestAcc 0.4968	BestValid 0.4949
	Epoch 1750:	Loss 1.4410	TrainAcc 0.4995	ValidAcc 0.4872	TestAcc 0.4906	BestValid 0.4949
	Epoch 1800:	Loss 1.4425	TrainAcc 0.5036	ValidAcc 0.4908	TestAcc 0.4940	BestValid 0.4949
	Epoch 1850:	Loss 1.4386	TrainAcc 0.5078	ValidAcc 0.4965	TestAcc 0.4991	BestValid 0.4965
	Epoch 1900:	Loss 1.4355	TrainAcc 0.5069	ValidAcc 0.4944	TestAcc 0.4982	BestValid 0.4965
	Epoch 1950:	Loss 1.4381	TrainAcc 0.5097	ValidAcc 0.4978	TestAcc 0.4998	BestValid 0.4978
	Epoch 2000:	Loss 1.4389	TrainAcc 0.4868	ValidAcc 0.4747	TestAcc 0.4747	BestValid 0.4978
	Epoch 2050:	Loss 1.4317	TrainAcc 0.4976	ValidAcc 0.4843	TestAcc 0.4873	BestValid 0.4978
	Epoch 2100:	Loss 1.4373	TrainAcc 0.5094	ValidAcc 0.4979	TestAcc 0.4993	BestValid 0.4979
	Epoch 2150:	Loss 1.4327	TrainAcc 0.5092	ValidAcc 0.4981	TestAcc 0.4997	BestValid 0.4981
	Epoch 2200:	Loss 1.4330	TrainAcc 0.5070	ValidAcc 0.4943	TestAcc 0.4961	BestValid 0.4981
	Epoch 2250:	Loss 1.4299	TrainAcc 0.5074	ValidAcc 0.4927	TestAcc 0.4952	BestValid 0.4981
	Epoch 2300:	Loss 1.4359	TrainAcc 0.5070	ValidAcc 0.4923	TestAcc 0.4952	BestValid 0.4981
	Epoch 2350:	Loss 1.4325	TrainAcc 0.5060	ValidAcc 0.4917	TestAcc 0.4939	BestValid 0.4981
	Epoch 2400:	Loss 1.4289	TrainAcc 0.5081	ValidAcc 0.4937	TestAcc 0.4957	BestValid 0.4981
	Epoch 2450:	Loss 1.4247	TrainAcc 0.5134	ValidAcc 0.5014	TestAcc 0.5027	BestValid 0.5014
	Epoch 2500:	Loss 1.4247	TrainAcc 0.5124	ValidAcc 0.4991	TestAcc 0.5005	BestValid 0.5014
	Epoch 2550:	Loss 1.4284	TrainAcc 0.5082	ValidAcc 0.4940	TestAcc 0.4960	BestValid 0.5014
	Epoch 2600:	Loss 1.4241	TrainAcc 0.5124	ValidAcc 0.4983	TestAcc 0.5000	BestValid 0.5014
	Epoch 2650:	Loss 1.4239	TrainAcc 0.5085	ValidAcc 0.4938	TestAcc 0.4957	BestValid 0.5014
	Epoch 2700:	Loss 1.4297	TrainAcc 0.5122	ValidAcc 0.4980	TestAcc 0.4989	BestValid 0.5014
	Epoch 2750:	Loss 1.4214	TrainAcc 0.5131	ValidAcc 0.4990	TestAcc 0.5004	BestValid 0.5014
	Epoch 2800:	Loss 1.4187	TrainAcc 0.5147	ValidAcc 0.5004	TestAcc 0.5013	BestValid 0.5014
	Epoch 2850:	Loss 1.4217	TrainAcc 0.5090	ValidAcc 0.4940	TestAcc 0.4956	BestValid 0.5014
	Epoch 2900:	Loss 1.4230	TrainAcc 0.5131	ValidAcc 0.4983	TestAcc 0.4991	BestValid 0.5014
	Epoch 2950:	Loss 1.4220	TrainAcc 0.5069	ValidAcc 0.4913	TestAcc 0.4944	BestValid 0.5014
	Epoch 3000:	Loss 1.4206	TrainAcc 0.5141	ValidAcc 0.4997	TestAcc 0.5002	BestValid 0.5014
	Epoch 3050:	Loss 1.4184	TrainAcc 0.5183	ValidAcc 0.5047	TestAcc 0.5057	BestValid 0.5047
	Epoch 3100:	Loss 1.4194	TrainAcc 0.5171	ValidAcc 0.5025	TestAcc 0.5027	BestValid 0.5047
	Epoch 3150:	Loss 1.4180	TrainAcc 0.5115	ValidAcc 0.4965	TestAcc 0.4976	BestValid 0.5047
	Epoch 3200:	Loss 1.4191	TrainAcc 0.4999	ValidAcc 0.4842	TestAcc 0.4859	BestValid 0.5047
	Epoch 3250:	Loss 1.4202	TrainAcc 0.5198	ValidAcc 0.5054	TestAcc 0.5065	BestValid 0.5054
	Epoch 3300:	Loss 1.4167	TrainAcc 0.5202	ValidAcc 0.5059	TestAcc 0.5058	BestValid 0.5059
	Epoch 3350:	Loss 1.4148	TrainAcc 0.5214	ValidAcc 0.5068	TestAcc 0.5067	BestValid 0.5068
	Epoch 3400:	Loss 1.4161	TrainAcc 0.5219	ValidAcc 0.5063	TestAcc 0.5070	BestValid 0.5068
	Epoch 3450:	Loss 1.4152	TrainAcc 0.5191	ValidAcc 0.5048	TestAcc 0.5048	BestValid 0.5068
	Epoch 3500:	Loss 1.4136	TrainAcc 0.5177	ValidAcc 0.5035	TestAcc 0.5038	BestValid 0.5068
	Epoch 3550:	Loss 1.4140	TrainAcc 0.5224	ValidAcc 0.5078	TestAcc 0.5075	BestValid 0.5078
	Epoch 3600:	Loss 1.4148	TrainAcc 0.5249	ValidAcc 0.5092	TestAcc 0.5089	BestValid 0.5092
	Epoch 3650:	Loss 1.4136	TrainAcc 0.5229	ValidAcc 0.5065	TestAcc 0.5067	BestValid 0.5092
	Epoch 3700:	Loss 1.4087	TrainAcc 0.5215	ValidAcc 0.5056	TestAcc 0.5070	BestValid 0.5092
	Epoch 3750:	Loss 1.4083	TrainAcc 0.5213	ValidAcc 0.5060	TestAcc 0.5066	BestValid 0.5092
	Epoch 3800:	Loss 1.4081	TrainAcc 0.5263	ValidAcc 0.5103	TestAcc 0.5096	BestValid 0.5103
	Epoch 3850:	Loss 1.4076	TrainAcc 0.5218	ValidAcc 0.5054	TestAcc 0.5061	BestValid 0.5103
	Epoch 3900:	Loss 1.4095	TrainAcc 0.5168	ValidAcc 0.4985	TestAcc 0.4987	BestValid 0.5103
	Epoch 3950:	Loss 1.4054	TrainAcc 0.5244	ValidAcc 0.5077	TestAcc 0.5083	BestValid 0.5103
	Epoch 4000:	Loss 1.4042	TrainAcc 0.5265	ValidAcc 0.5104	TestAcc 0.5097	BestValid 0.5104
	Epoch 4050:	Loss 1.4030	TrainAcc 0.5182	ValidAcc 0.4996	TestAcc 0.5002	BestValid 0.5104
	Epoch 4100:	Loss 1.4023	TrainAcc 0.5259	ValidAcc 0.5093	TestAcc 0.5089	BestValid 0.5104
	Epoch 4150:	Loss 1.4009	TrainAcc 0.5226	ValidAcc 0.5062	TestAcc 0.5062	BestValid 0.5104
	Epoch 4200:	Loss 1.4031	TrainAcc 0.5299	ValidAcc 0.5120	TestAcc 0.5111	BestValid 0.5120
	Epoch 4250:	Loss 1.4028	TrainAcc 0.5271	ValidAcc 0.5099	TestAcc 0.5089	BestValid 0.5120
	Epoch 4300:	Loss 1.4034	TrainAcc 0.5244	ValidAcc 0.5070	TestAcc 0.5066	BestValid 0.5120
	Epoch 4350:	Loss 1.4004	TrainAcc 0.5324	ValidAcc 0.5141	TestAcc 0.5122	BestValid 0.5141
	Epoch 4400:	Loss 1.3982	TrainAcc 0.5295	ValidAcc 0.5110	TestAcc 0.5100	BestValid 0.5141
	Epoch 4450:	Loss 1.4027	TrainAcc 0.5292	ValidAcc 0.5103	TestAcc 0.5098	BestValid 0.5141
	Epoch 4500:	Loss 1.3985	TrainAcc 0.5323	ValidAcc 0.5131	TestAcc 0.5130	BestValid 0.5141
	Epoch 4550:	Loss 1.3971	TrainAcc 0.5276	ValidAcc 0.5090	TestAcc 0.5077	BestValid 0.5141
	Epoch 4600:	Loss 1.3977	TrainAcc 0.5336	ValidAcc 0.5140	TestAcc 0.5117	BestValid 0.5141
	Epoch 4650:	Loss 1.4009	TrainAcc 0.5309	ValidAcc 0.5110	TestAcc 0.5102	BestValid 0.5141
	Epoch 4700:	Loss 1.3980	TrainAcc 0.5339	ValidAcc 0.5142	TestAcc 0.5123	BestValid 0.5142
	Epoch 4750:	Loss 1.3945	TrainAcc 0.5306	ValidAcc 0.5104	TestAcc 0.5106	BestValid 0.5142
	Epoch 4800:	Loss 1.3943	TrainAcc 0.5325	ValidAcc 0.5133	TestAcc 0.5110	BestValid 0.5142
	Epoch 4850:	Loss 1.3928	TrainAcc 0.5343	ValidAcc 0.5138	TestAcc 0.5130	BestValid 0.5142
	Epoch 4900:	Loss 1.3904	TrainAcc 0.5342	ValidAcc 0.5137	TestAcc 0.5128	BestValid 0.5142
	Epoch 4950:	Loss 1.3949	TrainAcc 0.5335	ValidAcc 0.5125	TestAcc 0.5110	BestValid 0.5142
	Epoch 5000:	Loss 1.3926	TrainAcc 0.5367	ValidAcc 0.5159	TestAcc 0.5143	BestValid 0.5159
****** Epoch Time (Excluding Evaluation Cost): 0.166 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.167 ms (Max: 0.218, Min: 0.038, Sum: 1.339)
Cluster-Wide Average, Compute: 32.914 ms (Max: 33.556, Min: 31.698, Sum: 263.316)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.060)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.015, Min: 0.014, Sum: 0.118)
Cluster-Wide Average, Communication-Graph: 123.724 ms (Max: 124.907, Min: 123.036, Sum: 989.794)
Cluster-Wide Average, Optimization: 3.270 ms (Max: 3.328, Min: 3.210, Sum: 26.158)
Cluster-Wide Average, Others: 5.604 ms (Max: 5.666, Min: 5.538, Sum: 44.835)
****** Breakdown Sum: 165.703 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.018 GB (Max: 7.514, Min: 6.934, Sum: 56.147)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.683 Gbps (Max: 51.610, Min: 16.829, Sum: 341.464)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 4.603 GB
****** Accuracy Results ******
Highest valid_acc: 0.5159
Target test_acc: 0.5143
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
