Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.033 seconds.
Building the CSC structure...
Building the Feature Vector...
        It takes 0.019 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Feature Vector...
        It takes 0.024 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.105 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Building the Label Vector...
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.132 Gbps (per GPU), 481.060 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.829 Gbps (per GPU), 478.635 Gbps (aggregated)
The layer-level communication performance: 59.816 Gbps (per GPU), 478.525 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.588 Gbps (per GPU), 476.704 Gbps (aggregated)
The layer-level communication performance: 59.546 Gbps (per GPU), 476.367 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.341 Gbps (per GPU), 474.730 Gbps (aggregated)
The layer-level communication performance: 59.296 Gbps (per GPU), 474.366 Gbps (aggregated)
The layer-level communication performance: 59.265 Gbps (per GPU), 474.123 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.535 Gbps (per GPU), 1244.278 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.535 Gbps (per GPU), 1244.278 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.457 Gbps (per GPU), 1243.655 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.529 Gbps (per GPU), 1244.231 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.544 Gbps (per GPU), 1244.349 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.520 Gbps (per GPU), 1244.162 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.446 Gbps (per GPU), 1243.564 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.521 Gbps (per GPU), 1244.164 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.168 Gbps (per GPU), 801.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.165 Gbps (per GPU), 801.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.171 Gbps (per GPU), 801.370 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.169 Gbps (per GPU), 801.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.172 Gbps (per GPU), 801.375 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.170 Gbps (per GPU), 801.357 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.171 Gbps (per GPU), 801.370 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.169 Gbps (per GPU), 801.352 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.011 Gbps (per GPU), 280.089 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.014 Gbps (per GPU), 280.113 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.012 Gbps (per GPU), 280.095 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.011 Gbps (per GPU), 280.086 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.012 Gbps (per GPU), 280.098 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.012 Gbps (per GPU), 280.095 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.006 Gbps (per GPU), 280.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.012 Gbps (per GPU), 280.094 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.00ms  0.81ms  0.97ms  1.23 11.16K  0.12M
 chk_1  1.00ms  0.82ms  0.98ms  1.22 11.16K  0.11M
 chk_2  0.97ms  0.79ms  0.95ms  1.24 11.16K  0.11M
 chk_3  0.97ms  0.75ms  0.91ms  1.30 11.16K  0.12M
 chk_4  0.97ms  0.82ms  0.98ms  1.20 11.16K  0.11M
 chk_5  0.97ms  0.81ms  0.97ms  1.20 11.16K  0.10M
 chk_6  0.97ms  0.81ms  0.97ms  1.20 11.16K  0.12M
 chk_7  0.97ms  0.82ms  0.98ms  1.20 11.16K  0.11M
   Avg  0.98  0.80  0.96
   Max  1.00  0.82  0.98
   Min  0.97  0.75  0.91
 Ratio  1.03  1.09  1.08
   Var  0.00  0.00  0.00
Profiling takes 0.334 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 32.068 ms
Partition 0 [0, 4) has cost: 27.068 ms
Partition 1 [4, 8) has cost: 25.654 ms
Partition 2 [8, 12) has cost: 25.654 ms
Partition 3 [12, 16) has cost: 25.654 ms
Partition 4 [16, 20) has cost: 25.654 ms
Partition 5 [20, 24) has cost: 25.654 ms
Partition 6 [24, 29) has cost: 32.068 ms
Partition 7 [29, 33) has cost: 26.945 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 26.433 ms
GPU 0, Compute+Comm Time: 12.995 ms, Bubble Time: 12.825 ms, Imbalance Overhead: 0.613 ms
GPU 1, Compute+Comm Time: 12.904 ms, Bubble Time: 12.614 ms, Imbalance Overhead: 0.915 ms
GPU 2, Compute+Comm Time: 12.904 ms, Bubble Time: 12.475 ms, Imbalance Overhead: 1.054 ms
GPU 3, Compute+Comm Time: 12.904 ms, Bubble Time: 12.215 ms, Imbalance Overhead: 1.314 ms
GPU 4, Compute+Comm Time: 12.904 ms, Bubble Time: 11.982 ms, Imbalance Overhead: 1.547 ms
GPU 5, Compute+Comm Time: 12.904 ms, Bubble Time: 11.745 ms, Imbalance Overhead: 1.784 ms
GPU 6, Compute+Comm Time: 14.942 ms, Bubble Time: 11.491 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 13.178 ms, Bubble Time: 11.711 ms, Imbalance Overhead: 1.544 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 47.222 ms
GPU 0, Compute+Comm Time: 23.266 ms, Bubble Time: 21.009 ms, Imbalance Overhead: 2.947 ms
GPU 1, Compute+Comm Time: 26.625 ms, Bubble Time: 20.597 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 22.250 ms, Bubble Time: 21.023 ms, Imbalance Overhead: 3.949 ms
GPU 3, Compute+Comm Time: 22.250 ms, Bubble Time: 21.399 ms, Imbalance Overhead: 3.573 ms
GPU 4, Compute+Comm Time: 22.250 ms, Bubble Time: 21.912 ms, Imbalance Overhead: 3.060 ms
GPU 5, Compute+Comm Time: 22.250 ms, Bubble Time: 22.385 ms, Imbalance Overhead: 2.587 ms
GPU 6, Compute+Comm Time: 22.250 ms, Bubble Time: 22.538 ms, Imbalance Overhead: 2.435 ms
GPU 7, Compute+Comm Time: 23.572 ms, Bubble Time: 22.828 ms, Imbalance Overhead: 0.822 ms
The estimated cost of the whole pipeline: 77.337 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 57.722 ms
Partition 0 [0, 8) has cost: 52.722 ms
Partition 1 [8, 16) has cost: 51.309 ms
Partition 2 [16, 25) has cost: 57.722 ms
Partition 3 [25, 33) has cost: 52.599 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.701 ms
GPU 0, Compute+Comm Time: 16.290 ms, Bubble Time: 13.301 ms, Imbalance Overhead: 1.110 ms
GPU 1, Compute+Comm Time: 16.721 ms, Bubble Time: 12.849 ms, Imbalance Overhead: 1.131 ms
GPU 2, Compute+Comm Time: 18.217 ms, Bubble Time: 12.484 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 16.858 ms, Bubble Time: 12.834 ms, Imbalance Overhead: 1.010 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.833 ms
GPU 0, Compute+Comm Time: 26.719 ms, Bubble Time: 20.495 ms, Imbalance Overhead: 1.619 ms
GPU 1, Compute+Comm Time: 28.893 ms, Bubble Time: 19.940 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 26.210 ms, Bubble Time: 20.538 ms, Imbalance Overhead: 2.085 ms
GPU 3, Compute+Comm Time: 26.403 ms, Bubble Time: 21.093 ms, Imbalance Overhead: 1.337 ms
    The estimated cost with 2 DP ways is 83.511 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 110.321 ms
Partition 0 [0, 16) has cost: 104.031 ms
Partition 1 [16, 33) has cost: 110.321 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.420 ms
GPU 0, Compute+Comm Time: 29.138 ms, Bubble Time: 15.927 ms, Imbalance Overhead: 1.354 ms
GPU 1, Compute+Comm Time: 31.852 ms, Bubble Time: 14.568 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.640 ms
GPU 0, Compute+Comm Time: 42.150 ms, Bubble Time: 19.490 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.990 ms, Bubble Time: 21.083 ms, Imbalance Overhead: 1.568 ms
    The estimated cost with 4 DP ways is 113.463 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 214.352 ms
Partition 0 [0, 33) has cost: 214.352 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 122.868 ms
GPU 0, Compute+Comm Time: 122.868 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 132.955 ms
GPU 0, Compute+Comm Time: 132.955 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 268.613 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5269	TrainAcc 0.4231	ValidAcc 0.4269	TestAcc 0.4274	BestValid 0.4269
	Epoch 50:	Loss 1.6279	TrainAcc 0.4606	ValidAcc 0.4605	TestAcc 0.4622	BestValid 0.4605
	Epoch 100:	Loss 1.6020	TrainAcc 0.4829	ValidAcc 0.4844	TestAcc 0.4805	BestValid 0.4844
	Epoch 150:	Loss 1.5796	TrainAcc 0.4856	ValidAcc 0.4862	TestAcc 0.4836	BestValid 0.4862
	Epoch 200:	Loss 1.5672	TrainAcc 0.4843	ValidAcc 0.4860	TestAcc 0.4851	BestValid 0.4862
	Epoch 250:	Loss 1.5486	TrainAcc 0.4843	ValidAcc 0.4846	TestAcc 0.4849	BestValid 0.4862
	Epoch 300:	Loss 1.5322	TrainAcc 0.4889	ValidAcc 0.4876	TestAcc 0.4877	BestValid 0.4876
	Epoch 350:	Loss 1.5198	TrainAcc 0.4848	ValidAcc 0.4811	TestAcc 0.4807	BestValid 0.4876
	Epoch 400:	Loss 1.5084	TrainAcc 0.4832	ValidAcc 0.4781	TestAcc 0.4772	BestValid 0.4876
	Epoch 450:	Loss 1.4987	TrainAcc 0.4770	ValidAcc 0.4711	TestAcc 0.4703	BestValid 0.4876
	Epoch 500:	Loss 1.4952	TrainAcc 0.4855	ValidAcc 0.4784	TestAcc 0.4801	BestValid 0.4876
	Epoch 550:	Loss 1.4885	TrainAcc 0.4725	ValidAcc 0.4640	TestAcc 0.4636	BestValid 0.4876
	Epoch 600:	Loss 1.4843	TrainAcc 0.4837	ValidAcc 0.4764	TestAcc 0.4775	BestValid 0.4876
	Epoch 650:	Loss 1.4803	TrainAcc 0.4843	ValidAcc 0.4775	TestAcc 0.4783	BestValid 0.4876
	Epoch 700:	Loss 1.4756	TrainAcc 0.4892	ValidAcc 0.4816	TestAcc 0.4836	BestValid 0.4876
	Epoch 750:	Loss 1.4722	TrainAcc 0.4870	ValidAcc 0.4795	TestAcc 0.4813	BestValid 0.4876
	Epoch 800:	Loss 1.4687	TrainAcc 0.4908	ValidAcc 0.4815	TestAcc 0.4842	BestValid 0.4876
	Epoch 850:	Loss 1.4665	TrainAcc 0.4926	ValidAcc 0.4834	TestAcc 0.4841	BestValid 0.4876
	Epoch 900:	Loss 1.4613	TrainAcc 0.4841	ValidAcc 0.4729	TestAcc 0.4751	BestValid 0.4876
	Epoch 950:	Loss 1.4644	TrainAcc 0.4918	ValidAcc 0.4824	TestAcc 0.4846	BestValid 0.4876
	Epoch 1000:	Loss 1.4625	TrainAcc 0.4974	ValidAcc 0.4877	TestAcc 0.4900	BestValid 0.4877
	Epoch 1050:	Loss 1.4603	TrainAcc 0.4863	ValidAcc 0.4758	TestAcc 0.4769	BestValid 0.4877
	Epoch 1100:	Loss 1.4645	TrainAcc 0.4881	ValidAcc 0.4764	TestAcc 0.4790	BestValid 0.4877
	Epoch 1150:	Loss 1.4523	TrainAcc 0.4950	ValidAcc 0.4850	TestAcc 0.4870	BestValid 0.4877
	Epoch 1200:	Loss 1.4590	TrainAcc 0.5051	ValidAcc 0.4948	TestAcc 0.4981	BestValid 0.4948
	Epoch 1250:	Loss 1.4554	TrainAcc 0.4998	ValidAcc 0.4899	TestAcc 0.4926	BestValid 0.4948
	Epoch 1300:	Loss 1.4492	TrainAcc 0.4981	ValidAcc 0.4861	TestAcc 0.4894	BestValid 0.4948
	Epoch 1350:	Loss 1.4553	TrainAcc 0.4807	ValidAcc 0.4688	TestAcc 0.4697	BestValid 0.4948
	Epoch 1400:	Loss 1.4510	TrainAcc 0.4784	ValidAcc 0.4671	TestAcc 0.4669	BestValid 0.4948
	Epoch 1450:	Loss 1.4473	TrainAcc 0.5016	ValidAcc 0.4896	TestAcc 0.4931	BestValid 0.4948
	Epoch 1500:	Loss 1.4487	TrainAcc 0.4927	ValidAcc 0.4809	TestAcc 0.4841	BestValid 0.4948
	Epoch 1550:	Loss 1.4465	TrainAcc 0.5020	ValidAcc 0.4890	TestAcc 0.4929	BestValid 0.4948
	Epoch 1600:	Loss 1.4483	TrainAcc 0.4915	ValidAcc 0.4794	TestAcc 0.4831	BestValid 0.4948
	Epoch 1650:	Loss 1.4429	TrainAcc 0.5024	ValidAcc 0.4895	TestAcc 0.4929	BestValid 0.4948
	Epoch 1700:	Loss 1.4419	TrainAcc 0.5060	ValidAcc 0.4936	TestAcc 0.4968	BestValid 0.4948
	Epoch 1750:	Loss 1.4410	TrainAcc 0.4995	ValidAcc 0.4873	TestAcc 0.4905	BestValid 0.4948
	Epoch 1800:	Loss 1.4424	TrainAcc 0.5036	ValidAcc 0.4909	TestAcc 0.4942	BestValid 0.4948
	Epoch 1850:	Loss 1.4386	TrainAcc 0.5078	ValidAcc 0.4966	TestAcc 0.4991	BestValid 0.4966
	Epoch 1900:	Loss 1.4356	TrainAcc 0.5070	ValidAcc 0.4943	TestAcc 0.4981	BestValid 0.4966
	Epoch 1950:	Loss 1.4381	TrainAcc 0.5099	ValidAcc 0.4979	TestAcc 0.4998	BestValid 0.4979
	Epoch 2000:	Loss 1.4389	TrainAcc 0.4868	ValidAcc 0.4748	TestAcc 0.4746	BestValid 0.4979
	Epoch 2050:	Loss 1.4317	TrainAcc 0.4976	ValidAcc 0.4842	TestAcc 0.4874	BestValid 0.4979
	Epoch 2100:	Loss 1.4372	TrainAcc 0.5094	ValidAcc 0.4979	TestAcc 0.4994	BestValid 0.4979
	Epoch 2150:	Loss 1.4327	TrainAcc 0.5092	ValidAcc 0.4979	TestAcc 0.4996	BestValid 0.4979
	Epoch 2200:	Loss 1.4330	TrainAcc 0.5070	ValidAcc 0.4941	TestAcc 0.4960	BestValid 0.4979
	Epoch 2250:	Loss 1.4300	TrainAcc 0.5074	ValidAcc 0.4925	TestAcc 0.4951	BestValid 0.4979
	Epoch 2300:	Loss 1.4359	TrainAcc 0.5069	ValidAcc 0.4924	TestAcc 0.4952	BestValid 0.4979
	Epoch 2350:	Loss 1.4325	TrainAcc 0.5060	ValidAcc 0.4916	TestAcc 0.4939	BestValid 0.4979
	Epoch 2400:	Loss 1.4289	TrainAcc 0.5081	ValidAcc 0.4936	TestAcc 0.4956	BestValid 0.4979
	Epoch 2450:	Loss 1.4247	TrainAcc 0.5133	ValidAcc 0.5013	TestAcc 0.5028	BestValid 0.5013
	Epoch 2500:	Loss 1.4247	TrainAcc 0.5124	ValidAcc 0.4991	TestAcc 0.5004	BestValid 0.5013
	Epoch 2550:	Loss 1.4284	TrainAcc 0.5082	ValidAcc 0.4940	TestAcc 0.4960	BestValid 0.5013
	Epoch 2600:	Loss 1.4241	TrainAcc 0.5125	ValidAcc 0.4983	TestAcc 0.5001	BestValid 0.5013
	Epoch 2650:	Loss 1.4239	TrainAcc 0.5085	ValidAcc 0.4939	TestAcc 0.4957	BestValid 0.5013
	Epoch 2700:	Loss 1.4297	TrainAcc 0.5121	ValidAcc 0.4981	TestAcc 0.4989	BestValid 0.5013
	Epoch 2750:	Loss 1.4214	TrainAcc 0.5130	ValidAcc 0.4991	TestAcc 0.5006	BestValid 0.5013
	Epoch 2800:	Loss 1.4187	TrainAcc 0.5147	ValidAcc 0.5004	TestAcc 0.5014	BestValid 0.5013
	Epoch 2850:	Loss 1.4217	TrainAcc 0.5091	ValidAcc 0.4942	TestAcc 0.4957	BestValid 0.5013
	Epoch 2900:	Loss 1.4230	TrainAcc 0.5132	ValidAcc 0.4984	TestAcc 0.4992	BestValid 0.5013
	Epoch 2950:	Loss 1.4219	TrainAcc 0.5069	ValidAcc 0.4911	TestAcc 0.4943	BestValid 0.5013
	Epoch 3000:	Loss 1.4206	TrainAcc 0.5141	ValidAcc 0.4998	TestAcc 0.5003	BestValid 0.5013
	Epoch 3050:	Loss 1.4184	TrainAcc 0.5184	ValidAcc 0.5047	TestAcc 0.5058	BestValid 0.5047
	Epoch 3100:	Loss 1.4195	TrainAcc 0.5170	ValidAcc 0.5025	TestAcc 0.5028	BestValid 0.5047
	Epoch 3150:	Loss 1.4180	TrainAcc 0.5114	ValidAcc 0.4965	TestAcc 0.4975	BestValid 0.5047
	Epoch 3200:	Loss 1.4191	TrainAcc 0.5001	ValidAcc 0.4845	TestAcc 0.4859	BestValid 0.5047
	Epoch 3250:	Loss 1.4202	TrainAcc 0.5198	ValidAcc 0.5056	TestAcc 0.5062	BestValid 0.5056
	Epoch 3300:	Loss 1.4167	TrainAcc 0.5203	ValidAcc 0.5061	TestAcc 0.5058	BestValid 0.5061
	Epoch 3350:	Loss 1.4148	TrainAcc 0.5216	ValidAcc 0.5067	TestAcc 0.5067	BestValid 0.5067
	Epoch 3400:	Loss 1.4161	TrainAcc 0.5220	ValidAcc 0.5063	TestAcc 0.5071	BestValid 0.5067
	Epoch 3450:	Loss 1.4151	TrainAcc 0.5190	ValidAcc 0.5049	TestAcc 0.5045	BestValid 0.5067
	Epoch 3500:	Loss 1.4136	TrainAcc 0.5177	ValidAcc 0.5034	TestAcc 0.5040	BestValid 0.5067
	Epoch 3550:	Loss 1.4140	TrainAcc 0.5223	ValidAcc 0.5078	TestAcc 0.5075	BestValid 0.5078
	Epoch 3600:	Loss 1.4148	TrainAcc 0.5249	ValidAcc 0.5092	TestAcc 0.5089	BestValid 0.5092
	Epoch 3650:	Loss 1.4136	TrainAcc 0.5230	ValidAcc 0.5064	TestAcc 0.5066	BestValid 0.5092
	Epoch 3700:	Loss 1.4087	TrainAcc 0.5213	ValidAcc 0.5056	TestAcc 0.5071	BestValid 0.5092
	Epoch 3750:	Loss 1.4083	TrainAcc 0.5213	ValidAcc 0.5060	TestAcc 0.5068	BestValid 0.5092
	Epoch 3800:	Loss 1.4081	TrainAcc 0.5263	ValidAcc 0.5103	TestAcc 0.5099	BestValid 0.5103
	Epoch 3850:	Loss 1.4076	TrainAcc 0.5217	ValidAcc 0.5055	TestAcc 0.5062	BestValid 0.5103
	Epoch 3900:	Loss 1.4094	TrainAcc 0.5168	ValidAcc 0.4986	TestAcc 0.4987	BestValid 0.5103
	Epoch 3950:	Loss 1.4054	TrainAcc 0.5243	ValidAcc 0.5081	TestAcc 0.5082	BestValid 0.5103
	Epoch 4000:	Loss 1.4042	TrainAcc 0.5265	ValidAcc 0.5104	TestAcc 0.5097	BestValid 0.5104
	Epoch 4050:	Loss 1.4029	TrainAcc 0.5182	ValidAcc 0.4993	TestAcc 0.5002	BestValid 0.5104
	Epoch 4100:	Loss 1.4023	TrainAcc 0.5258	ValidAcc 0.5091	TestAcc 0.5088	BestValid 0.5104
	Epoch 4150:	Loss 1.4009	TrainAcc 0.5225	ValidAcc 0.5061	TestAcc 0.5061	BestValid 0.5104
	Epoch 4200:	Loss 1.4031	TrainAcc 0.5300	ValidAcc 0.5119	TestAcc 0.5112	BestValid 0.5119
	Epoch 4250:	Loss 1.4028	TrainAcc 0.5269	ValidAcc 0.5098	TestAcc 0.5088	BestValid 0.5119
	Epoch 4300:	Loss 1.4034	TrainAcc 0.5244	ValidAcc 0.5070	TestAcc 0.5067	BestValid 0.5119
	Epoch 4350:	Loss 1.4004	TrainAcc 0.5324	ValidAcc 0.5141	TestAcc 0.5123	BestValid 0.5141
	Epoch 4400:	Loss 1.3982	TrainAcc 0.5295	ValidAcc 0.5111	TestAcc 0.5100	BestValid 0.5141
	Epoch 4450:	Loss 1.4027	TrainAcc 0.5293	ValidAcc 0.5104	TestAcc 0.5099	BestValid 0.5141
	Epoch 4500:	Loss 1.3985	TrainAcc 0.5325	ValidAcc 0.5130	TestAcc 0.5130	BestValid 0.5141
	Epoch 4550:	Loss 1.3970	TrainAcc 0.5279	ValidAcc 0.5091	TestAcc 0.5078	BestValid 0.5141
	Epoch 4600:	Loss 1.3978	TrainAcc 0.5335	ValidAcc 0.5142	TestAcc 0.5121	BestValid 0.5142
	Epoch 4650:	Loss 1.4009	TrainAcc 0.5309	ValidAcc 0.5111	TestAcc 0.5100	BestValid 0.5142
	Epoch 4700:	Loss 1.3980	TrainAcc 0.5340	ValidAcc 0.5143	TestAcc 0.5124	BestValid 0.5143
	Epoch 4750:	Loss 1.3945	TrainAcc 0.5310	ValidAcc 0.5105	TestAcc 0.5105	BestValid 0.5143
	Epoch 4800:	Loss 1.3943	TrainAcc 0.5326	ValidAcc 0.5131	TestAcc 0.5108	BestValid 0.5143
	Epoch 4850:	Loss 1.3927	TrainAcc 0.5344	ValidAcc 0.5138	TestAcc 0.5129	BestValid 0.5143
	Epoch 4900:	Loss 1.3903	TrainAcc 0.5343	ValidAcc 0.5137	TestAcc 0.5127	BestValid 0.5143
	Epoch 4950:	Loss 1.3949	TrainAcc 0.5335	ValidAcc 0.5124	TestAcc 0.5110	BestValid 0.5143
	Epoch 5000:	Loss 1.3926	TrainAcc 0.5368	ValidAcc 0.5159	TestAcc 0.5144	BestValid 0.5159
****** Epoch Time (Excluding Evaluation Cost): 0.166 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.172 ms (Max: 0.232, Min: 0.037, Sum: 1.378)
Cluster-Wide Average, Compute: 33.002 ms (Max: 33.536, Min: 31.781, Sum: 264.018)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.118)
Cluster-Wide Average, Communication-Graph: 123.746 ms (Max: 124.941, Min: 123.167, Sum: 989.970)
Cluster-Wide Average, Optimization: 3.298 ms (Max: 3.350, Min: 3.234, Sum: 26.381)
Cluster-Wide Average, Others: 5.614 ms (Max: 5.710, Min: 5.530, Sum: 44.916)
****** Breakdown Sum: 165.855 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.018 GB (Max: 7.514, Min: 6.934, Sum: 56.147)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.680 Gbps (Max: 51.457, Min: 16.827, Sum: 341.441)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 4.603 GB
****** Accuracy Results ******
Highest valid_acc: 0.5159
Target test_acc: 0.5144
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
