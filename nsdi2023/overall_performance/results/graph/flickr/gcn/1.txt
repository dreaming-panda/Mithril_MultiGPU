Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.026 seconds.
        It takes 0.023 seconds.
Building the Feature Vector...
        It takes 0.025 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.100 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781)WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.780 Gbps (per GPU), 478.243 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.489 Gbps (per GPU), 475.909 Gbps (aggregated)
The layer-level communication performance: 59.482 Gbps (per GPU), 475.858 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.209 Gbps (per GPU), 473.673 Gbps (aggregated)
The layer-level communication performance: 59.245 Gbps (per GPU), 473.963 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.995 Gbps (per GPU), 471.963 Gbps (aggregated)
The layer-level communication performance: 58.949 Gbps (per GPU), 471.596 Gbps (aggregated)
The layer-level communication performance: 58.915 Gbps (per GPU), 471.319 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.172 Gbps (per GPU), 1241.377 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.184 Gbps (per GPU), 1241.469 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.161 Gbps (per GPU), 1241.287 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.106 Gbps (per GPU), 1240.849 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.155 Gbps (per GPU), 1241.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.181 Gbps (per GPU), 1241.446 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.172 Gbps (per GPU), 1241.377 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.118 Gbps (per GPU), 1240.941 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.603 Gbps (per GPU), 804.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.606 Gbps (per GPU), 804.848 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.602 Gbps (per GPU), 804.817 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.604 Gbps (per GPU), 804.830 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 4): 98.855 Gbps (per GPU), 790.837 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.859 Gbps (per GPU), 790.868 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.859 Gbps (per GPU), 790.869 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.857 Gbps (per GPU), 790.856 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.794 Gbps (per GPU), 278.354 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.793 Gbps (per GPU), 278.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.791 Gbps (per GPU), 278.327 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.790 Gbps (per GPU), 278.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.792 Gbps (per GPU), 278.339 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.788 Gbps (per GPU), 278.308 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.791 Gbps (per GPU), 278.327 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.787 Gbps (per GPU), 278.293 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.77ms  0.56ms  0.36ms  2.11 11.16K  0.12M
 chk_1  0.77ms  0.56ms  0.37ms  2.10 11.16K  0.11M
 chk_2  0.74ms  0.53ms  0.34ms  2.20 11.16K  0.11M
 chk_3  0.71ms  0.49ms  0.30ms  2.35 11.16K  0.12M
 chk_4  0.74ms  0.56ms  0.37ms  2.02 11.16K  0.11M
 chk_5  0.73ms  0.61ms  0.35ms  2.06 11.16K  0.10M
 chk_6  0.73ms  0.62ms  0.36ms  2.04 11.16K  0.12M
 chk_7  0.74ms  0.56ms  0.37ms  2.02 11.16K  0.11M
   Avg  0.74  0.56  0.35
   Max  0.77  0.62  0.37
   Min  0.71  0.49  0.30
 Ratio  1.10  1.25  1.23
   Var  0.00  0.00  0.00
Profiling takes 0.224 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 19.409 ms
Partition 0 [0, 4) has cost: 19.409 ms
Partition 1 [4, 8) has cost: 17.971 ms
Partition 2 [8, 12) has cost: 17.971 ms
Partition 3 [12, 16) has cost: 17.971 ms
Partition 4 [16, 20) has cost: 17.971 ms
Partition 5 [20, 24) has cost: 17.971 ms
Partition 6 [24, 28) has cost: 17.971 ms
Partition 7 [28, 32) has cost: 16.294 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 20.862 ms
GPU 0, Compute+Comm Time: 11.211 ms, Bubble Time: 9.629 ms, Imbalance Overhead: 0.021 ms
GPU 1, Compute+Comm Time: 10.642 ms, Bubble Time: 9.619 ms, Imbalance Overhead: 0.600 ms
GPU 2, Compute+Comm Time: 10.642 ms, Bubble Time: 9.618 ms, Imbalance Overhead: 0.601 ms
GPU 3, Compute+Comm Time: 10.642 ms, Bubble Time: 9.575 ms, Imbalance Overhead: 0.644 ms
GPU 4, Compute+Comm Time: 10.642 ms, Bubble Time: 9.498 ms, Imbalance Overhead: 0.721 ms
GPU 5, Compute+Comm Time: 10.642 ms, Bubble Time: 9.498 ms, Imbalance Overhead: 0.722 ms
GPU 6, Compute+Comm Time: 10.642 ms, Bubble Time: 9.625 ms, Imbalance Overhead: 0.595 ms
GPU 7, Compute+Comm Time: 10.081 ms, Bubble Time: 9.825 ms, Imbalance Overhead: 0.956 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 33.175 ms
GPU 0, Compute+Comm Time: 15.768 ms, Bubble Time: 15.589 ms, Imbalance Overhead: 1.819 ms
GPU 1, Compute+Comm Time: 16.884 ms, Bubble Time: 15.261 ms, Imbalance Overhead: 1.031 ms
GPU 2, Compute+Comm Time: 16.884 ms, Bubble Time: 15.074 ms, Imbalance Overhead: 1.217 ms
GPU 3, Compute+Comm Time: 16.884 ms, Bubble Time: 15.115 ms, Imbalance Overhead: 1.177 ms
GPU 4, Compute+Comm Time: 16.884 ms, Bubble Time: 15.246 ms, Imbalance Overhead: 1.046 ms
GPU 5, Compute+Comm Time: 16.884 ms, Bubble Time: 15.325 ms, Imbalance Overhead: 0.967 ms
GPU 6, Compute+Comm Time: 16.884 ms, Bubble Time: 15.322 ms, Imbalance Overhead: 0.970 ms
GPU 7, Compute+Comm Time: 17.753 ms, Bubble Time: 15.338 ms, Imbalance Overhead: 0.085 ms
The estimated cost of the whole pipeline: 56.739 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 37.380 ms
Partition 0 [0, 8) has cost: 37.380 ms
Partition 1 [8, 16) has cost: 35.942 ms
Partition 2 [16, 24) has cost: 35.942 ms
Partition 3 [24, 32) has cost: 34.265 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.857 ms
GPU 0, Compute+Comm Time: 14.878 ms, Bubble Time: 10.896 ms, Imbalance Overhead: 0.083 ms
GPU 1, Compute+Comm Time: 14.610 ms, Bubble Time: 10.817 ms, Imbalance Overhead: 0.431 ms
GPU 2, Compute+Comm Time: 14.610 ms, Bubble Time: 10.946 ms, Imbalance Overhead: 0.301 ms
GPU 3, Compute+Comm Time: 14.312 ms, Bubble Time: 11.226 ms, Imbalance Overhead: 0.319 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 37.350 ms
GPU 0, Compute+Comm Time: 20.496 ms, Bubble Time: 16.253 ms, Imbalance Overhead: 0.601 ms
GPU 1, Compute+Comm Time: 21.083 ms, Bubble Time: 15.772 ms, Imbalance Overhead: 0.494 ms
GPU 2, Compute+Comm Time: 21.083 ms, Bubble Time: 15.523 ms, Imbalance Overhead: 0.744 ms
GPU 3, Compute+Comm Time: 21.501 ms, Bubble Time: 15.681 ms, Imbalance Overhead: 0.168 ms
    The estimated cost with 2 DP ways is 66.367 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 73.323 ms
Partition 0 [0, 16) has cost: 73.323 ms
Partition 1 [16, 32) has cost: 70.207 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.413 ms
GPU 0, Compute+Comm Time: 28.380 ms, Bubble Time: 14.033 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 28.097 ms, Bubble Time: 14.206 ms, Imbalance Overhead: 0.111 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 52.474 ms
GPU 0, Compute+Comm Time: 34.648 ms, Bubble Time: 17.606 ms, Imbalance Overhead: 0.220 ms
GPU 1, Compute+Comm Time: 35.172 ms, Bubble Time: 17.302 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 99.631 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 143.530 ms
Partition 0 [0, 32) has cost: 143.530 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 121.396 ms
GPU 0, Compute+Comm Time: 121.396 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 128.086 ms
GPU 0, Compute+Comm Time: 128.086 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 261.956 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9459	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.2580
	Epoch 50:	Loss 1.6307	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6238	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.6254	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.6270	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.6245	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.6255	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.6185	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.6223	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.6156	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 500:	Loss 1.6163	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 550:	Loss 1.6029	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 600:	Loss 1.6103	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 650:	Loss 1.5881	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 700:	Loss 1.5826	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 750:	Loss 1.5786	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 800:	Loss 1.5796	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 850:	Loss 1.5814	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 900:	Loss 1.5730	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 950:	Loss 1.5729	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1000:	Loss 1.5756	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1050:	Loss 1.5709	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5694	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5670	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5729	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5696	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5644	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5671	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5622	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5642	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5667	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5687	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5596	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5611	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5564	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5594	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5556	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1850:	Loss 1.5578	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5569	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1950:	Loss 1.5589	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2000:	Loss 1.5597	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5512	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2100:	Loss 1.5591	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2150:	Loss 1.5484	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2200:	Loss 1.5647	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2250:	Loss 1.5622	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2300:	Loss 1.5539	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2350:	Loss 1.5484	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2400:	Loss 1.5490	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2450:	Loss 1.5402	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2500:	Loss 1.5444	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2550:	Loss 1.5485	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2600:	Loss 1.5541	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2650:	Loss 1.5583	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2700:	Loss 1.5516	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2750:	Loss 1.5519	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2800:	Loss 1.5437	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2850:	Loss 1.5375	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2900:	Loss 1.5392	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2950:	Loss 1.5385	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5416	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3050:	Loss 1.5417	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5448	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5439	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3200:	Loss 1.5394	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3250:	Loss 1.5487	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5420	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5427	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5394	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5372	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5411	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5370	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5433	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5458	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5386	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5440	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5397	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5401	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5397	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5529	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5396	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5494	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5433	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5400	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5410	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5358	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5378	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5416	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5394	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5392	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5345	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5362	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5369	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5434	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5328	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.5395	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5470	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5327	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5328	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5415	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5339	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.157 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.183 ms (Max: 0.261, Min: 0.038, Sum: 1.466)
Cluster-Wide Average, Compute: 23.161 ms (Max: 23.486, Min: 22.181, Sum: 185.291)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.065)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.017, Min: 0.014, Sum: 0.126)
Cluster-Wide Average, Communication-Graph: 124.584 ms (Max: 125.592, Min: 124.194, Sum: 996.676)
Cluster-Wide Average, Optimization: 3.063 ms (Max: 3.121, Min: 3.001, Sum: 24.501)
Cluster-Wide Average, Others: 5.617 ms (Max: 5.625, Min: 5.601, Sum: 44.933)
****** Breakdown Sum: 156.632 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.066 GB (Max: 6.428, Min: 6.001, Sum: 48.526)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.374 Gbps (Max: 51.067, Min: 16.756, Sum: 338.991)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.018 GB
Total communication (cluster-wide, per-epoch): 4.601 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 49
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
