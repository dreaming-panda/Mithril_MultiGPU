Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...Building the CSR structure...
Building the CSR structure...

Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.033 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.018 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.103 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.745 Gbps (per GPU), 485.958 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.456 Gbps (per GPU), 483.645 Gbps (aggregated)
The layer-level communication performance: 60.446 Gbps (per GPU), 483.570 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.184 Gbps (per GPU), 481.475 Gbps (aggregated)
The layer-level communication performance: 60.148 Gbps (per GPU), 481.183 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.912 Gbps (per GPU), 479.298 Gbps (aggregated)
The layer-level communication performance: 59.869 Gbps (per GPU), 478.951 Gbps (aggregated)
The layer-level communication performance: 59.834 Gbps (per GPU), 478.673 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.786 Gbps (per GPU), 1246.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.772 Gbps (per GPU), 1246.172 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.743 Gbps (per GPU), 1245.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.772 Gbps (per GPU), 1246.173 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.789 Gbps (per GPU), 1246.308 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.769 Gbps (per GPU), 1246.149 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.743 Gbps (per GPU), 1245.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.780 Gbps (per GPU), 1246.239 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.883 Gbps (per GPU), 807.063 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.881 Gbps (per GPU), 807.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.886 Gbps (per GPU), 807.089 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.883 Gbps (per GPU), 807.063 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.882 Gbps (per GPU), 807.057 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.877 Gbps (per GPU), 807.017 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.887 Gbps (per GPU), 807.096 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.876 Gbps (per GPU), 807.005 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.032 Gbps (per GPU), 280.256 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.033 Gbps (per GPU), 280.260 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.031 Gbps (per GPU), 280.247 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.031 Gbps (per GPU), 280.245 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.027 Gbps (per GPU), 280.217 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.033 Gbps (per GPU), 280.266 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.030 Gbps (per GPU), 280.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.031 Gbps (per GPU), 280.247 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.75ms  0.56ms  0.36ms  2.05 11.16K  0.12M
 chk_1  0.75ms  0.56ms  0.37ms  2.04 11.16K  0.11M
 chk_2  0.72ms  0.53ms  0.34ms  2.13 11.16K  0.11M
 chk_3  0.68ms  0.49ms  0.30ms  2.28 11.16K  0.12M
 chk_4  0.75ms  0.56ms  0.37ms  2.03 11.16K  0.11M
 chk_5  0.74ms  0.55ms  0.36ms  2.08 11.16K  0.10M
 chk_6  0.74ms  0.55ms  0.36ms  2.07 11.16K  0.12M
 chk_7  0.75ms  0.56ms  0.37ms  2.03 11.16K  0.11M
   Avg  0.73  0.54  0.35
   Max  0.75  0.56  0.37
   Min  0.68  0.49  0.30
 Ratio  1.10  1.14  1.23
   Var  0.00  0.00  0.00
Profiling takes 0.220 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 18.945 ms
Partition 0 [0, 4) has cost: 18.945 ms
Partition 1 [4, 8) has cost: 17.423 ms
Partition 2 [8, 12) has cost: 17.423 ms
Partition 3 [12, 16) has cost: 17.423 ms
Partition 4 [16, 20) has cost: 17.423 ms
Partition 5 [20, 24) has cost: 17.423 ms
Partition 6 [24, 28) has cost: 17.423 ms
Partition 7 [28, 32) has cost: 15.889 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 20.167 ms
GPU 0, Compute+Comm Time: 11.002 ms, Bubble Time: 9.148 ms, Imbalance Overhead: 0.018 ms
GPU 1, Compute+Comm Time: 10.385 ms, Bubble Time: 9.223 ms, Imbalance Overhead: 0.559 ms
GPU 2, Compute+Comm Time: 10.385 ms, Bubble Time: 9.305 ms, Imbalance Overhead: 0.478 ms
GPU 3, Compute+Comm Time: 10.385 ms, Bubble Time: 9.340 ms, Imbalance Overhead: 0.442 ms
GPU 4, Compute+Comm Time: 10.385 ms, Bubble Time: 9.346 ms, Imbalance Overhead: 0.436 ms
GPU 5, Compute+Comm Time: 10.385 ms, Bubble Time: 9.425 ms, Imbalance Overhead: 0.357 ms
GPU 6, Compute+Comm Time: 10.385 ms, Bubble Time: 9.487 ms, Imbalance Overhead: 0.295 ms
GPU 7, Compute+Comm Time: 9.871 ms, Bubble Time: 9.627 ms, Imbalance Overhead: 0.669 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 31.970 ms
GPU 0, Compute+Comm Time: 15.421 ms, Bubble Time: 15.215 ms, Imbalance Overhead: 1.334 ms
GPU 1, Compute+Comm Time: 16.441 ms, Bubble Time: 14.994 ms, Imbalance Overhead: 0.535 ms
GPU 2, Compute+Comm Time: 16.441 ms, Bubble Time: 14.912 ms, Imbalance Overhead: 0.617 ms
GPU 3, Compute+Comm Time: 16.441 ms, Bubble Time: 14.806 ms, Imbalance Overhead: 0.723 ms
GPU 4, Compute+Comm Time: 16.441 ms, Bubble Time: 14.798 ms, Imbalance Overhead: 0.730 ms
GPU 5, Compute+Comm Time: 16.441 ms, Bubble Time: 14.768 ms, Imbalance Overhead: 0.761 ms
GPU 6, Compute+Comm Time: 16.441 ms, Bubble Time: 14.647 ms, Imbalance Overhead: 0.882 ms
GPU 7, Compute+Comm Time: 17.347 ms, Bubble Time: 14.549 ms, Imbalance Overhead: 0.075 ms
The estimated cost of the whole pipeline: 54.744 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 36.369 ms
Partition 0 [0, 8) has cost: 36.369 ms
Partition 1 [8, 16) has cost: 34.847 ms
Partition 2 [16, 24) has cost: 34.847 ms
Partition 3 [24, 32) has cost: 33.312 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.090 ms
GPU 0, Compute+Comm Time: 14.512 ms, Bubble Time: 10.578 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 14.203 ms, Bubble Time: 10.658 ms, Imbalance Overhead: 0.228 ms
GPU 2, Compute+Comm Time: 14.203 ms, Bubble Time: 10.747 ms, Imbalance Overhead: 0.139 ms
GPU 3, Compute+Comm Time: 13.946 ms, Bubble Time: 10.887 ms, Imbalance Overhead: 0.256 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 36.049 ms
GPU 0, Compute+Comm Time: 19.885 ms, Bubble Time: 15.627 ms, Imbalance Overhead: 0.536 ms
GPU 1, Compute+Comm Time: 20.395 ms, Bubble Time: 15.427 ms, Imbalance Overhead: 0.227 ms
GPU 2, Compute+Comm Time: 20.395 ms, Bubble Time: 15.298 ms, Imbalance Overhead: 0.355 ms
GPU 3, Compute+Comm Time: 20.847 ms, Bubble Time: 15.202 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 64.195 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 71.215 ms
Partition 0 [0, 16) has cost: 71.215 ms
Partition 1 [16, 32) has cost: 68.159 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.405 ms
GPU 0, Compute+Comm Time: 27.692 ms, Bubble Time: 13.713 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 27.410 ms, Bubble Time: 13.837 ms, Imbalance Overhead: 0.158 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.763 ms
GPU 0, Compute+Comm Time: 33.499 ms, Bubble Time: 16.957 ms, Imbalance Overhead: 0.307 ms
GPU 1, Compute+Comm Time: 33.981 ms, Bubble Time: 16.782 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 96.776 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 139.374 ms
Partition 0 [0, 32) has cost: 139.374 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 120.038 ms
GPU 0, Compute+Comm Time: 120.038 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 126.276 ms
GPU 0, Compute+Comm Time: 126.276 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 258.630 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9459	TrainAcc 0.0986	ValidAcc 0.0970	TestAcc 0.0953	BestValid 0.0970
	Epoch 50:	Loss 1.6610	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6259	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.6259	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.6256	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.6281	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.6252	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.6239	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.6221	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.6125	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 500:	Loss 1.6097	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.6026	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.6354	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 650:	Loss 1.6004	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5817	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5772	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5807	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5743	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5778	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5750	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5731	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5739	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5728	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5710	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5709	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5709	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5699	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5843	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1400:	Loss 1.5745	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1450:	Loss 1.5692	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1500:	Loss 1.5708	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1550:	Loss 1.5697	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5686	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5690	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1700:	Loss 1.5664	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1750:	Loss 1.5720	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1800:	Loss 1.5642	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1850:	Loss 1.5690	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1900:	Loss 1.5652	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1950:	Loss 1.5622	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2000:	Loss 1.5640	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2050:	Loss 1.5608	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.4239
	Epoch 2100:	Loss 1.5717	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.4239
	Epoch 2150:	Loss 1.5631	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2200:	Loss 1.5617	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.4239
	Epoch 2250:	Loss 1.5579	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5596	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.4239
	Epoch 2350:	Loss 1.5583	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.4239
	Epoch 2400:	Loss 1.5609	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5620	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5622	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.4239
	Epoch 2550:	Loss 1.5581	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.4239
	Epoch 2600:	Loss 1.5655	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5588	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.4239
	Epoch 2700:	Loss 1.5607	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5596	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5561	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5575	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.4239
	Epoch 2900:	Loss 1.5542	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5598	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5603	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5531	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5526	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5605	TrainAcc 0.0545	ValidAcc 0.0549	TestAcc 0.0559	BestValid 0.4239
	Epoch 3200:	Loss 1.5485	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5608	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.4239
	Epoch 3300:	Loss 1.5521	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5469	TrainAcc 0.0545	ValidAcc 0.0549	TestAcc 0.0559	BestValid 0.4239
	Epoch 3400:	Loss 1.5937	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3450:	Loss 1.5525	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5531	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3550:	Loss 1.5502	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3600:	Loss 1.5514	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3650:	Loss 1.5473	TrainAcc 0.0545	ValidAcc 0.0549	TestAcc 0.0559	BestValid 0.4239
	Epoch 3700:	Loss 1.5448	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3750:	Loss 1.5498	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3800:	Loss 1.5483	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5487	TrainAcc 0.0545	ValidAcc 0.0549	TestAcc 0.0559	BestValid 0.4239
	Epoch 3900:	Loss 1.5432	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5458	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4000:	Loss 1.5467	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4050:	Loss 1.5490	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5403	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5578	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5404	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5409	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5435	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5420	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5407	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5376	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5375	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5405	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5437	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5404	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5388	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.5377	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5563	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5401	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5453	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5403	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5355	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.156 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.146 ms (Max: 0.229, Min: 0.035, Sum: 1.168)
Cluster-Wide Average, Compute: 23.041 ms (Max: 23.524, Min: 21.831, Sum: 184.324)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.061)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.013, Sum: 0.118)
Cluster-Wide Average, Communication-Graph: 124.242 ms (Max: 125.431, Min: 123.802, Sum: 993.939)
Cluster-Wide Average, Optimization: 3.010 ms (Max: 3.065, Min: 2.956, Sum: 24.082)
Cluster-Wide Average, Others: 5.594 ms (Max: 5.631, Min: 5.556, Sum: 44.754)
****** Breakdown Sum: 156.056 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.066 GB (Max: 6.428, Min: 6.001, Sum: 48.526)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.485 Gbps (Max: 51.092, Min: 16.758, Sum: 339.882)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.018 GB
Total communication (cluster-wide, per-epoch): 4.601 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
