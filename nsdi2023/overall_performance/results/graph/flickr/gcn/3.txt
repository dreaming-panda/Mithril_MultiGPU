Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.034 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
        It takes 0.035 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
        It takes 0.021 seconds.
        It takes 0.018 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.008 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.009 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 61.436 Gbps (per GPU), 491.487 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 61.105 Gbps (per GPU), 488.837 Gbps (aggregated)
The layer-level communication performance: 61.119 Gbps (per GPU), 488.953 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.812 Gbps (per GPU), 486.498 Gbps (aggregated)
The layer-level communication performance: 60.850 Gbps (per GPU), 486.800 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.589 Gbps (per GPU), 484.715 Gbps (aggregated)
The layer-level communication performance: 60.546 Gbps (per GPU), 484.369 Gbps (aggregated)
The layer-level communication performance: 60.506 Gbps (per GPU), 484.052 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.128 Gbps (per GPU), 1249.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.134 Gbps (per GPU), 1249.071 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.128 Gbps (per GPU), 1249.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.082 Gbps (per GPU), 1248.653 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.137 Gbps (per GPU), 1249.098 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.154 Gbps (per GPU), 1249.234 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.134 Gbps (per GPU), 1249.071 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.076 Gbps (per GPU), 1248.606 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.757 Gbps (per GPU), 814.052 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.760 Gbps (per GPU), 814.079 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.757 Gbps (per GPU), 814.052 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.760 Gbps (per GPU), 814.079 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.754 Gbps (per GPU), 814.033 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.755 Gbps (per GPU), 814.040 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.757 Gbps (per GPU), 814.059 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.761 Gbps (per GPU), 814.086 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.778 Gbps (per GPU), 262.222 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.782 Gbps (per GPU), 262.253 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.780 Gbps (per GPU), 262.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.778 Gbps (per GPU), 262.222 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.777 Gbps (per GPU), 262.217 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.779 Gbps (per GPU), 262.234 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.772 Gbps (per GPU), 262.176 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.779 Gbps (per GPU), 262.229 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.76ms  0.56ms  0.36ms  2.10 11.16K  0.12M
 chk_1  0.77ms  0.56ms  0.37ms  2.08 11.16K  0.11M
 chk_2  0.73ms  0.53ms  0.34ms  2.18 11.16K  0.11M
 chk_3  0.70ms  0.49ms  0.30ms  2.34 11.16K  0.12M
 chk_4  0.77ms  0.56ms  0.37ms  2.09 11.16K  0.11M
 chk_5  0.73ms  0.55ms  0.35ms  2.07 11.16K  0.10M
 chk_6  0.74ms  0.55ms  0.36ms  2.06 11.16K  0.12M
 chk_7  0.74ms  0.56ms  0.37ms  2.02 11.16K  0.11M
   Avg  0.74  0.54  0.35
   Max  0.77  0.56  0.37
   Min  0.70  0.49  0.30
 Ratio  1.10  1.15  1.24
   Var  0.00  0.00  0.00
Profiling takes 0.220 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 18.998 ms
Partition 0 [0, 4) has cost: 18.998 ms
Partition 1 [4, 8) has cost: 17.424 ms
Partition 2 [8, 12) has cost: 17.424 ms
Partition 3 [12, 16) has cost: 17.424 ms
Partition 4 [16, 20) has cost: 17.424 ms
Partition 5 [20, 24) has cost: 17.424 ms
Partition 6 [24, 28) has cost: 17.424 ms
Partition 7 [28, 32) has cost: 15.878 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 20.066 ms
GPU 0, Compute+Comm Time: 10.935 ms, Bubble Time: 9.113 ms, Imbalance Overhead: 0.018 ms
GPU 1, Compute+Comm Time: 10.325 ms, Bubble Time: 9.182 ms, Imbalance Overhead: 0.559 ms
GPU 2, Compute+Comm Time: 10.325 ms, Bubble Time: 9.260 ms, Imbalance Overhead: 0.481 ms
GPU 3, Compute+Comm Time: 10.325 ms, Bubble Time: 9.294 ms, Imbalance Overhead: 0.447 ms
GPU 4, Compute+Comm Time: 10.325 ms, Bubble Time: 9.297 ms, Imbalance Overhead: 0.444 ms
GPU 5, Compute+Comm Time: 10.325 ms, Bubble Time: 9.375 ms, Imbalance Overhead: 0.366 ms
GPU 6, Compute+Comm Time: 10.325 ms, Bubble Time: 9.435 ms, Imbalance Overhead: 0.306 ms
GPU 7, Compute+Comm Time: 9.808 ms, Bubble Time: 9.567 ms, Imbalance Overhead: 0.691 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 31.931 ms
GPU 0, Compute+Comm Time: 15.367 ms, Bubble Time: 15.226 ms, Imbalance Overhead: 1.337 ms
GPU 1, Compute+Comm Time: 16.397 ms, Bubble Time: 15.015 ms, Imbalance Overhead: 0.519 ms
GPU 2, Compute+Comm Time: 16.397 ms, Bubble Time: 14.943 ms, Imbalance Overhead: 0.591 ms
GPU 3, Compute+Comm Time: 16.397 ms, Bubble Time: 14.815 ms, Imbalance Overhead: 0.719 ms
GPU 4, Compute+Comm Time: 16.397 ms, Bubble Time: 14.808 ms, Imbalance Overhead: 0.727 ms
GPU 5, Compute+Comm Time: 16.397 ms, Bubble Time: 14.760 ms, Imbalance Overhead: 0.774 ms
GPU 6, Compute+Comm Time: 16.397 ms, Bubble Time: 14.626 ms, Imbalance Overhead: 0.908 ms
GPU 7, Compute+Comm Time: 17.361 ms, Bubble Time: 14.509 ms, Imbalance Overhead: 0.061 ms
The estimated cost of the whole pipeline: 54.596 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 36.422 ms
Partition 0 [0, 8) has cost: 36.422 ms
Partition 1 [8, 16) has cost: 34.848 ms
Partition 2 [16, 24) has cost: 34.848 ms
Partition 3 [24, 32) has cost: 33.301 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 24.975 ms
GPU 0, Compute+Comm Time: 14.435 ms, Bubble Time: 10.540 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 14.130 ms, Bubble Time: 10.620 ms, Imbalance Overhead: 0.226 ms
GPU 2, Compute+Comm Time: 14.130 ms, Bubble Time: 10.702 ms, Imbalance Overhead: 0.143 ms
GPU 3, Compute+Comm Time: 13.872 ms, Bubble Time: 10.824 ms, Imbalance Overhead: 0.280 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 35.988 ms
GPU 0, Compute+Comm Time: 19.834 ms, Bubble Time: 15.626 ms, Imbalance Overhead: 0.529 ms
GPU 1, Compute+Comm Time: 20.348 ms, Bubble Time: 15.429 ms, Imbalance Overhead: 0.211 ms
GPU 2, Compute+Comm Time: 20.348 ms, Bubble Time: 15.288 ms, Imbalance Overhead: 0.352 ms
GPU 3, Compute+Comm Time: 20.826 ms, Bubble Time: 15.162 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 64.012 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 71.270 ms
Partition 0 [0, 16) has cost: 71.270 ms
Partition 1 [16, 32) has cost: 68.149 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.103 ms
GPU 0, Compute+Comm Time: 27.494 ms, Bubble Time: 13.609 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 27.213 ms, Bubble Time: 13.744 ms, Imbalance Overhead: 0.146 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.528 ms
GPU 0, Compute+Comm Time: 33.328 ms, Bubble Time: 16.904 ms, Imbalance Overhead: 0.295 ms
GPU 1, Compute+Comm Time: 33.844 ms, Bubble Time: 16.683 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 96.212 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 139.419 ms
Partition 0 [0, 32) has cost: 139.419 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 127.848 ms
GPU 0, Compute+Comm Time: 127.848 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 134.114 ms
GPU 0, Compute+Comm Time: 134.114 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 275.060 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.6604	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6259	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.6259	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.6256	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.6281	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.6250	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.6239	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.6214	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.6385	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 500:	Loss 1.6028	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.6015	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.6212	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5835	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5833	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5756	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5781	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5728	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5805	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5742	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5717	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5726	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5724	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1150:	Loss 1.5699	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5701	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1250:	Loss 1.5693	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1300:	Loss 1.5685	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1350:	Loss 1.5736	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1400:	Loss 1.5694	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1450:	Loss 1.5693	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1500:	Loss 1.5695	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1550:	Loss 1.5703	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1600:	Loss 1.5665	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1650:	Loss 1.5663	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1700:	Loss 1.5805	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1750:	Loss 1.6201	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.6139	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1850:	Loss 1.6007	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1900:	Loss 1.6037	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1950:	Loss 1.5960	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2000:	Loss 1.5871	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2050:	Loss 1.5760	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.6108	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2150:	Loss 1.5974	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2200:	Loss 1.5772	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5686	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5690	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5667	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2400:	Loss 1.5645	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5658	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5652	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5635	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2600:	Loss 1.5665	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5619	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5652	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5711	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5576	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5645	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5596	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5605	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5604	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5577	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5568	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5623	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5545	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5562	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5571	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5564	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5583	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5544	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5534	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5598	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5535	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5620	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5562	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5518	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5576	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5541	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5532	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5594	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5485	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5541	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5532	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5546	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5460	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5499	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5495	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5535	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5504	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5747	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5486	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5501	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5590	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5483	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5439	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.5469	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5518	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5451	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5478	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5430	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5470	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.156 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.169 ms (Max: 0.239, Min: 0.038, Sum: 1.348)
Cluster-Wide Average, Compute: 23.129 ms (Max: 23.543, Min: 21.901, Sum: 185.031)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.122)
Cluster-Wide Average, Communication-Graph: 123.995 ms (Max: 125.228, Min: 123.596, Sum: 991.957)
Cluster-Wide Average, Optimization: 3.035 ms (Max: 3.084, Min: 2.993, Sum: 24.280)
Cluster-Wide Average, Others: 5.594 ms (Max: 5.618, Min: 5.557, Sum: 44.755)
****** Breakdown Sum: 155.944 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.066 GB (Max: 6.428, Min: 6.001, Sum: 48.526)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.588 Gbps (Max: 51.168, Min: 16.786, Sum: 340.706)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.018 GB
Total communication (cluster-wide, per-epoch): 4.601 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 49
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
