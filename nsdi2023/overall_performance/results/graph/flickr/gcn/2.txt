Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.016 seconds.
        It takes 0.018 seconds.
        It takes 0.019 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.101 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.109 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.280 Gbps (per GPU), 482.242 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.963 Gbps (per GPU), 479.704 Gbps (aggregated)
The layer-level communication performance: 59.965 Gbps (per GPU), 479.718 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.723 Gbps (per GPU), 477.785 Gbps (aggregated)
The layer-level communication performance: 59.691 Gbps (per GPU), 477.530 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.472 Gbps (per GPU), 475.775 Gbps (aggregated)
The layer-level communication performance: 59.423 Gbps (per GPU), 475.385 Gbps (aggregated)
The layer-level communication performance: 59.390 Gbps (per GPU), 475.117 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.734 Gbps (per GPU), 1245.871 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.740 Gbps (per GPU), 1245.918 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.720 Gbps (per GPU), 1245.760 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.754 Gbps (per GPU), 1246.033 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.711 Gbps (per GPU), 1245.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.670 Gbps (per GPU), 1245.363 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.711 Gbps (per GPU), 1245.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.665 Gbps (per GPU), 1245.317 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.424 Gbps (per GPU), 811.389 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.426 Gbps (per GPU), 811.408 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.419 Gbps (per GPU), 811.356 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.425 Gbps (per GPU), 811.402 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.424 Gbps (per GPU), 811.389 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.427 Gbps (per GPU), 811.414 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.424 Gbps (per GPU), 811.395 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.426 Gbps (per GPU), 811.408 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.264 Gbps (per GPU), 242.114 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.265 Gbps (per GPU), 242.118 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.266 Gbps (per GPU), 242.129 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.265 Gbps (per GPU), 242.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.264 Gbps (per GPU), 242.111 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.264 Gbps (per GPU), 242.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.258 Gbps (per GPU), 242.067 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.260 Gbps (per GPU), 242.083 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.75ms  0.56ms  0.36ms  2.07 11.16K  0.12M
 chk_1  0.75ms  0.56ms  0.37ms  2.04 11.16K  0.11M
 chk_2  0.72ms  0.53ms  0.34ms  2.14 11.16K  0.11M
 chk_3  0.69ms  0.49ms  0.30ms  2.30 11.16K  0.12M
 chk_4  0.75ms  0.56ms  0.37ms  2.05 11.16K  0.11M
 chk_5  0.74ms  0.55ms  0.35ms  2.08 11.16K  0.10M
 chk_6  0.74ms  0.55ms  0.36ms  2.05 11.16K  0.12M
 chk_7  0.74ms  0.56ms  0.37ms  2.01 11.16K  0.11M
   Avg  0.73  0.54  0.35
   Max  0.75  0.56  0.37
   Min  0.69  0.49  0.30
 Ratio  1.10  1.14  1.24
   Var  0.00  0.00  0.00
Profiling takes 0.221 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 18.953 ms
Partition 0 [0, 4) has cost: 18.953 ms
Partition 1 [4, 8) has cost: 17.440 ms
Partition 2 [8, 12) has cost: 17.440 ms
Partition 3 [12, 16) has cost: 17.440 ms
Partition 4 [16, 20) has cost: 17.440 ms
Partition 5 [20, 24) has cost: 17.440 ms
Partition 6 [24, 28) has cost: 17.440 ms
Partition 7 [28, 32) has cost: 15.895 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 20.258 ms
GPU 0, Compute+Comm Time: 11.038 ms, Bubble Time: 9.205 ms, Imbalance Overhead: 0.015 ms
GPU 1, Compute+Comm Time: 10.421 ms, Bubble Time: 9.276 ms, Imbalance Overhead: 0.562 ms
GPU 2, Compute+Comm Time: 10.421 ms, Bubble Time: 9.352 ms, Imbalance Overhead: 0.485 ms
GPU 3, Compute+Comm Time: 10.421 ms, Bubble Time: 9.385 ms, Imbalance Overhead: 0.452 ms
GPU 4, Compute+Comm Time: 10.421 ms, Bubble Time: 9.385 ms, Imbalance Overhead: 0.452 ms
GPU 5, Compute+Comm Time: 10.421 ms, Bubble Time: 9.463 ms, Imbalance Overhead: 0.374 ms
GPU 6, Compute+Comm Time: 10.421 ms, Bubble Time: 9.521 ms, Imbalance Overhead: 0.316 ms
GPU 7, Compute+Comm Time: 9.903 ms, Bubble Time: 9.652 ms, Imbalance Overhead: 0.703 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 32.072 ms
GPU 0, Compute+Comm Time: 15.468 ms, Bubble Time: 15.254 ms, Imbalance Overhead: 1.351 ms
GPU 1, Compute+Comm Time: 16.494 ms, Bubble Time: 15.038 ms, Imbalance Overhead: 0.540 ms
GPU 2, Compute+Comm Time: 16.494 ms, Bubble Time: 14.961 ms, Imbalance Overhead: 0.617 ms
GPU 3, Compute+Comm Time: 16.494 ms, Bubble Time: 14.852 ms, Imbalance Overhead: 0.725 ms
GPU 4, Compute+Comm Time: 16.494 ms, Bubble Time: 14.852 ms, Imbalance Overhead: 0.726 ms
GPU 5, Compute+Comm Time: 16.494 ms, Bubble Time: 14.823 ms, Imbalance Overhead: 0.755 ms
GPU 6, Compute+Comm Time: 16.494 ms, Bubble Time: 14.710 ms, Imbalance Overhead: 0.868 ms
GPU 7, Compute+Comm Time: 17.391 ms, Bubble Time: 14.611 ms, Imbalance Overhead: 0.070 ms
The estimated cost of the whole pipeline: 54.947 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 36.393 ms
Partition 0 [0, 8) has cost: 36.393 ms
Partition 1 [8, 16) has cost: 34.880 ms
Partition 2 [16, 24) has cost: 34.880 ms
Partition 3 [24, 32) has cost: 33.335 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.177 ms
GPU 0, Compute+Comm Time: 14.548 ms, Bubble Time: 10.629 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 14.239 ms, Bubble Time: 10.708 ms, Imbalance Overhead: 0.230 ms
GPU 2, Compute+Comm Time: 14.239 ms, Bubble Time: 10.786 ms, Imbalance Overhead: 0.152 ms
GPU 3, Compute+Comm Time: 13.980 ms, Bubble Time: 10.907 ms, Imbalance Overhead: 0.290 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 36.153 ms
GPU 0, Compute+Comm Time: 19.946 ms, Bubble Time: 15.666 ms, Imbalance Overhead: 0.540 ms
GPU 1, Compute+Comm Time: 20.459 ms, Bubble Time: 15.473 ms, Imbalance Overhead: 0.221 ms
GPU 2, Compute+Comm Time: 20.459 ms, Bubble Time: 15.360 ms, Imbalance Overhead: 0.334 ms
GPU 3, Compute+Comm Time: 20.897 ms, Bubble Time: 15.256 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 64.397 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 71.273 ms
Partition 0 [0, 16) has cost: 71.273 ms
Partition 1 [16, 32) has cost: 68.214 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.325 ms
GPU 0, Compute+Comm Time: 27.645 ms, Bubble Time: 13.679 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 27.361 ms, Bubble Time: 13.824 ms, Imbalance Overhead: 0.139 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.694 ms
GPU 0, Compute+Comm Time: 33.467 ms, Bubble Time: 16.963 ms, Imbalance Overhead: 0.264 ms
GPU 1, Compute+Comm Time: 33.949 ms, Bubble Time: 16.745 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 96.620 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 139.487 ms
Partition 0 [0, 32) has cost: 139.487 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 137.997 ms
GPU 0, Compute+Comm Time: 137.997 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 144.220 ms
GPU 0, Compute+Comm Time: 144.220 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 296.328 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9459	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 50:	Loss 1.8371	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 100:	Loss 1.6292	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.6283	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.6267	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.6254	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.6217	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.6252	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.6218	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.6175	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 500:	Loss 1.6102	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 550:	Loss 1.6148	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5815	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 650:	Loss 1.5862	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5772	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 750:	Loss 1.5786	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 800:	Loss 1.5747	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 850:	Loss 1.5749	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 900:	Loss 1.5733	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 950:	Loss 1.5714	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1000:	Loss 1.5707	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5691	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5718	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5724	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5757	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5700	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5667	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5688	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5653	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5712	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5659	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5643	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5610	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5638	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5678	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5595	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5623	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5721	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5607	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1950:	Loss 1.5564	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2000:	Loss 1.5634	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2050:	Loss 1.5664	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5588	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2150:	Loss 1.5542	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2200:	Loss 1.5577	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2250:	Loss 1.5552	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5548	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5507	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2400:	Loss 1.5471	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5544	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2500:	Loss 1.5547	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2550:	Loss 1.5602	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2600:	Loss 1.5460	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2650:	Loss 1.5527	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2700:	Loss 1.5429	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2750:	Loss 1.5446	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5514	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2850:	Loss 1.5467	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2900:	Loss 1.5434	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2950:	Loss 1.5413	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3000:	Loss 1.5465	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3050:	Loss 1.5419	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5436	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3150:	Loss 1.5409	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3200:	Loss 1.5436	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3250:	Loss 1.5423	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3300:	Loss 1.5458	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3350:	Loss 1.5433	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3400:	Loss 1.5438	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5394	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3500:	Loss 1.5377	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5374	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3600:	Loss 1.5410	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3650:	Loss 1.5381	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5377	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3750:	Loss 1.5383	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5380	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3850:	Loss 1.5387	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5462	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3950:	Loss 1.5367	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4000:	Loss 1.5416	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5383	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4100:	Loss 1.5478	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4150:	Loss 1.5370	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5377	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5333	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5356	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4350:	Loss 1.5387	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5351	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5366	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5357	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5371	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4600:	Loss 1.5349	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4650:	Loss 1.5381	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4700:	Loss 1.5348	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.5323	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4800:	Loss 1.5388	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4850:	Loss 1.5490	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5343	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5319	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5381	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.156 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.157 ms (Max: 0.231, Min: 0.040, Sum: 1.254)
Cluster-Wide Average, Compute: 23.107 ms (Max: 23.589, Min: 22.128, Sum: 184.853)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.124)
Cluster-Wide Average, Communication-Graph: 124.115 ms (Max: 125.069, Min: 123.684, Sum: 992.920)
Cluster-Wide Average, Optimization: 3.079 ms (Max: 3.115, Min: 3.040, Sum: 24.634)
Cluster-Wide Average, Others: 5.585 ms (Max: 5.606, Min: 5.557, Sum: 44.683)
****** Breakdown Sum: 156.066 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.066 GB (Max: 6.428, Min: 6.001, Sum: 48.526)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.534 Gbps (Max: 51.290, Min: 16.829, Sum: 340.274)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.018 GB
Total communication (cluster-wide, per-epoch): 4.601 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 0
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
