Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
        It takes 0.023 seconds.
        It takes 0.020 seconds.
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.104 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.573 Gbps (per GPU), 476.585 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.291 Gbps (per GPU), 474.324 Gbps (aggregated)
The layer-level communication performance: 59.294 Gbps (per GPU), 474.352 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.017 Gbps (per GPU), 472.137 Gbps (aggregated)
The layer-level communication performance: 59.047 Gbps (per GPU), 472.378 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.792 Gbps (per GPU), 470.338 Gbps (aggregated)
The layer-level communication performance: 58.715 Gbps (per GPU), 469.720 Gbps (aggregated)
The layer-level communication performance: 58.744 Gbps (per GPU), 469.955 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.532 Gbps (per GPU), 1244.255 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.543 Gbps (per GPU), 1244.347 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.535 Gbps (per GPU), 1244.278 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.466 Gbps (per GPU), 1243.724 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.526 Gbps (per GPU), 1244.208 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.555 Gbps (per GPU), 1244.439 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.520 Gbps (per GPU), 1244.162 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.460 Gbps (per GPU), 1243.678 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.627 Gbps (per GPU), 805.017 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.643 Gbps (per GPU), 805.145 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.634 Gbps (per GPU), 805.074 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.642 Gbps (per GPU), 805.132 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.641 Gbps (per GPU), 805.126 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.614 Gbps (per GPU), 804.914 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.642 Gbps (per GPU), 805.132 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.540 Gbps (per GPU), 804.316 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.855 Gbps (per GPU), 254.843 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.855 Gbps (per GPU), 254.838 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.854 Gbps (per GPU), 254.831 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.856 Gbps (per GPU), 254.845 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.851 Gbps (per GPU), 254.811 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.856 Gbps (per GPU), 254.845 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.853 Gbps (per GPU), 254.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.849 Gbps (per GPU), 254.793 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.76ms  0.56ms  0.36ms  2.09 11.16K  0.12M
 chk_1  0.77ms  0.56ms  0.37ms  2.07 11.16K  0.11M
 chk_2  0.73ms  0.53ms  0.34ms  2.18 11.16K  0.11M
 chk_3  0.70ms  0.49ms  0.30ms  2.36 11.16K  0.12M
 chk_4  0.76ms  0.56ms  0.37ms  2.07 11.16K  0.11M
 chk_5  0.74ms  0.55ms  0.35ms  2.08 11.16K  0.10M
 chk_6  0.74ms  0.55ms  0.36ms  2.05 11.16K  0.12M
 chk_7  0.74ms  0.56ms  0.37ms  2.03 11.16K  0.11M
   Avg  0.74  0.55  0.35
   Max  0.77  0.56  0.37
   Min  0.70  0.49  0.30
 Ratio  1.09  1.15  1.24
   Var  0.00  0.00  0.00
Profiling takes 0.222 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 19.035 ms
Partition 0 [0, 4) has cost: 19.035 ms
Partition 1 [4, 8) has cost: 17.465 ms
Partition 2 [8, 12) has cost: 17.465 ms
Partition 3 [12, 16) has cost: 17.465 ms
Partition 4 [16, 20) has cost: 17.465 ms
Partition 5 [20, 24) has cost: 17.465 ms
Partition 6 [24, 28) has cost: 17.465 ms
Partition 7 [28, 32) has cost: 15.911 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 20.404 ms
GPU 0, Compute+Comm Time: 11.101 ms, Bubble Time: 9.283 ms, Imbalance Overhead: 0.020 ms
GPU 1, Compute+Comm Time: 10.488 ms, Bubble Time: 9.349 ms, Imbalance Overhead: 0.567 ms
GPU 2, Compute+Comm Time: 10.488 ms, Bubble Time: 9.427 ms, Imbalance Overhead: 0.489 ms
GPU 3, Compute+Comm Time: 10.488 ms, Bubble Time: 9.456 ms, Imbalance Overhead: 0.460 ms
GPU 4, Compute+Comm Time: 10.488 ms, Bubble Time: 9.456 ms, Imbalance Overhead: 0.460 ms
GPU 5, Compute+Comm Time: 10.488 ms, Bubble Time: 9.528 ms, Imbalance Overhead: 0.388 ms
GPU 6, Compute+Comm Time: 10.488 ms, Bubble Time: 9.583 ms, Imbalance Overhead: 0.333 ms
GPU 7, Compute+Comm Time: 9.967 ms, Bubble Time: 9.711 ms, Imbalance Overhead: 0.726 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 32.222 ms
GPU 0, Compute+Comm Time: 15.532 ms, Bubble Time: 15.362 ms, Imbalance Overhead: 1.327 ms
GPU 1, Compute+Comm Time: 16.565 ms, Bubble Time: 15.148 ms, Imbalance Overhead: 0.508 ms
GPU 2, Compute+Comm Time: 16.565 ms, Bubble Time: 15.071 ms, Imbalance Overhead: 0.585 ms
GPU 3, Compute+Comm Time: 16.565 ms, Bubble Time: 14.951 ms, Imbalance Overhead: 0.705 ms
GPU 4, Compute+Comm Time: 16.565 ms, Bubble Time: 14.943 ms, Imbalance Overhead: 0.713 ms
GPU 5, Compute+Comm Time: 16.565 ms, Bubble Time: 14.896 ms, Imbalance Overhead: 0.760 ms
GPU 6, Compute+Comm Time: 16.565 ms, Bubble Time: 14.763 ms, Imbalance Overhead: 0.893 ms
GPU 7, Compute+Comm Time: 17.522 ms, Bubble Time: 14.642 ms, Imbalance Overhead: 0.057 ms
The estimated cost of the whole pipeline: 55.256 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 36.500 ms
Partition 0 [0, 8) has cost: 36.500 ms
Partition 1 [8, 16) has cost: 34.930 ms
Partition 2 [16, 24) has cost: 34.930 ms
Partition 3 [24, 32) has cost: 33.376 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.322 ms
GPU 0, Compute+Comm Time: 14.621 ms, Bubble Time: 10.701 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 14.314 ms, Bubble Time: 10.768 ms, Imbalance Overhead: 0.241 ms
GPU 2, Compute+Comm Time: 14.314 ms, Bubble Time: 10.846 ms, Imbalance Overhead: 0.163 ms
GPU 3, Compute+Comm Time: 14.053 ms, Bubble Time: 10.957 ms, Imbalance Overhead: 0.313 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 36.279 ms
GPU 0, Compute+Comm Time: 20.008 ms, Bubble Time: 15.747 ms, Imbalance Overhead: 0.524 ms
GPU 1, Compute+Comm Time: 20.524 ms, Bubble Time: 15.547 ms, Imbalance Overhead: 0.208 ms
GPU 2, Compute+Comm Time: 20.524 ms, Bubble Time: 15.405 ms, Imbalance Overhead: 0.349 ms
GPU 3, Compute+Comm Time: 20.993 ms, Bubble Time: 15.286 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 64.681 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 71.430 ms
Partition 0 [0, 16) has cost: 71.430 ms
Partition 1 [16, 32) has cost: 68.305 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.636 ms
GPU 0, Compute+Comm Time: 27.846 ms, Bubble Time: 13.791 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 27.561 ms, Bubble Time: 13.912 ms, Imbalance Overhead: 0.164 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 51.038 ms
GPU 0, Compute+Comm Time: 33.672 ms, Bubble Time: 17.067 ms, Imbalance Overhead: 0.299 ms
GPU 1, Compute+Comm Time: 34.179 ms, Bubble Time: 16.859 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 97.308 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 139.735 ms
Partition 0 [0, 32) has cost: 139.735 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 131.431 ms
GPU 0, Compute+Comm Time: 131.431 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 137.677 ms
GPU 0, Compute+Comm Time: 137.677 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 282.563 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 50:	Loss 1.8372	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.2580
	Epoch 100:	Loss 1.6292	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.6283	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.6267	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.6254	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.6217	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.6249	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.6214	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.6143	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 500:	Loss 1.6002	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 550:	Loss 1.5907	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 600:	Loss 1.5799	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 650:	Loss 1.5799	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 700:	Loss 1.5747	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 750:	Loss 1.5796	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 800:	Loss 1.5741	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 850:	Loss 1.5727	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 900:	Loss 1.5724	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 950:	Loss 1.5711	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5693	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5674	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5689	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5710	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5731	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5668	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5654	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5720	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5647	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5672	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5633	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5620	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5592	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5608	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5645	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5649	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5605	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5664	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5702	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1950:	Loss 1.5537	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2000:	Loss 1.5609	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5530	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2100:	Loss 1.5643	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5574	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5558	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2250:	Loss 1.5563	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5470	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2350:	Loss 1.5475	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2400:	Loss 1.5478	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2450:	Loss 1.5499	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2500:	Loss 1.5496	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5507	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2600:	Loss 1.5459	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2650:	Loss 1.5532	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5426	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5463	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5450	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2850:	Loss 1.5486	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2900:	Loss 1.5417	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 2950:	Loss 1.5411	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5507	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3050:	Loss 1.5426	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5437	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3150:	Loss 1.5399	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3200:	Loss 1.5469	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5425	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5416	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5443	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3400:	Loss 1.5490	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5374	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3500:	Loss 1.5397	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5455	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5403	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5401	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5388	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3750:	Loss 1.5375	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5389	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3850:	Loss 1.5399	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 3900:	Loss 1.5384	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5388	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5350	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4050:	Loss 1.5399	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5476	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5352	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4200:	Loss 1.5391	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4250:	Loss 1.5355	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5374	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5382	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4400:	Loss 1.5354	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4450:	Loss 1.5331	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5355	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5362	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5378	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5431	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5333	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 4750:	Loss 1.5323	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5452	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5501	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5345	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5342	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5384	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.156 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.151 ms (Max: 0.208, Min: 0.035, Sum: 1.205)
Cluster-Wide Average, Compute: 23.084 ms (Max: 23.559, Min: 21.811, Sum: 184.674)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.061)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.120)
Cluster-Wide Average, Communication-Graph: 123.731 ms (Max: 125.029, Min: 123.167, Sum: 989.848)
Cluster-Wide Average, Optimization: 3.009 ms (Max: 3.064, Min: 2.946, Sum: 24.069)
Cluster-Wide Average, Others: 5.607 ms (Max: 5.645, Min: 5.577, Sum: 44.855)
****** Breakdown Sum: 155.604 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.066 GB (Max: 6.428, Min: 6.001, Sum: 48.526)
Cluster-Wide Average, Graph-Level Communication Throughput: 42.687 Gbps (Max: 51.530, Min: 16.814, Sum: 341.499)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 4.583 GB
Weight-sync communication (cluster-wide, per-epoch): 0.018 GB
Total communication (cluster-wide, per-epoch): 4.601 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 99
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
