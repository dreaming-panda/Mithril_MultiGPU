Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
        It takes 0.022 seconds.
        It takes 0.022 seconds.
        It takes 0.022 seconds.
        It takes 0.029 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.096 seconds.
        It takes 0.096 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.112 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.012 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.296 Gbps (per GPU), 482.370 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.012 Gbps (per GPU), 480.100 Gbps (aggregated)
The layer-level communication performance: 60.003 Gbps (per GPU), 480.022 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.770 Gbps (per GPU), 478.161 Gbps (aggregated)
The layer-level communication performance: 59.734 Gbps (per GPU), 477.871 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.536 Gbps (per GPU), 476.285 Gbps (aggregated)
The layer-level communication performance: 59.457 Gbps (per GPU), 475.658 Gbps (aggregated)
The layer-level communication performance: 59.489 Gbps (per GPU), 475.908 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.435 Gbps (per GPU), 1259.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.441 Gbps (per GPU), 1259.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.435 Gbps (per GPU), 1259.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.402 Gbps (per GPU), 1259.220 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.435 Gbps (per GPU), 1259.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.459 Gbps (per GPU), 1259.669 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.991 Gbps (per GPU), 1279.926 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 2): 154.934 Gbps (per GPU), 1239.474 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.408 Gbps (per GPU), 819.267 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.406 Gbps (per GPU), 819.247 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.403 Gbps (per GPU), 819.227 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.409 Gbps (per GPU), 819.273 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.403 Gbps (per GPU), 819.220 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.410 Gbps (per GPU), 819.280 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.407 Gbps (per GPU), 819.254 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.411 Gbps (per GPU), 819.287 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.146 Gbps (per GPU), 257.172 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.147 Gbps (per GPU), 257.177 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.145 Gbps (per GPU), 257.160 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.147 Gbps (per GPU), 257.177 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.141 Gbps (per GPU), 257.132 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.144 Gbps (per GPU), 257.151 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.141 Gbps (per GPU), 257.132 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.143 Gbps (per GPU), 257.148 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.68ms  0.39ms  0.36ms  1.91  2.81K  0.03M
 chk_1  0.69ms  0.39ms  0.36ms  1.91  2.82K  0.03M
 chk_2  0.68ms  0.39ms  0.35ms  1.92  2.80K  0.03M
 chk_3  0.68ms  0.39ms  0.35ms  1.93  2.80K  0.03M
 chk_4  0.68ms  0.39ms  0.36ms  1.91  2.82K  0.03M
 chk_5  0.69ms  0.39ms  0.36ms  1.90  2.75K  0.03M
 chk_6  0.67ms  0.38ms  0.35ms  1.92  2.71K  0.03M
 chk_7  0.68ms  0.39ms  0.35ms  1.92  2.76K  0.03M
 chk_8  0.68ms  0.39ms  0.36ms  1.91  2.79K  0.03M
 chk_9  0.68ms  0.39ms  0.35ms  1.93  2.81K  0.03M
chk_10  0.68ms  0.38ms  0.35ms  1.95  2.81K  0.03M
chk_11  0.68ms  0.39ms  0.36ms  1.91  2.74K  0.03M
chk_12  0.68ms  0.39ms  0.36ms  1.92  2.76K  0.03M
chk_13  0.68ms  0.39ms  0.36ms  1.92  2.75K  0.03M
chk_14  0.68ms  0.38ms  0.35ms  1.92  2.81K  0.03M
chk_15  0.68ms  0.39ms  0.35ms  1.92  2.77K  0.03M
chk_16  0.68ms  0.38ms  0.35ms  1.93  2.78K  0.03M
chk_17  0.69ms  0.39ms  0.36ms  1.92  2.79K  0.03M
chk_18  0.69ms  0.39ms  0.36ms  1.93  2.82K  0.03M
chk_19  0.68ms  0.38ms  0.35ms  1.94  2.81K  0.03M
chk_20  0.68ms  0.39ms  0.36ms  1.92  2.77K  0.03M
chk_21  0.69ms  0.39ms  0.36ms  1.93  2.84K  0.02M
chk_22  0.68ms  0.39ms  0.35ms  1.92  2.78K  0.03M
chk_23  0.68ms  0.39ms  0.36ms  1.92  2.80K  0.03M
chk_24  0.68ms  0.39ms  0.36ms  1.91  2.80K  0.03M
chk_25  0.68ms  0.38ms  0.35ms  1.94  2.81K  0.03M
chk_26  0.68ms  0.38ms  0.35ms  1.93  2.81K  0.03M
chk_27  0.69ms  0.39ms  0.36ms  1.93  2.79K  0.03M
chk_28  0.69ms  0.39ms  0.36ms  1.93  2.77K  0.03M
chk_29  0.68ms  0.38ms  0.35ms  1.92  2.77K  0.03M
chk_30  0.69ms  0.39ms  0.36ms  1.93  2.80K  0.03M
chk_31  0.68ms  0.39ms  0.35ms  1.93  2.78K  0.03M
   Avg  0.68  0.39  0.35
   Max  0.69  0.39  0.36
   Min  0.67  0.38  0.35
 Ratio  1.03  1.03  1.04
   Var  0.00  0.00  0.00
Profiling takes 0.632 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 58.998 ms
Partition 0 [0, 4) has cost: 58.998 ms
Partition 1 [4, 8) has cost: 49.538 ms
Partition 2 [8, 12) has cost: 49.538 ms
Partition 3 [12, 16) has cost: 49.538 ms
Partition 4 [16, 20) has cost: 49.538 ms
Partition 5 [20, 24) has cost: 49.538 ms
Partition 6 [24, 28) has cost: 49.538 ms
Partition 7 [28, 32) has cost: 48.513 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 28.873 ms
GPU 0, Compute+Comm Time: 24.235 ms, Bubble Time: 4.638 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 21.170 ms, Bubble Time: 4.722 ms, Imbalance Overhead: 2.982 ms
GPU 2, Compute+Comm Time: 21.170 ms, Bubble Time: 4.816 ms, Imbalance Overhead: 2.887 ms
GPU 3, Compute+Comm Time: 21.170 ms, Bubble Time: 4.904 ms, Imbalance Overhead: 2.799 ms
GPU 4, Compute+Comm Time: 21.170 ms, Bubble Time: 4.993 ms, Imbalance Overhead: 2.710 ms
GPU 5, Compute+Comm Time: 21.170 ms, Bubble Time: 5.084 ms, Imbalance Overhead: 2.619 ms
GPU 6, Compute+Comm Time: 21.170 ms, Bubble Time: 5.180 ms, Imbalance Overhead: 2.523 ms
GPU 7, Compute+Comm Time: 19.773 ms, Bubble Time: 5.299 ms, Imbalance Overhead: 3.800 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 52.589 ms
GPU 0, Compute+Comm Time: 38.213 ms, Bubble Time: 9.672 ms, Imbalance Overhead: 4.705 ms
GPU 1, Compute+Comm Time: 37.841 ms, Bubble Time: 9.511 ms, Imbalance Overhead: 5.237 ms
GPU 2, Compute+Comm Time: 37.841 ms, Bubble Time: 9.319 ms, Imbalance Overhead: 5.429 ms
GPU 3, Compute+Comm Time: 37.841 ms, Bubble Time: 9.119 ms, Imbalance Overhead: 5.630 ms
GPU 4, Compute+Comm Time: 37.841 ms, Bubble Time: 8.922 ms, Imbalance Overhead: 5.826 ms
GPU 5, Compute+Comm Time: 37.841 ms, Bubble Time: 8.742 ms, Imbalance Overhead: 6.006 ms
GPU 6, Compute+Comm Time: 37.841 ms, Bubble Time: 8.545 ms, Imbalance Overhead: 6.203 ms
GPU 7, Compute+Comm Time: 44.236 ms, Bubble Time: 8.353 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 85.535 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 108.535 ms
Partition 0 [0, 8) has cost: 108.535 ms
Partition 1 [8, 16) has cost: 99.075 ms
Partition 2 [16, 24) has cost: 99.075 ms
Partition 3 [24, 32) has cost: 98.050 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.080 ms
GPU 0, Compute+Comm Time: 26.439 ms, Bubble Time: 4.641 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 24.906 ms, Bubble Time: 4.721 ms, Imbalance Overhead: 1.453 ms
GPU 2, Compute+Comm Time: 24.906 ms, Bubble Time: 4.824 ms, Imbalance Overhead: 1.350 ms
GPU 3, Compute+Comm Time: 24.204 ms, Bubble Time: 4.973 ms, Imbalance Overhead: 1.904 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 52.645 ms
GPU 0, Compute+Comm Time: 41.791 ms, Bubble Time: 8.417 ms, Imbalance Overhead: 2.437 ms
GPU 1, Compute+Comm Time: 41.607 ms, Bubble Time: 8.214 ms, Imbalance Overhead: 2.825 ms
GPU 2, Compute+Comm Time: 41.607 ms, Bubble Time: 8.015 ms, Imbalance Overhead: 3.023 ms
GPU 3, Compute+Comm Time: 44.808 ms, Bubble Time: 7.837 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 87.911 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 207.610 ms
Partition 0 [0, 16) has cost: 207.610 ms
Partition 1 [16, 32) has cost: 197.125 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 43.767 ms
GPU 0, Compute+Comm Time: 39.038 ms, Bubble Time: 4.729 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 37.920 ms, Bubble Time: 4.844 ms, Imbalance Overhead: 1.003 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 63.411 ms
GPU 0, Compute+Comm Time: 55.050 ms, Bubble Time: 7.050 ms, Imbalance Overhead: 1.311 ms
GPU 1, Compute+Comm Time: 56.558 ms, Bubble Time: 6.853 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 112.536 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 404.736 ms
Partition 0 [0, 32) has cost: 404.736 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 142.775 ms
GPU 0, Compute+Comm Time: 142.775 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 160.110 ms
GPU 0, Compute+Comm Time: 160.110 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 318.029 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 162)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [162, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [194, 226)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 98)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [226, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [98, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[162, 194)...
+++++++++ Node 1 initializing the weights for op[34, 66)...
+++++++++ Node 4 initializing the weights for op[130, 162)...
+++++++++ Node 2 initializing the weights for op[66, 98)...
+++++++++ Node 6 initializing the weights for op[194, 226)...
+++++++++ Node 3 initializing the weights for op[98, 130)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[226, 257)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 6.2548	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 50:	Loss 1.6074	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6145	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5948	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5921	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5882	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5848	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5880	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5901	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5868	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5855	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5852	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5836	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5867	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5857	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5843	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5833	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5837	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5817	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5798	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5743	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5721	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5676	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5628	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5544	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5533	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5499	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5453	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5433	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5426	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5413	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5411	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5409	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5390	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5390	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5384	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5374	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5373	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5371	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1950:	Loss 1.5340	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2000:	Loss 1.5339	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5348	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5331	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5326	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5339	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5321	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5308	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5300	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2400:	Loss 1.5283	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5275	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5268	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5265	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2600:	Loss 1.5269	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5258	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5225	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5234	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5262	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5215	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5284	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5208	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5228	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5206	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5206	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5210	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5203	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5172	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5193	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5174	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5268	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5237	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5172	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5121	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5092	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5080	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5047	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5039	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5019	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.4972	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.4972	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5005	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.4964	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.4912	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.4932	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.4928	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.4912	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.4902	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.4922	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.4903	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5054	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.4863	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.4877	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.4880	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.4891	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.4867	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.4904	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.4875	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.4873	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.4844	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.4932	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.4818	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.4805	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.102 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 18.751 ms (Max: 19.644, Min: 16.688, Sum: 150.011)
Cluster-Wide Average, Compute: 65.254 ms (Max: 73.221, Min: 62.279, Sum: 522.031)
Cluster-Wide Average, Communication-Layer: 9.069 ms (Max: 10.204, Min: 7.011, Sum: 72.553)
Cluster-Wide Average, Bubble-Imbalance: 7.244 ms (Max: 10.013, Min: 1.976, Sum: 57.955)
Cluster-Wide Average, Communication-Graph: 0.493 ms (Max: 0.625, Min: 0.406, Sum: 3.941)
Cluster-Wide Average, Optimization: 0.174 ms (Max: 0.179, Min: 0.169, Sum: 1.391)
Cluster-Wide Average, Others: 1.301 ms (Max: 4.165, Min: 0.878, Sum: 10.410)
****** Breakdown Sum: 102.286 ms ******
Cluster-Wide Average, GPU Memory Consumption: 2.563 GB (Max: 3.278, Min: 2.233, Sum: 20.503)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 54.129 Gbps (Max: 60.376, Min: 40.433, Sum: 433.032)
Layer-level communication (cluster-wide, per-epoch): 0.465 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.465 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 0
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
