Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
        It takes 0.022 seconds.
        It takes 0.026 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
Building the Feature Vector...
        It takes 0.024 seconds.
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Feature Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.095 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
Building the Label Vector...
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.109 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.108 seconds.
        It takes 0.008 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.921 Gbps (per GPU), 487.365 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.626 Gbps (per GPU), 485.009 Gbps (aggregated)
The layer-level communication performance: 60.622 Gbps (per GPU), 484.978 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.391 Gbps (per GPU), 483.128 Gbps (aggregated)
The layer-level communication performance: 60.350 Gbps (per GPU), 482.802 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.111 Gbps (per GPU), 480.888 Gbps (aggregated)
The layer-level communication performance: 60.063 Gbps (per GPU), 480.505 Gbps (aggregated)
The layer-level communication performance: 60.030 Gbps (per GPU), 480.239 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.899 Gbps (per GPU), 1247.191 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.053 Gbps (per GPU), 1248.421 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.919 Gbps (per GPU), 1247.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.974 Gbps (per GPU), 1247.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.916 Gbps (per GPU), 1247.330 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.067 Gbps (per GPU), 1248.537 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.969 Gbps (per GPU), 1247.753 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 153.849 Gbps (per GPU), 1230.791 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.281 Gbps (per GPU), 810.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.282 Gbps (per GPU), 810.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.282 Gbps (per GPU), 810.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.283 Gbps (per GPU), 810.265 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.282 Gbps (per GPU), 810.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.283 Gbps (per GPU), 810.265 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.282 Gbps (per GPU), 810.259 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.285 Gbps (per GPU), 810.278 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.543 Gbps (per GPU), 244.347 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.543 Gbps (per GPU), 244.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.542 Gbps (per GPU), 244.335 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.542 Gbps (per GPU), 244.338 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.542 Gbps (per GPU), 244.332 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.542 Gbps (per GPU), 244.339 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.542 Gbps (per GPU), 244.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.543 Gbps (per GPU), 244.341 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.70ms  0.40ms  0.38ms  1.86  2.81K  0.03M
 chk_1  0.70ms  0.40ms  0.38ms  1.86  2.82K  0.03M
 chk_2  0.70ms  0.40ms  0.37ms  1.88  2.80K  0.03M
 chk_3  0.88ms  0.40ms  0.38ms  2.33  2.80K  0.03M
 chk_4  0.70ms  0.40ms  0.38ms  1.86  2.82K  0.03M
 chk_5  0.70ms  0.40ms  0.38ms  1.84  2.75K  0.03M
 chk_6  0.68ms  0.39ms  0.37ms  1.86  2.71K  0.03M
 chk_7  0.69ms  0.40ms  0.37ms  1.86  2.76K  0.03M
 chk_8  0.69ms  0.40ms  0.38ms  1.85  2.79K  0.03M
 chk_9  0.70ms  0.40ms  0.37ms  1.86  2.81K  0.03M
chk_10  0.70ms  0.40ms  0.37ms  1.88  2.81K  0.03M
chk_11  0.69ms  0.41ms  0.38ms  1.85  2.74K  0.03M
chk_12  0.70ms  0.40ms  0.38ms  1.85  2.76K  0.03M
chk_13  0.69ms  0.40ms  0.37ms  1.85  2.75K  0.03M
chk_14  0.70ms  0.40ms  0.37ms  1.86  2.81K  0.03M
chk_15  0.70ms  0.40ms  0.37ms  1.86  2.77K  0.03M
chk_16  0.69ms  0.40ms  0.37ms  1.86  2.78K  0.03M
chk_17  0.70ms  0.40ms  0.38ms  1.86  2.79K  0.03M
chk_18  0.70ms  0.40ms  0.37ms  1.88  2.82K  0.03M
chk_19  0.69ms  0.39ms  0.37ms  1.88  2.81K  0.03M
chk_20  0.70ms  0.40ms  0.37ms  1.87  2.77K  0.03M
chk_21  0.70ms  0.40ms  0.37ms  1.88  2.84K  0.02M
chk_22  0.70ms  0.40ms  0.38ms  1.86  2.78K  0.03M
chk_23  0.70ms  0.40ms  0.37ms  1.87  2.80K  0.03M
chk_24  0.70ms  0.40ms  0.37ms  1.86  2.80K  0.03M
chk_25  0.70ms  0.40ms  0.37ms  1.88  2.81K  0.03M
chk_26  0.70ms  0.40ms  0.37ms  1.88  2.81K  0.03M
chk_27  0.70ms  0.40ms  0.38ms  1.87  2.79K  0.03M
chk_28  0.70ms  0.40ms  0.38ms  1.86  2.77K  0.03M
chk_29  0.69ms  0.40ms  0.37ms  1.86  2.77K  0.03M
chk_30  0.70ms  0.40ms  0.38ms  1.86  2.80K  0.03M
chk_31  0.70ms  0.40ms  0.37ms  1.87  2.78K  0.03M
   Avg  0.70  0.40  0.37
   Max  0.88  0.41  0.38
   Min  0.68  0.39  0.37
 Ratio  1.28  1.03  1.04
   Var  0.00  0.00  0.00
Profiling takes 0.656 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 60.957 ms
Partition 0 [0, 4) has cost: 60.957 ms
Partition 1 [4, 8) has cost: 51.280 ms
Partition 2 [8, 12) has cost: 51.280 ms
Partition 3 [12, 16) has cost: 51.280 ms
Partition 4 [16, 20) has cost: 51.280 ms
Partition 5 [20, 24) has cost: 51.280 ms
Partition 6 [24, 28) has cost: 51.280 ms
Partition 7 [28, 32) has cost: 50.437 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 29.548 ms
GPU 0, Compute+Comm Time: 24.815 ms, Bubble Time: 4.734 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 21.720 ms, Bubble Time: 4.827 ms, Imbalance Overhead: 3.001 ms
GPU 2, Compute+Comm Time: 21.720 ms, Bubble Time: 4.923 ms, Imbalance Overhead: 2.906 ms
GPU 3, Compute+Comm Time: 21.720 ms, Bubble Time: 5.014 ms, Imbalance Overhead: 2.814 ms
GPU 4, Compute+Comm Time: 21.720 ms, Bubble Time: 5.109 ms, Imbalance Overhead: 2.719 ms
GPU 5, Compute+Comm Time: 21.720 ms, Bubble Time: 5.209 ms, Imbalance Overhead: 2.619 ms
GPU 6, Compute+Comm Time: 21.720 ms, Bubble Time: 5.309 ms, Imbalance Overhead: 2.520 ms
GPU 7, Compute+Comm Time: 20.382 ms, Bubble Time: 5.431 ms, Imbalance Overhead: 3.736 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 54.159 ms
GPU 0, Compute+Comm Time: 39.431 ms, Bubble Time: 10.080 ms, Imbalance Overhead: 4.648 ms
GPU 1, Compute+Comm Time: 38.936 ms, Bubble Time: 9.929 ms, Imbalance Overhead: 5.293 ms
GPU 2, Compute+Comm Time: 38.936 ms, Bubble Time: 9.743 ms, Imbalance Overhead: 5.479 ms
GPU 3, Compute+Comm Time: 38.936 ms, Bubble Time: 9.549 ms, Imbalance Overhead: 5.674 ms
GPU 4, Compute+Comm Time: 38.936 ms, Bubble Time: 9.192 ms, Imbalance Overhead: 6.031 ms
GPU 5, Compute+Comm Time: 38.936 ms, Bubble Time: 9.015 ms, Imbalance Overhead: 6.208 ms
GPU 6, Compute+Comm Time: 38.936 ms, Bubble Time: 8.826 ms, Imbalance Overhead: 6.397 ms
GPU 7, Compute+Comm Time: 45.518 ms, Bubble Time: 8.640 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 87.892 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 112.236 ms
Partition 0 [0, 8) has cost: 112.236 ms
Partition 1 [8, 16) has cost: 102.560 ms
Partition 2 [16, 24) has cost: 102.560 ms
Partition 3 [24, 32) has cost: 101.717 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.756 ms
GPU 0, Compute+Comm Time: 27.022 ms, Bubble Time: 4.734 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 25.472 ms, Bubble Time: 4.820 ms, Imbalance Overhead: 1.464 ms
GPU 2, Compute+Comm Time: 25.472 ms, Bubble Time: 4.927 ms, Imbalance Overhead: 1.357 ms
GPU 3, Compute+Comm Time: 24.801 ms, Bubble Time: 5.074 ms, Imbalance Overhead: 1.881 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 54.186 ms
GPU 0, Compute+Comm Time: 42.977 ms, Bubble Time: 8.630 ms, Imbalance Overhead: 2.580 ms
GPU 1, Compute+Comm Time: 42.728 ms, Bubble Time: 8.441 ms, Imbalance Overhead: 3.016 ms
GPU 2, Compute+Comm Time: 42.728 ms, Bubble Time: 8.249 ms, Imbalance Overhead: 3.209 ms
GPU 3, Compute+Comm Time: 46.117 ms, Bubble Time: 8.069 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 90.239 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 214.796 ms
Partition 0 [0, 16) has cost: 214.796 ms
Partition 1 [16, 32) has cost: 204.277 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 44.534 ms
GPU 0, Compute+Comm Time: 39.725 ms, Bubble Time: 4.809 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.613 ms, Bubble Time: 4.939 ms, Imbalance Overhead: 0.983 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 65.082 ms
GPU 0, Compute+Comm Time: 56.378 ms, Bubble Time: 7.194 ms, Imbalance Overhead: 1.510 ms
GPU 1, Compute+Comm Time: 58.036 ms, Bubble Time: 7.046 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 115.097 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 419.073 ms
Partition 0 [0, 32) has cost: 419.073 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 149.945 ms
GPU 0, Compute+Comm Time: 149.945 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 168.076 ms
GPU 0, Compute+Comm Time: 168.076 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 333.922 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 162)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [162, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [194, 226)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 98)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [226, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [98, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[162, 194)...
+++++++++ Node 4 initializing the weights for op[130, 162)...
+++++++++ Node 6 initializing the weights for op[194, 226)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[34, 66)...
+++++++++ Node 2 initializing the weights for op[66, 98)...
+++++++++ Node 3 initializing the weights for op[98, 130)...
+++++++++ Node 7 initializing the weights for op[226, 257)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.6911	TrainAcc 0.0812	ValidAcc 0.0807	TestAcc 0.0818	BestValid 0.0807
	Epoch 50:	Loss 1.6193	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.5938	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5904	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5896	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5875	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5885	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5858	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5866	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5872	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5831	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5841	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5857	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5846	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5790	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5722	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5667	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5615	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5565	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5479	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5479	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5444	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5427	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5394	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5404	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5388	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5383	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5377	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5372	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5381	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5364	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5360	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5354	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5347	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5339	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5335	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5326	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5331	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5319	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1950:	Loss 1.5314	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2000:	Loss 1.5309	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5307	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5311	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5310	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5307	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5291	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5277	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5293	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2400:	Loss 1.5288	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5285	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5280	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5271	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2600:	Loss 1.5273	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5275	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5257	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5236	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5244	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5256	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5242	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5262	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5204	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5229	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5197	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5205	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5191	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5200	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5198	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5187	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5186	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5178	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5190	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5164	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5142	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5165	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5119	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5162	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5126	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5149	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5171	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5115	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5120	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5137	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5099	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5094	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5084	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5092	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5092	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5168	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5099	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5072	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5055	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5083	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5029	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5008	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5013	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.5009	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5026	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5037	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5082	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5080	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5057	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.105 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 19.087 ms (Max: 20.164, Min: 17.347, Sum: 152.692)
Cluster-Wide Average, Compute: 66.296 ms (Max: 76.320, Min: 62.132, Sum: 530.368)
Cluster-Wide Average, Communication-Layer: 9.020 ms (Max: 9.952, Min: 7.156, Sum: 72.163)
Cluster-Wide Average, Bubble-Imbalance: 8.824 ms (Max: 12.477, Min: 1.619, Sum: 70.594)
Cluster-Wide Average, Communication-Graph: 0.519 ms (Max: 0.562, Min: 0.409, Sum: 4.149)
Cluster-Wide Average, Optimization: 0.176 ms (Max: 0.181, Min: 0.169, Sum: 1.404)
Cluster-Wide Average, Others: 1.297 ms (Max: 4.153, Min: 0.871, Sum: 10.374)
****** Breakdown Sum: 105.218 ms ******
Cluster-Wide Average, GPU Memory Consumption: 2.563 GB (Max: 3.278, Min: 2.233, Sum: 20.503)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 54.442 Gbps (Max: 61.500, Min: 39.868, Sum: 435.538)
Layer-level communication (cluster-wide, per-epoch): 0.465 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.465 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 49
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
