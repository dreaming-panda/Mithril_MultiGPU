Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...Building the CSR structure...
Building the CSR structure...

Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.030 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.029 seconds.
        It takes 0.019 seconds.
        It takes 0.029 seconds.
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
Number of vertices per chunk: 2790
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 54.482 Gbps (per GPU), 435.852 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.245 Gbps (per GPU), 433.964 Gbps (aggregated)
The layer-level communication performance: 54.241 Gbps (per GPU), 433.929 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.037 Gbps (per GPU), 432.295 Gbps (aggregated)
The layer-level communication performance: 54.010 Gbps (per GPU), 432.081 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 53.835 Gbps (per GPU), 430.680 Gbps (aggregated)
The layer-level communication performance: 53.797 Gbps (per GPU), 430.378 Gbps (aggregated)
The layer-level communication performance: 53.773 Gbps (per GPU), 430.185 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.160 Gbps (per GPU), 1249.281 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.169 Gbps (per GPU), 1249.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.169 Gbps (per GPU), 1249.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.172 Gbps (per GPU), 1249.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.172 Gbps (per GPU), 1249.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.184 Gbps (per GPU), 1249.470 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.175 Gbps (per GPU), 1249.401 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.172 Gbps (per GPU), 1249.379 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 97.568 Gbps (per GPU), 780.547 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.575 Gbps (per GPU), 780.602 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.568 Gbps (per GPU), 780.548 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.568 Gbps (per GPU), 780.542 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.574 Gbps (per GPU), 780.589 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.572 Gbps (per GPU), 780.578 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.552 Gbps (per GPU), 780.414 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.525 Gbps (per GPU), 780.201 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.204 Gbps (per GPU), 273.632 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.203 Gbps (per GPU), 273.625 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.206 Gbps (per GPU), 273.644 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.201 Gbps (per GPU), 273.612 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.201 Gbps (per GPU), 273.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.200 Gbps (per GPU), 273.600 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.198 Gbps (per GPU), 273.581 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.196 Gbps (per GPU), 273.569 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.69ms  0.40ms  0.38ms  1.85  2.81K  0.03M
 chk_1  0.70ms  0.40ms  0.38ms  1.86  2.82K  0.03M
 chk_2  0.70ms  0.40ms  0.37ms  1.87  2.80K  0.03M
 chk_3  0.70ms  0.40ms  0.38ms  1.87  2.80K  0.03M
 chk_4  0.70ms  0.40ms  0.38ms  1.85  2.82K  0.03M
 chk_5  0.70ms  0.41ms  0.38ms  1.84  2.75K  0.03M
 chk_6  0.71ms  0.40ms  0.37ms  1.91  2.71K  0.03M
 chk_7  0.70ms  0.40ms  0.38ms  1.86  2.76K  0.03M
 chk_8  0.70ms  0.40ms  0.38ms  1.86  2.79K  0.03M
 chk_9  0.70ms  0.40ms  0.38ms  1.86  2.81K  0.03M
chk_10  0.70ms  0.40ms  0.37ms  1.89  2.81K  0.03M
chk_11  0.70ms  0.41ms  0.38ms  1.85  2.74K  0.03M
chk_12  0.70ms  0.40ms  0.38ms  1.85  2.76K  0.03M
chk_13  0.70ms  0.40ms  0.38ms  1.86  2.75K  0.03M
chk_14  0.70ms  0.40ms  0.38ms  1.87  2.81K  0.03M
chk_15  0.70ms  0.40ms  0.38ms  1.86  2.77K  0.03M
chk_16  0.70ms  0.40ms  0.37ms  1.86  2.78K  0.03M
chk_17  0.70ms  0.41ms  0.38ms  1.86  2.79K  0.03M
chk_18  0.70ms  0.41ms  0.38ms  1.87  2.82K  0.03M
chk_19  0.69ms  0.40ms  0.37ms  1.87  2.81K  0.03M
chk_20  0.70ms  0.41ms  0.38ms  1.86  2.77K  0.03M
chk_21  0.70ms  0.40ms  0.38ms  1.87  2.84K  0.02M
chk_22  0.70ms  0.40ms  0.38ms  1.86  2.78K  0.03M
chk_23  0.70ms  0.40ms  0.38ms  1.85  2.80K  0.03M
chk_24  0.70ms  0.40ms  0.38ms  1.85  2.80K  0.03M
chk_25  0.70ms  0.40ms  0.37ms  1.88  2.81K  0.03M
chk_26  0.70ms  0.40ms  0.37ms  1.87  2.81K  0.03M
chk_27  0.70ms  0.40ms  0.38ms  1.86  2.79K  0.03M
chk_28  0.70ms  0.41ms  0.38ms  1.86  2.77K  0.03M
chk_29  0.70ms  0.40ms  0.38ms  1.86  2.77K  0.03M
chk_30  0.70ms  0.41ms  0.38ms  1.87  2.80K  0.03M
chk_31  0.70ms  0.41ms  0.38ms  1.86  2.78K  0.03M
   Avg  0.70  0.40  0.38
   Max  0.71  0.41  0.38
   Min  0.69  0.40  0.37
 Ratio  1.01  1.03  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.652 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 61.093 ms
Partition 0 [0, 4) has cost: 61.093 ms
Partition 1 [4, 8) has cost: 51.576 ms
Partition 2 [8, 12) has cost: 51.576 ms
Partition 3 [12, 16) has cost: 51.576 ms
Partition 4 [16, 20) has cost: 51.576 ms
Partition 5 [20, 24) has cost: 51.576 ms
Partition 6 [24, 28) has cost: 51.576 ms
Partition 7 [28, 32) has cost: 50.709 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.349 ms
GPU 0, Compute+Comm Time: 25.478 ms, Bubble Time: 4.871 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 22.380 ms, Bubble Time: 4.960 ms, Imbalance Overhead: 3.009 ms
GPU 2, Compute+Comm Time: 22.380 ms, Bubble Time: 5.057 ms, Imbalance Overhead: 2.912 ms
GPU 3, Compute+Comm Time: 22.380 ms, Bubble Time: 5.150 ms, Imbalance Overhead: 2.819 ms
GPU 4, Compute+Comm Time: 22.380 ms, Bubble Time: 5.242 ms, Imbalance Overhead: 2.727 ms
GPU 5, Compute+Comm Time: 22.380 ms, Bubble Time: 5.340 ms, Imbalance Overhead: 2.629 ms
GPU 6, Compute+Comm Time: 22.380 ms, Bubble Time: 5.441 ms, Imbalance Overhead: 2.528 ms
GPU 7, Compute+Comm Time: 21.031 ms, Bubble Time: 5.564 ms, Imbalance Overhead: 3.754 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 54.898 ms
GPU 0, Compute+Comm Time: 40.162 ms, Bubble Time: 10.080 ms, Imbalance Overhead: 4.655 ms
GPU 1, Compute+Comm Time: 39.681 ms, Bubble Time: 9.909 ms, Imbalance Overhead: 5.308 ms
GPU 2, Compute+Comm Time: 39.681 ms, Bubble Time: 9.727 ms, Imbalance Overhead: 5.490 ms
GPU 3, Compute+Comm Time: 39.681 ms, Bubble Time: 9.532 ms, Imbalance Overhead: 5.685 ms
GPU 4, Compute+Comm Time: 39.681 ms, Bubble Time: 9.349 ms, Imbalance Overhead: 5.867 ms
GPU 5, Compute+Comm Time: 39.681 ms, Bubble Time: 9.173 ms, Imbalance Overhead: 6.044 ms
GPU 6, Compute+Comm Time: 39.681 ms, Bubble Time: 8.979 ms, Imbalance Overhead: 6.238 ms
GPU 7, Compute+Comm Time: 46.099 ms, Bubble Time: 8.798 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 89.509 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 112.670 ms
Partition 0 [0, 8) has cost: 112.670 ms
Partition 1 [8, 16) has cost: 103.153 ms
Partition 2 [16, 24) has cost: 103.153 ms
Partition 3 [24, 32) has cost: 102.286 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 32.532 ms
GPU 0, Compute+Comm Time: 27.681 ms, Bubble Time: 4.851 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 26.132 ms, Bubble Time: 4.950 ms, Imbalance Overhead: 1.450 ms
GPU 2, Compute+Comm Time: 26.132 ms, Bubble Time: 5.059 ms, Imbalance Overhead: 1.342 ms
GPU 3, Compute+Comm Time: 25.456 ms, Bubble Time: 5.206 ms, Imbalance Overhead: 1.871 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 54.914 ms
GPU 0, Compute+Comm Time: 43.726 ms, Bubble Time: 8.758 ms, Imbalance Overhead: 2.430 ms
GPU 1, Compute+Comm Time: 43.489 ms, Bubble Time: 8.564 ms, Imbalance Overhead: 2.861 ms
GPU 2, Compute+Comm Time: 43.489 ms, Bubble Time: 8.379 ms, Imbalance Overhead: 3.046 ms
GPU 3, Compute+Comm Time: 46.705 ms, Bubble Time: 8.209 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 91.819 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 215.822 ms
Partition 0 [0, 16) has cost: 215.822 ms
Partition 1 [16, 32) has cost: 205.438 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.011 ms
GPU 0, Compute+Comm Time: 41.044 ms, Bubble Time: 4.967 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 39.931 ms, Bubble Time: 5.127 ms, Imbalance Overhead: 0.953 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 66.477 ms
GPU 0, Compute+Comm Time: 57.774 ms, Bubble Time: 7.381 ms, Imbalance Overhead: 1.322 ms
GPU 1, Compute+Comm Time: 59.270 ms, Bubble Time: 7.207 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 118.113 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 421.261 ms
Partition 0 [0, 32) has cost: 421.261 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 135.868 ms
GPU 0, Compute+Comm Time: 135.868 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 153.913 ms
GPU 0, Compute+Comm Time: 153.913 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 304.271 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 162)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [194, 226)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 98)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [226, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [98, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [162, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[162, 194)...
+++++++++ Node 1 initializing the weights for op[34, 66)...
+++++++++ Node 6 initializing the weights for op[194, 226)...
+++++++++ Node 3 initializing the weights for op[98, 130)...
+++++++++ Node 7 initializing the weights for op[226, 257)...
+++++++++ Node 2 initializing the weights for op[66, 98)...
+++++++++ Node 4 initializing the weights for op[130, 162)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 5.3702	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 50:	Loss 1.6190	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6211	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5974	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5935	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5891	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5893	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5882	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5861	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5859	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5845	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5827	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5780	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5768	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5785	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5815	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5783	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5754	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5778	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5747	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5656	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5603	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5557	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5546	TrainAcc 0.3970	ValidAcc 0.3995	TestAcc 0.3993	BestValid 0.4239
	Epoch 1200:	Loss 1.5494	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5485	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5461	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5452	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5436	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5416	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5401	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5404	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5397	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5383	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5411	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5362	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5380	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5376	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5361	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1950:	Loss 1.5354	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2000:	Loss 1.5356	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5337	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5348	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5341	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5315	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5327	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5339	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5314	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2400:	Loss 1.5313	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5308	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5288	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5289	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2600:	Loss 1.5279	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5294	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5299	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5280	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5266	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5247	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5248	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5209	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5234	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5221	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5243	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5221	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5235	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5228	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5218	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5197	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5164	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5197	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5158	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5183	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5205	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5165	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5167	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5217	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5170	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5140	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5181	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5134	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5125	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5135	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5130	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5116	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5145	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5086	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5094	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5096	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5253	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5461	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5359	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5199	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5123	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5170	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5105	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.5113	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5101	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5084	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5162	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5094	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5119	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.105 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 18.897 ms (Max: 19.996, Min: 17.368, Sum: 151.175)
Cluster-Wide Average, Compute: 66.023 ms (Max: 75.810, Min: 63.266, Sum: 528.181)
Cluster-Wide Average, Communication-Layer: 9.204 ms (Max: 10.129, Min: 7.051, Sum: 73.633)
Cluster-Wide Average, Bubble-Imbalance: 8.517 ms (Max: 11.201, Min: 1.697, Sum: 68.139)
Cluster-Wide Average, Communication-Graph: 0.504 ms (Max: 0.566, Min: 0.436, Sum: 4.036)
Cluster-Wide Average, Optimization: 0.175 ms (Max: 0.179, Min: 0.165, Sum: 1.400)
Cluster-Wide Average, Others: 1.313 ms (Max: 4.088, Min: 0.900, Sum: 10.505)
****** Breakdown Sum: 104.634 ms ******
Cluster-Wide Average, GPU Memory Consumption: 2.563 GB (Max: 3.278, Min: 2.233, Sum: 20.503)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 53.329 Gbps (Max: 60.567, Min: 40.053, Sum: 426.631)
Layer-level communication (cluster-wide, per-epoch): 0.465 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.465 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 1699
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
