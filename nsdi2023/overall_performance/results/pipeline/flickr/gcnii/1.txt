Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.026 seconds.
        It takes 0.017 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.024 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.095 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.113 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.680 Gbps (per GPU), 485.443 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.382 Gbps (per GPU), 483.056 Gbps (aggregated)
The layer-level communication performance: 60.375 Gbps (per GPU), 482.999 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.121 Gbps (per GPU), 480.968 Gbps (aggregated)
The layer-level communication performance: 60.150 Gbps (per GPU), 481.197 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.908 Gbps (per GPU), 479.267 Gbps (aggregated)
The layer-level communication performance: 59.859 Gbps (per GPU), 478.869 Gbps (aggregated)
The layer-level communication performance: 59.823 Gbps (per GPU), 478.580 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.227 Gbps (per GPU), 1241.814 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.244 Gbps (per GPU), 1241.952 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.227 Gbps (per GPU), 1241.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.192 Gbps (per GPU), 1241.538 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.238 Gbps (per GPU), 1241.906 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.258 Gbps (per GPU), 1242.067 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.235 Gbps (per GPU), 1241.883 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.189 Gbps (per GPU), 1241.515 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 98.742 Gbps (per GPU), 789.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.747 Gbps (per GPU), 789.975 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.744 Gbps (per GPU), 789.950 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.748 Gbps (per GPU), 789.981 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.747 Gbps (per GPU), 789.975 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.710 Gbps (per GPU), 789.683 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 4): 97.933 Gbps (per GPU), 783.463 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 4): 97.934 Gbps (per GPU), 783.476 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.962 Gbps (per GPU), 247.698 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.962 Gbps (per GPU), 247.697 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.962 Gbps (per GPU), 247.694 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.962 Gbps (per GPU), 247.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.959 Gbps (per GPU), 247.675 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.957 Gbps (per GPU), 247.660 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.961 Gbps (per GPU), 247.691 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.960 Gbps (per GPU), 247.678 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.46ms  0.56ms  1.36  2.81K  0.03M
 chk_1  0.41ms  0.37ms  0.56ms  1.51  2.82K  0.03M
 chk_2  0.41ms  0.39ms  0.52ms  1.35  2.80K  0.03M
 chk_3  0.41ms  0.39ms  0.53ms  1.35  2.80K  0.03M
 chk_4  0.41ms  0.39ms  0.52ms  1.34  2.82K  0.03M
 chk_5  0.41ms  0.39ms  0.53ms  1.34  2.75K  0.03M
 chk_6  0.42ms  0.39ms  0.52ms  1.34  2.71K  0.03M
 chk_7  0.41ms  0.39ms  0.52ms  1.33  2.76K  0.03M
 chk_8  0.45ms  0.39ms  0.52ms  1.35  2.79K  0.03M
 chk_9  0.49ms  0.39ms  0.52ms  1.35  2.81K  0.03M
chk_10  0.41ms  0.39ms  0.52ms  1.34  2.81K  0.03M
chk_11  0.41ms  0.39ms  0.53ms  1.34  2.74K  0.03M
chk_12  0.41ms  0.39ms  0.52ms  1.33  2.76K  0.03M
chk_13  0.49ms  0.39ms  0.53ms  1.34  2.75K  0.03M
chk_14  0.42ms  0.46ms  0.52ms  1.25  2.81K  0.03M
chk_15  0.43ms  0.36ms  0.52ms  1.46  2.77K  0.03M
chk_16  0.43ms  0.38ms  0.52ms  1.39  2.78K  0.03M
chk_17  0.44ms  0.39ms  0.53ms  1.35  2.79K  0.03M
chk_18  0.43ms  0.39ms  0.53ms  1.34  2.82K  0.03M
chk_19  0.44ms  0.38ms  0.52ms  1.35  2.81K  0.03M
chk_20  0.42ms  0.39ms  0.53ms  1.35  2.77K  0.03M
chk_21  0.44ms  0.39ms  0.53ms  1.34  2.84K  0.02M
chk_22  0.42ms  0.39ms  0.52ms  1.34  2.78K  0.03M
chk_23  0.41ms  0.39ms  0.52ms  1.35  2.80K  0.03M
chk_24  0.41ms  0.39ms  0.52ms  1.34  2.80K  0.03M
chk_25  0.41ms  0.39ms  0.52ms  1.35  2.81K  0.03M
chk_26  0.41ms  0.39ms  0.52ms  1.35  2.81K  0.03M
chk_27  0.41ms  0.40ms  0.53ms  1.33  2.79K  0.03M
chk_28  0.41ms  0.39ms  0.52ms  1.34  2.77K  0.03M
chk_29  0.41ms  0.39ms  0.52ms  1.35  2.77K  0.03M
chk_30  0.41ms  0.39ms  0.53ms  1.34  2.80K  0.03M
chk_31  0.41ms  0.39ms  0.53ms  1.34  2.78K  0.03M
   Avg  0.42  0.39  0.53
   Max  0.49  0.46  0.56
   Min  0.41  0.36  0.52
 Ratio  1.20  1.28  1.09
   Var  0.00  0.00  0.00
Profiling takes 0.647 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 62.864 ms
Partition 0 [0, 4) has cost: 51.282 ms
Partition 1 [4, 8) has cost: 50.291 ms
Partition 2 [8, 12) has cost: 50.291 ms
Partition 3 [12, 16) has cost: 50.291 ms
Partition 4 [16, 20) has cost: 50.291 ms
Partition 5 [20, 24) has cost: 50.291 ms
Partition 6 [24, 29) has cost: 62.864 ms
Partition 7 [29, 33) has cost: 54.569 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.766 ms
GPU 0, Compute+Comm Time: 21.997 ms, Bubble Time: 5.655 ms, Imbalance Overhead: 4.115 ms
GPU 1, Compute+Comm Time: 22.046 ms, Bubble Time: 5.594 ms, Imbalance Overhead: 4.126 ms
GPU 2, Compute+Comm Time: 22.046 ms, Bubble Time: 5.535 ms, Imbalance Overhead: 4.185 ms
GPU 3, Compute+Comm Time: 22.046 ms, Bubble Time: 5.491 ms, Imbalance Overhead: 4.229 ms
GPU 4, Compute+Comm Time: 22.046 ms, Bubble Time: 5.455 ms, Imbalance Overhead: 4.266 ms
GPU 5, Compute+Comm Time: 22.046 ms, Bubble Time: 5.402 ms, Imbalance Overhead: 4.318 ms
GPU 6, Compute+Comm Time: 26.381 ms, Bubble Time: 5.357 ms, Imbalance Overhead: 0.029 ms
GPU 7, Compute+Comm Time: 22.713 ms, Bubble Time: 5.584 ms, Imbalance Overhead: 3.469 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 55.491 ms
GPU 0, Compute+Comm Time: 41.269 ms, Bubble Time: 9.814 ms, Imbalance Overhead: 4.408 ms
GPU 1, Compute+Comm Time: 45.896 ms, Bubble Time: 9.435 ms, Imbalance Overhead: 0.160 ms
GPU 2, Compute+Comm Time: 37.658 ms, Bubble Time: 9.503 ms, Imbalance Overhead: 8.330 ms
GPU 3, Compute+Comm Time: 37.658 ms, Bubble Time: 9.569 ms, Imbalance Overhead: 8.264 ms
GPU 4, Compute+Comm Time: 37.658 ms, Bubble Time: 9.621 ms, Imbalance Overhead: 8.212 ms
GPU 5, Compute+Comm Time: 37.658 ms, Bubble Time: 9.687 ms, Imbalance Overhead: 8.146 ms
GPU 6, Compute+Comm Time: 37.658 ms, Bubble Time: 9.761 ms, Imbalance Overhead: 8.072 ms
GPU 7, Compute+Comm Time: 38.699 ms, Bubble Time: 9.837 ms, Imbalance Overhead: 6.956 ms
The estimated cost of the whole pipeline: 91.620 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 113.155 ms
Partition 0 [0, 8) has cost: 101.573 ms
Partition 1 [8, 16) has cost: 100.582 ms
Partition 2 [16, 25) has cost: 113.155 ms
Partition 3 [25, 33) has cost: 104.860 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 34.018 ms
GPU 0, Compute+Comm Time: 25.654 ms, Bubble Time: 5.390 ms, Imbalance Overhead: 2.973 ms
GPU 1, Compute+Comm Time: 26.131 ms, Bubble Time: 5.154 ms, Imbalance Overhead: 2.732 ms
GPU 2, Compute+Comm Time: 28.809 ms, Bubble Time: 5.155 ms, Imbalance Overhead: 0.054 ms
GPU 3, Compute+Comm Time: 26.437 ms, Bubble Time: 5.502 ms, Imbalance Overhead: 2.079 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 55.623 ms
GPU 0, Compute+Comm Time: 43.727 ms, Bubble Time: 9.155 ms, Imbalance Overhead: 2.742 ms
GPU 1, Compute+Comm Time: 46.613 ms, Bubble Time: 8.557 ms, Imbalance Overhead: 0.453 ms
GPU 2, Compute+Comm Time: 41.957 ms, Bubble Time: 8.628 ms, Imbalance Overhead: 5.038 ms
GPU 3, Compute+Comm Time: 42.067 ms, Bubble Time: 9.001 ms, Imbalance Overhead: 4.555 ms
    The estimated cost with 2 DP ways is 94.123 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 214.728 ms
Partition 0 [0, 17) has cost: 214.728 ms
Partition 1 [17, 33) has cost: 205.443 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.905 ms
GPU 0, Compute+Comm Time: 41.530 ms, Bubble Time: 4.966 ms, Imbalance Overhead: 0.409 ms
GPU 1, Compute+Comm Time: 40.536 ms, Bubble Time: 5.436 ms, Imbalance Overhead: 0.933 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 67.109 ms
GPU 0, Compute+Comm Time: 57.533 ms, Bubble Time: 7.953 ms, Imbalance Overhead: 1.624 ms
GPU 1, Compute+Comm Time: 59.127 ms, Bubble Time: 6.982 ms, Imbalance Overhead: 0.999 ms
    The estimated cost with 4 DP ways is 119.715 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 420.171 ms
Partition 0 [0, 33) has cost: 420.171 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 150.215 ms
GPU 0, Compute+Comm Time: 150.215 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 168.336 ms
GPU 0, Compute+Comm Time: 168.336 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 334.478 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9379	TrainAcc 0.4153	ValidAcc 0.4187	TestAcc 0.4183	BestValid 0.4187
	Epoch 50:	Loss 1.6079	TrainAcc 0.4608	ValidAcc 0.4600	TestAcc 0.4601	BestValid 0.4600
	Epoch 100:	Loss 1.5754	TrainAcc 0.4578	ValidAcc 0.4587	TestAcc 0.4591	BestValid 0.4600
	Epoch 150:	Loss 1.5458	TrainAcc 0.4562	ValidAcc 0.4548	TestAcc 0.4557	BestValid 0.4600
	Epoch 200:	Loss 1.5258	TrainAcc 0.4772	ValidAcc 0.4766	TestAcc 0.4742	BestValid 0.4766
	Epoch 250:	Loss 1.5084	TrainAcc 0.4899	ValidAcc 0.4820	TestAcc 0.4848	BestValid 0.4820
	Epoch 300:	Loss 1.4905	TrainAcc 0.5014	ValidAcc 0.4960	TestAcc 0.4959	BestValid 0.4960
	Epoch 350:	Loss 1.4805	TrainAcc 0.4936	ValidAcc 0.4855	TestAcc 0.4859	BestValid 0.4960
	Epoch 400:	Loss 1.4715	TrainAcc 0.5009	ValidAcc 0.4913	TestAcc 0.4943	BestValid 0.4960
	Epoch 450:	Loss 1.4690	TrainAcc 0.4880	ValidAcc 0.4806	TestAcc 0.4787	BestValid 0.4960
	Epoch 500:	Loss 1.4648	TrainAcc 0.4990	ValidAcc 0.4905	TestAcc 0.4903	BestValid 0.4960
	Epoch 550:	Loss 1.4575	TrainAcc 0.5061	ValidAcc 0.4959	TestAcc 0.4977	BestValid 0.4960
	Epoch 600:	Loss 1.4503	TrainAcc 0.5132	ValidAcc 0.5030	TestAcc 0.5051	BestValid 0.5030
	Epoch 650:	Loss 1.4497	TrainAcc 0.5067	ValidAcc 0.4971	TestAcc 0.4989	BestValid 0.5030
	Epoch 700:	Loss 1.4475	TrainAcc 0.5110	ValidAcc 0.5008	TestAcc 0.5010	BestValid 0.5030
	Epoch 750:	Loss 1.4415	TrainAcc 0.5169	ValidAcc 0.5054	TestAcc 0.5086	BestValid 0.5054
	Epoch 800:	Loss 1.4453	TrainAcc 0.5155	ValidAcc 0.5042	TestAcc 0.5045	BestValid 0.5054
	Epoch 850:	Loss 1.4366	TrainAcc 0.5180	ValidAcc 0.5060	TestAcc 0.5065	BestValid 0.5060
	Epoch 900:	Loss 1.4367	TrainAcc 0.5191	ValidAcc 0.5076	TestAcc 0.5096	BestValid 0.5076
	Epoch 950:	Loss 1.4349	TrainAcc 0.5202	ValidAcc 0.5080	TestAcc 0.5049	BestValid 0.5080
	Epoch 1000:	Loss 1.4327	TrainAcc 0.5236	ValidAcc 0.5095	TestAcc 0.5112	BestValid 0.5095
	Epoch 1050:	Loss 1.4287	TrainAcc 0.5239	ValidAcc 0.5099	TestAcc 0.5095	BestValid 0.5099
	Epoch 1100:	Loss 1.4262	TrainAcc 0.5218	ValidAcc 0.5065	TestAcc 0.5084	BestValid 0.5099
	Epoch 1150:	Loss 1.4267	TrainAcc 0.5215	ValidAcc 0.5083	TestAcc 0.5090	BestValid 0.5099
	Epoch 1200:	Loss 1.4217	TrainAcc 0.5261	ValidAcc 0.5108	TestAcc 0.5136	BestValid 0.5108
	Epoch 1250:	Loss 1.4271	TrainAcc 0.5256	ValidAcc 0.5125	TestAcc 0.5131	BestValid 0.5125
	Epoch 1300:	Loss 1.4215	TrainAcc 0.5267	ValidAcc 0.5122	TestAcc 0.5124	BestValid 0.5125
	Epoch 1350:	Loss 1.4151	TrainAcc 0.5276	ValidAcc 0.5125	TestAcc 0.5142	BestValid 0.5125
	Epoch 1400:	Loss 1.4182	TrainAcc 0.5286	ValidAcc 0.5124	TestAcc 0.5129	BestValid 0.5125
	Epoch 1450:	Loss 1.4153	TrainAcc 0.5289	ValidAcc 0.5128	TestAcc 0.5145	BestValid 0.5128
	Epoch 1500:	Loss 1.4154	TrainAcc 0.5197	ValidAcc 0.5049	TestAcc 0.5034	BestValid 0.5128
	Epoch 1550:	Loss 1.4117	TrainAcc 0.5269	ValidAcc 0.5106	TestAcc 0.5131	BestValid 0.5128
	Epoch 1600:	Loss 1.4087	TrainAcc 0.5233	ValidAcc 0.5066	TestAcc 0.5075	BestValid 0.5128
	Epoch 1650:	Loss 1.4049	TrainAcc 0.5270	ValidAcc 0.5100	TestAcc 0.5125	BestValid 0.5128
	Epoch 1700:	Loss 1.4087	TrainAcc 0.5330	ValidAcc 0.5143	TestAcc 0.5161	BestValid 0.5143
	Epoch 1750:	Loss 1.4093	TrainAcc 0.5285	ValidAcc 0.5107	TestAcc 0.5124	BestValid 0.5143
	Epoch 1800:	Loss 1.4047	TrainAcc 0.5244	ValidAcc 0.5047	TestAcc 0.5078	BestValid 0.5143
	Epoch 1850:	Loss 1.4053	TrainAcc 0.5354	ValidAcc 0.5158	TestAcc 0.5156	BestValid 0.5158
	Epoch 1900:	Loss 1.4004	TrainAcc 0.5285	ValidAcc 0.5092	TestAcc 0.5100	BestValid 0.5158
	Epoch 1950:	Loss 1.3980	TrainAcc 0.5329	ValidAcc 0.5127	TestAcc 0.5155	BestValid 0.5158
	Epoch 2000:	Loss 1.3968	TrainAcc 0.5281	ValidAcc 0.5103	TestAcc 0.5105	BestValid 0.5158
	Epoch 2050:	Loss 1.3960	TrainAcc 0.5321	ValidAcc 0.5111	TestAcc 0.5122	BestValid 0.5158
	Epoch 2100:	Loss 1.3977	TrainAcc 0.5374	ValidAcc 0.5166	TestAcc 0.5168	BestValid 0.5166
	Epoch 2150:	Loss 1.3987	TrainAcc 0.5400	ValidAcc 0.5194	TestAcc 0.5170	BestValid 0.5194
	Epoch 2200:	Loss 1.3906	TrainAcc 0.5398	ValidAcc 0.5184	TestAcc 0.5174	BestValid 0.5194
	Epoch 2250:	Loss 1.3916	TrainAcc 0.5310	ValidAcc 0.5079	TestAcc 0.5087	BestValid 0.5194
	Epoch 2300:	Loss 1.3922	TrainAcc 0.5253	ValidAcc 0.5062	TestAcc 0.5036	BestValid 0.5194
	Epoch 2350:	Loss 1.3879	TrainAcc 0.5317	ValidAcc 0.5086	TestAcc 0.5093	BestValid 0.5194
	Epoch 2400:	Loss 1.3871	TrainAcc 0.5320	ValidAcc 0.5083	TestAcc 0.5082	BestValid 0.5194
	Epoch 2450:	Loss 1.3848	TrainAcc 0.5377	ValidAcc 0.5115	TestAcc 0.5135	BestValid 0.5194
	Epoch 2500:	Loss 1.3854	TrainAcc 0.5404	ValidAcc 0.5166	TestAcc 0.5153	BestValid 0.5194
	Epoch 2550:	Loss 1.3825	TrainAcc 0.5442	ValidAcc 0.5194	TestAcc 0.5193	BestValid 0.5194
	Epoch 2600:	Loss 1.3825	TrainAcc 0.5374	ValidAcc 0.5125	TestAcc 0.5131	BestValid 0.5194
	Epoch 2650:	Loss 1.3826	TrainAcc 0.5368	ValidAcc 0.5100	TestAcc 0.5105	BestValid 0.5194
	Epoch 2700:	Loss 1.3792	TrainAcc 0.5440	ValidAcc 0.5177	TestAcc 0.5160	BestValid 0.5194
	Epoch 2750:	Loss 1.3782	TrainAcc 0.5359	ValidAcc 0.5098	TestAcc 0.5086	BestValid 0.5194
	Epoch 2800:	Loss 1.3749	TrainAcc 0.5442	ValidAcc 0.5172	TestAcc 0.5169	BestValid 0.5194
	Epoch 2850:	Loss 1.3796	TrainAcc 0.5459	ValidAcc 0.5183	TestAcc 0.5179	BestValid 0.5194
	Epoch 2900:	Loss 1.3750	TrainAcc 0.5378	ValidAcc 0.5092	TestAcc 0.5084	BestValid 0.5194
	Epoch 2950:	Loss 1.3732	TrainAcc 0.5439	ValidAcc 0.5126	TestAcc 0.5135	BestValid 0.5194
	Epoch 3000:	Loss 1.3746	TrainAcc 0.5356	ValidAcc 0.5078	TestAcc 0.5056	BestValid 0.5194
	Epoch 3050:	Loss 1.3717	TrainAcc 0.5509	ValidAcc 0.5186	TestAcc 0.5199	BestValid 0.5194
	Epoch 3100:	Loss 1.3691	TrainAcc 0.5392	ValidAcc 0.5102	TestAcc 0.5086	BestValid 0.5194
	Epoch 3150:	Loss 1.3681	TrainAcc 0.5368	ValidAcc 0.5066	TestAcc 0.5043	BestValid 0.5194
	Epoch 3200:	Loss 1.3695	TrainAcc 0.5433	ValidAcc 0.5130	TestAcc 0.5122	BestValid 0.5194
	Epoch 3250:	Loss 1.3668	TrainAcc 0.5456	ValidAcc 0.5137	TestAcc 0.5128	BestValid 0.5194
	Epoch 3300:	Loss 1.3658	TrainAcc 0.5521	ValidAcc 0.5192	TestAcc 0.5178	BestValid 0.5194
	Epoch 3350:	Loss 1.3627	TrainAcc 0.5481	ValidAcc 0.5157	TestAcc 0.5136	BestValid 0.5194
	Epoch 3400:	Loss 1.3615	TrainAcc 0.5431	ValidAcc 0.5107	TestAcc 0.5089	BestValid 0.5194
	Epoch 3450:	Loss 1.3634	TrainAcc 0.5509	ValidAcc 0.5171	TestAcc 0.5149	BestValid 0.5194
	Epoch 3500:	Loss 1.3592	TrainAcc 0.5500	ValidAcc 0.5137	TestAcc 0.5134	BestValid 0.5194
	Epoch 3550:	Loss 1.3598	TrainAcc 0.5458	ValidAcc 0.5128	TestAcc 0.5109	BestValid 0.5194
	Epoch 3600:	Loss 1.3566	TrainAcc 0.5485	ValidAcc 0.5131	TestAcc 0.5125	BestValid 0.5194
	Epoch 3650:	Loss 1.3535	TrainAcc 0.5604	ValidAcc 0.5226	TestAcc 0.5211	BestValid 0.5226
	Epoch 3700:	Loss 1.3576	TrainAcc 0.5488	ValidAcc 0.5135	TestAcc 0.5118	BestValid 0.5226
	Epoch 3750:	Loss 1.3537	TrainAcc 0.5516	ValidAcc 0.5164	TestAcc 0.5140	BestValid 0.5226
	Epoch 3800:	Loss 1.3535	TrainAcc 0.5494	ValidAcc 0.5145	TestAcc 0.5115	BestValid 0.5226
	Epoch 3850:	Loss 1.3524	TrainAcc 0.5500	ValidAcc 0.5134	TestAcc 0.5120	BestValid 0.5226
	Epoch 3900:	Loss 1.3507	TrainAcc 0.5499	ValidAcc 0.5129	TestAcc 0.5119	BestValid 0.5226
	Epoch 3950:	Loss 1.3504	TrainAcc 0.5540	ValidAcc 0.5166	TestAcc 0.5148	BestValid 0.5226
	Epoch 4000:	Loss 1.3481	TrainAcc 0.5593	ValidAcc 0.5214	TestAcc 0.5179	BestValid 0.5226
	Epoch 4050:	Loss 1.3488	TrainAcc 0.5520	ValidAcc 0.5143	TestAcc 0.5113	BestValid 0.5226
	Epoch 4100:	Loss 1.3483	TrainAcc 0.5613	ValidAcc 0.5200	TestAcc 0.5184	BestValid 0.5226
	Epoch 4150:	Loss 1.3455	TrainAcc 0.5550	ValidAcc 0.5160	TestAcc 0.5136	BestValid 0.5226
	Epoch 4200:	Loss 1.3458	TrainAcc 0.5537	ValidAcc 0.5139	TestAcc 0.5119	BestValid 0.5226
	Epoch 4250:	Loss 1.3472	TrainAcc 0.5512	ValidAcc 0.5120	TestAcc 0.5103	BestValid 0.5226
	Epoch 4300:	Loss 1.3446	TrainAcc 0.5509	ValidAcc 0.5111	TestAcc 0.5109	BestValid 0.5226
	Epoch 4350:	Loss 1.3451	TrainAcc 0.5584	ValidAcc 0.5162	TestAcc 0.5131	BestValid 0.5226
	Epoch 4400:	Loss 1.3445	TrainAcc 0.5629	ValidAcc 0.5191	TestAcc 0.5176	BestValid 0.5226
	Epoch 4450:	Loss 1.3440	TrainAcc 0.5616	ValidAcc 0.5204	TestAcc 0.5172	BestValid 0.5226
	Epoch 4500:	Loss 1.3421	TrainAcc 0.5684	ValidAcc 0.5217	TestAcc 0.5218	BestValid 0.5226
	Epoch 4550:	Loss 1.3404	TrainAcc 0.5643	ValidAcc 0.5198	TestAcc 0.5182	BestValid 0.5226
	Epoch 4600:	Loss 1.3381	TrainAcc 0.5644	ValidAcc 0.5206	TestAcc 0.5186	BestValid 0.5226
	Epoch 4650:	Loss 1.3379	TrainAcc 0.5681	ValidAcc 0.5226	TestAcc 0.5220	BestValid 0.5226
	Epoch 4700:	Loss 1.3391	TrainAcc 0.5703	ValidAcc 0.5224	TestAcc 0.5214	BestValid 0.5226
	Epoch 4750:	Loss 1.3347	TrainAcc 0.5579	ValidAcc 0.5145	TestAcc 0.5135	BestValid 0.5226
	Epoch 4800:	Loss 1.3370	TrainAcc 0.5606	ValidAcc 0.5154	TestAcc 0.5139	BestValid 0.5226
	Epoch 4850:	Loss 1.3358	TrainAcc 0.5662	ValidAcc 0.5216	TestAcc 0.5176	BestValid 0.5226
	Epoch 4900:	Loss 1.3377	TrainAcc 0.5479	ValidAcc 0.5050	TestAcc 0.5026	BestValid 0.5226
	Epoch 4950:	Loss 1.3363	TrainAcc 0.5600	ValidAcc 0.5143	TestAcc 0.5127	BestValid 0.5226
	Epoch 5000:	Loss 1.3345	TrainAcc 0.5694	ValidAcc 0.5223	TestAcc 0.5206	BestValid 0.5226
****** Epoch Time (Excluding Evaluation Cost): 0.095 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.816 ms (Max: 18.789, Min: 15.817, Sum: 142.531)
Cluster-Wide Average, Compute: 51.903 ms (Max: 62.521, Min: 48.302, Sum: 415.224)
Cluster-Wide Average, Communication-Layer: 14.335 ms (Max: 16.688, Min: 10.902, Sum: 114.683)
Cluster-Wide Average, Bubble-Imbalance: 9.295 ms (Max: 12.829, Min: 2.341, Sum: 74.362)
Cluster-Wide Average, Communication-Graph: 0.490 ms (Max: 0.534, Min: 0.444, Sum: 3.917)
Cluster-Wide Average, Optimization: 0.095 ms (Max: 0.112, Min: 0.087, Sum: 0.758)
Cluster-Wide Average, Others: 1.209 ms (Max: 4.069, Min: 0.790, Sum: 9.670)
****** Breakdown Sum: 95.143 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.004 GB (Max: 4.174, Min: 2.821, Sum: 24.030)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 68.766 Gbps (Max: 82.357, Min: 51.045, Sum: 550.125)
Layer-level communication (cluster-wide, per-epoch): 0.931 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.931 GB
****** Accuracy Results ******
Highest valid_acc: 0.5226
Target test_acc: 0.5211
Epoch to reach the target acc: 3649
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 828401 RUNNING AT gnerv2
=   EXIT CODE: 9
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Killed (signal 9)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
