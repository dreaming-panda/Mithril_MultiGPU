Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.025 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.027 seconds.
        It takes 0.020 seconds.
        It takes 0.028 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.717 Gbps (per GPU), 477.739 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.432 Gbps (per GPU), 475.453 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.435 Gbps (per GPU), 475.478 Gbps (aggregated)
The layer-level communication performance: 59.160 Gbps (per GPU), 473.280 Gbps (aggregated)
The layer-level communication performance: 59.193 Gbps (per GPU), 473.545 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.956 Gbps (per GPU), 471.651 Gbps (aggregated)
The layer-level communication performance: 58.907 Gbps (per GPU), 471.259 Gbps (aggregated)
The layer-level communication performance: 58.873 Gbps (per GPU), 470.981 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.146 Gbps (per GPU), 1249.164 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.157 Gbps (per GPU), 1249.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.149 Gbps (per GPU), 1249.191 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.105 Gbps (per GPU), 1248.839 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.137 Gbps (per GPU), 1249.099 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.172 Gbps (per GPU), 1249.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.154 Gbps (per GPU), 1249.234 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.102 Gbps (per GPU), 1248.817 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.634 Gbps (per GPU), 813.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.639 Gbps (per GPU), 813.113 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.639 Gbps (per GPU), 813.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.640 Gbps (per GPU), 813.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.635 Gbps (per GPU), 813.079 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.640 Gbps (per GPU), 813.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.518 Gbps (per GPU), 812.141 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.623 Gbps (per GPU), 812.987 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.252 Gbps (per GPU), 266.019 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.249 Gbps (per GPU), 265.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.247 Gbps (per GPU), 265.978 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.244 Gbps (per GPU), 265.955 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.245 Gbps (per GPU), 265.961 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.247 Gbps (per GPU), 265.978 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.245 Gbps (per GPU), 265.956 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.241 Gbps (per GPU), 265.928 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.35ms  0.52ms  1.47  2.81K  0.03M
 chk_1  0.41ms  0.36ms  0.52ms  1.45  2.82K  0.03M
 chk_2  0.41ms  0.36ms  0.52ms  1.46  2.80K  0.03M
 chk_3  0.41ms  0.36ms  0.52ms  1.45  2.80K  0.03M
 chk_4  0.41ms  0.36ms  0.52ms  1.46  2.82K  0.03M
 chk_5  0.41ms  0.36ms  0.53ms  1.45  2.75K  0.03M
 chk_6  0.41ms  0.35ms  0.52ms  1.46  2.71K  0.03M
 chk_7  0.41ms  0.36ms  0.52ms  1.46  2.76K  0.03M
 chk_8  0.41ms  0.36ms  0.52ms  1.45  2.79K  0.03M
 chk_9  0.41ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_10  0.41ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_11  0.41ms  0.36ms  0.53ms  1.45  2.74K  0.03M
chk_12  0.41ms  0.36ms  0.52ms  1.45  2.76K  0.03M
chk_13  0.41ms  0.36ms  0.52ms  1.45  2.75K  0.03M
chk_14  0.41ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_15  0.41ms  0.36ms  0.52ms  1.46  2.77K  0.03M
chk_16  0.41ms  0.36ms  0.52ms  1.46  2.78K  0.03M
chk_17  0.41ms  0.36ms  0.53ms  1.45  2.79K  0.03M
chk_18  0.42ms  0.36ms  0.53ms  1.46  2.82K  0.03M
chk_19  0.41ms  0.35ms  0.52ms  1.47  2.81K  0.03M
chk_20  0.41ms  0.36ms  0.53ms  1.45  2.77K  0.03M
chk_21  0.42ms  0.36ms  0.52ms  1.45  2.84K  0.02M
chk_22  0.41ms  0.36ms  0.52ms  1.46  2.78K  0.03M
chk_23  0.41ms  0.36ms  0.52ms  1.46  2.80K  0.03M
chk_24  0.41ms  0.36ms  0.52ms  1.46  2.80K  0.03M
chk_25  0.41ms  0.36ms  0.52ms  1.45  2.81K  0.03M
chk_26  0.42ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_27  0.41ms  0.36ms  0.53ms  1.46  2.79K  0.03M
chk_28  0.41ms  0.36ms  0.53ms  1.46  2.77K  0.03M
chk_29  0.41ms  0.36ms  0.52ms  1.46  2.77K  0.03M
chk_30  0.41ms  0.36ms  0.52ms  1.45  2.80K  0.03M
chk_31  0.41ms  0.36ms  0.53ms  1.46  2.78K  0.03M
   Avg  0.41  0.36  0.52
   Max  0.42  0.36  0.53
   Min  0.41  0.35  0.52
 Ratio  1.02  1.04  1.02
   Var  0.00  0.00  0.00
Profiling takes 0.608 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 57.459 ms
Partition 0 [0, 4) has cost: 47.655 ms
Partition 1 [4, 8) has cost: 45.967 ms
Partition 2 [8, 12) has cost: 45.967 ms
Partition 3 [12, 16) has cost: 45.967 ms
Partition 4 [16, 20) has cost: 45.967 ms
Partition 5 [20, 24) has cost: 45.967 ms
Partition 6 [24, 29) has cost: 57.459 ms
Partition 7 [29, 33) has cost: 51.219 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 29.197 ms
GPU 0, Compute+Comm Time: 20.844 ms, Bubble Time: 5.292 ms, Imbalance Overhead: 3.061 ms
GPU 1, Compute+Comm Time: 20.636 ms, Bubble Time: 5.175 ms, Imbalance Overhead: 3.386 ms
GPU 2, Compute+Comm Time: 20.636 ms, Bubble Time: 5.056 ms, Imbalance Overhead: 3.505 ms
GPU 3, Compute+Comm Time: 20.636 ms, Bubble Time: 4.939 ms, Imbalance Overhead: 3.622 ms
GPU 4, Compute+Comm Time: 20.636 ms, Bubble Time: 4.830 ms, Imbalance Overhead: 3.731 ms
GPU 5, Compute+Comm Time: 20.636 ms, Bubble Time: 4.716 ms, Imbalance Overhead: 3.845 ms
GPU 6, Compute+Comm Time: 24.599 ms, Bubble Time: 4.598 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 21.645 ms, Bubble Time: 4.682 ms, Imbalance Overhead: 2.870 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.472 ms
GPU 0, Compute+Comm Time: 39.139 ms, Bubble Time: 8.132 ms, Imbalance Overhead: 3.201 ms
GPU 1, Compute+Comm Time: 42.424 ms, Bubble Time: 8.047 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 34.896 ms, Bubble Time: 8.237 ms, Imbalance Overhead: 7.338 ms
GPU 3, Compute+Comm Time: 34.896 ms, Bubble Time: 8.434 ms, Imbalance Overhead: 7.142 ms
GPU 4, Compute+Comm Time: 34.896 ms, Bubble Time: 8.613 ms, Imbalance Overhead: 6.963 ms
GPU 5, Compute+Comm Time: 34.896 ms, Bubble Time: 8.808 ms, Imbalance Overhead: 6.768 ms
GPU 6, Compute+Comm Time: 34.896 ms, Bubble Time: 9.000 ms, Imbalance Overhead: 6.576 ms
GPU 7, Compute+Comm Time: 36.376 ms, Bubble Time: 9.200 ms, Imbalance Overhead: 4.896 ms
The estimated cost of the whole pipeline: 83.652 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 103.425 ms
Partition 0 [0, 8) has cost: 93.621 ms
Partition 1 [8, 16) has cost: 91.934 ms
Partition 2 [16, 25) has cost: 103.425 ms
Partition 3 [25, 33) has cost: 97.186 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.401 ms
GPU 0, Compute+Comm Time: 24.030 ms, Bubble Time: 4.903 ms, Imbalance Overhead: 2.468 ms
GPU 1, Compute+Comm Time: 24.387 ms, Bubble Time: 4.724 ms, Imbalance Overhead: 2.289 ms
GPU 2, Compute+Comm Time: 26.838 ms, Bubble Time: 4.563 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 24.893 ms, Bubble Time: 4.675 ms, Imbalance Overhead: 1.833 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.374 ms
GPU 0, Compute+Comm Time: 40.827 ms, Bubble Time: 7.548 ms, Imbalance Overhead: 1.999 ms
GPU 1, Compute+Comm Time: 42.947 ms, Bubble Time: 7.427 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 38.707 ms, Bubble Time: 7.648 ms, Imbalance Overhead: 4.020 ms
GPU 3, Compute+Comm Time: 38.983 ms, Bubble Time: 7.918 ms, Imbalance Overhead: 3.473 ms
    The estimated cost with 2 DP ways is 85.864 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 197.047 ms
Partition 0 [0, 17) has cost: 197.047 ms
Partition 1 [17, 33) has cost: 189.119 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 43.594 ms
GPU 0, Compute+Comm Time: 38.855 ms, Bubble Time: 4.739 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.060 ms, Bubble Time: 4.864 ms, Imbalance Overhead: 0.670 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.064 ms
GPU 0, Compute+Comm Time: 53.240 ms, Bubble Time: 6.769 ms, Imbalance Overhead: 1.054 ms
GPU 1, Compute+Comm Time: 54.442 ms, Bubble Time: 6.622 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 109.891 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 386.166 ms
Partition 0 [0, 33) has cost: 386.166 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 138.418 ms
GPU 0, Compute+Comm Time: 138.418 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 153.851 ms
GPU 0, Compute+Comm Time: 153.851 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 306.883 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2788	TrainAcc 0.4202	ValidAcc 0.4182	TestAcc 0.4206	BestValid 0.4182
	Epoch 50:	Loss 1.6237	TrainAcc 0.4584	ValidAcc 0.4581	TestAcc 0.4578	BestValid 0.4581
	Epoch 100:	Loss 1.5899	TrainAcc 0.4732	ValidAcc 0.4750	TestAcc 0.4728	BestValid 0.4750
	Epoch 150:	Loss 1.5673	TrainAcc 0.4607	ValidAcc 0.4609	TestAcc 0.4610	BestValid 0.4750
	Epoch 200:	Loss 1.5546	TrainAcc 0.4598	ValidAcc 0.4578	TestAcc 0.4590	BestValid 0.4750
	Epoch 250:	Loss 1.5461	TrainAcc 0.4697	ValidAcc 0.4685	TestAcc 0.4689	BestValid 0.4750
	Epoch 300:	Loss 1.5310	TrainAcc 0.4633	ValidAcc 0.4599	TestAcc 0.4624	BestValid 0.4750
	Epoch 350:	Loss 1.5223	TrainAcc 0.4775	ValidAcc 0.4762	TestAcc 0.4752	BestValid 0.4762
	Epoch 400:	Loss 1.5114	TrainAcc 0.4748	ValidAcc 0.4710	TestAcc 0.4705	BestValid 0.4762
	Epoch 450:	Loss 1.5014	TrainAcc 0.4898	ValidAcc 0.4861	TestAcc 0.4848	BestValid 0.4861
	Epoch 500:	Loss 1.4924	TrainAcc 0.5018	ValidAcc 0.4972	TestAcc 0.4964	BestValid 0.4972
	Epoch 550:	Loss 1.4863	TrainAcc 0.5045	ValidAcc 0.4991	TestAcc 0.5007	BestValid 0.4991
	Epoch 600:	Loss 1.4818	TrainAcc 0.5062	ValidAcc 0.4997	TestAcc 0.5008	BestValid 0.4997
	Epoch 650:	Loss 1.4775	TrainAcc 0.5070	ValidAcc 0.4994	TestAcc 0.5015	BestValid 0.4997
	Epoch 700:	Loss 1.4696	TrainAcc 0.5091	ValidAcc 0.5009	TestAcc 0.5018	BestValid 0.5009
	Epoch 750:	Loss 1.4644	TrainAcc 0.5107	ValidAcc 0.5039	TestAcc 0.5052	BestValid 0.5039
	Epoch 800:	Loss 1.4613	TrainAcc 0.5116	ValidAcc 0.5038	TestAcc 0.5037	BestValid 0.5039
	Epoch 850:	Loss 1.4595	TrainAcc 0.5139	ValidAcc 0.5071	TestAcc 0.5072	BestValid 0.5071
	Epoch 900:	Loss 1.4570	TrainAcc 0.5161	ValidAcc 0.5093	TestAcc 0.5077	BestValid 0.5093
	Epoch 950:	Loss 1.4553	TrainAcc 0.5175	ValidAcc 0.5109	TestAcc 0.5087	BestValid 0.5109
	Epoch 1000:	Loss 1.4520	TrainAcc 0.5149	ValidAcc 0.5078	TestAcc 0.5059	BestValid 0.5109
	Epoch 1050:	Loss 1.4501	TrainAcc 0.5177	ValidAcc 0.5091	TestAcc 0.5089	BestValid 0.5109
	Epoch 1100:	Loss 1.4452	TrainAcc 0.5173	ValidAcc 0.5095	TestAcc 0.5087	BestValid 0.5109
	Epoch 1150:	Loss 1.4453	TrainAcc 0.5195	ValidAcc 0.5122	TestAcc 0.5113	BestValid 0.5122
	Epoch 1200:	Loss 1.4458	TrainAcc 0.5049	ValidAcc 0.4986	TestAcc 0.4954	BestValid 0.5122
	Epoch 1250:	Loss 1.4415	TrainAcc 0.5185	ValidAcc 0.5115	TestAcc 0.5101	BestValid 0.5122
	Epoch 1300:	Loss 1.4396	TrainAcc 0.5141	ValidAcc 0.5042	TestAcc 0.5023	BestValid 0.5122
	Epoch 1350:	Loss 1.4350	TrainAcc 0.5081	ValidAcc 0.4999	TestAcc 0.4969	BestValid 0.5122
	Epoch 1400:	Loss 1.4340	TrainAcc 0.5194	ValidAcc 0.5116	TestAcc 0.5094	BestValid 0.5122
	Epoch 1450:	Loss 1.4349	TrainAcc 0.5183	ValidAcc 0.5077	TestAcc 0.5066	BestValid 0.5122
	Epoch 1500:	Loss 1.4332	TrainAcc 0.5169	ValidAcc 0.5085	TestAcc 0.5062	BestValid 0.5122
	Epoch 1550:	Loss 1.4290	TrainAcc 0.5067	ValidAcc 0.4970	TestAcc 0.4942	BestValid 0.5122
	Epoch 1600:	Loss 1.4253	TrainAcc 0.5151	ValidAcc 0.5061	TestAcc 0.5031	BestValid 0.5122
	Epoch 1650:	Loss 1.4287	TrainAcc 0.5200	ValidAcc 0.5079	TestAcc 0.5069	BestValid 0.5122
	Epoch 1700:	Loss 1.4240	TrainAcc 0.5169	ValidAcc 0.5055	TestAcc 0.5031	BestValid 0.5122
	Epoch 1750:	Loss 1.4261	TrainAcc 0.5095	ValidAcc 0.4975	TestAcc 0.4950	BestValid 0.5122
	Epoch 1800:	Loss 1.4221	TrainAcc 0.5118	ValidAcc 0.4997	TestAcc 0.4974	BestValid 0.5122
	Epoch 1850:	Loss 1.4239	TrainAcc 0.5114	ValidAcc 0.4975	TestAcc 0.4965	BestValid 0.5122
	Epoch 1900:	Loss 1.4203	TrainAcc 0.5142	ValidAcc 0.5012	TestAcc 0.4981	BestValid 0.5122
	Epoch 1950:	Loss 1.4214	TrainAcc 0.5187	ValidAcc 0.5058	TestAcc 0.5034	BestValid 0.5122
	Epoch 2000:	Loss 1.4185	TrainAcc 0.5031	ValidAcc 0.4901	TestAcc 0.4889	BestValid 0.5122
	Epoch 2050:	Loss 1.4166	TrainAcc 0.5146	ValidAcc 0.5009	TestAcc 0.4982	BestValid 0.5122
	Epoch 2100:	Loss 1.4123	TrainAcc 0.5133	ValidAcc 0.4985	TestAcc 0.4970	BestValid 0.5122
	Epoch 2150:	Loss 1.4143	TrainAcc 0.5114	ValidAcc 0.4980	TestAcc 0.4961	BestValid 0.5122
	Epoch 2200:	Loss 1.4124	TrainAcc 0.5113	ValidAcc 0.4960	TestAcc 0.4961	BestValid 0.5122
	Epoch 2250:	Loss 1.4090	TrainAcc 0.5182	ValidAcc 0.5033	TestAcc 0.5020	BestValid 0.5122
	Epoch 2300:	Loss 1.4113	TrainAcc 0.5182	ValidAcc 0.5032	TestAcc 0.5018	BestValid 0.5122
	Epoch 2350:	Loss 1.4078	TrainAcc 0.5018	ValidAcc 0.4896	TestAcc 0.4870	BestValid 0.5122
	Epoch 2400:	Loss 1.4085	TrainAcc 0.5187	ValidAcc 0.5035	TestAcc 0.5021	BestValid 0.5122
	Epoch 2450:	Loss 1.4127	TrainAcc 0.5160	ValidAcc 0.4998	TestAcc 0.4987	BestValid 0.5122
	Epoch 2500:	Loss 1.4054	TrainAcc 0.5159	ValidAcc 0.4994	TestAcc 0.4973	BestValid 0.5122
	Epoch 2550:	Loss 1.4027	TrainAcc 0.5235	ValidAcc 0.5072	TestAcc 0.5053	BestValid 0.5122
	Epoch 2600:	Loss 1.4010	TrainAcc 0.5240	ValidAcc 0.5078	TestAcc 0.5051	BestValid 0.5122
	Epoch 2650:	Loss 1.4051	TrainAcc 0.5290	ValidAcc 0.5113	TestAcc 0.5100	BestValid 0.5122
	Epoch 2700:	Loss 1.4013	TrainAcc 0.5271	ValidAcc 0.5106	TestAcc 0.5083	BestValid 0.5122
	Epoch 2750:	Loss 1.4008	TrainAcc 0.5320	ValidAcc 0.5134	TestAcc 0.5125	BestValid 0.5134
	Epoch 2800:	Loss 1.3980	TrainAcc 0.5284	ValidAcc 0.5106	TestAcc 0.5090	BestValid 0.5134
	Epoch 2850:	Loss 1.3960	TrainAcc 0.5300	ValidAcc 0.5132	TestAcc 0.5108	BestValid 0.5134
	Epoch 2900:	Loss 1.3922	TrainAcc 0.5167	ValidAcc 0.4985	TestAcc 0.4974	BestValid 0.5134
	Epoch 2950:	Loss 1.3942	TrainAcc 0.5250	ValidAcc 0.5071	TestAcc 0.5065	BestValid 0.5134
	Epoch 3000:	Loss 1.3973	TrainAcc 0.5267	ValidAcc 0.5080	TestAcc 0.5076	BestValid 0.5134
	Epoch 3050:	Loss 1.3895	TrainAcc 0.5312	ValidAcc 0.5119	TestAcc 0.5101	BestValid 0.5134
	Epoch 3100:	Loss 1.3887	TrainAcc 0.5277	ValidAcc 0.5083	TestAcc 0.5080	BestValid 0.5134
	Epoch 3150:	Loss 1.3858	TrainAcc 0.5285	ValidAcc 0.5093	TestAcc 0.5076	BestValid 0.5134
	Epoch 3200:	Loss 1.3844	TrainAcc 0.5384	ValidAcc 0.5166	TestAcc 0.5167	BestValid 0.5166
	Epoch 3250:	Loss 1.3875	TrainAcc 0.5282	ValidAcc 0.5074	TestAcc 0.5064	BestValid 0.5166
	Epoch 3300:	Loss 1.3848	TrainAcc 0.5407	ValidAcc 0.5184	TestAcc 0.5171	BestValid 0.5184
	Epoch 3350:	Loss 1.3853	TrainAcc 0.5335	ValidAcc 0.5124	TestAcc 0.5105	BestValid 0.5184
	Epoch 3400:	Loss 1.3793	TrainAcc 0.5333	ValidAcc 0.5118	TestAcc 0.5107	BestValid 0.5184
	Epoch 3450:	Loss 1.3818	TrainAcc 0.5379	ValidAcc 0.5154	TestAcc 0.5147	BestValid 0.5184
	Epoch 3500:	Loss 1.3780	TrainAcc 0.5417	ValidAcc 0.5169	TestAcc 0.5155	BestValid 0.5184
	Epoch 3550:	Loss 1.3774	TrainAcc 0.5299	ValidAcc 0.5082	TestAcc 0.5060	BestValid 0.5184
	Epoch 3600:	Loss 1.3759	TrainAcc 0.5373	ValidAcc 0.5151	TestAcc 0.5132	BestValid 0.5184
	Epoch 3650:	Loss 1.3712	TrainAcc 0.5362	ValidAcc 0.5121	TestAcc 0.5101	BestValid 0.5184
	Epoch 3700:	Loss 1.3727	TrainAcc 0.5432	ValidAcc 0.5182	TestAcc 0.5187	BestValid 0.5184
	Epoch 3750:	Loss 1.3722	TrainAcc 0.5375	ValidAcc 0.5124	TestAcc 0.5102	BestValid 0.5184
	Epoch 3800:	Loss 1.3737	TrainAcc 0.5382	ValidAcc 0.5134	TestAcc 0.5118	BestValid 0.5184
	Epoch 3850:	Loss 1.3664	TrainAcc 0.5435	ValidAcc 0.5160	TestAcc 0.5156	BestValid 0.5184
	Epoch 3900:	Loss 1.3672	TrainAcc 0.5458	ValidAcc 0.5200	TestAcc 0.5176	BestValid 0.5200
	Epoch 3950:	Loss 1.3688	TrainAcc 0.5434	ValidAcc 0.5171	TestAcc 0.5131	BestValid 0.5200
	Epoch 4000:	Loss 1.3607	TrainAcc 0.5461	ValidAcc 0.5182	TestAcc 0.5174	BestValid 0.5200
	Epoch 4050:	Loss 1.3603	TrainAcc 0.5405	ValidAcc 0.5130	TestAcc 0.5106	BestValid 0.5200
	Epoch 4100:	Loss 1.3637	TrainAcc 0.5531	ValidAcc 0.5219	TestAcc 0.5210	BestValid 0.5219
	Epoch 4150:	Loss 1.3561	TrainAcc 0.5529	ValidAcc 0.5207	TestAcc 0.5205	BestValid 0.5219
	Epoch 4200:	Loss 1.3574	TrainAcc 0.5504	ValidAcc 0.5202	TestAcc 0.5184	BestValid 0.5219
	Epoch 4250:	Loss 1.3588	TrainAcc 0.5498	ValidAcc 0.5192	TestAcc 0.5164	BestValid 0.5219
	Epoch 4300:	Loss 1.3562	TrainAcc 0.5477	ValidAcc 0.5183	TestAcc 0.5147	BestValid 0.5219
	Epoch 4350:	Loss 1.3569	TrainAcc 0.5404	ValidAcc 0.5095	TestAcc 0.5066	BestValid 0.5219
	Epoch 4400:	Loss 1.3526	TrainAcc 0.5498	ValidAcc 0.5176	TestAcc 0.5141	BestValid 0.5219
	Epoch 4450:	Loss 1.3494	TrainAcc 0.5367	ValidAcc 0.5059	TestAcc 0.5033	BestValid 0.5219
	Epoch 4500:	Loss 1.3506	TrainAcc 0.5551	ValidAcc 0.5223	TestAcc 0.5197	BestValid 0.5223
	Epoch 4550:	Loss 1.3523	TrainAcc 0.5574	ValidAcc 0.5223	TestAcc 0.5207	BestValid 0.5223
	Epoch 4600:	Loss 1.3490	TrainAcc 0.5520	ValidAcc 0.5190	TestAcc 0.5156	BestValid 0.5223
	Epoch 4650:	Loss 1.3495	TrainAcc 0.5504	ValidAcc 0.5138	TestAcc 0.5111	BestValid 0.5223
	Epoch 4700:	Loss 1.3468	TrainAcc 0.5571	ValidAcc 0.5220	TestAcc 0.5188	BestValid 0.5223
	Epoch 4750:	Loss 1.3442	TrainAcc 0.5609	ValidAcc 0.5240	TestAcc 0.5205	BestValid 0.5240
	Epoch 4800:	Loss 1.3448	TrainAcc 0.5678	ValidAcc 0.5292	TestAcc 0.5242	BestValid 0.5292
	Epoch 4850:	Loss 1.3391	TrainAcc 0.5584	ValidAcc 0.5225	TestAcc 0.5191	BestValid 0.5292
	Epoch 4900:	Loss 1.3419	TrainAcc 0.5619	ValidAcc 0.5256	TestAcc 0.5220	BestValid 0.5292
	Epoch 4950:	Loss 1.3384	TrainAcc 0.5637	ValidAcc 0.5240	TestAcc 0.5222	BestValid 0.5292
	Epoch 5000:	Loss 1.3452	TrainAcc 0.5642	ValidAcc 0.5243	TestAcc 0.5227	BestValid 0.5292
****** Epoch Time (Excluding Evaluation Cost): 0.095 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.811 ms (Max: 18.836, Min: 15.798, Sum: 142.489)
Cluster-Wide Average, Compute: 51.361 ms (Max: 62.561, Min: 46.993, Sum: 410.891)
Cluster-Wide Average, Communication-Layer: 14.323 ms (Max: 16.726, Min: 10.843, Sum: 114.585)
Cluster-Wide Average, Bubble-Imbalance: 9.854 ms (Max: 13.330, Min: 2.357, Sum: 78.828)
Cluster-Wide Average, Communication-Graph: 0.458 ms (Max: 0.509, Min: 0.391, Sum: 3.662)
Cluster-Wide Average, Optimization: 0.094 ms (Max: 0.114, Min: 0.087, Sum: 0.756)
Cluster-Wide Average, Others: 1.193 ms (Max: 4.062, Min: 0.776, Sum: 9.547)
****** Breakdown Sum: 95.095 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.004 GB (Max: 4.174, Min: 2.821, Sum: 24.030)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 68.827 Gbps (Max: 82.454, Min: 51.316, Sum: 550.619)
Layer-level communication (cluster-wide, per-epoch): 0.931 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.931 GB
****** Accuracy Results ******
Highest valid_acc: 0.5292
Target test_acc: 0.5242
Epoch to reach the target acc: 4799
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
