Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
        It takes 0.018 seconds.
        It takes 0.017 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.006 seconds.
        It takes 0.007 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.125 Gbps (per GPU), 473.002 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.830 Gbps (per GPU), 470.639 Gbps (aggregated)
The layer-level communication performance: 58.834 Gbps (per GPU), 470.674 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.579 Gbps (per GPU), 468.629 Gbps (aggregated)
The layer-level communication performance: 58.547 Gbps (per GPU), 468.378 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.346 Gbps (per GPU), 466.766 Gbps (aggregated)
The layer-level communication performance: 58.299 Gbps (per GPU), 466.393 Gbps (aggregated)
The layer-level communication performance: 58.265 Gbps (per GPU), 466.124 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.789 Gbps (per GPU), 1246.311 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.780 Gbps (per GPU), 1246.242 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.760 Gbps (per GPU), 1246.083 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.778 Gbps (per GPU), 1246.221 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.775 Gbps (per GPU), 1246.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.786 Gbps (per GPU), 1246.285 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.774 Gbps (per GPU), 1246.195 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.705 Gbps (per GPU), 1245.640 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.789 Gbps (per GPU), 798.313 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.792 Gbps (per GPU), 798.338 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.791 Gbps (per GPU), 798.326 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.788 Gbps (per GPU), 798.307 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.790 Gbps (per GPU), 798.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.793 Gbps (per GPU), 798.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.788 Gbps (per GPU), 798.301 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.792 Gbps (per GPU), 798.339 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.582 Gbps (per GPU), 276.660 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.584 Gbps (per GPU), 276.674 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.584 Gbps (per GPU), 276.676 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.583 Gbps (per GPU), 276.665 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.582 Gbps (per GPU), 276.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.579 Gbps (per GPU), 276.633 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.580 Gbps (per GPU), 276.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.581 Gbps (per GPU), 276.646 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.35ms  0.52ms  1.46  2.81K  0.03M
 chk_1  0.41ms  0.36ms  0.52ms  1.46  2.82K  0.03M
 chk_2  0.41ms  0.35ms  0.52ms  1.46  2.80K  0.03M
 chk_3  0.41ms  0.36ms  0.52ms  1.45  2.80K  0.03M
 chk_4  0.41ms  0.36ms  0.52ms  1.46  2.82K  0.03M
 chk_5  0.41ms  0.36ms  0.52ms  1.45  2.75K  0.03M
 chk_6  0.40ms  0.35ms  0.51ms  1.46  2.71K  0.03M
 chk_7  0.41ms  0.36ms  0.52ms  1.45  2.76K  0.03M
 chk_8  0.41ms  0.36ms  0.52ms  1.45  2.79K  0.03M
 chk_9  0.41ms  0.35ms  0.52ms  1.46  2.81K  0.03M
chk_10  0.41ms  0.35ms  0.52ms  1.46  2.81K  0.03M
chk_11  0.41ms  0.36ms  0.52ms  1.45  2.74K  0.03M
chk_12  0.41ms  0.36ms  0.52ms  1.45  2.76K  0.03M
chk_13  0.41ms  0.36ms  0.52ms  1.46  2.75K  0.03M
chk_14  0.41ms  0.35ms  0.52ms  1.46  2.81K  0.03M
chk_15  0.41ms  0.36ms  0.52ms  1.45  2.77K  0.03M
chk_16  0.41ms  0.36ms  0.52ms  1.45  2.78K  0.03M
chk_17  0.41ms  0.36ms  0.53ms  1.46  2.79K  0.03M
chk_18  0.41ms  0.36ms  0.52ms  1.45  2.82K  0.03M
chk_19  0.41ms  0.35ms  0.51ms  1.47  2.81K  0.03M
chk_20  0.41ms  0.36ms  0.52ms  1.45  2.77K  0.03M
chk_21  0.42ms  0.36ms  0.52ms  1.45  2.84K  0.02M
chk_22  0.41ms  0.36ms  0.52ms  1.46  2.78K  0.03M
chk_23  0.41ms  0.36ms  0.52ms  1.45  2.80K  0.03M
chk_24  0.41ms  0.36ms  0.52ms  1.45  2.80K  0.03M
chk_25  0.41ms  0.35ms  0.52ms  1.46  2.81K  0.03M
chk_26  0.41ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_27  0.41ms  0.36ms  0.53ms  1.45  2.79K  0.03M
chk_28  0.41ms  0.36ms  0.52ms  1.45  2.77K  0.03M
chk_29  0.41ms  0.35ms  0.52ms  1.46  2.77K  0.03M
chk_30  0.41ms  0.36ms  0.52ms  1.46  2.80K  0.03M
chk_31  0.41ms  0.36ms  0.52ms  1.46  2.78K  0.03M
   Avg  0.41  0.36  0.52
   Max  0.42  0.36  0.53
   Min  0.40  0.35  0.51
 Ratio  1.03  1.04  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.604 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 57.113 ms
Partition 0 [0, 4) has cost: 47.390 ms
Partition 1 [4, 8) has cost: 45.690 ms
Partition 2 [8, 12) has cost: 45.690 ms
Partition 3 [12, 16) has cost: 45.690 ms
Partition 4 [16, 20) has cost: 45.690 ms
Partition 5 [20, 24) has cost: 45.690 ms
Partition 6 [24, 29) has cost: 57.113 ms
Partition 7 [29, 33) has cost: 50.895 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 29.091 ms
GPU 0, Compute+Comm Time: 20.787 ms, Bubble Time: 5.286 ms, Imbalance Overhead: 3.017 ms
GPU 1, Compute+Comm Time: 20.577 ms, Bubble Time: 5.173 ms, Imbalance Overhead: 3.340 ms
GPU 2, Compute+Comm Time: 20.577 ms, Bubble Time: 5.041 ms, Imbalance Overhead: 3.472 ms
GPU 3, Compute+Comm Time: 20.577 ms, Bubble Time: 4.918 ms, Imbalance Overhead: 3.595 ms
GPU 4, Compute+Comm Time: 20.577 ms, Bubble Time: 4.812 ms, Imbalance Overhead: 3.701 ms
GPU 5, Compute+Comm Time: 20.577 ms, Bubble Time: 4.696 ms, Imbalance Overhead: 3.817 ms
GPU 6, Compute+Comm Time: 24.514 ms, Bubble Time: 4.576 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 21.556 ms, Bubble Time: 4.663 ms, Imbalance Overhead: 2.871 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.279 ms
GPU 0, Compute+Comm Time: 39.000 ms, Bubble Time: 8.101 ms, Imbalance Overhead: 3.178 ms
GPU 1, Compute+Comm Time: 42.260 ms, Bubble Time: 8.020 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 34.774 ms, Bubble Time: 8.210 ms, Imbalance Overhead: 7.295 ms
GPU 3, Compute+Comm Time: 34.774 ms, Bubble Time: 8.403 ms, Imbalance Overhead: 7.103 ms
GPU 4, Compute+Comm Time: 34.774 ms, Bubble Time: 8.579 ms, Imbalance Overhead: 6.926 ms
GPU 5, Compute+Comm Time: 34.774 ms, Bubble Time: 8.778 ms, Imbalance Overhead: 6.727 ms
GPU 6, Compute+Comm Time: 34.774 ms, Bubble Time: 8.975 ms, Imbalance Overhead: 6.530 ms
GPU 7, Compute+Comm Time: 36.263 ms, Bubble Time: 9.170 ms, Imbalance Overhead: 4.845 ms
The estimated cost of the whole pipeline: 83.338 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 102.803 ms
Partition 0 [0, 8) has cost: 93.080 ms
Partition 1 [8, 16) has cost: 91.381 ms
Partition 2 [16, 25) has cost: 102.803 ms
Partition 3 [25, 33) has cost: 96.586 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.341 ms
GPU 0, Compute+Comm Time: 23.989 ms, Bubble Time: 4.895 ms, Imbalance Overhead: 2.457 ms
GPU 1, Compute+Comm Time: 24.347 ms, Bubble Time: 4.714 ms, Imbalance Overhead: 2.280 ms
GPU 2, Compute+Comm Time: 26.786 ms, Bubble Time: 4.555 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 24.838 ms, Bubble Time: 4.669 ms, Imbalance Overhead: 1.834 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.224 ms
GPU 0, Compute+Comm Time: 40.707 ms, Bubble Time: 7.522 ms, Imbalance Overhead: 1.996 ms
GPU 1, Compute+Comm Time: 42.814 ms, Bubble Time: 7.410 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 38.594 ms, Bubble Time: 7.636 ms, Imbalance Overhead: 3.994 ms
GPU 3, Compute+Comm Time: 38.878 ms, Bubble Time: 7.900 ms, Imbalance Overhead: 3.446 ms
    The estimated cost with 2 DP ways is 85.643 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 195.883 ms
Partition 0 [0, 17) has cost: 195.883 ms
Partition 1 [17, 33) has cost: 187.966 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 43.891 ms
GPU 0, Compute+Comm Time: 39.121 ms, Bubble Time: 4.770 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.323 ms, Bubble Time: 4.872 ms, Imbalance Overhead: 0.696 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.287 ms
GPU 0, Compute+Comm Time: 53.432 ms, Bubble Time: 6.796 ms, Imbalance Overhead: 1.059 ms
GPU 1, Compute+Comm Time: 54.632 ms, Bubble Time: 6.655 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 110.436 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 383.850 ms
Partition 0 [0, 33) has cost: 383.850 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 133.643 ms
GPU 0, Compute+Comm Time: 133.643 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 148.969 ms
GPU 0, Compute+Comm Time: 148.969 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 296.743 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1, starting model training...
Num Stages: 8 / 8
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9923	TrainAcc 0.4222	ValidAcc 0.4245	TestAcc 0.4234	BestValid 0.4245
	Epoch 50:	Loss 1.6092	TrainAcc 0.4767	ValidAcc 0.4787	TestAcc 0.4765	BestValid 0.4787
	Epoch 100:	Loss 1.5773	TrainAcc 0.4768	ValidAcc 0.4775	TestAcc 0.4769	BestValid 0.4787
	Epoch 150:	Loss 1.5512	TrainAcc 0.4749	ValidAcc 0.4746	TestAcc 0.4741	BestValid 0.4787
	Epoch 200:	Loss 1.5308	TrainAcc 0.4942	ValidAcc 0.4953	TestAcc 0.4932	BestValid 0.4953
	Epoch 250:	Loss 1.5184	TrainAcc 0.4979	ValidAcc 0.4966	TestAcc 0.4950	BestValid 0.4966
	Epoch 300:	Loss 1.5006	TrainAcc 0.4986	ValidAcc 0.4959	TestAcc 0.4968	BestValid 0.4966
	Epoch 350:	Loss 1.4913	TrainAcc 0.5013	ValidAcc 0.4958	TestAcc 0.4961	BestValid 0.4966
	Epoch 400:	Loss 1.4792	TrainAcc 0.5018	ValidAcc 0.4958	TestAcc 0.4953	BestValid 0.4966
	Epoch 450:	Loss 1.4729	TrainAcc 0.4931	ValidAcc 0.4855	TestAcc 0.4846	BestValid 0.4966
	Epoch 500:	Loss 1.4638	TrainAcc 0.5036	ValidAcc 0.4965	TestAcc 0.4977	BestValid 0.4966
	Epoch 550:	Loss 1.4615	TrainAcc 0.5081	ValidAcc 0.5006	TestAcc 0.5015	BestValid 0.5006
	Epoch 600:	Loss 1.4537	TrainAcc 0.5084	ValidAcc 0.5000	TestAcc 0.5018	BestValid 0.5006
	Epoch 650:	Loss 1.4490	TrainAcc 0.5134	ValidAcc 0.5042	TestAcc 0.5064	BestValid 0.5042
	Epoch 700:	Loss 1.4466	TrainAcc 0.5143	ValidAcc 0.5045	TestAcc 0.5060	BestValid 0.5045
	Epoch 750:	Loss 1.4433	TrainAcc 0.5133	ValidAcc 0.5035	TestAcc 0.5051	BestValid 0.5045
	Epoch 800:	Loss 1.4423	TrainAcc 0.5145	ValidAcc 0.5038	TestAcc 0.5054	BestValid 0.5045
	Epoch 850:	Loss 1.4385	TrainAcc 0.5166	ValidAcc 0.5048	TestAcc 0.5074	BestValid 0.5048
	Epoch 900:	Loss 1.4371	TrainAcc 0.5168	ValidAcc 0.5041	TestAcc 0.5037	BestValid 0.5048
	Epoch 950:	Loss 1.4299	TrainAcc 0.5203	ValidAcc 0.5081	TestAcc 0.5097	BestValid 0.5081
	Epoch 1000:	Loss 1.4326	TrainAcc 0.5211	ValidAcc 0.5080	TestAcc 0.5093	BestValid 0.5081
	Epoch 1050:	Loss 1.4295	TrainAcc 0.5199	ValidAcc 0.5069	TestAcc 0.5063	BestValid 0.5081
	Epoch 1100:	Loss 1.4291	TrainAcc 0.5214	ValidAcc 0.5069	TestAcc 0.5070	BestValid 0.5081
	Epoch 1150:	Loss 1.4266	TrainAcc 0.5171	ValidAcc 0.5024	TestAcc 0.5027	BestValid 0.5081
	Epoch 1200:	Loss 1.4251	TrainAcc 0.5261	ValidAcc 0.5138	TestAcc 0.5123	BestValid 0.5138
	Epoch 1250:	Loss 1.4226	TrainAcc 0.5243	ValidAcc 0.5098	TestAcc 0.5092	BestValid 0.5138
	Epoch 1300:	Loss 1.4209	TrainAcc 0.5234	ValidAcc 0.5095	TestAcc 0.5096	BestValid 0.5138
	Epoch 1350:	Loss 1.4211	TrainAcc 0.5205	ValidAcc 0.5049	TestAcc 0.5044	BestValid 0.5138
	Epoch 1400:	Loss 1.4206	TrainAcc 0.5265	ValidAcc 0.5109	TestAcc 0.5111	BestValid 0.5138
	Epoch 1450:	Loss 1.4199	TrainAcc 0.5257	ValidAcc 0.5086	TestAcc 0.5095	BestValid 0.5138
	Epoch 1500:	Loss 1.4162	TrainAcc 0.5256	ValidAcc 0.5099	TestAcc 0.5094	BestValid 0.5138
	Epoch 1550:	Loss 1.4124	TrainAcc 0.5298	ValidAcc 0.5125	TestAcc 0.5124	BestValid 0.5138
	Epoch 1600:	Loss 1.4149	TrainAcc 0.5255	ValidAcc 0.5079	TestAcc 0.5084	BestValid 0.5138
	Epoch 1650:	Loss 1.4091	TrainAcc 0.5285	ValidAcc 0.5105	TestAcc 0.5106	BestValid 0.5138
	Epoch 1700:	Loss 1.4103	TrainAcc 0.5259	ValidAcc 0.5069	TestAcc 0.5070	BestValid 0.5138
	Epoch 1750:	Loss 1.4069	TrainAcc 0.5320	ValidAcc 0.5137	TestAcc 0.5133	BestValid 0.5138
	Epoch 1800:	Loss 1.4045	TrainAcc 0.5313	ValidAcc 0.5129	TestAcc 0.5123	BestValid 0.5138
	Epoch 1850:	Loss 1.4030	TrainAcc 0.5260	ValidAcc 0.5050	TestAcc 0.5041	BestValid 0.5138
	Epoch 1900:	Loss 1.4016	TrainAcc 0.5152	ValidAcc 0.4967	TestAcc 0.4963	BestValid 0.5138
	Epoch 1950:	Loss 1.3998	TrainAcc 0.5291	ValidAcc 0.5075	TestAcc 0.5065	BestValid 0.5138
	Epoch 2000:	Loss 1.4031	TrainAcc 0.5138	ValidAcc 0.4950	TestAcc 0.4941	BestValid 0.5138
	Epoch 2050:	Loss 1.3989	TrainAcc 0.5231	ValidAcc 0.5020	TestAcc 0.5026	BestValid 0.5138
	Epoch 2100:	Loss 1.3964	TrainAcc 0.5318	ValidAcc 0.5101	TestAcc 0.5100	BestValid 0.5138
	Epoch 2150:	Loss 1.3959	TrainAcc 0.5282	ValidAcc 0.5052	TestAcc 0.5049	BestValid 0.5138
	Epoch 2200:	Loss 1.3948	TrainAcc 0.5197	ValidAcc 0.4976	TestAcc 0.4979	BestValid 0.5138
	Epoch 2250:	Loss 1.3963	TrainAcc 0.5282	ValidAcc 0.5047	TestAcc 0.5050	BestValid 0.5138
	Epoch 2300:	Loss 1.3920	TrainAcc 0.5301	ValidAcc 0.5056	TestAcc 0.5056	BestValid 0.5138
	Epoch 2350:	Loss 1.3907	TrainAcc 0.5299	ValidAcc 0.5049	TestAcc 0.5061	BestValid 0.5138
	Epoch 2400:	Loss 1.3864	TrainAcc 0.5434	ValidAcc 0.5156	TestAcc 0.5167	BestValid 0.5156
	Epoch 2450:	Loss 1.3866	TrainAcc 0.5362	ValidAcc 0.5104	TestAcc 0.5103	BestValid 0.5156
	Epoch 2500:	Loss 1.3892	TrainAcc 0.5263	ValidAcc 0.5022	TestAcc 0.5013	BestValid 0.5156
	Epoch 2550:	Loss 1.3852	TrainAcc 0.5344	ValidAcc 0.5076	TestAcc 0.5071	BestValid 0.5156
	Epoch 2600:	Loss 1.3878	TrainAcc 0.5360	ValidAcc 0.5085	TestAcc 0.5079	BestValid 0.5156
	Epoch 2650:	Loss 1.3851	TrainAcc 0.5428	ValidAcc 0.5143	TestAcc 0.5142	BestValid 0.5156
	Epoch 2700:	Loss 1.3800	TrainAcc 0.5249	ValidAcc 0.4996	TestAcc 0.5000	BestValid 0.5156
	Epoch 2750:	Loss 1.3788	TrainAcc 0.5321	ValidAcc 0.5048	TestAcc 0.5038	BestValid 0.5156
	Epoch 2800:	Loss 1.3826	TrainAcc 0.5233	ValidAcc 0.4980	TestAcc 0.4980	BestValid 0.5156
	Epoch 2850:	Loss 1.3772	TrainAcc 0.5399	ValidAcc 0.5096	TestAcc 0.5102	BestValid 0.5156
	Epoch 2900:	Loss 1.3791	TrainAcc 0.5478	ValidAcc 0.5177	TestAcc 0.5171	BestValid 0.5177
	Epoch 2950:	Loss 1.3777	TrainAcc 0.5320	ValidAcc 0.5033	TestAcc 0.5035	BestValid 0.5177
	Epoch 3000:	Loss 1.3743	TrainAcc 0.5385	ValidAcc 0.5072	TestAcc 0.5072	BestValid 0.5177
	Epoch 3050:	Loss 1.3750	TrainAcc 0.5488	ValidAcc 0.5179	TestAcc 0.5173	BestValid 0.5179
	Epoch 3100:	Loss 1.3693	TrainAcc 0.5400	ValidAcc 0.5081	TestAcc 0.5086	BestValid 0.5179
	Epoch 3150:	Loss 1.3699	TrainAcc 0.5427	ValidAcc 0.5099	TestAcc 0.5101	BestValid 0.5179
	Epoch 3200:	Loss 1.3719	TrainAcc 0.5483	ValidAcc 0.5144	TestAcc 0.5162	BestValid 0.5179
	Epoch 3250:	Loss 1.3710	TrainAcc 0.5467	ValidAcc 0.5122	TestAcc 0.5128	BestValid 0.5179
	Epoch 3300:	Loss 1.3658	TrainAcc 0.5451	ValidAcc 0.5108	TestAcc 0.5106	BestValid 0.5179
	Epoch 3350:	Loss 1.3630	TrainAcc 0.5447	ValidAcc 0.5100	TestAcc 0.5110	BestValid 0.5179
	Epoch 3400:	Loss 1.3684	TrainAcc 0.5527	ValidAcc 0.5177	TestAcc 0.5176	BestValid 0.5179
	Epoch 3450:	Loss 1.3660	TrainAcc 0.5513	ValidAcc 0.5150	TestAcc 0.5157	BestValid 0.5179
	Epoch 3500:	Loss 1.3634	TrainAcc 0.5554	ValidAcc 0.5202	TestAcc 0.5191	BestValid 0.5202
	Epoch 3550:	Loss 1.3663	TrainAcc 0.5282	ValidAcc 0.4952	TestAcc 0.4969	BestValid 0.5202
	Epoch 3600:	Loss 1.3585	TrainAcc 0.5529	ValidAcc 0.5138	TestAcc 0.5152	BestValid 0.5202
	Epoch 3650:	Loss 1.3621	TrainAcc 0.5531	ValidAcc 0.5152	TestAcc 0.5156	BestValid 0.5202
	Epoch 3700:	Loss 1.3595	TrainAcc 0.5440	ValidAcc 0.5065	TestAcc 0.5084	BestValid 0.5202
	Epoch 3750:	Loss 1.3595	TrainAcc 0.5456	ValidAcc 0.5071	TestAcc 0.5092	BestValid 0.5202
	Epoch 3800:	Loss 1.3558	TrainAcc 0.5459	ValidAcc 0.5072	TestAcc 0.5088	BestValid 0.5202
	Epoch 3850:	Loss 1.3546	TrainAcc 0.5519	ValidAcc 0.5132	TestAcc 0.5131	BestValid 0.5202
	Epoch 3900:	Loss 1.3544	TrainAcc 0.5605	ValidAcc 0.5211	TestAcc 0.5206	BestValid 0.5211
	Epoch 3950:	Loss 1.3546	TrainAcc 0.5472	ValidAcc 0.5084	TestAcc 0.5097	BestValid 0.5211
	Epoch 4000:	Loss 1.3515	TrainAcc 0.5575	ValidAcc 0.5160	TestAcc 0.5167	BestValid 0.5211
	Epoch 4050:	Loss 1.3508	TrainAcc 0.5511	ValidAcc 0.5110	TestAcc 0.5117	BestValid 0.5211
	Epoch 4100:	Loss 1.3523	TrainAcc 0.5606	ValidAcc 0.5174	TestAcc 0.5179	BestValid 0.5211
	Epoch 4150:	Loss 1.3498	TrainAcc 0.5429	ValidAcc 0.5057	TestAcc 0.5063	BestValid 0.5211
	Epoch 4200:	Loss 1.3530	TrainAcc 0.5475	ValidAcc 0.5071	TestAcc 0.5082	BestValid 0.5211
	Epoch 4250:	Loss 1.3475	TrainAcc 0.5614	ValidAcc 0.5177	TestAcc 0.5175	BestValid 0.5211
	Epoch 4300:	Loss 1.3488	TrainAcc 0.5589	ValidAcc 0.5168	TestAcc 0.5157	BestValid 0.5211
	Epoch 4350:	Loss 1.3515	TrainAcc 0.5668	ValidAcc 0.5227	TestAcc 0.5234	BestValid 0.5227
	Epoch 4400:	Loss 1.3473	TrainAcc 0.5443	ValidAcc 0.5058	TestAcc 0.5056	BestValid 0.5227
	Epoch 4450:	Loss 1.3414	TrainAcc 0.5624	ValidAcc 0.5173	TestAcc 0.5172	BestValid 0.5227
	Epoch 4500:	Loss 1.3486	TrainAcc 0.5576	ValidAcc 0.5120	TestAcc 0.5136	BestValid 0.5227
	Epoch 4550:	Loss 1.3451	TrainAcc 0.5636	ValidAcc 0.5176	TestAcc 0.5190	BestValid 0.5227
	Epoch 4600:	Loss 1.3399	TrainAcc 0.5617	ValidAcc 0.5169	TestAcc 0.5173	BestValid 0.5227
	Epoch 4650:	Loss 1.3442	TrainAcc 0.5652	ValidAcc 0.5199	TestAcc 0.5195	BestValid 0.5227
	Epoch 4700:	Loss 1.3425	TrainAcc 0.5655	ValidAcc 0.5191	TestAcc 0.5193	BestValid 0.5227
	Epoch 4750:	Loss 1.3380	TrainAcc 0.5696	ValidAcc 0.5244	TestAcc 0.5228	BestValid 0.5244
	Epoch 4800:	Loss 1.3357	TrainAcc 0.5692	ValidAcc 0.5220	TestAcc 0.5220	BestValid 0.5244
	Epoch 4850:	Loss 1.3395	TrainAcc 0.5668	ValidAcc 0.5196	TestAcc 0.5194	BestValid 0.5244
	Epoch 4900:	Loss 1.3370	TrainAcc 0.5696	ValidAcc 0.5230	TestAcc 0.5220	BestValid 0.5244
	Epoch 4950:	Loss 1.3383	TrainAcc 0.5731	ValidAcc 0.5252	TestAcc 0.5249	BestValid 0.5252
	Epoch 5000:	Loss 1.3377	TrainAcc 0.5717	ValidAcc 0.5242	TestAcc 0.5236	BestValid 0.5252
****** Epoch Time (Excluding Evaluation Cost): 0.094 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.648 ms (Max: 18.657, Min: 15.622, Sum: 141.187)
Cluster-Wide Average, Compute: 51.023 ms (Max: 62.205, Min: 47.198, Sum: 408.186)
Cluster-Wide Average, Communication-Layer: 14.315 ms (Max: 16.762, Min: 10.811, Sum: 114.517)
Cluster-Wide Average, Bubble-Imbalance: 9.686 ms (Max: 13.221, Min: 2.242, Sum: 77.491)
Cluster-Wide Average, Communication-Graph: 0.451 ms (Max: 0.500, Min: 0.393, Sum: 3.611)
Cluster-Wide Average, Optimization: 0.094 ms (Max: 0.112, Min: 0.087, Sum: 0.751)
Cluster-Wide Average, Others: 1.190 ms (Max: 4.051, Min: 0.774, Sum: 9.520)
****** Breakdown Sum: 94.408 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.004 GB (Max: 4.174, Min: 2.821, Sum: 24.030)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 68.883 Gbps (Max: 82.697, Min: 51.490, Sum: 551.061)
Layer-level communication (cluster-wide, per-epoch): 0.931 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.931 GB
****** Accuracy Results ******
Highest valid_acc: 0.5252
Target test_acc: 0.5249
Epoch to reach the target acc: 4949
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
