Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INITDONE MPI INIT
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2

Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.021 seconds.
        It takes 0.019 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.120 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.119 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.191 Gbps (per GPU), 481.524 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.877 Gbps (per GPU), 479.019 Gbps (aggregated)
The layer-level communication performance: 59.866 Gbps (per GPU), 478.928 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.640 Gbps (per GPU), 477.121 Gbps (aggregated)
The layer-level communication performance: 59.606 Gbps (per GPU), 476.844 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.391 Gbps (per GPU), 475.126 Gbps (aggregated)
The layer-level communication performance: 59.343 Gbps (per GPU), 474.741 Gbps (aggregated)
The layer-level communication performance: 59.310 Gbps (per GPU), 474.477 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.734 Gbps (per GPU), 1245.873 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.711 Gbps (per GPU), 1245.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.656 Gbps (per GPU), 1245.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.708 Gbps (per GPU), 1245.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.711 Gbps (per GPU), 1245.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.711 Gbps (per GPU), 1245.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.647 Gbps (per GPU), 1245.178 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.717 Gbps (per GPU), 1245.733 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.073 Gbps (per GPU), 792.581 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.066 Gbps (per GPU), 792.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.073 Gbps (per GPU), 792.587 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.064 Gbps (per GPU), 792.512 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.073 Gbps (per GPU), 792.581 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.041 Gbps (per GPU), 792.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.073 Gbps (per GPU), 792.587 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.986 Gbps (per GPU), 791.889 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.294 Gbps (per GPU), 282.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.294 Gbps (per GPU), 282.350 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.291 Gbps (per GPU), 282.332 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.293 Gbps (per GPU), 282.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.294 Gbps (per GPU), 282.354 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.289 Gbps (per GPU), 282.312 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.294 Gbps (per GPU), 282.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.291 Gbps (per GPU), 282.332 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.45ms  0.85ms  1.52ms  3.40  2.81K  0.03M
 chk_1  0.45ms  0.85ms  1.53ms  3.41  2.82K  0.03M
 chk_2  0.44ms  0.84ms  1.52ms  3.42  2.80K  0.03M
 chk_3  0.45ms  0.84ms  1.52ms  3.35  2.80K  0.03M
 chk_4  0.46ms  0.84ms  1.52ms  3.33  2.82K  0.03M
 chk_5  0.46ms  0.85ms  1.52ms  3.32  2.75K  0.03M
 chk_6  0.45ms  0.84ms  1.51ms  3.36  2.71K  0.03M
 chk_7  0.45ms  0.84ms  1.52ms  3.35  2.76K  0.03M
 chk_8  0.45ms  0.84ms  1.52ms  3.37  2.79K  0.03M
 chk_9  0.45ms  0.84ms  1.53ms  3.38  2.81K  0.03M
chk_10  0.45ms  0.84ms  1.52ms  3.35  2.81K  0.03M
chk_11  0.46ms  0.84ms  1.53ms  3.35  2.74K  0.03M
chk_12  0.45ms  0.84ms  1.52ms  3.35  2.76K  0.03M
chk_13  0.46ms  0.84ms  1.52ms  3.29  2.75K  0.03M
chk_14  0.45ms  0.84ms  1.52ms  3.37  2.81K  0.03M
chk_15  0.45ms  0.84ms  1.52ms  3.36  2.77K  0.03M
chk_16  0.45ms  0.84ms  1.52ms  3.37  2.78K  0.03M
chk_17  0.46ms  0.85ms  1.53ms  3.34  2.79K  0.03M
chk_18  0.46ms  0.84ms  1.53ms  3.35  2.82K  0.03M
chk_19  0.45ms  0.83ms  1.51ms  3.38  2.81K  0.03M
chk_20  0.46ms  0.84ms  1.52ms  3.34  2.77K  0.03M
chk_21  0.64ms  0.85ms  1.52ms  2.39  2.84K  0.02M
chk_22  0.57ms  0.84ms  1.52ms  2.65  2.78K  0.03M
chk_23  0.55ms  0.84ms  1.52ms  2.75  2.80K  0.03M
chk_24  0.45ms  0.84ms  1.52ms  3.35  2.80K  0.03M
chk_25  0.45ms  0.84ms  1.52ms  3.38  2.81K  0.03M
chk_26  0.45ms  0.84ms  1.52ms  3.38  2.81K  0.03M
chk_27  0.45ms  0.84ms  1.53ms  3.37  2.79K  0.03M
chk_28  0.45ms  0.84ms  1.52ms  3.35  2.77K  0.03M
chk_29  0.45ms  0.84ms  1.52ms  3.38  2.77K  0.03M
chk_30  0.45ms  0.84ms  1.53ms  3.37  2.80K  0.03M
chk_31  0.45ms  0.84ms  1.53ms  3.38  2.78K  0.03M
   Avg  0.47  0.84  1.52
   Max  0.64  0.85  1.53
   Min  0.44  0.83  1.51
 Ratio  1.43  1.02  1.02
   Var  0.00  0.00  0.00
Profiling takes 1.137 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 48)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 152)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [256, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [152, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [308, 360)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [48, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [360, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 256)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[48, 100)...
+++++++++ Node 3 initializing the weights for op[152, 204)...
+++++++++ Node 0 initializing the weights for op[0, 48)...
+++++++++ Node 2 initializing the weights for op[100, 152)...
+++++++++ Node 4 initializing the weights for op[204, 256)...
+++++++++ Node 6 initializing the weights for op[308, 360)...
+++++++++ Node 7 initializing the weights for op[360, 421)...
+++++++++ Node 5 initializing the weights for op[256, 308)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2950	TrainAcc 0.4041	ValidAcc 0.4079	TestAcc 0.4026	BestValid 0.4079
	Epoch 50:	Loss 1.5537	TrainAcc 0.4650	ValidAcc 0.4609	TestAcc 0.4640	BestValid 0.4609
	Epoch 100:	Loss 1.4531	TrainAcc 0.5039	ValidAcc 0.5008	TestAcc 0.5027	BestValid 0.5008
	Epoch 150:	Loss 1.4078	TrainAcc 0.5182	ValidAcc 0.5125	TestAcc 0.5147	BestValid 0.5125
	Epoch 200:	Loss 1.3864	TrainAcc 0.5196	ValidAcc 0.5119	TestAcc 0.5137	BestValid 0.5125
	Epoch 250:	Loss 1.3712	TrainAcc 0.5285	ValidAcc 0.5169	TestAcc 0.5227	BestValid 0.5169
	Epoch 300:	Loss 1.3593	TrainAcc 0.5260	ValidAcc 0.5114	TestAcc 0.5163	BestValid 0.5169
	Epoch 350:	Loss 1.3442	TrainAcc 0.5287	ValidAcc 0.5154	TestAcc 0.5188	BestValid 0.5169
	Epoch 400:	Loss 1.3375	TrainAcc 0.5323	ValidAcc 0.5186	TestAcc 0.5223	BestValid 0.5186
	Epoch 450:	Loss 1.3269	TrainAcc 0.5427	ValidAcc 0.5266	TestAcc 0.5294	BestValid 0.5266
	Epoch 500:	Loss 1.3196	TrainAcc 0.5475	ValidAcc 0.5295	TestAcc 0.5320	BestValid 0.5295
	Epoch 550:	Loss 1.3139	TrainAcc 0.5435	ValidAcc 0.5216	TestAcc 0.5243	BestValid 0.5295
	Epoch 600:	Loss 1.3048	TrainAcc 0.5539	ValidAcc 0.5303	TestAcc 0.5324	BestValid 0.5303
	Epoch 650:	Loss 1.3025	TrainAcc 0.5529	ValidAcc 0.5303	TestAcc 0.5342	BestValid 0.5303
	Epoch 700:	Loss 1.2927	TrainAcc 0.5560	ValidAcc 0.5290	TestAcc 0.5350	BestValid 0.5303
	Epoch 750:	Loss 1.2897	TrainAcc 0.5432	ValidAcc 0.5175	TestAcc 0.5203	BestValid 0.5303
	Epoch 800:	Loss 1.2815	TrainAcc 0.5537	ValidAcc 0.5268	TestAcc 0.5322	BestValid 0.5303
	Epoch 850:	Loss 1.2760	TrainAcc 0.5661	ValidAcc 0.5335	TestAcc 0.5374	BestValid 0.5335
	Epoch 900:	Loss 1.2771	TrainAcc 0.5669	ValidAcc 0.5367	TestAcc 0.5374	BestValid 0.5367
	Epoch 950:	Loss 1.2668	TrainAcc 0.5706	ValidAcc 0.5311	TestAcc 0.5365	BestValid 0.5367
	Epoch 1000:	Loss 1.2675	TrainAcc 0.5555	ValidAcc 0.5255	TestAcc 0.5318	BestValid 0.5367
	Epoch 1050:	Loss 1.2548	TrainAcc 0.5741	ValidAcc 0.5363	TestAcc 0.5364	BestValid 0.5367
	Epoch 1100:	Loss 1.2527	TrainAcc 0.5723	ValidAcc 0.5354	TestAcc 0.5357	BestValid 0.5367
	Epoch 1150:	Loss 1.2470	TrainAcc 0.5725	ValidAcc 0.5293	TestAcc 0.5296	BestValid 0.5367
	Epoch 1200:	Loss 1.2417	TrainAcc 0.5820	ValidAcc 0.5342	TestAcc 0.5388	BestValid 0.5367
	Epoch 1250:	Loss 1.2416	TrainAcc 0.5725	ValidAcc 0.5295	TestAcc 0.5278	BestValid 0.5367
	Epoch 1300:	Loss 1.2374	TrainAcc 0.5813	ValidAcc 0.5356	TestAcc 0.5393	BestValid 0.5367
	Epoch 1350:	Loss 1.2287	TrainAcc 0.5743	ValidAcc 0.5317	TestAcc 0.5371	BestValid 0.5367
	Epoch 1400:	Loss 1.2252	TrainAcc 0.5780	ValidAcc 0.5282	TestAcc 0.5362	BestValid 0.5367
	Epoch 1450:	Loss 1.2224	TrainAcc 0.5869	ValidAcc 0.5333	TestAcc 0.5402	BestValid 0.5367
	Epoch 1500:	Loss 1.2163	TrainAcc 0.5810	ValidAcc 0.5238	TestAcc 0.5329	BestValid 0.5367
	Epoch 1550:	Loss 1.2178	TrainAcc 0.5917	ValidAcc 0.5322	TestAcc 0.5378	BestValid 0.5367
	Epoch 1600:	Loss 1.2105	TrainAcc 0.5874	ValidAcc 0.5329	TestAcc 0.5344	BestValid 0.5367
	Epoch 1650:	Loss 1.2044	TrainAcc 0.5951	ValidAcc 0.5362	TestAcc 0.5404	BestValid 0.5367
	Epoch 1700:	Loss 1.2072	TrainAcc 0.5979	ValidAcc 0.5366	TestAcc 0.5376	BestValid 0.5367
	Epoch 1750:	Loss 1.2024	TrainAcc 0.5993	ValidAcc 0.5364	TestAcc 0.5398	BestValid 0.5367
	Epoch 1800:	Loss 1.1988	TrainAcc 0.6002	ValidAcc 0.5354	TestAcc 0.5407	BestValid 0.5367
	Epoch 1850:	Loss 1.1868	TrainAcc 0.5905	ValidAcc 0.5186	TestAcc 0.5218	BestValid 0.5367
	Epoch 1900:	Loss 1.1844	TrainAcc 0.5981	ValidAcc 0.5329	TestAcc 0.5405	BestValid 0.5367
	Epoch 1950:	Loss 1.1794	TrainAcc 0.5853	ValidAcc 0.5238	TestAcc 0.5300	BestValid 0.5367
	Epoch 2000:	Loss 1.1796	TrainAcc 0.6004	ValidAcc 0.5271	TestAcc 0.5350	BestValid 0.5367
	Epoch 2050:	Loss 1.1733	TrainAcc 0.6033	ValidAcc 0.5279	TestAcc 0.5267	BestValid 0.5367
	Epoch 2100:	Loss 1.1706	TrainAcc 0.5915	ValidAcc 0.5268	TestAcc 0.5328	BestValid 0.5367
	Epoch 2150:	Loss 1.1670	TrainAcc 0.5929	ValidAcc 0.5253	TestAcc 0.5309	BestValid 0.5367
	Epoch 2200:	Loss 1.1651	TrainAcc 0.5957	ValidAcc 0.5250	TestAcc 0.5313	BestValid 0.5367
	Epoch 2250:	Loss 1.1619	TrainAcc 0.6176	ValidAcc 0.5354	TestAcc 0.5391	BestValid 0.5367
	Epoch 2300:	Loss 1.1547	TrainAcc 0.6178	ValidAcc 0.5361	TestAcc 0.5404	BestValid 0.5367
	Epoch 2350:	Loss 1.1468	TrainAcc 0.6000	ValidAcc 0.5241	TestAcc 0.5296	BestValid 0.5367
	Epoch 2400:	Loss 1.1479	TrainAcc 0.6229	ValidAcc 0.5227	TestAcc 0.5278	BestValid 0.5367
	Epoch 2450:	Loss 1.1546	TrainAcc 0.5526	ValidAcc 0.5068	TestAcc 0.5101	BestValid 0.5367
	Epoch 2500:	Loss 1.1434	TrainAcc 0.6227	ValidAcc 0.5300	TestAcc 0.5383	BestValid 0.5367
	Epoch 2550:	Loss 1.1358	TrainAcc 0.5989	ValidAcc 0.5221	TestAcc 0.5284	BestValid 0.5367
	Epoch 2600:	Loss 1.1325	TrainAcc 0.6334	ValidAcc 0.5281	TestAcc 0.5334	BestValid 0.5367
	Epoch 2650:	Loss 1.1284	TrainAcc 0.6314	ValidAcc 0.5314	TestAcc 0.5339	BestValid 0.5367
	Epoch 2700:	Loss 1.1275	TrainAcc 0.6000	ValidAcc 0.5142	TestAcc 0.5143	BestValid 0.5367
	Epoch 2750:	Loss 1.1216	TrainAcc 0.6323	ValidAcc 0.5337	TestAcc 0.5379	BestValid 0.5367
	Epoch 2800:	Loss 1.1172	TrainAcc 0.6311	ValidAcc 0.5321	TestAcc 0.5383	BestValid 0.5367
	Epoch 2850:	Loss 1.1121	TrainAcc 0.6461	ValidAcc 0.5272	TestAcc 0.5322	BestValid 0.5367
	Epoch 2900:	Loss 1.1165	TrainAcc 0.6385	ValidAcc 0.5224	TestAcc 0.5270	BestValid 0.5367
	Epoch 2950:	Loss 1.1084	TrainAcc 0.6277	ValidAcc 0.5203	TestAcc 0.5195	BestValid 0.5367
	Epoch 3000:	Loss 1.0993	TrainAcc 0.6397	ValidAcc 0.5298	TestAcc 0.5363	BestValid 0.5367
	Epoch 3050:	Loss 1.0934	TrainAcc 0.6363	ValidAcc 0.5276	TestAcc 0.5325	BestValid 0.5367
	Epoch 3100:	Loss 1.0893	TrainAcc 0.6551	ValidAcc 0.5253	TestAcc 0.5322	BestValid 0.5367
	Epoch 3150:	Loss 1.0876	TrainAcc 0.6530	ValidAcc 0.5249	TestAcc 0.5288	BestValid 0.5367
	Epoch 3200:	Loss 1.0928	TrainAcc 0.6391	ValidAcc 0.5207	TestAcc 0.5244	BestValid 0.5367
	Epoch 3250:	Loss 1.0772	TrainAcc 0.6421	ValidAcc 0.5264	TestAcc 0.5305	BestValid 0.5367
	Epoch 3300:	Loss 1.0796	TrainAcc 0.6584	ValidAcc 0.5172	TestAcc 0.5240	BestValid 0.5367
	Epoch 3350:	Loss 1.0706	TrainAcc 0.6502	ValidAcc 0.5136	TestAcc 0.5196	BestValid 0.5367
	Epoch 3400:	Loss 1.0654	TrainAcc 0.6653	ValidAcc 0.5241	TestAcc 0.5279	BestValid 0.5367
	Epoch 3450:	Loss 1.0612	TrainAcc 0.6601	ValidAcc 0.5288	TestAcc 0.5324	BestValid 0.5367
	Epoch 3500:	Loss 1.0590	TrainAcc 0.6432	ValidAcc 0.5222	TestAcc 0.5263	BestValid 0.5367
	Epoch 3550:	Loss 1.0567	TrainAcc 0.6673	ValidAcc 0.5222	TestAcc 0.5245	BestValid 0.5367
	Epoch 3600:	Loss 1.0492	TrainAcc 0.6726	ValidAcc 0.5222	TestAcc 0.5290	BestValid 0.5367
	Epoch 3650:	Loss 1.0525	TrainAcc 0.6613	ValidAcc 0.5257	TestAcc 0.5322	BestValid 0.5367
	Epoch 3700:	Loss 1.0429	TrainAcc 0.6666	ValidAcc 0.5240	TestAcc 0.5292	BestValid 0.5367
	Epoch 3750:	Loss 1.0415	TrainAcc 0.6689	ValidAcc 0.5253	TestAcc 0.5298	BestValid 0.5367
	Epoch 3800:	Loss 1.0393	TrainAcc 0.6594	ValidAcc 0.5102	TestAcc 0.5141	BestValid 0.5367
	Epoch 3850:	Loss 1.0421	TrainAcc 0.6816	ValidAcc 0.5194	TestAcc 0.5244	BestValid 0.5367
	Epoch 3900:	Loss 1.0232	TrainAcc 0.6642	ValidAcc 0.5214	TestAcc 0.5266	BestValid 0.5367
	Epoch 3950:	Loss 1.0215	TrainAcc 0.6805	ValidAcc 0.5142	TestAcc 0.5170	BestValid 0.5367
	Epoch 4000:	Loss 1.0297	TrainAcc 0.6557	ValidAcc 0.5179	TestAcc 0.5233	BestValid 0.5367
	Epoch 4050:	Loss 1.0166	TrainAcc 0.6528	ValidAcc 0.4990	TestAcc 0.5049	BestValid 0.5367
	Epoch 4100:	Loss 1.0143	TrainAcc 0.6772	ValidAcc 0.5100	TestAcc 0.5164	BestValid 0.5367
	Epoch 4150:	Loss 1.0077	TrainAcc 0.6813	ValidAcc 0.5003	TestAcc 0.5016	BestValid 0.5367
	Epoch 4200:	Loss 1.0020	TrainAcc 0.6897	ValidAcc 0.5196	TestAcc 0.5260	BestValid 0.5367
	Epoch 4250:	Loss 1.0003	TrainAcc 0.6910	ValidAcc 0.5155	TestAcc 0.5200	BestValid 0.5367
	Epoch 4300:	Loss 0.9984	TrainAcc 0.6886	ValidAcc 0.5204	TestAcc 0.5252	BestValid 0.5367
	Epoch 4350:	Loss 0.9966	TrainAcc 0.6780	ValidAcc 0.5005	TestAcc 0.5074	BestValid 0.5367
	Epoch 4400:	Loss 0.9961	TrainAcc 0.7066	ValidAcc 0.5140	TestAcc 0.5198	BestValid 0.5367
	Epoch 4450:	Loss 0.9820	TrainAcc 0.7037	ValidAcc 0.5141	TestAcc 0.5204	BestValid 0.5367
	Epoch 4500:	Loss 0.9767	TrainAcc 0.6998	ValidAcc 0.5162	TestAcc 0.5221	BestValid 0.5367
	Epoch 4550:	Loss 0.9769	TrainAcc 0.7108	ValidAcc 0.5109	TestAcc 0.5141	BestValid 0.5367
	Epoch 4600:	Loss 0.9694	TrainAcc 0.7108	ValidAcc 0.5168	TestAcc 0.5214	BestValid 0.5367
	Epoch 4650:	Loss 0.9716	TrainAcc 0.7026	ValidAcc 0.5144	TestAcc 0.5215	BestValid 0.5367
	Epoch 4700:	Loss 0.9653	TrainAcc 0.7051	ValidAcc 0.5144	TestAcc 0.5224	BestValid 0.5367
	Epoch 4750:	Loss 0.9615	TrainAcc 0.7129	ValidAcc 0.5127	TestAcc 0.5225	BestValid 0.5367
	Epoch 4800:	Loss 0.9532	TrainAcc 0.7006	ValidAcc 0.5143	TestAcc 0.5216	BestValid 0.5367
	Epoch 4850:	Loss 0.9476	TrainAcc 0.7231	ValidAcc 0.5105	TestAcc 0.5167	BestValid 0.5367
	Epoch 4900:	Loss 0.9505	TrainAcc 0.6749	ValidAcc 0.4984	TestAcc 0.5019	BestValid 0.5367
	Epoch 4950:	Loss 0.9489	TrainAcc 0.6880	ValidAcc 0.5158	TestAcc 0.5240	BestValid 0.5367
	Epoch 5000:	Loss 0.9370	TrainAcc 0.6945	ValidAcc 0.4910	TestAcc 0.4941	BestValid 0.5367
****** Epoch Time (Excluding Evaluation Cost): 0.161 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 27.970 ms (Max: 29.979, Min: 23.996, Sum: 223.763)
Cluster-Wide Average, Compute: 105.640 ms (Max: 123.086, Min: 96.979, Sum: 845.123)
Cluster-Wide Average, Communication-Layer: 10.566 ms (Max: 12.218, Min: 7.574, Sum: 84.527)
Cluster-Wide Average, Bubble-Imbalance: 14.763 ms (Max: 24.550, Min: 1.439, Sum: 118.107)
Cluster-Wide Average, Communication-Graph: 0.515 ms (Max: 0.630, Min: 0.452, Sum: 4.122)
Cluster-Wide Average, Optimization: 0.257 ms (Max: 0.282, Min: 0.248, Sum: 2.058)
Cluster-Wide Average, Others: 1.525 ms (Max: 4.287, Min: 1.109, Sum: 12.202)
****** Breakdown Sum: 161.238 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.378 GB (Max: 3.962, Min: 3.231, Sum: 27.021)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 46.587 Gbps (Max: 53.850, Min: 36.979, Sum: 372.694)
Layer-level communication (cluster-wide, per-epoch): 0.465 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.465 GB
****** Accuracy Results ******
Highest valid_acc: 0.5367
Target test_acc: 0.5374
Epoch to reach the target acc: 899
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
