Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3

Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.035 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.020 seconds.
        It takes 0.019 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.116 seconds.
Building the Label Vector...
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.614 Gbps (per GPU), 484.913 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.322 Gbps (per GPU), 482.578 Gbps (aggregated)
The layer-level communication performance: 60.308 Gbps (per GPU), 482.462 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.073 Gbps (per GPU), 480.588 Gbps (aggregated)
The layer-level communication performance: 60.044 Gbps (per GPU), 480.349 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.835 Gbps (per GPU), 478.680 Gbps (aggregated)
The layer-level communication performance: 59.784 Gbps (per GPU), 478.272 Gbps (aggregated)
The layer-level communication performance: 59.753 Gbps (per GPU), 478.025 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.259 Gbps (per GPU), 1250.072 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.256 Gbps (per GPU), 1250.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.256 Gbps (per GPU), 1250.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.207 Gbps (per GPU), 1249.653 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.262 Gbps (per GPU), 1250.095 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.271 Gbps (per GPU), 1250.165 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.256 Gbps (per GPU), 1250.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.215 Gbps (per GPU), 1249.723 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.707 Gbps (per GPU), 813.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.708 Gbps (per GPU), 813.664 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.708 Gbps (per GPU), 813.664 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.706 Gbps (per GPU), 813.645 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.697 Gbps (per GPU), 813.579 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.706 Gbps (per GPU), 813.651 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.706 Gbps (per GPU), 813.645 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.708 Gbps (per GPU), 813.663 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.289 Gbps (per GPU), 258.312 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.288 Gbps (per GPU), 258.307 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.284 Gbps (per GPU), 258.273 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.288 Gbps (per GPU), 258.302 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.286 Gbps (per GPU), 258.290 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.283 Gbps (per GPU), 258.265 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.285 Gbps (per GPU), 258.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.282 Gbps (per GPU), 258.258 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.45ms  0.86ms  1.53ms  3.42  2.81K  0.03M
 chk_1  0.47ms  0.85ms  1.54ms  3.25  2.82K  0.03M
 chk_2  0.45ms  0.85ms  1.53ms  3.40  2.80K  0.03M
 chk_3  0.46ms  0.85ms  1.54ms  3.36  2.80K  0.03M
 chk_4  0.46ms  0.85ms  1.53ms  3.35  2.82K  0.03M
 chk_5  0.46ms  0.85ms  1.54ms  3.33  2.75K  0.03M
 chk_6  0.45ms  0.84ms  1.52ms  3.38  2.71K  0.03M
 chk_7  0.46ms  0.85ms  1.53ms  3.36  2.76K  0.03M
 chk_8  0.45ms  0.85ms  1.53ms  3.36  2.79K  0.03M
 chk_9  0.46ms  0.84ms  1.53ms  3.35  2.81K  0.03M
chk_10  0.45ms  0.84ms  1.53ms  3.37  2.81K  0.03M
chk_11  0.46ms  0.85ms  1.53ms  3.34  2.74K  0.03M
chk_12  0.46ms  0.85ms  1.53ms  3.35  2.76K  0.03M
chk_13  0.46ms  0.85ms  1.53ms  3.36  2.75K  0.03M
chk_14  0.45ms  0.84ms  1.53ms  3.38  2.81K  0.03M
chk_15  0.45ms  0.85ms  1.53ms  3.37  2.77K  0.03M
chk_16  0.45ms  0.84ms  1.53ms  3.37  2.78K  0.03M
chk_17  0.46ms  0.85ms  1.53ms  3.34  2.79K  0.03M
chk_18  0.46ms  0.85ms  1.53ms  3.34  2.82K  0.03M
chk_19  0.45ms  0.84ms  1.52ms  3.38  2.81K  0.03M
chk_20  0.46ms  0.85ms  1.53ms  3.34  2.77K  0.03M
chk_21  0.46ms  0.85ms  1.53ms  3.34  2.84K  0.02M
chk_22  0.46ms  0.84ms  1.52ms  3.34  2.78K  0.03M
chk_23  0.46ms  0.85ms  1.53ms  3.35  2.80K  0.03M
chk_24  0.45ms  0.84ms  1.53ms  3.36  2.80K  0.03M
chk_25  0.45ms  0.85ms  1.53ms  3.37  2.81K  0.03M
chk_26  0.45ms  0.84ms  1.53ms  3.36  2.81K  0.03M
chk_27  0.46ms  0.85ms  1.53ms  3.35  2.79K  0.03M
chk_28  0.46ms  0.85ms  1.53ms  3.35  2.77K  0.03M
chk_29  0.46ms  0.84ms  1.53ms  3.35  2.77K  0.03M
chk_30  0.46ms  0.85ms  1.53ms  3.35  2.80K  0.03M
chk_31  0.46ms  0.85ms  1.53ms  3.35  2.78K  0.03M
   Avg  0.46  0.85  1.53
   Max  0.47  0.86  1.54
   Min  0.45  0.84  1.52
 Ratio  1.06  1.02  1.01
   Var  0.00  0.00  0.00
Profiling takes 1.139 s
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 256)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 48)
*** Node 0, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [308, 360)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [48, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [360, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 152)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [256, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [152, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 48)...
+++++++++ Node 1 initializing the weights for op[48, 100)...
+++++++++ Node 3 initializing the weights for op[152, 204)...
+++++++++ Node 4 initializing the weights for op[204, 256)...
+++++++++ Node 5 initializing the weights for op[256, 308)...
+++++++++ Node 6 initializing the weights for op[308, 360)...
+++++++++ Node 7 initializing the weights for op[360, 421)...
+++++++++ Node 2 initializing the weights for op[100, 152)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 2, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5483	TrainAcc 0.4212	ValidAcc 0.4232	TestAcc 0.4229	BestValid 0.4232
	Epoch 50:	Loss 1.5717	TrainAcc 0.4525	ValidAcc 0.4507	TestAcc 0.4523	BestValid 0.4507
	Epoch 100:	Loss 1.4789	TrainAcc 0.4989	ValidAcc 0.4970	TestAcc 0.4970	BestValid 0.4970
	Epoch 150:	Loss 1.4146	TrainAcc 0.5211	ValidAcc 0.5166	TestAcc 0.5174	BestValid 0.5166
	Epoch 200:	Loss 1.3871	TrainAcc 0.5280	ValidAcc 0.5198	TestAcc 0.5198	BestValid 0.5198
	Epoch 250:	Loss 1.3728	TrainAcc 0.5336	ValidAcc 0.5223	TestAcc 0.5266	BestValid 0.5223
	Epoch 300:	Loss 1.3606	TrainAcc 0.5284	ValidAcc 0.5163	TestAcc 0.5177	BestValid 0.5223
	Epoch 350:	Loss 1.3557	TrainAcc 0.5302	ValidAcc 0.5168	TestAcc 0.5175	BestValid 0.5223
	Epoch 400:	Loss 1.3455	TrainAcc 0.5171	ValidAcc 0.5066	TestAcc 0.5075	BestValid 0.5223
	Epoch 450:	Loss 1.3386	TrainAcc 0.5306	ValidAcc 0.5136	TestAcc 0.5180	BestValid 0.5223
	Epoch 500:	Loss 1.3323	TrainAcc 0.5416	ValidAcc 0.5247	TestAcc 0.5263	BestValid 0.5247
	Epoch 550:	Loss 1.3235	TrainAcc 0.5498	ValidAcc 0.5310	TestAcc 0.5334	BestValid 0.5310
	Epoch 600:	Loss 1.3200	TrainAcc 0.5511	ValidAcc 0.5291	TestAcc 0.5304	BestValid 0.5310
	Epoch 650:	Loss 1.3089	TrainAcc 0.5322	ValidAcc 0.5134	TestAcc 0.5176	BestValid 0.5310
	Epoch 700:	Loss 1.3025	TrainAcc 0.5516	ValidAcc 0.5267	TestAcc 0.5298	BestValid 0.5310
	Epoch 750:	Loss 1.2997	TrainAcc 0.5582	ValidAcc 0.5324	TestAcc 0.5349	BestValid 0.5324
	Epoch 800:	Loss 1.2958	TrainAcc 0.5552	ValidAcc 0.5269	TestAcc 0.5313	BestValid 0.5324
	Epoch 850:	Loss 1.2903	TrainAcc 0.5630	ValidAcc 0.5342	TestAcc 0.5377	BestValid 0.5342
	Epoch 900:	Loss 1.2898	TrainAcc 0.5610	ValidAcc 0.5329	TestAcc 0.5378	BestValid 0.5342
	Epoch 950:	Loss 1.2800	TrainAcc 0.5579	ValidAcc 0.5264	TestAcc 0.5314	BestValid 0.5342
	Epoch 1000:	Loss 1.2744	TrainAcc 0.5658	ValidAcc 0.5333	TestAcc 0.5362	BestValid 0.5342
	Epoch 1050:	Loss 1.2719	TrainAcc 0.5701	ValidAcc 0.5310	TestAcc 0.5390	BestValid 0.5342
	Epoch 1100:	Loss 1.2642	TrainAcc 0.5710	ValidAcc 0.5331	TestAcc 0.5397	BestValid 0.5342
	Epoch 1150:	Loss 1.2594	TrainAcc 0.5617	ValidAcc 0.5262	TestAcc 0.5314	BestValid 0.5342
	Epoch 1200:	Loss 1.2543	TrainAcc 0.5727	ValidAcc 0.5285	TestAcc 0.5364	BestValid 0.5342
	Epoch 1250:	Loss 1.2526	TrainAcc 0.5721	ValidAcc 0.5323	TestAcc 0.5359	BestValid 0.5342
	Epoch 1300:	Loss 1.2484	TrainAcc 0.5748	ValidAcc 0.5300	TestAcc 0.5348	BestValid 0.5342
	Epoch 1350:	Loss 1.2407	TrainAcc 0.5800	ValidAcc 0.5298	TestAcc 0.5357	BestValid 0.5342
	Epoch 1400:	Loss 1.2381	TrainAcc 0.5391	ValidAcc 0.5076	TestAcc 0.5127	BestValid 0.5342
	Epoch 1450:	Loss 1.2347	TrainAcc 0.5392	ValidAcc 0.5134	TestAcc 0.5156	BestValid 0.5342
	Epoch 1500:	Loss 1.2348	TrainAcc 0.5505	ValidAcc 0.5117	TestAcc 0.5133	BestValid 0.5342
	Epoch 1550:	Loss 1.2271	TrainAcc 0.5522	ValidAcc 0.5121	TestAcc 0.5179	BestValid 0.5342
	Epoch 1600:	Loss 1.2207	TrainAcc 0.5811	ValidAcc 0.5235	TestAcc 0.5314	BestValid 0.5342
	Epoch 1650:	Loss 1.2160	TrainAcc 0.5884	ValidAcc 0.5320	TestAcc 0.5351	BestValid 0.5342
	Epoch 1700:	Loss 1.2130	TrainAcc 0.5934	ValidAcc 0.5363	TestAcc 0.5392	BestValid 0.5363
	Epoch 1750:	Loss 1.2058	TrainAcc 0.5968	ValidAcc 0.5339	TestAcc 0.5408	BestValid 0.5363
	Epoch 1800:	Loss 1.2008	TrainAcc 0.5839	ValidAcc 0.5221	TestAcc 0.5294	BestValid 0.5363
	Epoch 1850:	Loss 1.2049	TrainAcc 0.5358	ValidAcc 0.4987	TestAcc 0.5032	BestValid 0.5363
	Epoch 1900:	Loss 1.1950	TrainAcc 0.6005	ValidAcc 0.5303	TestAcc 0.5360	BestValid 0.5363
	Epoch 1950:	Loss 1.1960	TrainAcc 0.5975	ValidAcc 0.5303	TestAcc 0.5370	BestValid 0.5363
	Epoch 2000:	Loss 1.1890	TrainAcc 0.5945	ValidAcc 0.5312	TestAcc 0.5359	BestValid 0.5363
	Epoch 2050:	Loss 1.1860	TrainAcc 0.5902	ValidAcc 0.5260	TestAcc 0.5309	BestValid 0.5363
	Epoch 2100:	Loss 1.1813	TrainAcc 0.5952	ValidAcc 0.5313	TestAcc 0.5326	BestValid 0.5363
	Epoch 2150:	Loss 1.1716	TrainAcc 0.6037	ValidAcc 0.5312	TestAcc 0.5355	BestValid 0.5363
	Epoch 2200:	Loss 1.1733	TrainAcc 0.5984	ValidAcc 0.5290	TestAcc 0.5303	BestValid 0.5363
	Epoch 2250:	Loss 1.1642	TrainAcc 0.6013	ValidAcc 0.5291	TestAcc 0.5332	BestValid 0.5363
	Epoch 2300:	Loss 1.1609	TrainAcc 0.5996	ValidAcc 0.5133	TestAcc 0.5204	BestValid 0.5363
	Epoch 2350:	Loss 1.1546	TrainAcc 0.6102	ValidAcc 0.5350	TestAcc 0.5373	BestValid 0.5363
	Epoch 2400:	Loss 1.1484	TrainAcc 0.6222	ValidAcc 0.5280	TestAcc 0.5340	BestValid 0.5363
	Epoch 2450:	Loss 1.1480	TrainAcc 0.6080	ValidAcc 0.5101	TestAcc 0.5162	BestValid 0.5363
	Epoch 2500:	Loss 1.1442	TrainAcc 0.6235	ValidAcc 0.5306	TestAcc 0.5349	BestValid 0.5363
	Epoch 2550:	Loss 1.1437	TrainAcc 0.6202	ValidAcc 0.5344	TestAcc 0.5353	BestValid 0.5363
	Epoch 2600:	Loss 1.1302	TrainAcc 0.6179	ValidAcc 0.5332	TestAcc 0.5354	BestValid 0.5363
	Epoch 2650:	Loss 1.1347	TrainAcc 0.6214	ValidAcc 0.5321	TestAcc 0.5342	BestValid 0.5363
	Epoch 2700:	Loss 1.1311	TrainAcc 0.5809	ValidAcc 0.5021	TestAcc 0.5037	BestValid 0.5363
	Epoch 2750:	Loss 1.1237	TrainAcc 0.6277	ValidAcc 0.5264	TestAcc 0.5315	BestValid 0.5363
	Epoch 2800:	Loss 1.1186	TrainAcc 0.6373	ValidAcc 0.5261	TestAcc 0.5328	BestValid 0.5363
	Epoch 2850:	Loss 1.1185	TrainAcc 0.6335	ValidAcc 0.5299	TestAcc 0.5358	BestValid 0.5363
	Epoch 2900:	Loss 1.1145	TrainAcc 0.6318	ValidAcc 0.5175	TestAcc 0.5253	BestValid 0.5363
	Epoch 2950:	Loss 1.1086	TrainAcc 0.6434	ValidAcc 0.5220	TestAcc 0.5299	BestValid 0.5363
	Epoch 3000:	Loss 1.1034	TrainAcc 0.6412	ValidAcc 0.5260	TestAcc 0.5321	BestValid 0.5363
	Epoch 3050:	Loss 1.0994	TrainAcc 0.6097	ValidAcc 0.5125	TestAcc 0.5155	BestValid 0.5363
	Epoch 3100:	Loss 1.0930	TrainAcc 0.6458	ValidAcc 0.5199	TestAcc 0.5274	BestValid 0.5363
	Epoch 3150:	Loss 1.0916	TrainAcc 0.6508	ValidAcc 0.5255	TestAcc 0.5294	BestValid 0.5363
	Epoch 3200:	Loss 1.0830	TrainAcc 0.6519	ValidAcc 0.5125	TestAcc 0.5210	BestValid 0.5363
	Epoch 3250:	Loss 1.0857	TrainAcc 0.6550	ValidAcc 0.5277	TestAcc 0.5333	BestValid 0.5363
	Epoch 3300:	Loss 1.0744	TrainAcc 0.6474	ValidAcc 0.5056	TestAcc 0.5128	BestValid 0.5363
	Epoch 3350:	Loss 1.0739	TrainAcc 0.6456	ValidAcc 0.5211	TestAcc 0.5266	BestValid 0.5363
	Epoch 3400:	Loss 1.0708	TrainAcc 0.6598	ValidAcc 0.5131	TestAcc 0.5162	BestValid 0.5363
	Epoch 3450:	Loss 1.0663	TrainAcc 0.6227	ValidAcc 0.5175	TestAcc 0.5191	BestValid 0.5363
	Epoch 3500:	Loss 1.0590	TrainAcc 0.6533	ValidAcc 0.5241	TestAcc 0.5279	BestValid 0.5363
	Epoch 3550:	Loss 1.0525	TrainAcc 0.6202	ValidAcc 0.5142	TestAcc 0.5173	BestValid 0.5363
	Epoch 3600:	Loss 1.0476	TrainAcc 0.6638	ValidAcc 0.5213	TestAcc 0.5247	BestValid 0.5363
	Epoch 3650:	Loss 1.0432	TrainAcc 0.6734	ValidAcc 0.5189	TestAcc 0.5222	BestValid 0.5363
	Epoch 3700:	Loss 1.0436	TrainAcc 0.6529	ValidAcc 0.5229	TestAcc 0.5259	BestValid 0.5363
	Epoch 3750:	Loss 1.0353	TrainAcc 0.5801	ValidAcc 0.4554	TestAcc 0.4658	BestValid 0.5363
	Epoch 3800:	Loss 1.0393	TrainAcc 0.6720	ValidAcc 0.5075	TestAcc 0.5110	BestValid 0.5363
	Epoch 3850:	Loss 1.0277	TrainAcc 0.6517	ValidAcc 0.4922	TestAcc 0.4984	BestValid 0.5363
	Epoch 3900:	Loss 1.0229	TrainAcc 0.6823	ValidAcc 0.5203	TestAcc 0.5227	BestValid 0.5363
	Epoch 3950:	Loss 1.0222	TrainAcc 0.6382	ValidAcc 0.5084	TestAcc 0.5107	BestValid 0.5363
	Epoch 4000:	Loss 1.0235	TrainAcc 0.6848	ValidAcc 0.5151	TestAcc 0.5184	BestValid 0.5363
	Epoch 4050:	Loss 1.0291	TrainAcc 0.6861	ValidAcc 0.5097	TestAcc 0.5168	BestValid 0.5363
	Epoch 4100:	Loss 1.0048	TrainAcc 0.6936	ValidAcc 0.5177	TestAcc 0.5204	BestValid 0.5363
	Epoch 4150:	Loss 1.0023	TrainAcc 0.6874	ValidAcc 0.5128	TestAcc 0.5150	BestValid 0.5363
	Epoch 4200:	Loss 0.9989	TrainAcc 0.6848	ValidAcc 0.5176	TestAcc 0.5208	BestValid 0.5363
	Epoch 4250:	Loss 0.9953	TrainAcc 0.6982	ValidAcc 0.5124	TestAcc 0.5181	BestValid 0.5363
	Epoch 4300:	Loss 0.9878	TrainAcc 0.6984	ValidAcc 0.5034	TestAcc 0.5106	BestValid 0.5363
	Epoch 4350:	Loss 0.9895	TrainAcc 0.7025	ValidAcc 0.5178	TestAcc 0.5201	BestValid 0.5363
	Epoch 4400:	Loss 0.9819	TrainAcc 0.7040	ValidAcc 0.5144	TestAcc 0.5179	BestValid 0.5363
	Epoch 4450:	Loss 0.9794	TrainAcc 0.6958	ValidAcc 0.4991	TestAcc 0.5058	BestValid 0.5363
	Epoch 4500:	Loss 0.9735	TrainAcc 0.7019	ValidAcc 0.5146	TestAcc 0.5149	BestValid 0.5363
	Epoch 4550:	Loss 0.9706	TrainAcc 0.6735	ValidAcc 0.5091	TestAcc 0.5108	BestValid 0.5363
	Epoch 4600:	Loss 0.9621	TrainAcc 0.7019	ValidAcc 0.5005	TestAcc 0.5070	BestValid 0.5363
	Epoch 4650:	Loss 0.9596	TrainAcc 0.6970	ValidAcc 0.4898	TestAcc 0.4961	BestValid 0.5363
	Epoch 4700:	Loss 0.9569	TrainAcc 0.7193	ValidAcc 0.5106	TestAcc 0.5143	BestValid 0.5363
	Epoch 4750:	Loss 0.9447	TrainAcc 0.6923	ValidAcc 0.4892	TestAcc 0.4975	BestValid 0.5363
	Epoch 4800:	Loss 0.9481	TrainAcc 0.7157	ValidAcc 0.5125	TestAcc 0.5149	BestValid 0.5363
	Epoch 4850:	Loss 0.9422	TrainAcc 0.7260	ValidAcc 0.5014	TestAcc 0.5063	BestValid 0.5363
	Epoch 4900:	Loss 0.9369	TrainAcc 0.6876	ValidAcc 0.4844	TestAcc 0.4907	BestValid 0.5363
	Epoch 4950:	Loss 0.9388	TrainAcc 0.7127	ValidAcc 0.4856	TestAcc 0.4919	BestValid 0.5363
	Epoch 5000:	Loss 0.9258	TrainAcc 0.7207	ValidAcc 0.4909	TestAcc 0.4973	BestValid 0.5363
****** Epoch Time (Excluding Evaluation Cost): 0.161 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 27.942 ms (Max: 29.891, Min: 24.017, Sum: 223.537)
Cluster-Wide Average, Compute: 105.343 ms (Max: 122.728, Min: 97.313, Sum: 842.744)
Cluster-Wide Average, Communication-Layer: 10.573 ms (Max: 12.233, Min: 7.592, Sum: 84.582)
Cluster-Wide Average, Bubble-Imbalance: 14.788 ms (Max: 24.065, Min: 1.450, Sum: 118.307)
Cluster-Wide Average, Communication-Graph: 0.507 ms (Max: 0.581, Min: 0.451, Sum: 4.055)
Cluster-Wide Average, Optimization: 0.257 ms (Max: 0.282, Min: 0.246, Sum: 2.054)
Cluster-Wide Average, Others: 1.494 ms (Max: 4.251, Min: 1.059, Sum: 11.948)
****** Breakdown Sum: 160.904 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.378 GB (Max: 3.962, Min: 3.231, Sum: 27.021)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 46.559 Gbps (Max: 54.129, Min: 36.899, Sum: 372.472)
Layer-level communication (cluster-wide, per-epoch): 0.465 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.465 GB
****** Accuracy Results ******
Highest valid_acc: 0.5363
Target test_acc: 0.5392
Epoch to reach the target acc: 1699
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
