Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3

Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.035 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.020 seconds.
        It takes 0.019 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.116 seconds.
Building the Label Vector...
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.614 Gbps (per GPU), 484.913 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.322 Gbps (per GPU), 482.578 Gbps (aggregated)
The layer-level communication performance: 60.308 Gbps (per GPU), 482.462 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.073 Gbps (per GPU), 480.588 Gbps (aggregated)
The layer-level communication performance: 60.044 Gbps (per GPU), 480.349 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.835 Gbps (per GPU), 478.680 Gbps (aggregated)
The layer-level communication performance: 59.784 Gbps (per GPU), 478.272 Gbps (aggregated)
The layer-level communication performance: 59.753 Gbps (per GPU), 478.025 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.259 Gbps (per GPU), 1250.072 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.256 Gbps (per GPU), 1250.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.256 Gbps (per GPU), 1250.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.207 Gbps (per GPU), 1249.653 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.262 Gbps (per GPU), 1250.095 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.271 Gbps (per GPU), 1250.165 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.256 Gbps (per GPU), 1250.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.215 Gbps (per GPU), 1249.723 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.707 Gbps (per GPU), 813.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.708 Gbps (per GPU), 813.664 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.708 Gbps (per GPU), 813.664 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.706 Gbps (per GPU), 813.645 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.697 Gbps (per GPU), 813.579 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.706 Gbps (per GPU), 813.651 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.706 Gbps (per GPU), 813.645 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.708 Gbps (per GPU), 813.663 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.289 Gbps (per GPU), 258.312 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.288 Gbps (per GPU), 258.307 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.284 Gbps (per GPU), 258.273 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.288 Gbps (per GPU), 258.302 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.286 Gbps (per GPU), 258.290 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.283 Gbps (per GPU), 258.265 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.285 Gbps (per GPU), 258.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.282 Gbps (per GPU), 258.258 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.45ms  0.86ms  1.53ms  3.42  2.81K  0.03M
 chk_1  0.47ms  0.85ms  1.54ms  3.25  2.82K  0.03M
 chk_2  0.45ms  0.85ms  1.53ms  3.40  2.80K  0.03M
 chk_3  0.46ms  0.85ms  1.54ms  3.36  2.80K  0.03M
 chk_4  0.46ms  0.85ms  1.53ms  3.35  2.82K  0.03M
 chk_5  0.46ms  0.85ms  1.54ms  3.33  2.75K  0.03M
 chk_6  0.45ms  0.84ms  1.52ms  3.38  2.71K  0.03M
 chk_7  0.46ms  0.85ms  1.53ms  3.36  2.76K  0.03M
 chk_8  0.45ms  0.85ms  1.53ms  3.36  2.79K  0.03M
 chk_9  0.46ms  0.84ms  1.53ms  3.35  2.81K  0.03M
chk_10  0.45ms  0.84ms  1.53ms  3.37  2.81K  0.03M
chk_11  0.46ms  0.85ms  1.53ms  3.34  2.74K  0.03M
chk_12  0.46ms  0.85ms  1.53ms  3.35  2.76K  0.03M
chk_13  0.46ms  0.85ms  1.53ms  3.36  2.75K  0.03M
chk_14  0.45ms  0.84ms  1.53ms  3.38  2.81K  0.03M
chk_15  0.45ms  0.85ms  1.53ms  3.37  2.77K  0.03M
chk_16  0.45ms  0.84ms  1.53ms  3.37  2.78K  0.03M
chk_17  0.46ms  0.85ms  1.53ms  3.34  2.79K  0.03M
chk_18  0.46ms  0.85ms  1.53ms  3.34  2.82K  0.03M
chk_19  0.45ms  0.84ms  1.52ms  3.38  2.81K  0.03M
chk_20  0.46ms  0.85ms  1.53ms  3.34  2.77K  0.03M
chk_21  0.46ms  0.85ms  1.53ms  3.34  2.84K  0.02M
chk_22  0.46ms  0.84ms  1.52ms  3.34  2.78K  0.03M
chk_23  0.46ms  0.85ms  1.53ms  3.35  2.80K  0.03M
chk_24  0.45ms  0.84ms  1.53ms  3.36  2.80K  0.03M
chk_25  0.45ms  0.85ms  1.53ms  3.37  2.81K  0.03M
chk_26  0.45ms  0.84ms  1.53ms  3.36  2.81K  0.03M
chk_27  0.46ms  0.85ms  1.53ms  3.35  2.79K  0.03M
chk_28  0.46ms  0.85ms  1.53ms  3.35  2.77K  0.03M
chk_29  0.46ms  0.84ms  1.53ms  3.35  2.77K  0.03M
chk_30  0.46ms  0.85ms  1.53ms  3.35  2.80K  0.03M
chk_31  0.46ms  0.85ms  1.53ms  3.35  2.78K  0.03M
   Avg  0.46  0.85  1.53
   Max  0.47  0.86  1.54
   Min  0.45  0.84  1.52
 Ratio  1.06  1.02  1.01
   Var  0.00  0.00  0.00
Profiling takes 1.139 s
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 256)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 48)
*** Node 0, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [308, 360)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [48, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [360, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 152)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [256, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [152, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 48)...
+++++++++ Node 1 initializing the weights for op[48, 100)...
+++++++++ Node 3 initializing the weights for op[152, 204)...
+++++++++ Node 4 initializing the weights for op[204, 256)...
+++++++++ Node 5 initializing the weights for op[256, 308)...
+++++++++ Node 6 initializing the weights for op[308, 360)...
+++++++++ Node 7 initializing the weights for op[360, 421)...
+++++++++ Node 2 initializing the weights for op[100, 152)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 2, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5483	TrainAcc 0.4212	ValidAcc 0.4232	TestAcc 0.4229	BestValid 0.4232
	Epoch 50:	Loss 1.5717	TrainAcc 0.4525	ValidAcc 0.4507	TestAcc 0.4523	BestValid 0.4507
	Epoch 100:	Loss 1.4789	TrainAcc 0.4989	ValidAcc 0.4970	TestAcc 0.4970	BestValid 0.4970
	Epoch 150:	Loss 1.4146	TrainAcc 0.5211	ValidAcc 0.5166	TestAcc 0.5174	BestValid 0.5166
	Epoch 200:	Loss 1.3871	TrainAcc 0.5280	ValidAcc 0.5198	TestAcc 0.5198	BestValid 0.5198
	Epoch 250:	Loss 1.3728	TrainAcc 0.5336	ValidAcc 0.5223	TestAcc 0.5266	BestValid 0.5223
	Epoch 300:	Loss 1.3606	TrainAcc 0.5284	ValidAcc 0.5163	TestAcc 0.5177	BestValid 0.5223
