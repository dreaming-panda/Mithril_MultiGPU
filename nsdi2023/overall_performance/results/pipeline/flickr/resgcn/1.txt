Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INITInitialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2

Initialized node 3 on machine gnerv2
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.033 seconds.
Building the CSC structure...
        It takes 0.033 seconds.
Building the CSC structure...
        It takes 0.033 seconds.
Building the CSC structure...
        It takes 0.033 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.024 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.026 seconds.
        It takes 0.026 seconds.
        It takes 0.027 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.402 seconds.
        It takes 0.403 seconds.
        It takes 0.400 seconds.
        It takes 0.404 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.009 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.010 seconds.
        It takes 0.012 seconds.
        It takes 0.012 seconds.
        It takes 0.431 seconds.
        It takes 0.431 seconds.
        It takes 0.431 seconds.
        It takes 0.431 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.010 seconds.
        It takes 0.010 seconds.
        It takes 0.010 seconds.
        It takes 0.011 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.160 Gbps (per GPU), 457.278 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.893 Gbps (per GPU), 455.144 Gbps (aggregated)
The layer-level communication performance: 56.883 Gbps (per GPU), 455.066 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.678 Gbps (per GPU), 453.428 Gbps (aggregated)
The layer-level communication performance: 56.642 Gbps (per GPU), 453.137 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.473 Gbps (per GPU), 451.783 Gbps (aggregated)
The layer-level communication performance: 56.432 Gbps (per GPU), 451.455 Gbps (aggregated)
The layer-level communication performance: 56.403 Gbps (per GPU), 451.223 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.749 Gbps (per GPU), 1269.991 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.740 Gbps (per GPU), 1269.921 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.740 Gbps (per GPU), 1269.919 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.746 Gbps (per GPU), 1269.967 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.743 Gbps (per GPU), 1269.943 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.731 Gbps (per GPU), 1269.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.680 Gbps (per GPU), 1269.438 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.811 Gbps (per GPU), 1262.489 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 87.056 Gbps (per GPU), 696.449 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 87.066 Gbps (per GPU), 696.531 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 87.063 Gbps (per GPU), 696.502 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 87.067 Gbps (per GPU), 696.535 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 87.062 Gbps (per GPU), 696.497 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 87.067 Gbps (per GPU), 696.535 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 87.051 Gbps (per GPU), 696.405 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 87.063 Gbps (per GPU), 696.506 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.301 Gbps (per GPU), 282.411 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.306 Gbps (per GPU), 282.446 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.302 Gbps (per GPU), 282.420 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.302 Gbps (per GPU), 282.416 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.300 Gbps (per GPU), 282.403 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.304 Gbps (per GPU), 282.429 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.295 Gbps (per GPU), 282.359 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.303 Gbps (per GPU), 282.424 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.43ms  0.80ms  1.44ms  3.31  2.81K  0.03M
 chk_1  0.46ms  0.80ms  1.44ms  3.15  2.82K  0.03M
 chk_2  0.43ms  0.79ms  1.43ms  3.29  2.80K  0.03M
 chk_3  0.44ms  0.80ms  1.43ms  3.23  2.80K  0.03M
 chk_4  0.44ms  0.80ms  1.44ms  3.23  2.82K  0.03M
 chk_5  0.45ms  0.80ms  1.44ms  3.22  2.75K  0.03M
 chk_6  0.43ms  0.79ms  1.42ms  3.28  2.71K  0.03M
 chk_7  0.44ms  0.80ms  1.43ms  3.25  2.76K  0.03M
 chk_8  0.44ms  0.79ms  1.43ms  3.25  2.79K  0.03M
 chk_9  0.44ms  0.79ms  1.43ms  3.27  2.81K  0.03M
chk_10  0.44ms  0.79ms  1.43ms  3.26  2.81K  0.03M
chk_11  0.44ms  0.80ms  1.44ms  3.25  2.74K  0.03M
chk_12  0.44ms  0.80ms  1.43ms  3.24  2.76K  0.03M
chk_13  0.44ms  0.80ms  1.43ms  3.25  2.75K  0.03M
chk_14  0.44ms  0.79ms  1.43ms  3.27  2.81K  0.03M
chk_15  0.44ms  0.80ms  1.43ms  3.26  2.77K  0.03M
chk_16  0.44ms  0.79ms  1.43ms  3.26  2.78K  0.03M
chk_17  0.44ms  0.80ms  1.44ms  3.25  2.79K  0.03M
chk_18  0.44ms  0.80ms  1.43ms  3.23  2.82K  0.03M
chk_19  0.44ms  0.79ms  1.42ms  3.27  2.81K  0.03M
chk_20  0.44ms  0.80ms  1.44ms  3.24  2.77K  0.03M
chk_21  0.44ms  0.80ms  1.44ms  3.24  2.84K  0.02M
chk_22  0.44ms  0.79ms  1.43ms  3.24  2.78K  0.03M
chk_23  0.44ms  0.79ms  1.43ms  3.25  2.80K  0.03M
chk_24  0.44ms  0.79ms  1.43ms  3.25  2.80K  0.03M
chk_25  0.44ms  0.80ms  1.43ms  3.27  2.81K  0.03M
chk_26  0.44ms  0.79ms  1.43ms  3.26  2.81K  0.03M
chk_27  0.44ms  0.80ms  1.44ms  3.24  2.79K  0.03M
chk_28  0.44ms  0.80ms  1.44ms  3.25  2.77K  0.03M
chk_29  0.44ms  0.79ms  1.44ms  3.28  2.77K  0.03M
chk_30  0.44ms  0.80ms  1.44ms  3.25  2.80K  0.03M
chk_31  0.44ms  0.80ms  1.44ms  3.26  2.78K  0.03M
   Avg  0.44  0.80  1.43
   Max  0.46  0.80  1.44
   Min  0.43  0.79  1.42
 Ratio  1.05  1.02  1.02
   Var  0.00  0.00  0.00
Profiling takes 1.078 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 48)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [48, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [256, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 152)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [152, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [360, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
Node 6, Pipeline Input Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 256)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [308, 360)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 48)...
+++++++++ Node 1 initializing the weights for op[48, 100)...
+++++++++ Node 2 initializing the weights for op[100, 152)...
+++++++++ Node 3 initializing the weights for op[152, 204)...
+++++++++ Node 4 initializing the weights for op[204, 256)...
+++++++++ Node 5 initializing the weights for op[256, 308)...
+++++++++ Node 7 initializing the weights for op[360, 421)...
+++++++++ Node 6 initializing the weights for op[308, 360)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5033	TrainAcc 0.4210	ValidAcc 0.4233	TestAcc 0.4233	BestValid 0.4233
	Epoch 50:	Loss 1.5275	TrainAcc 0.4778	ValidAcc 0.4742	TestAcc 0.4764	BestValid 0.4742
	Epoch 100:	Loss 1.4294	TrainAcc 0.5131	ValidAcc 0.5118	TestAcc 0.5100	BestValid 0.5118
	Epoch 150:	Loss 1.3948	TrainAcc 0.5220	ValidAcc 0.5213	TestAcc 0.5171	BestValid 0.5213
	Epoch 200:	Loss 1.3799	TrainAcc 0.5291	ValidAcc 0.5209	TestAcc 0.5197	BestValid 0.5213
	Epoch 250:	Loss 1.3649	TrainAcc 0.5344	ValidAcc 0.5239	TestAcc 0.5257	BestValid 0.5239
	Epoch 300:	Loss 1.3556	TrainAcc 0.5336	ValidAcc 0.5201	TestAcc 0.5197	BestValid 0.5239
	Epoch 350:	Loss 1.3422	TrainAcc 0.5378	ValidAcc 0.5236	TestAcc 0.5232	BestValid 0.5239
	Epoch 400:	Loss 1.3372	TrainAcc 0.5457	ValidAcc 0.5304	TestAcc 0.5300	BestValid 0.5304
	Epoch 450:	Loss 1.3255	TrainAcc 0.5472	ValidAcc 0.5273	TestAcc 0.5302	BestValid 0.5304
	Epoch 500:	Loss 1.3215	TrainAcc 0.5512	ValidAcc 0.5318	TestAcc 0.5328	BestValid 0.5318
	Epoch 550:	Loss 1.3100	TrainAcc 0.5533	ValidAcc 0.5310	TestAcc 0.5312	BestValid 0.5318
	Epoch 600:	Loss 1.3035	TrainAcc 0.5575	ValidAcc 0.5333	TestAcc 0.5347	BestValid 0.5333
	Epoch 650:	Loss 1.2980	TrainAcc 0.5596	ValidAcc 0.5351	TestAcc 0.5348	BestValid 0.5351
	Epoch 700:	Loss 1.2954	TrainAcc 0.5424	ValidAcc 0.5196	TestAcc 0.5240	BestValid 0.5351
	Epoch 750:	Loss 1.2920	TrainAcc 0.5446	ValidAcc 0.5209	TestAcc 0.5227	BestValid 0.5351
	Epoch 800:	Loss 1.2818	TrainAcc 0.5630	ValidAcc 0.5324	TestAcc 0.5345	BestValid 0.5351
	Epoch 850:	Loss 1.2781	TrainAcc 0.5694	ValidAcc 0.5361	TestAcc 0.5369	BestValid 0.5361
	Epoch 900:	Loss 1.2750	TrainAcc 0.5351	ValidAcc 0.5086	TestAcc 0.5063	BestValid 0.5361
	Epoch 950:	Loss 1.2652	TrainAcc 0.5674	ValidAcc 0.5300	TestAcc 0.5309	BestValid 0.5361
	Epoch 1000:	Loss 1.2618	TrainAcc 0.5730	ValidAcc 0.5337	TestAcc 0.5349	BestValid 0.5361
	Epoch 1050:	Loss 1.2584	TrainAcc 0.5751	ValidAcc 0.5359	TestAcc 0.5374	BestValid 0.5361
	Epoch 1100:	Loss 1.2533	TrainAcc 0.5752	ValidAcc 0.5316	TestAcc 0.5331	BestValid 0.5361
	Epoch 1150:	Loss 1.2489	TrainAcc 0.5812	ValidAcc 0.5372	TestAcc 0.5385	BestValid 0.5372
	Epoch 1200:	Loss 1.2453	TrainAcc 0.5812	ValidAcc 0.5371	TestAcc 0.5382	BestValid 0.5372
	Epoch 1250:	Loss 1.2396	TrainAcc 0.5777	ValidAcc 0.5343	TestAcc 0.5382	BestValid 0.5372
	Epoch 1300:	Loss 1.2397	TrainAcc 0.5861	ValidAcc 0.5367	TestAcc 0.5405	BestValid 0.5372
	Epoch 1350:	Loss 1.2344	TrainAcc 0.5785	ValidAcc 0.5314	TestAcc 0.5336	BestValid 0.5372
	Epoch 1400:	Loss 1.2242	TrainAcc 0.5914	ValidAcc 0.5386	TestAcc 0.5418	BestValid 0.5386
	Epoch 1450:	Loss 1.2205	TrainAcc 0.5535	ValidAcc 0.5084	TestAcc 0.5070	BestValid 0.5386
	Epoch 1500:	Loss 1.2238	TrainAcc 0.5691	ValidAcc 0.5163	TestAcc 0.5171	BestValid 0.5386
	Epoch 1550:	Loss 1.2142	TrainAcc 0.5834	ValidAcc 0.5251	TestAcc 0.5266	BestValid 0.5386
	Epoch 1600:	Loss 1.2093	TrainAcc 0.5969	ValidAcc 0.5320	TestAcc 0.5357	BestValid 0.5386
	Epoch 1650:	Loss 1.2046	TrainAcc 0.5916	ValidAcc 0.5346	TestAcc 0.5377	BestValid 0.5386
	Epoch 1700:	Loss 1.2022	TrainAcc 0.5986	ValidAcc 0.5333	TestAcc 0.5373	BestValid 0.5386
	Epoch 1750:	Loss 1.2033	TrainAcc 0.5959	ValidAcc 0.5234	TestAcc 0.5301	BestValid 0.5386
	Epoch 1800:	Loss 1.1936	TrainAcc 0.6028	ValidAcc 0.5365	TestAcc 0.5414	BestValid 0.5386
	Epoch 1850:	Loss 1.1973	TrainAcc 0.5979	ValidAcc 0.5337	TestAcc 0.5363	BestValid 0.5386
	Epoch 1900:	Loss 1.1870	TrainAcc 0.5806	ValidAcc 0.5112	TestAcc 0.5138	BestValid 0.5386
	Epoch 1950:	Loss 1.1819	TrainAcc 0.6031	ValidAcc 0.5320	TestAcc 0.5359	BestValid 0.5386
	Epoch 2000:	Loss 1.1784	TrainAcc 0.6124	ValidAcc 0.5310	TestAcc 0.5366	BestValid 0.5386
	Epoch 2050:	Loss 1.1786	TrainAcc 0.6050	ValidAcc 0.5251	TestAcc 0.5308	BestValid 0.5386
	Epoch 2100:	Loss 1.1666	TrainAcc 0.6166	ValidAcc 0.5348	TestAcc 0.5386	BestValid 0.5386
	Epoch 2150:	Loss 1.1613	TrainAcc 0.6056	ValidAcc 0.5332	TestAcc 0.5353	BestValid 0.5386
	Epoch 2200:	Loss 1.1624	TrainAcc 0.6154	ValidAcc 0.5265	TestAcc 0.5331	BestValid 0.5386
	Epoch 2250:	Loss 1.1521	TrainAcc 0.6197	ValidAcc 0.5316	TestAcc 0.5345	BestValid 0.5386
	Epoch 2300:	Loss 1.1550	TrainAcc 0.6219	ValidAcc 0.5354	TestAcc 0.5385	BestValid 0.5386
	Epoch 2350:	Loss 1.1451	TrainAcc 0.6268	ValidAcc 0.5265	TestAcc 0.5331	BestValid 0.5386
	Epoch 2400:	Loss 1.1461	TrainAcc 0.6271	ValidAcc 0.5243	TestAcc 0.5296	BestValid 0.5386
	Epoch 2450:	Loss 1.1352	TrainAcc 0.6336	ValidAcc 0.5308	TestAcc 0.5367	BestValid 0.5386
	Epoch 2500:	Loss 1.1365	TrainAcc 0.6210	ValidAcc 0.5307	TestAcc 0.5339	BestValid 0.5386
	Epoch 2550:	Loss 1.1262	TrainAcc 0.6357	ValidAcc 0.5348	TestAcc 0.5391	BestValid 0.5386
	Epoch 2600:	Loss 1.1236	TrainAcc 0.6007	ValidAcc 0.5055	TestAcc 0.5084	BestValid 0.5386
	Epoch 2650:	Loss 1.1169	TrainAcc 0.6091	ValidAcc 0.5290	TestAcc 0.5298	BestValid 0.5386
	Epoch 2700:	Loss 1.1174	TrainAcc 0.6367	ValidAcc 0.5220	TestAcc 0.5254	BestValid 0.5386
	Epoch 2750:	Loss 1.1114	TrainAcc 0.6453	ValidAcc 0.5251	TestAcc 0.5302	BestValid 0.5386
	Epoch 2800:	Loss 1.1062	TrainAcc 0.5984	ValidAcc 0.5221	TestAcc 0.5213	BestValid 0.5386
	Epoch 2850:	Loss 1.0990	TrainAcc 0.6396	ValidAcc 0.5147	TestAcc 0.5196	BestValid 0.5386
	Epoch 2900:	Loss 1.0968	TrainAcc 0.6494	ValidAcc 0.5260	TestAcc 0.5296	BestValid 0.5386
	Epoch 2950:	Loss 1.0919	TrainAcc 0.6509	ValidAcc 0.5247	TestAcc 0.5319	BestValid 0.5386
	Epoch 3000:	Loss 1.0866	TrainAcc 0.6439	ValidAcc 0.5331	TestAcc 0.5337	BestValid 0.5386
	Epoch 3050:	Loss 1.0816	TrainAcc 0.6323	ValidAcc 0.5320	TestAcc 0.5317	BestValid 0.5386
	Epoch 3100:	Loss 1.0800	TrainAcc 0.6203	ValidAcc 0.5130	TestAcc 0.5128	BestValid 0.5386
	Epoch 3150:	Loss 1.0822	TrainAcc 0.6586	ValidAcc 0.5234	TestAcc 0.5281	BestValid 0.5386
	Epoch 3200:	Loss 1.0808	TrainAcc 0.6458	ValidAcc 0.5294	TestAcc 0.5316	BestValid 0.5386
	Epoch 3250:	Loss 1.0703	TrainAcc 0.6606	ValidAcc 0.5297	TestAcc 0.5331	BestValid 0.5386
	Epoch 3300:	Loss 1.0649	TrainAcc 0.6563	ValidAcc 0.5280	TestAcc 0.5338	BestValid 0.5386
	Epoch 3350:	Loss 1.0630	TrainAcc 0.6653	ValidAcc 0.5295	TestAcc 0.5350	BestValid 0.5386
	Epoch 3400:	Loss 1.0571	TrainAcc 0.6603	ValidAcc 0.5253	TestAcc 0.5279	BestValid 0.5386
	Epoch 3450:	Loss 1.0435	TrainAcc 0.6537	ValidAcc 0.5198	TestAcc 0.5240	BestValid 0.5386
	Epoch 3500:	Loss 1.0568	TrainAcc 0.6724	ValidAcc 0.5208	TestAcc 0.5228	BestValid 0.5386
	Epoch 3550:	Loss 1.0364	TrainAcc 0.6719	ValidAcc 0.5126	TestAcc 0.5153	BestValid 0.5386
	Epoch 3600:	Loss 1.0352	TrainAcc 0.6628	ValidAcc 0.5027	TestAcc 0.5053	BestValid 0.5386
	Epoch 3650:	Loss 1.0272	TrainAcc 0.6807	ValidAcc 0.5158	TestAcc 0.5187	BestValid 0.5386
	Epoch 3700:	Loss 1.0328	TrainAcc 0.6709	ValidAcc 0.5174	TestAcc 0.5222	BestValid 0.5386
	Epoch 3750:	Loss 1.0225	TrainAcc 0.6007	ValidAcc 0.5026	TestAcc 0.5052	BestValid 0.5386
	Epoch 3800:	Loss 1.0155	TrainAcc 0.6798	ValidAcc 0.5059	TestAcc 0.5094	BestValid 0.5386
	Epoch 3850:	Loss 1.0126	TrainAcc 0.6699	ValidAcc 0.5140	TestAcc 0.5148	BestValid 0.5386
	Epoch 3900:	Loss 1.0038	TrainAcc 0.6811	ValidAcc 0.5207	TestAcc 0.5223	BestValid 0.5386
	Epoch 3950:	Loss 1.0037	TrainAcc 0.6858	ValidAcc 0.5219	TestAcc 0.5221	BestValid 0.5386
	Epoch 4000:	Loss 0.9984	TrainAcc 0.6803	ValidAcc 0.5223	TestAcc 0.5250	BestValid 0.5386
	Epoch 4050:	Loss 0.9964	TrainAcc 0.6946	ValidAcc 0.5091	TestAcc 0.5096	BestValid 0.5386
	Epoch 4100:	Loss 0.9936	TrainAcc 0.6984	ValidAcc 0.5146	TestAcc 0.5155	BestValid 0.5386
	Epoch 4150:	Loss 0.9891	TrainAcc 0.6928	ValidAcc 0.5223	TestAcc 0.5218	BestValid 0.5386
	Epoch 4200:	Loss 0.9894	TrainAcc 0.6919	ValidAcc 0.5139	TestAcc 0.5141	BestValid 0.5386
	Epoch 4250:	Loss 0.9819	TrainAcc 0.7005	ValidAcc 0.5211	TestAcc 0.5215	BestValid 0.5386
	Epoch 4300:	Loss 0.9727	TrainAcc 0.6953	ValidAcc 0.5183	TestAcc 0.5202	BestValid 0.5386
	Epoch 4350:	Loss 0.9768	TrainAcc 0.6978	ValidAcc 0.5223	TestAcc 0.5233	BestValid 0.5386
	Epoch 4400:	Loss 0.9688	TrainAcc 0.7089	ValidAcc 0.5161	TestAcc 0.5161	BestValid 0.5386
	Epoch 4450:	Loss 0.9703	TrainAcc 0.6815	ValidAcc 0.5029	TestAcc 0.5026	BestValid 0.5386
	Epoch 4500:	Loss 0.9579	TrainAcc 0.7162	ValidAcc 0.5174	TestAcc 0.5174	BestValid 0.5386
	Epoch 4550:	Loss 0.9505	TrainAcc 0.7177	ValidAcc 0.5065	TestAcc 0.5080	BestValid 0.5386
	Epoch 4600:	Loss 0.9486	TrainAcc 0.6966	ValidAcc 0.5166	TestAcc 0.5173	BestValid 0.5386
	Epoch 4650:	Loss 0.9406	TrainAcc 0.7171	ValidAcc 0.4999	TestAcc 0.4976	BestValid 0.5386
	Epoch 4700:	Loss 0.9370	TrainAcc 0.6993	ValidAcc 0.5062	TestAcc 0.5037	BestValid 0.5386
	Epoch 4750:	Loss 0.9381	TrainAcc 0.7047	ValidAcc 0.5123	TestAcc 0.5136	BestValid 0.5386
	Epoch 4800:	Loss 0.9281	TrainAcc 0.7289	ValidAcc 0.5073	TestAcc 0.5067	BestValid 0.5386
	Epoch 4850:	Loss 0.9277	TrainAcc 0.7304	ValidAcc 0.5117	TestAcc 0.5123	BestValid 0.5386
	Epoch 4900:	Loss 0.9222	TrainAcc 0.7086	ValidAcc 0.5181	TestAcc 0.5198	BestValid 0.5386
	Epoch 4950:	Loss 0.9181	TrainAcc 0.7213	ValidAcc 0.4988	TestAcc 0.4997	BestValid 0.5386
	Epoch 5000:	Loss 0.9107	TrainAcc 0.7345	ValidAcc 0.5133	TestAcc 0.5131	BestValid 0.5386
****** Epoch Time (Excluding Evaluation Cost): 0.163 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 28.251 ms (Max: 30.512, Min: 24.219, Sum: 226.009)
Cluster-Wide Average, Compute: 106.141 ms (Max: 124.617, Min: 91.878, Sum: 849.128)
Cluster-Wide Average, Communication-Layer: 10.642 ms (Max: 12.242, Min: 7.527, Sum: 85.135)
Cluster-Wide Average, Bubble-Imbalance: 15.704 ms (Max: 31.221, Min: 1.413, Sum: 125.630)
Cluster-Wide Average, Communication-Graph: 0.540 ms (Max: 0.680, Min: 0.474, Sum: 4.322)
Cluster-Wide Average, Optimization: 0.258 ms (Max: 0.284, Min: 0.249, Sum: 2.065)
Cluster-Wide Average, Others: 1.517 ms (Max: 4.289, Min: 1.080, Sum: 12.139)
****** Breakdown Sum: 163.053 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.378 GB (Max: 3.962, Min: 3.231, Sum: 27.021)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 46.236 Gbps (Max: 52.670, Min: 36.780, Sum: 369.888)
Layer-level communication (cluster-wide, per-epoch): 0.465 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.465 GB
****** Accuracy Results ******
Highest valid_acc: 0.5386
Target test_acc: 0.5418
Epoch to reach the target acc: 1399
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 3386529 RUNNING AT gnerv2
=   EXIT CODE: 9
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
[proxy:0:0@gnerv2] HYDU_sock_write (lib/utils/sock.c:250): write error (Broken pipe)
[proxy:0:0@gnerv2] PMIP_send_hdr_upstream (proxy/pmip_cb.c:40): sock write error
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Killed (signal 9)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
