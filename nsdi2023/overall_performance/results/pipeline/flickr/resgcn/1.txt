Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 4 on machine gnerv8

DONE MPI INITInitialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8

Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INITDONE MPI INIT
Initialized node 3 on machine gnerv4

Initialized node 1 on machine gnerv4
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.019 seconds.
        It takes 0.019 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.025 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.025 seconds.
        It takes 0.024 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.112 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.728 Gbps (per GPU), 485.824 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.411 Gbps (per GPU), 483.290 Gbps (aggregated)
The layer-level communication performance: 60.406 Gbps (per GPU), 483.244 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.162 Gbps (per GPU), 481.299 Gbps (aggregated)
The layer-level communication performance: 60.130 Gbps (per GPU), 481.038 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.915 Gbps (per GPU), 479.319 Gbps (aggregated)
The layer-level communication performance: 59.866 Gbps (per GPU), 478.930 Gbps (aggregated)
The layer-level communication performance: 59.831 Gbps (per GPU), 478.649 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.264 Gbps (per GPU), 1266.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.255 Gbps (per GPU), 1266.038 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.207 Gbps (per GPU), 1265.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.237 Gbps (per GPU), 1265.897 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.264 Gbps (per GPU), 1266.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.234 Gbps (per GPU), 1265.871 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.213 Gbps (per GPU), 1265.704 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.228 Gbps (per GPU), 1265.823 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.105 Gbps (per GPU), 832.844 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.104 Gbps (per GPU), 832.830 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.110 Gbps (per GPU), 832.878 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.108 Gbps (per GPU), 832.864 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.105 Gbps (per GPU), 832.837 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.108 Gbps (per GPU), 832.864 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.065 Gbps (per GPU), 832.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.109 Gbps (per GPU), 832.871 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.826 Gbps (per GPU), 302.607 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.826 Gbps (per GPU), 302.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.822 Gbps (per GPU), 302.575 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.824 Gbps (per GPU), 302.593 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.819 Gbps (per GPU), 302.549 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.816 Gbps (per GPU), 302.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.821 Gbps (per GPU), 302.571 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.786 Gbps (per GPU), 302.288 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.68ms  1.31ms  4.15  2.81K  0.03M
 chk_1  0.32ms  0.69ms  1.31ms  4.16  2.82K  0.03M
 chk_2  0.34ms  0.68ms  1.31ms  3.81  2.80K  0.03M
 chk_3  0.32ms  0.68ms  1.31ms  4.11  2.80K  0.03M
 chk_4  0.33ms  0.68ms  1.32ms  4.05  2.82K  0.03M
 chk_5  0.33ms  0.69ms  1.32ms  4.03  2.75K  0.03M
 chk_6  0.31ms  0.67ms  1.31ms  4.18  2.71K  0.03M
 chk_7  0.32ms  0.68ms  1.31ms  4.12  2.76K  0.03M
 chk_8  0.32ms  0.68ms  1.32ms  4.10  2.79K  0.03M
 chk_9  0.32ms  0.68ms  1.31ms  4.12  2.81K  0.03M
chk_10  0.32ms  0.68ms  1.31ms  4.14  2.81K  0.03M
chk_11  0.32ms  0.69ms  1.32ms  4.08  2.74K  0.03M
chk_12  0.32ms  0.68ms  1.32ms  4.10  2.76K  0.03M
chk_13  0.32ms  0.68ms  1.31ms  4.12  2.75K  0.03M
chk_14  0.32ms  0.68ms  1.32ms  4.12  2.81K  0.03M
chk_15  0.32ms  0.68ms  1.31ms  4.11  2.77K  0.03M
chk_16  0.32ms  0.68ms  1.31ms  4.13  2.78K  0.03M
chk_17  0.32ms  0.68ms  1.32ms  4.10  2.79K  0.03M
chk_18  0.32ms  0.68ms  1.32ms  4.08  2.82K  0.03M
chk_19  0.32ms  0.67ms  1.31ms  4.11  2.81K  0.03M
chk_20  0.32ms  0.68ms  1.32ms  4.07  2.77K  0.03M
chk_21  0.32ms  0.68ms  1.32ms  4.09  2.84K  0.02M
chk_22  0.32ms  0.68ms  1.31ms  4.06  2.78K  0.03M
chk_23  0.32ms  0.68ms  1.32ms  4.07  2.80K  0.03M
chk_24  0.32ms  0.68ms  1.31ms  4.09  2.80K  0.03M
chk_25  0.32ms  0.68ms  1.31ms  4.13  2.81K  0.03M
chk_26  0.32ms  0.68ms  1.31ms  4.12  2.81K  0.03M
chk_27  0.32ms  0.68ms  1.31ms  4.08  2.79K  0.03M
chk_28  0.32ms  0.68ms  1.32ms  4.08  2.77K  0.03M
chk_29  0.32ms  0.68ms  1.31ms  4.09  2.77K  0.03M
chk_30  0.32ms  0.68ms  1.32ms  4.07  2.80K  0.03M
chk_31  0.32ms  0.68ms  1.31ms  4.09  2.78K  0.03M
   Avg  0.32  0.68  1.31
   Max  0.34  0.69  1.32
   Min  0.31  0.67  1.31
 Ratio  1.10  1.02  1.01
   Var  0.00  0.00  0.00
Profiling takes 0.954 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 27)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [27, 55)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [111, 139)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [139, 167)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [167, 195)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [195, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [83, 111)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [55, 83)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[27, 55)...
+++++++++ Node 2 initializing the weights for op[55, 83)...
+++++++++ Node 4 initializing the weights for op[111, 139)...
+++++++++ Node 3 initializing the weights for op[83, 111)...
+++++++++ Node 5 initializing the weights for op[139, 167)...
+++++++++ Node 0 initializing the weights for op[0, 27)...
+++++++++ Node 6 initializing the weights for op[167, 195)...
+++++++++ Node 7 initializing the weights for op[195, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.4591	TrainAcc 0.4215	ValidAcc 0.4237	TestAcc 0.4233	BestValid 0.4237
	Epoch 50:	Loss 1.5659	TrainAcc 0.4580	ValidAcc 0.4589	TestAcc 0.4602	BestValid 0.4589
	Epoch 100:	Loss 1.5041	TrainAcc 0.4658	ValidAcc 0.4668	TestAcc 0.4653	BestValid 0.4668
	Epoch 150:	Loss 1.4661	TrainAcc 0.4798	ValidAcc 0.4784	TestAcc 0.4761	BestValid 0.4784
	Epoch 200:	Loss 1.4518	TrainAcc 0.4895	ValidAcc 0.4877	TestAcc 0.4863	BestValid 0.4877
	Epoch 250:	Loss 1.4407	TrainAcc 0.4960	ValidAcc 0.4932	TestAcc 0.4909	BestValid 0.4932
	Epoch 300:	Loss 1.4316	TrainAcc 0.4455	ValidAcc 0.4426	TestAcc 0.4400	BestValid 0.4932
	Epoch 350:	Loss 1.4244	TrainAcc 0.5036	ValidAcc 0.4985	TestAcc 0.4985	BestValid 0.4985
	Epoch 400:	Loss 1.4203	TrainAcc 0.5051	ValidAcc 0.4970	TestAcc 0.5000	BestValid 0.4985
	Epoch 450:	Loss 1.4179	TrainAcc 0.5020	ValidAcc 0.4971	TestAcc 0.4948	BestValid 0.4985
	Epoch 500:	Loss 1.4085	TrainAcc 0.5022	ValidAcc 0.4963	TestAcc 0.4951	BestValid 0.4985
	Epoch 550:	Loss 1.4016	TrainAcc 0.5123	ValidAcc 0.5027	TestAcc 0.5055	BestValid 0.5027
	Epoch 600:	Loss 1.4023	TrainAcc 0.5067	ValidAcc 0.4971	TestAcc 0.5015	BestValid 0.5027
	Epoch 650:	Loss 1.3983	TrainAcc 0.5105	ValidAcc 0.4987	TestAcc 0.5044	BestValid 0.5027
	Epoch 700:	Loss 1.3889	TrainAcc 0.5187	ValidAcc 0.5091	TestAcc 0.5102	BestValid 0.5091
	Epoch 750:	Loss 1.3894	TrainAcc 0.5132	ValidAcc 0.4998	TestAcc 0.5024	BestValid 0.5091
	Epoch 800:	Loss 1.3836	TrainAcc 0.5220	ValidAcc 0.5104	TestAcc 0.5126	BestValid 0.5104
	Epoch 850:	Loss 1.3801	TrainAcc 0.5119	ValidAcc 0.4999	TestAcc 0.5025	BestValid 0.5104
	Epoch 900:	Loss 1.3753	TrainAcc 0.5241	ValidAcc 0.5086	TestAcc 0.5094	BestValid 0.5104
	Epoch 950:	Loss 1.3732	TrainAcc 0.5192	ValidAcc 0.5069	TestAcc 0.5076	BestValid 0.5104
	Epoch 1000:	Loss 1.3697	TrainAcc 0.5305	ValidAcc 0.5154	TestAcc 0.5149	BestValid 0.5154
	Epoch 1050:	Loss 1.3634	TrainAcc 0.5117	ValidAcc 0.4970	TestAcc 0.5004	BestValid 0.5154
	Epoch 1100:	Loss 1.3623	TrainAcc 0.5268	ValidAcc 0.5075	TestAcc 0.5098	BestValid 0.5154
	Epoch 1150:	Loss 1.3552	TrainAcc 0.5323	ValidAcc 0.5126	TestAcc 0.5147	BestValid 0.5154
	Epoch 1200:	Loss 1.3572	TrainAcc 0.5059	ValidAcc 0.4853	TestAcc 0.4856	BestValid 0.5154
	Epoch 1250:	Loss 1.3530	TrainAcc 0.5157	ValidAcc 0.4997	TestAcc 0.5013	BestValid 0.5154
	Epoch 1300:	Loss 1.3519	TrainAcc 0.5281	ValidAcc 0.5078	TestAcc 0.5078	BestValid 0.5154
	Epoch 1350:	Loss 1.3521	TrainAcc 0.5326	ValidAcc 0.5131	TestAcc 0.5136	BestValid 0.5154
	Epoch 1400:	Loss 1.3460	TrainAcc 0.5322	ValidAcc 0.5152	TestAcc 0.5110	BestValid 0.5154
	Epoch 1450:	Loss 1.3483	TrainAcc 0.5366	ValidAcc 0.5142	TestAcc 0.5112	BestValid 0.5154
	Epoch 1500:	Loss 1.3379	TrainAcc 0.5412	ValidAcc 0.5196	TestAcc 0.5212	BestValid 0.5196
	Epoch 1550:	Loss 1.3368	TrainAcc 0.5361	ValidAcc 0.5165	TestAcc 0.5161	BestValid 0.5196
	Epoch 1600:	Loss 1.3361	TrainAcc 0.5156	ValidAcc 0.4970	TestAcc 0.5011	BestValid 0.5196
	Epoch 1650:	Loss 1.3326	TrainAcc 0.5343	ValidAcc 0.5150	TestAcc 0.5121	BestValid 0.5196
	Epoch 1700:	Loss 1.3336	TrainAcc 0.5424	ValidAcc 0.5174	TestAcc 0.5188	BestValid 0.5196
	Epoch 1750:	Loss 1.3268	TrainAcc 0.5429	ValidAcc 0.5195	TestAcc 0.5190	BestValid 0.5196
	Epoch 1800:	Loss 1.3284	TrainAcc 0.5426	ValidAcc 0.5160	TestAcc 0.5142	BestValid 0.5196
	Epoch 1850:	Loss 1.3227	TrainAcc 0.5410	ValidAcc 0.5170	TestAcc 0.5120	BestValid 0.5196
	Epoch 1900:	Loss 1.3246	TrainAcc 0.5339	ValidAcc 0.5054	TestAcc 0.5063	BestValid 0.5196
	Epoch 1950:	Loss 1.3164	TrainAcc 0.5527	ValidAcc 0.5208	TestAcc 0.5212	BestValid 0.5208
	Epoch 2000:	Loss 1.3155	TrainAcc 0.5464	ValidAcc 0.5168	TestAcc 0.5186	BestValid 0.5208
	Epoch 2050:	Loss 1.3198	TrainAcc 0.5224	ValidAcc 0.4930	TestAcc 0.4971	BestValid 0.5208
	Epoch 2100:	Loss 1.3134	TrainAcc 0.5431	ValidAcc 0.5098	TestAcc 0.5120	BestValid 0.5208
	Epoch 2150:	Loss 1.3096	TrainAcc 0.5554	ValidAcc 0.5218	TestAcc 0.5237	BestValid 0.5218
	Epoch 2200:	Loss 1.3089	TrainAcc 0.5336	ValidAcc 0.5107	TestAcc 0.5108	BestValid 0.5218
	Epoch 2250:	Loss 1.3032	TrainAcc 0.5469	ValidAcc 0.5139	TestAcc 0.5156	BestValid 0.5218
	Epoch 2300:	Loss 1.3077	TrainAcc 0.5555	ValidAcc 0.5196	TestAcc 0.5208	BestValid 0.5218
	Epoch 2350:	Loss 1.3073	TrainAcc 0.5072	ValidAcc 0.4810	TestAcc 0.4816	BestValid 0.5218
	Epoch 2400:	Loss 1.3022	TrainAcc 0.5450	ValidAcc 0.5092	TestAcc 0.5093	BestValid 0.5218
	Epoch 2450:	Loss 1.2976	TrainAcc 0.5603	ValidAcc 0.5233	TestAcc 0.5231	BestValid 0.5233
	Epoch 2500:	Loss 1.2963	TrainAcc 0.5464	ValidAcc 0.5127	TestAcc 0.5130	BestValid 0.5233
	Epoch 2550:	Loss 1.2938	TrainAcc 0.5538	ValidAcc 0.5146	TestAcc 0.5154	BestValid 0.5233
	Epoch 2600:	Loss 1.3015	TrainAcc 0.4586	ValidAcc 0.4462	TestAcc 0.4497	BestValid 0.5233
	Epoch 2650:	Loss 1.2891	TrainAcc 0.5540	ValidAcc 0.5125	TestAcc 0.5158	BestValid 0.5233
	Epoch 2700:	Loss 1.2937	TrainAcc 0.5338	ValidAcc 0.4993	TestAcc 0.4989	BestValid 0.5233
	Epoch 2750:	Loss 1.2878	TrainAcc 0.5576	ValidAcc 0.5197	TestAcc 0.5208	BestValid 0.5233
	Epoch 2800:	Loss 1.2874	TrainAcc 0.5572	ValidAcc 0.5197	TestAcc 0.5203	BestValid 0.5233
	Epoch 2850:	Loss 1.2827	TrainAcc 0.5656	ValidAcc 0.5226	TestAcc 0.5232	BestValid 0.5233
	Epoch 2900:	Loss 1.2847	TrainAcc 0.5430	ValidAcc 0.5015	TestAcc 0.5067	BestValid 0.5233
	Epoch 2950:	Loss 1.2916	TrainAcc 0.5610	ValidAcc 0.5241	TestAcc 0.5230	BestValid 0.5241
	Epoch 3000:	Loss 1.2832	TrainAcc 0.5700	ValidAcc 0.5251	TestAcc 0.5263	BestValid 0.5251
	Epoch 3050:	Loss 1.2796	TrainAcc 0.5593	ValidAcc 0.5219	TestAcc 0.5208	BestValid 0.5251
	Epoch 3100:	Loss 1.2773	TrainAcc 0.5452	ValidAcc 0.5087	TestAcc 0.5119	BestValid 0.5251
	Epoch 3150:	Loss 1.2764	TrainAcc 0.5507	ValidAcc 0.5108	TestAcc 0.5125	BestValid 0.5251
	Epoch 3200:	Loss 1.2753	TrainAcc 0.5675	ValidAcc 0.5224	TestAcc 0.5251	BestValid 0.5251
	Epoch 3250:	Loss 1.2742	TrainAcc 0.5714	ValidAcc 0.5260	TestAcc 0.5259	BestValid 0.5260
	Epoch 3300:	Loss 1.2743	TrainAcc 0.5684	ValidAcc 0.5220	TestAcc 0.5228	BestValid 0.5260
	Epoch 3350:	Loss 1.2681	TrainAcc 0.5619	ValidAcc 0.5166	TestAcc 0.5199	BestValid 0.5260
	Epoch 3400:	Loss 1.2692	TrainAcc 0.5695	ValidAcc 0.5256	TestAcc 0.5276	BestValid 0.5260
	Epoch 3450:	Loss 1.2677	TrainAcc 0.5395	ValidAcc 0.5026	TestAcc 0.5066	BestValid 0.5260
	Epoch 3500:	Loss 1.2849	TrainAcc 0.4927	ValidAcc 0.4749	TestAcc 0.4777	BestValid 0.5260
	Epoch 3550:	Loss 1.2687	TrainAcc 0.5388	ValidAcc 0.4965	TestAcc 0.4927	BestValid 0.5260
	Epoch 3600:	Loss 1.2657	TrainAcc 0.5161	ValidAcc 0.4863	TestAcc 0.4899	BestValid 0.5260
	Epoch 3650:	Loss 1.2609	TrainAcc 0.5446	ValidAcc 0.5052	TestAcc 0.5089	BestValid 0.5260
	Epoch 3700:	Loss 1.2596	TrainAcc 0.5524	ValidAcc 0.5110	TestAcc 0.5149	BestValid 0.5260
	Epoch 3750:	Loss 1.2573	TrainAcc 0.5737	ValidAcc 0.5253	TestAcc 0.5268	BestValid 0.5260
	Epoch 3800:	Loss 1.2625	TrainAcc 0.5695	ValidAcc 0.5124	TestAcc 0.5177	BestValid 0.5260
	Epoch 3850:	Loss 1.2615	TrainAcc 0.5341	ValidAcc 0.4998	TestAcc 0.5020	BestValid 0.5260
	Epoch 3900:	Loss 1.2572	TrainAcc 0.5717	ValidAcc 0.5264	TestAcc 0.5285	BestValid 0.5264
	Epoch 3950:	Loss 1.2546	TrainAcc 0.5660	ValidAcc 0.5212	TestAcc 0.5210	BestValid 0.5264
	Epoch 4000:	Loss 1.2511	TrainAcc 0.5774	ValidAcc 0.5228	TestAcc 0.5229	BestValid 0.5264
	Epoch 4050:	Loss 1.2517	TrainAcc 0.5560	ValidAcc 0.5055	TestAcc 0.5064	BestValid 0.5264
	Epoch 4100:	Loss 1.2467	TrainAcc 0.5772	ValidAcc 0.5206	TestAcc 0.5208	BestValid 0.5264
	Epoch 4150:	Loss 1.2528	TrainAcc 0.5312	ValidAcc 0.4890	TestAcc 0.4946	BestValid 0.5264
	Epoch 4200:	Loss 1.2487	TrainAcc 0.5682	ValidAcc 0.5155	TestAcc 0.5184	BestValid 0.5264
	Epoch 4250:	Loss 1.2489	TrainAcc 0.5723	ValidAcc 0.5145	TestAcc 0.5167	BestValid 0.5264
	Epoch 4300:	Loss 1.2473	TrainAcc 0.5861	ValidAcc 0.5265	TestAcc 0.5297	BestValid 0.5265
	Epoch 4350:	Loss 1.2445	TrainAcc 0.5796	ValidAcc 0.5245	TestAcc 0.5261	BestValid 0.5265
	Epoch 4400:	Loss 1.2607	TrainAcc 0.5585	ValidAcc 0.5179	TestAcc 0.5172	BestValid 0.5265
	Epoch 4450:	Loss 1.2460	TrainAcc 0.5843	ValidAcc 0.5246	TestAcc 0.5279	BestValid 0.5265
	Epoch 4500:	Loss 1.2435	TrainAcc 0.5680	ValidAcc 0.5118	TestAcc 0.5122	BestValid 0.5265
	Epoch 4550:	Loss 1.2440	TrainAcc 0.5762	ValidAcc 0.5215	TestAcc 0.5210	BestValid 0.5265
	Epoch 4600:	Loss 1.2376	TrainAcc 0.5774	ValidAcc 0.5180	TestAcc 0.5169	BestValid 0.5265
	Epoch 4650:	Loss 1.2426	TrainAcc 0.5842	ValidAcc 0.5256	TestAcc 0.5268	BestValid 0.5265
	Epoch 4700:	Loss 1.2470	TrainAcc 0.5425	ValidAcc 0.5033	TestAcc 0.5048	BestValid 0.5265
	Epoch 4750:	Loss 1.2388	TrainAcc 0.5691	ValidAcc 0.5099	TestAcc 0.5166	BestValid 0.5265
	Epoch 4800:	Loss 1.2343	TrainAcc 0.5712	ValidAcc 0.5198	TestAcc 0.5214	BestValid 0.5265
	Epoch 4850:	Loss 1.2382	TrainAcc 0.5867	ValidAcc 0.5270	TestAcc 0.5281	BestValid 0.5270
	Epoch 4900:	Loss 1.2369	TrainAcc 0.4875	ValidAcc 0.4656	TestAcc 0.4682	BestValid 0.5270
	Epoch 4950:	Loss 1.2298	TrainAcc 0.5761	ValidAcc 0.5196	TestAcc 0.5219	BestValid 0.5270
	Epoch 5000:	Loss 1.2309	TrainAcc 0.5827	ValidAcc 0.5275	TestAcc 0.5289	BestValid 0.5275
****** Epoch Time (Excluding Evaluation Cost): 0.134 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 23.577 ms (Max: 25.850, Min: 19.175, Sum: 188.616)
Cluster-Wide Average, Compute: 82.421 ms (Max: 101.289, Min: 76.195, Sum: 659.365)
Cluster-Wide Average, Communication-Layer: 10.103 ms (Max: 11.735, Min: 7.560, Sum: 80.825)
Cluster-Wide Average, Bubble-Imbalance: 16.232 ms (Max: 22.994, Min: 1.356, Sum: 129.857)
Cluster-Wide Average, Communication-Graph: 0.557 ms (Max: 0.671, Min: 0.479, Sum: 4.459)
Cluster-Wide Average, Optimization: 0.096 ms (Max: 0.119, Min: 0.088, Sum: 0.769)
Cluster-Wide Average, Others: 1.197 ms (Max: 4.042, Min: 0.781, Sum: 9.575)
****** Breakdown Sum: 134.183 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.068 GB (Max: 3.684, Min: 2.922, Sum: 24.544)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 48.683 Gbps (Max: 56.273, Min: 37.424, Sum: 389.461)
Layer-level communication (cluster-wide, per-epoch): 0.465 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.465 GB
****** Accuracy Results ******
Highest valid_acc: 0.5275
Target test_acc: 0.5289
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
