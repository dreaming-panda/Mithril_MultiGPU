Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.100 seconds.
Building the CSC structure...
        It takes 0.100 seconds.
Building the CSC structure...
        It takes 0.100 seconds.
Building the CSC structure...
        It takes 0.100 seconds.
Building the CSC structure...
        It takes 0.103 seconds.
Building the CSC structure...
        It takes 0.103 seconds.
Building the CSC structure...
        It takes 0.103 seconds.
Building the CSC structure...
        It takes 0.103 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.027 seconds.
        It takes 0.027 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.307 seconds.
        It takes 0.307 seconds.
        It takes 0.307 seconds.
Building the Label Vector...
        It takes 0.307 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.309 seconds.
        It takes 0.308 seconds.
        It takes 0.313 seconds.
        It takes 0.313 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.012 seconds.
        It takes 0.012 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.957 Gbps (per GPU), 479.658 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.658 Gbps (per GPU), 477.266 Gbps (aggregated)
The layer-level communication performance: 59.651 Gbps (per GPU), 477.209 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.418 Gbps (per GPU), 475.342 Gbps (aggregated)
The layer-level communication performance: 59.390 Gbps (per GPU), 475.118 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.155 Gbps (per GPU), 473.242 Gbps (aggregated)
The layer-level communication performance: 59.108 Gbps (per GPU), 472.865 Gbps (aggregated)
The layer-level communication performance: 59.071 Gbps (per GPU), 472.565 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.972 Gbps (per GPU), 1255.780 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.970 Gbps (per GPU), 1255.756 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.975 Gbps (per GPU), 1255.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.964 Gbps (per GPU), 1255.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.984 Gbps (per GPU), 1255.875 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.961 Gbps (per GPU), 1255.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.970 Gbps (per GPU), 1255.756 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.970 Gbps (per GPU), 1255.756 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.327 Gbps (per GPU), 826.620 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.327 Gbps (per GPU), 826.620 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.326 Gbps (per GPU), 826.607 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.324 Gbps (per GPU), 826.593 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.327 Gbps (per GPU), 826.613 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.327 Gbps (per GPU), 826.620 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.330 Gbps (per GPU), 826.640 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.329 Gbps (per GPU), 826.633 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 40.239 Gbps (per GPU), 321.911 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.235 Gbps (per GPU), 321.883 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.236 Gbps (per GPU), 321.888 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.236 Gbps (per GPU), 321.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.239 Gbps (per GPU), 321.910 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.240 Gbps (per GPU), 321.916 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.239 Gbps (per GPU), 321.914 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.236 Gbps (per GPU), 321.892 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.92ms  0.72ms  0.87ms  1.27  2.81K  0.03M
 chk_1  0.92ms  0.72ms  0.87ms  1.28  2.82K  0.03M
 chk_2  0.92ms  0.72ms  0.87ms  1.27  2.80K  0.03M
 chk_3  0.92ms  0.72ms  0.87ms  1.27  2.80K  0.03M
 chk_4  0.92ms  0.72ms  0.87ms  1.27  2.82K  0.03M
 chk_5  0.97ms  0.73ms  0.88ms  1.33  2.75K  0.03M
 chk_6  0.91ms  0.72ms  0.87ms  1.27  2.71K  0.03M
 chk_7  0.92ms  0.72ms  0.87ms  1.27  2.76K  0.03M
 chk_8  0.91ms  0.72ms  0.87ms  1.27  2.79K  0.03M
 chk_9  0.91ms  0.72ms  0.87ms  1.27  2.81K  0.03M
chk_10  0.92ms  0.72ms  0.87ms  1.27  2.81K  0.03M
chk_11  0.92ms  0.72ms  0.88ms  1.27  2.74K  0.03M
chk_12  0.91ms  0.72ms  0.87ms  1.27  2.76K  0.03M
chk_13  0.91ms  0.72ms  0.87ms  1.26  2.75K  0.03M
chk_14  0.91ms  0.72ms  0.87ms  1.26  2.81K  0.03M
chk_15  0.91ms  0.72ms  0.87ms  1.26  2.77K  0.03M
chk_16  0.91ms  0.72ms  0.87ms  1.26  2.78K  0.03M
chk_17  0.91ms  0.73ms  0.88ms  1.26  2.79K  0.03M
chk_18  0.92ms  0.72ms  0.87ms  1.26  2.82K  0.03M
chk_19  0.91ms  0.72ms  0.87ms  1.27  2.81K  0.03M
chk_20  0.92ms  0.72ms  0.87ms  1.27  2.77K  0.03M
chk_21  0.92ms  0.73ms  0.87ms  1.27  2.84K  0.02M
chk_22  0.91ms  0.72ms  0.87ms  1.27  2.78K  0.03M
chk_23  0.91ms  0.72ms  0.87ms  1.27  2.80K  0.03M
chk_24  0.91ms  0.72ms  0.87ms  1.27  2.80K  0.03M
chk_25  0.91ms  0.72ms  0.87ms  1.26  2.81K  0.03M
chk_26  0.91ms  0.72ms  0.87ms  1.27  2.81K  0.03M
chk_27  0.92ms  0.72ms  0.87ms  1.27  2.79K  0.03M
chk_28  0.92ms  0.72ms  0.88ms  1.27  2.77K  0.03M
chk_29  0.91ms  0.72ms  0.87ms  1.27  2.77K  0.03M
chk_30  0.92ms  0.72ms  0.88ms  1.27  2.80K  0.03M
chk_31  0.92ms  0.72ms  0.87ms  1.27  2.78K  0.03M
   Avg  0.92  0.72  0.87
   Max  0.97  0.73  0.88
   Min  0.91  0.72  0.87
 Ratio  1.06  1.02  1.01
   Var  0.00  0.00  0.00
Profiling takes 1.014 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 0 owns the model-level partition [0, 35)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 1, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 1 owns the model-level partition [35, 67)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 2, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 2 owns the model-level partition [67, 99)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 3, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 3, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 3 owns the model-level partition [99, 131)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 6, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 6 owns the model-level partition [195, 227)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [227, 262)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 5, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 5 owns the model-level partition [163, 195)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 4, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 4, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 4 owns the model-level partition [131, 163)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[35, 67)...
+++++++++ Node 2 initializing the weights for op[67, 99)...
+++++++++ Node 3 initializing the weights for op[99, 131)...
+++++++++ Node 0 initializing the weights for op[0, 35)...
+++++++++ Node 5 initializing the weights for op[163, 195)...
+++++++++ Node 6 initializing the weights for op[195, 227)...
+++++++++ Node 4 initializing the weights for op[131, 163)...
+++++++++ Node 7 initializing the weights for op[227, 262)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5113	TrainAcc 0.3945	ValidAcc 0.3980	TestAcc 0.3962	BestValid 0.3980
	Epoch 50:	Loss 1.6133	TrainAcc 0.4223	ValidAcc 0.4246	TestAcc 0.4244	BestValid 0.4246
	Epoch 100:	Loss 1.5823	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4246
	Epoch 150:	Loss 1.5746	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4246
	Epoch 200:	Loss 1.5723	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4246
	Epoch 250:	Loss 1.5663	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4246
	Epoch 300:	Loss 1.5546	TrainAcc 0.4231	ValidAcc 0.4255	TestAcc 0.4249	BestValid 0.4255
	Epoch 350:	Loss 1.5250	TrainAcc 0.4690	ValidAcc 0.4705	TestAcc 0.4682	BestValid 0.4705
	Epoch 400:	Loss 1.5094	TrainAcc 0.4562	ValidAcc 0.4579	TestAcc 0.4551	BestValid 0.4705
	Epoch 450:	Loss 1.5043	TrainAcc 0.4608	ValidAcc 0.4615	TestAcc 0.4609	BestValid 0.4705
	Epoch 500:	Loss 1.4989	TrainAcc 0.3278	ValidAcc 0.3315	TestAcc 0.3278	BestValid 0.4705
	Epoch 550:	Loss 1.4947	TrainAcc 0.2295	ValidAcc 0.2300	TestAcc 0.2295	BestValid 0.4705
	Epoch 600:	Loss 1.4936	TrainAcc 0.3157	ValidAcc 0.3131	TestAcc 0.3103	BestValid 0.4705
	Epoch 650:	Loss 1.4882	TrainAcc 0.2037	ValidAcc 0.2020	TestAcc 0.2032	BestValid 0.4705
	Epoch 700:	Loss 1.4872	TrainAcc 0.2052	ValidAcc 0.2035	TestAcc 0.2047	BestValid 0.4705
	Epoch 750:	Loss 1.4849	TrainAcc 0.2225	ValidAcc 0.2208	TestAcc 0.2204	BestValid 0.4705
	Epoch 800:	Loss 1.4832	TrainAcc 0.1762	ValidAcc 0.1747	TestAcc 0.1754	BestValid 0.4705
	Epoch 850:	Loss 1.4807	TrainAcc 0.1195	ValidAcc 0.1190	TestAcc 0.1209	BestValid 0.4705
	Epoch 900:	Loss 1.4781	TrainAcc 0.1025	ValidAcc 0.1005	TestAcc 0.1014	BestValid 0.4705
	Epoch 950:	Loss 1.4753	TrainAcc 0.1168	ValidAcc 0.1152	TestAcc 0.1169	BestValid 0.4705
	Epoch 1000:	Loss 1.4686	TrainAcc 0.1227	ValidAcc 0.1215	TestAcc 0.1182	BestValid 0.4705
	Epoch 1050:	Loss 1.4570	TrainAcc 0.1360	ValidAcc 0.1324	TestAcc 0.1304	BestValid 0.4705
	Epoch 1100:	Loss 1.4480	TrainAcc 0.1204	ValidAcc 0.1172	TestAcc 0.1152	BestValid 0.4705
	Epoch 1150:	Loss 1.4399	TrainAcc 0.1253	ValidAcc 0.1218	TestAcc 0.1201	BestValid 0.4705
	Epoch 1200:	Loss 1.4343	TrainAcc 0.1502	ValidAcc 0.1473	TestAcc 0.1447	BestValid 0.4705
	Epoch 1250:	Loss 1.4322	TrainAcc 0.1433	ValidAcc 0.1393	TestAcc 0.1370	BestValid 0.4705
	Epoch 1300:	Loss 1.4291	TrainAcc 0.1537	ValidAcc 0.1502	TestAcc 0.1485	BestValid 0.4705
	Epoch 1350:	Loss 1.4250	TrainAcc 0.1537	ValidAcc 0.1506	TestAcc 0.1486	BestValid 0.4705
	Epoch 1400:	Loss 1.4270	TrainAcc 0.1542	ValidAcc 0.1502	TestAcc 0.1485	BestValid 0.4705
	Epoch 1450:	Loss 1.4273	TrainAcc 0.1611	ValidAcc 0.1600	TestAcc 0.1587	BestValid 0.4705
	Epoch 1500:	Loss 1.4210	TrainAcc 0.1467	ValidAcc 0.1447	TestAcc 0.1466	BestValid 0.4705
	Epoch 1550:	Loss 1.4228	TrainAcc 0.1362	ValidAcc 0.1315	TestAcc 0.1341	BestValid 0.4705
	Epoch 1600:	Loss 1.4177	TrainAcc 0.1587	ValidAcc 0.1546	TestAcc 0.1561	BestValid 0.4705
	Epoch 1650:	Loss 1.4171	TrainAcc 0.1129	ValidAcc 0.1107	TestAcc 0.1127	BestValid 0.4705
	Epoch 1700:	Loss 1.4178	TrainAcc 0.1211	ValidAcc 0.1178	TestAcc 0.1200	BestValid 0.4705
	Epoch 1750:	Loss 1.4143	TrainAcc 0.1146	ValidAcc 0.1118	TestAcc 0.1146	BestValid 0.4705
	Epoch 1800:	Loss 1.4105	TrainAcc 0.1202	ValidAcc 0.1187	TestAcc 0.1202	BestValid 0.4705
	Epoch 1850:	Loss 1.4121	TrainAcc 0.1335	ValidAcc 0.1294	TestAcc 0.1318	BestValid 0.4705
	Epoch 1900:	Loss 1.4100	TrainAcc 0.1206	ValidAcc 0.1166	TestAcc 0.1199	BestValid 0.4705
	Epoch 1950:	Loss 1.4082	TrainAcc 0.1118	ValidAcc 0.1077	TestAcc 0.1112	BestValid 0.4705
	Epoch 2000:	Loss 1.4074	TrainAcc 0.1076	ValidAcc 0.1044	TestAcc 0.1046	BestValid 0.4705
	Epoch 2050:	Loss 1.4067	TrainAcc 0.1023	ValidAcc 0.1004	TestAcc 0.1013	BestValid 0.4705
	Epoch 2100:	Loss 1.4063	TrainAcc 0.1435	ValidAcc 0.1374	TestAcc 0.1418	BestValid 0.4705
	Epoch 2150:	Loss 1.4041	TrainAcc 0.1287	ValidAcc 0.1228	TestAcc 0.1260	BestValid 0.4705
	Epoch 2200:	Loss 1.4013	TrainAcc 0.1504	ValidAcc 0.1432	TestAcc 0.1475	BestValid 0.4705
	Epoch 2250:	Loss 1.4005	TrainAcc 0.2223	ValidAcc 0.2192	TestAcc 0.2217	BestValid 0.4705
	Epoch 2300:	Loss 1.4031	TrainAcc 0.2588	ValidAcc 0.2531	TestAcc 0.2576	BestValid 0.4705
	Epoch 2350:	Loss 1.4010	TrainAcc 0.2755	ValidAcc 0.2757	TestAcc 0.2719	BestValid 0.4705
	Epoch 2400:	Loss 1.3973	TrainAcc 0.4026	ValidAcc 0.4028	TestAcc 0.4025	BestValid 0.4705
	Epoch 2450:	Loss 1.3966	TrainAcc 0.4211	ValidAcc 0.4200	TestAcc 0.4218	BestValid 0.4705
	Epoch 2500:	Loss 1.3929	TrainAcc 0.4228	ValidAcc 0.4224	TestAcc 0.4223	BestValid 0.4705
	Epoch 2550:	Loss 1.3925	TrainAcc 0.4239	ValidAcc 0.4226	TestAcc 0.4238	BestValid 0.4705
	Epoch 2600:	Loss 1.3922	TrainAcc 0.4208	ValidAcc 0.4187	TestAcc 0.4196	BestValid 0.4705
	Epoch 2650:	Loss 1.3903	TrainAcc 0.3943	ValidAcc 0.3900	TestAcc 0.3932	BestValid 0.4705
	Epoch 2700:	Loss 1.3899	TrainAcc 0.3189	ValidAcc 0.3202	TestAcc 0.3142	BestValid 0.4705
	Epoch 2750:	Loss 1.3875	TrainAcc 0.4031	ValidAcc 0.3988	TestAcc 0.4014	BestValid 0.4705
	Epoch 2800:	Loss 1.3860	TrainAcc 0.3686	ValidAcc 0.3668	TestAcc 0.3665	BestValid 0.4705
	Epoch 2850:	Loss 1.3867	TrainAcc 0.3187	ValidAcc 0.3198	TestAcc 0.3150	BestValid 0.4705
	Epoch 2900:	Loss 1.3823	TrainAcc 0.2281	ValidAcc 0.2284	TestAcc 0.2238	BestValid 0.4705
	Epoch 2950:	Loss 1.3853	TrainAcc 0.4089	ValidAcc 0.4042	TestAcc 0.4065	BestValid 0.4705
	Epoch 3000:	Loss 1.3856	TrainAcc 0.3865	ValidAcc 0.3834	TestAcc 0.3835	BestValid 0.4705
	Epoch 3050:	Loss 1.3803	TrainAcc 0.4183	ValidAcc 0.4163	TestAcc 0.4186	BestValid 0.4705
	Epoch 3100:	Loss 1.3793	TrainAcc 0.4187	ValidAcc 0.4162	TestAcc 0.4182	BestValid 0.4705
	Epoch 3150:	Loss 1.3815	TrainAcc 0.3809	ValidAcc 0.3753	TestAcc 0.3771	BestValid 0.4705
	Epoch 3200:	Loss 1.3805	TrainAcc 0.4219	ValidAcc 0.4209	TestAcc 0.4219	BestValid 0.4705
	Epoch 3250:	Loss 1.3782	TrainAcc 0.4198	ValidAcc 0.4170	TestAcc 0.4196	BestValid 0.4705
	Epoch 3300:	Loss 1.3786	TrainAcc 0.3738	ValidAcc 0.3700	TestAcc 0.3701	BestValid 0.4705
	Epoch 3350:	Loss 1.3778	TrainAcc 0.3604	ValidAcc 0.3588	TestAcc 0.3578	BestValid 0.4705
	Epoch 3400:	Loss 1.3739	TrainAcc 0.2827	ValidAcc 0.2805	TestAcc 0.2757	BestValid 0.4705
	Epoch 3450:	Loss 1.3767	TrainAcc 0.3970	ValidAcc 0.3924	TestAcc 0.3960	BestValid 0.4705
	Epoch 3500:	Loss 1.3772	TrainAcc 0.4165	ValidAcc 0.4127	TestAcc 0.4151	BestValid 0.4705
	Epoch 3550:	Loss 1.3764	TrainAcc 0.4046	ValidAcc 0.3989	TestAcc 0.4024	BestValid 0.4705
	Epoch 3600:	Loss 1.3806	TrainAcc 0.4093	ValidAcc 0.4088	TestAcc 0.4090	BestValid 0.4705
	Epoch 3650:	Loss 1.3767	TrainAcc 0.3865	ValidAcc 0.3819	TestAcc 0.3839	BestValid 0.4705
	Epoch 3700:	Loss 1.3774	TrainAcc 0.3808	ValidAcc 0.3767	TestAcc 0.3790	BestValid 0.4705
	Epoch 3750:	Loss 1.3747	TrainAcc 0.4136	ValidAcc 0.4109	TestAcc 0.4129	BestValid 0.4705
	Epoch 3800:	Loss 1.3744	TrainAcc 0.4093	ValidAcc 0.4056	TestAcc 0.4079	BestValid 0.4705
	Epoch 3850:	Loss 1.3735	TrainAcc 0.3959	ValidAcc 0.3894	TestAcc 0.3931	BestValid 0.4705
	Epoch 3900:	Loss 1.3731	TrainAcc 0.4094	ValidAcc 0.4066	TestAcc 0.4094	BestValid 0.4705
	Epoch 3950:	Loss 1.3721	TrainAcc 0.4103	ValidAcc 0.4109	TestAcc 0.4102	BestValid 0.4705
	Epoch 4000:	Loss 1.3725	TrainAcc 0.1762	ValidAcc 0.1734	TestAcc 0.1706	BestValid 0.4705
	Epoch 4050:	Loss 1.3742	TrainAcc 0.4123	ValidAcc 0.4086	TestAcc 0.4109	BestValid 0.4705
	Epoch 4100:	Loss 1.3733	TrainAcc 0.4073	ValidAcc 0.4039	TestAcc 0.4060	BestValid 0.4705
	Epoch 4150:	Loss 1.3677	TrainAcc 0.3725	ValidAcc 0.3685	TestAcc 0.3694	BestValid 0.4705
	Epoch 4200:	Loss 1.3725	TrainAcc 0.2749	ValidAcc 0.2745	TestAcc 0.2689	BestValid 0.4705
	Epoch 4250:	Loss 1.3705	TrainAcc 0.4193	ValidAcc 0.4164	TestAcc 0.4180	BestValid 0.4705
	Epoch 4300:	Loss 1.3669	TrainAcc 0.4050	ValidAcc 0.3993	TestAcc 0.4042	BestValid 0.4705
	Epoch 4350:	Loss 1.3698	TrainAcc 0.3996	ValidAcc 0.3999	TestAcc 0.3995	BestValid 0.4705
	Epoch 4400:	Loss 1.3674	TrainAcc 0.3892	ValidAcc 0.3834	TestAcc 0.3888	BestValid 0.4705
	Epoch 4450:	Loss 1.3694	TrainAcc 0.4177	ValidAcc 0.4140	TestAcc 0.4181	BestValid 0.4705
	Epoch 4500:	Loss 1.3682	TrainAcc 0.4166	ValidAcc 0.4132	TestAcc 0.4167	BestValid 0.4705
	Epoch 4550:	Loss 1.3673	TrainAcc 0.4105	ValidAcc 0.4078	TestAcc 0.4097	BestValid 0.4705
	Epoch 4600:	Loss 1.3704	TrainAcc 0.3985	ValidAcc 0.3922	TestAcc 0.3969	BestValid 0.4705
	Epoch 4650:	Loss 1.3695	TrainAcc 0.3990	ValidAcc 0.3934	TestAcc 0.3978	BestValid 0.4705
	Epoch 4700:	Loss 1.3638	TrainAcc 0.3989	ValidAcc 0.3958	TestAcc 0.3975	BestValid 0.4705
	Epoch 4750:	Loss 1.3661	TrainAcc 0.4090	ValidAcc 0.4048	TestAcc 0.4073	BestValid 0.4705
	Epoch 4800:	Loss 1.3657	TrainAcc 0.4043	ValidAcc 0.4026	TestAcc 0.4030	BestValid 0.4705
	Epoch 4850:	Loss 1.3678	TrainAcc 0.4088	ValidAcc 0.4057	TestAcc 0.4075	BestValid 0.4705
	Epoch 4900:	Loss 1.3730	TrainAcc 0.4170	ValidAcc 0.4135	TestAcc 0.4172	BestValid 0.4705
	Epoch 4950:	Loss 1.3650	TrainAcc 0.3888	ValidAcc 0.3827	TestAcc 0.3867	BestValid 0.4705
	Epoch 5000:	Loss 1.3630	TrainAcc 0.4009	ValidAcc 0.4019	TestAcc 0.4014	BestValid 0.4705
****** Epoch Time (Excluding Evaluation Cost): 0.135 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 24.444 ms (Max: 24.979, Min: 22.144, Sum: 195.551)
Cluster-Wide Average, Compute: 92.925 ms (Max: 99.398, Min: 87.684, Sum: 743.396)
Cluster-Wide Average, Communication-Layer: 10.512 ms (Max: 12.318, Min: 7.479, Sum: 84.096)
Cluster-Wide Average, Bubble-Imbalance: 5.455 ms (Max: 9.500, Min: 2.307, Sum: 43.637)
Cluster-Wide Average, Communication-Graph: 0.555 ms (Max: 0.677, Min: 0.466, Sum: 4.442)
Cluster-Wide Average, Optimization: 0.096 ms (Max: 0.114, Min: 0.089, Sum: 0.765)
Cluster-Wide Average, Others: 1.234 ms (Max: 4.009, Min: 0.812, Sum: 9.876)
****** Breakdown Sum: 135.220 ms ******
Cluster-Wide Average, GPU Memory Consumption: 2.920 GB (Max: 3.639, Min: 2.801, Sum: 23.358)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 46.902 Gbps (Max: 55.832, Min: 36.771, Sum: 375.219)
Layer-level communication (cluster-wide, per-epoch): 0.465 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.465 GB
****** Accuracy Results ******
Highest valid_acc: 0.4705
Target test_acc: 0.4682
Epoch to reach the target acc: 349
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
