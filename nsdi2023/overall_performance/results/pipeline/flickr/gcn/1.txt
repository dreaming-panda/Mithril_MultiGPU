Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...Building the CSR structure...
Building the CSR structure...
Building the CSR structure...

        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.019 seconds.
        It takes 0.019 seconds.
        It takes 0.019 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.009 seconds.
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.114 seconds.
Building the Label Vector...
        It takes 0.112 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.415 Gbps (per GPU), 483.319 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.114 Gbps (per GPU), 480.914 Gbps (aggregated)
The layer-level communication performance: 60.104 Gbps (per GPU), 480.831 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.851 Gbps (per GPU), 478.805 Gbps (aggregated)
The layer-level communication performance: 59.819 Gbps (per GPU), 478.550 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.584 Gbps (per GPU), 476.670 Gbps (aggregated)
The layer-level communication performance: 59.504 Gbps (per GPU), 476.031 Gbps (aggregated)
The layer-level communication performance: 59.537 Gbps (per GPU), 476.293 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.865 Gbps (per GPU), 1262.917 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.090 Gbps (per GPU), 1256.720 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.052 Gbps (per GPU), 1256.413 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.090 Gbps (per GPU), 1256.720 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.868 Gbps (per GPU), 1262.940 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.108 Gbps (per GPU), 1256.862 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.923 Gbps (per GPU), 1255.380 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.019 Gbps (per GPU), 1256.154 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.866 Gbps (per GPU), 806.927 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.919 Gbps (per GPU), 807.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.869 Gbps (per GPU), 806.953 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.919 Gbps (per GPU), 807.354 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.829 Gbps (per GPU), 806.636 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.916 Gbps (per GPU), 807.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.915 Gbps (per GPU), 807.322 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.760 Gbps (per GPU), 806.080 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.030 Gbps (per GPU), 264.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.029 Gbps (per GPU), 264.228 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.028 Gbps (per GPU), 264.221 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.026 Gbps (per GPU), 264.208 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.025 Gbps (per GPU), 264.204 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.026 Gbps (per GPU), 264.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.023 Gbps (per GPU), 264.181 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.023 Gbps (per GPU), 264.186 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.31ms  0.26ms  0.21ms  1.46  2.81K  0.03M
 chk_1  0.31ms  0.26ms  0.21ms  1.48  2.82K  0.03M
 chk_2  0.31ms  0.26ms  0.21ms  1.47  2.80K  0.03M
 chk_3  0.31ms  0.26ms  0.21ms  1.47  2.80K  0.03M
 chk_4  0.31ms  0.26ms  0.21ms  1.46  2.82K  0.03M
 chk_5  0.32ms  0.26ms  0.22ms  1.45  2.75K  0.03M
 chk_6  0.30ms  0.25ms  0.21ms  1.47  2.71K  0.03M
 chk_7  0.31ms  0.26ms  0.21ms  1.48  2.76K  0.03M
 chk_8  0.31ms  0.26ms  0.21ms  1.45  2.79K  0.03M
 chk_9  0.31ms  0.26ms  0.21ms  1.46  2.81K  0.03M
chk_10  0.31ms  0.25ms  0.21ms  1.48  2.81K  0.03M
chk_11  0.31ms  0.26ms  0.21ms  1.44  2.74K  0.03M
chk_12  0.31ms  0.26ms  0.21ms  1.46  2.76K  0.03M
chk_13  0.31ms  0.26ms  0.21ms  1.45  2.75K  0.03M
chk_14  0.31ms  0.26ms  0.21ms  1.47  2.81K  0.03M
chk_15  0.31ms  0.26ms  0.21ms  1.46  2.77K  0.03M
chk_16  0.31ms  0.26ms  0.21ms  1.45  2.78K  0.03M
chk_17  0.31ms  0.26ms  0.21ms  1.46  2.79K  0.03M
chk_18  0.31ms  0.26ms  0.21ms  1.46  2.82K  0.03M
chk_19  0.30ms  0.25ms  0.21ms  1.48  2.81K  0.03M
chk_20  0.31ms  0.26ms  0.21ms  1.46  2.77K  0.03M
chk_21  0.31ms  0.26ms  0.21ms  1.46  2.84K  0.02M
chk_22  0.31ms  0.26ms  0.21ms  1.46  2.78K  0.03M
chk_23  0.31ms  0.27ms  0.21ms  1.45  2.80K  0.03M
chk_24  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
chk_25  0.31ms  0.25ms  0.21ms  1.47  2.81K  0.03M
chk_26  0.31ms  0.25ms  0.21ms  1.47  2.81K  0.03M
chk_27  0.31ms  0.26ms  0.21ms  1.46  2.79K  0.03M
chk_28  0.31ms  0.26ms  0.21ms  1.45  2.77K  0.03M
chk_29  0.31ms  0.26ms  0.21ms  1.46  2.77K  0.03M
chk_30  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
chk_31  0.31ms  0.26ms  0.21ms  1.46  2.78K  0.03M
   Avg  0.31  0.26  0.21
   Max  0.32  0.27  0.22
   Min  0.30  0.25  0.21
 Ratio  1.05  1.06  1.06
   Var  0.00  0.00  0.00
Profiling takes 0.382 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 34.591 ms
Partition 0 [0, 4) has cost: 34.591 ms
Partition 1 [4, 8) has cost: 32.942 ms
Partition 2 [8, 12) has cost: 32.942 ms
Partition 3 [12, 16) has cost: 32.942 ms
Partition 4 [16, 20) has cost: 32.942 ms
Partition 5 [20, 24) has cost: 32.942 ms
Partition 6 [24, 28) has cost: 32.942 ms
Partition 7 [28, 32) has cost: 31.468 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 20.831 ms
GPU 0, Compute+Comm Time: 17.240 ms, Bubble Time: 3.590 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 16.427 ms, Bubble Time: 3.608 ms, Imbalance Overhead: 0.796 ms
GPU 2, Compute+Comm Time: 16.427 ms, Bubble Time: 3.633 ms, Imbalance Overhead: 0.771 ms
GPU 3, Compute+Comm Time: 16.427 ms, Bubble Time: 3.654 ms, Imbalance Overhead: 0.750 ms
GPU 4, Compute+Comm Time: 16.427 ms, Bubble Time: 3.674 ms, Imbalance Overhead: 0.730 ms
GPU 5, Compute+Comm Time: 16.427 ms, Bubble Time: 3.697 ms, Imbalance Overhead: 0.707 ms
GPU 6, Compute+Comm Time: 16.427 ms, Bubble Time: 3.729 ms, Imbalance Overhead: 0.675 ms
GPU 7, Compute+Comm Time: 15.279 ms, Bubble Time: 3.778 ms, Imbalance Overhead: 1.775 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 32.557 ms
GPU 0, Compute+Comm Time: 25.644 ms, Bubble Time: 5.869 ms, Imbalance Overhead: 1.045 ms
GPU 1, Compute+Comm Time: 25.970 ms, Bubble Time: 5.851 ms, Imbalance Overhead: 0.737 ms
GPU 2, Compute+Comm Time: 25.970 ms, Bubble Time: 5.816 ms, Imbalance Overhead: 0.772 ms
GPU 3, Compute+Comm Time: 25.970 ms, Bubble Time: 5.786 ms, Imbalance Overhead: 0.802 ms
GPU 4, Compute+Comm Time: 25.970 ms, Bubble Time: 5.764 ms, Imbalance Overhead: 0.824 ms
GPU 5, Compute+Comm Time: 25.970 ms, Bubble Time: 5.742 ms, Imbalance Overhead: 0.846 ms
GPU 6, Compute+Comm Time: 25.970 ms, Bubble Time: 5.713 ms, Imbalance Overhead: 0.874 ms
GPU 7, Compute+Comm Time: 26.805 ms, Bubble Time: 5.694 ms, Imbalance Overhead: 0.059 ms
The estimated cost of the whole pipeline: 56.058 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 67.533 ms
Partition 0 [0, 8) has cost: 67.533 ms
Partition 1 [8, 16) has cost: 65.884 ms
Partition 2 [16, 24) has cost: 65.884 ms
Partition 3 [24, 32) has cost: 64.410 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 24.299 ms
GPU 0, Compute+Comm Time: 20.546 ms, Bubble Time: 3.745 ms, Imbalance Overhead: 0.008 ms
GPU 1, Compute+Comm Time: 20.138 ms, Bubble Time: 3.755 ms, Imbalance Overhead: 0.406 ms
GPU 2, Compute+Comm Time: 20.138 ms, Bubble Time: 3.789 ms, Imbalance Overhead: 0.372 ms
GPU 3, Compute+Comm Time: 19.564 ms, Bubble Time: 3.853 ms, Imbalance Overhead: 0.883 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 35.826 ms
GPU 0, Compute+Comm Time: 29.573 ms, Bubble Time: 5.645 ms, Imbalance Overhead: 0.607 ms
GPU 1, Compute+Comm Time: 29.739 ms, Bubble Time: 5.607 ms, Imbalance Overhead: 0.480 ms
GPU 2, Compute+Comm Time: 29.739 ms, Bubble Time: 5.571 ms, Imbalance Overhead: 0.516 ms
GPU 3, Compute+Comm Time: 30.155 ms, Bubble Time: 5.559 ms, Imbalance Overhead: 0.111 ms
    The estimated cost with 2 DP ways is 63.131 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 133.417 ms
Partition 0 [0, 16) has cost: 133.417 ms
Partition 1 [16, 32) has cost: 130.294 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 38.062 ms
GPU 0, Compute+Comm Time: 33.906 ms, Bubble Time: 4.157 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 33.413 ms, Bubble Time: 4.222 ms, Imbalance Overhead: 0.428 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 49.119 ms
GPU 0, Compute+Comm Time: 43.308 ms, Bubble Time: 5.445 ms, Imbalance Overhead: 0.366 ms
GPU 1, Compute+Comm Time: 43.600 ms, Bubble Time: 5.490 ms, Imbalance Overhead: 0.029 ms
    The estimated cost with 4 DP ways is 91.541 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 263.711 ms
Partition 0 [0, 32) has cost: 263.711 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 134.378 ms
GPU 0, Compute+Comm Time: 134.378 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 144.264 ms
GPU 0, Compute+Comm Time: 144.264 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 292.574 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 4 initializing the weights for op[81, 101)...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 7 initializing the weights for op[141, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...



*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9459	TrainAcc 0.0709	ValidAcc 0.0721	TestAcc 0.0735	BestValid 0.0721
	Epoch 50:	Loss 1.6760	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6631	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.6253	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.6239	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.6236	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.6283	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.6248	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.6280	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.6247	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.6215	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.6194	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.6227	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.6270	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.6191	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.6190	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.6156	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.6156	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.6161	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.6086	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.6092	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.6001	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5992	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5937	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.6069	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5962	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5888	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5867	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5839	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5915	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5824	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5772	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5786	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5797	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5745	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5721	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5716	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5697	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5736	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1950:	Loss 1.5687	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2000:	Loss 1.5693	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5656	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5674	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5661	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5638	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5621	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5644	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5658	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2400:	Loss 1.5651	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5650	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5638	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5614	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2600:	Loss 1.5675	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5633	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5589	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5595	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5556	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5577	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5556	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5546	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5570	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5529	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5549	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5570	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5568	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5595	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5551	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5549	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5538	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5537	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5524	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5535	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5508	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5529	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5510	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5519	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5526	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5515	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5508	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5493	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5501	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5535	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5525	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5529	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5551	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5547	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5530	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5529	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5514	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5492	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5496	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5558	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5527	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5644	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5613	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.5553	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5599	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5595	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5582	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5591	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5574	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.064 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 12.450 ms (Max: 13.113, Min: 10.278, Sum: 99.599)
Cluster-Wide Average, Compute: 37.073 ms (Max: 39.258, Min: 35.878, Sum: 296.581)
Cluster-Wide Average, Communication-Layer: 7.652 ms (Max: 8.460, Min: 5.848, Sum: 61.215)
Cluster-Wide Average, Bubble-Imbalance: 4.718 ms (Max: 6.159, Min: 3.931, Sum: 37.740)
Cluster-Wide Average, Communication-Graph: 0.461 ms (Max: 0.502, Min: 0.409, Sum: 3.691)
Cluster-Wide Average, Optimization: 0.089 ms (Max: 0.092, Min: 0.084, Sum: 0.712)
Cluster-Wide Average, Others: 1.174 ms (Max: 3.940, Min: 0.769, Sum: 9.392)
****** Breakdown Sum: 63.616 ms ******
Cluster-Wide Average, GPU Memory Consumption: 2.768 GB (Max: 3.194, Min: 2.456, Sum: 22.142)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 64.207 Gbps (Max: 73.948, Min: 45.435, Sum: 513.656)
Layer-level communication (cluster-wide, per-epoch): 0.465 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.465 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 49
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
