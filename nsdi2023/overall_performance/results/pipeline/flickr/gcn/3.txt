Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.022 seconds.
        It takes 0.021 seconds.
        It takes 0.023 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.027 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.965 Gbps (per GPU), 479.723 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.676 Gbps (per GPU), 477.410 Gbps (aggregated)
The layer-level communication performance: 59.671 Gbps (per GPU), 477.365 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.397 Gbps (per GPU), 475.178 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.177 Gbps (per GPU), 473.417 Gbps (aggregated)
The layer-level communication performance: 59.430 Gbps (per GPU), 475.439 Gbps (aggregated)
The layer-level communication performance: 59.129 Gbps (per GPU), 473.029 Gbps (aggregated)
The layer-level communication performance: 59.094 Gbps (per GPU), 472.753 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.495 Gbps (per GPU), 1251.961 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.501 Gbps (per GPU), 1252.006 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.483 Gbps (per GPU), 1251.868 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.472 Gbps (per GPU), 1251.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.495 Gbps (per GPU), 1251.961 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.513 Gbps (per GPU), 1252.101 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.498 Gbps (per GPU), 1251.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.480 Gbps (per GPU), 1251.843 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.934 Gbps (per GPU), 815.470 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.936 Gbps (per GPU), 815.490 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.938 Gbps (per GPU), 815.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.940 Gbps (per GPU), 815.517 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.933 Gbps (per GPU), 815.464 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.940 Gbps (per GPU), 815.522 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.935 Gbps (per GPU), 815.477 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.935 Gbps (per GPU), 815.484 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.862 Gbps (per GPU), 270.899 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.864 Gbps (per GPU), 270.910 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.863 Gbps (per GPU), 270.902 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.864 Gbps (per GPU), 270.912 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.862 Gbps (per GPU), 270.898 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.857 Gbps (per GPU), 270.858 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.858 Gbps (per GPU), 270.864 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.851 Gbps (per GPU), 270.805 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.31ms  0.26ms  0.21ms  1.47  2.81K  0.03M
 chk_1  0.31ms  0.26ms  0.21ms  1.46  2.82K  0.03M
 chk_2  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
 chk_3  0.31ms  0.26ms  0.21ms  1.45  2.80K  0.03M
 chk_4  0.31ms  0.26ms  0.21ms  1.47  2.82K  0.03M
 chk_5  0.32ms  0.26ms  0.22ms  1.44  2.75K  0.03M
 chk_6  0.30ms  0.25ms  0.21ms  1.47  2.71K  0.03M
 chk_7  0.31ms  0.26ms  0.21ms  1.47  2.76K  0.03M
 chk_8  0.31ms  0.26ms  0.21ms  1.45  2.79K  0.03M
 chk_9  0.31ms  0.26ms  0.21ms  1.46  2.81K  0.03M
chk_10  0.31ms  0.26ms  0.21ms  1.48  2.81K  0.03M
chk_11  0.31ms  0.26ms  0.21ms  1.45  2.74K  0.03M
chk_12  0.31ms  0.26ms  0.21ms  1.46  2.76K  0.03M
chk_13  0.31ms  0.26ms  0.21ms  1.47  2.75K  0.03M
chk_14  0.31ms  0.26ms  0.21ms  1.47  2.81K  0.03M
chk_15  0.31ms  0.26ms  0.21ms  1.46  2.77K  0.03M
chk_16  0.31ms  0.26ms  0.21ms  1.46  2.78K  0.03M
chk_17  0.31ms  0.26ms  0.21ms  1.45  2.79K  0.03M
chk_18  0.31ms  0.26ms  0.21ms  1.47  2.82K  0.03M
chk_19  0.31ms  0.25ms  0.21ms  1.48  2.81K  0.03M
chk_20  0.31ms  0.26ms  0.21ms  1.46  2.77K  0.03M
chk_21  0.31ms  0.26ms  0.21ms  1.47  2.84K  0.02M
chk_22  0.31ms  0.26ms  0.21ms  1.45  2.78K  0.03M
chk_23  0.31ms  0.26ms  0.21ms  1.47  2.80K  0.03M
chk_24  0.31ms  0.26ms  0.21ms  1.47  2.80K  0.03M
chk_25  0.31ms  0.25ms  0.21ms  1.47  2.81K  0.03M
chk_26  0.31ms  0.26ms  0.21ms  1.47  2.81K  0.03M
chk_27  0.31ms  0.26ms  0.21ms  1.46  2.79K  0.03M
chk_28  0.31ms  0.26ms  0.21ms  1.47  2.77K  0.03M
chk_29  0.31ms  0.26ms  0.21ms  1.46  2.77K  0.03M
chk_30  0.31ms  0.26ms  0.21ms  1.47  2.80K  0.03M
chk_31  0.31ms  0.26ms  0.21ms  1.46  2.78K  0.03M
   Avg  0.31  0.26  0.21
   Max  0.32  0.26  0.22
   Min  0.30  0.25  0.21
 Ratio  1.05  1.04  1.06
   Var  0.00  0.00  0.00
Profiling takes 0.382 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 34.708 ms
Partition 0 [0, 4) has cost: 34.708 ms
Partition 1 [4, 8) has cost: 33.068 ms
Partition 2 [8, 12) has cost: 33.068 ms
Partition 3 [12, 16) has cost: 33.068 ms
Partition 4 [16, 20) has cost: 33.068 ms
Partition 5 [20, 24) has cost: 33.068 ms
Partition 6 [24, 28) has cost: 33.068 ms
Partition 7 [28, 32) has cost: 31.569 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 20.948 ms
GPU 0, Compute+Comm Time: 17.352 ms, Bubble Time: 3.597 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 16.545 ms, Bubble Time: 3.616 ms, Imbalance Overhead: 0.788 ms
GPU 2, Compute+Comm Time: 16.545 ms, Bubble Time: 3.646 ms, Imbalance Overhead: 0.758 ms
GPU 3, Compute+Comm Time: 16.545 ms, Bubble Time: 3.670 ms, Imbalance Overhead: 0.733 ms
GPU 4, Compute+Comm Time: 16.545 ms, Bubble Time: 3.694 ms, Imbalance Overhead: 0.710 ms
GPU 5, Compute+Comm Time: 16.545 ms, Bubble Time: 3.720 ms, Imbalance Overhead: 0.683 ms
GPU 6, Compute+Comm Time: 16.545 ms, Bubble Time: 3.752 ms, Imbalance Overhead: 0.652 ms
GPU 7, Compute+Comm Time: 15.380 ms, Bubble Time: 3.801 ms, Imbalance Overhead: 1.767 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 32.606 ms
GPU 0, Compute+Comm Time: 25.714 ms, Bubble Time: 5.894 ms, Imbalance Overhead: 0.998 ms
GPU 1, Compute+Comm Time: 26.049 ms, Bubble Time: 5.872 ms, Imbalance Overhead: 0.685 ms
GPU 2, Compute+Comm Time: 26.049 ms, Bubble Time: 5.835 ms, Imbalance Overhead: 0.722 ms
GPU 3, Compute+Comm Time: 26.049 ms, Bubble Time: 5.807 ms, Imbalance Overhead: 0.750 ms
GPU 4, Compute+Comm Time: 26.049 ms, Bubble Time: 5.785 ms, Imbalance Overhead: 0.773 ms
GPU 5, Compute+Comm Time: 26.049 ms, Bubble Time: 5.762 ms, Imbalance Overhead: 0.795 ms
GPU 6, Compute+Comm Time: 26.049 ms, Bubble Time: 5.736 ms, Imbalance Overhead: 0.821 ms
GPU 7, Compute+Comm Time: 26.882 ms, Bubble Time: 5.717 ms, Imbalance Overhead: 0.007 ms
The estimated cost of the whole pipeline: 56.232 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 67.776 ms
Partition 0 [0, 8) has cost: 67.776 ms
Partition 1 [8, 16) has cost: 66.136 ms
Partition 2 [16, 24) has cost: 66.136 ms
Partition 3 [24, 32) has cost: 64.637 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 24.422 ms
GPU 0, Compute+Comm Time: 20.665 ms, Bubble Time: 3.755 ms, Imbalance Overhead: 0.002 ms
GPU 1, Compute+Comm Time: 20.262 ms, Bubble Time: 3.771 ms, Imbalance Overhead: 0.389 ms
GPU 2, Compute+Comm Time: 20.262 ms, Bubble Time: 3.815 ms, Imbalance Overhead: 0.345 ms
GPU 3, Compute+Comm Time: 19.682 ms, Bubble Time: 3.883 ms, Imbalance Overhead: 0.857 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 35.842 ms
GPU 0, Compute+Comm Time: 29.657 ms, Bubble Time: 5.672 ms, Imbalance Overhead: 0.512 ms
GPU 1, Compute+Comm Time: 29.827 ms, Bubble Time: 5.625 ms, Imbalance Overhead: 0.390 ms
GPU 2, Compute+Comm Time: 29.827 ms, Bubble Time: 5.586 ms, Imbalance Overhead: 0.429 ms
GPU 3, Compute+Comm Time: 30.244 ms, Bubble Time: 5.574 ms, Imbalance Overhead: 0.024 ms
    The estimated cost with 2 DP ways is 63.277 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 133.913 ms
Partition 0 [0, 16) has cost: 133.913 ms
Partition 1 [16, 32) has cost: 130.773 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 37.973 ms
GPU 0, Compute+Comm Time: 33.830 ms, Bubble Time: 4.143 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 33.338 ms, Bubble Time: 4.217 ms, Imbalance Overhead: 0.419 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.828 ms
GPU 0, Compute+Comm Time: 43.145 ms, Bubble Time: 5.410 ms, Imbalance Overhead: 0.274 ms
GPU 1, Compute+Comm Time: 43.438 ms, Bubble Time: 5.361 ms, Imbalance Overhead: 0.029 ms
    The estimated cost with 4 DP ways is 91.141 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 264.686 ms
Partition 0 [0, 32) has cost: 264.686 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 131.372 ms
GPU 0, Compute+Comm Time: 131.372 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 141.151 ms
GPU 0, Compute+Comm Time: 141.151 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 286.149 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 4 initializing the weights for op[81, 101)...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 7 initializing the weights for op[141, 160)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9459	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.0952
	Epoch 50:	Loss 1.9459	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.7819	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.6118	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5995	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5939	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5930	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.6079	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5822	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5722	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5745	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5762	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5818	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5721	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5765	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5686	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5660	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5640	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5713	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5687	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5620	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5607	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5589	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5593	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5593	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5585	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5610	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5586	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5590	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5576	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5569	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5598	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5587	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5557	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5551	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5537	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5547	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5547	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5533	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1950:	Loss 1.5514	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2000:	Loss 1.5514	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5587	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5560	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5526	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5568	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5558	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5554	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5564	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2400:	Loss 1.5571	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5572	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5560	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5558	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2600:	Loss 1.5564	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5569	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5541	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5579	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5589	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5544	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5543	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5537	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5518	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5540	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5503	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5548	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5566	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5544	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5526	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5519	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5525	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5530	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5510	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5526	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5539	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5526	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5506	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5549	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5559	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5506	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5507	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5519	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5529	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5516	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5509	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5518	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5513	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5558	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5493	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5514	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5507	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5491	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5497	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5480	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5494	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5511	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5507	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.5511	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5499	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5515	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5539	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5507	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5490	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.064 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 12.437 ms (Max: 13.083, Min: 10.294, Sum: 99.497)
Cluster-Wide Average, Compute: 37.100 ms (Max: 39.060, Min: 35.948, Sum: 296.803)
Cluster-Wide Average, Communication-Layer: 7.667 ms (Max: 8.487, Min: 5.838, Sum: 61.334)
Cluster-Wide Average, Bubble-Imbalance: 4.891 ms (Max: 6.481, Min: 3.626, Sum: 39.125)
Cluster-Wide Average, Communication-Graph: 0.458 ms (Max: 0.501, Min: 0.401, Sum: 3.667)
Cluster-Wide Average, Optimization: 0.089 ms (Max: 0.092, Min: 0.084, Sum: 0.711)
Cluster-Wide Average, Others: 1.180 ms (Max: 3.958, Min: 0.774, Sum: 9.440)
****** Breakdown Sum: 63.822 ms ******
Cluster-Wide Average, GPU Memory Consumption: 2.768 GB (Max: 3.194, Min: 2.456, Sum: 22.142)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 64.081 Gbps (Max: 73.688, Min: 45.140, Sum: 512.649)
Layer-level communication (cluster-wide, per-epoch): 0.465 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.465 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 49
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
