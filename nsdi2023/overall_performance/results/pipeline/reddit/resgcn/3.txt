Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7

Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.357 seconds.
Building the CSC structure...
        It takes 2.388 seconds.
Building the CSC structure...
        It takes 2.463 seconds.
Building the CSC structure...
        It takes 2.497 seconds.
Building the CSC structure...
        It takes 2.526 seconds.
Building the CSC structure...
        It takes 2.566 seconds.
Building the CSC structure...
        It takes 2.593 seconds.
Building the CSC structure...
        It takes 2.693 seconds.
Building the CSC structure...
        It takes 2.332 seconds.
        It takes 2.398 seconds.
        It takes 2.318 seconds.
        It takes 2.380 seconds.
        It takes 2.361 seconds.
        It takes 2.367 seconds.
        It takes 2.462 seconds.
        It takes 2.566 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.341 seconds.
Building the Label Vector...
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.343 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.047 seconds.
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.296 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.301 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.037 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.322 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.035 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.327 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.267 Gbps (per GPU), 474.140 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.974 Gbps (per GPU), 471.790 Gbps (aggregated)
The layer-level communication performance: 58.972 Gbps (per GPU), 471.778 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.725 Gbps (per GPU), 469.800 Gbps (aggregated)
The layer-level communication performance: 58.691 Gbps (per GPU), 469.524 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.488 Gbps (per GPU), 467.902 Gbps (aggregated)
The layer-level communication performance: 58.436 Gbps (per GPU), 467.487 Gbps (aggregated)
The layer-level communication performance: 58.398 Gbps (per GPU), 467.186 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.777 Gbps (per GPU), 1278.215 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.756 Gbps (per GPU), 1278.045 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.716 Gbps (per GPU), 1277.729 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.753 Gbps (per GPU), 1278.021 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.771 Gbps (per GPU), 1278.167 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.759 Gbps (per GPU), 1278.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.725 Gbps (per GPU), 1277.799 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.762 Gbps (per GPU), 1278.094 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.502 Gbps (per GPU), 828.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.502 Gbps (per GPU), 828.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.503 Gbps (per GPU), 828.021 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.502 Gbps (per GPU), 828.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.503 Gbps (per GPU), 828.021 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.502 Gbps (per GPU), 828.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.500 Gbps (per GPU), 828.000 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.502 Gbps (per GPU), 828.014 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.348 Gbps (per GPU), 314.781 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.347 Gbps (per GPU), 314.775 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.348 Gbps (per GPU), 314.786 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.348 Gbps (per GPU), 314.783 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.346 Gbps (per GPU), 314.770 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.347 Gbps (per GPU), 314.772 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.347 Gbps (per GPU), 314.779 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.347 Gbps (per GPU), 314.778 Gbps (aggregated, cluster-wide)

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 1142113 RUNNING AT gnerv8
=   EXIT CODE: 9
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
[proxy:0:0@gnerv7] HYD_pmcd_pmip_control_cmd_cb (proxy/pmip_cb.c:480): assert (!closed) failed
[proxy:0:0@gnerv7] HYDT_dmxu_poll_wait_for_event (lib/tools/demux/demux_poll.c:76): callback returned error status
[proxy:0:0@gnerv7] main (proxy/pmip.c:127): demux engine error waiting for event
srun: error: gnerv7: task 0: Exited with exit code 7
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Killed (signal 9)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
