Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.880 seconds.
Building the CSC structure...
        It takes 1.915 seconds.
Building the CSC structure...
        It takes 1.929 seconds.
Building the CSC structure...
        It takes 2.356 seconds.
Building the CSC structure...
        It takes 2.359 seconds.
Building the CSC structure...
        It takes 2.585 seconds.
Building the CSC structure...
        It takes 2.631 seconds.
Building the CSC structure...
        It takes 2.677 seconds.
Building the CSC structure...
        It takes 1.799 seconds.
        It takes 1.859 seconds.
        It takes 1.847 seconds.
        It takes 2.166 seconds.
        It takes 2.298 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 2.343 seconds.
        It takes 0.029 seconds.
        It takes 2.347 seconds.
        It takes 2.313 seconds.
        It takes 0.304 seconds.
Building the Label Vector...
        It takes 0.305 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
        It takes 0.266 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.037 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 55.658 Gbps (per GPU), 445.265 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.393 Gbps (per GPU), 443.143 Gbps (aggregated)
The layer-level communication performance: 55.392 Gbps (per GPU), 443.139 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.173 Gbps (per GPU), 441.387 Gbps (aggregated)
The layer-level communication performance: 55.153 Gbps (per GPU), 441.222 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.963 Gbps (per GPU), 439.707 Gbps (aggregated)
The layer-level communication performance: 54.924 Gbps (per GPU), 439.394 Gbps (aggregated)
The layer-level communication performance: 54.900 Gbps (per GPU), 439.198 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.043 Gbps (per GPU), 1256.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.031 Gbps (per GPU), 1256.251 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.043 Gbps (per GPU), 1256.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.037 Gbps (per GPU), 1256.297 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.046 Gbps (per GPU), 1256.369 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.031 Gbps (per GPU), 1256.250 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.043 Gbps (per GPU), 1256.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.029 Gbps (per GPU), 1256.231 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.073 Gbps (per GPU), 816.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.074 Gbps (per GPU), 816.595 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.074 Gbps (per GPU), 816.595 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.074 Gbps (per GPU), 816.588 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.074 Gbps (per GPU), 816.588 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.073 Gbps (per GPU), 816.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.076 Gbps (per GPU), 816.607 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.074 Gbps (per GPU), 816.595 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.243 Gbps (per GPU), 297.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.244 Gbps (per GPU), 297.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.243 Gbps (per GPU), 297.943 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.242 Gbps (per GPU), 297.936 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.242 Gbps (per GPU), 297.939 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.243 Gbps (per GPU), 297.944 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.243 Gbps (per GPU), 297.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.244 Gbps (per GPU), 297.952 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.42ms  2.36ms  2.29ms  1.06  8.38K  3.53M
 chk_1  2.77ms  2.61ms  2.51ms  1.10  6.74K  3.60M
 chk_2  2.65ms  2.49ms  2.39ms  1.11  7.27K  3.53M
 chk_3  2.66ms  2.50ms  2.42ms  1.10  7.92K  3.61M
 chk_4  2.59ms  2.47ms  2.42ms  1.07  5.33K  3.68M
 chk_5  2.57ms  2.36ms  2.27ms  1.13 10.07K  3.45M
 chk_6  2.76ms  2.54ms  2.43ms  1.14  9.41K  3.48M
 chk_7  2.60ms  2.42ms  2.33ms  1.11  8.12K  3.60M
 chk_8  2.72ms  2.56ms  2.50ms  1.09  6.09K  3.64M
 chk_9  2.51ms  2.26ms  2.13ms  1.18 11.10K  3.38M
chk_10  2.74ms  2.61ms  2.55ms  1.08  5.67K  3.63M
chk_11  2.60ms  2.42ms  2.40ms  1.08  8.16K  3.54M
chk_12  2.86ms  2.65ms  2.56ms  1.12  7.24K  3.55M
chk_13  2.65ms  2.51ms  2.44ms  1.08  5.41K  3.68M
chk_14  2.90ms  2.72ms  2.63ms  1.10  7.14K  3.53M
chk_15  2.75ms  2.52ms  2.41ms  1.14  9.25K  3.49M
chk_16  2.55ms  2.45ms  2.37ms  1.08  4.78K  3.77M
chk_17  2.72ms  2.53ms  2.45ms  1.11  6.85K  3.60M
chk_18  2.52ms  2.34ms  2.26ms  1.11  7.47K  3.57M
chk_19  2.59ms  2.62ms  2.40ms  1.09  4.88K  3.75M
chk_20  2.58ms  2.39ms  2.36ms  1.10  7.00K  3.63M
chk_21  2.55ms  2.41ms  2.37ms  1.08  5.41K  3.68M
chk_22  2.74ms  2.50ms  2.38ms  1.15 11.07K  3.39M
chk_23  2.67ms  2.48ms  2.42ms  1.10  7.23K  3.64M
chk_24  2.67ms  2.48ms  2.36ms  1.13 10.13K  3.43M
chk_25  2.52ms  2.36ms  2.30ms  1.10  6.40K  3.57M
chk_26  2.71ms  2.60ms  2.53ms  1.07  5.78K  3.55M
chk_27  2.59ms  2.39ms  2.28ms  1.14  9.34K  3.48M
chk_28  2.91ms  2.75ms  2.66ms  1.09  6.37K  3.57M
chk_29  2.71ms  2.59ms  2.52ms  1.08  5.16K  3.78M
chk_30  2.59ms  2.47ms  2.40ms  1.08  5.44K  3.67M
chk_31  2.73ms  2.66ms  2.52ms  1.08  6.33K  3.63M
   Avg  2.66  2.50  2.41
   Max  2.91  2.75  2.66
   Min  2.42  2.26  2.13
 Ratio  1.20  1.21  1.25
   Var  0.01  0.01  0.01
Profiling takes 2.810 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 325.117 ms
Partition 0 [0, 4) has cost: 325.117 ms
Partition 1 [4, 8) has cost: 319.999 ms
Partition 2 [8, 12) has cost: 319.999 ms
Partition 3 [12, 16) has cost: 319.999 ms
Partition 4 [16, 20) has cost: 319.999 ms
Partition 5 [20, 24) has cost: 319.999 ms
Partition 6 [24, 28) has cost: 319.999 ms
Partition 7 [28, 32) has cost: 317.260 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 155.123 ms
GPU 0, Compute+Comm Time: 122.467 ms, Bubble Time: 27.845 ms, Imbalance Overhead: 4.811 ms
GPU 1, Compute+Comm Time: 120.248 ms, Bubble Time: 27.556 ms, Imbalance Overhead: 7.319 ms
GPU 2, Compute+Comm Time: 120.248 ms, Bubble Time: 27.460 ms, Imbalance Overhead: 7.415 ms
GPU 3, Compute+Comm Time: 120.248 ms, Bubble Time: 27.292 ms, Imbalance Overhead: 7.583 ms
GPU 4, Compute+Comm Time: 120.248 ms, Bubble Time: 27.160 ms, Imbalance Overhead: 7.715 ms
GPU 5, Compute+Comm Time: 120.248 ms, Bubble Time: 27.101 ms, Imbalance Overhead: 7.774 ms
GPU 6, Compute+Comm Time: 120.248 ms, Bubble Time: 27.042 ms, Imbalance Overhead: 7.833 ms
GPU 7, Compute+Comm Time: 119.058 ms, Bubble Time: 27.179 ms, Imbalance Overhead: 8.887 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 292.130 ms
GPU 0, Compute+Comm Time: 224.990 ms, Bubble Time: 50.952 ms, Imbalance Overhead: 16.187 ms
GPU 1, Compute+Comm Time: 226.539 ms, Bubble Time: 50.854 ms, Imbalance Overhead: 14.737 ms
GPU 2, Compute+Comm Time: 226.539 ms, Bubble Time: 50.955 ms, Imbalance Overhead: 14.635 ms
GPU 3, Compute+Comm Time: 226.539 ms, Bubble Time: 51.057 ms, Imbalance Overhead: 14.534 ms
GPU 4, Compute+Comm Time: 226.539 ms, Bubble Time: 51.350 ms, Imbalance Overhead: 14.240 ms
GPU 5, Compute+Comm Time: 226.539 ms, Bubble Time: 51.690 ms, Imbalance Overhead: 13.901 ms
GPU 6, Compute+Comm Time: 226.539 ms, Bubble Time: 51.942 ms, Imbalance Overhead: 13.648 ms
GPU 7, Compute+Comm Time: 229.439 ms, Bubble Time: 52.909 ms, Imbalance Overhead: 9.782 ms
The estimated cost of the whole pipeline: 469.616 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 645.117 ms
Partition 0 [0, 8) has cost: 645.117 ms
Partition 1 [8, 16) has cost: 639.999 ms
Partition 2 [16, 24) has cost: 639.999 ms
Partition 3 [24, 32) has cost: 637.259 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 163.484 ms
GPU 0, Compute+Comm Time: 133.897 ms, Bubble Time: 26.279 ms, Imbalance Overhead: 3.308 ms
GPU 1, Compute+Comm Time: 132.831 ms, Bubble Time: 25.559 ms, Imbalance Overhead: 5.094 ms
GPU 2, Compute+Comm Time: 132.831 ms, Bubble Time: 25.158 ms, Imbalance Overhead: 5.495 ms
GPU 3, Compute+Comm Time: 132.259 ms, Bubble Time: 24.741 ms, Imbalance Overhead: 6.484 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 297.098 ms
GPU 0, Compute+Comm Time: 240.696 ms, Bubble Time: 45.083 ms, Imbalance Overhead: 11.318 ms
GPU 1, Compute+Comm Time: 241.456 ms, Bubble Time: 45.562 ms, Imbalance Overhead: 10.080 ms
GPU 2, Compute+Comm Time: 241.456 ms, Bubble Time: 46.035 ms, Imbalance Overhead: 9.607 ms
GPU 3, Compute+Comm Time: 242.744 ms, Bubble Time: 47.328 ms, Imbalance Overhead: 7.027 ms
    The estimated cost with 2 DP ways is 483.611 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1285.116 ms
Partition 0 [0, 16) has cost: 1285.116 ms
Partition 1 [16, 32) has cost: 1277.258 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 210.950 ms
GPU 0, Compute+Comm Time: 183.148 ms, Bubble Time: 22.936 ms, Imbalance Overhead: 4.865 ms
GPU 1, Compute+Comm Time: 182.333 ms, Bubble Time: 23.116 ms, Imbalance Overhead: 5.501 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 339.574 ms
GPU 0, Compute+Comm Time: 295.468 ms, Bubble Time: 36.685 ms, Imbalance Overhead: 7.421 ms
GPU 1, Compute+Comm Time: 296.495 ms, Bubble Time: 36.962 ms, Imbalance Overhead: 6.117 ms
    The estimated cost with 4 DP ways is 578.050 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2562.374 ms
Partition 0 [0, 32) has cost: 2562.374 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 515.802 ms
GPU 0, Compute+Comm Time: 515.802 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 630.423 ms
GPU 0, Compute+Comm Time: 630.423 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1203.536 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 4 initializing the weights for op[81, 101)...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 7 initializing the weights for op[141, 160)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...


*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000

The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7136	TrainAcc 0.0386	ValidAcc 0.0379	TestAcc 0.0365	BestValid 0.0379
	Epoch 50:	Loss 3.4415	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.3247	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.2867	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 3.2756	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 250:	Loss 3.1659	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 300:	Loss 3.2642	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 350:	Loss 3.2005	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 400:	Loss 3.3017	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 450:	Loss 3.2200	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 500:	Loss 3.0651	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 550:	Loss 3.1203	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 600:	Loss 3.2122	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 650:	Loss 3.1053	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 700:	Loss 3.0833	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 750:	Loss 3.1765	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 800:	Loss 3.1638	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.0584
	Epoch 850:	Loss 3.0434	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 900:	Loss 3.1396	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 950:	Loss 3.3760	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 1000:	Loss 3.2241	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 1050:	Loss 3.0257	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 1100:	Loss 3.1510	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 1150:	Loss 3.2283	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 1200:	Loss 3.1921	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 1250:	Loss 3.3173	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1300:	Loss 3.4583	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 1350:	Loss 3.2722	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 1400:	Loss 3.2554	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 1450:	Loss 3.2694	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1500:	Loss 3.2388	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1550:	Loss 3.1733	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1600:	Loss 3.0740	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1650:	Loss 3.1061	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1700:	Loss 3.1526	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1750:	Loss 3.1313	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1800:	Loss 3.2885	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1466
	Epoch 1850:	Loss 3.4339	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 1900:	Loss 3.2089	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 1950:	Loss 3.0567	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2000:	Loss 3.1503	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2050:	Loss 3.3825	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2100:	Loss 3.1089	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1466
	Epoch 2150:	Loss 3.0184	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2200:	Loss 2.9206	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2250:	Loss 2.9753	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2300:	Loss 3.0107	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2350:	Loss 3.2872	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2400:	Loss 3.2288	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2450:	Loss 3.2020	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2500:	Loss 3.2000	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2550:	Loss 3.1896	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2600:	Loss 3.1087	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2650:	Loss 3.1719	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2700:	Loss 3.2880	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2750:	Loss 3.1800	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2800:	Loss 3.0366	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 2850:	Loss 3.0564	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2900:	Loss 3.2731	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2950:	Loss 3.1755	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1466
	Epoch 3000:	Loss 3.1181	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3050:	Loss 3.1622	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3100:	Loss 3.2091	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3150:	Loss 3.2019	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3200:	Loss 3.2601	TrainAcc 0.1274	ValidAcc 0.1188	TestAcc 0.1179	BestValid 0.1466
	Epoch 3250:	Loss 3.0953	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3300:	Loss 3.0831	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3350:	Loss 3.0688	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3400:	Loss 3.1345	TrainAcc 0.0683	ValidAcc 0.0658	TestAcc 0.0674	BestValid 0.1466
	Epoch 3450:	Loss 3.0653	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3500:	Loss 3.1030	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 3550:	Loss 3.3108	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3600:	Loss 3.3084	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3650:	Loss 3.2762	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3700:	Loss 3.2762	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3750:	Loss 3.2868	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3800:	Loss 3.2396	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3850:	Loss 3.1316	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3900:	Loss 3.0948	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 3950:	Loss 3.1519	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 4000:	Loss 3.0338	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4050:	Loss 3.2859	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4100:	Loss 3.0659	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4150:	Loss 3.2536	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4200:	Loss 3.4452	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4250:	Loss 3.3697	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4300:	Loss 3.4746	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4350:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4400:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4450:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4500:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4550:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4600:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4650:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4700:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4750:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4800:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4850:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4900:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4950:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 5000:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
****** Epoch Time (Excluding Evaluation Cost): 0.357 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 70.017 ms (Max: 72.798, Min: 55.896, Sum: 560.137)
Cluster-Wide Average, Compute: 238.236 ms (Max: 251.375, Min: 232.957, Sum: 1905.889)
Cluster-Wide Average, Communication-Layer: 25.161 ms (Max: 30.383, Min: 16.776, Sum: 201.292)
Cluster-Wide Average, Bubble-Imbalance: 19.285 ms (Max: 22.837, Min: 14.725, Sum: 154.283)
Cluster-Wide Average, Communication-Graph: 0.496 ms (Max: 0.579, Min: 0.425, Sum: 3.970)
Cluster-Wide Average, Optimization: 0.096 ms (Max: 0.101, Min: 0.091, Sum: 0.772)
Cluster-Wide Average, Others: 3.768 ms (Max: 17.703, Min: 1.769, Sum: 30.142)
****** Breakdown Sum: 357.061 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.852 GB (Max: 6.698, Min: 5.534, Sum: 46.817)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 51.269 Gbps (Max: 58.499, Min: 44.241, Sum: 410.154)
Layer-level communication (cluster-wide, per-epoch): 1.215 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 1.215 GB
****** Accuracy Results ******
Highest valid_acc: 0.1466
Target test_acc: 0.1483
Epoch to reach the target acc: 1199
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
