Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.889 seconds.
Building the CSC structure...
        It takes 1.891 seconds.
Building the CSC structure...
        It takes 2.025 seconds.
Building the CSC structure...
        It takes 2.233 seconds.
Building the CSC structure...
        It takes 2.306 seconds.
Building the CSC structure...
        It takes 2.495 seconds.
Building the CSC structure...
        It takes 2.622 seconds.
Building the CSC structure...
        It takes 2.669 seconds.
Building the CSC structure...
        It takes 1.788 seconds.
        It takes 1.883 seconds.
        It takes 1.846 seconds.
        It takes 2.205 seconds.
        It takes 2.218 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.360 seconds.
        It takes 2.270 seconds.
        It takes 0.242 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 2.309 seconds.
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.031 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.273 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.743 Gbps (per GPU), 469.943 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.453 Gbps (per GPU), 467.624 Gbps (aggregated)
The layer-level communication performance: 58.462 Gbps (per GPU), 467.693 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.223 Gbps (per GPU), 465.784 Gbps (aggregated)
The layer-level communication performance: 58.186 Gbps (per GPU), 465.487 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.993 Gbps (per GPU), 463.946 Gbps (aggregated)
The layer-level communication performance: 57.913 Gbps (per GPU), 463.307 Gbps (aggregated)
The layer-level communication performance: 57.960 Gbps (per GPU), 463.684 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.663 Gbps (per GPU), 1261.303 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.778 Gbps (per GPU), 1262.228 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.663 Gbps (per GPU), 1261.303 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.683 Gbps (per GPU), 1261.466 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.666 Gbps (per GPU), 1261.326 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.687 Gbps (per GPU), 1261.495 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.661 Gbps (per GPU), 1261.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.687 Gbps (per GPU), 1261.492 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.264 Gbps (per GPU), 818.108 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.252 Gbps (per GPU), 818.015 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.262 Gbps (per GPU), 818.095 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.261 Gbps (per GPU), 818.088 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.263 Gbps (per GPU), 818.101 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.260 Gbps (per GPU), 818.082 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.243 Gbps (per GPU), 817.942 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.260 Gbps (per GPU), 818.082 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.400 Gbps (per GPU), 315.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.397 Gbps (per GPU), 315.179 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.399 Gbps (per GPU), 315.193 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.399 Gbps (per GPU), 315.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.400 Gbps (per GPU), 315.200 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.400 Gbps (per GPU), 315.197 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.398 Gbps (per GPU), 315.188 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.399 Gbps (per GPU), 315.194 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.42ms  2.35ms  2.25ms  1.08  8.38K  3.53M
 chk_1  2.74ms  2.63ms  2.51ms  1.09  6.74K  3.60M
 chk_2  2.67ms  2.48ms  2.41ms  1.11  7.27K  3.53M
 chk_3  2.72ms  2.51ms  2.42ms  1.12  7.92K  3.61M
 chk_4  2.59ms  2.48ms  2.42ms  1.07  5.33K  3.68M
 chk_5  2.55ms  2.38ms  2.27ms  1.12 10.07K  3.45M
 chk_6  2.76ms  2.57ms  2.44ms  1.13  9.41K  3.48M
 chk_7  2.60ms  2.45ms  2.35ms  1.11  8.12K  3.60M
 chk_8  2.71ms  2.60ms  2.52ms  1.08  6.09K  3.64M
 chk_9  2.52ms  2.28ms  2.15ms  1.17 11.10K  3.38M
chk_10  2.74ms  2.63ms  2.56ms  1.07  5.67K  3.63M
chk_11  2.60ms  2.46ms  2.36ms  1.10  8.16K  3.54M
chk_12  2.82ms  2.68ms  2.58ms  1.10  7.24K  3.55M
chk_13  2.63ms  2.54ms  2.46ms  1.07  5.41K  3.68M
chk_14  2.89ms  2.74ms  2.65ms  1.09  7.14K  3.53M
chk_15  2.73ms  2.54ms  2.43ms  1.13  9.25K  3.49M
chk_16  2.53ms  2.45ms  2.40ms  1.06  4.78K  3.77M
chk_17  2.70ms  2.54ms  2.47ms  1.09  6.85K  3.60M
chk_18  2.52ms  2.63ms  2.26ms  1.16  7.47K  3.57M
chk_19  2.58ms  2.43ms  2.40ms  1.07  4.88K  3.75M
chk_20  2.58ms  2.44ms  2.36ms  1.09  7.00K  3.63M
chk_21  2.56ms  2.45ms  2.37ms  1.08  5.41K  3.68M
chk_22  2.76ms  2.52ms  2.39ms  1.15 11.07K  3.39M
chk_23  2.69ms  2.50ms  2.44ms  1.10  7.23K  3.64M
chk_24  2.69ms  2.49ms  2.36ms  1.14 10.13K  3.43M
chk_25  2.52ms  2.38ms  2.31ms  1.09  6.40K  3.57M
chk_26  2.73ms  2.61ms  2.53ms  1.08  5.78K  3.55M
chk_27  2.61ms  2.39ms  2.31ms  1.13  9.34K  3.48M
chk_28  2.95ms  2.75ms  2.68ms  1.10  6.37K  3.57M
chk_29  2.73ms  2.59ms  2.53ms  1.08  5.16K  3.78M
chk_30  2.61ms  2.50ms  2.42ms  1.08  5.44K  3.67M
chk_31  2.76ms  2.65ms  2.53ms  1.09  6.33K  3.63M
   Avg  2.66  2.52  2.42
   Max  2.95  2.75  2.68
   Min  2.42  2.28  2.15
 Ratio  1.22  1.21  1.25
   Var  0.01  0.01  0.01
Profiling takes 2.832 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 327.105 ms
Partition 0 [0, 4) has cost: 327.105 ms
Partition 1 [4, 8) has cost: 322.509 ms
Partition 2 [8, 12) has cost: 322.509 ms
Partition 3 [12, 16) has cost: 322.509 ms
Partition 4 [16, 20) has cost: 322.509 ms
Partition 5 [20, 24) has cost: 322.509 ms
Partition 6 [24, 28) has cost: 322.509 ms
Partition 7 [28, 32) has cost: 319.430 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 159.900 ms
GPU 0, Compute+Comm Time: 123.079 ms, Bubble Time: 27.674 ms, Imbalance Overhead: 9.147 ms
GPU 1, Compute+Comm Time: 121.272 ms, Bubble Time: 27.285 ms, Imbalance Overhead: 11.343 ms
GPU 2, Compute+Comm Time: 121.272 ms, Bubble Time: 27.178 ms, Imbalance Overhead: 11.450 ms
GPU 3, Compute+Comm Time: 121.272 ms, Bubble Time: 27.032 ms, Imbalance Overhead: 11.596 ms
GPU 4, Compute+Comm Time: 121.272 ms, Bubble Time: 26.918 ms, Imbalance Overhead: 11.710 ms
GPU 5, Compute+Comm Time: 121.272 ms, Bubble Time: 26.917 ms, Imbalance Overhead: 11.711 ms
GPU 6, Compute+Comm Time: 121.272 ms, Bubble Time: 26.915 ms, Imbalance Overhead: 11.712 ms
GPU 7, Compute+Comm Time: 119.772 ms, Bubble Time: 27.089 ms, Imbalance Overhead: 13.038 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 292.106 ms
GPU 0, Compute+Comm Time: 225.038 ms, Bubble Time: 51.131 ms, Imbalance Overhead: 15.937 ms
GPU 1, Compute+Comm Time: 226.618 ms, Bubble Time: 50.983 ms, Imbalance Overhead: 14.505 ms
GPU 2, Compute+Comm Time: 226.618 ms, Bubble Time: 51.016 ms, Imbalance Overhead: 14.472 ms
GPU 3, Compute+Comm Time: 226.618 ms, Bubble Time: 51.048 ms, Imbalance Overhead: 14.440 ms
GPU 4, Compute+Comm Time: 226.618 ms, Bubble Time: 51.304 ms, Imbalance Overhead: 14.184 ms
GPU 5, Compute+Comm Time: 226.618 ms, Bubble Time: 51.598 ms, Imbalance Overhead: 13.891 ms
GPU 6, Compute+Comm Time: 226.618 ms, Bubble Time: 51.821 ms, Imbalance Overhead: 13.667 ms
GPU 7, Compute+Comm Time: 229.407 ms, Bubble Time: 52.748 ms, Imbalance Overhead: 9.952 ms
The estimated cost of the whole pipeline: 474.607 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 649.613 ms
Partition 0 [0, 8) has cost: 649.613 ms
Partition 1 [8, 16) has cost: 645.018 ms
Partition 2 [16, 24) has cost: 645.018 ms
Partition 3 [24, 32) has cost: 641.938 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 169.510 ms
GPU 0, Compute+Comm Time: 135.469 ms, Bubble Time: 25.973 ms, Imbalance Overhead: 8.068 ms
GPU 1, Compute+Comm Time: 134.695 ms, Bubble Time: 25.308 ms, Imbalance Overhead: 9.507 ms
GPU 2, Compute+Comm Time: 134.695 ms, Bubble Time: 25.007 ms, Imbalance Overhead: 9.808 ms
GPU 3, Compute+Comm Time: 133.869 ms, Bubble Time: 26.227 ms, Imbalance Overhead: 9.414 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 295.554 ms
GPU 0, Compute+Comm Time: 240.317 ms, Bubble Time: 45.153 ms, Imbalance Overhead: 10.083 ms
GPU 1, Compute+Comm Time: 241.047 ms, Bubble Time: 45.498 ms, Imbalance Overhead: 9.009 ms
GPU 2, Compute+Comm Time: 241.047 ms, Bubble Time: 45.918 ms, Imbalance Overhead: 8.589 ms
GPU 3, Compute+Comm Time: 242.375 ms, Bubble Time: 47.275 ms, Imbalance Overhead: 5.904 ms
    The estimated cost with 2 DP ways is 488.317 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1294.631 ms
Partition 0 [0, 16) has cost: 1294.631 ms
Partition 1 [16, 32) has cost: 1286.956 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 212.592 ms
GPU 0, Compute+Comm Time: 185.712 ms, Bubble Time: 22.732 ms, Imbalance Overhead: 4.149 ms
GPU 1, Compute+Comm Time: 184.918 ms, Bubble Time: 23.174 ms, Imbalance Overhead: 4.500 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 336.842 ms
GPU 0, Compute+Comm Time: 293.759 ms, Bubble Time: 36.908 ms, Imbalance Overhead: 6.176 ms
GPU 1, Compute+Comm Time: 294.816 ms, Bubble Time: 36.789 ms, Imbalance Overhead: 5.238 ms
    The estimated cost with 4 DP ways is 576.907 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2581.587 ms
Partition 0 [0, 32) has cost: 2581.587 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 498.954 ms
GPU 0, Compute+Comm Time: 498.954 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 608.326 ms
GPU 0, Compute+Comm Time: 608.326 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1162.644 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 4 initializing the weights for op[81, 101)...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 7 initializing the weights for op[141, 160)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7136	TrainAcc 0.0398	ValidAcc 0.0348	TestAcc 0.0370	BestValid 0.0348
	Epoch 50:	Loss 3.3680	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.3047	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.2219	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 3.1142	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 250:	Loss 2.9961	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 300:	Loss 3.2961	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 350:	Loss 3.0656	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 400:	Loss 3.0830	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 450:	Loss 3.1672	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 500:	Loss 3.0781	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 550:	Loss 3.1222	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 600:	Loss 3.2750	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 650:	Loss 3.1113	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 700:	Loss 3.2609	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 750:	Loss 3.0882	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 800:	Loss 3.0387	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 850:	Loss 3.2218	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 900:	Loss 3.1903	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 950:	Loss 3.2440	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1000:	Loss 3.1920	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1050:	Loss 3.2170	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1100:	Loss 3.0657	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1150:	Loss 2.9769	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1200:	Loss 3.2018	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1250:	Loss 3.0805	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1300:	Loss 2.9647	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 1350:	Loss 2.9968	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 1400:	Loss 2.9374	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 1450:	Loss 3.1642	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 1500:	Loss 3.1019	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 1550:	Loss 3.1605	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 1600:	Loss 3.1445	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 1650:	Loss 3.0429	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 1700:	Loss 3.0597	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 1750:	Loss 3.0270	TrainAcc 0.1744	ValidAcc 0.2010	TestAcc 0.2009	BestValid 0.2010
	Epoch 1800:	Loss 2.9952	TrainAcc 0.1191	ValidAcc 0.1072	TestAcc 0.1059	BestValid 0.2010
	Epoch 1850:	Loss 3.1819	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 1900:	Loss 3.0443	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 1950:	Loss 3.0648	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2000:	Loss 3.0245	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2050:	Loss 3.1119	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2100:	Loss 3.1446	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2010
	Epoch 2150:	Loss 3.1281	TrainAcc 0.0220	ValidAcc 0.0196	TestAcc 0.0201	BestValid 0.2010
	Epoch 2200:	Loss 3.4311	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.2010
	Epoch 2250:	Loss 3.3361	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2300:	Loss 3.2271	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2350:	Loss 3.3002	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2400:	Loss 3.1601	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2450:	Loss 3.0885	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2500:	Loss 3.2027	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2550:	Loss 3.1014	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 2600:	Loss 3.1560	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2650:	Loss 3.1911	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.2010
	Epoch 2700:	Loss 3.1484	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2750:	Loss 3.0600	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2800:	Loss 3.0144	TrainAcc 0.0292	ValidAcc 0.0063	TestAcc 0.0073	BestValid 0.2010
	Epoch 2850:	Loss 3.0579	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2900:	Loss 3.4368	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 2950:	Loss 3.1708	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2010
	Epoch 3000:	Loss 3.2356	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 3050:	Loss 3.1313	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 3100:	Loss 3.1294	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 3150:	Loss 3.0621	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 3200:	Loss 2.9280	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 3250:	Loss 2.9249	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 3300:	Loss 3.2904	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 3350:	Loss 3.3344	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 3400:	Loss 3.3805	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 3450:	Loss 3.2915	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 3500:	Loss 3.3173	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 3550:	Loss 3.3651	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 3600:	Loss 3.3338	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 3650:	Loss 3.3509	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 3700:	Loss 3.3457	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 3750:	Loss 3.3558	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 3800:	Loss 3.3370	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 3850:	Loss 3.3020	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 3900:	Loss 3.3012	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 3950:	Loss 3.3620	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4000:	Loss 3.3789	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 4050:	Loss 3.3675	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 4100:	Loss 3.3623	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 4150:	Loss 3.3162	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4200:	Loss 3.3229	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4250:	Loss 3.3425	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4300:	Loss 3.3413	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 4350:	Loss 3.3334	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 4400:	Loss 3.2897	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4450:	Loss 3.3005	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4500:	Loss 3.2697	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4550:	Loss 3.2656	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4600:	Loss 3.2994	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4650:	Loss 3.2617	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4700:	Loss 3.1520	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4750:	Loss 3.0342	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 4800:	Loss 2.9991	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4850:	Loss 3.0425	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4900:	Loss 3.1874	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2010
	Epoch 4950:	Loss 3.2492	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
	Epoch 5000:	Loss 3.0637	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2010
****** Epoch Time (Excluding Evaluation Cost): 0.358 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 70.190 ms (Max: 73.183, Min: 55.663, Sum: 561.520)
Cluster-Wide Average, Compute: 238.105 ms (Max: 252.922, Min: 231.096, Sum: 1904.841)
Cluster-Wide Average, Communication-Layer: 25.149 ms (Max: 30.380, Min: 16.893, Sum: 201.194)
Cluster-Wide Average, Bubble-Imbalance: 20.012 ms (Max: 25.380, Min: 13.671, Sum: 160.096)
Cluster-Wide Average, Communication-Graph: 0.506 ms (Max: 0.634, Min: 0.428, Sum: 4.047)
Cluster-Wide Average, Optimization: 0.097 ms (Max: 0.102, Min: 0.092, Sum: 0.773)
Cluster-Wide Average, Others: 3.780 ms (Max: 17.837, Min: 1.762, Sum: 30.242)
****** Breakdown Sum: 357.839 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.852 GB (Max: 6.698, Min: 5.534, Sum: 46.817)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 51.280 Gbps (Max: 58.954, Min: 43.855, Sum: 410.243)
Layer-level communication (cluster-wide, per-epoch): 1.215 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 1.215 GB
****** Accuracy Results ******
Highest valid_acc: 0.2010
Target test_acc: 0.2009
Epoch to reach the target acc: 1749
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
