Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.916 seconds.
Building the CSC structure...
        It takes 1.932 seconds.
Building the CSC structure...
        It takes 1.981 seconds.
Building the CSC structure...
        It takes 2.346 seconds.
Building the CSC structure...
        It takes 2.375 seconds.
Building the CSC structure...
        It takes 2.530 seconds.
Building the CSC structure...
        It takes 2.537 seconds.
Building the CSC structure...
        It takes 2.647 seconds.
Building the CSC structure...
        It takes 1.846 seconds.
        It takes 1.851 seconds.
        It takes 1.845 seconds.
        It takes 2.327 seconds.
        It takes 2.327 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.361 seconds.
        It takes 2.370 seconds.
        It takes 2.287 seconds.
        It takes 0.298 seconds.
Building the Label Vector...
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.302 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.309 seconds.
Building the Label Vector...
        It takes 0.311 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.040 seconds.
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.033 seconds.
        It takes 0.040 seconds.
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.296 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.386 Gbps (per GPU), 459.084 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.114 Gbps (per GPU), 456.908 Gbps (aggregated)
The layer-level communication performance: 57.103 Gbps (per GPU), 456.826 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.885 Gbps (per GPU), 455.079 Gbps (aggregated)
The layer-level communication performance: 56.850 Gbps (per GPU), 454.799 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.643 Gbps (per GPU), 453.141 Gbps (aggregated)
The layer-level communication performance: 56.614 Gbps (per GPU), 452.909 Gbps (aggregated)
The layer-level communication performance: 56.573 Gbps (per GPU), 452.582 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.743 Gbps (per GPU), 1261.943 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.731 Gbps (per GPU), 1261.848 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.743 Gbps (per GPU), 1261.943 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.719 Gbps (per GPU), 1261.753 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.752 Gbps (per GPU), 1262.018 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.734 Gbps (per GPU), 1261.870 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.746 Gbps (per GPU), 1261.967 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.734 Gbps (per GPU), 1261.872 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.288 Gbps (per GPU), 810.304 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.287 Gbps (per GPU), 810.298 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.288 Gbps (per GPU), 810.304 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.286 Gbps (per GPU), 810.291 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.288 Gbps (per GPU), 810.304 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.288 Gbps (per GPU), 810.304 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.271 Gbps (per GPU), 810.167 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.287 Gbps (per GPU), 810.298 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.729 Gbps (per GPU), 317.835 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.729 Gbps (per GPU), 317.829 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.729 Gbps (per GPU), 317.835 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.728 Gbps (per GPU), 317.826 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.728 Gbps (per GPU), 317.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.727 Gbps (per GPU), 317.818 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.729 Gbps (per GPU), 317.834 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.729 Gbps (per GPU), 317.835 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.39ms  2.36ms  2.26ms  1.06  8.38K  3.53M
 chk_1  2.79ms  2.61ms  2.51ms  1.11  6.74K  3.60M
 chk_2  2.63ms  2.49ms  2.40ms  1.09  7.27K  3.53M
 chk_3  2.66ms  2.50ms  2.42ms  1.10  7.92K  3.61M
 chk_4  2.57ms  2.48ms  2.40ms  1.07  5.33K  3.68M
 chk_5  2.57ms  2.37ms  2.24ms  1.15 10.07K  3.45M
 chk_6  2.74ms  2.55ms  2.44ms  1.12  9.41K  3.48M
 chk_7  2.61ms  2.44ms  2.34ms  1.11  8.12K  3.60M
 chk_8  2.69ms  2.57ms  2.51ms  1.07  6.09K  3.64M
 chk_9  2.50ms  2.26ms  2.15ms  1.16 11.10K  3.38M
chk_10  2.74ms  2.62ms  2.57ms  1.07  5.67K  3.63M
chk_11  2.61ms  2.43ms  2.36ms  1.10  8.16K  3.54M
chk_12  2.81ms  2.67ms  2.58ms  1.09  7.24K  3.55M
chk_13  2.63ms  2.51ms  2.44ms  1.08  5.41K  3.68M
chk_14  2.90ms  2.72ms  2.63ms  1.10  7.14K  3.53M
chk_15  2.73ms  2.51ms  2.42ms  1.13  9.25K  3.49M
chk_16  2.54ms  2.44ms  2.39ms  1.06  4.78K  3.77M
chk_17  2.69ms  2.54ms  2.47ms  1.09  6.85K  3.60M
chk_18  2.52ms  2.35ms  2.26ms  1.12  7.47K  3.57M
chk_19  2.56ms  2.49ms  2.39ms  1.07  4.88K  3.75M
chk_20  2.58ms  2.44ms  2.33ms  1.11  7.00K  3.63M
chk_21  2.55ms  2.44ms  2.37ms  1.08  5.41K  3.68M
chk_22  2.77ms  2.52ms  2.39ms  1.16 11.07K  3.39M
chk_23  2.67ms  2.50ms  2.44ms  1.09  7.23K  3.64M
chk_24  2.70ms  2.51ms  2.38ms  1.14 10.13K  3.43M
chk_25  2.50ms  2.39ms  2.30ms  1.09  6.40K  3.57M
chk_26  2.73ms  2.61ms  2.53ms  1.08  5.78K  3.55M
chk_27  2.60ms  2.42ms  2.30ms  1.13  9.34K  3.48M
chk_28  2.91ms  2.77ms  2.67ms  1.09  6.37K  3.57M
chk_29  2.72ms  2.61ms  2.57ms  1.06  5.16K  3.78M
chk_30  2.58ms  2.51ms  2.41ms  1.07  5.44K  3.67M
chk_31  2.76ms  2.64ms  2.54ms  1.09  6.33K  3.63M
   Avg  2.65  2.51  2.42
   Max  2.91  2.77  2.67
   Min  2.39  2.26  2.15
 Ratio  1.22  1.23  1.24
   Var  0.01  0.01  0.01
Profiling takes 2.816 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 325.738 ms
Partition 0 [0, 4) has cost: 325.738 ms
Partition 1 [4, 8) has cost: 321.079 ms
Partition 2 [8, 12) has cost: 321.079 ms
Partition 3 [12, 16) has cost: 321.079 ms
Partition 4 [16, 20) has cost: 321.079 ms
Partition 5 [20, 24) has cost: 321.079 ms
Partition 6 [24, 28) has cost: 321.079 ms
Partition 7 [28, 32) has cost: 318.220 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 154.773 ms
GPU 0, Compute+Comm Time: 122.332 ms, Bubble Time: 27.830 ms, Imbalance Overhead: 4.611 ms
GPU 1, Compute+Comm Time: 120.285 ms, Bubble Time: 27.439 ms, Imbalance Overhead: 7.049 ms
GPU 2, Compute+Comm Time: 120.285 ms, Bubble Time: 27.281 ms, Imbalance Overhead: 7.207 ms
GPU 3, Compute+Comm Time: 120.285 ms, Bubble Time: 27.028 ms, Imbalance Overhead: 7.461 ms
GPU 4, Compute+Comm Time: 120.285 ms, Bubble Time: 26.836 ms, Imbalance Overhead: 7.653 ms
GPU 5, Compute+Comm Time: 120.285 ms, Bubble Time: 26.764 ms, Imbalance Overhead: 7.724 ms
GPU 6, Compute+Comm Time: 120.285 ms, Bubble Time: 26.693 ms, Imbalance Overhead: 7.796 ms
GPU 7, Compute+Comm Time: 119.053 ms, Bubble Time: 26.842 ms, Imbalance Overhead: 8.879 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 292.267 ms
GPU 0, Compute+Comm Time: 225.149 ms, Bubble Time: 51.071 ms, Imbalance Overhead: 16.047 ms
GPU 1, Compute+Comm Time: 226.776 ms, Bubble Time: 50.967 ms, Imbalance Overhead: 14.524 ms
GPU 2, Compute+Comm Time: 226.776 ms, Bubble Time: 51.001 ms, Imbalance Overhead: 14.490 ms
GPU 3, Compute+Comm Time: 226.776 ms, Bubble Time: 51.034 ms, Imbalance Overhead: 14.456 ms
GPU 4, Compute+Comm Time: 226.776 ms, Bubble Time: 51.354 ms, Imbalance Overhead: 14.137 ms
GPU 5, Compute+Comm Time: 226.776 ms, Bubble Time: 51.732 ms, Imbalance Overhead: 13.759 ms
GPU 6, Compute+Comm Time: 226.776 ms, Bubble Time: 52.023 ms, Imbalance Overhead: 13.468 ms
GPU 7, Compute+Comm Time: 229.388 ms, Bubble Time: 53.019 ms, Imbalance Overhead: 9.860 ms
The estimated cost of the whole pipeline: 469.392 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 646.818 ms
Partition 0 [0, 8) has cost: 646.818 ms
Partition 1 [8, 16) has cost: 642.159 ms
Partition 2 [16, 24) has cost: 642.159 ms
Partition 3 [24, 32) has cost: 639.299 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 162.713 ms
GPU 0, Compute+Comm Time: 133.672 ms, Bubble Time: 26.000 ms, Imbalance Overhead: 3.042 ms
GPU 1, Compute+Comm Time: 132.654 ms, Bubble Time: 25.330 ms, Imbalance Overhead: 4.729 ms
GPU 2, Compute+Comm Time: 132.654 ms, Bubble Time: 24.932 ms, Imbalance Overhead: 5.127 ms
GPU 3, Compute+Comm Time: 132.097 ms, Bubble Time: 24.485 ms, Imbalance Overhead: 6.131 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 295.302 ms
GPU 0, Compute+Comm Time: 240.191 ms, Bubble Time: 45.123 ms, Imbalance Overhead: 9.989 ms
GPU 1, Compute+Comm Time: 240.871 ms, Bubble Time: 45.481 ms, Imbalance Overhead: 8.951 ms
GPU 2, Compute+Comm Time: 240.871 ms, Bubble Time: 45.813 ms, Imbalance Overhead: 8.619 ms
GPU 3, Compute+Comm Time: 242.141 ms, Bubble Time: 47.083 ms, Imbalance Overhead: 6.078 ms
    The estimated cost with 2 DP ways is 480.916 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1288.976 ms
Partition 0 [0, 16) has cost: 1288.976 ms
Partition 1 [16, 32) has cost: 1281.458 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 210.691 ms
GPU 0, Compute+Comm Time: 183.260 ms, Bubble Time: 22.815 ms, Imbalance Overhead: 4.615 ms
GPU 1, Compute+Comm Time: 182.464 ms, Bubble Time: 23.164 ms, Imbalance Overhead: 5.062 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 337.206 ms
GPU 0, Compute+Comm Time: 294.286 ms, Bubble Time: 36.760 ms, Imbalance Overhead: 6.160 ms
GPU 1, Compute+Comm Time: 295.245 ms, Bubble Time: 36.841 ms, Imbalance Overhead: 5.120 ms
    The estimated cost with 4 DP ways is 575.292 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2570.434 ms
Partition 0 [0, 32) has cost: 2570.434 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 490.717 ms
GPU 0, Compute+Comm Time: 490.717 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 605.012 ms
GPU 0, Compute+Comm Time: 605.012 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1150.516 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 4 initializing the weights for op[81, 101)...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 7 initializing the weights for op[141, 160)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7136	TrainAcc 0.0209	ValidAcc 0.0164	TestAcc 0.0168	BestValid 0.0164
	Epoch 50:	Loss 3.4188	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.3507	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.3138	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 3.2381	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 250:	Loss 3.2046	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 300:	Loss 3.3172	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 350:	Loss 3.2558	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 400:	Loss 3.2035	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 450:	Loss 3.0302	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0603
	Epoch 500:	Loss 3.0001	TrainAcc 0.1191	ValidAcc 0.1001	TestAcc 0.1028	BestValid 0.1001
	Epoch 550:	Loss 2.9512	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 600:	Loss 3.3329	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 650:	Loss 3.0826	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 700:	Loss 3.1163	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 750:	Loss 3.1170	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 800:	Loss 3.0654	TrainAcc 0.0712	ValidAcc 0.0595	TestAcc 0.0589	BestValid 0.1001
	Epoch 850:	Loss 3.1250	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 900:	Loss 3.0749	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1001
	Epoch 950:	Loss 3.0612	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1001
	Epoch 1000:	Loss 3.2765	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 1050:	Loss 3.1562	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 1100:	Loss 3.0446	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1001
	Epoch 1150:	Loss 3.1044	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1001
	Epoch 1200:	Loss 3.0764	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 1250:	Loss 3.0338	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.1001
	Epoch 1300:	Loss 3.0516	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 1350:	Loss 3.0926	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 1400:	Loss 3.0389	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 1450:	Loss 3.1525	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 1500:	Loss 3.0891	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1001
	Epoch 1550:	Loss 3.1165	TrainAcc 0.0501	ValidAcc 0.0417	TestAcc 0.0453	BestValid 0.1001
	Epoch 1600:	Loss 2.9258	TrainAcc 0.1396	ValidAcc 0.1348	TestAcc 0.1369	BestValid 0.1348
	Epoch 1650:	Loss 2.7702	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 1700:	Loss 3.1187	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 1750:	Loss 3.4263	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 1800:	Loss 3.3417	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 1850:	Loss 3.1314	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1900:	Loss 3.1784	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 1950:	Loss 3.1244	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2000:	Loss 3.0339	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2050:	Loss 3.2440	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2100:	Loss 3.2148	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2150:	Loss 3.0405	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2200:	Loss 3.0889	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 2250:	Loss 3.0963	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2300:	Loss 3.1391	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2350:	Loss 3.1795	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2400:	Loss 3.2599	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 2450:	Loss 3.2989	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1466
	Epoch 2500:	Loss 3.3954	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1466
	Epoch 2550:	Loss 3.3559	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2600:	Loss 3.4208	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2650:	Loss 3.3928	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2700:	Loss 3.2491	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2750:	Loss 3.2120	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2800:	Loss 3.2309	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2850:	Loss 3.0402	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2900:	Loss 3.3664	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1466
	Epoch 2950:	Loss 3.4073	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3000:	Loss 3.3520	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3050:	Loss 3.3010	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3100:	Loss 3.3864	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3150:	Loss 3.4120	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3200:	Loss 3.3990	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3250:	Loss 3.4023	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3300:	Loss 3.4085	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3350:	Loss 3.4010	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3400:	Loss 3.3889	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3450:	Loss 3.3689	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3500:	Loss 3.3740	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3550:	Loss 3.3530	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3600:	Loss 3.3525	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3650:	Loss 3.3641	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3700:	Loss 3.3641	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3750:	Loss 3.3662	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3800:	Loss 3.3754	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3850:	Loss 3.3812	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3900:	Loss 3.4111	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 3950:	Loss 3.3780	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4000:	Loss 3.3603	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4050:	Loss 3.3883	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4100:	Loss 3.3748	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4150:	Loss 3.3685	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4200:	Loss 3.3635	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4250:	Loss 3.3627	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4300:	Loss 3.3585	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4350:	Loss 3.3586	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4400:	Loss 3.3606	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4450:	Loss 3.3611	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4500:	Loss 3.3601	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4550:	Loss 3.3551	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4600:	Loss 3.3595	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4650:	Loss 3.3773	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4700:	Loss 3.4152	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4750:	Loss 3.3653	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4800:	Loss 3.3671	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4850:	Loss 3.3615	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4900:	Loss 3.3640	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 4950:	Loss 3.3607	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 5000:	Loss 3.3591	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
****** Epoch Time (Excluding Evaluation Cost): 0.356 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 69.988 ms (Max: 72.736, Min: 55.859, Sum: 559.901)
Cluster-Wide Average, Compute: 238.021 ms (Max: 251.108, Min: 231.015, Sum: 1904.165)
Cluster-Wide Average, Communication-Layer: 25.193 ms (Max: 30.472, Min: 16.868, Sum: 201.547)
Cluster-Wide Average, Bubble-Imbalance: 19.161 ms (Max: 23.711, Min: 14.485, Sum: 153.288)
Cluster-Wide Average, Communication-Graph: 0.484 ms (Max: 0.559, Min: 0.429, Sum: 3.872)
Cluster-Wide Average, Optimization: 0.096 ms (Max: 0.102, Min: 0.091, Sum: 0.771)
Cluster-Wide Average, Others: 3.768 ms (Max: 17.692, Min: 1.771, Sum: 30.145)
****** Breakdown Sum: 356.711 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.852 GB (Max: 6.698, Min: 5.534, Sum: 46.817)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 51.200 Gbps (Max: 58.613, Min: 43.940, Sum: 409.600)
Layer-level communication (cluster-wide, per-epoch): 1.215 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 1.215 GB
****** Accuracy Results ******
Highest valid_acc: 0.1466
Target test_acc: 0.1483
Epoch to reach the target acc: 1649
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
