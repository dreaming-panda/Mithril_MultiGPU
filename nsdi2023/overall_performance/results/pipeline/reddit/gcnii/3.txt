Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.046 seconds.
Building the CSC structure...
        It takes 2.260 seconds.
Building the CSC structure...
        It takes 2.398 seconds.
Building the CSC structure...
        It takes 2.457 seconds.
Building the CSC structure...
        It takes 2.473 seconds.
Building the CSC structure...
        It takes 2.508 seconds.
Building the CSC structure...
        It takes 2.580 seconds.
Building the CSC structure...
        It takes 2.629 seconds.
Building the CSC structure...
        It takes 1.826 seconds.
        It takes 2.255 seconds.
        It takes 2.284 seconds.
        It takes 2.373 seconds.
        It takes 2.360 seconds.
        It takes 2.334 seconds.
        It takes 2.347 seconds.
        It takes 2.321 seconds.
Building the Feature Vector...
        It takes 0.242 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.272 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.042 seconds.
Building the Feature Vector...
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.309 seconds.
Building the Label Vector...
        It takes 0.312 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.040 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.032 seconds.
        It takes 0.251 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 56.919 Gbps (per GPU), 455.349 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.654 Gbps (per GPU), 453.230 Gbps (aggregated)
The layer-level communication performance: 56.656 Gbps (per GPU), 453.247 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.425 Gbps (per GPU), 451.403 Gbps (aggregated)
The layer-level communication performance: 56.400 Gbps (per GPU), 451.203 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.209 Gbps (per GPU), 449.671 Gbps (aggregated)
The layer-level communication performance: 56.162 Gbps (per GPU), 449.297 Gbps (aggregated)
The layer-level communication performance: 56.138 Gbps (per GPU), 449.101 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.046 Gbps (per GPU), 1256.368 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.034 Gbps (per GPU), 1256.273 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.049 Gbps (per GPU), 1256.391 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.031 Gbps (per GPU), 1256.250 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.055 Gbps (per GPU), 1256.438 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.034 Gbps (per GPU), 1256.273 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.046 Gbps (per GPU), 1256.372 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.034 Gbps (per GPU), 1256.273 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.447 Gbps (per GPU), 803.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.449 Gbps (per GPU), 803.590 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.470 Gbps (per GPU), 803.756 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.433 Gbps (per GPU), 803.468 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.447 Gbps (per GPU), 803.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.449 Gbps (per GPU), 803.590 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.464 Gbps (per GPU), 803.712 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.449 Gbps (per GPU), 803.590 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.103 Gbps (per GPU), 256.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.103 Gbps (per GPU), 256.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.103 Gbps (per GPU), 256.821 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.102 Gbps (per GPU), 256.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.101 Gbps (per GPU), 256.808 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.103 Gbps (per GPU), 256.820 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.101 Gbps (per GPU), 256.804 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.102 Gbps (per GPU), 256.819 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.43ms  2.72ms  3.03  8.38K  3.53M
 chk_1  0.76ms  2.74ms  2.91ms  3.82  6.74K  3.60M
 chk_2  0.80ms  2.63ms  2.79ms  3.48  7.27K  3.53M
 chk_3  0.81ms  2.65ms  2.83ms  3.50  7.92K  3.61M
 chk_4  0.63ms  2.57ms  2.74ms  4.32  5.33K  3.68M
 chk_5  1.01ms  2.57ms  2.76ms  2.74 10.07K  3.45M
 chk_6  0.97ms  2.75ms  2.93ms  3.04  9.41K  3.48M
 chk_7  0.82ms  2.57ms  2.75ms  3.34  8.12K  3.60M
 chk_8  0.68ms  2.69ms  2.84ms  4.15  6.09K  3.64M
 chk_9  1.10ms  2.52ms  2.70ms  2.45 11.10K  3.38M
chk_10  0.66ms  2.73ms  2.89ms  4.40  5.67K  3.63M
chk_11  0.83ms  2.60ms  2.75ms  3.32  8.16K  3.54M
chk_12  0.80ms  2.77ms  2.95ms  3.69  7.24K  3.55M
chk_13  0.64ms  2.62ms  2.75ms  4.30  5.41K  3.68M
chk_14  0.78ms  2.86ms  3.04ms  3.88  7.14K  3.53M
chk_15  0.96ms  2.72ms  2.91ms  3.05  9.25K  3.49M
chk_16  0.60ms  2.54ms  2.70ms  4.50  4.78K  3.77M
chk_17  0.79ms  2.66ms  2.84ms  3.60  6.85K  3.60M
chk_18  0.81ms  2.47ms  2.65ms  3.25  7.47K  3.57M
chk_19  0.61ms  2.55ms  2.69ms  4.42  4.88K  3.75M
chk_20  0.77ms  2.53ms  2.70ms  3.49  7.00K  3.63M
chk_21  0.64ms  2.54ms  2.69ms  4.22  5.41K  3.68M
chk_22  1.10ms  2.75ms  2.96ms  2.68 11.07K  3.39M
chk_23  0.80ms  2.64ms  2.80ms  3.50  7.23K  3.64M
chk_24  1.01ms  2.70ms  2.89ms  2.85 10.13K  3.43M
chk_25  0.73ms  2.53ms  2.68ms  3.65  6.40K  3.57M
chk_26  0.67ms  2.73ms  2.87ms  4.32  5.78K  3.55M
chk_27  0.96ms  2.60ms  2.78ms  2.89  9.34K  3.48M
chk_28  0.73ms  2.90ms  3.06ms  4.19  6.37K  3.57M
chk_29  0.64ms  2.70ms  2.84ms  4.47  5.16K  3.78M
chk_30  0.64ms  2.60ms  2.74ms  4.28  5.44K  3.67M
chk_31  0.73ms  2.72ms  2.89ms  3.98  6.33K  3.63M
   Avg  0.79  2.64  2.81
   Max  1.10  2.90  3.06
   Min  0.60  2.43  2.65
 Ratio  1.84  1.19  1.16
   Var  0.02  0.01  0.01
Profiling takes 2.408 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 363.678 ms
Partition 0 [0, 5) has cost: 363.678 ms
Partition 1 [5, 9) has cost: 338.299 ms
Partition 2 [9, 13) has cost: 338.299 ms
Partition 3 [13, 17) has cost: 338.299 ms
Partition 4 [17, 21) has cost: 338.299 ms
Partition 5 [21, 25) has cost: 338.299 ms
Partition 6 [25, 29) has cost: 338.299 ms
Partition 7 [29, 33) has cost: 343.763 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.327 ms
GPU 0, Compute+Comm Time: 132.516 ms, Bubble Time: 28.841 ms, Imbalance Overhead: 2.969 ms
GPU 1, Compute+Comm Time: 125.405 ms, Bubble Time: 28.577 ms, Imbalance Overhead: 10.345 ms
GPU 2, Compute+Comm Time: 125.405 ms, Bubble Time: 28.601 ms, Imbalance Overhead: 10.321 ms
GPU 3, Compute+Comm Time: 125.405 ms, Bubble Time: 28.528 ms, Imbalance Overhead: 10.394 ms
GPU 4, Compute+Comm Time: 125.405 ms, Bubble Time: 28.446 ms, Imbalance Overhead: 10.476 ms
GPU 5, Compute+Comm Time: 125.405 ms, Bubble Time: 28.487 ms, Imbalance Overhead: 10.435 ms
GPU 6, Compute+Comm Time: 125.405 ms, Bubble Time: 28.723 ms, Imbalance Overhead: 10.199 ms
GPU 7, Compute+Comm Time: 126.587 ms, Bubble Time: 29.123 ms, Imbalance Overhead: 8.617 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 316.944 ms
GPU 0, Compute+Comm Time: 243.371 ms, Bubble Time: 56.526 ms, Imbalance Overhead: 17.048 ms
GPU 1, Compute+Comm Time: 239.090 ms, Bubble Time: 55.693 ms, Imbalance Overhead: 22.162 ms
GPU 2, Compute+Comm Time: 239.090 ms, Bubble Time: 55.117 ms, Imbalance Overhead: 22.738 ms
GPU 3, Compute+Comm Time: 239.090 ms, Bubble Time: 55.027 ms, Imbalance Overhead: 22.828 ms
GPU 4, Compute+Comm Time: 239.090 ms, Bubble Time: 55.124 ms, Imbalance Overhead: 22.731 ms
GPU 5, Compute+Comm Time: 239.090 ms, Bubble Time: 55.241 ms, Imbalance Overhead: 22.614 ms
GPU 6, Compute+Comm Time: 239.090 ms, Bubble Time: 55.102 ms, Imbalance Overhead: 22.752 ms
GPU 7, Compute+Comm Time: 257.357 ms, Bubble Time: 55.606 ms, Imbalance Overhead: 3.982 ms
The estimated cost of the whole pipeline: 505.335 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 701.977 ms
Partition 0 [0, 9) has cost: 701.977 ms
Partition 1 [9, 17) has cost: 676.599 ms
Partition 2 [17, 25) has cost: 676.599 ms
Partition 3 [25, 33) has cost: 682.062 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.553 ms
GPU 0, Compute+Comm Time: 141.520 ms, Bubble Time: 26.684 ms, Imbalance Overhead: 2.349 ms
GPU 1, Compute+Comm Time: 137.669 ms, Bubble Time: 26.255 ms, Imbalance Overhead: 6.628 ms
GPU 2, Compute+Comm Time: 137.669 ms, Bubble Time: 26.177 ms, Imbalance Overhead: 6.706 ms
GPU 3, Compute+Comm Time: 138.247 ms, Bubble Time: 25.933 ms, Imbalance Overhead: 6.373 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 315.679 ms
GPU 0, Compute+Comm Time: 255.081 ms, Bubble Time: 48.229 ms, Imbalance Overhead: 12.369 ms
GPU 1, Compute+Comm Time: 252.886 ms, Bubble Time: 48.493 ms, Imbalance Overhead: 14.300 ms
GPU 2, Compute+Comm Time: 252.886 ms, Bubble Time: 48.638 ms, Imbalance Overhead: 14.155 ms
GPU 3, Compute+Comm Time: 262.980 ms, Bubble Time: 49.583 ms, Imbalance Overhead: 3.115 ms
    The estimated cost with 2 DP ways is 510.543 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1378.576 ms
Partition 0 [0, 17) has cost: 1378.576 ms
Partition 1 [17, 33) has cost: 1358.661 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 218.453 ms
GPU 0, Compute+Comm Time: 190.093 ms, Bubble Time: 23.329 ms, Imbalance Overhead: 5.031 ms
GPU 1, Compute+Comm Time: 188.269 ms, Bubble Time: 23.991 ms, Imbalance Overhead: 6.193 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 355.191 ms
GPU 0, Compute+Comm Time: 307.304 ms, Bubble Time: 39.139 ms, Imbalance Overhead: 8.748 ms
GPU 1, Compute+Comm Time: 311.884 ms, Bubble Time: 38.046 ms, Imbalance Overhead: 5.261 ms
    The estimated cost with 4 DP ways is 602.327 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2737.237 ms
Partition 0 [0, 33) has cost: 2737.237 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 585.459 ms
GPU 0, Compute+Comm Time: 585.459 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 708.634 ms
GPU 0, Compute+Comm Time: 708.634 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1358.798 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7923	TrainAcc 0.0461	ValidAcc 0.0429	TestAcc 0.0429	BestValid 0.0429
	Epoch 50:	Loss 1.8854	TrainAcc 0.6924	ValidAcc 0.7099	TestAcc 0.7067	BestValid 0.7099
	Epoch 100:	Loss 1.2976	TrainAcc 0.7881	ValidAcc 0.7995	TestAcc 0.7954	BestValid 0.7995
	Epoch 150:	Loss 1.0488	TrainAcc 0.8360	ValidAcc 0.8447	TestAcc 0.8402	BestValid 0.8447
	Epoch 200:	Loss 0.9204	TrainAcc 0.8601	ValidAcc 0.8663	TestAcc 0.8629	BestValid 0.8663
	Epoch 250:	Loss 0.8406	TrainAcc 0.8722	ValidAcc 0.8785	TestAcc 0.8745	BestValid 0.8785
	Epoch 300:	Loss 0.7795	TrainAcc 0.8835	ValidAcc 0.8895	TestAcc 0.8854	BestValid 0.8895
	Epoch 350:	Loss 0.7342	TrainAcc 0.8914	ValidAcc 0.8967	TestAcc 0.8929	BestValid 0.8967
	Epoch 400:	Loss 0.7011	TrainAcc 0.8961	ValidAcc 0.9009	TestAcc 0.8967	BestValid 0.9009
	Epoch 450:	Loss 0.6777	TrainAcc 0.9013	ValidAcc 0.9057	TestAcc 0.9010	BestValid 0.9057
	Epoch 500:	Loss 0.6592	TrainAcc 0.9049	ValidAcc 0.9081	TestAcc 0.9035	BestValid 0.9081
	Epoch 550:	Loss 0.6364	TrainAcc 0.9081	ValidAcc 0.9111	TestAcc 0.9063	BestValid 0.9111
	Epoch 600:	Loss 0.6181	TrainAcc 0.9110	ValidAcc 0.9143	TestAcc 0.9090	BestValid 0.9143
	Epoch 650:	Loss 0.6027	TrainAcc 0.9142	ValidAcc 0.9168	TestAcc 0.9118	BestValid 0.9168
	Epoch 700:	Loss 0.5923	TrainAcc 0.9155	ValidAcc 0.9176	TestAcc 0.9134	BestValid 0.9176
	Epoch 750:	Loss 0.5831	TrainAcc 0.9180	ValidAcc 0.9202	TestAcc 0.9153	BestValid 0.9202
	Epoch 800:	Loss 0.5664	TrainAcc 0.9195	ValidAcc 0.9219	TestAcc 0.9174	BestValid 0.9219
	Epoch 850:	Loss 0.5556	TrainAcc 0.9220	ValidAcc 0.9243	TestAcc 0.9194	BestValid 0.9243
	Epoch 900:	Loss 0.5552	TrainAcc 0.9231	ValidAcc 0.9248	TestAcc 0.9202	BestValid 0.9248
	Epoch 950:	Loss 0.5423	TrainAcc 0.9241	ValidAcc 0.9257	TestAcc 0.9214	BestValid 0.9257
	Epoch 1000:	Loss 0.5336	TrainAcc 0.9255	ValidAcc 0.9272	TestAcc 0.9224	BestValid 0.9272
	Epoch 1050:	Loss 0.5332	TrainAcc 0.9264	ValidAcc 0.9274	TestAcc 0.9231	BestValid 0.9274
	Epoch 1100:	Loss 0.5257	TrainAcc 0.9280	ValidAcc 0.9290	TestAcc 0.9247	BestValid 0.9290
	Epoch 1150:	Loss 0.5179	TrainAcc 0.9280	ValidAcc 0.9285	TestAcc 0.9245	BestValid 0.9290
	Epoch 1200:	Loss 0.5107	TrainAcc 0.9294	ValidAcc 0.9298	TestAcc 0.9258	BestValid 0.9298
	Epoch 1250:	Loss 0.5051	TrainAcc 0.9305	ValidAcc 0.9306	TestAcc 0.9265	BestValid 0.9306
	Epoch 1300:	Loss 0.4954	TrainAcc 0.9310	ValidAcc 0.9304	TestAcc 0.9271	BestValid 0.9306
	Epoch 1350:	Loss 0.4937	TrainAcc 0.9314	ValidAcc 0.9306	TestAcc 0.9275	BestValid 0.9306
	Epoch 1400:	Loss 0.4915	TrainAcc 0.9334	ValidAcc 0.9329	TestAcc 0.9294	BestValid 0.9329
	Epoch 1450:	Loss 0.4851	TrainAcc 0.9341	ValidAcc 0.9334	TestAcc 0.9301	BestValid 0.9334
	Epoch 1500:	Loss 0.4847	TrainAcc 0.9331	ValidAcc 0.9319	TestAcc 0.9288	BestValid 0.9334
	Epoch 1550:	Loss 0.4793	TrainAcc 0.9357	ValidAcc 0.9343	TestAcc 0.9313	BestValid 0.9343
	Epoch 1600:	Loss 0.4737	TrainAcc 0.9357	ValidAcc 0.9343	TestAcc 0.9310	BestValid 0.9343
	Epoch 1650:	Loss 0.4678	TrainAcc 0.9349	ValidAcc 0.9333	TestAcc 0.9302	BestValid 0.9343
	Epoch 1700:	Loss 0.4621	TrainAcc 0.9363	ValidAcc 0.9344	TestAcc 0.9315	BestValid 0.9344
	Epoch 1750:	Loss 0.4603	TrainAcc 0.9365	ValidAcc 0.9344	TestAcc 0.9320	BestValid 0.9344
	Epoch 1800:	Loss 0.4567	TrainAcc 0.9367	ValidAcc 0.9346	TestAcc 0.9321	BestValid 0.9346
	Epoch 1850:	Loss 0.4512	TrainAcc 0.9381	ValidAcc 0.9358	TestAcc 0.9335	BestValid 0.9358
	Epoch 1900:	Loss 0.4494	TrainAcc 0.9391	ValidAcc 0.9366	TestAcc 0.9338	BestValid 0.9366
	Epoch 1950:	Loss 0.4474	TrainAcc 0.9389	ValidAcc 0.9365	TestAcc 0.9337	BestValid 0.9366
	Epoch 2000:	Loss 0.4440	TrainAcc 0.9382	ValidAcc 0.9360	TestAcc 0.9335	BestValid 0.9366
	Epoch 2050:	Loss 0.4447	TrainAcc 0.9399	ValidAcc 0.9371	TestAcc 0.9344	BestValid 0.9371
	Epoch 2100:	Loss 0.4358	TrainAcc 0.9396	ValidAcc 0.9369	TestAcc 0.9345	BestValid 0.9371
	Epoch 2150:	Loss 0.4369	TrainAcc 0.9394	ValidAcc 0.9366	TestAcc 0.9342	BestValid 0.9371
	Epoch 2200:	Loss 0.4399	TrainAcc 0.9401	ValidAcc 0.9368	TestAcc 0.9346	BestValid 0.9371
	Epoch 2250:	Loss 0.4316	TrainAcc 0.9408	ValidAcc 0.9371	TestAcc 0.9350	BestValid 0.9371
	Epoch 2300:	Loss 0.4293	TrainAcc 0.9400	ValidAcc 0.9367	TestAcc 0.9349	BestValid 0.9371
	Epoch 2350:	Loss 0.4239	TrainAcc 0.9409	ValidAcc 0.9375	TestAcc 0.9355	BestValid 0.9375
	Epoch 2400:	Loss 0.4226	TrainAcc 0.9418	ValidAcc 0.9383	TestAcc 0.9364	BestValid 0.9383
	Epoch 2450:	Loss 0.4217	TrainAcc 0.9409	ValidAcc 0.9371	TestAcc 0.9355	BestValid 0.9383
	Epoch 2500:	Loss 0.4190	TrainAcc 0.9413	ValidAcc 0.9376	TestAcc 0.9356	BestValid 0.9383
	Epoch 2550:	Loss 0.4160	TrainAcc 0.9418	ValidAcc 0.9379	TestAcc 0.9359	BestValid 0.9383
	Epoch 2600:	Loss 0.4152	TrainAcc 0.9419	ValidAcc 0.9378	TestAcc 0.9358	BestValid 0.9383
	Epoch 2650:	Loss 0.4128	TrainAcc 0.9430	ValidAcc 0.9389	TestAcc 0.9372	BestValid 0.9389
	Epoch 2700:	Loss 0.4109	TrainAcc 0.9424	ValidAcc 0.9382	TestAcc 0.9366	BestValid 0.9389
	Epoch 2750:	Loss 0.4121	TrainAcc 0.9441	ValidAcc 0.9402	TestAcc 0.9385	BestValid 0.9402
	Epoch 2800:	Loss 0.4056	TrainAcc 0.9441	ValidAcc 0.9399	TestAcc 0.9379	BestValid 0.9402
	Epoch 2850:	Loss 0.4061	TrainAcc 0.9433	ValidAcc 0.9394	TestAcc 0.9373	BestValid 0.9402
	Epoch 2900:	Loss 0.4058	TrainAcc 0.9436	ValidAcc 0.9397	TestAcc 0.9375	BestValid 0.9402
	Epoch 2950:	Loss 0.3992	TrainAcc 0.9430	ValidAcc 0.9390	TestAcc 0.9368	BestValid 0.9402
	Epoch 3000:	Loss 0.3989	TrainAcc 0.9459	ValidAcc 0.9414	TestAcc 0.9399	BestValid 0.9414
	Epoch 3050:	Loss 0.3952	TrainAcc 0.9421	ValidAcc 0.9379	TestAcc 0.9357	BestValid 0.9414
	Epoch 3100:	Loss 0.3943	TrainAcc 0.9430	ValidAcc 0.9387	TestAcc 0.9368	BestValid 0.9414
	Epoch 3150:	Loss 0.3945	TrainAcc 0.9448	ValidAcc 0.9406	TestAcc 0.9382	BestValid 0.9414
	Epoch 3200:	Loss 0.3894	TrainAcc 0.9455	ValidAcc 0.9414	TestAcc 0.9394	BestValid 0.9414
	Epoch 3250:	Loss 0.3911	TrainAcc 0.9474	ValidAcc 0.9435	TestAcc 0.9412	BestValid 0.9435
	Epoch 3300:	Loss 0.3917	TrainAcc 0.9453	ValidAcc 0.9415	TestAcc 0.9390	BestValid 0.9435
	Epoch 3350:	Loss 0.3904	TrainAcc 0.9456	ValidAcc 0.9418	TestAcc 0.9395	BestValid 0.9435
	Epoch 3400:	Loss 0.3862	TrainAcc 0.9442	ValidAcc 0.9398	TestAcc 0.9379	BestValid 0.9435
	Epoch 3450:	Loss 0.3860	TrainAcc 0.9462	ValidAcc 0.9415	TestAcc 0.9397	BestValid 0.9435
	Epoch 3500:	Loss 0.3842	TrainAcc 0.9457	ValidAcc 0.9411	TestAcc 0.9393	BestValid 0.9435
	Epoch 3550:	Loss 0.3801	TrainAcc 0.9472	ValidAcc 0.9427	TestAcc 0.9410	BestValid 0.9435
	Epoch 3600:	Loss 0.3784	TrainAcc 0.9466	ValidAcc 0.9418	TestAcc 0.9405	BestValid 0.9435
	Epoch 3650:	Loss 0.3804	TrainAcc 0.9473	ValidAcc 0.9428	TestAcc 0.9410	BestValid 0.9435
	Epoch 3700:	Loss 0.3799	TrainAcc 0.9456	ValidAcc 0.9406	TestAcc 0.9392	BestValid 0.9435
	Epoch 3750:	Loss 0.3737	TrainAcc 0.9455	ValidAcc 0.9402	TestAcc 0.9389	BestValid 0.9435
	Epoch 3800:	Loss 0.3774	TrainAcc 0.9478	ValidAcc 0.9427	TestAcc 0.9410	BestValid 0.9435
	Epoch 3850:	Loss 0.3785	TrainAcc 0.9471	ValidAcc 0.9418	TestAcc 0.9403	BestValid 0.9435
	Epoch 3900:	Loss 0.3727	TrainAcc 0.9489	ValidAcc 0.9440	TestAcc 0.9424	BestValid 0.9440
	Epoch 3950:	Loss 0.3741	TrainAcc 0.9465	ValidAcc 0.9414	TestAcc 0.9395	BestValid 0.9440
	Epoch 4000:	Loss 0.3688	TrainAcc 0.9461	ValidAcc 0.9404	TestAcc 0.9395	BestValid 0.9440
	Epoch 4050:	Loss 0.3690	TrainAcc 0.9476	ValidAcc 0.9421	TestAcc 0.9406	BestValid 0.9440
	Epoch 4100:	Loss 0.3682	TrainAcc 0.9496	ValidAcc 0.9444	TestAcc 0.9425	BestValid 0.9444
	Epoch 4150:	Loss 0.3623	TrainAcc 0.9480	ValidAcc 0.9427	TestAcc 0.9413	BestValid 0.9444
	Epoch 4200:	Loss 0.3660	TrainAcc 0.9490	ValidAcc 0.9434	TestAcc 0.9421	BestValid 0.9444
	Epoch 4250:	Loss 0.3627	TrainAcc 0.9484	ValidAcc 0.9422	TestAcc 0.9411	BestValid 0.9444
	Epoch 4300:	Loss 0.3608	TrainAcc 0.9480	ValidAcc 0.9418	TestAcc 0.9407	BestValid 0.9444
	Epoch 4350:	Loss 0.3606	TrainAcc 0.9478	ValidAcc 0.9420	TestAcc 0.9407	BestValid 0.9444
	Epoch 4400:	Loss 0.3581	TrainAcc 0.9506	ValidAcc 0.9452	TestAcc 0.9438	BestValid 0.9452
	Epoch 4450:	Loss 0.3617	TrainAcc 0.9520	ValidAcc 0.9469	TestAcc 0.9445	BestValid 0.9469
	Epoch 4500:	Loss 0.3583	TrainAcc 0.9487	ValidAcc 0.9431	TestAcc 0.9416	BestValid 0.9469
	Epoch 4550:	Loss 0.3534	TrainAcc 0.9484	ValidAcc 0.9424	TestAcc 0.9414	BestValid 0.9469
	Epoch 4600:	Loss 0.3546	TrainAcc 0.9496	ValidAcc 0.9446	TestAcc 0.9429	BestValid 0.9469
	Epoch 4650:	Loss 0.3562	TrainAcc 0.9473	ValidAcc 0.9421	TestAcc 0.9402	BestValid 0.9469
	Epoch 4700:	Loss 0.3519	TrainAcc 0.9508	ValidAcc 0.9452	TestAcc 0.9438	BestValid 0.9469
	Epoch 4750:	Loss 0.3567	TrainAcc 0.9509	ValidAcc 0.9460	TestAcc 0.9442	BestValid 0.9469
	Epoch 4800:	Loss 0.3524	TrainAcc 0.9521	ValidAcc 0.9463	TestAcc 0.9453	BestValid 0.9469
	Epoch 4850:	Loss 0.3517	TrainAcc 0.9508	ValidAcc 0.9448	TestAcc 0.9437	BestValid 0.9469
	Epoch 4900:	Loss 0.3528	TrainAcc 0.9472	ValidAcc 0.9418	TestAcc 0.9401	BestValid 0.9469
	Epoch 4950:	Loss 0.3499	TrainAcc 0.9511	ValidAcc 0.9449	TestAcc 0.9442	BestValid 0.9469
	Epoch 5000:	Loss 0.3496	TrainAcc 0.9537	ValidAcc 0.9473	TestAcc 0.9466	BestValid 0.9473
****** Epoch Time (Excluding Evaluation Cost): 0.415 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 80.125 ms (Max: 82.589, Min: 66.458, Sum: 641.002)
Cluster-Wide Average, Compute: 261.748 ms (Max: 286.436, Min: 253.405, Sum: 2093.983)
Cluster-Wide Average, Communication-Layer: 41.613 ms (Max: 47.213, Min: 31.950, Sum: 332.903)
Cluster-Wide Average, Bubble-Imbalance: 27.716 ms (Max: 34.531, Min: 12.417, Sum: 221.727)
Cluster-Wide Average, Communication-Graph: 0.460 ms (Max: 0.518, Min: 0.407, Sum: 3.679)
Cluster-Wide Average, Optimization: 0.100 ms (Max: 0.123, Min: 0.091, Sum: 0.801)
Cluster-Wide Average, Others: 3.736 ms (Max: 17.400, Min: 1.773, Sum: 29.885)
****** Breakdown Sum: 415.498 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.304 GB (Max: 8.059, Min: 6.018, Sum: 50.433)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 61.711 Gbps (Max: 73.912, Min: 45.232, Sum: 493.690)
Layer-level communication (cluster-wide, per-epoch): 2.430 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 2.430 GB
****** Accuracy Results ******
Highest valid_acc: 0.9473
Target test_acc: 0.9466
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
