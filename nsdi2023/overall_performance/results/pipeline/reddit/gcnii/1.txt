Initialized node 2 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.881 seconds.
Building the CSC structure...
        It takes 1.882 seconds.
Building the CSC structure...
        It takes 1.931 seconds.
Building the CSC structure...
        It takes 2.426 seconds.
Building the CSC structure...
        It takes 2.592 seconds.
Building the CSC structure...
        It takes 2.629 seconds.
Building the CSC structure...
        It takes 2.654 seconds.
Building the CSC structure...
        It takes 2.663 seconds.
Building the CSC structure...
        It takes 1.788 seconds.
        It takes 1.858 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.305 seconds.
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 2.298 seconds.
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.046 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.284 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.031 seconds.
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 6.574 seconds.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
        It takes 6.211 seconds.
        It takes 5.984 seconds.
        It takes 6.015 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.260 seconds.
Building the Label Vector...
Building the Feature Vector...
csr in-out ready !Start Cost Model Initialization...
        It takes 0.040 seconds.
csr in-out ready !Start Cost Model Initialization...
        It takes 0.310 seconds.
Building the Label Vector...
        It takes 0.044 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.979 Gbps (per GPU), 455.829 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.728 Gbps (per GPU), 453.822 Gbps (aggregated)
The layer-level communication performance: 56.733 Gbps (per GPU), 453.861 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.507 Gbps (per GPU), 452.056 Gbps (aggregated)
The layer-level communication performance: 56.483 Gbps (per GPU), 451.867 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.380 Gbps (per GPU), 451.043 Gbps (aggregated)
The layer-level communication performance: 56.247 Gbps (per GPU), 449.977 Gbps (aggregated)
The layer-level communication performance: 56.225 Gbps (per GPU), 449.797 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.569 Gbps (per GPU), 1268.551 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.581 Gbps (per GPU), 1268.646 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.563 Gbps (per GPU), 1268.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.581 Gbps (per GPU), 1268.646 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.566 Gbps (per GPU), 1268.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.584 Gbps (per GPU), 1268.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.566 Gbps (per GPU), 1268.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.581 Gbps (per GPU), 1268.650 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.705 Gbps (per GPU), 797.636 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.706 Gbps (per GPU), 797.649 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.704 Gbps (per GPU), 797.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.706 Gbps (per GPU), 797.649 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.705 Gbps (per GPU), 797.636 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.705 Gbps (per GPU), 797.636 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.704 Gbps (per GPU), 797.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.707 Gbps (per GPU), 797.654 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.986 Gbps (per GPU), 271.890 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.986 Gbps (per GPU), 271.885 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.984 Gbps (per GPU), 271.876 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.985 Gbps (per GPU), 271.881 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.985 Gbps (per GPU), 271.883 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.984 Gbps (per GPU), 271.872 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.985 Gbps (per GPU), 271.879 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.983 Gbps (per GPU), 271.867 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.41ms  2.71ms  3.05  8.38K  3.53M
 chk_1  0.75ms  2.60ms  2.91ms  3.88  6.74K  3.60M
 chk_2  0.79ms  2.58ms  2.77ms  3.51  7.27K  3.53M
 chk_3  0.80ms  2.64ms  2.82ms  3.53  7.92K  3.61M
 chk_4  0.62ms  2.57ms  2.72ms  4.37  5.33K  3.68M
 chk_5  1.00ms  2.57ms  2.76ms  2.75 10.07K  3.45M
 chk_6  0.96ms  2.73ms  2.92ms  3.05  9.41K  3.48M
 chk_7  0.82ms  2.57ms  2.73ms  3.34  8.12K  3.60M
 chk_8  0.68ms  2.70ms  2.85ms  4.22  6.09K  3.64M
 chk_9  1.09ms  2.51ms  2.68ms  2.45 11.10K  3.38M
chk_10  0.65ms  2.73ms  3.00ms  4.64  5.67K  3.63M
chk_11  0.82ms  2.60ms  2.74ms  3.35  8.16K  3.54M
chk_12  0.79ms  2.79ms  2.96ms  3.74  7.24K  3.55M
chk_13  0.63ms  2.62ms  2.78ms  4.40  5.41K  3.68M
chk_14  0.78ms  2.87ms  3.02ms  3.90  7.14K  3.53M
chk_15  0.95ms  2.72ms  2.89ms  3.06  9.25K  3.49M
chk_16  0.59ms  2.55ms  2.70ms  4.56  4.78K  3.77M
chk_17  0.76ms  2.68ms  2.83ms  3.73  6.85K  3.60M
chk_18  0.81ms  2.49ms  2.67ms  3.31  7.47K  3.57M
chk_19  0.60ms  2.54ms  2.69ms  4.46  4.88K  3.75M
chk_20  0.77ms  2.60ms  2.74ms  3.56  7.00K  3.63M
chk_21  0.63ms  2.58ms  2.70ms  4.28  5.41K  3.68M
chk_22  1.09ms  2.78ms  2.97ms  2.72 11.07K  3.39M
chk_23  0.79ms  2.66ms  2.81ms  3.56  7.23K  3.64M
chk_24  1.01ms  2.73ms  2.93ms  2.90 10.13K  3.43M
chk_25  0.72ms  2.55ms  2.71ms  3.74  6.40K  3.57M
chk_26  0.66ms  2.73ms  2.89ms  4.41  5.78K  3.55M
chk_27  0.95ms  2.62ms  2.82ms  2.96  9.34K  3.48M
chk_28  0.72ms  2.91ms  3.09ms  4.29  6.37K  3.57M
chk_29  0.63ms  2.72ms  2.87ms  4.58  5.16K  3.78M
chk_30  0.63ms  2.60ms  2.78ms  4.38  5.44K  3.67M
chk_31  0.72ms  2.74ms  2.90ms  4.03  6.33K  3.63M
   Avg  0.78  2.65  2.82
   Max  1.09  2.91  3.09
   Min  0.59  2.41  2.67
 Ratio  1.84  1.21  1.16
   Var  0.02  0.01  0.01
Profiling takes 2.395 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 363.849 ms
Partition 0 [0, 5) has cost: 363.849 ms
Partition 1 [5, 9) has cost: 338.764 ms
Partition 2 [9, 13) has cost: 338.764 ms
Partition 3 [13, 17) has cost: 338.764 ms
Partition 4 [17, 21) has cost: 338.764 ms
Partition 5 [21, 25) has cost: 338.764 ms
Partition 6 [25, 29) has cost: 338.764 ms
Partition 7 [29, 33) has cost: 344.421 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.375 ms
GPU 0, Compute+Comm Time: 132.475 ms, Bubble Time: 28.945 ms, Imbalance Overhead: 2.956 ms
GPU 1, Compute+Comm Time: 125.468 ms, Bubble Time: 28.642 ms, Imbalance Overhead: 10.266 ms
GPU 2, Compute+Comm Time: 125.468 ms, Bubble Time: 28.470 ms, Imbalance Overhead: 10.438 ms
GPU 3, Compute+Comm Time: 125.468 ms, Bubble Time: 28.300 ms, Imbalance Overhead: 10.608 ms
GPU 4, Compute+Comm Time: 125.468 ms, Bubble Time: 28.211 ms, Imbalance Overhead: 10.697 ms
GPU 5, Compute+Comm Time: 125.468 ms, Bubble Time: 28.160 ms, Imbalance Overhead: 10.748 ms
GPU 6, Compute+Comm Time: 125.468 ms, Bubble Time: 28.373 ms, Imbalance Overhead: 10.535 ms
GPU 7, Compute+Comm Time: 126.824 ms, Bubble Time: 28.727 ms, Imbalance Overhead: 8.824 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 317.290 ms
GPU 0, Compute+Comm Time: 243.765 ms, Bubble Time: 55.754 ms, Imbalance Overhead: 17.771 ms
GPU 1, Compute+Comm Time: 239.463 ms, Bubble Time: 55.005 ms, Imbalance Overhead: 22.822 ms
GPU 2, Compute+Comm Time: 239.463 ms, Bubble Time: 54.495 ms, Imbalance Overhead: 23.331 ms
GPU 3, Compute+Comm Time: 239.463 ms, Bubble Time: 54.580 ms, Imbalance Overhead: 23.246 ms
GPU 4, Compute+Comm Time: 239.463 ms, Bubble Time: 54.773 ms, Imbalance Overhead: 23.054 ms
GPU 5, Compute+Comm Time: 239.463 ms, Bubble Time: 55.043 ms, Imbalance Overhead: 22.783 ms
GPU 6, Compute+Comm Time: 239.463 ms, Bubble Time: 55.338 ms, Imbalance Overhead: 22.489 ms
GPU 7, Compute+Comm Time: 257.542 ms, Bubble Time: 55.951 ms, Imbalance Overhead: 3.797 ms
The estimated cost of the whole pipeline: 505.748 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 702.613 ms
Partition 0 [0, 9) has cost: 702.613 ms
Partition 1 [9, 17) has cost: 677.528 ms
Partition 2 [17, 25) has cost: 677.528 ms
Partition 3 [25, 33) has cost: 683.185 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.753 ms
GPU 0, Compute+Comm Time: 141.588 ms, Bubble Time: 26.805 ms, Imbalance Overhead: 2.360 ms
GPU 1, Compute+Comm Time: 137.793 ms, Bubble Time: 26.352 ms, Imbalance Overhead: 6.609 ms
GPU 2, Compute+Comm Time: 137.793 ms, Bubble Time: 26.060 ms, Imbalance Overhead: 6.900 ms
GPU 3, Compute+Comm Time: 138.507 ms, Bubble Time: 25.590 ms, Imbalance Overhead: 6.656 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 316.076 ms
GPU 0, Compute+Comm Time: 255.634 ms, Bubble Time: 47.727 ms, Imbalance Overhead: 12.715 ms
GPU 1, Compute+Comm Time: 253.457 ms, Bubble Time: 48.289 ms, Imbalance Overhead: 14.330 ms
GPU 2, Compute+Comm Time: 253.457 ms, Bubble Time: 48.843 ms, Imbalance Overhead: 13.776 ms
GPU 3, Compute+Comm Time: 263.456 ms, Bubble Time: 49.794 ms, Imbalance Overhead: 2.825 ms
    The estimated cost with 2 DP ways is 511.171 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1380.141 ms
Partition 0 [0, 17) has cost: 1380.141 ms
Partition 1 [17, 33) has cost: 1360.713 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 218.817 ms
GPU 0, Compute+Comm Time: 190.573 ms, Bubble Time: 23.488 ms, Imbalance Overhead: 4.755 ms
GPU 1, Compute+Comm Time: 188.914 ms, Bubble Time: 24.162 ms, Imbalance Overhead: 5.741 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 355.369 ms
GPU 0, Compute+Comm Time: 308.044 ms, Bubble Time: 39.432 ms, Imbalance Overhead: 7.893 ms
GPU 1, Compute+Comm Time: 312.546 ms, Bubble Time: 38.300 ms, Imbalance Overhead: 4.524 ms
    The estimated cost with 4 DP ways is 602.895 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2740.853 ms
Partition 0 [0, 33) has cost: 2740.853 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 560.252 ms
GPU 0, Compute+Comm Time: 560.252 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 683.229 ms
GPU 0, Compute+Comm Time: 683.229 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1305.654 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7714	TrainAcc 0.0846	ValidAcc 0.0762	TestAcc 0.0769	BestValid 0.0762
	Epoch 50:	Loss 1.9038	TrainAcc 0.6701	ValidAcc 0.6851	TestAcc 0.6800	BestValid 0.6851
	Epoch 100:	Loss 1.2893	TrainAcc 0.7909	ValidAcc 0.8035	TestAcc 0.7978	BestValid 0.8035
	Epoch 150:	Loss 1.0373	TrainAcc 0.8392	ValidAcc 0.8483	TestAcc 0.8427	BestValid 0.8483
	Epoch 200:	Loss 0.9079	TrainAcc 0.8605	ValidAcc 0.8684	TestAcc 0.8626	BestValid 0.8684
	Epoch 250:	Loss 0.8295	TrainAcc 0.8774	ValidAcc 0.8840	TestAcc 0.8790	BestValid 0.8840
	Epoch 300:	Loss 0.7701	TrainAcc 0.8857	ValidAcc 0.8911	TestAcc 0.8863	BestValid 0.8911
	Epoch 350:	Loss 0.7283	TrainAcc 0.8916	ValidAcc 0.8961	TestAcc 0.8919	BestValid 0.8961
	Epoch 400:	Loss 0.6933	TrainAcc 0.8978	ValidAcc 0.9007	TestAcc 0.8973	BestValid 0.9007
	Epoch 450:	Loss 0.6712	TrainAcc 0.9025	ValidAcc 0.9057	TestAcc 0.9012	BestValid 0.9057
	Epoch 500:	Loss 0.6511	TrainAcc 0.9064	ValidAcc 0.9095	TestAcc 0.9046	BestValid 0.9095
	Epoch 550:	Loss 0.6337	TrainAcc 0.9093	ValidAcc 0.9118	TestAcc 0.9077	BestValid 0.9118
	Epoch 600:	Loss 0.6106	TrainAcc 0.9127	ValidAcc 0.9147	TestAcc 0.9099	BestValid 0.9147
	Epoch 650:	Loss 0.5955	TrainAcc 0.9142	ValidAcc 0.9160	TestAcc 0.9123	BestValid 0.9160
	Epoch 700:	Loss 0.5846	TrainAcc 0.9172	ValidAcc 0.9186	TestAcc 0.9150	BestValid 0.9186
	Epoch 750:	Loss 0.5741	TrainAcc 0.9188	ValidAcc 0.9202	TestAcc 0.9166	BestValid 0.9202
	Epoch 800:	Loss 0.5696	TrainAcc 0.9198	ValidAcc 0.9205	TestAcc 0.9178	BestValid 0.9205
	Epoch 850:	Loss 0.5526	TrainAcc 0.9222	ValidAcc 0.9224	TestAcc 0.9198	BestValid 0.9224
	Epoch 900:	Loss 0.5456	TrainAcc 0.9234	ValidAcc 0.9236	TestAcc 0.9213	BestValid 0.9236
	Epoch 950:	Loss 0.5345	TrainAcc 0.9252	ValidAcc 0.9250	TestAcc 0.9224	BestValid 0.9250
	Epoch 1000:	Loss 0.5328	TrainAcc 0.9261	ValidAcc 0.9254	TestAcc 0.9231	BestValid 0.9254
	Epoch 1050:	Loss 0.5258	TrainAcc 0.9277	ValidAcc 0.9268	TestAcc 0.9244	BestValid 0.9268
	Epoch 1100:	Loss 0.5185	TrainAcc 0.9286	ValidAcc 0.9268	TestAcc 0.9249	BestValid 0.9268
	Epoch 1150:	Loss 0.5107	TrainAcc 0.9301	ValidAcc 0.9285	TestAcc 0.9263	BestValid 0.9285
	Epoch 1200:	Loss 0.5076	TrainAcc 0.9307	ValidAcc 0.9295	TestAcc 0.9266	BestValid 0.9295
	Epoch 1250:	Loss 0.5014	TrainAcc 0.9318	ValidAcc 0.9302	TestAcc 0.9279	BestValid 0.9302
	Epoch 1300:	Loss 0.4939	TrainAcc 0.9322	ValidAcc 0.9308	TestAcc 0.9282	BestValid 0.9308
	Epoch 1350:	Loss 0.4850	TrainAcc 0.9331	ValidAcc 0.9317	TestAcc 0.9289	BestValid 0.9317
	Epoch 1400:	Loss 0.4796	TrainAcc 0.9341	ValidAcc 0.9324	TestAcc 0.9302	BestValid 0.9324
	Epoch 1450:	Loss 0.4823	TrainAcc 0.9344	ValidAcc 0.9327	TestAcc 0.9304	BestValid 0.9327
	Epoch 1500:	Loss 0.4740	TrainAcc 0.9355	ValidAcc 0.9335	TestAcc 0.9310	BestValid 0.9335
	Epoch 1550:	Loss 0.4682	TrainAcc 0.9367	ValidAcc 0.9346	TestAcc 0.9320	BestValid 0.9346
	Epoch 1600:	Loss 0.4637	TrainAcc 0.9365	ValidAcc 0.9342	TestAcc 0.9321	BestValid 0.9346
	Epoch 1650:	Loss 0.4621	TrainAcc 0.9369	ValidAcc 0.9343	TestAcc 0.9323	BestValid 0.9346
	Epoch 1700:	Loss 0.4597	TrainAcc 0.9370	ValidAcc 0.9345	TestAcc 0.9324	BestValid 0.9346
	Epoch 1750:	Loss 0.4571	TrainAcc 0.9379	ValidAcc 0.9355	TestAcc 0.9332	BestValid 0.9355
	Epoch 1800:	Loss 0.4527	TrainAcc 0.9389	ValidAcc 0.9359	TestAcc 0.9337	BestValid 0.9359
	Epoch 1850:	Loss 0.4467	TrainAcc 0.9394	ValidAcc 0.9364	TestAcc 0.9345	BestValid 0.9364
	Epoch 1900:	Loss 0.4461	TrainAcc 0.9396	ValidAcc 0.9367	TestAcc 0.9346	BestValid 0.9367
	Epoch 1950:	Loss 0.4423	TrainAcc 0.9405	ValidAcc 0.9371	TestAcc 0.9353	BestValid 0.9371
	Epoch 2000:	Loss 0.4404	TrainAcc 0.9407	ValidAcc 0.9375	TestAcc 0.9355	BestValid 0.9375
	Epoch 2050:	Loss 0.4381	TrainAcc 0.9412	ValidAcc 0.9380	TestAcc 0.9354	BestValid 0.9380
	Epoch 2100:	Loss 0.4377	TrainAcc 0.9412	ValidAcc 0.9382	TestAcc 0.9356	BestValid 0.9382
	Epoch 2150:	Loss 0.4318	TrainAcc 0.9413	ValidAcc 0.9379	TestAcc 0.9355	BestValid 0.9382
	Epoch 2200:	Loss 0.4342	TrainAcc 0.9422	ValidAcc 0.9391	TestAcc 0.9371	BestValid 0.9391
	Epoch 2250:	Loss 0.4272	TrainAcc 0.9429	ValidAcc 0.9398	TestAcc 0.9372	BestValid 0.9398
	Epoch 2300:	Loss 0.4256	TrainAcc 0.9430	ValidAcc 0.9399	TestAcc 0.9373	BestValid 0.9399
	Epoch 2350:	Loss 0.4204	TrainAcc 0.9437	ValidAcc 0.9406	TestAcc 0.9380	BestValid 0.9406
	Epoch 2400:	Loss 0.4193	TrainAcc 0.9442	ValidAcc 0.9407	TestAcc 0.9387	BestValid 0.9407
	Epoch 2450:	Loss 0.4145	TrainAcc 0.9444	ValidAcc 0.9410	TestAcc 0.9385	BestValid 0.9410
	Epoch 2500:	Loss 0.4127	TrainAcc 0.9451	ValidAcc 0.9414	TestAcc 0.9391	BestValid 0.9414
	Epoch 2550:	Loss 0.4118	TrainAcc 0.9449	ValidAcc 0.9418	TestAcc 0.9390	BestValid 0.9418
	Epoch 2600:	Loss 0.4086	TrainAcc 0.9457	ValidAcc 0.9423	TestAcc 0.9400	BestValid 0.9423
	Epoch 2650:	Loss 0.4077	TrainAcc 0.9455	ValidAcc 0.9426	TestAcc 0.9397	BestValid 0.9426
	Epoch 2700:	Loss 0.4043	TrainAcc 0.9460	ValidAcc 0.9428	TestAcc 0.9403	BestValid 0.9428
	Epoch 2750:	Loss 0.4042	TrainAcc 0.9455	ValidAcc 0.9428	TestAcc 0.9397	BestValid 0.9428
	Epoch 2800:	Loss 0.4029	TrainAcc 0.9461	ValidAcc 0.9429	TestAcc 0.9403	BestValid 0.9429
	Epoch 2850:	Loss 0.4013	TrainAcc 0.9466	ValidAcc 0.9437	TestAcc 0.9408	BestValid 0.9437
	Epoch 2900:	Loss 0.4012	TrainAcc 0.9465	ValidAcc 0.9431	TestAcc 0.9404	BestValid 0.9437
	Epoch 2950:	Loss 0.3990	TrainAcc 0.9478	ValidAcc 0.9442	TestAcc 0.9416	BestValid 0.9442
	Epoch 3000:	Loss 0.3951	TrainAcc 0.9470	ValidAcc 0.9439	TestAcc 0.9407	BestValid 0.9442
	Epoch 3050:	Loss 0.3952	TrainAcc 0.9433	ValidAcc 0.9407	TestAcc 0.9371	BestValid 0.9442
	Epoch 3100:	Loss 0.3907	TrainAcc 0.9475	ValidAcc 0.9444	TestAcc 0.9410	BestValid 0.9444
	Epoch 3150:	Loss 0.3866	TrainAcc 0.9472	ValidAcc 0.9440	TestAcc 0.9411	BestValid 0.9444
	Epoch 3200:	Loss 0.3894	TrainAcc 0.9478	ValidAcc 0.9448	TestAcc 0.9414	BestValid 0.9448
	Epoch 3250:	Loss 0.3857	TrainAcc 0.9478	ValidAcc 0.9447	TestAcc 0.9415	BestValid 0.9448
	Epoch 3300:	Loss 0.3817	TrainAcc 0.9495	ValidAcc 0.9456	TestAcc 0.9431	BestValid 0.9456
	Epoch 3350:	Loss 0.3836	TrainAcc 0.9480	ValidAcc 0.9443	TestAcc 0.9412	BestValid 0.9456
	Epoch 3400:	Loss 0.3840	TrainAcc 0.9494	ValidAcc 0.9452	TestAcc 0.9430	BestValid 0.9456
	Epoch 3450:	Loss 0.3814	TrainAcc 0.9476	ValidAcc 0.9441	TestAcc 0.9409	BestValid 0.9456
	Epoch 3500:	Loss 0.3778	TrainAcc 0.9488	ValidAcc 0.9446	TestAcc 0.9421	BestValid 0.9456
	Epoch 3550:	Loss 0.3788	TrainAcc 0.9493	ValidAcc 0.9455	TestAcc 0.9428	BestValid 0.9456
	Epoch 3600:	Loss 0.3765	TrainAcc 0.9483	ValidAcc 0.9444	TestAcc 0.9417	BestValid 0.9456
	Epoch 3650:	Loss 0.3792	TrainAcc 0.9498	ValidAcc 0.9453	TestAcc 0.9430	BestValid 0.9456
	Epoch 3700:	Loss 0.3772	TrainAcc 0.9488	ValidAcc 0.9451	TestAcc 0.9423	BestValid 0.9456
	Epoch 3750:	Loss 0.3719	TrainAcc 0.9491	ValidAcc 0.9447	TestAcc 0.9420	BestValid 0.9456
	Epoch 3800:	Loss 0.3718	TrainAcc 0.9477	ValidAcc 0.9439	TestAcc 0.9407	BestValid 0.9456
	Epoch 3850:	Loss 0.3676	TrainAcc 0.9510	ValidAcc 0.9469	TestAcc 0.9443	BestValid 0.9469
	Epoch 3900:	Loss 0.3710	TrainAcc 0.9516	ValidAcc 0.9474	TestAcc 0.9452	BestValid 0.9474
	Epoch 3950:	Loss 0.3627	TrainAcc 0.9500	ValidAcc 0.9457	TestAcc 0.9429	BestValid 0.9474
	Epoch 4000:	Loss 0.3664	TrainAcc 0.9504	ValidAcc 0.9464	TestAcc 0.9433	BestValid 0.9474
	Epoch 4050:	Loss 0.3619	TrainAcc 0.9492	ValidAcc 0.9451	TestAcc 0.9422	BestValid 0.9474
	Epoch 4100:	Loss 0.3645	TrainAcc 0.9504	ValidAcc 0.9463	TestAcc 0.9433	BestValid 0.9474
	Epoch 4150:	Loss 0.3624	TrainAcc 0.9524	ValidAcc 0.9479	TestAcc 0.9457	BestValid 0.9479
	Epoch 4200:	Loss 0.3585	TrainAcc 0.9515	ValidAcc 0.9469	TestAcc 0.9451	BestValid 0.9479
	Epoch 4250:	Loss 0.3595	TrainAcc 0.9518	ValidAcc 0.9475	TestAcc 0.9451	BestValid 0.9479
	Epoch 4300:	Loss 0.3584	TrainAcc 0.9504	ValidAcc 0.9461	TestAcc 0.9434	BestValid 0.9479
	Epoch 4350:	Loss 0.3564	TrainAcc 0.9513	ValidAcc 0.9465	TestAcc 0.9445	BestValid 0.9479
	Epoch 4400:	Loss 0.3563	TrainAcc 0.9522	ValidAcc 0.9475	TestAcc 0.9456	BestValid 0.9479
	Epoch 4450:	Loss 0.3530	TrainAcc 0.9526	ValidAcc 0.9477	TestAcc 0.9462	BestValid 0.9479
	Epoch 4500:	Loss 0.3512	TrainAcc 0.9520	ValidAcc 0.9473	TestAcc 0.9454	BestValid 0.9479
	Epoch 4550:	Loss 0.3564	TrainAcc 0.9524	ValidAcc 0.9477	TestAcc 0.9454	BestValid 0.9479
	Epoch 4600:	Loss 0.3518	TrainAcc 0.9528	ValidAcc 0.9479	TestAcc 0.9457	BestValid 0.9479
	Epoch 4650:	Loss 0.3520	TrainAcc 0.9524	ValidAcc 0.9471	TestAcc 0.9457	BestValid 0.9479
	Epoch 4700:	Loss 0.3498	TrainAcc 0.9508	ValidAcc 0.9465	TestAcc 0.9440	BestValid 0.9479
	Epoch 4750:	Loss 0.3484	TrainAcc 0.9526	ValidAcc 0.9472	TestAcc 0.9454	BestValid 0.9479
	Epoch 4800:	Loss 0.3461	TrainAcc 0.9517	ValidAcc 0.9468	TestAcc 0.9444	BestValid 0.9479
	Epoch 4850:	Loss 0.3435	TrainAcc 0.9502	ValidAcc 0.9451	TestAcc 0.9436	BestValid 0.9479
	Epoch 4900:	Loss 0.3439	TrainAcc 0.9510	ValidAcc 0.9457	TestAcc 0.9435	BestValid 0.9479
	Epoch 4950:	Loss 0.3435	TrainAcc 0.9524	ValidAcc 0.9471	TestAcc 0.9454	BestValid 0.9479
	Epoch 5000:	Loss 0.3468	TrainAcc 0.9550	ValidAcc 0.9498	TestAcc 0.9489	BestValid 0.9498
****** Epoch Time (Excluding Evaluation Cost): 0.414 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 80.095 ms (Max: 82.549, Min: 66.415, Sum: 640.763)
Cluster-Wide Average, Compute: 262.183 ms (Max: 285.869, Min: 253.295, Sum: 2097.464)
Cluster-Wide Average, Communication-Layer: 41.694 ms (Max: 47.392, Min: 31.935, Sum: 333.555)
Cluster-Wide Average, Bubble-Imbalance: 26.938 ms (Max: 32.803, Min: 12.828, Sum: 215.500)
Cluster-Wide Average, Communication-Graph: 0.442 ms (Max: 0.486, Min: 0.409, Sum: 3.538)
Cluster-Wide Average, Optimization: 0.099 ms (Max: 0.121, Min: 0.091, Sum: 0.795)
Cluster-Wide Average, Others: 3.730 ms (Max: 17.393, Min: 1.772, Sum: 29.837)
****** Breakdown Sum: 415.181 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.304 GB (Max: 8.059, Min: 6.018, Sum: 50.433)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 61.590 Gbps (Max: 73.688, Min: 45.288, Sum: 492.716)
Layer-level communication (cluster-wide, per-epoch): 2.430 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 2.430 GB
****** Accuracy Results ******
Highest valid_acc: 0.9498
Target test_acc: 0.9489
Epoch to reach the target acc: 4999
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
