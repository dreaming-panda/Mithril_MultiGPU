Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.168 seconds.
Building the CSC structure...
        It takes 2.366 seconds.
Building the CSC structure...
        It takes 2.388 seconds.
Building the CSC structure...
        It takes 2.433 seconds.
Building the CSC structure...
        It takes 2.454 seconds.
Building the CSC structure...
        It takes 2.468 seconds.
Building the CSC structure...
        It takes 2.559 seconds.
Building the CSC structure...
        It takes 2.561 seconds.
Building the CSC structure...
        It takes 1.853 seconds.
        It takes 2.275 seconds.
        It takes 2.338 seconds.
        It takes 2.308 seconds.
        It takes 2.345 seconds.
        It takes 2.349 seconds.
        It takes 2.315 seconds.
        It takes 2.348 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.310 seconds.
Building the Label Vector...
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.287 seconds.
Building the Label Vector...
        It takes 0.043 seconds.
        It takes 0.035 seconds.
        It takes 0.045 seconds.
Building the Feature Vector...
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.253 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.031 seconds.
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.938 Gbps (per GPU), 463.505 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.655 Gbps (per GPU), 461.242 Gbps (aggregated)
The layer-level communication performance: 57.653 Gbps (per GPU), 461.222 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.414 Gbps (per GPU), 459.309 Gbps (aggregated)
The layer-level communication performance: 57.379 Gbps (per GPU), 459.035 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.194 Gbps (per GPU), 457.550 Gbps (aggregated)
The layer-level communication performance: 57.154 Gbps (per GPU), 457.234 Gbps (aggregated)
The layer-level communication performance: 57.119 Gbps (per GPU), 456.953 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.554 Gbps (per GPU), 1252.428 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.518 Gbps (per GPU), 1252.148 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.545 Gbps (per GPU), 1252.358 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.469 Gbps (per GPU), 1251.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.432 Gbps (per GPU), 1251.453 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.518 Gbps (per GPU), 1252.148 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.396 Gbps (per GPU), 1251.167 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.522 Gbps (per GPU), 1252.174 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 96.992 Gbps (per GPU), 775.933 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.992 Gbps (per GPU), 775.939 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.992 Gbps (per GPU), 775.939 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.992 Gbps (per GPU), 775.933 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.001 Gbps (per GPU), 776.010 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.993 Gbps (per GPU), 775.944 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.994 Gbps (per GPU), 775.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.992 Gbps (per GPU), 775.933 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.124 Gbps (per GPU), 280.988 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.985 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.986 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.982 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.124 Gbps (per GPU), 280.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.987 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.986 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.42ms  2.72ms  3.03  8.38K  3.53M
 chk_1  0.76ms  2.78ms  2.92ms  3.84  6.74K  3.60M
 chk_2  0.80ms  2.62ms  2.81ms  3.52  7.27K  3.53M
 chk_3  0.81ms  2.66ms  2.84ms  3.51  7.92K  3.61M
 chk_4  0.63ms  2.58ms  2.74ms  4.33  5.33K  3.68M
 chk_5  1.01ms  2.59ms  2.77ms  2.75 10.07K  3.45M
 chk_6  0.96ms  2.74ms  2.93ms  3.04  9.41K  3.48M
 chk_7  0.82ms  2.58ms  2.75ms  3.34  8.12K  3.60M
 chk_8  0.68ms  2.69ms  2.85ms  4.17  6.09K  3.64M
 chk_9  1.10ms  2.53ms  2.70ms  2.45 11.10K  3.38M
chk_10  0.66ms  2.74ms  2.86ms  4.36  5.67K  3.63M
chk_11  0.83ms  2.60ms  2.76ms  3.34  8.16K  3.54M
chk_12  0.80ms  2.79ms  2.96ms  3.71  7.24K  3.55M
chk_13  0.64ms  2.63ms  2.79ms  4.35  5.41K  3.68M
chk_14  0.78ms  2.88ms  3.04ms  3.88  7.14K  3.53M
chk_15  0.96ms  2.72ms  2.91ms  3.04  9.25K  3.49M
chk_16  0.60ms  2.56ms  2.70ms  4.51  4.78K  3.77M
chk_17  0.77ms  2.67ms  2.84ms  3.71  6.85K  3.60M
chk_18  0.81ms  2.50ms  2.66ms  3.27  7.47K  3.57M
chk_19  0.61ms  2.55ms  2.70ms  4.43  4.88K  3.75M
chk_20  0.77ms  2.58ms  2.72ms  3.51  7.00K  3.63M
chk_21  0.64ms  2.55ms  2.71ms  4.24  5.41K  3.68M
chk_22  1.10ms  2.77ms  2.97ms  2.70 11.07K  3.39M
chk_23  0.80ms  2.65ms  2.82ms  3.53  7.23K  3.64M
chk_24  1.01ms  2.68ms  2.88ms  2.84 10.13K  3.43M
chk_25  0.73ms  2.53ms  2.69ms  3.68  6.40K  3.57M
chk_26  0.66ms  2.75ms  2.90ms  4.37  5.78K  3.55M
chk_27  0.96ms  2.61ms  2.81ms  2.93  9.34K  3.48M
chk_28  0.73ms  2.92ms  3.08ms  4.22  6.37K  3.57M
chk_29  0.63ms  2.72ms  2.86ms  4.50  5.16K  3.78M
chk_30  0.64ms  2.60ms  2.76ms  4.32  5.44K  3.67M
chk_31  0.73ms  2.74ms  2.91ms  4.01  6.33K  3.63M
   Avg  0.79  2.65  2.82
   Max  1.10  2.92  3.08
   Min  0.60  2.42  2.66
 Ratio  1.83  1.21  1.16
   Var  0.02  0.01  0.01
Profiling takes 2.411 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 365.106 ms
Partition 0 [0, 5) has cost: 365.106 ms
Partition 1 [5, 9) has cost: 339.775 ms
Partition 2 [9, 13) has cost: 339.775 ms
Partition 3 [13, 17) has cost: 339.775 ms
Partition 4 [17, 21) has cost: 339.775 ms
Partition 5 [21, 25) has cost: 339.775 ms
Partition 6 [25, 29) has cost: 339.775 ms
Partition 7 [29, 33) has cost: 345.195 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.911 ms
GPU 0, Compute+Comm Time: 132.772 ms, Bubble Time: 29.005 ms, Imbalance Overhead: 3.134 ms
GPU 1, Compute+Comm Time: 125.672 ms, Bubble Time: 28.699 ms, Imbalance Overhead: 10.540 ms
GPU 2, Compute+Comm Time: 125.672 ms, Bubble Time: 28.748 ms, Imbalance Overhead: 10.490 ms
GPU 3, Compute+Comm Time: 125.672 ms, Bubble Time: 28.612 ms, Imbalance Overhead: 10.627 ms
GPU 4, Compute+Comm Time: 125.672 ms, Bubble Time: 28.522 ms, Imbalance Overhead: 10.717 ms
GPU 5, Compute+Comm Time: 125.672 ms, Bubble Time: 28.593 ms, Imbalance Overhead: 10.645 ms
GPU 6, Compute+Comm Time: 125.672 ms, Bubble Time: 28.847 ms, Imbalance Overhead: 10.391 ms
GPU 7, Compute+Comm Time: 126.834 ms, Bubble Time: 29.209 ms, Imbalance Overhead: 8.867 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 318.072 ms
GPU 0, Compute+Comm Time: 244.095 ms, Bubble Time: 56.627 ms, Imbalance Overhead: 17.351 ms
GPU 1, Compute+Comm Time: 239.837 ms, Bubble Time: 55.861 ms, Imbalance Overhead: 22.375 ms
GPU 2, Compute+Comm Time: 239.837 ms, Bubble Time: 55.287 ms, Imbalance Overhead: 22.949 ms
GPU 3, Compute+Comm Time: 239.837 ms, Bubble Time: 55.168 ms, Imbalance Overhead: 23.068 ms
GPU 4, Compute+Comm Time: 239.837 ms, Bubble Time: 55.300 ms, Imbalance Overhead: 22.935 ms
GPU 5, Compute+Comm Time: 239.837 ms, Bubble Time: 55.471 ms, Imbalance Overhead: 22.764 ms
GPU 6, Compute+Comm Time: 239.837 ms, Bubble Time: 55.294 ms, Imbalance Overhead: 22.942 ms
GPU 7, Compute+Comm Time: 258.067 ms, Bubble Time: 55.876 ms, Imbalance Overhead: 4.129 ms
The estimated cost of the whole pipeline: 507.132 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 704.881 ms
Partition 0 [0, 9) has cost: 704.881 ms
Partition 1 [9, 17) has cost: 679.550 ms
Partition 2 [17, 25) has cost: 679.550 ms
Partition 3 [25, 33) has cost: 684.970 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 171.522 ms
GPU 0, Compute+Comm Time: 141.879 ms, Bubble Time: 26.823 ms, Imbalance Overhead: 2.820 ms
GPU 1, Compute+Comm Time: 138.034 ms, Bubble Time: 26.407 ms, Imbalance Overhead: 7.081 ms
GPU 2, Compute+Comm Time: 138.034 ms, Bubble Time: 26.375 ms, Imbalance Overhead: 7.113 ms
GPU 3, Compute+Comm Time: 138.606 ms, Bubble Time: 26.131 ms, Imbalance Overhead: 6.785 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 317.148 ms
GPU 0, Compute+Comm Time: 256.013 ms, Bubble Time: 48.492 ms, Imbalance Overhead: 12.643 ms
GPU 1, Compute+Comm Time: 253.838 ms, Bubble Time: 48.745 ms, Imbalance Overhead: 14.565 ms
GPU 2, Compute+Comm Time: 253.838 ms, Bubble Time: 48.866 ms, Imbalance Overhead: 14.444 ms
GPU 3, Compute+Comm Time: 263.905 ms, Bubble Time: 49.892 ms, Imbalance Overhead: 3.352 ms
    The estimated cost with 2 DP ways is 513.103 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1384.431 ms
Partition 0 [0, 17) has cost: 1384.431 ms
Partition 1 [17, 33) has cost: 1364.520 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 221.704 ms
GPU 0, Compute+Comm Time: 192.711 ms, Bubble Time: 23.657 ms, Imbalance Overhead: 5.336 ms
GPU 1, Compute+Comm Time: 190.865 ms, Bubble Time: 24.220 ms, Imbalance Overhead: 6.620 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 359.014 ms
GPU 0, Compute+Comm Time: 310.440 ms, Bubble Time: 39.211 ms, Imbalance Overhead: 9.363 ms
GPU 1, Compute+Comm Time: 315.036 ms, Bubble Time: 38.444 ms, Imbalance Overhead: 5.534 ms
    The estimated cost with 4 DP ways is 609.755 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2748.950 ms
Partition 0 [0, 33) has cost: 2748.950 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 546.676 ms
GPU 0, Compute+Comm Time: 546.676 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 670.494 ms
GPU 0, Compute+Comm Time: 670.494 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1278.028 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.8095	TrainAcc 0.0079	ValidAcc 0.0073	TestAcc 0.0081	BestValid 0.0073
	Epoch 50:	Loss 1.8990	TrainAcc 0.6708	ValidAcc 0.6890	TestAcc 0.6828	BestValid 0.6890
	Epoch 100:	Loss 1.3085	TrainAcc 0.7850	ValidAcc 0.7981	TestAcc 0.7928	BestValid 0.7981
	Epoch 150:	Loss 1.0540	TrainAcc 0.8384	ValidAcc 0.8460	TestAcc 0.8416	BestValid 0.8460
	Epoch 200:	Loss 0.9198	TrainAcc 0.8605	ValidAcc 0.8681	TestAcc 0.8624	BestValid 0.8681
	Epoch 250:	Loss 0.8377	TrainAcc 0.8730	ValidAcc 0.8803	TestAcc 0.8736	BestValid 0.8803
	Epoch 300:	Loss 0.7787	TrainAcc 0.8810	ValidAcc 0.8863	TestAcc 0.8819	BestValid 0.8863
	Epoch 350:	Loss 0.7408	TrainAcc 0.8890	ValidAcc 0.8930	TestAcc 0.8892	BestValid 0.8930
	Epoch 400:	Loss 0.7025	TrainAcc 0.8950	ValidAcc 0.8979	TestAcc 0.8955	BestValid 0.8979
	Epoch 450:	Loss 0.6768	TrainAcc 0.8995	ValidAcc 0.9027	TestAcc 0.8993	BestValid 0.9027
	Epoch 500:	Loss 0.6619	TrainAcc 0.9039	ValidAcc 0.9061	TestAcc 0.9027	BestValid 0.9061
	Epoch 550:	Loss 0.6312	TrainAcc 0.9073	ValidAcc 0.9087	TestAcc 0.9051	BestValid 0.9087
	Epoch 600:	Loss 0.6203	TrainAcc 0.9117	ValidAcc 0.9121	TestAcc 0.9084	BestValid 0.9121
	Epoch 650:	Loss 0.6030	TrainAcc 0.9128	ValidAcc 0.9136	TestAcc 0.9100	BestValid 0.9136
	Epoch 700:	Loss 0.5939	TrainAcc 0.9160	ValidAcc 0.9157	TestAcc 0.9120	BestValid 0.9157
	Epoch 750:	Loss 0.5804	TrainAcc 0.9179	ValidAcc 0.9181	TestAcc 0.9139	BestValid 0.9181
	Epoch 800:	Loss 0.5703	TrainAcc 0.9203	ValidAcc 0.9195	TestAcc 0.9159	BestValid 0.9195
	Epoch 850:	Loss 0.5573	TrainAcc 0.9214	ValidAcc 0.9199	TestAcc 0.9167	BestValid 0.9199
	Epoch 900:	Loss 0.5524	TrainAcc 0.9235	ValidAcc 0.9217	TestAcc 0.9185	BestValid 0.9217
	Epoch 950:	Loss 0.5419	TrainAcc 0.9247	ValidAcc 0.9230	TestAcc 0.9199	BestValid 0.9230
	Epoch 1000:	Loss 0.5338	TrainAcc 0.9257	ValidAcc 0.9239	TestAcc 0.9207	BestValid 0.9239
	Epoch 1050:	Loss 0.5295	TrainAcc 0.9271	ValidAcc 0.9250	TestAcc 0.9224	BestValid 0.9250
	Epoch 1100:	Loss 0.5223	TrainAcc 0.9273	ValidAcc 0.9247	TestAcc 0.9226	BestValid 0.9250
	Epoch 1150:	Loss 0.5118	TrainAcc 0.9292	ValidAcc 0.9267	TestAcc 0.9244	BestValid 0.9267
	Epoch 1200:	Loss 0.5087	TrainAcc 0.9295	ValidAcc 0.9269	TestAcc 0.9248	BestValid 0.9269
	Epoch 1250:	Loss 0.5044	TrainAcc 0.9304	ValidAcc 0.9275	TestAcc 0.9257	BestValid 0.9275
	Epoch 1300:	Loss 0.5019	TrainAcc 0.9309	ValidAcc 0.9280	TestAcc 0.9262	BestValid 0.9280
	Epoch 1350:	Loss 0.4980	TrainAcc 0.9321	ValidAcc 0.9297	TestAcc 0.9271	BestValid 0.9297
	Epoch 1400:	Loss 0.4925	TrainAcc 0.9330	ValidAcc 0.9300	TestAcc 0.9277	BestValid 0.9300
	Epoch 1450:	Loss 0.4842	TrainAcc 0.9341	ValidAcc 0.9316	TestAcc 0.9292	BestValid 0.9316
	Epoch 1500:	Loss 0.4808	TrainAcc 0.9348	ValidAcc 0.9321	TestAcc 0.9296	BestValid 0.9321
	Epoch 1550:	Loss 0.4735	TrainAcc 0.9356	ValidAcc 0.9329	TestAcc 0.9307	BestValid 0.9329
	Epoch 1600:	Loss 0.4727	TrainAcc 0.9361	ValidAcc 0.9331	TestAcc 0.9308	BestValid 0.9331
	Epoch 1650:	Loss 0.4717	TrainAcc 0.9365	ValidAcc 0.9337	TestAcc 0.9314	BestValid 0.9337
	Epoch 1700:	Loss 0.4640	TrainAcc 0.9366	ValidAcc 0.9340	TestAcc 0.9314	BestValid 0.9340
	Epoch 1750:	Loss 0.4605	TrainAcc 0.9371	ValidAcc 0.9345	TestAcc 0.9317	BestValid 0.9345
	Epoch 1800:	Loss 0.4601	TrainAcc 0.9381	ValidAcc 0.9353	TestAcc 0.9328	BestValid 0.9353
	Epoch 1850:	Loss 0.4577	TrainAcc 0.9387	ValidAcc 0.9363	TestAcc 0.9337	BestValid 0.9363
	Epoch 1900:	Loss 0.4495	TrainAcc 0.9390	ValidAcc 0.9360	TestAcc 0.9335	BestValid 0.9363
	Epoch 1950:	Loss 0.4464	TrainAcc 0.9392	ValidAcc 0.9368	TestAcc 0.9344	BestValid 0.9368
	Epoch 2000:	Loss 0.4470	TrainAcc 0.9392	ValidAcc 0.9364	TestAcc 0.9342	BestValid 0.9368
	Epoch 2050:	Loss 0.4453	TrainAcc 0.9402	ValidAcc 0.9371	TestAcc 0.9349	BestValid 0.9371
	Epoch 2100:	Loss 0.4394	TrainAcc 0.9405	ValidAcc 0.9375	TestAcc 0.9351	BestValid 0.9375
	Epoch 2150:	Loss 0.4363	TrainAcc 0.9405	ValidAcc 0.9377	TestAcc 0.9351	BestValid 0.9377
	Epoch 2200:	Loss 0.4344	TrainAcc 0.9401	ValidAcc 0.9368	TestAcc 0.9351	BestValid 0.9377
	Epoch 2250:	Loss 0.4330	TrainAcc 0.9417	ValidAcc 0.9386	TestAcc 0.9361	BestValid 0.9386
	Epoch 2300:	Loss 0.4300	TrainAcc 0.9423	ValidAcc 0.9398	TestAcc 0.9369	BestValid 0.9398
	Epoch 2350:	Loss 0.4309	TrainAcc 0.9417	ValidAcc 0.9388	TestAcc 0.9362	BestValid 0.9398
	Epoch 2400:	Loss 0.4233	TrainAcc 0.9427	ValidAcc 0.9395	TestAcc 0.9371	BestValid 0.9398
	Epoch 2450:	Loss 0.4219	TrainAcc 0.9420	ValidAcc 0.9385	TestAcc 0.9368	BestValid 0.9398
	Epoch 2500:	Loss 0.4178	TrainAcc 0.9434	ValidAcc 0.9405	TestAcc 0.9379	BestValid 0.9405
	Epoch 2550:	Loss 0.4176	TrainAcc 0.9440	ValidAcc 0.9412	TestAcc 0.9380	BestValid 0.9412
	Epoch 2600:	Loss 0.4171	TrainAcc 0.9442	ValidAcc 0.9415	TestAcc 0.9382	BestValid 0.9415
	Epoch 2650:	Loss 0.4173	TrainAcc 0.9433	ValidAcc 0.9402	TestAcc 0.9378	BestValid 0.9415
	Epoch 2700:	Loss 0.4111	TrainAcc 0.9451	ValidAcc 0.9417	TestAcc 0.9392	BestValid 0.9417
	Epoch 2750:	Loss 0.4071	TrainAcc 0.9456	ValidAcc 0.9429	TestAcc 0.9398	BestValid 0.9429
	Epoch 2800:	Loss 0.4066	TrainAcc 0.9438	ValidAcc 0.9402	TestAcc 0.9378	BestValid 0.9429
	Epoch 2850:	Loss 0.4071	TrainAcc 0.9449	ValidAcc 0.9412	TestAcc 0.9391	BestValid 0.9429
	Epoch 2900:	Loss 0.4037	TrainAcc 0.9449	ValidAcc 0.9413	TestAcc 0.9390	BestValid 0.9429
	Epoch 2950:	Loss 0.4035	TrainAcc 0.9462	ValidAcc 0.9424	TestAcc 0.9403	BestValid 0.9429
	Epoch 3000:	Loss 0.4002	TrainAcc 0.9465	ValidAcc 0.9430	TestAcc 0.9409	BestValid 0.9430
	Epoch 3050:	Loss 0.3988	TrainAcc 0.9470	ValidAcc 0.9438	TestAcc 0.9410	BestValid 0.9438
	Epoch 3100:	Loss 0.3970	TrainAcc 0.9470	ValidAcc 0.9431	TestAcc 0.9412	BestValid 0.9438
	Epoch 3150:	Loss 0.3948	TrainAcc 0.9466	ValidAcc 0.9431	TestAcc 0.9403	BestValid 0.9438
	Epoch 3200:	Loss 0.3957	TrainAcc 0.9469	ValidAcc 0.9430	TestAcc 0.9407	BestValid 0.9438
	Epoch 3250:	Loss 0.3959	TrainAcc 0.9471	ValidAcc 0.9433	TestAcc 0.9411	BestValid 0.9438
	Epoch 3300:	Loss 0.3909	TrainAcc 0.9470	ValidAcc 0.9427	TestAcc 0.9411	BestValid 0.9438
	Epoch 3350:	Loss 0.3836	TrainAcc 0.9468	ValidAcc 0.9421	TestAcc 0.9407	BestValid 0.9438
	Epoch 3400:	Loss 0.3835	TrainAcc 0.9465	ValidAcc 0.9416	TestAcc 0.9402	BestValid 0.9438
	Epoch 3450:	Loss 0.3850	TrainAcc 0.9473	ValidAcc 0.9426	TestAcc 0.9413	BestValid 0.9438
	Epoch 3500:	Loss 0.3825	TrainAcc 0.9463	ValidAcc 0.9417	TestAcc 0.9400	BestValid 0.9438
	Epoch 3550:	Loss 0.3762	TrainAcc 0.9483	ValidAcc 0.9440	TestAcc 0.9422	BestValid 0.9440
	Epoch 3600:	Loss 0.3790	TrainAcc 0.9489	ValidAcc 0.9442	TestAcc 0.9423	BestValid 0.9442
	Epoch 3650:	Loss 0.3785	TrainAcc 0.9474	ValidAcc 0.9428	TestAcc 0.9407	BestValid 0.9442
	Epoch 3700:	Loss 0.3837	TrainAcc 0.9480	ValidAcc 0.9425	TestAcc 0.9413	BestValid 0.9442
	Epoch 3750:	Loss 0.3785	TrainAcc 0.9471	ValidAcc 0.9420	TestAcc 0.9406	BestValid 0.9442
	Epoch 3800:	Loss 0.3739	TrainAcc 0.9489	ValidAcc 0.9443	TestAcc 0.9424	BestValid 0.9443
	Epoch 3850:	Loss 0.3726	TrainAcc 0.9476	ValidAcc 0.9428	TestAcc 0.9410	BestValid 0.9443
	Epoch 3900:	Loss 0.3730	TrainAcc 0.9500	ValidAcc 0.9464	TestAcc 0.9442	BestValid 0.9464
	Epoch 3950:	Loss 0.3727	TrainAcc 0.9504	ValidAcc 0.9461	TestAcc 0.9444	BestValid 0.9464
	Epoch 4000:	Loss 0.3684	TrainAcc 0.9466	ValidAcc 0.9414	TestAcc 0.9398	BestValid 0.9464
	Epoch 4050:	Loss 0.3636	TrainAcc 0.9486	ValidAcc 0.9434	TestAcc 0.9418	BestValid 0.9464
	Epoch 4100:	Loss 0.3639	TrainAcc 0.9504	ValidAcc 0.9455	TestAcc 0.9438	BestValid 0.9464
	Epoch 4150:	Loss 0.3665	TrainAcc 0.9511	ValidAcc 0.9460	TestAcc 0.9443	BestValid 0.9464
	Epoch 4200:	Loss 0.3644	TrainAcc 0.9516	ValidAcc 0.9468	TestAcc 0.9452	BestValid 0.9468
	Epoch 4250:	Loss 0.3638	TrainAcc 0.9510	ValidAcc 0.9460	TestAcc 0.9443	BestValid 0.9468
	Epoch 4300:	Loss 0.3634	TrainAcc 0.9513	ValidAcc 0.9465	TestAcc 0.9448	BestValid 0.9468
	Epoch 4350:	Loss 0.3626	TrainAcc 0.9510	ValidAcc 0.9463	TestAcc 0.9447	BestValid 0.9468
	Epoch 4400:	Loss 0.3601	TrainAcc 0.9496	ValidAcc 0.9440	TestAcc 0.9425	BestValid 0.9468
	Epoch 4450:	Loss 0.3635	TrainAcc 0.9522	ValidAcc 0.9472	TestAcc 0.9455	BestValid 0.9472
	Epoch 4500:	Loss 0.3581	TrainAcc 0.9519	ValidAcc 0.9462	TestAcc 0.9455	BestValid 0.9472
	Epoch 4550:	Loss 0.3582	TrainAcc 0.9510	ValidAcc 0.9458	TestAcc 0.9440	BestValid 0.9472
	Epoch 4600:	Loss 0.3564	TrainAcc 0.9515	ValidAcc 0.9462	TestAcc 0.9448	BestValid 0.9472
	Epoch 4650:	Loss 0.3524	TrainAcc 0.9520	ValidAcc 0.9466	TestAcc 0.9455	BestValid 0.9472
	Epoch 4700:	Loss 0.3514	TrainAcc 0.9532	ValidAcc 0.9478	TestAcc 0.9463	BestValid 0.9478
	Epoch 4750:	Loss 0.3495	TrainAcc 0.9522	ValidAcc 0.9466	TestAcc 0.9452	BestValid 0.9478
	Epoch 4800:	Loss 0.3487	TrainAcc 0.9524	ValidAcc 0.9461	TestAcc 0.9456	BestValid 0.9478
	Epoch 4850:	Loss 0.3566	TrainAcc 0.9527	ValidAcc 0.9464	TestAcc 0.9460	BestValid 0.9478
	Epoch 4900:	Loss 0.3549	TrainAcc 0.9520	ValidAcc 0.9467	TestAcc 0.9454	BestValid 0.9478
	Epoch 4950:	Loss 0.3462	TrainAcc 0.9525	ValidAcc 0.9469	TestAcc 0.9458	BestValid 0.9478
	Epoch 5000:	Loss 0.3482	TrainAcc 0.9535	ValidAcc 0.9485	TestAcc 0.9465	BestValid 0.9485
****** Epoch Time (Excluding Evaluation Cost): 0.415 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 80.172 ms (Max: 82.552, Min: 66.808, Sum: 641.374)
Cluster-Wide Average, Compute: 262.932 ms (Max: 285.440, Min: 254.141, Sum: 2103.452)
Cluster-Wide Average, Communication-Layer: 41.674 ms (Max: 47.107, Min: 32.067, Sum: 333.392)
Cluster-Wide Average, Bubble-Imbalance: 26.923 ms (Max: 33.438, Min: 13.645, Sum: 215.384)
Cluster-Wide Average, Communication-Graph: 0.466 ms (Max: 0.507, Min: 0.410, Sum: 3.729)
Cluster-Wide Average, Optimization: 0.101 ms (Max: 0.123, Min: 0.093, Sum: 0.805)
Cluster-Wide Average, Others: 3.734 ms (Max: 17.359, Min: 1.778, Sum: 29.870)
****** Breakdown Sum: 416.001 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.304 GB (Max: 8.059, Min: 6.018, Sum: 50.433)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 61.622 Gbps (Max: 73.516, Min: 45.475, Sum: 492.975)
Layer-level communication (cluster-wide, per-epoch): 2.430 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 2.430 GB
****** Accuracy Results ******
Highest valid_acc: 0.9485
Target test_acc: 0.9465
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
