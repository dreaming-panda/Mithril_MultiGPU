Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.916 seconds.
Building the CSC structure...
        It takes 2.197 seconds.
Building the CSC structure...
        It takes 2.375 seconds.
Building the CSC structure...
        It takes 2.384 seconds.
Building the CSC structure...
        It takes 2.457 seconds.
Building the CSC structure...
        It takes 2.561 seconds.
Building the CSC structure...
        It takes 2.657 seconds.
Building the CSC structure...
        It takes 2.678 seconds.
Building the CSC structure...
        It takes 1.848 seconds.
        It takes 2.310 seconds.
        It takes 2.267 seconds.
Building the Feature Vector...
        It takes 2.389 seconds.
        It takes 2.238 seconds.
        It takes 2.409 seconds.
        It takes 2.299 seconds.
        It takes 2.320 seconds.
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.264 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
        It takes 0.264 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.048 seconds.
Building the Feature Vector...
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.243 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.037 seconds.
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.260 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 55.608 Gbps (per GPU), 444.863 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.366 Gbps (per GPU), 442.927 Gbps (aggregated)
The layer-level communication performance: 55.367 Gbps (per GPU), 442.939 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.138 Gbps (per GPU), 441.104 Gbps (aggregated)
The layer-level communication performance: 55.111 Gbps (per GPU), 440.888 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.908 Gbps (per GPU), 439.266 Gbps (aggregated)
The layer-level communication performance: 54.869 Gbps (per GPU), 438.952 Gbps (aggregated)
The layer-level communication performance: 54.838 Gbps (per GPU), 438.707 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.955 Gbps (per GPU), 1255.641 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.955 Gbps (per GPU), 1255.642 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.958 Gbps (per GPU), 1255.662 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.964 Gbps (per GPU), 1255.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.958 Gbps (per GPU), 1255.662 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.961 Gbps (per GPU), 1255.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.958 Gbps (per GPU), 1255.662 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.887 Gbps (per GPU), 1255.099 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.270 Gbps (per GPU), 794.163 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.271 Gbps (per GPU), 794.169 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.270 Gbps (per GPU), 794.156 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.270 Gbps (per GPU), 794.156 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.269 Gbps (per GPU), 794.150 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.272 Gbps (per GPU), 794.175 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.269 Gbps (per GPU), 794.150 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.272 Gbps (per GPU), 794.175 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.310 Gbps (per GPU), 274.479 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.310 Gbps (per GPU), 274.483 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.311 Gbps (per GPU), 274.487 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.309 Gbps (per GPU), 274.473 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.310 Gbps (per GPU), 274.478 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.311 Gbps (per GPU), 274.484 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.312 Gbps (per GPU), 274.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.310 Gbps (per GPU), 274.480 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.31ms  2.57ms  2.40ms  1.38  8.38K  3.53M
 chk_1  3.49ms  2.82ms  2.67ms  1.31  6.74K  3.60M
 chk_2  3.41ms  2.67ms  2.54ms  1.34  7.27K  3.53M
 chk_3  3.43ms  2.71ms  2.56ms  1.34  7.92K  3.61M
 chk_4  3.14ms  2.64ms  2.54ms  1.24  5.33K  3.68M
 chk_5  3.54ms  2.63ms  2.42ms  1.47 10.07K  3.45M
 chk_6  3.68ms  2.80ms  2.59ms  1.42  9.41K  3.48M
 chk_7  3.36ms  2.64ms  2.48ms  1.35  8.12K  3.60M
 chk_8  3.34ms  2.74ms  2.63ms  1.27  6.09K  3.64M
 chk_9  3.58ms  2.54ms  2.31ms  1.55 11.10K  3.38M
chk_10  3.33ms  2.79ms  2.69ms  1.24  5.67K  3.63M
chk_11  3.38ms  2.65ms  2.49ms  1.36  8.16K  3.54M
chk_12  3.57ms  2.86ms  2.71ms  1.32  7.24K  3.55M
chk_13  3.21ms  2.69ms  2.58ms  1.24  5.41K  3.68M
chk_14  3.64ms  2.93ms  2.78ms  1.31  7.14K  3.53M
chk_15  3.63ms  2.80ms  2.58ms  1.41  9.25K  3.49M
chk_16  3.09ms  2.62ms  2.51ms  1.23  4.78K  3.77M
chk_17  3.42ms  2.75ms  2.61ms  1.31  6.85K  3.60M
chk_18  3.26ms  2.56ms  2.41ms  1.35  7.47K  3.57M
chk_19  3.09ms  2.64ms  2.52ms  1.23  4.88K  3.75M
chk_20  3.31ms  2.62ms  2.49ms  1.33  7.00K  3.63M
chk_21  3.12ms  2.59ms  2.49ms  1.26  5.41K  3.68M
chk_22  3.85ms  2.80ms  2.55ms  1.51 11.07K  3.39M
chk_23  3.42ms  2.70ms  2.57ms  1.33  7.23K  3.64M
chk_24  3.67ms  2.77ms  2.56ms  1.44 10.13K  3.43M
chk_25  3.17ms  2.58ms  2.46ms  1.29  6.40K  3.57M
chk_26  3.37ms  2.78ms  2.67ms  1.26  5.78K  3.55M
chk_27  3.50ms  2.67ms  2.47ms  1.41  9.34K  3.48M
chk_28  3.59ms  2.99ms  2.82ms  1.27  6.37K  3.57M
chk_29  3.30ms  2.80ms  2.66ms  1.24  5.16K  3.78M
chk_30  3.18ms  2.68ms  2.55ms  1.25  5.44K  3.67M
chk_31  3.44ms  2.81ms  2.72ms  1.26  6.33K  3.63M
   Avg  3.40  2.71  2.56
   Max  3.85  2.99  2.82
   Min  3.09  2.54  2.31
 Ratio  1.25  1.18  1.22
   Var  0.04  0.01  0.01
Profiling takes 3.236 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 369.418 ms
Partition 0 [0, 4) has cost: 369.418 ms
Partition 1 [4, 8) has cost: 347.409 ms
Partition 2 [8, 12) has cost: 347.409 ms
Partition 3 [12, 16) has cost: 347.409 ms
Partition 4 [16, 20) has cost: 347.409 ms
Partition 5 [20, 24) has cost: 347.409 ms
Partition 6 [24, 28) has cost: 347.409 ms
Partition 7 [28, 32) has cost: 342.595 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 166.944 ms
GPU 0, Compute+Comm Time: 134.459 ms, Bubble Time: 29.364 ms, Imbalance Overhead: 3.120 ms
GPU 1, Compute+Comm Time: 128.044 ms, Bubble Time: 29.146 ms, Imbalance Overhead: 9.754 ms
GPU 2, Compute+Comm Time: 128.044 ms, Bubble Time: 29.166 ms, Imbalance Overhead: 9.733 ms
GPU 3, Compute+Comm Time: 128.044 ms, Bubble Time: 29.025 ms, Imbalance Overhead: 9.874 ms
GPU 4, Compute+Comm Time: 128.044 ms, Bubble Time: 29.012 ms, Imbalance Overhead: 9.888 ms
GPU 5, Compute+Comm Time: 128.044 ms, Bubble Time: 29.037 ms, Imbalance Overhead: 9.863 ms
GPU 6, Compute+Comm Time: 128.044 ms, Bubble Time: 29.222 ms, Imbalance Overhead: 9.677 ms
GPU 7, Compute+Comm Time: 126.310 ms, Bubble Time: 29.636 ms, Imbalance Overhead: 10.997 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 323.370 ms
GPU 0, Compute+Comm Time: 243.097 ms, Bubble Time: 57.669 ms, Imbalance Overhead: 22.604 ms
GPU 1, Compute+Comm Time: 246.177 ms, Bubble Time: 56.792 ms, Imbalance Overhead: 20.401 ms
GPU 2, Compute+Comm Time: 246.177 ms, Bubble Time: 56.345 ms, Imbalance Overhead: 20.848 ms
GPU 3, Compute+Comm Time: 246.177 ms, Bubble Time: 56.265 ms, Imbalance Overhead: 20.928 ms
GPU 4, Compute+Comm Time: 246.177 ms, Bubble Time: 56.283 ms, Imbalance Overhead: 20.910 ms
GPU 5, Compute+Comm Time: 246.177 ms, Bubble Time: 56.555 ms, Imbalance Overhead: 20.638 ms
GPU 6, Compute+Comm Time: 246.177 ms, Bubble Time: 56.521 ms, Imbalance Overhead: 20.672 ms
GPU 7, Compute+Comm Time: 261.771 ms, Bubble Time: 57.009 ms, Imbalance Overhead: 4.590 ms
The estimated cost of the whole pipeline: 514.829 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 716.827 ms
Partition 0 [0, 8) has cost: 716.827 ms
Partition 1 [8, 16) has cost: 694.818 ms
Partition 2 [16, 24) has cost: 694.818 ms
Partition 3 [24, 32) has cost: 690.004 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.216 ms
GPU 0, Compute+Comm Time: 143.683 ms, Bubble Time: 27.260 ms, Imbalance Overhead: 2.273 ms
GPU 1, Compute+Comm Time: 140.187 ms, Bubble Time: 26.729 ms, Imbalance Overhead: 6.301 ms
GPU 2, Compute+Comm Time: 140.187 ms, Bubble Time: 26.626 ms, Imbalance Overhead: 6.403 ms
GPU 3, Compute+Comm Time: 139.387 ms, Bubble Time: 26.360 ms, Imbalance Overhead: 7.470 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 322.107 ms
GPU 0, Compute+Comm Time: 258.432 ms, Bubble Time: 49.399 ms, Imbalance Overhead: 14.276 ms
GPU 1, Compute+Comm Time: 259.861 ms, Bubble Time: 49.557 ms, Imbalance Overhead: 12.689 ms
GPU 2, Compute+Comm Time: 259.861 ms, Bubble Time: 49.539 ms, Imbalance Overhead: 12.707 ms
GPU 3, Compute+Comm Time: 268.475 ms, Bubble Time: 50.568 ms, Imbalance Overhead: 3.064 ms
    The estimated cost with 2 DP ways is 520.090 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1411.644 ms
Partition 0 [0, 16) has cost: 1411.644 ms
Partition 1 [16, 32) has cost: 1384.821 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 222.093 ms
GPU 0, Compute+Comm Time: 193.292 ms, Bubble Time: 23.792 ms, Imbalance Overhead: 5.009 ms
GPU 1, Compute+Comm Time: 191.200 ms, Bubble Time: 24.341 ms, Imbalance Overhead: 6.552 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 362.978 ms
GPU 0, Compute+Comm Time: 313.697 ms, Bubble Time: 39.886 ms, Imbalance Overhead: 9.394 ms
GPU 1, Compute+Comm Time: 318.863 ms, Bubble Time: 38.947 ms, Imbalance Overhead: 5.167 ms
    The estimated cost with 4 DP ways is 614.324 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2796.466 ms
Partition 0 [0, 32) has cost: 2796.466 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 557.807 ms
GPU 0, Compute+Comm Time: 557.807 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 684.749 ms
GPU 0, Compute+Comm Time: 684.749 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1304.684 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 162)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [162, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [194, 226)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 98)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [226, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [98, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[162, 194)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 6 initializing the weights for op[194, 226)...
+++++++++ Node 1 initializing the weights for op[34, 66)...
+++++++++ Node 7 initializing the weights for op[226, 257)...
+++++++++ Node 3 initializing the weights for op[98, 130)...
+++++++++ Node 4 initializing the weights for op[130, 162)...
+++++++++ Node 2 initializing the weights for op[66, 98)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.3968	TrainAcc 0.0183	ValidAcc 0.0167	TestAcc 0.0158	BestValid 0.0167
	Epoch 50:	Loss 3.2969	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.1920	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.0560	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 2.9373	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 250:	Loss 2.7988	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 300:	Loss 2.7288	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 350:	Loss 2.5989	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 400:	Loss 2.4992	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0584
	Epoch 450:	Loss 2.3995	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0584
	Epoch 500:	Loss 2.2914	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0584
	Epoch 550:	Loss 2.2017	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0584
	Epoch 600:	Loss 2.2218	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0584
	Epoch 650:	Loss 2.2399	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.0584
	Epoch 700:	Loss 2.2504	TrainAcc 0.0504	ValidAcc 0.0422	TestAcc 0.0455	BestValid 0.0584
	Epoch 750:	Loss 2.1899	TrainAcc 0.0501	ValidAcc 0.0425	TestAcc 0.0455	BestValid 0.0584
	Epoch 800:	Loss 2.1164	TrainAcc 0.0516	ValidAcc 0.0438	TestAcc 0.0470	BestValid 0.0584
	Epoch 850:	Loss 2.1143	TrainAcc 0.0731	ValidAcc 0.0637	TestAcc 0.0666	BestValid 0.0637
	Epoch 900:	Loss 2.2179	TrainAcc 0.0724	ValidAcc 0.0660	TestAcc 0.0667	BestValid 0.0660
	Epoch 950:	Loss 2.1167	TrainAcc 0.1077	ValidAcc 0.1469	TestAcc 0.1485	BestValid 0.1469
	Epoch 1000:	Loss 2.0797	TrainAcc 0.0413	ValidAcc 0.0354	TestAcc 0.0378	BestValid 0.1469
	Epoch 1050:	Loss 2.0144	TrainAcc 0.0899	ValidAcc 0.1134	TestAcc 0.1143	BestValid 0.1469
	Epoch 1100:	Loss 2.0099	TrainAcc 0.0288	ValidAcc 0.0251	TestAcc 0.0257	BestValid 0.1469
	Epoch 1150:	Loss 1.9704	TrainAcc 0.0604	ValidAcc 0.0768	TestAcc 0.0777	BestValid 0.1469
	Epoch 1200:	Loss 1.9487	TrainAcc 0.0307	ValidAcc 0.0272	TestAcc 0.0269	BestValid 0.1469
	Epoch 1250:	Loss 1.9028	TrainAcc 0.0503	ValidAcc 0.0418	TestAcc 0.0454	BestValid 0.1469
	Epoch 1300:	Loss 1.9070	TrainAcc 0.0505	ValidAcc 0.0420	TestAcc 0.0454	BestValid 0.1469
	Epoch 1350:	Loss 1.8921	TrainAcc 0.0505	ValidAcc 0.0422	TestAcc 0.0455	BestValid 0.1469
	Epoch 1400:	Loss 1.8675	TrainAcc 0.0578	ValidAcc 0.0618	TestAcc 0.0648	BestValid 0.1469
	Epoch 1450:	Loss 1.9836	TrainAcc 0.0711	ValidAcc 0.0627	TestAcc 0.0663	BestValid 0.1469
	Epoch 1500:	Loss 1.8879	TrainAcc 0.0531	ValidAcc 0.0444	TestAcc 0.0481	BestValid 0.1469
	Epoch 1550:	Loss 1.7624	TrainAcc 0.0505	ValidAcc 0.0425	TestAcc 0.0462	BestValid 0.1469
	Epoch 1600:	Loss 1.7016	TrainAcc 0.0509	ValidAcc 0.0427	TestAcc 0.0464	BestValid 0.1469
	Epoch 1650:	Loss 1.6054	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0451	BestValid 0.1469
	Epoch 1700:	Loss 1.5892	TrainAcc 0.1197	ValidAcc 0.1162	TestAcc 0.1127	BestValid 0.1469
	Epoch 1750:	Loss 1.5532	TrainAcc 0.1397	ValidAcc 0.1386	TestAcc 0.1358	BestValid 0.1469
	Epoch 1800:	Loss 1.5728	TrainAcc 0.1347	ValidAcc 0.1303	TestAcc 0.1275	BestValid 0.1469
	Epoch 1850:	Loss 1.5334	TrainAcc 0.1599	ValidAcc 0.1785	TestAcc 0.1800	BestValid 0.1785
	Epoch 1900:	Loss 1.4914	TrainAcc 0.1068	ValidAcc 0.0962	TestAcc 0.0968	BestValid 0.1785
	Epoch 1950:	Loss 1.4226	TrainAcc 0.0875	ValidAcc 0.0765	TestAcc 0.0774	BestValid 0.1785
	Epoch 2000:	Loss 1.4601	TrainAcc 0.1028	ValidAcc 0.0934	TestAcc 0.0939	BestValid 0.1785
	Epoch 2050:	Loss 1.4008	TrainAcc 0.0733	ValidAcc 0.0626	TestAcc 0.0625	BestValid 0.1785
	Epoch 2100:	Loss 1.3844	TrainAcc 0.1152	ValidAcc 0.1031	TestAcc 0.1023	BestValid 0.1785
	Epoch 2150:	Loss 1.3380	TrainAcc 0.0773	ValidAcc 0.0673	TestAcc 0.0667	BestValid 0.1785
	Epoch 2200:	Loss 1.2836	TrainAcc 0.0752	ValidAcc 0.0646	TestAcc 0.0644	BestValid 0.1785
	Epoch 2250:	Loss 1.2558	TrainAcc 0.0852	ValidAcc 0.0740	TestAcc 0.0748	BestValid 0.1785
	Epoch 2300:	Loss 1.2386	TrainAcc 0.0709	ValidAcc 0.0603	TestAcc 0.0597	BestValid 0.1785
	Epoch 2350:	Loss 1.3089	TrainAcc 0.0701	ValidAcc 0.0595	TestAcc 0.0588	BestValid 0.1785
	Epoch 2400:	Loss 1.2470	TrainAcc 0.0771	ValidAcc 0.0671	TestAcc 0.0666	BestValid 0.1785
	Epoch 2450:	Loss 1.2312	TrainAcc 0.0730	ValidAcc 0.0625	TestAcc 0.0625	BestValid 0.1785
	Epoch 2500:	Loss 1.2010	TrainAcc 0.0771	ValidAcc 0.0670	TestAcc 0.0669	BestValid 0.1785
	Epoch 2550:	Loss 1.1833	TrainAcc 0.0952	ValidAcc 0.0839	TestAcc 0.0829	BestValid 0.1785
	Epoch 2600:	Loss 1.1742	TrainAcc 0.0999	ValidAcc 0.0908	TestAcc 0.0913	BestValid 0.1785
	Epoch 2650:	Loss 1.1812	TrainAcc 0.0995	ValidAcc 0.0898	TestAcc 0.0907	BestValid 0.1785
	Epoch 2700:	Loss 1.1356	TrainAcc 0.1004	ValidAcc 0.0923	TestAcc 0.0925	BestValid 0.1785
	Epoch 2750:	Loss 1.1266	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1785
	Epoch 2800:	Loss 1.1052	TrainAcc 0.0985	ValidAcc 0.0907	TestAcc 0.0911	BestValid 0.1785
	Epoch 2850:	Loss 1.0821	TrainAcc 0.0332	ValidAcc 0.0344	TestAcc 0.0358	BestValid 0.1785
	Epoch 2900:	Loss 1.1031	TrainAcc 0.0814	ValidAcc 0.0703	TestAcc 0.0711	BestValid 0.1785
	Epoch 2950:	Loss 1.1052	TrainAcc 0.0935	ValidAcc 0.0825	TestAcc 0.0829	BestValid 0.1785
	Epoch 3000:	Loss 1.1041	TrainAcc 0.0765	ValidAcc 0.0666	TestAcc 0.0662	BestValid 0.1785
	Epoch 3050:	Loss 1.1723	TrainAcc 0.0352	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1785
	Epoch 3100:	Loss 1.0768	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1785
	Epoch 3150:	Loss 1.0741	TrainAcc 0.0340	ValidAcc 0.0348	TestAcc 0.0363	BestValid 0.1785
	Epoch 3200:	Loss 1.0729	TrainAcc 0.0345	ValidAcc 0.0353	TestAcc 0.0369	BestValid 0.1785
	Epoch 3250:	Loss 1.1234	TrainAcc 0.0730	ValidAcc 0.0624	TestAcc 0.0620	BestValid 0.1785
	Epoch 3300:	Loss 1.1224	TrainAcc 0.0344	ValidAcc 0.0351	TestAcc 0.0368	BestValid 0.1785
	Epoch 3350:	Loss 1.1594	TrainAcc 0.1003	ValidAcc 0.0918	TestAcc 0.0922	BestValid 0.1785
	Epoch 3400:	Loss 1.1637	TrainAcc 0.0798	ValidAcc 0.0906	TestAcc 0.0911	BestValid 0.1785
	Epoch 3450:	Loss 1.0649	TrainAcc 0.0786	ValidAcc 0.0682	TestAcc 0.0684	BestValid 0.1785
	Epoch 3500:	Loss 1.0603	TrainAcc 0.0341	ValidAcc 0.0352	TestAcc 0.0366	BestValid 0.1785
	Epoch 3550:	Loss 1.0218	TrainAcc 0.0703	ValidAcc 0.0596	TestAcc 0.0589	BestValid 0.1785
	Epoch 3600:	Loss 1.0075	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1785
	Epoch 3650:	Loss 1.0030	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1785
	Epoch 3700:	Loss 0.9833	TrainAcc 0.0697	ValidAcc 0.0589	TestAcc 0.0582	BestValid 0.1785
	Epoch 3750:	Loss 1.0024	TrainAcc 0.0712	ValidAcc 0.0605	TestAcc 0.0598	BestValid 0.1785
	Epoch 3800:	Loss 0.9714	TrainAcc 0.0701	ValidAcc 0.0595	TestAcc 0.0588	BestValid 0.1785
	Epoch 3850:	Loss 0.9660	TrainAcc 0.0893	ValidAcc 0.0771	TestAcc 0.0758	BestValid 0.1785
	Epoch 3900:	Loss 0.9541	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1785
	Epoch 3950:	Loss 0.9366	TrainAcc 0.0701	ValidAcc 0.0595	TestAcc 0.0588	BestValid 0.1785
	Epoch 4000:	Loss 0.9501	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.1785
	Epoch 4050:	Loss 0.9911	TrainAcc 0.0705	ValidAcc 0.0600	TestAcc 0.0592	BestValid 0.1785
	Epoch 4100:	Loss 0.9623	TrainAcc 0.0706	ValidAcc 0.0600	TestAcc 0.0593	BestValid 0.1785
	Epoch 4150:	Loss 0.9933	TrainAcc 0.0690	ValidAcc 0.0585	TestAcc 0.0575	BestValid 0.1785
	Epoch 4200:	Loss 0.9886	TrainAcc 0.0701	ValidAcc 0.0595	TestAcc 0.0587	BestValid 0.1785
	Epoch 4250:	Loss 0.9237	TrainAcc 0.0274	ValidAcc 0.0445	TestAcc 0.0434	BestValid 0.1785
	Epoch 4300:	Loss 0.9013	TrainAcc 0.0708	ValidAcc 0.0604	TestAcc 0.0599	BestValid 0.1785
	Epoch 4350:	Loss 0.9981	TrainAcc 0.0702	ValidAcc 0.0597	TestAcc 0.0589	BestValid 0.1785
	Epoch 4400:	Loss 0.9053	TrainAcc 0.0863	ValidAcc 0.0747	TestAcc 0.0727	BestValid 0.1785
	Epoch 4450:	Loss 0.8878	TrainAcc 0.0537	ValidAcc 0.0690	TestAcc 0.0661	BestValid 0.1785
	Epoch 4500:	Loss 0.8941	TrainAcc 0.0186	ValidAcc 0.0173	TestAcc 0.0165	BestValid 0.1785
	Epoch 4550:	Loss 0.8718	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1785
	Epoch 4600:	Loss 0.8804	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1785
	Epoch 4650:	Loss 0.8808	TrainAcc 0.0395	ValidAcc 0.0629	TestAcc 0.0607	BestValid 0.1785
	Epoch 4700:	Loss 0.8793	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1785
	Epoch 4750:	Loss 0.8631	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1785
	Epoch 4800:	Loss 0.8542	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1785
	Epoch 4850:	Loss 0.8377	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1785
	Epoch 4900:	Loss 0.8960	TrainAcc 0.0867	ValidAcc 0.0749	TestAcc 0.0730	BestValid 0.1785
	Epoch 4950:	Loss 0.8750	TrainAcc 0.0170	ValidAcc 0.0160	TestAcc 0.0164	BestValid 0.1785
	Epoch 5000:	Loss 0.8534	TrainAcc 0.0182	ValidAcc 0.0167	TestAcc 0.0157	BestValid 0.1785
****** Epoch Time (Excluding Evaluation Cost): 0.534 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 98.355 ms (Max: 100.855, Min: 85.575, Sum: 786.837)
Cluster-Wide Average, Compute: 378.694 ms (Max: 403.974, Min: 366.492, Sum: 3029.555)
Cluster-Wide Average, Communication-Layer: 25.695 ms (Max: 31.010, Min: 16.977, Sum: 205.560)
Cluster-Wide Average, Bubble-Imbalance: 27.559 ms (Max: 36.955, Min: 11.494, Sum: 220.476)
Cluster-Wide Average, Communication-Graph: 0.498 ms (Max: 0.544, Min: 0.453, Sum: 3.984)
Cluster-Wide Average, Optimization: 0.188 ms (Max: 0.202, Min: 0.179, Sum: 1.504)
Cluster-Wide Average, Others: 3.828 ms (Max: 17.373, Min: 1.880, Sum: 30.627)
****** Breakdown Sum: 534.818 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.352 GB (Max: 6.575, Min: 5.006, Sum: 42.815)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 50.188 Gbps (Max: 55.955, Min: 43.597, Sum: 401.505)
Layer-level communication (cluster-wide, per-epoch): 1.215 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 1.215 GB
****** Accuracy Results ******
Highest valid_acc: 0.1785
Target test_acc: 0.1800
Epoch to reach the target acc: 1849
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 0] Success 
[MPI Rank 6] Success 
[MPI Rank 1] Success 
[MPI Rank 7] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
