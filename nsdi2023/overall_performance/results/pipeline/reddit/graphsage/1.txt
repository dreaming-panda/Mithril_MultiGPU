Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.831 seconds.
Building the CSC structure...
        It takes 1.850 seconds.
Building the CSC structure...
        It takes 2.083 seconds.
Building the CSC structure...
        It takes 2.358 seconds.
Building the CSC structure...
        It takes 2.359 seconds.
Building the CSC structure...
        It takes 2.483 seconds.
Building the CSC structure...
        It takes 2.502 seconds.
Building the CSC structure...
        It takes 2.502 seconds.
Building the CSC structure...
        It takes 1.812 seconds.
        It takes 1.850 seconds.
        It takes 1.854 seconds.
        It takes 2.193 seconds.
Building the Feature Vector...
        It takes 2.296 seconds.
Building the Feature Vector...
        It takes 2.307 seconds.
        It takes 2.340 seconds.
        It takes 2.361 seconds.
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.042 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.279 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.044 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.289 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.032 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.302 Gbps (per GPU), 466.414 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.048 Gbps (per GPU), 464.383 Gbps (aggregated)
The layer-level communication performance: 58.039 Gbps (per GPU), 464.313 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.808 Gbps (per GPU), 462.464 Gbps (aggregated)
The layer-level communication performance: 57.774 Gbps (per GPU), 462.189 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.588 Gbps (per GPU), 460.701 Gbps (aggregated)
The layer-level communication performance: 57.546 Gbps (per GPU), 460.370 Gbps (aggregated)
The layer-level communication performance: 57.507 Gbps (per GPU), 460.059 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.692 Gbps (per GPU), 1261.540 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.695 Gbps (per GPU), 1261.563 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.648 Gbps (per GPU), 1261.184 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.713 Gbps (per GPU), 1261.706 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.689 Gbps (per GPU), 1261.516 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.701 Gbps (per GPU), 1261.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.660 Gbps (per GPU), 1261.279 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.710 Gbps (per GPU), 1261.683 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.701 Gbps (per GPU), 805.609 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.701 Gbps (per GPU), 805.609 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.701 Gbps (per GPU), 805.609 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.714 Gbps (per GPU), 805.712 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.701 Gbps (per GPU), 805.609 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.700 Gbps (per GPU), 805.603 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.702 Gbps (per GPU), 805.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.712 Gbps (per GPU), 805.693 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.077 Gbps (per GPU), 256.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.077 Gbps (per GPU), 256.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.076 Gbps (per GPU), 256.610 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.076 Gbps (per GPU), 256.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.076 Gbps (per GPU), 256.608 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.077 Gbps (per GPU), 256.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.077 Gbps (per GPU), 256.613 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.077 Gbps (per GPU), 256.616 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.31ms  2.56ms  2.35ms  1.41  8.38K  3.53M
 chk_1  3.63ms  2.79ms  2.65ms  1.37  6.74K  3.60M
 chk_2  3.40ms  2.66ms  2.51ms  1.35  7.27K  3.53M
 chk_3  3.44ms  2.69ms  2.54ms  1.36  7.92K  3.61M
 chk_4  3.15ms  2.62ms  2.52ms  1.25  5.33K  3.68M
 chk_5  3.53ms  2.61ms  2.40ms  1.47 10.07K  3.45M
 chk_6  3.67ms  2.78ms  2.58ms  1.42  9.41K  3.48M
 chk_7  3.34ms  2.58ms  2.47ms  1.35  8.12K  3.60M
 chk_8  3.32ms  2.73ms  2.61ms  1.27  6.09K  3.64M
 chk_9  3.56ms  2.52ms  2.30ms  1.55 11.10K  3.38M
chk_10  3.31ms  3.04ms  2.68ms  1.23  5.67K  3.63M
chk_11  3.37ms  2.60ms  2.48ms  1.36  8.16K  3.54M
chk_12  3.56ms  2.82ms  2.70ms  1.32  7.24K  3.55M
chk_13  3.19ms  2.66ms  2.54ms  1.25  5.41K  3.68M
chk_14  3.63ms  2.89ms  2.76ms  1.31  7.14K  3.53M
chk_15  3.62ms  2.75ms  2.56ms  1.41  9.25K  3.49M
chk_16  3.07ms  2.56ms  2.49ms  1.23  4.78K  3.77M
chk_17  3.39ms  2.96ms  2.59ms  1.31  6.85K  3.60M
chk_18  3.23ms  2.48ms  2.39ms  1.35  7.47K  3.57M
chk_19  3.10ms  2.59ms  2.50ms  1.24  4.88K  3.75M
chk_20  3.31ms  2.59ms  2.47ms  1.34  7.00K  3.63M
chk_21  3.12ms  2.58ms  2.47ms  1.26  5.41K  3.68M
chk_22  3.79ms  2.79ms  2.54ms  1.49 11.07K  3.39M
chk_23  3.41ms  2.68ms  2.54ms  1.34  7.23K  3.64M
chk_24  3.67ms  2.95ms  2.53ms  1.45 10.13K  3.43M
chk_25  3.18ms  2.55ms  2.44ms  1.31  6.40K  3.57M
chk_26  3.36ms  2.75ms  2.65ms  1.27  5.78K  3.55M
chk_27  3.39ms  2.63ms  2.45ms  1.38  9.34K  3.48M
chk_28  3.58ms  2.97ms  2.81ms  1.27  6.37K  3.57M
chk_29  3.28ms  2.77ms  2.65ms  1.24  5.16K  3.78M
chk_30  3.14ms  2.67ms  2.53ms  1.24  5.44K  3.67M
chk_31  3.40ms  2.92ms  2.70ms  1.26  6.33K  3.63M
   Avg  3.39  2.71  2.54
   Max  3.79  3.04  2.81
   Min  3.07  2.48  2.30
 Ratio  1.23  1.22  1.22
   Var  0.04  0.02  0.01
Profiling takes 3.244 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 368.676 ms
Partition 0 [0, 4) has cost: 368.676 ms
Partition 1 [4, 8) has cost: 346.986 ms
Partition 2 [8, 12) has cost: 346.986 ms
Partition 3 [12, 16) has cost: 346.986 ms
Partition 4 [16, 20) has cost: 346.986 ms
Partition 5 [20, 24) has cost: 346.986 ms
Partition 6 [24, 28) has cost: 346.986 ms
Partition 7 [28, 32) has cost: 341.641 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.239 ms
GPU 0, Compute+Comm Time: 133.889 ms, Bubble Time: 31.347 ms, Imbalance Overhead: 8.002 ms
GPU 1, Compute+Comm Time: 127.666 ms, Bubble Time: 30.882 ms, Imbalance Overhead: 14.690 ms
GPU 2, Compute+Comm Time: 127.666 ms, Bubble Time: 30.728 ms, Imbalance Overhead: 14.844 ms
GPU 3, Compute+Comm Time: 127.666 ms, Bubble Time: 30.345 ms, Imbalance Overhead: 15.228 ms
GPU 4, Compute+Comm Time: 127.666 ms, Bubble Time: 30.020 ms, Imbalance Overhead: 15.553 ms
GPU 5, Compute+Comm Time: 127.666 ms, Bubble Time: 29.557 ms, Imbalance Overhead: 16.016 ms
GPU 6, Compute+Comm Time: 127.666 ms, Bubble Time: 29.273 ms, Imbalance Overhead: 16.300 ms
GPU 7, Compute+Comm Time: 125.647 ms, Bubble Time: 29.346 ms, Imbalance Overhead: 18.245 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 327.841 ms
GPU 0, Compute+Comm Time: 241.568 ms, Bubble Time: 57.240 ms, Imbalance Overhead: 29.033 ms
GPU 1, Compute+Comm Time: 244.894 ms, Bubble Time: 56.406 ms, Imbalance Overhead: 26.541 ms
GPU 2, Compute+Comm Time: 244.894 ms, Bubble Time: 56.011 ms, Imbalance Overhead: 26.937 ms
GPU 3, Compute+Comm Time: 244.894 ms, Bubble Time: 56.025 ms, Imbalance Overhead: 26.922 ms
GPU 4, Compute+Comm Time: 244.894 ms, Bubble Time: 56.027 ms, Imbalance Overhead: 26.921 ms
GPU 5, Compute+Comm Time: 244.894 ms, Bubble Time: 56.289 ms, Imbalance Overhead: 26.658 ms
GPU 6, Compute+Comm Time: 244.894 ms, Bubble Time: 56.213 ms, Imbalance Overhead: 26.734 ms
GPU 7, Compute+Comm Time: 260.360 ms, Bubble Time: 56.646 ms, Imbalance Overhead: 10.835 ms
The estimated cost of the whole pipeline: 526.134 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 715.662 ms
Partition 0 [0, 8) has cost: 715.662 ms
Partition 1 [8, 16) has cost: 693.973 ms
Partition 2 [16, 24) has cost: 693.973 ms
Partition 3 [24, 32) has cost: 688.628 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 182.103 ms
GPU 0, Compute+Comm Time: 144.525 ms, Bubble Time: 29.579 ms, Imbalance Overhead: 7.999 ms
GPU 1, Compute+Comm Time: 141.289 ms, Bubble Time: 28.060 ms, Imbalance Overhead: 12.755 ms
GPU 2, Compute+Comm Time: 141.289 ms, Bubble Time: 28.805 ms, Imbalance Overhead: 12.009 ms
GPU 3, Compute+Comm Time: 140.161 ms, Bubble Time: 29.640 ms, Imbalance Overhead: 12.302 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 325.617 ms
GPU 0, Compute+Comm Time: 258.873 ms, Bubble Time: 48.742 ms, Imbalance Overhead: 18.002 ms
GPU 1, Compute+Comm Time: 260.686 ms, Bubble Time: 49.055 ms, Imbalance Overhead: 15.876 ms
GPU 2, Compute+Comm Time: 260.686 ms, Bubble Time: 48.897 ms, Imbalance Overhead: 16.034 ms
GPU 3, Compute+Comm Time: 268.999 ms, Bubble Time: 49.905 ms, Imbalance Overhead: 6.713 ms
    The estimated cost with 2 DP ways is 533.106 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1409.635 ms
Partition 0 [0, 16) has cost: 1409.635 ms
Partition 1 [16, 32) has cost: 1382.601 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 227.877 ms
GPU 0, Compute+Comm Time: 196.208 ms, Bubble Time: 25.206 ms, Imbalance Overhead: 6.462 ms
GPU 1, Compute+Comm Time: 194.061 ms, Bubble Time: 24.057 ms, Imbalance Overhead: 9.758 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 366.680 ms
GPU 0, Compute+Comm Time: 317.349 ms, Bubble Time: 42.711 ms, Imbalance Overhead: 6.620 ms
GPU 1, Compute+Comm Time: 322.486 ms, Bubble Time: 38.743 ms, Imbalance Overhead: 5.452 ms
    The estimated cost with 4 DP ways is 624.285 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2792.236 ms
Partition 0 [0, 32) has cost: 2792.236 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 598.014 ms
GPU 0, Compute+Comm Time: 598.014 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 720.461 ms
GPU 0, Compute+Comm Time: 720.461 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1384.399 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 162)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [162, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 98)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [194, 226)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [98, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [226, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 66)...
+++++++++ Node 3 initializing the weights for op[98, 130)...
+++++++++ Node 4 initializing the weights for op[130, 162)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 2 initializing the weights for op[66, 98)...
+++++++++ Node 5 initializing the weights for op[162, 194)...
+++++++++ Node 6 initializing the weights for op[194, 226)...
+++++++++ Node 7 initializing the weights for op[226, 257)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.6617	TrainAcc 0.0137	ValidAcc 0.0119	TestAcc 0.0128	BestValid 0.0119
	Epoch 50:	Loss 3.2807	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.0308	TrainAcc 0.1078	ValidAcc 0.1467	TestAcc 0.1484	BestValid 0.1467
	Epoch 150:	Loss 2.8060	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1467
	Epoch 200:	Loss 2.7167	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1467
	Epoch 250:	Loss 2.6805	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1467
	Epoch 300:	Loss 2.8350	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1467
	Epoch 350:	Loss 2.7089	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 400:	Loss 2.5580	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 450:	Loss 2.4395	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 500:	Loss 2.3422	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 550:	Loss 2.2311	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 600:	Loss 2.1326	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 650:	Loss 2.1256	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 700:	Loss 2.0262	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 750:	Loss 2.0206	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 800:	Loss 1.9812	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 850:	Loss 1.9781	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 900:	Loss 2.0001	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 950:	Loss 1.9415	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1467
	Epoch 1000:	Loss 1.8757	TrainAcc 0.0500	ValidAcc 0.0417	TestAcc 0.0453	BestValid 0.1467
	Epoch 1050:	Loss 1.8145	TrainAcc 0.0501	ValidAcc 0.0418	TestAcc 0.0452	BestValid 0.1467
	Epoch 1100:	Loss 1.7871	TrainAcc 0.0502	ValidAcc 0.0419	TestAcc 0.0453	BestValid 0.1467
	Epoch 1150:	Loss 1.7654	TrainAcc 0.0504	ValidAcc 0.0423	TestAcc 0.0455	BestValid 0.1467
	Epoch 1200:	Loss 1.7830	TrainAcc 0.0537	ValidAcc 0.0446	TestAcc 0.0481	BestValid 0.1467
	Epoch 1250:	Loss 1.7050	TrainAcc 0.0559	ValidAcc 0.0478	TestAcc 0.0504	BestValid 0.1467
	Epoch 1300:	Loss 1.6106	TrainAcc 0.0889	ValidAcc 0.0780	TestAcc 0.0760	BestValid 0.1467
	Epoch 1350:	Loss 1.5923	TrainAcc 0.0734	ValidAcc 0.0629	TestAcc 0.0612	BestValid 0.1467
	Epoch 1400:	Loss 1.5736	TrainAcc 0.1138	ValidAcc 0.0992	TestAcc 0.1004	BestValid 0.1467
	Epoch 1450:	Loss 1.7873	TrainAcc 0.0833	ValidAcc 0.0916	TestAcc 0.0875	BestValid 0.1467
	Epoch 1500:	Loss 1.9020	TrainAcc 0.1021	ValidAcc 0.1105	TestAcc 0.1062	BestValid 0.1467
	Epoch 1550:	Loss 1.7183	TrainAcc 0.1300	ValidAcc 0.1235	TestAcc 0.1227	BestValid 0.1467
	Epoch 1600:	Loss 1.6617	TrainAcc 0.1289	ValidAcc 0.1207	TestAcc 0.1194	BestValid 0.1467
	Epoch 1650:	Loss 1.5892	TrainAcc 0.2456	ValidAcc 0.2588	TestAcc 0.2554	BestValid 0.2588
	Epoch 1700:	Loss 1.5848	TrainAcc 0.1737	ValidAcc 0.1701	TestAcc 0.1686	BestValid 0.2588
	Epoch 1750:	Loss 1.5809	TrainAcc 0.2031	ValidAcc 0.2314	TestAcc 0.2346	BestValid 0.2588
	Epoch 1800:	Loss 1.5325	TrainAcc 0.1255	ValidAcc 0.1162	TestAcc 0.1197	BestValid 0.2588
	Epoch 1850:	Loss 1.5197	TrainAcc 0.1038	ValidAcc 0.0940	TestAcc 0.0942	BestValid 0.2588
	Epoch 1900:	Loss 1.4730	TrainAcc 0.1038	ValidAcc 0.0945	TestAcc 0.0951	BestValid 0.2588
	Epoch 1950:	Loss 1.4297	TrainAcc 0.1034	ValidAcc 0.0940	TestAcc 0.0944	BestValid 0.2588
	Epoch 2000:	Loss 1.4053	TrainAcc 0.1025	ValidAcc 0.0929	TestAcc 0.0935	BestValid 0.2588
	Epoch 2050:	Loss 1.3857	TrainAcc 0.1127	ValidAcc 0.1028	TestAcc 0.1028	BestValid 0.2588
	Epoch 2100:	Loss 1.3766	TrainAcc 0.1203	ValidAcc 0.1091	TestAcc 0.1090	BestValid 0.2588
	Epoch 2150:	Loss 1.3294	TrainAcc 0.1029	ValidAcc 0.0929	TestAcc 0.0938	BestValid 0.2588
	Epoch 2200:	Loss 1.3289	TrainAcc 0.1035	ValidAcc 0.0937	TestAcc 0.0945	BestValid 0.2588
	Epoch 2250:	Loss 1.4015	TrainAcc 0.1034	ValidAcc 0.0939	TestAcc 0.0944	BestValid 0.2588
	Epoch 2300:	Loss 1.3682	TrainAcc 0.1025	ValidAcc 0.0927	TestAcc 0.0934	BestValid 0.2588
	Epoch 2350:	Loss 1.2763	TrainAcc 0.1029	ValidAcc 0.0931	TestAcc 0.0939	BestValid 0.2588
	Epoch 2400:	Loss 1.2877	TrainAcc 0.1093	ValidAcc 0.1009	TestAcc 0.1014	BestValid 0.2588
	Epoch 2450:	Loss 1.2895	TrainAcc 0.0987	ValidAcc 0.0882	TestAcc 0.0893	BestValid 0.2588
	Epoch 2500:	Loss 1.2509	TrainAcc 0.1018	ValidAcc 0.0921	TestAcc 0.0926	BestValid 0.2588
	Epoch 2550:	Loss 1.2197	TrainAcc 0.0885	ValidAcc 0.0780	TestAcc 0.0783	BestValid 0.2588
	Epoch 2600:	Loss 1.2291	TrainAcc 0.0932	ValidAcc 0.0827	TestAcc 0.0833	BestValid 0.2588
	Epoch 2650:	Loss 1.2013	TrainAcc 0.1148	ValidAcc 0.1048	TestAcc 0.1046	BestValid 0.2588
	Epoch 2700:	Loss 1.1804	TrainAcc 0.0967	ValidAcc 0.0864	TestAcc 0.0876	BestValid 0.2588
	Epoch 2750:	Loss 1.1519	TrainAcc 0.1067	ValidAcc 0.0980	TestAcc 0.0978	BestValid 0.2588
	Epoch 2800:	Loss 1.1399	TrainAcc 0.1059	ValidAcc 0.0981	TestAcc 0.0983	BestValid 0.2588
	Epoch 2850:	Loss 1.1465	TrainAcc 0.1081	ValidAcc 0.1000	TestAcc 0.1003	BestValid 0.2588
	Epoch 2900:	Loss 1.1698	TrainAcc 0.1036	ValidAcc 0.0945	TestAcc 0.0949	BestValid 0.2588
	Epoch 2950:	Loss 1.1441	TrainAcc 0.1072	ValidAcc 0.0984	TestAcc 0.0984	BestValid 0.2588
	Epoch 3000:	Loss 1.1110	TrainAcc 0.0920	ValidAcc 0.0810	TestAcc 0.0824	BestValid 0.2588
	Epoch 3050:	Loss 1.1250	TrainAcc 0.1185	ValidAcc 0.1074	TestAcc 0.1068	BestValid 0.2588
	Epoch 3100:	Loss 1.0943	TrainAcc 0.1180	ValidAcc 0.1072	TestAcc 0.1066	BestValid 0.2588
	Epoch 3150:	Loss 1.0994	TrainAcc 0.1124	ValidAcc 0.1018	TestAcc 0.1016	BestValid 0.2588
	Epoch 3200:	Loss 1.1543	TrainAcc 0.1152	ValidAcc 0.1044	TestAcc 0.1044	BestValid 0.2588
	Epoch 3250:	Loss 1.1103	TrainAcc 0.1163	ValidAcc 0.1057	TestAcc 0.1054	BestValid 0.2588
	Epoch 3300:	Loss 1.0844	TrainAcc 0.0925	ValidAcc 0.0818	TestAcc 0.0828	BestValid 0.2588
	Epoch 3350:	Loss 1.1392	TrainAcc 0.1032	ValidAcc 0.0937	TestAcc 0.0942	BestValid 0.2588
	Epoch 3400:	Loss 1.1981	TrainAcc 0.1035	ValidAcc 0.0956	TestAcc 0.0958	BestValid 0.2588
	Epoch 3450:	Loss 1.1179	TrainAcc 0.1047	ValidAcc 0.0969	TestAcc 0.0967	BestValid 0.2588
	Epoch 3500:	Loss 1.0824	TrainAcc 0.1054	ValidAcc 0.0963	TestAcc 0.0964	BestValid 0.2588
	Epoch 3550:	Loss 1.0360	TrainAcc 0.1203	ValidAcc 0.1092	TestAcc 0.1090	BestValid 0.2588
	Epoch 3600:	Loss 1.0241	TrainAcc 0.1207	ValidAcc 0.1095	TestAcc 0.1093	BestValid 0.2588
	Epoch 3650:	Loss 0.9902	TrainAcc 0.1032	ValidAcc 0.0939	TestAcc 0.0943	BestValid 0.2588
	Epoch 3700:	Loss 0.9818	TrainAcc 0.1034	ValidAcc 0.0937	TestAcc 0.0944	BestValid 0.2588
	Epoch 3750:	Loss 1.0454	TrainAcc 0.1078	ValidAcc 0.0965	TestAcc 0.0965	BestValid 0.2588
	Epoch 3800:	Loss 0.9999	TrainAcc 0.1039	ValidAcc 0.0949	TestAcc 0.0955	BestValid 0.2588
	Epoch 3850:	Loss 0.9562	TrainAcc 0.1195	ValidAcc 0.1077	TestAcc 0.1073	BestValid 0.2588
	Epoch 3900:	Loss 0.9502	TrainAcc 0.1187	ValidAcc 0.1072	TestAcc 0.1068	BestValid 0.2588
	Epoch 3950:	Loss 0.9394	TrainAcc 0.1033	ValidAcc 0.0938	TestAcc 0.0943	BestValid 0.2588
	Epoch 4000:	Loss 0.9466	TrainAcc 0.1097	ValidAcc 0.0970	TestAcc 0.0973	BestValid 0.2588
	Epoch 4050:	Loss 0.9896	TrainAcc 0.1034	ValidAcc 0.0940	TestAcc 0.0945	BestValid 0.2588
	Epoch 4100:	Loss 0.9630	TrainAcc 0.1158	ValidAcc 0.1013	TestAcc 0.1014	BestValid 0.2588
	Epoch 4150:	Loss 0.9164	TrainAcc 0.0689	ValidAcc 0.0652	TestAcc 0.0644	BestValid 0.2588
	Epoch 4200:	Loss 0.8990	TrainAcc 0.1034	ValidAcc 0.0939	TestAcc 0.0943	BestValid 0.2588
	Epoch 4250:	Loss 0.9189	TrainAcc 0.1158	ValidAcc 0.1079	TestAcc 0.1076	BestValid 0.2588
	Epoch 4300:	Loss 0.9087	TrainAcc 0.1033	ValidAcc 0.0938	TestAcc 0.0944	BestValid 0.2588
	Epoch 4350:	Loss 0.9187	TrainAcc 0.0587	ValidAcc 0.0581	TestAcc 0.0575	BestValid 0.2588
	Epoch 4400:	Loss 0.8851	TrainAcc 0.1264	ValidAcc 0.1129	TestAcc 0.1120	BestValid 0.2588
	Epoch 4450:	Loss 0.8625	TrainAcc 0.1513	ValidAcc 0.1357	TestAcc 0.1340	BestValid 0.2588
	Epoch 4500:	Loss 0.8576	TrainAcc 0.1065	ValidAcc 0.0966	TestAcc 0.0967	BestValid 0.2588
	Epoch 4550:	Loss 0.8603	TrainAcc 0.1215	ValidAcc 0.1081	TestAcc 0.1085	BestValid 0.2588
	Epoch 4600:	Loss 0.8566	TrainAcc 0.1118	ValidAcc 0.1034	TestAcc 0.1029	BestValid 0.2588
	Epoch 4650:	Loss 0.9202	TrainAcc 0.1054	ValidAcc 0.0967	TestAcc 0.0972	BestValid 0.2588
	Epoch 4700:	Loss 0.8806	TrainAcc 0.1210	ValidAcc 0.1071	TestAcc 0.1066	BestValid 0.2588
	Epoch 4750:	Loss 0.8722	TrainAcc 0.1319	ValidAcc 0.1158	TestAcc 0.1145	BestValid 0.2588
	Epoch 4800:	Loss 0.8686	TrainAcc 0.1228	ValidAcc 0.1090	TestAcc 0.1091	BestValid 0.2588
	Epoch 4850:	Loss 0.8378	TrainAcc 0.1129	ValidAcc 0.1026	TestAcc 0.1021	BestValid 0.2588
	Epoch 4900:	Loss 0.8166	TrainAcc 0.1322	ValidAcc 0.1161	TestAcc 0.1150	BestValid 0.2588
	Epoch 4950:	Loss 0.8396	TrainAcc 0.1186	ValidAcc 0.1041	TestAcc 0.1040	BestValid 0.2588
	Epoch 5000:	Loss 0.8801	TrainAcc 0.1287	ValidAcc 0.1155	TestAcc 0.1152	BestValid 0.2588
****** Epoch Time (Excluding Evaluation Cost): 0.533 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 98.038 ms (Max: 100.630, Min: 85.222, Sum: 784.307)
Cluster-Wide Average, Compute: 376.948 ms (Max: 402.554, Min: 363.463, Sum: 3015.586)
Cluster-Wide Average, Communication-Layer: 25.810 ms (Max: 31.177, Min: 16.876, Sum: 206.478)
Cluster-Wide Average, Bubble-Imbalance: 27.862 ms (Max: 37.790, Min: 11.670, Sum: 222.898)
Cluster-Wide Average, Communication-Graph: 0.534 ms (Max: 0.586, Min: 0.502, Sum: 4.270)
Cluster-Wide Average, Optimization: 0.188 ms (Max: 0.203, Min: 0.179, Sum: 1.508)
Cluster-Wide Average, Others: 3.837 ms (Max: 17.383, Min: 1.884, Sum: 30.697)
****** Breakdown Sum: 533.218 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.352 GB (Max: 6.573, Min: 5.006, Sum: 42.813)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 49.969 Gbps (Max: 55.943, Min: 43.053, Sum: 399.751)
Layer-level communication (cluster-wide, per-epoch): 1.215 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 1.215 GB
****** Accuracy Results ******
Highest valid_acc: 0.2588
Target test_acc: 0.2554
Epoch to reach the target acc: 1649
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
