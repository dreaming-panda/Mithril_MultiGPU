Initialized node 2 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.984 seconds.
Building the CSC structure...
        It takes 2.054 seconds.
Building the CSC structure...
        It takes 2.056 seconds.
Building the CSC structure...
        It takes 2.082 seconds.
Building the CSC structure...
        It takes 2.379 seconds.
Building the CSC structure...
        It takes 2.394 seconds.
Building the CSC structure...
        It takes 2.448 seconds.
Building the CSC structure...
        It takes 2.456 seconds.
Building the CSC structure...
        It takes 1.843 seconds.
        It takes 1.834 seconds.
        It takes 1.853 seconds.
        It takes 1.894 seconds.
        It takes 2.361 seconds.
        It takes 2.351 seconds.
        It takes 2.356 seconds.
        It takes 2.467 seconds.
Building the Feature Vector...
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.310 seconds.
Building the Label Vector...
        It takes 0.304 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.039 seconds.
        It takes 0.034 seconds.
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.289 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
        It takes 0.246 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.981 Gbps (per GPU), 463.851 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.681 Gbps (per GPU), 461.447 Gbps (aggregated)
The layer-level communication performance: 57.677 Gbps (per GPU), 461.415 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.441 Gbps (per GPU), 459.532 Gbps (aggregated)
The layer-level communication performance: 57.415 Gbps (per GPU), 459.321 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.211 Gbps (per GPU), 457.692 Gbps (aggregated)
The layer-level communication performance: 57.161 Gbps (per GPU), 457.284 Gbps (aggregated)
The layer-level communication performance: 57.124 Gbps (per GPU), 456.992 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.669 Gbps (per GPU), 1261.350 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.672 Gbps (per GPU), 1261.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.778 Gbps (per GPU), 1262.225 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.675 Gbps (per GPU), 1261.397 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.669 Gbps (per GPU), 1261.350 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.672 Gbps (per GPU), 1261.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.806 Gbps (per GPU), 1262.445 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.672 Gbps (per GPU), 1261.377 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.408 Gbps (per GPU), 803.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.409 Gbps (per GPU), 803.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.409 Gbps (per GPU), 803.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.410 Gbps (per GPU), 803.282 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.409 Gbps (per GPU), 803.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.409 Gbps (per GPU), 803.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.409 Gbps (per GPU), 803.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.410 Gbps (per GPU), 803.282 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 29.352 Gbps (per GPU), 234.817 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.353 Gbps (per GPU), 234.823 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.352 Gbps (per GPU), 234.817 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.352 Gbps (per GPU), 234.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.353 Gbps (per GPU), 234.821 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.352 Gbps (per GPU), 234.815 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.353 Gbps (per GPU), 234.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 29.352 Gbps (per GPU), 234.819 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.30ms  2.57ms  2.41ms  1.37  8.38K  3.53M
 chk_1  3.48ms  2.81ms  2.66ms  1.31  6.74K  3.60M
 chk_2  3.44ms  2.67ms  2.54ms  1.35  7.27K  3.53M
 chk_3  3.44ms  2.72ms  2.56ms  1.34  7.92K  3.61M
 chk_4  3.13ms  2.64ms  2.54ms  1.23  5.33K  3.68M
 chk_5  3.53ms  2.65ms  2.42ms  1.46 10.07K  3.45M
 chk_6  3.68ms  2.82ms  2.59ms  1.42  9.41K  3.48M
 chk_7  3.36ms  2.65ms  2.50ms  1.34  8.12K  3.60M
 chk_8  3.35ms  2.76ms  2.64ms  1.27  6.09K  3.64M
 chk_9  3.61ms  2.56ms  2.47ms  1.46 11.10K  3.38M
chk_10  3.33ms  2.80ms  2.68ms  1.24  5.67K  3.63M
chk_11  3.38ms  2.67ms  2.47ms  1.37  8.16K  3.54M
chk_12  3.57ms  2.86ms  2.72ms  1.31  7.24K  3.55M
chk_13  3.20ms  2.69ms  2.58ms  1.24  5.41K  3.68M
chk_14  3.64ms  2.93ms  2.78ms  1.31  7.14K  3.53M
chk_15  3.63ms  2.79ms  2.59ms  1.40  9.25K  3.49M
chk_16  2.99ms  2.62ms  2.53ms  1.18  4.78K  3.77M
chk_17  3.40ms  2.75ms  2.62ms  1.29  6.85K  3.60M
chk_18  3.24ms  2.56ms  2.43ms  1.33  7.47K  3.57M
chk_19  3.12ms  2.63ms  2.54ms  1.23  4.88K  3.75M
chk_20  3.32ms  2.61ms  2.51ms  1.32  7.00K  3.63M
chk_21  3.12ms  2.61ms  2.53ms  1.24  5.41K  3.68M
chk_22  3.84ms  2.81ms  2.57ms  1.49 11.07K  3.39M
chk_23  3.42ms  2.70ms  2.57ms  1.33  7.23K  3.64M
chk_24  3.68ms  2.77ms  2.55ms  1.45 10.13K  3.43M
chk_25  3.18ms  2.58ms  2.45ms  1.29  6.40K  3.57M
chk_26  3.37ms  2.79ms  2.66ms  1.26  5.78K  3.55M
chk_27  3.50ms  2.67ms  2.45ms  1.43  9.34K  3.48M
chk_28  3.59ms  2.97ms  2.81ms  1.28  6.37K  3.57M
chk_29  3.30ms  2.77ms  2.65ms  1.25  5.16K  3.78M
chk_30  3.19ms  2.65ms  2.55ms  1.25  5.44K  3.67M
chk_31  3.45ms  2.81ms  2.70ms  1.28  6.33K  3.63M
   Avg  3.40  2.72  2.57
   Max  3.84  2.97  2.81
   Min  2.99  2.56  2.41
 Ratio  1.28  1.16  1.16
   Var  0.04  0.01  0.01
Profiling takes 3.254 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 369.452 ms
Partition 0 [0, 4) has cost: 369.452 ms
Partition 1 [4, 8) has cost: 347.554 ms
Partition 2 [8, 12) has cost: 347.554 ms
Partition 3 [12, 16) has cost: 347.554 ms
Partition 4 [16, 20) has cost: 347.554 ms
Partition 5 [20, 24) has cost: 347.554 ms
Partition 6 [24, 28) has cost: 347.554 ms
Partition 7 [28, 32) has cost: 342.975 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 166.448 ms
GPU 0, Compute+Comm Time: 134.016 ms, Bubble Time: 29.209 ms, Imbalance Overhead: 3.223 ms
GPU 1, Compute+Comm Time: 127.635 ms, Bubble Time: 29.018 ms, Imbalance Overhead: 9.794 ms
GPU 2, Compute+Comm Time: 127.635 ms, Bubble Time: 29.018 ms, Imbalance Overhead: 9.795 ms
GPU 3, Compute+Comm Time: 127.635 ms, Bubble Time: 28.905 ms, Imbalance Overhead: 9.908 ms
GPU 4, Compute+Comm Time: 127.635 ms, Bubble Time: 28.898 ms, Imbalance Overhead: 9.914 ms
GPU 5, Compute+Comm Time: 127.635 ms, Bubble Time: 28.911 ms, Imbalance Overhead: 9.902 ms
GPU 6, Compute+Comm Time: 127.635 ms, Bubble Time: 29.106 ms, Imbalance Overhead: 9.707 ms
GPU 7, Compute+Comm Time: 125.882 ms, Bubble Time: 29.543 ms, Imbalance Overhead: 11.022 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 322.277 ms
GPU 0, Compute+Comm Time: 242.808 ms, Bubble Time: 57.535 ms, Imbalance Overhead: 21.934 ms
GPU 1, Compute+Comm Time: 245.634 ms, Bubble Time: 56.637 ms, Imbalance Overhead: 20.006 ms
GPU 2, Compute+Comm Time: 245.634 ms, Bubble Time: 56.188 ms, Imbalance Overhead: 20.455 ms
GPU 3, Compute+Comm Time: 245.634 ms, Bubble Time: 56.161 ms, Imbalance Overhead: 20.482 ms
GPU 4, Compute+Comm Time: 245.634 ms, Bubble Time: 56.108 ms, Imbalance Overhead: 20.535 ms
GPU 5, Compute+Comm Time: 245.634 ms, Bubble Time: 56.317 ms, Imbalance Overhead: 20.326 ms
GPU 6, Compute+Comm Time: 245.634 ms, Bubble Time: 56.234 ms, Imbalance Overhead: 20.409 ms
GPU 7, Compute+Comm Time: 261.151 ms, Bubble Time: 56.673 ms, Imbalance Overhead: 4.452 ms
The estimated cost of the whole pipeline: 513.160 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 717.006 ms
Partition 0 [0, 8) has cost: 717.006 ms
Partition 1 [8, 16) has cost: 695.109 ms
Partition 2 [16, 24) has cost: 695.109 ms
Partition 3 [24, 32) has cost: 690.529 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 172.459 ms
GPU 0, Compute+Comm Time: 143.130 ms, Bubble Time: 27.074 ms, Imbalance Overhead: 2.256 ms
GPU 1, Compute+Comm Time: 139.600 ms, Bubble Time: 26.575 ms, Imbalance Overhead: 6.285 ms
GPU 2, Compute+Comm Time: 139.600 ms, Bubble Time: 26.455 ms, Imbalance Overhead: 6.405 ms
GPU 3, Compute+Comm Time: 138.776 ms, Bubble Time: 26.174 ms, Imbalance Overhead: 7.509 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 321.202 ms
GPU 0, Compute+Comm Time: 257.738 ms, Bubble Time: 49.183 ms, Imbalance Overhead: 14.282 ms
GPU 1, Compute+Comm Time: 259.062 ms, Bubble Time: 49.407 ms, Imbalance Overhead: 12.734 ms
GPU 2, Compute+Comm Time: 259.062 ms, Bubble Time: 49.430 ms, Imbalance Overhead: 12.711 ms
GPU 3, Compute+Comm Time: 267.690 ms, Bubble Time: 50.461 ms, Imbalance Overhead: 3.051 ms
    The estimated cost with 2 DP ways is 518.345 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1412.115 ms
Partition 0 [0, 16) has cost: 1412.115 ms
Partition 1 [16, 32) has cost: 1385.638 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 220.325 ms
GPU 0, Compute+Comm Time: 191.912 ms, Bubble Time: 23.593 ms, Imbalance Overhead: 4.820 ms
GPU 1, Compute+Comm Time: 189.755 ms, Bubble Time: 24.276 ms, Imbalance Overhead: 6.294 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 360.818 ms
GPU 0, Compute+Comm Time: 311.859 ms, Bubble Time: 39.721 ms, Imbalance Overhead: 9.238 ms
GPU 1, Compute+Comm Time: 317.048 ms, Bubble Time: 38.853 ms, Imbalance Overhead: 4.917 ms
    The estimated cost with 4 DP ways is 610.200 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2797.753 ms
Partition 0 [0, 32) has cost: 2797.753 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 631.008 ms
GPU 0, Compute+Comm Time: 631.008 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 757.576 ms
GPU 0, Compute+Comm Time: 757.576 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1458.013 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 162)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [162, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 98)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [194, 226)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [98, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [226, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 66)...
+++++++++ Node 4 initializing the weights for op[130, 162)...
+++++++++ Node 2 initializing the weights for op[66, 98)...
+++++++++ Node 5 initializing the weights for op[162, 194)...
+++++++++ Node 3 initializing the weights for op[98, 130)...
+++++++++ Node 7 initializing the weights for op[226, 257)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 6 initializing the weights for op[194, 226)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.0324	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 50:	Loss 3.2760	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.0836	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 150:	Loss 2.8833	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 200:	Loss 2.7587	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 250:	Loss 2.7265	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 300:	Loss 2.7507	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 350:	Loss 2.6427	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 400:	Loss 2.4026	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 450:	Loss 2.2689	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 500:	Loss 2.1557	TrainAcc 0.0500	ValidAcc 0.0418	TestAcc 0.0452	BestValid 0.1466
	Epoch 550:	Loss 2.0398	TrainAcc 0.0500	ValidAcc 0.0419	TestAcc 0.0453	BestValid 0.1466
	Epoch 600:	Loss 1.9231	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 650:	Loss 1.8522	TrainAcc 0.0691	ValidAcc 0.0585	TestAcc 0.0575	BestValid 0.1466
	Epoch 700:	Loss 1.8330	TrainAcc 0.0691	ValidAcc 0.0585	TestAcc 0.0575	BestValid 0.1466
	Epoch 750:	Loss 1.8068	TrainAcc 0.0695	ValidAcc 0.0593	TestAcc 0.0581	BestValid 0.1466
	Epoch 800:	Loss 1.7305	TrainAcc 0.0692	ValidAcc 0.0586	TestAcc 0.0576	BestValid 0.1466
	Epoch 850:	Loss 1.6489	TrainAcc 0.0707	ValidAcc 0.0602	TestAcc 0.0590	BestValid 0.1466
	Epoch 900:	Loss 1.5881	TrainAcc 0.0932	ValidAcc 0.0822	TestAcc 0.0829	BestValid 0.1466
	Epoch 950:	Loss 1.5385	TrainAcc 0.1411	ValidAcc 0.1221	TestAcc 0.1240	BestValid 0.1466
	Epoch 1000:	Loss 1.4192	TrainAcc 0.1572	ValidAcc 0.1377	TestAcc 0.1397	BestValid 0.1466
	Epoch 1050:	Loss 1.3797	TrainAcc 0.1415	ValidAcc 0.1246	TestAcc 0.1237	BestValid 0.1466
	Epoch 1100:	Loss 1.3876	TrainAcc 0.1609	ValidAcc 0.1444	TestAcc 0.1461	BestValid 0.1466
	Epoch 1150:	Loss 1.3489	TrainAcc 0.1093	ValidAcc 0.1083	TestAcc 0.1088	BestValid 0.1466
	Epoch 1200:	Loss 1.3578	TrainAcc 0.1211	ValidAcc 0.1137	TestAcc 0.1127	BestValid 0.1466
	Epoch 1250:	Loss 1.3233	TrainAcc 0.1034	ValidAcc 0.1004	TestAcc 0.0990	BestValid 0.1466
	Epoch 1300:	Loss 1.3292	TrainAcc 0.1764	ValidAcc 0.2042	TestAcc 0.2050	BestValid 0.2042
	Epoch 1350:	Loss 1.3575	TrainAcc 0.1896	ValidAcc 0.2132	TestAcc 0.2157	BestValid 0.2132
	Epoch 1400:	Loss 1.3766	TrainAcc 0.2572	ValidAcc 0.2758	TestAcc 0.2770	BestValid 0.2758
	Epoch 1450:	Loss 1.7128	TrainAcc 0.1986	ValidAcc 0.2195	TestAcc 0.2191	BestValid 0.2758
	Epoch 1500:	Loss 1.5171	TrainAcc 0.1644	ValidAcc 0.1461	TestAcc 0.1457	BestValid 0.2758
	Epoch 1550:	Loss 1.4790	TrainAcc 0.2695	ValidAcc 0.2963	TestAcc 0.2953	BestValid 0.2963
	Epoch 1600:	Loss 1.4872	TrainAcc 0.2644	ValidAcc 0.2879	TestAcc 0.2873	BestValid 0.2963
	Epoch 1650:	Loss 1.4415	TrainAcc 0.1849	ValidAcc 0.1644	TestAcc 0.1622	BestValid 0.2963
	Epoch 1700:	Loss 1.5060	TrainAcc 0.0841	ValidAcc 0.0701	TestAcc 0.0701	BestValid 0.2963
	Epoch 1750:	Loss 1.6105	TrainAcc 0.0835	ValidAcc 0.0693	TestAcc 0.0668	BestValid 0.2963
	Epoch 1800:	Loss 1.6534	TrainAcc 0.1429	ValidAcc 0.1196	TestAcc 0.1210	BestValid 0.2963
	Epoch 1850:	Loss 1.6609	TrainAcc 0.0961	ValidAcc 0.0801	TestAcc 0.0800	BestValid 0.2963
	Epoch 1900:	Loss 1.5327	TrainAcc 0.1169	ValidAcc 0.0984	TestAcc 0.0986	BestValid 0.2963
	Epoch 1950:	Loss 1.3078	TrainAcc 0.1041	ValidAcc 0.0884	TestAcc 0.0889	BestValid 0.2963
	Epoch 2000:	Loss 1.3033	TrainAcc 0.1153	ValidAcc 0.0970	TestAcc 0.0975	BestValid 0.2963
	Epoch 2050:	Loss 1.2807	TrainAcc 0.1197	ValidAcc 0.1054	TestAcc 0.1057	BestValid 0.2963
	Epoch 2100:	Loss 1.2330	TrainAcc 0.1125	ValidAcc 0.0954	TestAcc 0.0961	BestValid 0.2963
	Epoch 2150:	Loss 1.2631	TrainAcc 0.1122	ValidAcc 0.0951	TestAcc 0.0959	BestValid 0.2963
	Epoch 2200:	Loss 1.2136	TrainAcc 0.1000	ValidAcc 0.0854	TestAcc 0.0854	BestValid 0.2963
	Epoch 2250:	Loss 1.1300	TrainAcc 0.1142	ValidAcc 0.0974	TestAcc 0.0978	BestValid 0.2963
	Epoch 2300:	Loss 1.1434	TrainAcc 0.1184	ValidAcc 0.1003	TestAcc 0.1002	BestValid 0.2963
	Epoch 2350:	Loss 1.1961	TrainAcc 0.1367	ValidAcc 0.1160	TestAcc 0.1150	BestValid 0.2963
	Epoch 2400:	Loss 1.1008	TrainAcc 0.1193	ValidAcc 0.0970	TestAcc 0.0971	BestValid 0.2963
	Epoch 2450:	Loss 1.0733	TrainAcc 0.1329	ValidAcc 0.1129	TestAcc 0.1128	BestValid 0.2963
	Epoch 2500:	Loss 1.0644	TrainAcc 0.1171	ValidAcc 0.0957	TestAcc 0.0964	BestValid 0.2963
	Epoch 2550:	Loss 1.0801	TrainAcc 0.1153	ValidAcc 0.0947	TestAcc 0.0954	BestValid 0.2963
	Epoch 2600:	Loss 1.2123	TrainAcc 0.1027	ValidAcc 0.0862	TestAcc 0.0869	BestValid 0.2963
	Epoch 2650:	Loss 1.1395	TrainAcc 0.1145	ValidAcc 0.0960	TestAcc 0.0967	BestValid 0.2963
	Epoch 2700:	Loss 1.0355	TrainAcc 0.1328	ValidAcc 0.1086	TestAcc 0.1074	BestValid 0.2963
	Epoch 2750:	Loss 0.9994	TrainAcc 0.1417	ValidAcc 0.1198	TestAcc 0.1185	BestValid 0.2963
	Epoch 2800:	Loss 0.9618	TrainAcc 0.1084	ValidAcc 0.0911	TestAcc 0.0921	BestValid 0.2963
	Epoch 2850:	Loss 0.9830	TrainAcc 0.1290	ValidAcc 0.1099	TestAcc 0.1097	BestValid 0.2963
	Epoch 2900:	Loss 0.9677	TrainAcc 0.1247	ValidAcc 0.1063	TestAcc 0.1065	BestValid 0.2963
	Epoch 2950:	Loss 0.9954	TrainAcc 0.1415	ValidAcc 0.1161	TestAcc 0.1141	BestValid 0.2963
	Epoch 3000:	Loss 0.9529	TrainAcc 0.1138	ValidAcc 0.0935	TestAcc 0.0938	BestValid 0.2963
	Epoch 3050:	Loss 0.9233	TrainAcc 0.1193	ValidAcc 0.1018	TestAcc 0.1011	BestValid 0.2963
	Epoch 3100:	Loss 0.8928	TrainAcc 0.1115	ValidAcc 0.0937	TestAcc 0.0945	BestValid 0.2963
	Epoch 3150:	Loss 0.8803	TrainAcc 0.1031	ValidAcc 0.0879	TestAcc 0.0884	BestValid 0.2963
	Epoch 3200:	Loss 0.8768	TrainAcc 0.1231	ValidAcc 0.1041	TestAcc 0.1045	BestValid 0.2963
	Epoch 3250:	Loss 0.8402	TrainAcc 0.0998	ValidAcc 0.0855	TestAcc 0.0859	BestValid 0.2963
	Epoch 3300:	Loss 0.8160	TrainAcc 0.1000	ValidAcc 0.0846	TestAcc 0.0852	BestValid 0.2963
	Epoch 3350:	Loss 0.8160	TrainAcc 0.0957	ValidAcc 0.0825	TestAcc 0.0806	BestValid 0.2963
	Epoch 3400:	Loss 0.7903	TrainAcc 0.0974	ValidAcc 0.0820	TestAcc 0.0829	BestValid 0.2963
	Epoch 3450:	Loss 0.8136	TrainAcc 0.1121	ValidAcc 0.0951	TestAcc 0.0959	BestValid 0.2963
	Epoch 3500:	Loss 0.9189	TrainAcc 0.0933	ValidAcc 0.0787	TestAcc 0.0792	BestValid 0.2963
	Epoch 3550:	Loss 0.8270	TrainAcc 0.1050	ValidAcc 0.0887	TestAcc 0.0898	BestValid 0.2963
	Epoch 3600:	Loss 0.7925	TrainAcc 0.1136	ValidAcc 0.0966	TestAcc 0.0974	BestValid 0.2963
	Epoch 3650:	Loss 0.7772	TrainAcc 0.1037	ValidAcc 0.0852	TestAcc 0.0859	BestValid 0.2963
	Epoch 3700:	Loss 0.7655	TrainAcc 0.1222	ValidAcc 0.1051	TestAcc 0.1052	BestValid 0.2963
	Epoch 3750:	Loss 0.8556	TrainAcc 0.0879	ValidAcc 0.0737	TestAcc 0.0742	BestValid 0.2963
	Epoch 3800:	Loss 0.7568	TrainAcc 0.1061	ValidAcc 0.0898	TestAcc 0.0904	BestValid 0.2963
	Epoch 3850:	Loss 0.7578	TrainAcc 0.0985	ValidAcc 0.0827	TestAcc 0.0836	BestValid 0.2963
	Epoch 3900:	Loss 0.7353	TrainAcc 0.1064	ValidAcc 0.0898	TestAcc 0.0909	BestValid 0.2963
	Epoch 3950:	Loss 0.7488	TrainAcc 0.0983	ValidAcc 0.0823	TestAcc 0.0833	BestValid 0.2963
	Epoch 4000:	Loss 0.7332	TrainAcc 0.1049	ValidAcc 0.0886	TestAcc 0.0894	BestValid 0.2963
	Epoch 4050:	Loss 0.7416	TrainAcc 0.1415	ValidAcc 0.1227	TestAcc 0.1216	BestValid 0.2963
	Epoch 4100:	Loss 0.7221	TrainAcc 0.0850	ValidAcc 0.0712	TestAcc 0.0719	BestValid 0.2963
	Epoch 4150:	Loss 0.7255	TrainAcc 0.0888	ValidAcc 0.0743	TestAcc 0.0750	BestValid 0.2963
	Epoch 4200:	Loss 0.7005	TrainAcc 0.1021	ValidAcc 0.0863	TestAcc 0.0875	BestValid 0.2963
	Epoch 4250:	Loss 0.6971	TrainAcc 0.1050	ValidAcc 0.0887	TestAcc 0.0895	BestValid 0.2963
	Epoch 4300:	Loss 0.6793	TrainAcc 0.1005	ValidAcc 0.0848	TestAcc 0.0859	BestValid 0.2963
	Epoch 4350:	Loss 0.6866	TrainAcc 0.1161	ValidAcc 0.0978	TestAcc 0.0989	BestValid 0.2963
	Epoch 4400:	Loss 0.7114	TrainAcc 0.1117	ValidAcc 0.0945	TestAcc 0.0955	BestValid 0.2963
	Epoch 4450:	Loss 0.7462	TrainAcc 0.1044	ValidAcc 0.0882	TestAcc 0.0892	BestValid 0.2963
	Epoch 4500:	Loss 0.7098	TrainAcc 0.1047	ValidAcc 0.0879	TestAcc 0.0886	BestValid 0.2963
	Epoch 4550:	Loss 0.6740	TrainAcc 0.0976	ValidAcc 0.0819	TestAcc 0.0828	BestValid 0.2963
	Epoch 4600:	Loss 0.6701	TrainAcc 0.1016	ValidAcc 0.0858	TestAcc 0.0865	BestValid 0.2963
	Epoch 4650:	Loss 0.6929	TrainAcc 0.1060	ValidAcc 0.0888	TestAcc 0.0894	BestValid 0.2963
	Epoch 4700:	Loss 0.6886	TrainAcc 0.0986	ValidAcc 0.0830	TestAcc 0.0836	BestValid 0.2963
	Epoch 4750:	Loss 0.6805	TrainAcc 0.1014	ValidAcc 0.0846	TestAcc 0.0853	BestValid 0.2963
	Epoch 4800:	Loss 0.6495	TrainAcc 0.0710	ValidAcc 0.0596	TestAcc 0.0587	BestValid 0.2963
	Epoch 4850:	Loss 0.6463	TrainAcc 0.0922	ValidAcc 0.0774	TestAcc 0.0776	BestValid 0.2963
	Epoch 4900:	Loss 0.6449	TrainAcc 0.1004	ValidAcc 0.0829	TestAcc 0.0830	BestValid 0.2963
	Epoch 4950:	Loss 0.6386	TrainAcc 0.0918	ValidAcc 0.0761	TestAcc 0.0770	BestValid 0.2963
	Epoch 5000:	Loss 0.6283	TrainAcc 0.0997	ValidAcc 0.0831	TestAcc 0.0844	BestValid 0.2963
****** Epoch Time (Excluding Evaluation Cost): 0.531 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 97.624 ms (Max: 100.354, Min: 85.194, Sum: 780.992)
Cluster-Wide Average, Compute: 376.318 ms (Max: 403.015, Min: 361.811, Sum: 3010.542)
Cluster-Wide Average, Communication-Layer: 25.673 ms (Max: 31.051, Min: 16.885, Sum: 205.385)
Cluster-Wide Average, Bubble-Imbalance: 27.729 ms (Max: 36.419, Min: 10.608, Sum: 221.830)
Cluster-Wide Average, Communication-Graph: 0.510 ms (Max: 0.648, Min: 0.450, Sum: 4.079)
Cluster-Wide Average, Optimization: 0.186 ms (Max: 0.202, Min: 0.176, Sum: 1.489)
Cluster-Wide Average, Others: 3.807 ms (Max: 17.260, Min: 1.867, Sum: 30.457)
****** Breakdown Sum: 531.847 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.352 GB (Max: 6.575, Min: 5.006, Sum: 42.815)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 50.245 Gbps (Max: 56.167, Min: 43.804, Sum: 401.961)
Layer-level communication (cluster-wide, per-epoch): 1.215 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 1.215 GB
****** Accuracy Results ******
Highest valid_acc: 0.2963
Target test_acc: 0.2953
Epoch to reach the target acc: 1549
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
