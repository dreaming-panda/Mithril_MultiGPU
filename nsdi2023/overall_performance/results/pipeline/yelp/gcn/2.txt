Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2

Initialized node 1 on machine gnerv2
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.276 seconds.
Building the CSC structure...
        It takes 0.291 seconds.
Building the CSC structure...
        It takes 0.304 seconds.
Building the CSC structure...
        It takes 0.305 seconds.
Building the CSC structure...
        It takes 0.311 seconds.
Building the CSC structure...
        It takes 0.315 seconds.
Building the CSC structure...
        It takes 0.327 seconds.
Building the CSC structure...
        It takes 0.334 seconds.
Building the CSC structure...
        It takes 0.271 seconds.
        It takes 0.284 seconds.
        It takes 0.284 seconds.
        It takes 0.290 seconds.
        It takes 0.308 seconds.
        It takes 0.315 seconds.
        It takes 0.311 seconds.
        It takes 0.329 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.478 seconds.
Building the Label Vector...
        It takes 0.480 seconds.
Building the Label Vector...
        It takes 0.475 seconds.
Building the Label Vector...
        It takes 0.538 seconds.
Building the Label Vector...
        It takes 0.499 seconds.
Building the Label Vector...
        It takes 0.492 seconds.
Building the Label Vector...
        It takes 0.541 seconds.
Building the Label Vector...
        It takes 0.520 seconds.
Building the Label Vector...
        It takes 0.189 seconds.
        It takes 0.181 seconds.
        It takes 0.187 seconds.
        It takes 0.182 seconds.
        It takes 0.198 seconds.
        It takes 0.190 seconds.
        It takes 0.198 seconds.
        It takes 0.198 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/yelp/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 100
Number of feature dimensions: 300
Number of vertices: 716847
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
train nodes 537635, valid nodes 107527, test nodes 71685
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 24122) 1-[24122, 44991) 2-[44991, 66905) 3-[66905, 90565) 4-[90565, 109350) 5-[109350, 132203) 6-[132203, 154486) 7-[154486, 177346) 8-[177346, 198991) ... 31-[695934, 716847)
716847, 13954819, 13954819
Number of vertices per chunk: 22402
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.892 Gbps (per GPU), 479.138 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.598 Gbps (per GPU), 476.781 Gbps (aggregated)
The layer-level communication performance: 59.593 Gbps (per GPU), 476.746 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.320 Gbps (per GPU), 474.562 Gbps (aggregated)
The layer-level communication performance: 59.293 Gbps (per GPU), 474.341 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.089 Gbps (per GPU), 472.712 Gbps (aggregated)
The layer-level communication performance: 59.036 Gbps (per GPU), 472.285 Gbps (aggregated)
The layer-level communication performance: 59.010 Gbps (per GPU), 472.083 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.290 Gbps (per GPU), 1266.322 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.282 Gbps (per GPU), 1266.253 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.291 Gbps (per GPU), 1266.324 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.285 Gbps (per GPU), 1266.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.288 Gbps (per GPU), 1266.301 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.291 Gbps (per GPU), 1266.324 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.276 Gbps (per GPU), 1266.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.282 Gbps (per GPU), 1266.252 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 93.990 Gbps (per GPU), 751.920 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.982 Gbps (per GPU), 751.853 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.992 Gbps (per GPU), 751.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.989 Gbps (per GPU), 751.914 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.991 Gbps (per GPU), 751.926 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.978 Gbps (per GPU), 751.825 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.991 Gbps (per GPU), 751.931 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.991 Gbps (per GPU), 751.926 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.990 Gbps (per GPU), 279.920 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.989 Gbps (per GPU), 279.914 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.990 Gbps (per GPU), 279.922 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.989 Gbps (per GPU), 279.909 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.990 Gbps (per GPU), 279.920 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.989 Gbps (per GPU), 279.914 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.988 Gbps (per GPU), 279.903 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.988 Gbps (per GPU), 279.906 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.48ms  1.21ms  0.95ms  1.56 24.12K  0.35M
 chk_1  1.56ms  1.28ms  1.03ms  1.51 20.87K  0.49M
 chk_2  1.44ms  1.16ms  0.90ms  1.60 21.91K  0.46M
 chk_3  1.51ms  1.23ms  0.96ms  1.57 23.66K  0.35M
 chk_4  1.45ms  1.20ms  0.96ms  1.50 18.79K  0.62M
 chk_5  1.48ms  1.20ms  0.94ms  1.58 22.85K  0.37M
 chk_6  1.52ms  1.24ms  0.98ms  1.55 22.28K  0.40M
 chk_7  1.47ms  1.19ms  0.92ms  1.59 22.86K  0.39M
 chk_8  1.49ms  1.22ms  0.96ms  1.56 21.64K  0.45M
 chk_9  1.44ms  1.17ms  0.90ms  1.60 22.92K  0.37M
chk_10  1.53ms  1.27ms  1.03ms  1.49 20.30K  0.56M
chk_11  1.49ms  1.22ms  0.95ms  1.58 23.32K  0.33M
chk_12  1.53ms  1.25ms  1.00ms  1.53 21.10K  0.49M
chk_13  1.48ms  1.20ms  0.95ms  1.55 20.79K  0.49M
chk_14  1.45ms  1.17ms  0.90ms  1.61 23.53K  0.36M
chk_15  1.40ms  1.13ms  0.86ms  1.64 23.21K  0.39M
chk_16  1.43ms  1.15ms  0.87ms  1.64 24.39K  0.32M
chk_17  1.48ms  1.20ms  0.92ms  1.60 23.94K  0.34M
chk_18  1.49ms  1.21ms  0.95ms  1.56 21.61K  0.45M
chk_19  1.45ms  1.18ms  0.90ms  1.62 23.89K  0.34M
chk_20  1.53ms  1.24ms  0.99ms  1.54 21.64K  0.47M
chk_21  1.48ms  1.20ms  0.93ms  1.59 23.43K  0.34M
chk_22  1.47ms  1.19ms  0.92ms  1.59 22.84K  0.37M
chk_23  1.49ms  1.21ms  0.94ms  1.59 23.35K  0.37M
chk_24  1.50ms  1.22ms  0.96ms  1.56 22.72K  0.40M
chk_25  1.54ms  1.26ms  1.00ms  1.54 21.95K  0.46M
chk_26  1.53ms  1.24ms  0.98ms  1.55 22.06K  0.41M
chk_27  1.47ms  1.19ms  0.92ms  1.60 23.02K  0.35M
chk_28  1.48ms  1.20ms  0.93ms  1.59 22.97K  0.36M
chk_29  1.45ms  1.16ms  0.91ms  1.59 22.14K  0.45M
chk_30  1.52ms  1.24ms  0.98ms  1.55 21.84K  0.44M
chk_31  1.53ms  1.25ms  1.00ms  1.52 20.91K  0.49M
   Avg  1.49  1.21  0.95
   Max  1.56  1.28  1.03
   Min  1.40  1.13  0.86
 Ratio  1.11  1.13  1.21
   Var  0.00  0.00  0.00
Profiling takes 1.448 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 159)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
+++++++++ Node 7 initializing the weights for op[141, 159)...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 4 initializing the weights for op[81, 101)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 0.6931	TrainAcc 0.1661	ValidAcc 0.1650	TestAcc 0.1650	BestValid 0.1650
	Epoch 50:	Loss 0.5325	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.1650
	Epoch 100:	Loss 0.3759	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.1650
	Epoch 150:	Loss 0.3182	TrainAcc 0.3069	ValidAcc 0.3067	TestAcc 0.3066	BestValid 0.3067
	Epoch 200:	Loss 0.3065	TrainAcc 0.3195	ValidAcc 0.3195	TestAcc 0.3189	BestValid 0.3195
	Epoch 250:	Loss 0.3052	TrainAcc 0.3189	ValidAcc 0.3183	TestAcc 0.3182	BestValid 0.3195
	Epoch 300:	Loss 0.3068	TrainAcc 0.3119	ValidAcc 0.3114	TestAcc 0.3109	BestValid 0.3195
	Epoch 350:	Loss 0.3057	TrainAcc 0.3014	ValidAcc 0.3008	TestAcc 0.3005	BestValid 0.3195
	Epoch 400:	Loss 0.3023	TrainAcc 0.2908	ValidAcc 0.2900	TestAcc 0.2899	BestValid 0.3195
	Epoch 450:	Loss 0.3042	TrainAcc 0.3037	ValidAcc 0.3029	TestAcc 0.3027	BestValid 0.3195
	Epoch 500:	Loss 0.3019	TrainAcc 0.2877	ValidAcc 0.2867	TestAcc 0.2866	BestValid 0.3195
	Epoch 550:	Loss 0.3028	TrainAcc 0.2855	ValidAcc 0.2844	TestAcc 0.2844	BestValid 0.3195
	Epoch 600:	Loss 0.3094	TrainAcc 0.2798	ValidAcc 0.2788	TestAcc 0.2788	BestValid 0.3195
	Epoch 650:	Loss 0.3047	TrainAcc 0.2779	ValidAcc 0.2769	TestAcc 0.2768	BestValid 0.3195
	Epoch 700:	Loss 0.3018	TrainAcc 0.2749	ValidAcc 0.2739	TestAcc 0.2738	BestValid 0.3195
	Epoch 750:	Loss 0.3033	TrainAcc 0.2718	ValidAcc 0.2708	TestAcc 0.2707	BestValid 0.3195
	Epoch 800:	Loss 0.3059	TrainAcc 0.2390	ValidAcc 0.2393	TestAcc 0.2393	BestValid 0.3195
	Epoch 850:	Loss 0.3050	TrainAcc 0.2798	ValidAcc 0.2788	TestAcc 0.2787	BestValid 0.3195
	Epoch 900:	Loss 0.3046	TrainAcc 0.2610	ValidAcc 0.2600	TestAcc 0.2599	BestValid 0.3195
	Epoch 950:	Loss 0.3112	TrainAcc 0.2610	ValidAcc 0.2600	TestAcc 0.2599	BestValid 0.3195
	Epoch 1000:	Loss 0.3110	TrainAcc 0.2567	ValidAcc 0.2556	TestAcc 0.2555	BestValid 0.3195
	Epoch 1050:	Loss 0.3079	TrainAcc 0.2475	ValidAcc 0.2463	TestAcc 0.2463	BestValid 0.3195
	Epoch 1100:	Loss 0.3057	TrainAcc 0.2545	ValidAcc 0.2534	TestAcc 0.2534	BestValid 0.3195
	Epoch 1150:	Loss 0.3088	TrainAcc 0.2855	ValidAcc 0.2846	TestAcc 0.2845	BestValid 0.3195
	Epoch 1200:	Loss 0.3128	TrainAcc 0.2443	ValidAcc 0.2432	TestAcc 0.2432	BestValid 0.3195
	Epoch 1250:	Loss 0.3061	TrainAcc 0.2510	ValidAcc 0.2499	TestAcc 0.2499	BestValid 0.3195
	Epoch 1300:	Loss 0.3173	TrainAcc 0.2798	ValidAcc 0.2789	TestAcc 0.2787	BestValid 0.3195
	Epoch 1350:	Loss 0.3197	TrainAcc 0.2576	ValidAcc 0.2564	TestAcc 0.2565	BestValid 0.3195
	Epoch 1400:	Loss 0.3117	TrainAcc 0.2516	ValidAcc 0.2503	TestAcc 0.2505	BestValid 0.3195
	Epoch 1450:	Loss 0.3338	TrainAcc 0.2568	ValidAcc 0.2556	TestAcc 0.2557	BestValid 0.3195
	Epoch 1500:	Loss 0.3830	TrainAcc 0.1414	ValidAcc 0.1421	TestAcc 0.1418	BestValid 0.3195
	Epoch 1550:	Loss 0.4432	TrainAcc 0.1413	ValidAcc 0.1419	TestAcc 0.1419	BestValid 0.3195
	Epoch 1600:	Loss 0.3912	TrainAcc 0.1870	ValidAcc 0.1869	TestAcc 0.1877	BestValid 0.3195
	Epoch 1650:	Loss 0.3575	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3195
	Epoch 1700:	Loss 0.3402	TrainAcc 0.2254	ValidAcc 0.2251	TestAcc 0.2261	BestValid 0.3195
	Epoch 1750:	Loss 0.3157	TrainAcc 0.2836	ValidAcc 0.2825	TestAcc 0.2826	BestValid 0.3195
	Epoch 1800:	Loss 0.3100	TrainAcc 0.2604	ValidAcc 0.2591	TestAcc 0.2592	BestValid 0.3195
	Epoch 1850:	Loss 0.3081	TrainAcc 0.2568	ValidAcc 0.2555	TestAcc 0.2556	BestValid 0.3195
	Epoch 1900:	Loss 0.3129	TrainAcc 0.2542	ValidAcc 0.2530	TestAcc 0.2531	BestValid 0.3195
	Epoch 1950:	Loss 0.3465	TrainAcc 0.3058	ValidAcc 0.3047	TestAcc 0.3051	BestValid 0.3195
	Epoch 2000:	Loss 0.3385	TrainAcc 0.3140	ValidAcc 0.3133	TestAcc 0.3139	BestValid 0.3195
	Epoch 2050:	Loss 0.3125	TrainAcc 0.2591	ValidAcc 0.2578	TestAcc 0.2579	BestValid 0.3195
	Epoch 2100:	Loss 0.3086	TrainAcc 0.2586	ValidAcc 0.2573	TestAcc 0.2574	BestValid 0.3195
	Epoch 2150:	Loss 0.3085	TrainAcc 0.2551	ValidAcc 0.2538	TestAcc 0.2540	BestValid 0.3195
	Epoch 2200:	Loss 0.3071	TrainAcc 0.2516	ValidAcc 0.2504	TestAcc 0.2506	BestValid 0.3195
	Epoch 2250:	Loss 0.3198	TrainAcc 0.2542	ValidAcc 0.2530	TestAcc 0.2531	BestValid 0.3195
	Epoch 2300:	Loss 0.3402	TrainAcc 0.3112	ValidAcc 0.3102	TestAcc 0.3109	BestValid 0.3195
	Epoch 2350:	Loss 0.3149	TrainAcc 0.2639	ValidAcc 0.2627	TestAcc 0.2628	BestValid 0.3195
	Epoch 2400:	Loss 0.3125	TrainAcc 0.2586	ValidAcc 0.2573	TestAcc 0.2574	BestValid 0.3195
	Epoch 2450:	Loss 0.3508	TrainAcc 0.3149	ValidAcc 0.3142	TestAcc 0.3145	BestValid 0.3195
	Epoch 2500:	Loss 0.3640	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3195
	Epoch 2550:	Loss 0.3403	TrainAcc 0.3184	ValidAcc 0.3178	TestAcc 0.3180	BestValid 0.3195
	Epoch 2600:	Loss 0.3094	TrainAcc 0.2591	ValidAcc 0.2578	TestAcc 0.2579	BestValid 0.3195
	Epoch 2650:	Loss 0.3351	TrainAcc 0.3022	ValidAcc 0.3011	TestAcc 0.3014	BestValid 0.3195
	Epoch 2700:	Loss 0.3201	TrainAcc 0.2604	ValidAcc 0.2591	TestAcc 0.2592	BestValid 0.3195
	Epoch 2750:	Loss 0.3078	TrainAcc 0.2586	ValidAcc 0.2573	TestAcc 0.2574	BestValid 0.3195
	Epoch 2800:	Loss 0.3157	TrainAcc 0.2542	ValidAcc 0.2530	TestAcc 0.2531	BestValid 0.3195
	Epoch 2850:	Loss 0.3738	TrainAcc 0.2978	ValidAcc 0.2967	TestAcc 0.2970	BestValid 0.3195
	Epoch 2900:	Loss 0.3598	TrainAcc 0.1861	ValidAcc 0.1861	TestAcc 0.1870	BestValid 0.3195
	Epoch 2950:	Loss 0.3146	TrainAcc 0.2670	ValidAcc 0.2658	TestAcc 0.2660	BestValid 0.3195
	Epoch 3000:	Loss 0.3084	TrainAcc 0.2576	ValidAcc 0.2564	TestAcc 0.2565	BestValid 0.3195
	Epoch 3050:	Loss 0.3248	TrainAcc 0.2559	ValidAcc 0.2547	TestAcc 0.2548	BestValid 0.3195
	Epoch 3100:	Loss 0.3324	TrainAcc 0.3069	ValidAcc 0.3060	TestAcc 0.3064	BestValid 0.3195
	Epoch 3150:	Loss 0.3185	TrainAcc 0.2604	ValidAcc 0.2591	TestAcc 0.2592	BestValid 0.3195
	Epoch 3200:	Loss 0.3143	TrainAcc 0.2542	ValidAcc 0.2530	TestAcc 0.2531	BestValid 0.3195
	Epoch 3250:	Loss 0.3198	TrainAcc 0.2559	ValidAcc 0.2547	TestAcc 0.2548	BestValid 0.3195
	Epoch 3300:	Loss 0.3098	TrainAcc 0.2559	ValidAcc 0.2547	TestAcc 0.2548	BestValid 0.3195
	Epoch 3350:	Loss 0.3089	TrainAcc 0.2516	ValidAcc 0.2504	TestAcc 0.2506	BestValid 0.3195
	Epoch 3400:	Loss 0.3326	TrainAcc 0.2559	ValidAcc 0.2547	TestAcc 0.2548	BestValid 0.3195
	Epoch 3450:	Loss 0.3307	TrainAcc 0.3067	ValidAcc 0.3057	TestAcc 0.3064	BestValid 0.3195
	Epoch 3500:	Loss 0.3077	TrainAcc 0.2542	ValidAcc 0.2530	TestAcc 0.2531	BestValid 0.3195
	Epoch 3550:	Loss 0.3209	TrainAcc 0.2604	ValidAcc 0.2591	TestAcc 0.2592	BestValid 0.3195
	Epoch 3600:	Loss 0.3203	TrainAcc 0.3149	ValidAcc 0.3142	TestAcc 0.3145	BestValid 0.3195
	Epoch 3650:	Loss 0.3110	TrainAcc 0.2516	ValidAcc 0.2504	TestAcc 0.2506	BestValid 0.3195
	Epoch 3700:	Loss 0.3071	TrainAcc 0.2499	ValidAcc 0.2486	TestAcc 0.2489	BestValid 0.3195
	Epoch 3750:	Loss 0.3710	TrainAcc 0.2757	ValidAcc 0.2746	TestAcc 0.2748	BestValid 0.3195
	Epoch 3800:	Loss 0.3361	TrainAcc 0.2885	ValidAcc 0.2874	TestAcc 0.2876	BestValid 0.3195
	Epoch 3850:	Loss 0.3122	TrainAcc 0.2551	ValidAcc 0.2538	TestAcc 0.2540	BestValid 0.3195
	Epoch 3900:	Loss 0.3122	TrainAcc 0.2474	ValidAcc 0.2461	TestAcc 0.2464	BestValid 0.3195
	Epoch 3950:	Loss 0.3271	TrainAcc 0.2568	ValidAcc 0.2556	TestAcc 0.2557	BestValid 0.3195
	Epoch 4000:	Loss 0.3185	TrainAcc 0.2809	ValidAcc 0.2798	TestAcc 0.2800	BestValid 0.3195
	Epoch 4050:	Loss 0.3074	TrainAcc 0.2466	ValidAcc 0.2454	TestAcc 0.2457	BestValid 0.3195
	Epoch 4100:	Loss 0.3125	TrainAcc 0.2434	ValidAcc 0.2422	TestAcc 0.2425	BestValid 0.3195
	Epoch 4150:	Loss 0.3109	TrainAcc 0.2434	ValidAcc 0.2422	TestAcc 0.2425	BestValid 0.3195
	Epoch 4200:	Loss 0.3064	TrainAcc 0.2434	ValidAcc 0.2422	TestAcc 0.2425	BestValid 0.3195
	Epoch 4250:	Loss 0.3164	TrainAcc 0.2400	ValidAcc 0.2387	TestAcc 0.2390	BestValid 0.3195
	Epoch 4300:	Loss 0.3299	TrainAcc 0.3041	ValidAcc 0.3031	TestAcc 0.3036	BestValid 0.3195
	Epoch 4350:	Loss 0.3129	TrainAcc 0.2466	ValidAcc 0.2454	TestAcc 0.2457	BestValid 0.3195
	Epoch 4400:	Loss 0.3126	TrainAcc 0.2400	ValidAcc 0.2387	TestAcc 0.2390	BestValid 0.3195
	Epoch 4450:	Loss 0.3399	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3195
	Epoch 4500:	Loss 0.3251	TrainAcc 0.2499	ValidAcc 0.2486	TestAcc 0.2489	BestValid 0.3195
	Epoch 4550:	Loss 0.3093	TrainAcc 0.2466	ValidAcc 0.2454	TestAcc 0.2457	BestValid 0.3195
	Epoch 4600:	Loss 0.3192	TrainAcc 0.2506	ValidAcc 0.2494	TestAcc 0.2496	BestValid 0.3195
	Epoch 4650:	Loss 0.3449	TrainAcc 0.2563	ValidAcc 0.2558	TestAcc 0.2570	BestValid 0.3195
	Epoch 4700:	Loss 0.3166	TrainAcc 0.2551	ValidAcc 0.2538	TestAcc 0.2540	BestValid 0.3195
	Epoch 4750:	Loss 0.3075	TrainAcc 0.2499	ValidAcc 0.2486	TestAcc 0.2489	BestValid 0.3195
	Epoch 4800:	Loss 0.3147	TrainAcc 0.2465	ValidAcc 0.2453	TestAcc 0.2455	BestValid 0.3195
	Epoch 4850:	Loss 0.3384	TrainAcc 0.2848	ValidAcc 0.2837	TestAcc 0.2839	BestValid 0.3195
	Epoch 4900:	Loss 0.3217	TrainAcc 0.2656	ValidAcc 0.2644	TestAcc 0.2646	BestValid 0.3195
	Epoch 4950:	Loss 0.3066	TrainAcc 0.2499	ValidAcc 0.2486	TestAcc 0.2489	BestValid 0.3195
	Epoch 5000:	Loss 0.3200	TrainAcc 0.2741	ValidAcc 0.2729	TestAcc 0.2730	BestValid 0.3195
****** Epoch Time (Excluding Evaluation Cost): 0.263 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 43.534 ms (Max: 45.074, Min: 41.775, Sum: 348.269)
Cluster-Wide Average, Compute: 141.365 ms (Max: 151.371, Min: 138.208, Sum: 1130.923)
Cluster-Wide Average, Communication-Layer: 48.294 ms (Max: 69.246, Min: 29.630, Sum: 386.352)
Cluster-Wide Average, Bubble-Imbalance: 24.280 ms (Max: 43.704, Min: 5.481, Sum: 194.239)
Cluster-Wide Average, Communication-Graph: 0.443 ms (Max: 0.493, Min: 0.388, Sum: 3.545)
Cluster-Wide Average, Optimization: 0.089 ms (Max: 0.091, Min: 0.086, Sum: 0.714)
Cluster-Wide Average, Others: 5.830 ms (Max: 9.629, Min: 5.198, Sum: 46.639)
****** Breakdown Sum: 263.835 ms ******
Cluster-Wide Average, GPU Memory Consumption: 8.303 GB (Max: 9.751, Min: 8.047, Sum: 66.423)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 85.146 Gbps (Max: 110.131, Min: 66.254, Sum: 681.164)
Layer-level communication (cluster-wide, per-epoch): 3.739 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 3.739 GB
****** Accuracy Results ******
Highest valid_acc: 0.3195
Target test_acc: 0.3189
Epoch to reach the target acc: 199
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
