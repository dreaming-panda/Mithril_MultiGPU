Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 0 on machine gnerv2
DONE MPI INIT
DONE MPI INIT

Initialized node 3 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.373 seconds.
Building the CSC structure...
        It takes 0.373 seconds.
Building the CSC structure...
        It takes 0.373 seconds.
Building the CSC structure...
        It takes 0.387 seconds.
Building the CSC structure...
        It takes 0.418 seconds.
Building the CSC structure...
        It takes 0.419 seconds.
Building the CSC structure...
        It takes 0.419 seconds.
Building the CSC structure...
        It takes 0.420 seconds.
Building the CSC structure...
        It takes 0.283 seconds.
        It takes 0.286 seconds.
        It takes 0.288 seconds.
        It takes 0.290 seconds.
        It takes 0.340 seconds.
        It takes 0.341 seconds.
        It takes 0.342 seconds.
        It takes 0.343 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 1.822 seconds.
        It takes 1.946 seconds.
        It takes 1.924 seconds.
        It takes 1.948 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 2.331 seconds.
        It takes 2.345 seconds.
        It takes 2.325 seconds.
        It takes 2.356 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.695 seconds.
        It takes 0.695 seconds.
        It takes 0.695 seconds.
        It takes 0.695 seconds.
        It takes 0.817 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/yelp/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 100
Number of feature dimensions: 300
Number of vertices: 716847
Number of GPUs: 8
        It takes 0.817 seconds.
        It takes 0.817 seconds.
        It takes 0.818 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
716847, 13954819, 13954819
Number of vertices per chunk: 22402
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 537635, valid nodes 107527, test nodes 71685
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 24122) 1-[24122, 44991) 2-[44991, 66905) 3-[66905, 90565) 4-[90565, 109350) 5-[109350, 132203) 6-[132203, 154486) 7-[154486, 177346) 8-[177346, 198991) ... 31-[695934, 716847)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.427 Gbps (per GPU), 451.414 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.193 Gbps (per GPU), 449.541 Gbps (aggregated)
The layer-level communication performance: 56.182 Gbps (per GPU), 449.456 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.971 Gbps (per GPU), 447.771 Gbps (aggregated)
The layer-level communication performance: 55.946 Gbps (per GPU), 447.572 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.763 Gbps (per GPU), 446.101 Gbps (aggregated)
The layer-level communication performance: 55.717 Gbps (per GPU), 445.740 Gbps (aggregated)
The layer-level communication performance: 55.692 Gbps (per GPU), 445.540 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.809 Gbps (per GPU), 1246.473 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.812 Gbps (per GPU), 1246.496 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.763 Gbps (per GPU), 1246.103 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.803 Gbps (per GPU), 1246.427 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.803 Gbps (per GPU), 1246.427 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.800 Gbps (per GPU), 1246.404 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.792 Gbps (per GPU), 1246.334 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.800 Gbps (per GPU), 1246.404 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.142 Gbps (per GPU), 793.136 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.140 Gbps (per GPU), 793.124 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.144 Gbps (per GPU), 793.149 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.143 Gbps (per GPU), 793.143 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.144 Gbps (per GPU), 793.149 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.133 Gbps (per GPU), 793.068 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.142 Gbps (per GPU), 793.137 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.144 Gbps (per GPU), 793.149 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.472 Gbps (per GPU), 251.779 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.472 Gbps (per GPU), 251.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.471 Gbps (per GPU), 251.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.472 Gbps (per GPU), 251.778 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.471 Gbps (per GPU), 251.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.472 Gbps (per GPU), 251.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.472 Gbps (per GPU), 251.773 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.472 Gbps (per GPU), 251.776 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.48ms  1.21ms  0.95ms  1.56 24.12K  0.35M
 chk_1  1.56ms  1.28ms  1.04ms  1.51 20.87K  0.49M
 chk_2  1.44ms  1.16ms  0.90ms  1.60 21.91K  0.46M
 chk_3  1.51ms  1.23ms  0.96ms  1.57 23.66K  0.35M
 chk_4  1.46ms  1.20ms  0.97ms  1.51 18.79K  0.62M
 chk_5  1.48ms  1.20ms  0.94ms  1.58 22.85K  0.37M
 chk_6  1.52ms  1.24ms  0.97ms  1.56 22.28K  0.40M
 chk_7  1.46ms  1.19ms  0.93ms  1.58 22.86K  0.39M
 chk_8  1.50ms  1.22ms  0.96ms  1.56 21.64K  0.45M
 chk_9  1.44ms  1.17ms  0.90ms  1.60 22.92K  0.37M
chk_10  1.53ms  1.27ms  1.03ms  1.49 20.30K  0.56M
chk_11  1.50ms  1.22ms  0.95ms  1.58 23.32K  0.33M
chk_12  1.53ms  1.25ms  1.00ms  1.53 21.10K  0.49M
chk_13  1.48ms  1.20ms  0.95ms  1.55 20.79K  0.49M
chk_14  1.45ms  1.17ms  0.90ms  1.61 23.53K  0.36M
chk_15  1.40ms  1.13ms  0.86ms  1.63 23.21K  0.39M
chk_16  1.43ms  1.15ms  0.87ms  1.64 24.39K  0.32M
chk_17  1.47ms  1.20ms  0.92ms  1.60 23.94K  0.34M
chk_18  1.49ms  1.21ms  0.95ms  1.57 21.61K  0.45M
chk_19  1.45ms  1.18ms  0.90ms  1.61 23.89K  0.34M
chk_20  1.53ms  1.25ms  0.99ms  1.54 21.64K  0.47M
chk_21  1.48ms  1.20ms  0.93ms  1.59 23.43K  0.34M
chk_22  1.47ms  1.20ms  0.93ms  1.59 22.84K  0.37M
chk_23  1.49ms  1.21ms  0.94ms  1.58 23.35K  0.37M
chk_24  1.50ms  1.22ms  0.96ms  1.56 22.72K  0.40M
chk_25  1.54ms  1.26ms  1.00ms  1.54 21.95K  0.46M
chk_26  1.53ms  1.25ms  0.98ms  1.55 22.06K  0.41M
chk_27  1.47ms  1.19ms  0.92ms  1.60 23.02K  0.35M
chk_28  1.47ms  1.20ms  0.93ms  1.59 22.97K  0.36M
chk_29  1.45ms  1.17ms  0.91ms  1.59 22.14K  0.45M
chk_30  1.52ms  1.24ms  0.98ms  1.56 21.84K  0.44M
chk_31  1.54ms  1.25ms  1.00ms  1.53 20.91K  0.49M
   Avg  1.49  1.21  0.95
   Max  1.56  1.28  1.04
   Min  1.40  1.13  0.86
 Ratio  1.11  1.13  1.20
   Var  0.00  0.00  0.00
Profiling takes 1.467 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 4, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 4, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 716847
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 159)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 716847
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 4 initializing the weights for op[81, 101)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 7 initializing the weights for op[141, 159)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 0.6931	TrainAcc 0.1952	ValidAcc 0.1943	TestAcc 0.1946	BestValid 0.1943
	Epoch 50:	Loss 0.4558	TrainAcc 0.1951	ValidAcc 0.1955	TestAcc 0.1953	BestValid 0.1955
	Epoch 100:	Loss 0.3624	TrainAcc 0.1951	ValidAcc 0.1955	TestAcc 0.1953	BestValid 0.1955
	Epoch 150:	Loss 0.3235	TrainAcc 0.2735	ValidAcc 0.2737	TestAcc 0.2738	BestValid 0.2737
	Epoch 200:	Loss 0.3100	TrainAcc 0.3174	ValidAcc 0.3171	TestAcc 0.3167	BestValid 0.3171
	Epoch 250:	Loss 0.3063	TrainAcc 0.3319	ValidAcc 0.3318	TestAcc 0.3313	BestValid 0.3318
	Epoch 300:	Loss 0.3164	TrainAcc 0.2390	ValidAcc 0.2393	TestAcc 0.2393	BestValid 0.3318
	Epoch 350:	Loss 0.3092	TrainAcc 0.3161	ValidAcc 0.3157	TestAcc 0.3154	BestValid 0.3318
	Epoch 400:	Loss 0.3064	TrainAcc 0.3167	ValidAcc 0.3160	TestAcc 0.3160	BestValid 0.3318
	Epoch 450:	Loss 0.3068	TrainAcc 0.3242	ValidAcc 0.3236	TestAcc 0.3234	BestValid 0.3318
	Epoch 500:	Loss 0.3102	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3318
	Epoch 550:	Loss 0.3104	TrainAcc 0.3082	ValidAcc 0.3073	TestAcc 0.3074	BestValid 0.3318
	Epoch 600:	Loss 0.3058	TrainAcc 0.2994	ValidAcc 0.2986	TestAcc 0.2985	BestValid 0.3318
	Epoch 650:	Loss 0.3114	TrainAcc 0.2908	ValidAcc 0.2908	TestAcc 0.2909	BestValid 0.3318
	Epoch 700:	Loss 0.3209	TrainAcc 0.1428	ValidAcc 0.1432	TestAcc 0.1431	BestValid 0.3318
	Epoch 750:	Loss 0.3172	TrainAcc 0.2892	ValidAcc 0.2883	TestAcc 0.2881	BestValid 0.3318
	Epoch 800:	Loss 0.3072	TrainAcc 0.2865	ValidAcc 0.2856	TestAcc 0.2853	BestValid 0.3318
	Epoch 850:	Loss 0.3080	TrainAcc 0.3284	ValidAcc 0.3279	TestAcc 0.3274	BestValid 0.3318
	Epoch 900:	Loss 0.3315	TrainAcc 0.3274	ValidAcc 0.3271	TestAcc 0.3271	BestValid 0.3318
	Epoch 950:	Loss 0.3186	TrainAcc 0.2749	ValidAcc 0.2739	TestAcc 0.2738	BestValid 0.3318
	Epoch 1000:	Loss 0.3075	TrainAcc 0.2799	ValidAcc 0.2789	TestAcc 0.2788	BestValid 0.3318
	Epoch 1050:	Loss 0.3190	TrainAcc 0.2735	ValidAcc 0.2737	TestAcc 0.2738	BestValid 0.3318
	Epoch 1100:	Loss 0.3278	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3318
	Epoch 1150:	Loss 0.3214	TrainAcc 0.2799	ValidAcc 0.2789	TestAcc 0.2788	BestValid 0.3318
	Epoch 1200:	Loss 0.3117	TrainAcc 0.2799	ValidAcc 0.2789	TestAcc 0.2788	BestValid 0.3318
	Epoch 1250:	Loss 0.3394	TrainAcc 0.2390	ValidAcc 0.2393	TestAcc 0.2393	BestValid 0.3318
	Epoch 1300:	Loss 0.3541	TrainAcc 0.1417	ValidAcc 0.1419	TestAcc 0.1424	BestValid 0.3318
	Epoch 1350:	Loss 0.3259	TrainAcc 0.3195	ValidAcc 0.3189	TestAcc 0.3186	BestValid 0.3318
	Epoch 1400:	Loss 0.3114	TrainAcc 0.2886	ValidAcc 0.2876	TestAcc 0.2874	BestValid 0.3318
	Epoch 1450:	Loss 0.3703	TrainAcc 0.3318	ValidAcc 0.3317	TestAcc 0.3312	BestValid 0.3318
	Epoch 1500:	Loss 0.3601	TrainAcc 0.3143	ValidAcc 0.3136	TestAcc 0.3142	BestValid 0.3318
	Epoch 1550:	Loss 0.3264	TrainAcc 0.2948	ValidAcc 0.2937	TestAcc 0.2940	BestValid 0.3318
	Epoch 1600:	Loss 0.3169	TrainAcc 0.2832	ValidAcc 0.2820	TestAcc 0.2822	BestValid 0.3318
	Epoch 1650:	Loss 0.3197	TrainAcc 0.2782	ValidAcc 0.2771	TestAcc 0.2772	BestValid 0.3318
	Epoch 1700:	Loss 0.3101	TrainAcc 0.2799	ValidAcc 0.2789	TestAcc 0.2788	BestValid 0.3318
	Epoch 1750:	Loss 0.3241	TrainAcc 0.2767	ValidAcc 0.2757	TestAcc 0.2756	BestValid 0.3318
	Epoch 1800:	Loss 0.3761	TrainAcc 0.1415	ValidAcc 0.1421	TestAcc 0.1419	BestValid 0.3318
	Epoch 1850:	Loss 0.3961	TrainAcc 0.2566	ValidAcc 0.2562	TestAcc 0.2574	BestValid 0.3318
	Epoch 1900:	Loss 0.3514	TrainAcc 0.3163	ValidAcc 0.3157	TestAcc 0.3159	BestValid 0.3318
	Epoch 1950:	Loss 0.3202	TrainAcc 0.2785	ValidAcc 0.2774	TestAcc 0.2775	BestValid 0.3318
	Epoch 2000:	Loss 0.3220	TrainAcc 0.2698	ValidAcc 0.2686	TestAcc 0.2688	BestValid 0.3318
	Epoch 2050:	Loss 0.3827	TrainAcc 0.1423	ValidAcc 0.1423	TestAcc 0.1428	BestValid 0.3318
	Epoch 2100:	Loss 0.4651	TrainAcc 0.1415	ValidAcc 0.1421	TestAcc 0.1418	BestValid 0.3318
	Epoch 2150:	Loss 0.5324	TrainAcc 0.1414	ValidAcc 0.1419	TestAcc 0.1419	BestValid 0.3318
	Epoch 2200:	Loss 0.5821	TrainAcc 0.1417	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3318
	Epoch 2250:	Loss 0.4030	TrainAcc 0.1413	ValidAcc 0.1421	TestAcc 0.1417	BestValid 0.3318
	Epoch 2300:	Loss 0.3804	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.3318
	Epoch 2350:	Loss 0.3720	TrainAcc 0.1410	ValidAcc 0.1419	TestAcc 0.1414	BestValid 0.3318
	Epoch 2400:	Loss 0.3689	TrainAcc 0.1411	ValidAcc 0.1419	TestAcc 0.1416	BestValid 0.3318
	Epoch 2450:	Loss 0.3537	TrainAcc 0.1414	ValidAcc 0.1421	TestAcc 0.1418	BestValid 0.3318
	Epoch 2500:	Loss 0.3329	TrainAcc 0.3318	ValidAcc 0.3316	TestAcc 0.3311	BestValid 0.3318
	Epoch 2550:	Loss 0.3148	TrainAcc 0.3198	ValidAcc 0.3193	TestAcc 0.3194	BestValid 0.3318
	Epoch 2600:	Loss 0.3228	TrainAcc 0.2790	ValidAcc 0.2780	TestAcc 0.2778	BestValid 0.3318
	Epoch 2650:	Loss 0.3589	TrainAcc 0.1414	ValidAcc 0.1421	TestAcc 0.1418	BestValid 0.3318
	Epoch 2700:	Loss 0.4284	TrainAcc 0.1413	ValidAcc 0.1419	TestAcc 0.1415	BestValid 0.3318
	Epoch 2750:	Loss 0.3867	TrainAcc 0.1416	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 2800:	Loss 0.3464	TrainAcc 0.2564	ValidAcc 0.2559	TestAcc 0.2572	BestValid 0.3318
	Epoch 2850:	Loss 0.3213	TrainAcc 0.3150	ValidAcc 0.3143	TestAcc 0.3146	BestValid 0.3318
	Epoch 2900:	Loss 0.3265	TrainAcc 0.2745	ValidAcc 0.2733	TestAcc 0.2734	BestValid 0.3318
	Epoch 2950:	Loss 0.3530	TrainAcc 0.1416	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3000:	Loss 0.4118	TrainAcc 0.1416	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3050:	Loss 0.4829	TrainAcc 0.1416	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3100:	Loss 0.6049	TrainAcc 0.1416	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3150:	Loss 0.6833	TrainAcc 0.1417	ValidAcc 0.1417	TestAcc 0.1424	BestValid 0.3318
	Epoch 3200:	Loss 0.6925	TrainAcc 0.1416	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3250:	Loss 0.6912	TrainAcc 0.1416	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3300:	Loss 0.5194	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3350:	Loss 0.4030	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3400:	Loss 0.3839	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3450:	Loss 0.3953	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3500:	Loss 0.4065	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3550:	Loss 0.3800	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3600:	Loss 0.3789	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3650:	Loss 0.3939	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3700:	Loss 0.4103	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3750:	Loss 0.3820	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3800:	Loss 0.3748	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3850:	Loss 0.3878	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3900:	Loss 0.4098	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 3950:	Loss 0.4278	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 4000:	Loss 0.3838	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 4050:	Loss 0.3758	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 4100:	Loss 0.3858	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 4150:	Loss 0.3959	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 4200:	Loss 0.3786	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 4250:	Loss 0.3681	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 4300:	Loss 0.3593	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 4350:	Loss 0.3547	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3318
	Epoch 4400:	Loss 0.3430	TrainAcc 0.2254	ValidAcc 0.2251	TestAcc 0.2261	BestValid 0.3318
	Epoch 4450:	Loss 0.3334	TrainAcc 0.3134	ValidAcc 0.3127	TestAcc 0.3133	BestValid 0.3318
	Epoch 4500:	Loss 0.3191	TrainAcc 0.3041	ValidAcc 0.3031	TestAcc 0.3036	BestValid 0.3318
	Epoch 4550:	Loss 0.3150	TrainAcc 0.2872	ValidAcc 0.2860	TestAcc 0.2862	BestValid 0.3318
	Epoch 4600:	Loss 0.3214	TrainAcc 0.2715	ValidAcc 0.2702	TestAcc 0.2703	BestValid 0.3318
	Epoch 4650:	Loss 0.3339	TrainAcc 0.3140	ValidAcc 0.3133	TestAcc 0.3139	BestValid 0.3318
	Epoch 4700:	Loss 0.3255	TrainAcc 0.2722	ValidAcc 0.2710	TestAcc 0.2711	BestValid 0.3318
	Epoch 4750:	Loss 0.3162	TrainAcc 0.2715	ValidAcc 0.2702	TestAcc 0.2703	BestValid 0.3318
	Epoch 4800:	Loss 0.3218	TrainAcc 0.2715	ValidAcc 0.2702	TestAcc 0.2703	BestValid 0.3318
	Epoch 4850:	Loss 0.3548	TrainAcc 0.3134	ValidAcc 0.3128	TestAcc 0.3133	BestValid 0.3318
	Epoch 4900:	Loss 0.3467	TrainAcc 0.2563	ValidAcc 0.2558	TestAcc 0.2570	BestValid 0.3318
	Epoch 4950:	Loss 0.3210	TrainAcc 0.2788	ValidAcc 0.2777	TestAcc 0.2779	BestValid 0.3318
	Epoch 5000:	Loss 0.3156	TrainAcc 0.2722	ValidAcc 0.2710	TestAcc 0.2711	BestValid 0.3318
****** Epoch Time (Excluding Evaluation Cost): 0.263 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 43.610 ms (Max: 45.090, Min: 41.908, Sum: 348.879)
Cluster-Wide Average, Compute: 141.559 ms (Max: 151.927, Min: 138.321, Sum: 1132.471)
Cluster-Wide Average, Communication-Layer: 48.353 ms (Max: 69.294, Min: 29.680, Sum: 386.822)
Cluster-Wide Average, Bubble-Imbalance: 23.923 ms (Max: 43.430, Min: 5.520, Sum: 191.385)
Cluster-Wide Average, Communication-Graph: 0.440 ms (Max: 0.474, Min: 0.387, Sum: 3.521)
Cluster-Wide Average, Optimization: 0.089 ms (Max: 0.091, Min: 0.088, Sum: 0.716)
Cluster-Wide Average, Others: 5.831 ms (Max: 9.579, Min: 5.202, Sum: 46.648)
****** Breakdown Sum: 263.805 ms ******
Cluster-Wide Average, GPU Memory Consumption: 8.303 GB (Max: 9.751, Min: 8.047, Sum: 66.425)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 85.002 Gbps (Max: 110.058, Min: 66.208, Sum: 680.015)
Layer-level communication (cluster-wide, per-epoch): 3.739 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 3.739 GB
****** Accuracy Results ******
Highest valid_acc: 0.3318
Target test_acc: 0.3313
Epoch to reach the target acc: 249
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
