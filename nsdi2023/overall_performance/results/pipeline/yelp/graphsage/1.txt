Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv2
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.247 seconds.
Building the CSC structure...
        It takes 0.274 seconds.
Building the CSC structure...
        It takes 0.288 seconds.
Building the CSC structure...
        It takes 0.293 seconds.
Building the CSC structure...
        It takes 0.299 seconds.
Building the CSC structure...
        It takes 0.302 seconds.
Building the CSC structure...
        It takes 0.315 seconds.
Building the CSC structure...
        It takes 0.325 seconds.
Building the CSC structure...
        It takes 0.243 seconds.
        It takes 0.278 seconds.
        It takes 0.283 seconds.
        It takes 0.278 seconds.
        It takes 0.293 seconds.
        It takes 0.306 seconds.
        It takes 0.315 seconds.
        It takes 0.315 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.476 seconds.
Building the Label Vector...
        It takes 0.482 seconds.
Building the Label Vector...
        It takes 0.476 seconds.
Building the Label Vector...
        It takes 0.489 seconds.
Building the Label Vector...
        It takes 0.477 seconds.
Building the Label Vector...
        It takes 0.528 seconds.
Building the Label Vector...
        It takes 0.505 seconds.
Building the Label Vector...
        It takes 0.189 seconds.
        It takes 0.534 seconds.
Building the Label Vector...
        It takes 0.182 seconds.
        It takes 0.188 seconds.
        It takes 0.188 seconds.
        It takes 0.185 seconds.
        It takes 0.198 seconds.
        It takes 0.199 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/yelp/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 100
Number of feature dimensions: 300
Number of vertices: 716847
Number of GPUs: 8
        It takes 0.193 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
train nodes 537635, valid nodes 107527, test nodes 71685
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 24122) 1-[24122, 44991) 2-[44991, 66905) 3-[66905, 90565) 4-[90565, 109350) 5-[109350, 132203) 6-[132203, 154486) 7-[154486, 177346) 8-[177346, 198991) ... 31-[695934, 716847)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.019 Gbps (per GPU), 448.153 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.771 Gbps (per GPU), 446.168 Gbps (aggregated)
The layer-level communication performance: 55.761 Gbps (per GPU), 446.091 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.541 Gbps (per GPU), 444.331 Gbps (aggregated)
The layer-level communication performance: 55.512 Gbps (per GPU), 444.093 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.322 Gbps (per GPU), 442.575 Gbps (aggregated)
The layer-level communication performance: 55.278 Gbps (per GPU), 442.222 Gbps (aggregated)
The layer-level communication performance: 55.254 Gbps (per GPU), 442.033 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.250 Gbps (per GPU), 1250.002 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.230 Gbps (per GPU), 1249.839 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.236 Gbps (per GPU), 1249.886 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.259 Gbps (per GPU), 1250.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.236 Gbps (per GPU), 1249.886 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.262 Gbps (per GPU), 1250.095 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.239 Gbps (per GPU), 1249.909 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.239 Gbps (per GPU), 1249.909 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.497 Gbps (per GPU), 811.978 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.498 Gbps (per GPU), 811.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.500 Gbps (per GPU), 812.003 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.499 Gbps (per GPU), 811.991 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.498 Gbps (per GPU), 811.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.496 Gbps (per GPU), 811.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.499 Gbps (per GPU), 811.991 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.497 Gbps (per GPU), 811.978 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.736 Gbps (per GPU), 269.886 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.735 Gbps (per GPU), 269.877 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.736 Gbps (per GPU), 269.884 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.735 Gbps (per GPU), 269.878 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.735 Gbps (per GPU), 269.877 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.734 Gbps (per GPU), 269.871 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.735 Gbps (per GPU), 269.881 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.735 Gbps (per GPU), 269.881 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.88ms  1.76ms  1.47ms  1.96 24.12K  0.35M
 chk_1  2.84ms  1.78ms  1.52ms  1.87 20.87K  0.49M
 chk_2  2.77ms  1.66ms  1.39ms  1.99 21.91K  0.46M
 chk_3  2.88ms  1.76ms  1.47ms  1.95 23.66K  0.35M
 chk_4  2.63ms  1.65ms  1.40ms  1.87 18.79K  0.62M
 chk_5  2.82ms  1.72ms  1.44ms  1.95 22.85K  0.37M
 chk_6  2.86ms  1.74ms  1.47ms  1.94 22.28K  0.40M
 chk_7  2.80ms  1.71ms  1.43ms  1.96 22.86K  0.39M
 chk_8  2.82ms  1.72ms  1.45ms  1.94 21.64K  0.45M
 chk_9  2.79ms  1.69ms  1.41ms  1.98 22.92K  0.37M
chk_10  2.75ms  1.75ms  1.49ms  1.85 20.30K  0.56M
chk_11  2.86ms  1.74ms  1.46ms  1.96 23.32K  0.33M
chk_12  2.81ms  1.74ms  1.48ms  1.90 21.10K  0.49M
chk_13  2.76ms  1.69ms  1.42ms  1.94 20.79K  0.49M
chk_14  2.81ms  1.69ms  1.41ms  2.00 23.53K  0.36M
chk_15  2.75ms  1.65ms  1.36ms  2.02 23.21K  0.39M
chk_16  2.82ms  1.69ms  1.40ms  2.02 24.39K  0.32M
chk_17  2.86ms  1.73ms  1.44ms  1.98 23.94K  0.34M
chk_18  2.80ms  1.71ms  1.44ms  1.95 21.61K  0.45M
chk_19  2.83ms  1.70ms  1.42ms  2.00 23.89K  0.34M
chk_20  2.84ms  1.75ms  1.48ms  1.92 21.64K  0.47M
chk_21  2.84ms  1.72ms  1.44ms  1.97 23.43K  0.34M
chk_22  2.80ms  1.71ms  1.86ms  1.64 22.84K  0.37M
chk_23  2.85ms  1.73ms  1.45ms  1.96 23.35K  0.37M
chk_24  2.84ms  1.74ms  1.46ms  1.94 22.72K  0.40M
chk_25  2.87ms  1.76ms  1.49ms  1.92 21.95K  0.46M
chk_26  2.86ms  1.75ms  1.48ms  1.94 22.06K  0.41M
chk_27  2.81ms  1.71ms  1.43ms  1.97 23.02K  0.35M
chk_28  2.82ms  1.71ms  1.43ms  1.97 22.97K  0.36M
chk_29  2.77ms  1.67ms  1.40ms  1.99 22.14K  0.45M
chk_30  2.84ms  1.74ms  1.47ms  1.93 21.84K  0.44M
chk_31  2.81ms  1.74ms  1.48ms  1.90 20.91K  0.49M
   Avg  2.81  1.72  1.46
   Max  2.88  1.78  1.86
   Min  2.63  1.65  1.36
 Ratio  1.10  1.08  1.36
   Var  0.00  0.00  0.01
Profiling takes 2.326 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 98)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [98, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 162)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [162, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [194, 226)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [226, 256)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[66, 98)...
+++++++++ Node 5 initializing the weights for op[162, 194)...
+++++++++ Node 3 initializing the weights for op[98, 130)...
+++++++++ Node 7 initializing the weights for op[226, 256)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 4 initializing the weights for op[130, 162)...
+++++++++ Node 6 initializing the weights for op[194, 226)...
+++++++++ Node 1 initializing the weights for op[34, 66)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 48.3314	TrainAcc 0.1861	ValidAcc 0.1850	TestAcc 0.1852	BestValid 0.1850
	Epoch 50:	Loss 0.6931	TrainAcc 0.1592	ValidAcc 0.1586	TestAcc 0.1584	BestValid 0.1850
	Epoch 100:	Loss 0.6931	TrainAcc 0.1632	ValidAcc 0.1626	TestAcc 0.1625	BestValid 0.1850
	Epoch 150:	Loss 0.6931	TrainAcc 0.1711	ValidAcc 0.1703	TestAcc 0.1703	BestValid 0.1850
	Epoch 200:	Loss 0.6931	TrainAcc 0.1722	ValidAcc 0.1714	TestAcc 0.1714	BestValid 0.1850
	Epoch 250:	Loss 0.6931	TrainAcc 0.1697	ValidAcc 0.1689	TestAcc 0.1690	BestValid 0.1850
	Epoch 300:	Loss 0.6931	TrainAcc 0.1769	ValidAcc 0.1762	TestAcc 0.1763	BestValid 0.1850
	Epoch 350:	Loss 0.6931	TrainAcc 0.1769	ValidAcc 0.1762	TestAcc 0.1763	BestValid 0.1850
	Epoch 400:	Loss 0.6931	TrainAcc 0.1763	ValidAcc 0.1757	TestAcc 0.1759	BestValid 0.1850
	Epoch 450:	Loss 0.6931	TrainAcc 0.1769	ValidAcc 0.1762	TestAcc 0.1763	BestValid 0.1850
	Epoch 500:	Loss 0.6931	TrainAcc 0.1768	ValidAcc 0.1761	TestAcc 0.1762	BestValid 0.1850
	Epoch 550:	Loss 0.6931	TrainAcc 0.1772	ValidAcc 0.1765	TestAcc 0.1767	BestValid 0.1850
	Epoch 600:	Loss 0.6931	TrainAcc 0.1798	ValidAcc 0.1790	TestAcc 0.1793	BestValid 0.1850
	Epoch 650:	Loss 0.6931	TrainAcc 0.1819	ValidAcc 0.1812	TestAcc 0.1815	BestValid 0.1850
	Epoch 700:	Loss 0.6931	TrainAcc 0.1821	ValidAcc 0.1814	TestAcc 0.1817	BestValid 0.1850
	Epoch 750:	Loss 0.6931	TrainAcc 0.1838	ValidAcc 0.1831	TestAcc 0.1833	BestValid 0.1850
	Epoch 800:	Loss 0.6931	TrainAcc 0.1882	ValidAcc 0.1876	TestAcc 0.1878	BestValid 0.1876
	Epoch 850:	Loss 0.6931	TrainAcc 0.1907	ValidAcc 0.1903	TestAcc 0.1902	BestValid 0.1903
	Epoch 900:	Loss 0.6931	TrainAcc 0.1933	ValidAcc 0.1928	TestAcc 0.1927	BestValid 0.1928
	Epoch 950:	Loss 0.6931	TrainAcc 0.1931	ValidAcc 0.1925	TestAcc 0.1925	BestValid 0.1928
	Epoch 1000:	Loss 0.6931	TrainAcc 0.1901	ValidAcc 0.1894	TestAcc 0.1894	BestValid 0.1928
	Epoch 1050:	Loss 0.6931	TrainAcc 0.1881	ValidAcc 0.1873	TestAcc 0.1875	BestValid 0.1928
	Epoch 1100:	Loss 0.6931	TrainAcc 0.1860	ValidAcc 0.1852	TestAcc 0.1854	BestValid 0.1928
	Epoch 1150:	Loss 0.6931	TrainAcc 0.1800	ValidAcc 0.1791	TestAcc 0.1792	BestValid 0.1928
	Epoch 1200:	Loss 0.6931	TrainAcc 0.1792	ValidAcc 0.1784	TestAcc 0.1784	BestValid 0.1928
	Epoch 1250:	Loss 0.6931	TrainAcc 0.1880	ValidAcc 0.1875	TestAcc 0.1875	BestValid 0.1928
	Epoch 1300:	Loss 0.6931	TrainAcc 0.1812	ValidAcc 0.1806	TestAcc 0.1808	BestValid 0.1928
	Epoch 1350:	Loss 0.6931	TrainAcc 0.1763	ValidAcc 0.1759	TestAcc 0.1760	BestValid 0.1928
	Epoch 1400:	Loss 0.6931	TrainAcc 0.1749	ValidAcc 0.1745	TestAcc 0.1744	BestValid 0.1928
	Epoch 1450:	Loss 0.6931	TrainAcc 0.1770	ValidAcc 0.1764	TestAcc 0.1765	BestValid 0.1928
	Epoch 1500:	Loss 0.6931	TrainAcc 0.1941	ValidAcc 0.1936	TestAcc 0.1934	BestValid 0.1936
	Epoch 1550:	Loss 0.6931	TrainAcc 0.1958	ValidAcc 0.1952	TestAcc 0.1951	BestValid 0.1952
	Epoch 1600:	Loss 0.6930	TrainAcc 0.1879	ValidAcc 0.1873	TestAcc 0.1875	BestValid 0.1952
	Epoch 1650:	Loss 0.6930	TrainAcc 0.1965	ValidAcc 0.1960	TestAcc 0.1960	BestValid 0.1960
	Epoch 1700:	Loss 0.6930	TrainAcc 0.1871	ValidAcc 0.1865	TestAcc 0.1866	BestValid 0.1960
	Epoch 1750:	Loss 0.6930	TrainAcc 0.1925	ValidAcc 0.1919	TestAcc 0.1921	BestValid 0.1960
	Epoch 1800:	Loss 0.6930	TrainAcc 0.2134	ValidAcc 0.2127	TestAcc 0.2129	BestValid 0.2127
	Epoch 1850:	Loss 0.6930	TrainAcc 0.2279	ValidAcc 0.2274	TestAcc 0.2276	BestValid 0.2274
	Epoch 1900:	Loss 0.6929	TrainAcc 0.2183	ValidAcc 0.2176	TestAcc 0.2179	BestValid 0.2274
	Epoch 1950:	Loss 0.6928	TrainAcc 0.2189	ValidAcc 0.2183	TestAcc 0.2185	BestValid 0.2274
	Epoch 2000:	Loss 0.6926	TrainAcc 0.1900	ValidAcc 0.1894	TestAcc 0.1897	BestValid 0.2274
	Epoch 2050:	Loss 0.6909	TrainAcc 0.1949	ValidAcc 0.1942	TestAcc 0.1945	BestValid 0.2274
	Epoch 2100:	Loss 0.4186	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2150:	Loss 0.3220	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2200:	Loss 0.3165	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2250:	Loss 0.3009	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2300:	Loss 0.2936	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2350:	Loss 0.2926	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2400:	Loss 0.2901	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2450:	Loss 0.2886	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2500:	Loss 0.2875	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2550:	Loss 0.2861	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2600:	Loss 0.2855	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2650:	Loss 0.2842	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2700:	Loss 0.2836	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2750:	Loss 0.2831	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2800:	Loss 0.2821	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2850:	Loss 0.2816	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2900:	Loss 0.2812	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 2950:	Loss 0.2808	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3000:	Loss 0.2803	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3050:	Loss 0.2796	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3100:	Loss 0.2791	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3150:	Loss 0.2789	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3200:	Loss 0.2788	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3250:	Loss 0.2780	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3300:	Loss 0.2775	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3350:	Loss 0.2773	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3400:	Loss 0.2766	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3450:	Loss 0.2764	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3500:	Loss 0.2761	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3550:	Loss 0.2757	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3600:	Loss 0.2755	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3650:	Loss 0.2753	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3700:	Loss 0.2750	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3750:	Loss 0.2746	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3800:	Loss 0.2743	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3850:	Loss 0.2740	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3900:	Loss 0.2738	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 3950:	Loss 0.2736	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4000:	Loss 0.2735	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4050:	Loss 0.2732	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4100:	Loss 0.2731	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4150:	Loss 0.2730	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4200:	Loss 0.2728	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4250:	Loss 0.2730	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4300:	Loss 0.2726	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4350:	Loss 0.2724	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4400:	Loss 0.2723	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4450:	Loss 0.2721	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4500:	Loss 0.2721	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4550:	Loss 0.2720	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4600:	Loss 0.2720	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4650:	Loss 0.2720	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4700:	Loss 0.2716	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4750:	Loss 0.2715	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4800:	Loss 0.2713	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4850:	Loss 0.2713	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4900:	Loss 0.2714	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 4950:	Loss 0.2711	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
	Epoch 5000:	Loss 0.2711	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2274
****** Epoch Time (Excluding Evaluation Cost): 0.378 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 64.154 ms (Max: 67.297, Min: 61.965, Sum: 513.229)
Cluster-Wide Average, Compute: 222.631 ms (Max: 258.859, Min: 212.924, Sum: 1781.048)
Cluster-Wide Average, Communication-Layer: 51.763 ms (Max: 70.854, Min: 33.858, Sum: 414.104)
Cluster-Wide Average, Bubble-Imbalance: 33.759 ms (Max: 56.670, Min: 14.639, Sum: 270.071)
Cluster-Wide Average, Communication-Graph: 0.445 ms (Max: 0.495, Min: 0.388, Sum: 3.558)
Cluster-Wide Average, Optimization: 0.177 ms (Max: 0.182, Min: 0.173, Sum: 1.416)
Cluster-Wide Average, Others: 5.955 ms (Max: 9.705, Min: 5.323, Sum: 47.642)
****** Breakdown Sum: 378.883 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.545 GB (Max: 9.528, Min: 7.176, Sum: 60.362)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 78.202 Gbps (Max: 101.679, Min: 63.593, Sum: 625.615)
Layer-level communication (cluster-wide, per-epoch): 3.739 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 3.739 GB
****** Accuracy Results ******
Highest valid_acc: 0.2274
Target test_acc: 0.2276
Epoch to reach the target acc: 1849
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
