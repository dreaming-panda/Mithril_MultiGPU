Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
Initialized node 5 on machine gnerv8
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INIT
Initialized node 0 on machine gnerv7

Initialized node 1 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
        It takes 0.010 seconds.
        It takes 0.011 seconds.
        It takes 0.012 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.018 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.016 seconds.
        It takes 0.016 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.452 seconds.
        It takes 0.451 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.002 seconds.
        It takes 0.514 seconds.
Building the Label Vector...
        It takes 0.511 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.002 seconds.
        It takes 0.618 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/physics/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 300
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 8
        It takes 0.612 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.620 seconds.
Building the Label Vector...
        It takes 0.620 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 1025) 1-[1025, 2154) 2-[2154, 3262) 3-[3262, 4284) 4-[4284, 5334) 5-[5334, 6511) 6-[6511, 7597) 7-[7597, 8680) 8-[8680, 9764) ... 31-[33337, 34493)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.959 Gbps (per GPU), 487.669 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.637 Gbps (per GPU), 485.094 Gbps (aggregated)
The layer-level communication performance: 60.637 Gbps (per GPU), 485.093 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.369 Gbps (per GPU), 482.955 Gbps (aggregated)
The layer-level communication performance: 60.347 Gbps (per GPU), 482.772 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.105 Gbps (per GPU), 480.844 Gbps (aggregated)
The layer-level communication performance: 60.055 Gbps (per GPU), 480.436 Gbps (aggregated)
The layer-level communication performance: 60.024 Gbps (per GPU), 480.191 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.097 Gbps (per GPU), 1264.773 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.079 Gbps (per GPU), 1264.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.094 Gbps (per GPU), 1264.749 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.085 Gbps (per GPU), 1264.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.073 Gbps (per GPU), 1264.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.085 Gbps (per GPU), 1264.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.073 Gbps (per GPU), 1264.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.079 Gbps (per GPU), 1264.630 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 105.113 Gbps (per GPU), 840.901 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.112 Gbps (per GPU), 840.899 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.113 Gbps (per GPU), 840.907 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.114 Gbps (per GPU), 840.914 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.071 Gbps (per GPU), 840.570 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.072 Gbps (per GPU), 840.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.020 Gbps (per GPU), 840.163 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.963 Gbps (per GPU), 839.708 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 40.051 Gbps (per GPU), 320.408 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.052 Gbps (per GPU), 320.413 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.050 Gbps (per GPU), 320.402 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.051 Gbps (per GPU), 320.411 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.052 Gbps (per GPU), 320.416 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.047 Gbps (per GPU), 320.378 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.052 Gbps (per GPU), 320.414 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.050 Gbps (per GPU), 320.401 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.28ms  0.30ms  0.42ms  4.26  1.02K  0.02M
 chk_1  1.36ms  0.30ms  0.42ms  4.53  1.13K  0.01M
 chk_2  1.35ms  0.30ms  0.42ms  4.45  1.11K  0.01M
 chk_3  1.29ms  0.30ms  0.42ms  4.35  1.02K  0.02M
 chk_4  1.30ms  0.30ms  0.42ms  4.35  1.05K  0.02M
 chk_5  1.95ms  0.30ms  0.42ms  6.53  1.18K  0.01M
 chk_6  1.33ms  0.30ms  0.42ms  4.44  1.09K  0.02M
 chk_7  1.33ms  0.30ms  0.42ms  4.46  1.08K  0.02M
 chk_8  1.33ms  0.30ms  0.42ms  4.42  1.08K  0.01M
 chk_9  1.31ms  0.30ms  0.42ms  4.40  1.06K  0.02M
chk_10  1.31ms  0.30ms  0.42ms  4.33  1.04K  0.02M
chk_11  1.36ms  0.30ms  0.42ms  4.56  1.11K  0.01M
chk_12  1.35ms  0.30ms  0.42ms  4.55  1.11K  0.01M
chk_13  1.27ms  0.29ms  0.41ms  4.36  1.01K  0.02M
chk_14  1.34ms  0.30ms  0.44ms  4.51  1.09K  0.01M
chk_15  1.33ms  0.34ms  0.42ms  3.90  1.03K  0.02M
chk_16  1.24ms  0.29ms  0.41ms  4.23  0.98K  0.02M
chk_17  1.34ms  0.30ms  0.42ms  4.47  1.09K  0.01M
chk_18  1.29ms  0.30ms  0.42ms  4.33  1.04K  0.02M
chk_19  2.02ms  0.30ms  0.42ms  6.80  1.12K  0.01M
chk_20  1.36ms  0.30ms  0.42ms  4.56  1.13K  0.01M
chk_21  1.29ms  0.30ms  0.42ms  4.38  1.05K  0.02M
chk_22  1.35ms  0.30ms  0.42ms  4.50  1.11K  0.01M
chk_23  1.34ms  0.30ms  0.42ms  4.48  1.10K  0.01M
chk_24  1.27ms  0.29ms  0.42ms  4.32  1.02K  0.02M
chk_25  1.26ms  0.29ms  0.41ms  4.33  1.01K  0.02M
chk_26  1.36ms  0.30ms  0.42ms  4.54  1.12K  0.01M
chk_27  1.32ms  0.30ms  0.42ms  4.44  1.08K  0.02M
chk_28  1.32ms  0.36ms  0.42ms  3.70  1.07K  0.02M
chk_29  1.35ms  0.30ms  0.42ms  4.54  1.11K  0.01M
chk_30  1.34ms  0.30ms  0.42ms  4.51  1.10K  0.01M
chk_31  1.38ms  0.30ms  0.42ms  4.63  1.16K  0.01M
   Avg  1.36  0.30  0.42
   Max  2.02  0.36  0.44
   Min  1.24  0.29  0.41
 Ratio  1.63  1.22  1.08
   Var  0.03  0.00  0.00
Profiling takes 0.890 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 34493
Node 0, Local Vertex Begin: 0, Num Local Vertices: 34493
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 34493
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.1027	TrainAcc 0.6000	ValidAcc 0.2640	TestAcc 0.2830	BestValid 0.2640
	Epoch 10:	Loss 1.4789	TrainAcc 0.9400	ValidAcc 0.9200	TestAcc 0.9130	BestValid 0.9200
	Epoch 20:	Loss 1.0183	TrainAcc 0.9400	ValidAcc 0.9200	TestAcc 0.9330	BestValid 0.9200
	Epoch 30:	Loss 0.6296	TrainAcc 0.9600	ValidAcc 0.9340	TestAcc 0.9420	BestValid 0.9340
	Epoch 40:	Loss 0.6471	TrainAcc 0.9600	ValidAcc 0.9300	TestAcc 0.9400	BestValid 0.9340
	Epoch 50:	Loss 0.3415	TrainAcc 0.9700	ValidAcc 0.9440	TestAcc 0.9420	BestValid 0.9440
	Epoch 60:	Loss 0.7885	TrainAcc 0.9900	ValidAcc 0.9440	TestAcc 0.9440	BestValid 0.9440
	Epoch 70:	Loss 0.2890	TrainAcc 0.9900	ValidAcc 0.9440	TestAcc 0.9420	BestValid 0.9440
	Epoch 80:	Loss 0.5816	TrainAcc 0.9900	ValidAcc 0.9440	TestAcc 0.9420	BestValid 0.9440
	Epoch 90:	Loss 0.4344	TrainAcc 0.9900	ValidAcc 0.9480	TestAcc 0.9420	BestValid 0.9480
	Epoch 100:	Loss 0.3916	TrainAcc 0.9900	ValidAcc 0.9500	TestAcc 0.9420	BestValid 0.9500
	Epoch 110:	Loss 0.1399	TrainAcc 0.9900	ValidAcc 0.9460	TestAcc 0.9420	BestValid 0.9500
	Epoch 120:	Loss 0.1389	TrainAcc 0.9900	ValidAcc 0.9480	TestAcc 0.9420	BestValid 0.9500
	Epoch 130:	Loss 0.5126	TrainAcc 0.9900	ValidAcc 0.9480	TestAcc 0.9440	BestValid 0.9500
	Epoch 140:	Loss 0.2802	TrainAcc 0.9900	ValidAcc 0.9480	TestAcc 0.9420	BestValid 0.9500
	Epoch 150:	Loss 0.8018	TrainAcc 0.9900	ValidAcc 0.9460	TestAcc 0.9430	BestValid 0.9500
	Epoch 160:	Loss 0.0751	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9430	BestValid 0.9500
	Epoch 170:	Loss 0.3797	TrainAcc 1.0000	ValidAcc 0.9440	TestAcc 0.9430	BestValid 0.9500
	Epoch 180:	Loss 0.2087	TrainAcc 1.0000	ValidAcc 0.9460	TestAcc 0.9430	BestValid 0.9500
	Epoch 190:	Loss 0.0773	TrainAcc 1.0000	ValidAcc 0.9460	TestAcc 0.9450	BestValid 0.9500
	Epoch 200:	Loss 0.0848	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9440	BestValid 0.9500
	Epoch 210:	Loss 0.2037	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9450	BestValid 0.9500
	Epoch 220:	Loss 0.2511	TrainAcc 1.0000	ValidAcc 0.9440	TestAcc 0.9450	BestValid 0.9500
	Epoch 230:	Loss 0.1193	TrainAcc 1.0000	ValidAcc 0.9440	TestAcc 0.9420	BestValid 0.9500
	Epoch 240:	Loss 0.0806	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9400	BestValid 0.9500
	Epoch 250:	Loss 0.1181	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9420	BestValid 0.9500
	Epoch 260:	Loss 0.6059	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9410	BestValid 0.9500
	Epoch 270:	Loss 0.3458	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9430	BestValid 0.9500
	Epoch 280:	Loss 0.3371	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9420	BestValid 0.9500
	Epoch 290:	Loss 0.0868	TrainAcc 1.0000	ValidAcc 0.9440	TestAcc 0.9420	BestValid 0.9500
	Epoch 300:	Loss 0.0698	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9420	BestValid 0.9500
****** Epoch Time (Excluding Evaluation Cost): 0.109 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.317 ms (Max: 21.052, Min: 12.356, Sum: 138.537)
Cluster-Wide Average, Compute: 49.236 ms (Max: 85.430, Min: 42.523, Sum: 393.884)
Cluster-Wide Average, Communication-Layer: 10.431 ms (Max: 11.740, Min: 8.203, Sum: 83.450)
Cluster-Wide Average, Bubble-Imbalance: 30.475 ms (Max: 37.976, Min: 1.267, Sum: 243.799)
Cluster-Wide Average, Communication-Graph: 0.524 ms (Max: 0.663, Min: 0.463, Sum: 4.192)
Cluster-Wide Average, Optimization: 0.126 ms (Max: 0.314, Min: 0.096, Sum: 1.008)
Cluster-Wide Average, Others: 0.686 ms (Max: 1.523, Min: 0.501, Sum: 5.490)
****** Breakdown Sum: 108.795 ms ******
Cluster-Wide Average, GPU Memory Consumption: 2.624 GB (Max: 4.686, Min: 2.315, Sum: 20.989)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 36.409 Gbps (Max: 42.097, Min: 26.111, Sum: 291.275)
Layer-level communication (cluster-wide, per-epoch): 0.360 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.360 GB
****** Accuracy Results ******
Highest valid_acc: 0.9500
Target test_acc: 0.9420
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
