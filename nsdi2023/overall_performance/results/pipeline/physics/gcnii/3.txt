Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 0 on machine gnerv7
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
DONE MPI INITInitialized node 6 on machine gnerv8

Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
        It takes 0.012 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.014 seconds.
Building the Feature Vector...
        It takes 0.011 seconds.
        It takes 0.011 seconds.
        It takes 0.012 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.452 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.461 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.513 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.515 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.612 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/physics/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 300
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 8
        It takes 0.617 seconds.
Building the Label Vector...
        It takes 0.617 seconds.
        It takes 0.618 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.004 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
34493, 530417, 530417
Number of vertices per chunk: 1078
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 1025) 1-[1025, 2154) 2-[2154, 3262) 3-[3262, 4284) 4-[4284, 5334) 5-[5334, 6511) 6-[6511, 7597) 7-[7597, 8680) 8-[8680, 9764) ... 31-[33337, 34493)
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.907 Gbps (per GPU), 479.258 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.619 Gbps (per GPU), 476.952 Gbps (aggregated)
The layer-level communication performance: 60.002 Gbps (per GPU), 480.017 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.373 Gbps (per GPU), 474.982 Gbps (aggregated)
The layer-level communication performance: 59.724 Gbps (per GPU), 477.791 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.521 Gbps (per GPU), 476.165 Gbps (aggregated)
The layer-level communication performance: 59.092 Gbps (per GPU), 472.732 Gbps (aggregated)
The layer-level communication performance: 59.437 Gbps (per GPU), 475.493 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.362 Gbps (per GPU), 1266.898 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.350 Gbps (per GPU), 1266.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.329 Gbps (per GPU), 1266.635 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.231 Gbps (per GPU), 1265.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.321 Gbps (per GPU), 1266.565 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.326 Gbps (per GPU), 1266.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.329 Gbps (per GPU), 1266.635 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 2): 151.836 Gbps (per GPU), 1214.684 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.882 Gbps (per GPU), 839.057 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.881 Gbps (per GPU), 839.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.883 Gbps (per GPU), 839.064 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.882 Gbps (per GPU), 839.057 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.838 Gbps (per GPU), 838.700 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.828 Gbps (per GPU), 838.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.746 Gbps (per GPU), 837.967 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.726 Gbps (per GPU), 837.807 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.359 Gbps (per GPU), 314.872 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.360 Gbps (per GPU), 314.878 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.358 Gbps (per GPU), 314.862 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.359 Gbps (per GPU), 314.871 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.355 Gbps (per GPU), 314.842 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.359 Gbps (per GPU), 314.871 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.354 Gbps (per GPU), 314.834 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.355 Gbps (per GPU), 314.840 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.27ms  0.29ms  0.40ms  4.36  1.02K  0.02M
 chk_1  1.35ms  0.29ms  0.41ms  4.64  1.13K  0.01M
 chk_2  1.33ms  0.29ms  0.41ms  4.54  1.11K  0.01M
 chk_3  1.26ms  0.29ms  0.40ms  4.36  1.02K  0.02M
 chk_4  1.29ms  0.29ms  0.41ms  4.48  1.05K  0.02M
 chk_5  1.39ms  0.29ms  0.41ms  4.79  1.18K  0.01M
 chk_6  1.84ms  0.29ms  0.41ms  6.32  1.09K  0.02M
 chk_7  1.32ms  0.29ms  0.41ms  4.55  1.08K  0.02M
 chk_8  1.33ms  0.29ms  0.41ms  4.59  1.08K  0.01M
 chk_9  1.30ms  0.29ms  0.41ms  4.54  1.06K  0.02M
chk_10  1.29ms  0.29ms  0.41ms  4.51  1.04K  0.02M
chk_11  1.34ms  0.29ms  0.41ms  4.62  1.11K  0.01M
chk_12  1.34ms  0.29ms  0.41ms  4.63  1.11K  0.01M
chk_13  1.25ms  0.28ms  0.40ms  4.44  1.01K  0.02M
chk_14  1.33ms  0.29ms  0.41ms  4.61  1.09K  0.01M
chk_15  1.28ms  0.29ms  0.41ms  4.46  1.03K  0.02M
chk_16  1.23ms  0.28ms  0.40ms  4.35  0.98K  0.02M
chk_17  1.33ms  0.29ms  0.41ms  4.61  1.09K  0.01M
chk_18  1.28ms  0.29ms  0.41ms  4.46  1.04K  0.02M
chk_19  1.34ms  0.29ms  0.41ms  4.65  1.12K  0.01M
chk_20  1.35ms  0.29ms  0.41ms  4.67  1.13K  0.01M
chk_21  1.75ms  0.29ms  0.41ms  6.11  1.05K  0.02M
chk_22  1.34ms  0.29ms  0.41ms  4.61  1.11K  0.01M
chk_23  1.33ms  0.29ms  0.41ms  4.56  1.10K  0.01M
chk_24  1.27ms  0.28ms  0.40ms  4.44  1.02K  0.02M
chk_25  1.26ms  0.28ms  0.40ms  4.46  1.01K  0.02M
chk_26  1.35ms  0.29ms  0.41ms  4.66  1.12K  0.01M
chk_27  1.40ms  0.29ms  0.41ms  4.83  1.08K  0.02M
chk_28  1.31ms  0.29ms  0.41ms  4.52  1.07K  0.02M
chk_29  1.34ms  0.29ms  0.41ms  4.68  1.11K  0.01M
chk_30  1.34ms  0.29ms  0.41ms  4.64  1.10K  0.01M
chk_31  1.38ms  0.29ms  0.41ms  4.79  1.16K  0.01M
   Avg  1.35  0.29  0.41
   Max  1.84  0.29  0.41
   Min  1.23  0.28  0.40
 Ratio  1.50  1.04  1.04
   Var  0.02  0.00  0.00
Profiling takes 0.870 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 34493
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 34493
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6424	TrainAcc 0.6500	ValidAcc 0.4960	TestAcc 0.5010	BestValid 0.4960
	Epoch 10:	Loss 1.2631	TrainAcc 0.9400	ValidAcc 0.9220	TestAcc 0.9120	BestValid 0.9220
	Epoch 20:	Loss 0.9282	TrainAcc 0.9500	ValidAcc 0.9160	TestAcc 0.9290	BestValid 0.9220
	Epoch 30:	Loss 1.0783	TrainAcc 0.9400	ValidAcc 0.9260	TestAcc 0.9340	BestValid 0.9260
	Epoch 40:	Loss 0.7370	TrainAcc 0.9500	ValidAcc 0.9240	TestAcc 0.9360	BestValid 0.9260
	Epoch 50:	Loss 0.9268	TrainAcc 0.9500	ValidAcc 0.9300	TestAcc 0.9380	BestValid 0.9300
	Epoch 60:	Loss 0.5788	TrainAcc 0.9700	ValidAcc 0.9420	TestAcc 0.9400	BestValid 0.9420
	Epoch 70:	Loss 0.2211	TrainAcc 0.9900	ValidAcc 0.9460	TestAcc 0.9430	BestValid 0.9460
	Epoch 80:	Loss 0.4433	TrainAcc 0.9900	ValidAcc 0.9420	TestAcc 0.9410	BestValid 0.9460
	Epoch 90:	Loss 0.1865	TrainAcc 0.9900	ValidAcc 0.9440	TestAcc 0.9400	BestValid 0.9460
	Epoch 100:	Loss 0.4496	TrainAcc 0.9900	ValidAcc 0.9400	TestAcc 0.9400	BestValid 0.9460
	Epoch 110:	Loss 0.3465	TrainAcc 0.9900	ValidAcc 0.9400	TestAcc 0.9400	BestValid 0.9460
	Epoch 120:	Loss 0.7208	TrainAcc 0.9900	ValidAcc 0.9420	TestAcc 0.9450	BestValid 0.9460
	Epoch 130:	Loss 0.2430	TrainAcc 0.9900	ValidAcc 0.9420	TestAcc 0.9420	BestValid 0.9460
	Epoch 140:	Loss 0.4702	TrainAcc 0.9900	ValidAcc 0.9400	TestAcc 0.9420	BestValid 0.9460
	Epoch 150:	Loss 0.5258	TrainAcc 0.9900	ValidAcc 0.9400	TestAcc 0.9420	BestValid 0.9460
	Epoch 160:	Loss 0.6347	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9430	BestValid 0.9460
	Epoch 170:	Loss 0.1994	TrainAcc 1.0000	ValidAcc 0.9440	TestAcc 0.9440	BestValid 0.9460
	Epoch 180:	Loss 0.1133	TrainAcc 1.0000	ValidAcc 0.9440	TestAcc 0.9440	BestValid 0.9460
	Epoch 190:	Loss 0.1744	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9450	BestValid 0.9460
	Epoch 200:	Loss 0.1115	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9450	BestValid 0.9460
	Epoch 210:	Loss 0.0925	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9450	BestValid 0.9460
	Epoch 220:	Loss 0.0959	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9430	BestValid 0.9460
	Epoch 230:	Loss 0.0756	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9420	BestValid 0.9460
	Epoch 240:	Loss 0.1505	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9410	BestValid 0.9460
	Epoch 250:	Loss 0.0740	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9430	BestValid 0.9460
	Epoch 260:	Loss 0.0837	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9420	BestValid 0.9460
	Epoch 270:	Loss 0.1119	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9420	BestValid 0.9460
	Epoch 280:	Loss 0.0726	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9430	BestValid 0.9460
	Epoch 290:	Loss 0.3429	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9420	BestValid 0.9460
	Epoch 300:	Loss 0.3614	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9420	BestValid 0.9460
****** Epoch Time (Excluding Evaluation Cost): 0.107 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.103 ms (Max: 20.627, Min: 12.348, Sum: 136.821)
Cluster-Wide Average, Compute: 49.192 ms (Max: 83.755, Min: 42.386, Sum: 393.536)
Cluster-Wide Average, Communication-Layer: 10.412 ms (Max: 11.634, Min: 8.170, Sum: 83.298)
Cluster-Wide Average, Bubble-Imbalance: 28.921 ms (Max: 36.135, Min: 1.190, Sum: 231.372)
Cluster-Wide Average, Communication-Graph: 0.507 ms (Max: 0.558, Min: 0.447, Sum: 4.055)
Cluster-Wide Average, Optimization: 0.127 ms (Max: 0.317, Min: 0.096, Sum: 1.015)
Cluster-Wide Average, Others: 0.673 ms (Max: 1.550, Min: 0.497, Sum: 5.381)
****** Breakdown Sum: 106.935 ms ******
Cluster-Wide Average, GPU Memory Consumption: 2.624 GB (Max: 4.686, Min: 2.315, Sum: 20.989)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 36.471 Gbps (Max: 41.856, Min: 25.946, Sum: 291.768)
Layer-level communication (cluster-wide, per-epoch): 0.360 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.360 GB
****** Accuracy Results ******
Highest valid_acc: 0.9460
Target test_acc: 0.9430
Epoch to reach the target acc: 69
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
