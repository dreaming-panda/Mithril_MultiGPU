Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 4 on machine gnerv8
Initialized node 5 on machine gnerv8
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 0 on machine gnerv7
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7

Initialized node 1 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.051 seconds.
        It takes 0.054 seconds.
        It takes 0.052 seconds.
        It takes 0.049 seconds.
        It takes 0.055 seconds.
        It takes 0.051 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.023 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.070 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
        It takes 0.028 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.241 Gbps (per GPU), 481.928 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.933 Gbps (per GPU), 479.463 Gbps (aggregated)
The layer-level communication performance: 59.922 Gbps (per GPU), 479.378 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.677 Gbps (per GPU), 477.412 Gbps (aggregated)
The layer-level communication performance: 59.643 Gbps (per GPU), 477.140 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.440 Gbps (per GPU), 475.521 Gbps (aggregated)
The layer-level communication performance: 59.393 Gbps (per GPU), 475.143 Gbps (aggregated)
The layer-level communication performance: 59.361 Gbps (per GPU), 474.885 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.365 Gbps (per GPU), 1266.922 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.365 Gbps (per GPU), 1266.922 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.356 Gbps (per GPU), 1266.850 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.365 Gbps (per GPU), 1266.922 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.368 Gbps (per GPU), 1266.947 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.377 Gbps (per GPU), 1267.018 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.356 Gbps (per GPU), 1266.850 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.356 Gbps (per GPU), 1266.850 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.729 Gbps (per GPU), 829.829 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.721 Gbps (per GPU), 829.768 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.731 Gbps (per GPU), 829.850 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.714 Gbps (per GPU), 829.713 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.701 Gbps (per GPU), 829.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.715 Gbps (per GPU), 829.720 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.719 Gbps (per GPU), 829.754 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.699 Gbps (per GPU), 829.590 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.353 Gbps (per GPU), 290.825 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.354 Gbps (per GPU), 290.835 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.353 Gbps (per GPU), 290.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.349 Gbps (per GPU), 290.792 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.350 Gbps (per GPU), 290.797 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.352 Gbps (per GPU), 290.812 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.351 Gbps (per GPU), 290.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.349 Gbps (per GPU), 290.789 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.59ms  0.60ms  0.36ms  1.65  5.55K  0.06M
 chk_1  0.60ms  0.52ms  0.37ms  1.63  5.75K  0.05M
 chk_2  0.59ms  0.52ms  0.37ms  1.60  5.20K  0.07M
 chk_3  0.60ms  0.52ms  0.37ms  1.63  5.57K  0.06M
 chk_4  0.59ms  0.51ms  0.37ms  1.60  5.05K  0.08M
 chk_5  0.61ms  0.54ms  0.40ms  1.54  4.73K  0.11M
 chk_6  0.58ms  0.51ms  0.36ms  1.60  4.98K  0.08M
 chk_7  0.59ms  0.52ms  0.37ms  1.60  5.01K  0.09M
 chk_8  0.59ms  0.52ms  0.36ms  1.65  5.73K  0.05M
 chk_9  0.58ms  0.52ms  0.37ms  1.58  4.54K  0.11M
chk_10  0.59ms  0.52ms  0.37ms  1.61  5.36K  0.07M
chk_11  0.60ms  0.52ms  0.37ms  1.61  5.39K  0.08M
chk_12  0.59ms  0.52ms  0.36ms  1.65  5.77K  0.05M
chk_13  0.59ms  0.52ms  0.37ms  1.61  5.43K  0.06M
chk_14  0.59ms  0.51ms  0.36ms  1.62  5.46K  0.06M
chk_15  0.59ms  0.51ms  0.36ms  1.65  5.88K  0.04M
chk_16  0.60ms  0.52ms  0.37ms  1.61  5.50K  0.06M
chk_17  0.60ms  0.53ms  0.39ms  1.55  4.86K  0.09M
chk_18  0.62ms  0.54ms  0.39ms  1.58  5.39K  0.07M
chk_19  0.60ms  0.52ms  0.37ms  1.60  5.20K  0.07M
chk_20  0.59ms  0.51ms  0.36ms  1.65  5.51K  0.06M
chk_21  0.59ms  0.51ms  0.35ms  1.66  5.81K  0.05M
chk_22  0.59ms  0.52ms  0.37ms  1.61  5.32K  0.07M
chk_23  0.61ms  0.53ms  0.38ms  1.59  5.39K  0.07M
chk_24  0.59ms  0.52ms  0.38ms  1.54  4.62K  0.11M
chk_25  0.59ms  0.52ms  0.37ms  1.60  5.04K  0.08M
chk_26  0.58ms  0.51ms  0.37ms  1.57  4.55K  0.11M
chk_27  0.58ms  0.51ms  0.36ms  1.62  5.30K  0.06M
chk_28  0.59ms  0.51ms  0.36ms  1.63  5.58K  0.06M
chk_29  0.60ms  0.53ms  0.38ms  1.57  4.98K  0.09M
chk_30  0.59ms  0.52ms  0.36ms  1.63  5.50K  0.07M
chk_31  0.60ms  0.52ms  0.37ms  1.63  5.38K  0.07M
   Avg  0.59  0.52  0.37
   Max  0.62  0.60  0.40
   Min  0.58  0.51  0.35
 Ratio  1.06  1.18  1.13
   Var  0.00  0.00  0.00
Profiling takes 0.690 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 162)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 98)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [162, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [98, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 6, starting model training...
Num Stages: 8 / 8
*** Node 7, starting model training...
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [194, 226)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 169343
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [226, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 66)...
+++++++++ Node 4 initializing the weights for op[130, 162)...
+++++++++ Node 2 initializing the weights for op[66, 98)...
+++++++++ Node 5 initializing the weights for op[162, 194)...
+++++++++ Node 3 initializing the weights for op[98, 130)...
+++++++++ Node 6 initializing the weights for op[194, 226)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[226, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 9.4807	TrainAcc 0.0038	ValidAcc 0.0032	TestAcc 0.0031	BestValid 0.0032
	Epoch 50:	Loss 3.6887	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 100:	Loss 3.6882	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 150:	Loss 3.1133	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 200:	Loss 2.8693	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 250:	Loss 2.8438	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 300:	Loss 2.8702	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 350:	Loss 2.8412	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 400:	Loss 2.8182	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 450:	Loss 2.8070	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 500:	Loss 2.8008	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 550:	Loss 2.7994	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 600:	Loss 2.7927	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 650:	Loss 2.7888	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 700:	Loss 2.7841	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 750:	Loss 2.7806	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 800:	Loss 2.7770	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 850:	Loss 2.7726	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 900:	Loss 2.7716	TrainAcc 0.1790	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 950:	Loss 2.7688	TrainAcc 0.1821	ValidAcc 0.0799	TestAcc 0.0597	BestValid 0.0799
	Epoch 1000:	Loss 2.7694	TrainAcc 0.1826	ValidAcc 0.0800	TestAcc 0.0597	BestValid 0.0800
	Epoch 1050:	Loss 2.7599	TrainAcc 0.1997	ValidAcc 0.0924	TestAcc 0.0743	BestValid 0.0924
	Epoch 1100:	Loss 2.7332	TrainAcc 0.1949	ValidAcc 0.0909	TestAcc 0.0723	BestValid 0.0924
	Epoch 1150:	Loss 2.6790	TrainAcc 0.2154	ValidAcc 0.0913	TestAcc 0.0714	BestValid 0.0924
	Epoch 1200:	Loss 2.6619	TrainAcc 0.2181	ValidAcc 0.0926	TestAcc 0.0726	BestValid 0.0926
	Epoch 1250:	Loss 2.6447	TrainAcc 0.2193	ValidAcc 0.0932	TestAcc 0.0731	BestValid 0.0932
	Epoch 1300:	Loss 2.6442	TrainAcc 0.2208	ValidAcc 0.0940	TestAcc 0.0737	BestValid 0.0940
	Epoch 1350:	Loss 2.6204	TrainAcc 0.2197	ValidAcc 0.0934	TestAcc 0.0735	BestValid 0.0940
	Epoch 1400:	Loss 2.5754	TrainAcc 0.2181	ValidAcc 0.0931	TestAcc 0.0733	BestValid 0.0940
	Epoch 1450:	Loss 2.5510	TrainAcc 0.1191	ValidAcc 0.0630	TestAcc 0.0506	BestValid 0.0940
	Epoch 1500:	Loss 2.4834	TrainAcc 0.2222	ValidAcc 0.0960	TestAcc 0.0764	BestValid 0.0960
	Epoch 1550:	Loss 2.4230	TrainAcc 0.2238	ValidAcc 0.0961	TestAcc 0.0760	BestValid 0.0961
	Epoch 1600:	Loss 2.3952	TrainAcc 0.3030	ValidAcc 0.3115	TestAcc 0.2796	BestValid 0.3115
	Epoch 1650:	Loss 2.3780	TrainAcc 0.2682	ValidAcc 0.2910	TestAcc 0.2713	BestValid 0.3115
	Epoch 1700:	Loss 2.3786	TrainAcc 0.1802	ValidAcc 0.0773	TestAcc 0.0592	BestValid 0.3115
	Epoch 1750:	Loss 2.3609	TrainAcc 0.1797	ValidAcc 0.0764	TestAcc 0.0587	BestValid 0.3115
	Epoch 1800:	Loss 2.3573	TrainAcc 0.1802	ValidAcc 0.0769	TestAcc 0.0591	BestValid 0.3115
	Epoch 1850:	Loss 2.3616	TrainAcc 0.1814	ValidAcc 0.0774	TestAcc 0.0595	BestValid 0.3115
	Epoch 1900:	Loss 2.3465	TrainAcc 0.2140	ValidAcc 0.0946	TestAcc 0.0745	BestValid 0.3115
	Epoch 1950:	Loss 2.3411	TrainAcc 0.2039	ValidAcc 0.0881	TestAcc 0.0683	BestValid 0.3115
	Epoch 2000:	Loss 2.3351	TrainAcc 0.2166	ValidAcc 0.0939	TestAcc 0.0733	BestValid 0.3115
	Epoch 2050:	Loss 2.3192	TrainAcc 0.2289	ValidAcc 0.1010	TestAcc 0.0796	BestValid 0.3115
	Epoch 2100:	Loss 2.3317	TrainAcc 0.2211	ValidAcc 0.0949	TestAcc 0.0743	BestValid 0.3115
	Epoch 2150:	Loss 2.3253	TrainAcc 0.2319	ValidAcc 0.1020	TestAcc 0.0803	BestValid 0.3115
	Epoch 2200:	Loss 2.3331	TrainAcc 0.1797	ValidAcc 0.0771	TestAcc 0.0590	BestValid 0.3115
	Epoch 2250:	Loss 2.3138	TrainAcc 0.2331	ValidAcc 0.1031	TestAcc 0.0813	BestValid 0.3115
	Epoch 2300:	Loss 2.3164	TrainAcc 0.1804	ValidAcc 0.0772	TestAcc 0.0590	BestValid 0.3115
	Epoch 2350:	Loss 2.3178	TrainAcc 0.2307	ValidAcc 0.1008	TestAcc 0.0795	BestValid 0.3115
	Epoch 2400:	Loss 2.3129	TrainAcc 0.2417	ValidAcc 0.1090	TestAcc 0.0866	BestValid 0.3115
	Epoch 2450:	Loss 2.3118	TrainAcc 0.2444	ValidAcc 0.1105	TestAcc 0.0881	BestValid 0.3115
	Epoch 2500:	Loss 2.3010	TrainAcc 0.2237	ValidAcc 0.0985	TestAcc 0.0769	BestValid 0.3115
	Epoch 2550:	Loss 2.3046	TrainAcc 0.2128	ValidAcc 0.0904	TestAcc 0.0709	BestValid 0.3115
	Epoch 2600:	Loss 2.2994	TrainAcc 0.1840	ValidAcc 0.0784	TestAcc 0.0604	BestValid 0.3115
	Epoch 2650:	Loss 2.3036	TrainAcc 0.2128	ValidAcc 0.0900	TestAcc 0.0704	BestValid 0.3115
	Epoch 2700:	Loss 2.3023	TrainAcc 0.1942	ValidAcc 0.0832	TestAcc 0.0646	BestValid 0.3115
	Epoch 2750:	Loss 2.2888	TrainAcc 0.2155	ValidAcc 0.0920	TestAcc 0.0720	BestValid 0.3115
	Epoch 2800:	Loss 2.2893	TrainAcc 0.1919	ValidAcc 0.0823	TestAcc 0.0632	BestValid 0.3115
	Epoch 2850:	Loss 2.2823	TrainAcc 0.2176	ValidAcc 0.0923	TestAcc 0.0719	BestValid 0.3115
	Epoch 2900:	Loss 2.2980	TrainAcc 0.2009	ValidAcc 0.0852	TestAcc 0.0660	BestValid 0.3115
	Epoch 2950:	Loss 2.2838	TrainAcc 0.2265	ValidAcc 0.0983	TestAcc 0.0772	BestValid 0.3115
	Epoch 3000:	Loss 2.2802	TrainAcc 0.2310	ValidAcc 0.1011	TestAcc 0.0790	BestValid 0.3115
	Epoch 3050:	Loss 2.2850	TrainAcc 0.2315	ValidAcc 0.1026	TestAcc 0.0799	BestValid 0.3115
	Epoch 3100:	Loss 2.2738	TrainAcc 0.2227	ValidAcc 0.0964	TestAcc 0.0755	BestValid 0.3115
	Epoch 3150:	Loss 2.2784	TrainAcc 0.2227	ValidAcc 0.0973	TestAcc 0.0765	BestValid 0.3115
	Epoch 3200:	Loss 2.2754	TrainAcc 0.2356	ValidAcc 0.1049	TestAcc 0.0828	BestValid 0.3115
	Epoch 3250:	Loss 2.2665	TrainAcc 0.1930	ValidAcc 0.0848	TestAcc 0.0661	BestValid 0.3115
	Epoch 3300:	Loss 2.2623	TrainAcc 0.2210	ValidAcc 0.0950	TestAcc 0.0743	BestValid 0.3115
	Epoch 3350:	Loss 2.2673	TrainAcc 0.2148	ValidAcc 0.0932	TestAcc 0.0731	BestValid 0.3115
	Epoch 3400:	Loss 2.2678	TrainAcc 0.2256	ValidAcc 0.0999	TestAcc 0.0787	BestValid 0.3115
	Epoch 3450:	Loss 2.2667	TrainAcc 0.1830	ValidAcc 0.0784	TestAcc 0.0601	BestValid 0.3115
	Epoch 3500:	Loss 2.2612	TrainAcc 0.1828	ValidAcc 0.0785	TestAcc 0.0605	BestValid 0.3115
	Epoch 3550:	Loss 2.2628	TrainAcc 0.2259	ValidAcc 0.0962	TestAcc 0.0758	BestValid 0.3115
	Epoch 3600:	Loss 2.2653	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.3115
	Epoch 3650:	Loss 2.2448	TrainAcc 0.1798	ValidAcc 0.0766	TestAcc 0.0588	BestValid 0.3115
	Epoch 3700:	Loss 2.2469	TrainAcc 0.1753	ValidAcc 0.0748	TestAcc 0.0572	BestValid 0.3115
	Epoch 3750:	Loss 2.2711	TrainAcc 0.1776	ValidAcc 0.0761	TestAcc 0.0581	BestValid 0.3115
	Epoch 3800:	Loss 2.2572	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.3115
	Epoch 3850:	Loss 2.2598	TrainAcc 0.1853	ValidAcc 0.0792	TestAcc 0.0609	BestValid 0.3115
	Epoch 3900:	Loss 2.2884	TrainAcc 0.1811	ValidAcc 0.0749	TestAcc 0.0597	BestValid 0.3115
	Epoch 3950:	Loss 2.2900	TrainAcc 0.2258	ValidAcc 0.0966	TestAcc 0.0766	BestValid 0.3115
	Epoch 4000:	Loss 2.2817	TrainAcc 0.1967	ValidAcc 0.0881	TestAcc 0.0674	BestValid 0.3115
	Epoch 4050:	Loss 2.2691	TrainAcc 0.1790	ValidAcc 0.0773	TestAcc 0.0591	BestValid 0.3115
	Epoch 4100:	Loss 2.2533	TrainAcc 0.0437	ValidAcc 0.0184	TestAcc 0.0175	BestValid 0.3115
	Epoch 4150:	Loss 2.2749	TrainAcc 0.1193	ValidAcc 0.0574	TestAcc 0.0486	BestValid 0.3115
	Epoch 4200:	Loss 2.2601	TrainAcc 0.1876	ValidAcc 0.0788	TestAcc 0.0611	BestValid 0.3115
	Epoch 4250:	Loss 2.2734	TrainAcc 0.1748	ValidAcc 0.0750	TestAcc 0.0608	BestValid 0.3115
	Epoch 4300:	Loss 2.2574	TrainAcc 0.0234	ValidAcc 0.0128	TestAcc 0.0107	BestValid 0.3115
	Epoch 4350:	Loss 2.2533	TrainAcc 0.0741	ValidAcc 0.0368	TestAcc 0.0312	BestValid 0.3115
	Epoch 4400:	Loss 2.2525	TrainAcc 0.0408	ValidAcc 0.0172	TestAcc 0.0155	BestValid 0.3115
	Epoch 4450:	Loss 2.2451	TrainAcc 0.0742	ValidAcc 0.0376	TestAcc 0.0328	BestValid 0.3115
	Epoch 4500:	Loss 2.2549	TrainAcc 0.0411	ValidAcc 0.0179	TestAcc 0.0159	BestValid 0.3115
	Epoch 4550:	Loss 2.2321	TrainAcc 0.1628	ValidAcc 0.0748	TestAcc 0.0577	BestValid 0.3115
	Epoch 4600:	Loss 2.2268	TrainAcc 0.0591	ValidAcc 0.0271	TestAcc 0.0240	BestValid 0.3115
	Epoch 4650:	Loss 2.2682	TrainAcc 0.0809	ValidAcc 0.0386	TestAcc 0.0336	BestValid 0.3115
	Epoch 4700:	Loss 2.2662	TrainAcc 0.2020	ValidAcc 0.0919	TestAcc 0.0720	BestValid 0.3115
	Epoch 4750:	Loss 2.2545	TrainAcc 0.2162	ValidAcc 0.0935	TestAcc 0.0746	BestValid 0.3115
	Epoch 4800:	Loss 2.2403	TrainAcc 0.0196	ValidAcc 0.0087	TestAcc 0.0082	BestValid 0.3115
	Epoch 4850:	Loss 2.2760	TrainAcc 0.0654	ValidAcc 0.0365	TestAcc 0.0280	BestValid 0.3115
	Epoch 4900:	Loss 2.2521	TrainAcc 0.0958	ValidAcc 0.0528	TestAcc 0.0414	BestValid 0.3115
	Epoch 4950:	Loss 2.2233	TrainAcc 0.2231	ValidAcc 0.0966	TestAcc 0.0771	BestValid 0.3115
	Epoch 5000:	Loss 2.2301	TrainAcc 0.2154	ValidAcc 0.0932	TestAcc 0.0732	BestValid 0.3115
****** Epoch Time (Excluding Evaluation Cost): 0.140 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 32.694 ms (Max: 34.886, Min: 20.042, Sum: 261.548)
Cluster-Wide Average, Compute: 77.313 ms (Max: 86.085, Min: 74.261, Sum: 618.505)
Cluster-Wide Average, Communication-Layer: 13.467 ms (Max: 15.435, Min: 9.935, Sum: 107.739)
Cluster-Wide Average, Bubble-Imbalance: 12.486 ms (Max: 14.688, Min: 7.090, Sum: 99.884)
Cluster-Wide Average, Communication-Graph: 0.473 ms (Max: 0.514, Min: 0.414, Sum: 3.787)
Cluster-Wide Average, Optimization: 0.178 ms (Max: 0.180, Min: 0.174, Sum: 1.425)
Cluster-Wide Average, Others: 3.303 ms (Max: 16.129, Min: 1.453, Sum: 26.427)
****** Breakdown Sum: 139.915 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.339 GB (Max: 4.044, Min: 3.046, Sum: 26.710)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 69.329 Gbps (Max: 83.418, Min: 49.813, Sum: 554.633)
Layer-level communication (cluster-wide, per-epoch): 0.883 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.883 GB
****** Accuracy Results ******
Highest valid_acc: 0.3115
Target test_acc: 0.2796
Epoch to reach the target acc: 1599
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
