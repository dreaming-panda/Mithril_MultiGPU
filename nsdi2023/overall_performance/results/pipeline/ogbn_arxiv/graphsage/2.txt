Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT

Initialized node 6 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
        It takes 0.054 seconds.
        It takes 0.050 seconds.
        It takes 0.050 seconds.
        It takes 0.052 seconds.
        It takes 0.055 seconds.
        It takes 0.051 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.069 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.023 seconds.
        It takes 0.025 seconds.
        It takes 0.027 seconds.
        It takes 0.025 seconds.
        It takes 0.028 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.639 Gbps (per GPU), 485.110 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.344 Gbps (per GPU), 482.755 Gbps (aggregated)
The layer-level communication performance: 60.348 Gbps (per GPU), 482.782 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.104 Gbps (per GPU), 480.832 Gbps (aggregated)
The layer-level communication performance: 60.078 Gbps (per GPU), 480.626 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.853 Gbps (per GPU), 478.823 Gbps (aggregated)
The layer-level communication performance: 59.801 Gbps (per GPU), 478.406 Gbps (aggregated)
The layer-level communication performance: 59.769 Gbps (per GPU), 478.149 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.264 Gbps (per GPU), 1266.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.257 Gbps (per GPU), 1266.059 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.264 Gbps (per GPU), 1266.110 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.255 Gbps (per GPU), 1266.038 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.177 Gbps (per GPU), 1265.417 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.165 Gbps (per GPU), 1265.322 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.162 Gbps (per GPU), 1265.298 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.028 Gbps (per GPU), 1264.225 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.174 Gbps (per GPU), 833.395 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.173 Gbps (per GPU), 833.381 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.174 Gbps (per GPU), 833.395 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.163 Gbps (per GPU), 833.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.175 Gbps (per GPU), 833.402 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.173 Gbps (per GPU), 833.381 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.175 Gbps (per GPU), 833.402 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.173 Gbps (per GPU), 833.382 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.829 Gbps (per GPU), 302.633 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.829 Gbps (per GPU), 302.634 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.828 Gbps (per GPU), 302.620 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.828 Gbps (per GPU), 302.621 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.820 Gbps (per GPU), 302.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.824 Gbps (per GPU), 302.595 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.823 Gbps (per GPU), 302.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.820 Gbps (per GPU), 302.564 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.59ms  0.51ms  0.36ms  1.66  5.55K  0.06M
 chk_1  0.60ms  0.51ms  0.36ms  1.66  5.75K  0.05M
 chk_2  0.60ms  0.52ms  0.37ms  1.62  5.20K  0.07M
 chk_3  0.60ms  0.52ms  0.37ms  1.65  5.57K  0.06M
 chk_4  0.60ms  0.51ms  0.36ms  1.66  5.05K  0.08M
 chk_5  0.61ms  0.54ms  0.40ms  1.55  4.73K  0.11M
 chk_6  0.59ms  0.51ms  0.37ms  1.60  4.98K  0.08M
 chk_7  0.60ms  0.52ms  0.37ms  1.60  5.01K  0.09M
 chk_8  0.59ms  0.52ms  0.36ms  1.65  5.73K  0.05M
 chk_9  0.59ms  0.51ms  0.37ms  1.59  4.54K  0.11M
chk_10  0.59ms  0.52ms  0.36ms  1.63  5.36K  0.07M
chk_11  0.60ms  0.52ms  0.37ms  1.63  5.39K  0.08M
chk_12  0.60ms  0.51ms  0.36ms  1.67  5.77K  0.05M
chk_13  0.60ms  0.52ms  0.36ms  1.64  5.43K  0.06M
chk_14  0.59ms  0.51ms  0.36ms  1.64  5.46K  0.06M
chk_15  0.59ms  0.51ms  0.36ms  1.66  5.88K  0.04M
chk_16  0.60ms  0.52ms  0.37ms  1.63  5.50K  0.06M
chk_17  0.61ms  0.53ms  0.39ms  1.58  4.86K  0.09M
chk_18  0.62ms  0.54ms  0.39ms  1.59  5.39K  0.07M
chk_19  0.60ms  0.52ms  0.37ms  1.61  5.20K  0.07M
chk_20  0.59ms  0.51ms  0.36ms  1.66  5.51K  0.06M
chk_21  0.59ms  0.50ms  0.35ms  1.68  5.81K  0.05M
chk_22  0.59ms  0.52ms  0.36ms  1.63  5.32K  0.07M
chk_23  0.61ms  0.53ms  0.38ms  1.59  5.39K  0.07M
chk_24  0.59ms  0.52ms  0.38ms  1.55  4.62K  0.11M
chk_25  0.59ms  0.52ms  0.37ms  1.59  5.04K  0.08M
chk_26  0.58ms  0.51ms  0.37ms  1.57  4.55K  0.11M
chk_27  0.58ms  0.51ms  0.36ms  1.62  5.30K  0.06M
chk_28  0.59ms  0.51ms  0.36ms  1.64  5.58K  0.06M
chk_29  0.60ms  0.53ms  0.38ms  1.57  4.98K  0.09M
chk_30  0.59ms  0.52ms  0.36ms  1.64  5.50K  0.07M
chk_31  0.60ms  0.52ms  0.37ms  1.63  5.38K  0.07M
   Avg  0.60  0.52  0.37
   Max  0.62  0.54  0.40
   Min  0.58  0.50  0.35
 Ratio  1.06  1.07  1.13
   Var  0.00  0.00  0.00
Profiling takes 0.699 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 162)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 98)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [194, 226)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [98, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [226, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [162, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 66)...
+++++++++ Node 4 initializing the weights for op[130, 162)...
+++++++++ Node 2 initializing the weights for op[66, 98)...
+++++++++ Node 5 initializing the weights for op[162, 194)...
+++++++++ Node 3 initializing the weights for op[98, 130)...
+++++++++ Node 6 initializing the weights for op[194, 226)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[226, 257)...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 8.6419	TrainAcc 0.0139	ValidAcc 0.0061	TestAcc 0.0054	BestValid 0.0061
	Epoch 50:	Loss 3.6884	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 100:	Loss 3.1434	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 150:	Loss 2.8710	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 200:	Loss 2.8443	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 250:	Loss 2.8515	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 300:	Loss 2.8762	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 350:	Loss 2.8601	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 400:	Loss 2.8230	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 450:	Loss 2.8159	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 500:	Loss 2.8093	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 550:	Loss 2.8150	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 600:	Loss 2.8105	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 650:	Loss 2.8025	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 700:	Loss 2.8010	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 750:	Loss 2.7951	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 800:	Loss 2.7962	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 850:	Loss 2.7912	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 900:	Loss 2.7914	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 950:	Loss 2.7894	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1000:	Loss 2.7898	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1050:	Loss 2.7871	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1100:	Loss 2.7845	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1150:	Loss 2.7835	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1200:	Loss 2.7822	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1250:	Loss 2.7828	TrainAcc 0.1792	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1300:	Loss 2.7766	TrainAcc 0.1792	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1350:	Loss 2.7748	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1400:	Loss 2.7758	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1450:	Loss 2.7732	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1500:	Loss 2.7758	TrainAcc 0.1791	ValidAcc 0.0764	TestAcc 0.0587	BestValid 0.0764
	Epoch 1550:	Loss 2.7734	TrainAcc 0.1840	ValidAcc 0.0830	TestAcc 0.0613	BestValid 0.0830
	Epoch 1600:	Loss 2.7756	TrainAcc 0.1854	ValidAcc 0.0845	TestAcc 0.0620	BestValid 0.0845
	Epoch 1650:	Loss 2.7651	TrainAcc 0.1875	ValidAcc 0.0882	TestAcc 0.0636	BestValid 0.0882
	Epoch 1700:	Loss 2.7703	TrainAcc 0.1877	ValidAcc 0.0895	TestAcc 0.0638	BestValid 0.0895
	Epoch 1750:	Loss 2.7429	TrainAcc 0.1903	ValidAcc 0.0924	TestAcc 0.0746	BestValid 0.0924
	Epoch 1800:	Loss 2.7196	TrainAcc 0.1730	ValidAcc 0.0685	TestAcc 0.0522	BestValid 0.0924
	Epoch 1850:	Loss 2.6751	TrainAcc 0.2151	ValidAcc 0.0926	TestAcc 0.0726	BestValid 0.0926
	Epoch 1900:	Loss 2.6661	TrainAcc 0.1846	ValidAcc 0.0779	TestAcc 0.0596	BestValid 0.0926
	Epoch 1950:	Loss 2.6414	TrainAcc 0.1792	ValidAcc 0.0730	TestAcc 0.0565	BestValid 0.0926
	Epoch 2000:	Loss 2.6317	TrainAcc 0.2032	ValidAcc 0.0857	TestAcc 0.0661	BestValid 0.0926
	Epoch 2050:	Loss 2.6276	TrainAcc 0.0582	ValidAcc 0.0217	TestAcc 0.0178	BestValid 0.0926
	Epoch 2100:	Loss 2.4999	TrainAcc 0.2168	ValidAcc 0.0924	TestAcc 0.0729	BestValid 0.0926
	Epoch 2150:	Loss 2.4569	TrainAcc 0.2126	ValidAcc 0.0898	TestAcc 0.0704	BestValid 0.0926
	Epoch 2200:	Loss 2.4357	TrainAcc 0.1847	ValidAcc 0.0790	TestAcc 0.0608	BestValid 0.0926
	Epoch 2250:	Loss 2.4101	TrainAcc 0.1843	ValidAcc 0.0791	TestAcc 0.0606	BestValid 0.0926
	Epoch 2300:	Loss 2.3951	TrainAcc 0.1820	ValidAcc 0.0778	TestAcc 0.0599	BestValid 0.0926
	Epoch 2350:	Loss 2.3752	TrainAcc 0.1821	ValidAcc 0.0778	TestAcc 0.0598	BestValid 0.0926
	Epoch 2400:	Loss 2.3685	TrainAcc 0.1922	ValidAcc 0.0828	TestAcc 0.0637	BestValid 0.0926
	Epoch 2450:	Loss 2.3783	TrainAcc 0.1901	ValidAcc 0.0812	TestAcc 0.0628	BestValid 0.0926
	Epoch 2500:	Loss 2.3525	TrainAcc 0.1906	ValidAcc 0.0809	TestAcc 0.0626	BestValid 0.0926
	Epoch 2550:	Loss 2.3407	TrainAcc 0.1794	ValidAcc 0.0767	TestAcc 0.0588	BestValid 0.0926
	Epoch 2600:	Loss 2.3513	TrainAcc 0.2320	ValidAcc 0.1030	TestAcc 0.0811	BestValid 0.1030
	Epoch 2650:	Loss 2.3472	TrainAcc 0.2080	ValidAcc 0.0916	TestAcc 0.0711	BestValid 0.1030
	Epoch 2700:	Loss 2.3356	TrainAcc 0.2108	ValidAcc 0.0898	TestAcc 0.0699	BestValid 0.1030
	Epoch 2750:	Loss 2.3361	TrainAcc 0.2209	ValidAcc 0.0943	TestAcc 0.0736	BestValid 0.1030
	Epoch 2800:	Loss 2.3178	TrainAcc 0.2102	ValidAcc 0.0888	TestAcc 0.0693	BestValid 0.1030
	Epoch 2850:	Loss 2.3129	TrainAcc 0.1959	ValidAcc 0.0828	TestAcc 0.0639	BestValid 0.1030
	Epoch 2900:	Loss 2.3316	TrainAcc 0.1858	ValidAcc 0.0793	TestAcc 0.0612	BestValid 0.1030
	Epoch 2950:	Loss 2.3268	TrainAcc 0.2145	ValidAcc 0.0907	TestAcc 0.0712	BestValid 0.1030
	Epoch 3000:	Loss 2.3097	TrainAcc 0.1801	ValidAcc 0.0769	TestAcc 0.0589	BestValid 0.1030
	Epoch 3050:	Loss 2.3139	TrainAcc 0.1792	ValidAcc 0.0764	TestAcc 0.0587	BestValid 0.1030
	Epoch 3100:	Loss 2.3035	TrainAcc 0.1806	ValidAcc 0.0773	TestAcc 0.0593	BestValid 0.1030
	Epoch 3150:	Loss 2.3033	TrainAcc 0.2194	ValidAcc 0.0934	TestAcc 0.0728	BestValid 0.1030
	Epoch 3200:	Loss 2.3086	TrainAcc 0.2264	ValidAcc 0.0986	TestAcc 0.0774	BestValid 0.1030
	Epoch 3250:	Loss 2.3006	TrainAcc 0.1865	ValidAcc 0.0801	TestAcc 0.0618	BestValid 0.1030
	Epoch 3300:	Loss 2.2943	TrainAcc 0.1989	ValidAcc 0.0843	TestAcc 0.0652	BestValid 0.1030
	Epoch 3350:	Loss 2.2936	TrainAcc 0.2290	ValidAcc 0.1006	TestAcc 0.0795	BestValid 0.1030
	Epoch 3400:	Loss 2.2782	TrainAcc 0.2042	ValidAcc 0.0883	TestAcc 0.0688	BestValid 0.1030
	Epoch 3450:	Loss 2.2780	TrainAcc 0.2107	ValidAcc 0.0894	TestAcc 0.0702	BestValid 0.1030
	Epoch 3500:	Loss 2.2830	TrainAcc 0.2231	ValidAcc 0.0947	TestAcc 0.0747	BestValid 0.1030
	Epoch 3550:	Loss 2.2677	TrainAcc 0.1826	ValidAcc 0.0782	TestAcc 0.0599	BestValid 0.1030
	Epoch 3600:	Loss 2.2695	TrainAcc 0.2185	ValidAcc 0.0930	TestAcc 0.0735	BestValid 0.1030
	Epoch 3650:	Loss 2.2645	TrainAcc 0.2242	ValidAcc 0.0955	TestAcc 0.0754	BestValid 0.1030
	Epoch 3700:	Loss 2.2548	TrainAcc 0.2245	ValidAcc 0.0958	TestAcc 0.0753	BestValid 0.1030
	Epoch 3750:	Loss 2.2627	TrainAcc 0.2315	ValidAcc 0.1025	TestAcc 0.0810	BestValid 0.1030
	Epoch 3800:	Loss 2.2546	TrainAcc 0.2312	ValidAcc 0.1036	TestAcc 0.0817	BestValid 0.1036
	Epoch 3850:	Loss 2.2602	TrainAcc 0.2218	ValidAcc 0.0946	TestAcc 0.0749	BestValid 0.1036
	Epoch 3900:	Loss 2.2642	TrainAcc 0.2324	ValidAcc 0.1023	TestAcc 0.0816	BestValid 0.1036
	Epoch 3950:	Loss 2.2604	TrainAcc 0.2070	ValidAcc 0.0924	TestAcc 0.0721	BestValid 0.1036
	Epoch 4000:	Loss 2.2446	TrainAcc 0.1833	ValidAcc 0.0789	TestAcc 0.0606	BestValid 0.1036
	Epoch 4050:	Loss 2.2472	TrainAcc 0.1805	ValidAcc 0.0771	TestAcc 0.0593	BestValid 0.1036
	Epoch 4100:	Loss 2.2525	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0587	BestValid 0.1036
	Epoch 4150:	Loss 2.2454	TrainAcc 0.1792	ValidAcc 0.0765	TestAcc 0.0587	BestValid 0.1036
	Epoch 4200:	Loss 2.2371	TrainAcc 0.1806	ValidAcc 0.0774	TestAcc 0.0594	BestValid 0.1036
	Epoch 4250:	Loss 2.2359	TrainAcc 0.1837	ValidAcc 0.0788	TestAcc 0.0606	BestValid 0.1036
	Epoch 4300:	Loss 2.2329	TrainAcc 0.2088	ValidAcc 0.0933	TestAcc 0.0726	BestValid 0.1036
	Epoch 4350:	Loss 2.2407	TrainAcc 0.1829	ValidAcc 0.0784	TestAcc 0.0602	BestValid 0.1036
	Epoch 4400:	Loss 2.2249	TrainAcc 0.1801	ValidAcc 0.0772	TestAcc 0.0591	BestValid 0.1036
	Epoch 4450:	Loss 2.2143	TrainAcc 0.2080	ValidAcc 0.0914	TestAcc 0.0713	BestValid 0.1036
	Epoch 4500:	Loss 2.2099	TrainAcc 0.2132	ValidAcc 0.0963	TestAcc 0.0763	BestValid 0.1036
	Epoch 4550:	Loss 2.2161	TrainAcc 0.2134	ValidAcc 0.0958	TestAcc 0.0750	BestValid 0.1036
	Epoch 4600:	Loss 2.2185	TrainAcc 0.1869	ValidAcc 0.0799	TestAcc 0.0611	BestValid 0.1036
	Epoch 4650:	Loss 2.2248	TrainAcc 0.1792	ValidAcc 0.0763	TestAcc 0.0587	BestValid 0.1036
	Epoch 4700:	Loss 2.2207	TrainAcc 0.2207	ValidAcc 0.1005	TestAcc 0.0796	BestValid 0.1036
	Epoch 4750:	Loss 2.2120	TrainAcc 0.1848	ValidAcc 0.0788	TestAcc 0.0609	BestValid 0.1036
	Epoch 4800:	Loss 2.2115	TrainAcc 0.2056	ValidAcc 0.0863	TestAcc 0.0675	BestValid 0.1036
	Epoch 4850:	Loss 2.2247	TrainAcc 0.1950	ValidAcc 0.0821	TestAcc 0.0639	BestValid 0.1036
	Epoch 4900:	Loss 2.2445	TrainAcc 0.1791	ValidAcc 0.0765	TestAcc 0.0587	BestValid 0.1036
	Epoch 4950:	Loss 2.2208	TrainAcc 0.1849	ValidAcc 0.0791	TestAcc 0.0605	BestValid 0.1036
	Epoch 5000:	Loss 2.2228	TrainAcc 0.2170	ValidAcc 0.0917	TestAcc 0.0729	BestValid 0.1036
****** Epoch Time (Excluding Evaluation Cost): 0.138 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 32.470 ms (Max: 34.648, Min: 19.779, Sum: 259.761)
Cluster-Wide Average, Compute: 76.529 ms (Max: 85.595, Min: 73.613, Sum: 612.231)
Cluster-Wide Average, Communication-Layer: 13.375 ms (Max: 15.224, Min: 9.780, Sum: 106.999)
Cluster-Wide Average, Bubble-Imbalance: 12.311 ms (Max: 15.696, Min: 6.673, Sum: 98.490)
Cluster-Wide Average, Communication-Graph: 0.458 ms (Max: 0.535, Min: 0.395, Sum: 3.661)
Cluster-Wide Average, Optimization: 0.177 ms (Max: 0.180, Min: 0.173, Sum: 1.412)
Cluster-Wide Average, Others: 3.278 ms (Max: 16.104, Min: 1.433, Sum: 26.227)
****** Breakdown Sum: 138.598 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.339 GB (Max: 4.044, Min: 3.046, Sum: 26.710)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 69.802 Gbps (Max: 83.569, Min: 49.948, Sum: 558.416)
Layer-level communication (cluster-wide, per-epoch): 0.883 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.883 GB
****** Accuracy Results ******
Highest valid_acc: 0.1036
Target test_acc: 0.0817
Epoch to reach the target acc: 3799
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
