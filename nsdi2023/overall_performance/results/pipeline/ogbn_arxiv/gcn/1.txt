Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
DONE MPI INIT
Initialized node 3 on machine gnerv7
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
        It takes 0.053 seconds.
        It takes 0.054 seconds.
        It takes 0.050 seconds.
        It takes 0.059 seconds.
        It takes 0.059 seconds.
        It takes 0.059 seconds.
        It takes 0.059 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.127 seconds.
        It takes 0.124 seconds.
        It takes 0.128 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.126 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.135 seconds.
        It takes 0.135 seconds.
        It takes 0.134 seconds.
        It takes 0.135 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.046 seconds.
        It takes 0.046 seconds.
        It takes 0.046 seconds.
        It takes 0.046 seconds.
        It takes 0.054 seconds.
        It takes 0.054 seconds.
        It takes 0.054 seconds.
        It takes 0.054 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.071 Gbps (per GPU), 480.570 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.720 Gbps (per GPU), 485.758 Gbps (aggregated)
The layer-level communication performance: 59.760 Gbps (per GPU), 478.084 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.520 Gbps (per GPU), 476.160 Gbps (aggregated)
The layer-level communication performance: 59.494 Gbps (per GPU), 475.950 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.288 Gbps (per GPU), 474.303 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.186 Gbps (per GPU), 481.486 Gbps (aggregated)
The layer-level communication performance: 59.211 Gbps (per GPU), 473.691 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 146.590 Gbps (per GPU), 1172.718 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.569 Gbps (per GPU), 1172.555 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.585 Gbps (per GPU), 1172.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.475 Gbps (per GPU), 1171.798 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.585 Gbps (per GPU), 1172.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.467 Gbps (per GPU), 1171.737 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.590 Gbps (per GPU), 1172.717 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 146.496 Gbps (per GPU), 1171.965 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 93.703 Gbps (per GPU), 749.624 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.717 Gbps (per GPU), 749.736 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.717 Gbps (per GPU), 749.736 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.713 Gbps (per GPU), 749.702 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.717 Gbps (per GPU), 749.736 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.700 Gbps (per GPU), 749.596 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.694 Gbps (per GPU), 749.552 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 93.641 Gbps (per GPU), 749.128 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.298 Gbps (per GPU), 298.383 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.297 Gbps (per GPU), 298.378 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.298 Gbps (per GPU), 298.382 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.298 Gbps (per GPU), 298.381 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.294 Gbps (per GPU), 298.355 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.295 Gbps (per GPU), 298.359 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.294 Gbps (per GPU), 298.356 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.294 Gbps (per GPU), 298.355 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.30ms  0.33ms  0.23ms  1.46  5.55K  0.06M
 chk_1  0.30ms  0.33ms  0.23ms  1.44  5.75K  0.05M
 chk_2  0.31ms  0.34ms  0.24ms  1.42  5.20K  0.07M
 chk_3  0.31ms  0.34ms  0.24ms  1.43  5.57K  0.06M
 chk_4  0.31ms  0.34ms  0.24ms  1.42  5.05K  0.08M
 chk_5  0.33ms  0.36ms  0.27ms  1.37  4.73K  0.11M
 chk_6  0.30ms  0.33ms  0.24ms  1.42  4.98K  0.08M
 chk_7  0.31ms  0.34ms  0.24ms  1.40  5.01K  0.09M
 chk_8  0.30ms  0.33ms  0.23ms  1.45  5.73K  0.05M
 chk_9  0.31ms  0.34ms  0.24ms  1.40  4.54K  0.11M
chk_10  0.31ms  0.34ms  0.24ms  1.43  5.36K  0.07M
chk_11  0.31ms  0.34ms  0.24ms  1.41  5.39K  0.08M
chk_12  0.30ms  0.33ms  0.23ms  1.45  5.77K  0.05M
chk_13  0.31ms  0.34ms  0.24ms  1.42  5.43K  0.06M
chk_14  0.31ms  0.33ms  0.23ms  1.44  5.46K  0.06M
chk_15  0.30ms  0.33ms  0.23ms  1.46  5.88K  0.04M
chk_16  0.31ms  0.34ms  0.24ms  1.43  5.50K  0.06M
chk_17  0.33ms  0.36ms  0.26ms  1.39  4.86K  0.09M
chk_18  0.33ms  0.36ms  0.26ms  1.38  5.39K  0.07M
chk_19  0.31ms  0.35ms  0.24ms  1.42  5.20K  0.07M
chk_20  0.30ms  0.33ms  0.23ms  1.46  5.51K  0.06M
chk_21  0.29ms  0.32ms  0.22ms  1.48  5.81K  0.05M
chk_22  0.31ms  0.34ms  0.24ms  1.44  5.32K  0.07M
chk_23  0.32ms  0.35ms  0.25ms  1.39  5.39K  0.07M
chk_24  0.32ms  0.35ms  0.25ms  1.38  4.62K  0.11M
chk_25  0.31ms  0.39ms  0.24ms  1.59  5.04K  0.08M
chk_26  0.31ms  0.34ms  0.24ms  1.41  4.55K  0.11M
chk_27  0.30ms  0.38ms  0.23ms  1.67  5.30K  0.06M
chk_28  0.31ms  0.34ms  0.23ms  1.44  5.58K  0.06M
chk_29  0.32ms  0.35ms  0.25ms  1.38  4.98K  0.09M
chk_30  0.31ms  0.34ms  0.23ms  1.47  5.50K  0.07M
chk_31  0.31ms  0.34ms  0.23ms  1.45  5.38K  0.07M
   Avg  0.31  0.34  0.24
   Max  0.33  0.39  0.27
   Min  0.29  0.32  0.22
 Ratio  1.14  1.20  1.21
   Var  0.00  0.00  0.00
Profiling takes 0.449 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
+++++++++ Node 4 initializing the weights for op[81, 101)...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
+++++++++ Node 7 initializing the weights for op[141, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.6889	TrainAcc 0.0215	ValidAcc 0.0113	TestAcc 0.0096	BestValid 0.0113
	Epoch 50:	Loss 3.6889	TrainAcc 0.0315	ValidAcc 0.0379	TestAcc 0.0385	BestValid 0.0379
	Epoch 100:	Loss 3.6889	TrainAcc 0.0315	ValidAcc 0.0379	TestAcc 0.0385	BestValid 0.0379
	Epoch 150:	Loss 3.6889	TrainAcc 0.0315	ValidAcc 0.0379	TestAcc 0.0385	BestValid 0.0379
	Epoch 200:	Loss 3.6889	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299	BestValid 0.0413
	Epoch 250:	Loss 3.6887	TrainAcc 0.0597	ValidAcc 0.0345	TestAcc 0.0291	BestValid 0.0413
	Epoch 300:	Loss 3.2901	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 350:	Loss 2.9113	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 400:	Loss 2.8427	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 450:	Loss 2.7653	TrainAcc 0.1804	ValidAcc 0.0765	TestAcc 0.0588	BestValid 0.0765
	Epoch 500:	Loss 2.5527	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0765
	Epoch 550:	Loss 2.4901	TrainAcc 0.1799	ValidAcc 0.0764	TestAcc 0.0588	BestValid 0.0765
	Epoch 600:	Loss 2.4806	TrainAcc 0.2842	ValidAcc 0.3063	TestAcc 0.2778	BestValid 0.3063
	Epoch 650:	Loss 2.4534	TrainAcc 0.2922	ValidAcc 0.3131	TestAcc 0.2870	BestValid 0.3131
	Epoch 700:	Loss 2.4285	TrainAcc 0.2948	ValidAcc 0.3168	TestAcc 0.2940	BestValid 0.3168
	Epoch 750:	Loss 2.4080	TrainAcc 0.3039	ValidAcc 0.3292	TestAcc 0.3172	BestValid 0.3292
	Epoch 800:	Loss 2.3927	TrainAcc 0.3103	ValidAcc 0.3470	TestAcc 0.3502	BestValid 0.3470
	Epoch 850:	Loss 2.3794	TrainAcc 0.3121	ValidAcc 0.3489	TestAcc 0.3511	BestValid 0.3489
	Epoch 900:	Loss 2.3624	TrainAcc 0.3155	ValidAcc 0.3673	TestAcc 0.3866	BestValid 0.3673
	Epoch 950:	Loss 2.3413	TrainAcc 0.3190	ValidAcc 0.3745	TestAcc 0.4103	BestValid 0.3745
	Epoch 1000:	Loss 2.2664	TrainAcc 0.3120	ValidAcc 0.2477	TestAcc 0.2289	BestValid 0.3745
	Epoch 1050:	Loss 2.2090	TrainAcc 0.3208	ValidAcc 0.2287	TestAcc 0.2067	BestValid 0.3745
	Epoch 1100:	Loss 2.1665	TrainAcc 0.2734	ValidAcc 0.1393	TestAcc 0.1047	BestValid 0.3745
	Epoch 1150:	Loss 2.1504	TrainAcc 0.2745	ValidAcc 0.1400	TestAcc 0.1062	BestValid 0.3745
	Epoch 1200:	Loss 2.1175	TrainAcc 0.3403	ValidAcc 0.2696	TestAcc 0.2572	BestValid 0.3745
	Epoch 1250:	Loss 2.0920	TrainAcc 0.3319	ValidAcc 0.2557	TestAcc 0.2419	BestValid 0.3745
	Epoch 1300:	Loss 2.0709	TrainAcc 0.3511	ValidAcc 0.2641	TestAcc 0.2529	BestValid 0.3745
	Epoch 1350:	Loss 2.0582	TrainAcc 0.3469	ValidAcc 0.2542	TestAcc 0.2390	BestValid 0.3745
	Epoch 1400:	Loss 2.0490	TrainAcc 0.3763	ValidAcc 0.3650	TestAcc 0.3646	BestValid 0.3745
	Epoch 1450:	Loss 2.0896	TrainAcc 0.3054	ValidAcc 0.1501	TestAcc 0.1194	BestValid 0.3745
	Epoch 1500:	Loss 2.0512	TrainAcc 0.4404	ValidAcc 0.4136	TestAcc 0.4018	BestValid 0.4136
	Epoch 1550:	Loss 1.9927	TrainAcc 0.3816	ValidAcc 0.2683	TestAcc 0.2451	BestValid 0.4136
	Epoch 1600:	Loss 1.9662	TrainAcc 0.3875	ValidAcc 0.2813	TestAcc 0.2647	BestValid 0.4136
	Epoch 1650:	Loss 1.9530	TrainAcc 0.3394	ValidAcc 0.1702	TestAcc 0.1399	BestValid 0.4136
	Epoch 1700:	Loss 1.9573	TrainAcc 0.3777	ValidAcc 0.2481	TestAcc 0.2408	BestValid 0.4136
	Epoch 1750:	Loss 1.9511	TrainAcc 0.3107	ValidAcc 0.1462	TestAcc 0.1205	BestValid 0.4136
	Epoch 1800:	Loss 1.8895	TrainAcc 0.4023	ValidAcc 0.3011	TestAcc 0.2957	BestValid 0.4136
	Epoch 1850:	Loss 1.8669	TrainAcc 0.3931	ValidAcc 0.2619	TestAcc 0.2377	BestValid 0.4136
	Epoch 1900:	Loss 1.8464	TrainAcc 0.3611	ValidAcc 0.2032	TestAcc 0.1657	BestValid 0.4136
	Epoch 1950:	Loss 1.8344	TrainAcc 0.4740	ValidAcc 0.4239	TestAcc 0.3908	BestValid 0.4239
	Epoch 2000:	Loss 1.8256	TrainAcc 0.3295	ValidAcc 0.1565	TestAcc 0.1282	BestValid 0.4239
	Epoch 2050:	Loss 1.8215	TrainAcc 0.4508	ValidAcc 0.4057	TestAcc 0.3699	BestValid 0.4239
	Epoch 2100:	Loss 1.8080	TrainAcc 0.3746	ValidAcc 0.2174	TestAcc 0.1840	BestValid 0.4239
	Epoch 2150:	Loss 1.8031	TrainAcc 0.4014	ValidAcc 0.2955	TestAcc 0.2793	BestValid 0.4239
	Epoch 2200:	Loss 1.8110	TrainAcc 0.3294	ValidAcc 0.1576	TestAcc 0.1288	BestValid 0.4239
	Epoch 2250:	Loss 1.7849	TrainAcc 0.4580	ValidAcc 0.4222	TestAcc 0.3920	BestValid 0.4239
	Epoch 2300:	Loss 1.7809	TrainAcc 0.4636	ValidAcc 0.4355	TestAcc 0.4072	BestValid 0.4355
	Epoch 2350:	Loss 1.7819	TrainAcc 0.2864	ValidAcc 0.1866	TestAcc 0.1998	BestValid 0.4355
	Epoch 2400:	Loss 1.7716	TrainAcc 0.4566	ValidAcc 0.4091	TestAcc 0.3899	BestValid 0.4355
	Epoch 2450:	Loss 1.7580	TrainAcc 0.2783	ValidAcc 0.1343	TestAcc 0.1137	BestValid 0.4355
	Epoch 2500:	Loss 1.7507	TrainAcc 0.4373	ValidAcc 0.3244	TestAcc 0.3092	BestValid 0.4355
	Epoch 2550:	Loss 1.7514	TrainAcc 0.3570	ValidAcc 0.1906	TestAcc 0.1701	BestValid 0.4355
	Epoch 2600:	Loss 1.7444	TrainAcc 0.4852	ValidAcc 0.4493	TestAcc 0.4240	BestValid 0.4493
	Epoch 2650:	Loss 1.7393	TrainAcc 0.3587	ValidAcc 0.1837	TestAcc 0.1569	BestValid 0.4493
	Epoch 2700:	Loss 1.7381	TrainAcc 0.3472	ValidAcc 0.1975	TestAcc 0.1776	BestValid 0.4493
	Epoch 2750:	Loss 1.7306	TrainAcc 0.3575	ValidAcc 0.1959	TestAcc 0.1762	BestValid 0.4493
	Epoch 2800:	Loss 1.7328	TrainAcc 0.4158	ValidAcc 0.3093	TestAcc 0.3379	BestValid 0.4493
	Epoch 2850:	Loss 1.7235	TrainAcc 0.4771	ValidAcc 0.4256	TestAcc 0.3924	BestValid 0.4493
	Epoch 2900:	Loss 1.7264	TrainAcc 0.4046	ValidAcc 0.2879	TestAcc 0.3069	BestValid 0.4493
	Epoch 2950:	Loss 1.7223	TrainAcc 0.3666	ValidAcc 0.1910	TestAcc 0.1625	BestValid 0.4493
	Epoch 3000:	Loss 1.7048	TrainAcc 0.4431	ValidAcc 0.3484	TestAcc 0.3468	BestValid 0.4493
	Epoch 3050:	Loss 1.7008	TrainAcc 0.3717	ValidAcc 0.1938	TestAcc 0.1536	BestValid 0.4493
	Epoch 3100:	Loss 1.7018	TrainAcc 0.3406	ValidAcc 0.1639	TestAcc 0.1370	BestValid 0.4493
	Epoch 3150:	Loss 1.6923	TrainAcc 0.3699	ValidAcc 0.1913	TestAcc 0.1578	BestValid 0.4493
	Epoch 3200:	Loss 1.6934	TrainAcc 0.3149	ValidAcc 0.2507	TestAcc 0.3037	BestValid 0.4493
	Epoch 3250:	Loss 1.6901	TrainAcc 0.3160	ValidAcc 0.1526	TestAcc 0.1300	BestValid 0.4493
	Epoch 3300:	Loss 1.6797	TrainAcc 0.5303	ValidAcc 0.4972	TestAcc 0.4850	BestValid 0.4972
	Epoch 3350:	Loss 1.6741	TrainAcc 0.4502	ValidAcc 0.4103	TestAcc 0.3768	BestValid 0.4972
	Epoch 3400:	Loss 1.6735	TrainAcc 0.4210	ValidAcc 0.3153	TestAcc 0.2965	BestValid 0.4972
	Epoch 3450:	Loss 1.6669	TrainAcc 0.3827	ValidAcc 0.1917	TestAcc 0.1605	BestValid 0.4972
	Epoch 3500:	Loss 1.6686	TrainAcc 0.4277	ValidAcc 0.2823	TestAcc 0.2822	BestValid 0.4972
	Epoch 3550:	Loss 1.6736	TrainAcc 0.3094	ValidAcc 0.1633	TestAcc 0.1338	BestValid 0.4972
	Epoch 3600:	Loss 1.6656	TrainAcc 0.3770	ValidAcc 0.1864	TestAcc 0.1554	BestValid 0.4972
	Epoch 3650:	Loss 1.6563	TrainAcc 0.2797	ValidAcc 0.1366	TestAcc 0.1169	BestValid 0.4972
	Epoch 3700:	Loss 1.6587	TrainAcc 0.4811	ValidAcc 0.4204	TestAcc 0.3784	BestValid 0.4972
	Epoch 3750:	Loss 1.6606	TrainAcc 0.5350	ValidAcc 0.5387	TestAcc 0.5229	BestValid 0.5387
	Epoch 3800:	Loss 1.6589	TrainAcc 0.4265	ValidAcc 0.3157	TestAcc 0.3542	BestValid 0.5387
	Epoch 3850:	Loss 1.6407	TrainAcc 0.4720	ValidAcc 0.4136	TestAcc 0.3731	BestValid 0.5387
	Epoch 3900:	Loss 1.6400	TrainAcc 0.3760	ValidAcc 0.1982	TestAcc 0.1682	BestValid 0.5387
	Epoch 3950:	Loss 1.6288	TrainAcc 0.3541	ValidAcc 0.1791	TestAcc 0.1547	BestValid 0.5387
	Epoch 4000:	Loss 1.6332	TrainAcc 0.4973	ValidAcc 0.4331	TestAcc 0.3982	BestValid 0.5387
	Epoch 4050:	Loss 1.6407	TrainAcc 0.4894	ValidAcc 0.4159	TestAcc 0.3752	BestValid 0.5387
	Epoch 4100:	Loss 1.6301	TrainAcc 0.3693	ValidAcc 0.3370	TestAcc 0.3033	BestValid 0.5387
	Epoch 4150:	Loss 1.6224	TrainAcc 0.4148	ValidAcc 0.2192	TestAcc 0.1846	BestValid 0.5387
	Epoch 4200:	Loss 1.6262	TrainAcc 0.2787	ValidAcc 0.1356	TestAcc 0.1152	BestValid 0.5387
	Epoch 4250:	Loss 1.6210	TrainAcc 0.5543	ValidAcc 0.5329	TestAcc 0.5124	BestValid 0.5387
	Epoch 4300:	Loss 1.6158	TrainAcc 0.2797	ValidAcc 0.1329	TestAcc 0.1123	BestValid 0.5387
	Epoch 4350:	Loss 1.6084	TrainAcc 0.5176	ValidAcc 0.4378	TestAcc 0.3987	BestValid 0.5387
	Epoch 4400:	Loss 1.6079	TrainAcc 0.3243	ValidAcc 0.1591	TestAcc 0.1504	BestValid 0.5387
	Epoch 4450:	Loss 1.6098	TrainAcc 0.5385	ValidAcc 0.5234	TestAcc 0.5345	BestValid 0.5387
	Epoch 4500:	Loss 1.6119	TrainAcc 0.3017	ValidAcc 0.1321	TestAcc 0.1070	BestValid 0.5387
	Epoch 4550:	Loss 1.6100	TrainAcc 0.3872	ValidAcc 0.1996	TestAcc 0.1729	BestValid 0.5387
	Epoch 4600:	Loss 1.6069	TrainAcc 0.3675	ValidAcc 0.1726	TestAcc 0.1460	BestValid 0.5387
	Epoch 4650:	Loss 1.6102	TrainAcc 0.3075	ValidAcc 0.1384	TestAcc 0.1152	BestValid 0.5387
	Epoch 4700:	Loss 1.6091	TrainAcc 0.5321	ValidAcc 0.4604	TestAcc 0.4237	BestValid 0.5387
	Epoch 4750:	Loss 1.6060	TrainAcc 0.3570	ValidAcc 0.2471	TestAcc 0.2757	BestValid 0.5387
	Epoch 4800:	Loss 1.5993	TrainAcc 0.3257	ValidAcc 0.1723	TestAcc 0.1546	BestValid 0.5387
	Epoch 4850:	Loss 1.5959	TrainAcc 0.3885	ValidAcc 0.1952	TestAcc 0.1740	BestValid 0.5387
	Epoch 4900:	Loss 1.6114	TrainAcc 0.4270	ValidAcc 0.2690	TestAcc 0.2609	BestValid 0.5387
	Epoch 4950:	Loss 1.6020	TrainAcc 0.3720	ValidAcc 0.2051	TestAcc 0.1780	BestValid 0.5387
	Epoch 5000:	Loss 1.6084	TrainAcc 0.4561	ValidAcc 0.3917	TestAcc 0.3579	BestValid 0.5387
****** Epoch Time (Excluding Evaluation Cost): 0.102 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 26.293 ms (Max: 29.103, Min: 13.095, Sum: 210.347)
Cluster-Wide Average, Compute: 47.242 ms (Max: 57.356, Min: 44.781, Sum: 377.936)
Cluster-Wide Average, Communication-Layer: 12.173 ms (Max: 14.353, Min: 8.676, Sum: 97.383)
Cluster-Wide Average, Bubble-Imbalance: 12.784 ms (Max: 15.948, Min: 6.541, Sum: 102.275)
Cluster-Wide Average, Communication-Graph: 0.466 ms (Max: 0.508, Min: 0.392, Sum: 3.729)
Cluster-Wide Average, Optimization: 0.091 ms (Max: 0.098, Min: 0.089, Sum: 0.727)
Cluster-Wide Average, Others: 3.157 ms (Max: 15.957, Min: 1.317, Sum: 25.259)
****** Breakdown Sum: 102.207 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.467 GB (Max: 3.878, Min: 3.196, Sum: 27.737)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 76.825 Gbps (Max: 90.647, Min: 59.238, Sum: 614.603)
Layer-level communication (cluster-wide, per-epoch): 0.883 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.883 GB
****** Accuracy Results ******
Highest valid_acc: 0.5387
Target test_acc: 0.5229
Epoch to reach the target acc: 3749
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
