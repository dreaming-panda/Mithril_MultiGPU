Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INITDONE MPI INIT
Initialized node 7 on machine gnerv8

Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.048 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
        It takes 0.048 seconds.
        It takes 0.051 seconds.
        It takes 0.050 seconds.
        It takes 0.052 seconds.
        It takes 0.055 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.049 seconds.
Building the Label Vector...
        It takes 0.051 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.027 seconds.
        It takes 0.025 seconds.
        It takes 0.026 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 61.248 Gbps (per GPU), 489.985 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.952 Gbps (per GPU), 487.617 Gbps (aggregated)
The layer-level communication performance: 60.947 Gbps (per GPU), 487.576 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.697 Gbps (per GPU), 485.578 Gbps (aggregated)
The layer-level communication performance: 60.664 Gbps (per GPU), 485.315 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.436 Gbps (per GPU), 483.485 Gbps (aggregated)
The layer-level communication performance: 60.384 Gbps (per GPU), 483.075 Gbps (aggregated)
The layer-level communication performance: 60.352 Gbps (per GPU), 482.814 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.972 Gbps (per GPU), 1271.772 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.956 Gbps (per GPU), 1271.652 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.957 Gbps (per GPU), 1271.652 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.956 Gbps (per GPU), 1271.652 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.968 Gbps (per GPU), 1271.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.968 Gbps (per GPU), 1271.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.959 Gbps (per GPU), 1271.676 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.953 Gbps (per GPU), 1271.627 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.854 Gbps (per GPU), 838.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.853 Gbps (per GPU), 838.826 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.855 Gbps (per GPU), 838.840 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.855 Gbps (per GPU), 838.840 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.845 Gbps (per GPU), 838.756 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.846 Gbps (per GPU), 838.770 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.824 Gbps (per GPU), 838.588 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.789 Gbps (per GPU), 838.309 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.435 Gbps (per GPU), 307.482 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.436 Gbps (per GPU), 307.490 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.434 Gbps (per GPU), 307.471 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.428 Gbps (per GPU), 307.424 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.434 Gbps (per GPU), 307.471 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.434 Gbps (per GPU), 307.475 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.434 Gbps (per GPU), 307.468 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.429 Gbps (per GPU), 307.433 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.31ms  0.33ms  0.23ms  1.45  5.55K  0.06M
 chk_1  0.30ms  0.34ms  0.23ms  1.44  5.75K  0.05M
 chk_2  0.31ms  0.34ms  0.24ms  1.42  5.20K  0.07M
 chk_3  0.31ms  0.44ms  0.24ms  1.83  5.57K  0.06M
 chk_4  0.31ms  0.34ms  0.24ms  1.43  5.05K  0.08M
 chk_5  0.34ms  0.36ms  0.27ms  1.36  4.73K  0.11M
 chk_6  0.31ms  0.34ms  0.24ms  1.41  4.98K  0.08M
 chk_7  0.31ms  0.35ms  0.24ms  1.42  5.01K  0.09M
 chk_8  0.30ms  0.33ms  0.23ms  1.45  5.73K  0.05M
 chk_9  0.31ms  0.34ms  0.24ms  1.40  4.54K  0.11M
chk_10  0.31ms  0.34ms  0.24ms  1.43  5.36K  0.07M
chk_11  0.32ms  0.35ms  0.24ms  1.41  5.39K  0.08M
chk_12  0.30ms  0.33ms  0.23ms  1.45  5.77K  0.05M
chk_13  0.31ms  0.34ms  0.24ms  1.42  5.43K  0.06M
chk_14  0.30ms  0.34ms  0.23ms  1.44  5.46K  0.06M
chk_15  0.30ms  0.33ms  0.23ms  1.46  5.88K  0.04M
chk_16  0.31ms  0.34ms  0.24ms  1.43  5.50K  0.06M
chk_17  0.33ms  0.36ms  0.26ms  1.38  4.86K  0.09M
chk_18  0.33ms  0.36ms  0.26ms  1.37  5.39K  0.07M
chk_19  0.31ms  0.34ms  0.24ms  1.42  5.20K  0.07M
chk_20  0.30ms  0.33ms  0.23ms  1.46  5.51K  0.06M
chk_21  0.29ms  0.33ms  0.22ms  1.48  5.81K  0.05M
chk_22  0.30ms  0.34ms  0.24ms  1.44  5.32K  0.07M
chk_23  0.32ms  0.35ms  0.25ms  1.40  5.39K  0.07M
chk_24  0.32ms  0.35ms  0.25ms  1.38  4.62K  0.11M
chk_25  0.32ms  0.34ms  0.24ms  1.41  5.04K  0.08M
chk_26  0.31ms  0.34ms  0.24ms  1.39  4.55K  0.11M
chk_27  0.30ms  0.33ms  0.23ms  1.44  5.30K  0.06M
chk_28  0.31ms  0.34ms  0.23ms  1.43  5.58K  0.06M
chk_29  0.32ms  0.35ms  0.25ms  1.38  4.98K  0.09M
chk_30  0.31ms  0.34ms  0.23ms  1.45  5.50K  0.07M
chk_31  0.31ms  0.34ms  0.24ms  1.44  5.38K  0.07M
   Avg  0.31  0.34  0.24
   Max  0.34  0.44  0.27
   Min  0.29  0.33  0.22
 Ratio  1.14  1.34  1.22
   Var  0.00  0.00  0.00
Profiling takes 0.452 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 4 initializing the weights for op[81, 101)...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
+++++++++ Node 7 initializing the weights for op[141, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.6889	TrainAcc 0.0048	ValidAcc 0.0025	TestAcc 0.0011	BestValid 0.0025
	Epoch 50:	Loss 3.6712	TrainAcc 0.0247	ValidAcc 0.0077	TestAcc 0.0071	BestValid 0.0077
	Epoch 100:	Loss 3.0380	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 150:	Loss 2.7347	TrainAcc 0.2212	ValidAcc 0.0983	TestAcc 0.0775	BestValid 0.0983
	Epoch 200:	Loss 2.4637	TrainAcc 0.2103	ValidAcc 0.0899	TestAcc 0.0700	BestValid 0.0983
	Epoch 250:	Loss 2.3241	TrainAcc 0.2242	ValidAcc 0.0978	TestAcc 0.0771	BestValid 0.0983
	Epoch 300:	Loss 2.2957	TrainAcc 0.2152	ValidAcc 0.0993	TestAcc 0.0777	BestValid 0.0993
	Epoch 350:	Loss 2.2374	TrainAcc 0.2367	ValidAcc 0.1053	TestAcc 0.0830	BestValid 0.1053
	Epoch 400:	Loss 2.1967	TrainAcc 0.2359	ValidAcc 0.1046	TestAcc 0.0828	BestValid 0.1053
	Epoch 450:	Loss 2.1875	TrainAcc 0.2388	ValidAcc 0.1061	TestAcc 0.0844	BestValid 0.1061
	Epoch 500:	Loss 2.1759	TrainAcc 0.2438	ValidAcc 0.1077	TestAcc 0.0858	BestValid 0.1077
	Epoch 550:	Loss 2.1690	TrainAcc 0.2395	ValidAcc 0.1433	TestAcc 0.1430	BestValid 0.1433
	Epoch 600:	Loss 2.1299	TrainAcc 0.2479	ValidAcc 0.1106	TestAcc 0.0881	BestValid 0.1433
	Epoch 650:	Loss 2.1008	TrainAcc 0.3421	ValidAcc 0.2553	TestAcc 0.2396	BestValid 0.2553
	Epoch 700:	Loss 2.0610	TrainAcc 0.4145	ValidAcc 0.3798	TestAcc 0.3445	BestValid 0.3798
	Epoch 750:	Loss 2.0084	TrainAcc 0.4311	ValidAcc 0.4679	TestAcc 0.4597	BestValid 0.4679
	Epoch 800:	Loss 1.9853	TrainAcc 0.3113	ValidAcc 0.1565	TestAcc 0.1229	BestValid 0.4679
	Epoch 850:	Loss 1.9663	TrainAcc 0.3210	ValidAcc 0.2276	TestAcc 0.2314	BestValid 0.4679
	Epoch 900:	Loss 1.9383	TrainAcc 0.4387	ValidAcc 0.4096	TestAcc 0.3867	BestValid 0.4679
	Epoch 950:	Loss 1.9192	TrainAcc 0.4437	ValidAcc 0.4159	TestAcc 0.3970	BestValid 0.4679
	Epoch 1000:	Loss 1.8865	TrainAcc 0.3994	ValidAcc 0.3191	TestAcc 0.3020	BestValid 0.4679
	Epoch 1050:	Loss 1.8858	TrainAcc 0.3330	ValidAcc 0.1585	TestAcc 0.1294	BestValid 0.4679
	Epoch 1100:	Loss 1.8559	TrainAcc 0.4527	ValidAcc 0.4704	TestAcc 0.4681	BestValid 0.4704
	Epoch 1150:	Loss 1.8369	TrainAcc 0.4031	ValidAcc 0.2859	TestAcc 0.2605	BestValid 0.4704
	Epoch 1200:	Loss 1.8326	TrainAcc 0.3627	ValidAcc 0.2788	TestAcc 0.2681	BestValid 0.4704
	Epoch 1250:	Loss 1.8166	TrainAcc 0.4627	ValidAcc 0.4277	TestAcc 0.4086	BestValid 0.4704
	Epoch 1300:	Loss 1.8079	TrainAcc 0.4325	ValidAcc 0.3577	TestAcc 0.3580	BestValid 0.4704
	Epoch 1350:	Loss 1.7953	TrainAcc 0.4031	ValidAcc 0.2892	TestAcc 0.2709	BestValid 0.4704
	Epoch 1400:	Loss 1.7795	TrainAcc 0.5258	ValidAcc 0.5514	TestAcc 0.5605	BestValid 0.5514
	Epoch 1450:	Loss 1.7851	TrainAcc 0.3465	ValidAcc 0.1697	TestAcc 0.1427	BestValid 0.5514
	Epoch 1500:	Loss 1.7676	TrainAcc 0.4444	ValidAcc 0.3791	TestAcc 0.3881	BestValid 0.5514
	Epoch 1550:	Loss 1.7675	TrainAcc 0.5315	ValidAcc 0.5369	TestAcc 0.5291	BestValid 0.5514
	Epoch 1600:	Loss 1.7521	TrainAcc 0.5292	ValidAcc 0.5510	TestAcc 0.5612	BestValid 0.5514
	Epoch 1650:	Loss 1.7467	TrainAcc 0.5229	ValidAcc 0.4853	TestAcc 0.4646	BestValid 0.5514
	Epoch 1700:	Loss 1.7327	TrainAcc 0.4805	ValidAcc 0.4842	TestAcc 0.4973	BestValid 0.5514
	Epoch 1750:	Loss 1.7417	TrainAcc 0.4166	ValidAcc 0.2795	TestAcc 0.2509	BestValid 0.5514
	Epoch 1800:	Loss 1.7242	TrainAcc 0.5224	ValidAcc 0.5341	TestAcc 0.5448	BestValid 0.5514
	Epoch 1850:	Loss 1.7212	TrainAcc 0.5040	ValidAcc 0.4747	TestAcc 0.4579	BestValid 0.5514
	Epoch 1900:	Loss 1.7161	TrainAcc 0.4189	ValidAcc 0.3505	TestAcc 0.3704	BestValid 0.5514
	Epoch 1950:	Loss 1.7103	TrainAcc 0.4408	ValidAcc 0.3247	TestAcc 0.3147	BestValid 0.5514
	Epoch 2000:	Loss 1.7033	TrainAcc 0.5183	ValidAcc 0.5183	TestAcc 0.5161	BestValid 0.5514
	Epoch 2050:	Loss 1.7007	TrainAcc 0.3383	ValidAcc 0.1540	TestAcc 0.1301	BestValid 0.5514
	Epoch 2100:	Loss 1.7057	TrainAcc 0.4969	ValidAcc 0.4582	TestAcc 0.4656	BestValid 0.5514
	Epoch 2150:	Loss 1.6883	TrainAcc 0.4121	ValidAcc 0.2686	TestAcc 0.2433	BestValid 0.5514
	Epoch 2200:	Loss 1.6936	TrainAcc 0.4705	ValidAcc 0.4226	TestAcc 0.3918	BestValid 0.5514
	Epoch 2250:	Loss 1.6897	TrainAcc 0.3567	ValidAcc 0.1902	TestAcc 0.1751	BestValid 0.5514
	Epoch 2300:	Loss 1.6767	TrainAcc 0.4472	ValidAcc 0.4732	TestAcc 0.4800	BestValid 0.5514
	Epoch 2350:	Loss 1.6828	TrainAcc 0.4282	ValidAcc 0.2937	TestAcc 0.2739	BestValid 0.5514
	Epoch 2400:	Loss 1.6781	TrainAcc 0.4562	ValidAcc 0.3653	TestAcc 0.3787	BestValid 0.5514
	Epoch 2450:	Loss 1.6766	TrainAcc 0.5114	ValidAcc 0.5131	TestAcc 0.5267	BestValid 0.5514
	Epoch 2500:	Loss 1.6687	TrainAcc 0.3817	ValidAcc 0.1948	TestAcc 0.1680	BestValid 0.5514
	Epoch 2550:	Loss 1.6716	TrainAcc 0.4538	ValidAcc 0.3078	TestAcc 0.2862	BestValid 0.5514
	Epoch 2600:	Loss 1.6616	TrainAcc 0.5579	ValidAcc 0.5617	TestAcc 0.5624	BestValid 0.5617
	Epoch 2650:	Loss 1.6725	TrainAcc 0.3670	ValidAcc 0.1888	TestAcc 0.1522	BestValid 0.5617
	Epoch 2700:	Loss 1.6647	TrainAcc 0.4387	ValidAcc 0.2911	TestAcc 0.2629	BestValid 0.5617
	Epoch 2750:	Loss 1.6580	TrainAcc 0.5059	ValidAcc 0.4551	TestAcc 0.4312	BestValid 0.5617
	Epoch 2800:	Loss 1.6584	TrainAcc 0.3830	ValidAcc 0.1973	TestAcc 0.1678	BestValid 0.5617
	Epoch 2850:	Loss 1.6459	TrainAcc 0.4665	ValidAcc 0.4511	TestAcc 0.4393	BestValid 0.5617
	Epoch 2900:	Loss 1.6532	TrainAcc 0.4345	ValidAcc 0.2905	TestAcc 0.2960	BestValid 0.5617
	Epoch 2950:	Loss 1.6461	TrainAcc 0.3872	ValidAcc 0.2019	TestAcc 0.1671	BestValid 0.5617
	Epoch 3000:	Loss 1.6433	TrainAcc 0.4162	ValidAcc 0.2188	TestAcc 0.1943	BestValid 0.5617
	Epoch 3050:	Loss 1.6510	TrainAcc 0.4042	ValidAcc 0.2074	TestAcc 0.1791	BestValid 0.5617
	Epoch 3100:	Loss 1.6398	TrainAcc 0.5380	ValidAcc 0.5619	TestAcc 0.5685	BestValid 0.5619
	Epoch 3150:	Loss 1.6398	TrainAcc 0.3048	ValidAcc 0.1513	TestAcc 0.1271	BestValid 0.5619
	Epoch 3200:	Loss 1.6528	TrainAcc 0.4859	ValidAcc 0.3998	TestAcc 0.3828	BestValid 0.5619
	Epoch 3250:	Loss 1.6585	TrainAcc 0.4704	ValidAcc 0.4481	TestAcc 0.4491	BestValid 0.5619
	Epoch 3300:	Loss 1.6457	TrainAcc 0.4965	ValidAcc 0.4291	TestAcc 0.3954	BestValid 0.5619
	Epoch 3350:	Loss 1.6412	TrainAcc 0.3268	ValidAcc 0.1627	TestAcc 0.1355	BestValid 0.5619
	Epoch 3400:	Loss 1.6406	TrainAcc 0.3533	ValidAcc 0.1795	TestAcc 0.1531	BestValid 0.5619
	Epoch 3450:	Loss 1.6360	TrainAcc 0.4360	ValidAcc 0.4093	TestAcc 0.3829	BestValid 0.5619
	Epoch 3500:	Loss 1.6401	TrainAcc 0.4866	ValidAcc 0.4064	TestAcc 0.4175	BestValid 0.5619
	Epoch 3550:	Loss 1.6362	TrainAcc 0.4329	ValidAcc 0.2851	TestAcc 0.2600	BestValid 0.5619
	Epoch 3600:	Loss 1.6377	TrainAcc 0.4949	ValidAcc 0.4650	TestAcc 0.4663	BestValid 0.5619
	Epoch 3650:	Loss 1.6310	TrainAcc 0.4199	ValidAcc 0.3461	TestAcc 0.3434	BestValid 0.5619
	Epoch 3700:	Loss 1.6266	TrainAcc 0.4421	ValidAcc 0.3130	TestAcc 0.2995	BestValid 0.5619
	Epoch 3750:	Loss 1.6298	TrainAcc 0.4874	ValidAcc 0.4377	TestAcc 0.4106	BestValid 0.5619
	Epoch 3800:	Loss 1.6399	TrainAcc 0.3651	ValidAcc 0.1846	TestAcc 0.1570	BestValid 0.5619
	Epoch 3850:	Loss 1.6366	TrainAcc 0.3856	ValidAcc 0.2056	TestAcc 0.1784	BestValid 0.5619
	Epoch 3900:	Loss 1.6287	TrainAcc 0.4837	ValidAcc 0.3927	TestAcc 0.3724	BestValid 0.5619
	Epoch 3950:	Loss 1.6410	TrainAcc 0.3676	ValidAcc 0.1824	TestAcc 0.1532	BestValid 0.5619
	Epoch 4000:	Loss 1.6244	TrainAcc 0.4456	ValidAcc 0.2926	TestAcc 0.2934	BestValid 0.5619
	Epoch 4050:	Loss 1.6265	TrainAcc 0.3197	ValidAcc 0.1536	TestAcc 0.1320	BestValid 0.5619
	Epoch 4100:	Loss 1.6141	TrainAcc 0.4649	ValidAcc 0.3495	TestAcc 0.3363	BestValid 0.5619
	Epoch 4150:	Loss 1.6168	TrainAcc 0.2920	ValidAcc 0.1434	TestAcc 0.1136	BestValid 0.5619
	Epoch 4200:	Loss 1.6236	TrainAcc 0.3541	ValidAcc 0.1641	TestAcc 0.1393	BestValid 0.5619
	Epoch 4250:	Loss 1.6141	TrainAcc 0.4869	ValidAcc 0.4450	TestAcc 0.4142	BestValid 0.5619
	Epoch 4300:	Loss 1.6189	TrainAcc 0.3773	ValidAcc 0.2542	TestAcc 0.2310	BestValid 0.5619
	Epoch 4350:	Loss 1.6187	TrainAcc 0.4323	ValidAcc 0.2866	TestAcc 0.2631	BestValid 0.5619
	Epoch 4400:	Loss 1.6178	TrainAcc 0.3784	ValidAcc 0.1963	TestAcc 0.1758	BestValid 0.5619
	Epoch 4450:	Loss 1.6204	TrainAcc 0.3692	ValidAcc 0.1879	TestAcc 0.1581	BestValid 0.5619
	Epoch 4500:	Loss 1.6074	TrainAcc 0.5001	ValidAcc 0.4668	TestAcc 0.4838	BestValid 0.5619
	Epoch 4550:	Loss 1.6189	TrainAcc 0.5561	ValidAcc 0.5420	TestAcc 0.5226	BestValid 0.5619
	Epoch 4600:	Loss 1.6272	TrainAcc 0.3371	ValidAcc 0.1698	TestAcc 0.1343	BestValid 0.5619
	Epoch 4650:	Loss 1.6205	TrainAcc 0.3470	ValidAcc 0.1746	TestAcc 0.1400	BestValid 0.5619
	Epoch 4700:	Loss 1.6196	TrainAcc 0.3313	ValidAcc 0.1629	TestAcc 0.1344	BestValid 0.5619
	Epoch 4750:	Loss 1.6225	TrainAcc 0.4384	ValidAcc 0.3347	TestAcc 0.3037	BestValid 0.5619
	Epoch 4800:	Loss 1.6134	TrainAcc 0.3760	ValidAcc 0.2469	TestAcc 0.2196	BestValid 0.5619
	Epoch 4850:	Loss 1.6155	TrainAcc 0.4319	ValidAcc 0.4184	TestAcc 0.4030	BestValid 0.5619
	Epoch 4900:	Loss 1.6420	TrainAcc 0.2492	ValidAcc 0.1309	TestAcc 0.1076	BestValid 0.5619
	Epoch 4950:	Loss 1.6593	TrainAcc 0.5321	ValidAcc 0.5404	TestAcc 0.5331	BestValid 0.5619
	Epoch 5000:	Loss 1.6598	TrainAcc 0.4364	ValidAcc 0.3976	TestAcc 0.3564	BestValid 0.5619
****** Epoch Time (Excluding Evaluation Cost): 0.102 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 26.331 ms (Max: 29.142, Min: 13.065, Sum: 210.651)
Cluster-Wide Average, Compute: 47.156 ms (Max: 57.478, Min: 44.999, Sum: 377.245)
Cluster-Wide Average, Communication-Layer: 12.137 ms (Max: 14.056, Min: 8.785, Sum: 97.098)
Cluster-Wide Average, Bubble-Imbalance: 12.839 ms (Max: 14.959, Min: 6.297, Sum: 102.710)
Cluster-Wide Average, Communication-Graph: 0.451 ms (Max: 0.507, Min: 0.394, Sum: 3.605)
Cluster-Wide Average, Optimization: 0.090 ms (Max: 0.093, Min: 0.089, Sum: 0.722)
Cluster-Wide Average, Others: 3.170 ms (Max: 15.998, Min: 1.333, Sum: 25.362)
****** Breakdown Sum: 102.174 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.467 GB (Max: 3.878, Min: 3.196, Sum: 27.737)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 77.004 Gbps (Max: 91.207, Min: 59.170, Sum: 616.031)
Layer-level communication (cluster-wide, per-epoch): 0.883 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.883 GB
****** Accuracy Results ******
Highest valid_acc: 0.5619
Target test_acc: 0.5685
Epoch to reach the target acc: 3099
[MPI Rank 0] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
