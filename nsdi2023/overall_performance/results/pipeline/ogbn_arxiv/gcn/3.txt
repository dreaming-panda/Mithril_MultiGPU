Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 2 on machine gnerv7

DONE MPI INIT
Initialized node 3 on machine gnerv7
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
DONE MPI INIT
Initialized node 7 on machine gnerv8
Initialized node 5 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.066 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
        It takes 0.050 seconds.
        It takes 0.053 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
        It takes 0.052 seconds.
        It takes 0.055 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.051 seconds.
Building the Label Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.023 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
        It takes 0.028 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 61.169 Gbps (per GPU), 489.355 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.844 Gbps (per GPU), 486.752 Gbps (aggregated)
The layer-level communication performance: 60.843 Gbps (per GPU), 486.746 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.565 Gbps (per GPU), 484.521 Gbps (aggregated)
The layer-level communication performance: 60.541 Gbps (per GPU), 484.328 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.287 Gbps (per GPU), 482.299 Gbps (aggregated)
The layer-level communication performance: 60.234 Gbps (per GPU), 481.872 Gbps (aggregated)
The layer-level communication performance: 60.207 Gbps (per GPU), 481.655 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.298 Gbps (per GPU), 1274.380 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.288 Gbps (per GPU), 1274.308 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.301 Gbps (per GPU), 1274.404 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.295 Gbps (per GPU), 1274.356 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.297 Gbps (per GPU), 1274.379 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.307 Gbps (per GPU), 1274.453 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.246 Gbps (per GPU), 1273.969 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.298 Gbps (per GPU), 1274.380 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.372 Gbps (per GPU), 794.978 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.382 Gbps (per GPU), 795.053 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.374 Gbps (per GPU), 794.991 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.363 Gbps (per GPU), 794.903 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.389 Gbps (per GPU), 795.110 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.382 Gbps (per GPU), 795.060 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.389 Gbps (per GPU), 795.110 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.364 Gbps (per GPU), 794.909 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.468 Gbps (per GPU), 291.747 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.468 Gbps (per GPU), 291.741 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.466 Gbps (per GPU), 291.727 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.468 Gbps (per GPU), 291.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.468 Gbps (per GPU), 291.741 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.464 Gbps (per GPU), 291.715 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.468 Gbps (per GPU), 291.741 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.459 Gbps (per GPU), 291.675 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.30ms  0.33ms  0.22ms  1.47  5.55K  0.06M
 chk_1  0.30ms  0.33ms  0.23ms  1.46  5.75K  0.05M
 chk_2  0.31ms  0.34ms  0.24ms  1.43  5.20K  0.07M
 chk_3  0.31ms  0.34ms  0.23ms  1.44  5.57K  0.06M
 chk_4  0.31ms  0.34ms  0.23ms  1.44  5.05K  0.08M
 chk_5  0.33ms  0.36ms  0.26ms  1.38  4.73K  0.11M
 chk_6  0.30ms  0.33ms  0.23ms  1.43  4.98K  0.08M
 chk_7  0.31ms  0.34ms  0.24ms  1.41  5.01K  0.09M
 chk_8  0.30ms  0.33ms  0.23ms  1.46  5.73K  0.05M
 chk_9  0.31ms  0.34ms  0.24ms  1.41  4.54K  0.11M
chk_10  0.31ms  0.34ms  0.23ms  1.44  5.36K  0.07M
chk_11  0.31ms  0.34ms  0.24ms  1.43  5.39K  0.08M
chk_12  0.30ms  0.33ms  0.23ms  1.46  5.77K  0.05M
chk_13  0.58ms  0.34ms  0.24ms  2.47  5.43K  0.06M
chk_14  0.30ms  0.33ms  0.23ms  1.45  5.46K  0.06M
chk_15  0.30ms  0.33ms  0.22ms  1.47  5.88K  0.04M
chk_16  0.31ms  0.34ms  0.23ms  1.43  5.50K  0.06M
chk_17  0.33ms  0.35ms  0.26ms  1.37  4.86K  0.09M
chk_18  0.33ms  0.36ms  0.26ms  1.38  5.39K  0.07M
chk_19  0.31ms  0.34ms  0.24ms  1.42  5.20K  0.07M
chk_20  0.30ms  0.33ms  0.22ms  1.47  5.51K  0.06M
chk_21  0.29ms  0.32ms  0.22ms  1.49  5.81K  0.05M
chk_22  0.30ms  0.33ms  0.23ms  1.45  5.32K  0.07M
chk_23  0.32ms  0.35ms  0.25ms  1.40  5.39K  0.07M
chk_24  0.32ms  0.34ms  0.25ms  1.38  4.62K  0.11M
chk_25  0.31ms  0.34ms  0.24ms  1.41  5.04K  0.08M
chk_26  0.31ms  0.34ms  0.24ms  1.39  4.55K  0.11M
chk_27  0.30ms  0.33ms  0.23ms  1.44  5.30K  0.06M
chk_28  0.30ms  0.33ms  0.23ms  1.44  5.58K  0.06M
chk_29  0.32ms  0.35ms  0.25ms  1.38  4.98K  0.09M
chk_30  0.30ms  0.33ms  0.23ms  1.45  5.50K  0.07M
chk_31  0.31ms  0.33ms  0.23ms  1.44  5.38K  0.07M
   Avg  0.32  0.34  0.24
   Max  0.58  0.36  0.26
   Min  0.29  0.32  0.22
 Ratio  1.99  1.13  1.22
   Var  0.00  0.00  0.00
Profiling takes 0.443 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 21)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 3, starting model training...
Num Stages: 8 / 8
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [21, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 61)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 101)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [61, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [101, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 7, starting model training...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [121, 141)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 169343
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [141, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 5 initializing the weights for op[101, 121)...
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 6 initializing the weights for op[121, 141)...
+++++++++ Node 3 initializing the weights for op[61, 81)...
+++++++++ Node 7 initializing the weights for op[141, 160)...
+++++++++ Node 4 initializing the weights for op[81, 101)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.6889	TrainAcc 0.0434	ValidAcc 0.0563	TestAcc 0.0521	BestValid 0.0563
	Epoch 50:	Loss 3.6889	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299	BestValid 0.0563
	Epoch 100:	Loss 3.6889	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299	BestValid 0.0563
	Epoch 150:	Loss 3.6889	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299	BestValid 0.0563
	Epoch 200:	Loss 3.6889	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299	BestValid 0.0563
	Epoch 250:	Loss 3.6889	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299	BestValid 0.0563
	Epoch 300:	Loss 3.6886	TrainAcc 0.0396	ValidAcc 0.0168	TestAcc 0.0151	BestValid 0.0563
	Epoch 350:	Loss 3.3183	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 400:	Loss 2.8988	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 450:	Loss 2.8574	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 500:	Loss 2.7035	TrainAcc 0.1799	ValidAcc 0.0764	TestAcc 0.0588	BestValid 0.0764
	Epoch 550:	Loss 2.5633	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0764
	Epoch 600:	Loss 2.4678	TrainAcc 0.1880	ValidAcc 0.0807	TestAcc 0.0623	BestValid 0.0807
	Epoch 650:	Loss 2.4048	TrainAcc 0.2261	ValidAcc 0.1050	TestAcc 0.0826	BestValid 0.1050
	Epoch 700:	Loss 2.3362	TrainAcc 0.3084	ValidAcc 0.3165	TestAcc 0.3358	BestValid 0.3165
	Epoch 750:	Loss 2.2975	TrainAcc 0.3300	ValidAcc 0.3337	TestAcc 0.3027	BestValid 0.3337
	Epoch 800:	Loss 2.2663	TrainAcc 0.3487	ValidAcc 0.3542	TestAcc 0.3337	BestValid 0.3542
	Epoch 850:	Loss 2.2485	TrainAcc 0.3501	ValidAcc 0.3518	TestAcc 0.3273	BestValid 0.3542
	Epoch 900:	Loss 2.2248	TrainAcc 0.3579	ValidAcc 0.3627	TestAcc 0.3462	BestValid 0.3627
	Epoch 950:	Loss 2.1926	TrainAcc 0.3023	ValidAcc 0.2469	TestAcc 0.2874	BestValid 0.3627
	Epoch 1000:	Loss 2.1479	TrainAcc 0.3990	ValidAcc 0.4014	TestAcc 0.4001	BestValid 0.4014
	Epoch 1050:	Loss 2.1112	TrainAcc 0.4103	ValidAcc 0.3937	TestAcc 0.3807	BestValid 0.4014
	Epoch 1100:	Loss 2.0937	TrainAcc 0.4139	ValidAcc 0.4258	TestAcc 0.4409	BestValid 0.4258
	Epoch 1150:	Loss 2.0718	TrainAcc 0.4049	ValidAcc 0.3843	TestAcc 0.3707	BestValid 0.4258
	Epoch 1200:	Loss 2.0448	TrainAcc 0.3555	ValidAcc 0.2564	TestAcc 0.2734	BestValid 0.4258
	Epoch 1250:	Loss 2.0200	TrainAcc 0.4162	ValidAcc 0.4143	TestAcc 0.4128	BestValid 0.4258
	Epoch 1300:	Loss 1.9804	TrainAcc 0.3093	ValidAcc 0.1612	TestAcc 0.1305	BestValid 0.4258
	Epoch 1350:	Loss 1.9394	TrainAcc 0.4195	ValidAcc 0.3720	TestAcc 0.3365	BestValid 0.4258
	Epoch 1400:	Loss 1.9225	TrainAcc 0.4209	ValidAcc 0.3851	TestAcc 0.3638	BestValid 0.4258
	Epoch 1450:	Loss 1.9260	TrainAcc 0.4700	ValidAcc 0.5028	TestAcc 0.5045	BestValid 0.5028
	Epoch 1500:	Loss 1.9019	TrainAcc 0.4146	ValidAcc 0.3831	TestAcc 0.3609	BestValid 0.5028
	Epoch 1550:	Loss 1.8890	TrainAcc 0.4288	ValidAcc 0.3530	TestAcc 0.3457	BestValid 0.5028
	Epoch 1600:	Loss 1.8672	TrainAcc 0.4665	ValidAcc 0.4859	TestAcc 0.4866	BestValid 0.5028
	Epoch 1650:	Loss 1.8550	TrainAcc 0.4773	ValidAcc 0.4685	TestAcc 0.4610	BestValid 0.5028
	Epoch 1700:	Loss 1.8251	TrainAcc 0.3773	ValidAcc 0.2451	TestAcc 0.2155	BestValid 0.5028
	Epoch 1750:	Loss 1.8230	TrainAcc 0.4750	ValidAcc 0.4356	TestAcc 0.4138	BestValid 0.5028
	Epoch 1800:	Loss 1.7996	TrainAcc 0.3505	ValidAcc 0.2250	TestAcc 0.2004	BestValid 0.5028
	Epoch 1850:	Loss 1.7972	TrainAcc 0.4641	ValidAcc 0.4008	TestAcc 0.3644	BestValid 0.5028
	Epoch 1900:	Loss 1.7898	TrainAcc 0.3128	ValidAcc 0.1410	TestAcc 0.1156	BestValid 0.5028
	Epoch 1950:	Loss 1.7763	TrainAcc 0.4742	ValidAcc 0.4762	TestAcc 0.4691	BestValid 0.5028
	Epoch 2000:	Loss 1.7727	TrainAcc 0.3771	ValidAcc 0.2957	TestAcc 0.2778	BestValid 0.5028
	Epoch 2050:	Loss 1.7701	TrainAcc 0.4997	ValidAcc 0.5307	TestAcc 0.5408	BestValid 0.5307
	Epoch 2100:	Loss 1.7595	TrainAcc 0.4225	ValidAcc 0.3145	TestAcc 0.2948	BestValid 0.5307
	Epoch 2150:	Loss 1.7545	TrainAcc 0.4381	ValidAcc 0.3750	TestAcc 0.3836	BestValid 0.5307
	Epoch 2200:	Loss 1.7559	TrainAcc 0.5050	ValidAcc 0.4633	TestAcc 0.4375	BestValid 0.5307
	Epoch 2250:	Loss 1.7460	TrainAcc 0.4926	ValidAcc 0.5179	TestAcc 0.5234	BestValid 0.5307
	Epoch 2300:	Loss 1.7386	TrainAcc 0.5017	ValidAcc 0.4524	TestAcc 0.4240	BestValid 0.5307
	Epoch 2350:	Loss 1.7440	TrainAcc 0.4935	ValidAcc 0.4452	TestAcc 0.4223	BestValid 0.5307
	Epoch 2400:	Loss 1.7315	TrainAcc 0.3653	ValidAcc 0.1776	TestAcc 0.1494	BestValid 0.5307
	Epoch 2450:	Loss 1.7268	TrainAcc 0.5324	ValidAcc 0.5300	TestAcc 0.5192	BestValid 0.5307
	Epoch 2500:	Loss 1.7185	TrainAcc 0.3680	ValidAcc 0.1814	TestAcc 0.1504	BestValid 0.5307
	Epoch 2550:	Loss 1.7129	TrainAcc 0.3660	ValidAcc 0.1721	TestAcc 0.1406	BestValid 0.5307
	Epoch 2600:	Loss 1.7135	TrainAcc 0.3339	ValidAcc 0.1518	TestAcc 0.1275	BestValid 0.5307
	Epoch 2650:	Loss 1.7131	TrainAcc 0.3628	ValidAcc 0.1861	TestAcc 0.1489	BestValid 0.5307
	Epoch 2700:	Loss 1.7124	TrainAcc 0.3548	ValidAcc 0.1783	TestAcc 0.1454	BestValid 0.5307
	Epoch 2750:	Loss 1.6998	TrainAcc 0.5108	ValidAcc 0.4699	TestAcc 0.4565	BestValid 0.5307
	Epoch 2800:	Loss 1.6982	TrainAcc 0.4435	ValidAcc 0.3900	TestAcc 0.3698	BestValid 0.5307
	Epoch 2850:	Loss 1.6926	TrainAcc 0.5317	ValidAcc 0.5214	TestAcc 0.4990	BestValid 0.5307
	Epoch 2900:	Loss 1.6919	TrainAcc 0.3584	ValidAcc 0.1857	TestAcc 0.1556	BestValid 0.5307
	Epoch 2950:	Loss 1.6915	TrainAcc 0.5033	ValidAcc 0.4372	TestAcc 0.3946	BestValid 0.5307
	Epoch 3000:	Loss 1.6868	TrainAcc 0.5000	ValidAcc 0.4413	TestAcc 0.4531	BestValid 0.5307
	Epoch 3050:	Loss 1.6842	TrainAcc 0.5349	ValidAcc 0.5102	TestAcc 0.4802	BestValid 0.5307
	Epoch 3100:	Loss 1.6755	TrainAcc 0.3630	ValidAcc 0.1834	TestAcc 0.1447	BestValid 0.5307
	Epoch 3150:	Loss 1.6736	TrainAcc 0.4510	ValidAcc 0.3911	TestAcc 0.3879	BestValid 0.5307
	Epoch 3200:	Loss 1.6807	TrainAcc 0.2952	ValidAcc 0.1319	TestAcc 0.1115	BestValid 0.5307
	Epoch 3250:	Loss 1.6743	TrainAcc 0.5388	ValidAcc 0.5194	TestAcc 0.4925	BestValid 0.5307
	Epoch 3300:	Loss 1.6786	TrainAcc 0.5262	ValidAcc 0.4542	TestAcc 0.4169	BestValid 0.5307
	Epoch 3350:	Loss 1.6649	TrainAcc 0.3481	ValidAcc 0.1567	TestAcc 0.1319	BestValid 0.5307
	Epoch 3400:	Loss 1.6664	TrainAcc 0.5032	ValidAcc 0.4368	TestAcc 0.4026	BestValid 0.5307
	Epoch 3450:	Loss 1.6598	TrainAcc 0.5285	ValidAcc 0.4872	TestAcc 0.4623	BestValid 0.5307
	Epoch 3500:	Loss 1.6676	TrainAcc 0.5379	ValidAcc 0.5237	TestAcc 0.4922	BestValid 0.5307
	Epoch 3550:	Loss 1.6635	TrainAcc 0.3729	ValidAcc 0.1785	TestAcc 0.1507	BestValid 0.5307
	Epoch 3600:	Loss 1.6632	TrainAcc 0.5077	ValidAcc 0.4282	TestAcc 0.3868	BestValid 0.5307
	Epoch 3650:	Loss 1.6579	TrainAcc 0.5245	ValidAcc 0.4760	TestAcc 0.4471	BestValid 0.5307
	Epoch 3700:	Loss 1.6583	TrainAcc 0.5067	ValidAcc 0.4799	TestAcc 0.4422	BestValid 0.5307
	Epoch 3750:	Loss 1.6507	TrainAcc 0.3349	ValidAcc 0.1464	TestAcc 0.1221	BestValid 0.5307
	Epoch 3800:	Loss 1.6537	TrainAcc 0.4694	ValidAcc 0.3617	TestAcc 0.3724	BestValid 0.5307
	Epoch 3850:	Loss 1.6505	TrainAcc 0.4916	ValidAcc 0.3843	TestAcc 0.3476	BestValid 0.5307
	Epoch 3900:	Loss 1.6488	TrainAcc 0.4814	ValidAcc 0.4058	TestAcc 0.3611	BestValid 0.5307
	Epoch 3950:	Loss 1.6429	TrainAcc 0.3481	ValidAcc 0.1729	TestAcc 0.1422	BestValid 0.5307
	Epoch 4000:	Loss 1.6386	TrainAcc 0.4852	ValidAcc 0.4192	TestAcc 0.3767	BestValid 0.5307
	Epoch 4050:	Loss 1.6409	TrainAcc 0.3240	ValidAcc 0.1451	TestAcc 0.1221	BestValid 0.5307
	Epoch 4100:	Loss 1.6391	TrainAcc 0.4158	ValidAcc 0.2709	TestAcc 0.2427	BestValid 0.5307
	Epoch 4150:	Loss 1.6323	TrainAcc 0.3794	ValidAcc 0.1980	TestAcc 0.1615	BestValid 0.5307
	Epoch 4200:	Loss 1.6371	TrainAcc 0.5086	ValidAcc 0.4344	TestAcc 0.3936	BestValid 0.5307
	Epoch 4250:	Loss 1.6367	TrainAcc 0.3433	ValidAcc 0.1534	TestAcc 0.1252	BestValid 0.5307
	Epoch 4300:	Loss 1.6279	TrainAcc 0.4940	ValidAcc 0.3904	TestAcc 0.4131	BestValid 0.5307
	Epoch 4350:	Loss 1.6311	TrainAcc 0.3656	ValidAcc 0.1754	TestAcc 0.1445	BestValid 0.5307
	Epoch 4400:	Loss 1.6220	TrainAcc 0.5270	ValidAcc 0.4744	TestAcc 0.4492	BestValid 0.5307
	Epoch 4450:	Loss 1.6196	TrainAcc 0.5201	ValidAcc 0.4554	TestAcc 0.4230	BestValid 0.5307
	Epoch 4500:	Loss 1.6151	TrainAcc 0.2975	ValidAcc 0.1361	TestAcc 0.1173	BestValid 0.5307
	Epoch 4550:	Loss 1.6197	TrainAcc 0.4973	ValidAcc 0.4195	TestAcc 0.3809	BestValid 0.5307
	Epoch 4600:	Loss 1.6149	TrainAcc 0.3064	ValidAcc 0.1291	TestAcc 0.1067	BestValid 0.5307
	Epoch 4650:	Loss 1.6169	TrainAcc 0.3933	ValidAcc 0.2075	TestAcc 0.1761	BestValid 0.5307
	Epoch 4700:	Loss 1.6169	TrainAcc 0.3534	ValidAcc 0.1635	TestAcc 0.1361	BestValid 0.5307
	Epoch 4750:	Loss 1.6102	TrainAcc 0.4746	ValidAcc 0.3601	TestAcc 0.3578	BestValid 0.5307
	Epoch 4800:	Loss 1.6114	TrainAcc 0.5230	ValidAcc 0.4647	TestAcc 0.4254	BestValid 0.5307
	Epoch 4850:	Loss 1.6063	TrainAcc 0.5016	ValidAcc 0.4649	TestAcc 0.4474	BestValid 0.5307
	Epoch 4900:	Loss 1.6144	TrainAcc 0.4997	ValidAcc 0.4454	TestAcc 0.4025	BestValid 0.5307
	Epoch 4950:	Loss 1.6107	TrainAcc 0.3722	ValidAcc 0.1862	TestAcc 0.1558	BestValid 0.5307
	Epoch 5000:	Loss 1.6028	TrainAcc 0.2525	ValidAcc 0.1083	TestAcc 0.0920	BestValid 0.5307
****** Epoch Time (Excluding Evaluation Cost): 0.102 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 26.215 ms (Max: 29.193, Min: 12.888, Sum: 209.719)
Cluster-Wide Average, Compute: 46.909 ms (Max: 57.593, Min: 44.907, Sum: 375.268)
Cluster-Wide Average, Communication-Layer: 12.121 ms (Max: 14.035, Min: 8.708, Sum: 96.968)
Cluster-Wide Average, Bubble-Imbalance: 13.382 ms (Max: 16.254, Min: 6.561, Sum: 107.059)
Cluster-Wide Average, Communication-Graph: 0.449 ms (Max: 0.506, Min: 0.391, Sum: 3.590)
Cluster-Wide Average, Optimization: 0.090 ms (Max: 0.091, Min: 0.088, Sum: 0.717)
Cluster-Wide Average, Others: 3.154 ms (Max: 15.972, Min: 1.318, Sum: 25.234)
****** Breakdown Sum: 102.319 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.467 GB (Max: 3.878, Min: 3.196, Sum: 27.737)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 77.130 Gbps (Max: 91.388, Min: 59.601, Sum: 617.041)
Layer-level communication (cluster-wide, per-epoch): 0.883 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.883 GB
****** Accuracy Results ******
Highest valid_acc: 0.5307
Target test_acc: 0.5408
Epoch to reach the target acc: 2049
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
