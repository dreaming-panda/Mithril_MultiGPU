Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7

Initialized node 1 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.053 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
        It takes 0.056 seconds.
        It takes 0.057 seconds.
        It takes 0.054 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
        It takes 0.057 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.069 seconds.
Building the Label Vector...
        It takes 0.070 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.071 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.787 Gbps (per GPU), 486.297 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.495 Gbps (per GPU), 483.959 Gbps (aggregated)
The layer-level communication performance: 60.488 Gbps (per GPU), 483.904 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.229 Gbps (per GPU), 481.832 Gbps (aggregated)
The layer-level communication performance: 60.197 Gbps (per GPU), 481.576 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.988 Gbps (per GPU), 479.907 Gbps (aggregated)
The layer-level communication performance: 59.939 Gbps (per GPU), 479.511 Gbps (aggregated)
The layer-level communication performance: 59.903 Gbps (per GPU), 479.225 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.183 Gbps (per GPU), 1273.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.156 Gbps (per GPU), 1273.245 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.177 Gbps (per GPU), 1273.413 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.153 Gbps (per GPU), 1273.221 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.186 Gbps (per GPU), 1273.485 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.153 Gbps (per GPU), 1273.220 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.176 Gbps (per GPU), 1273.411 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.168 Gbps (per GPU), 1273.344 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.599 Gbps (per GPU), 836.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.597 Gbps (per GPU), 836.776 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.599 Gbps (per GPU), 836.795 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.597 Gbps (per GPU), 836.776 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.600 Gbps (per GPU), 836.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.597 Gbps (per GPU), 836.776 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.600 Gbps (per GPU), 836.797 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.594 Gbps (per GPU), 836.755 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.102 Gbps (per GPU), 296.820 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.104 Gbps (per GPU), 296.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.103 Gbps (per GPU), 296.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.103 Gbps (per GPU), 296.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.104 Gbps (per GPU), 296.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.095 Gbps (per GPU), 296.761 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.099 Gbps (per GPU), 296.796 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.097 Gbps (per GPU), 296.773 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.33ms  0.46ms  0.61ms  1.85  5.55K  0.06M
 chk_1  0.33ms  0.47ms  0.62ms  1.88  5.75K  0.05M
 chk_2  0.32ms  0.47ms  0.62ms  1.92  5.20K  0.07M
 chk_3  0.33ms  0.47ms  0.62ms  1.90  5.57K  0.06M
 chk_4  0.32ms  0.46ms  0.61ms  1.91  5.05K  0.08M
 chk_5  0.31ms  0.48ms  0.63ms  2.03  4.73K  0.11M
 chk_6  0.39ms  0.46ms  0.61ms  1.57  4.98K  0.08M
 chk_7  0.32ms  0.46ms  0.61ms  1.93  5.01K  0.09M
 chk_8  0.35ms  0.46ms  0.62ms  1.76  5.73K  0.05M
 chk_9  0.31ms  0.46ms  0.60ms  1.96  4.54K  0.11M
chk_10  0.32ms  0.47ms  0.62ms  1.92  5.36K  0.07M
chk_11  0.32ms  0.47ms  0.62ms  1.92  5.39K  0.08M
chk_12  0.33ms  0.46ms  0.62ms  1.88  5.77K  0.05M
chk_13  0.32ms  0.46ms  0.61ms  1.91  5.43K  0.06M
chk_14  0.32ms  0.46ms  0.61ms  1.88  5.46K  0.06M
chk_15  0.33ms  0.46ms  0.62ms  1.88  5.88K  0.04M
chk_16  0.32ms  0.46ms  0.61ms  1.90  5.50K  0.06M
chk_17  0.31ms  0.48ms  0.63ms  2.00  4.86K  0.09M
chk_18  0.32ms  0.49ms  0.64ms  1.98  5.39K  0.07M
chk_19  0.32ms  0.47ms  0.62ms  1.93  5.20K  0.07M
chk_20  0.32ms  0.46ms  0.61ms  1.88  5.51K  0.06M
chk_21  0.33ms  0.45ms  0.61ms  1.86  5.81K  0.05M
chk_22  0.32ms  0.46ms  0.61ms  1.90  5.32K  0.07M
chk_23  0.33ms  0.48ms  0.63ms  1.94  5.39K  0.07M
chk_24  0.31ms  0.46ms  0.61ms  1.99  4.62K  0.11M
chk_25  0.32ms  0.47ms  0.61ms  1.94  5.04K  0.08M
chk_26  0.31ms  0.46ms  0.61ms  1.98  4.55K  0.11M
chk_27  0.32ms  0.45ms  0.60ms  1.88  5.30K  0.06M
chk_28  0.33ms  0.46ms  0.61ms  1.87  5.58K  0.06M
chk_29  0.32ms  0.47ms  0.62ms  1.97  4.98K  0.09M
chk_30  0.43ms  0.46ms  0.62ms  1.42  5.50K  0.07M
chk_31  0.32ms  0.46ms  0.62ms  1.90  5.38K  0.07M
   Avg  0.33  0.46  0.62
   Max  0.43  0.49  0.64
   Min  0.31  0.45  0.60
 Ratio  1.42  1.08  1.06
   Var  0.00  0.00  0.00
Profiling takes 0.674 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 0, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...


*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000

The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 6.4987	TrainAcc 0.0209	ValidAcc 0.0186	TestAcc 0.0168	BestValid 0.0186
	Epoch 50:	Loss 3.7007	TrainAcc 0.0583	ValidAcc 0.0402	TestAcc 0.0379	BestValid 0.0402
	Epoch 100:	Loss 3.7019	TrainAcc 0.0601	ValidAcc 0.0503	TestAcc 0.0485	BestValid 0.0503
	Epoch 150:	Loss 3.6949	TrainAcc 0.0580	ValidAcc 0.0494	TestAcc 0.0512	BestValid 0.0503
	Epoch 200:	Loss 3.6910	TrainAcc 0.0501	ValidAcc 0.0373	TestAcc 0.0356	BestValid 0.0503
	Epoch 250:	Loss 3.6952	TrainAcc 0.0497	ValidAcc 0.0383	TestAcc 0.0354	BestValid 0.0503
	Epoch 300:	Loss 3.6931	TrainAcc 0.0497	ValidAcc 0.0406	TestAcc 0.0363	BestValid 0.0503
	Epoch 350:	Loss 3.6911	TrainAcc 0.0497	ValidAcc 0.0408	TestAcc 0.0370	BestValid 0.0503
	Epoch 400:	Loss 3.6903	TrainAcc 0.0518	ValidAcc 0.0454	TestAcc 0.0403	BestValid 0.0503
	Epoch 450:	Loss 3.6915	TrainAcc 0.0527	ValidAcc 0.0470	TestAcc 0.0418	BestValid 0.0503
	Epoch 500:	Loss 3.6895	TrainAcc 0.0563	ValidAcc 0.0497	TestAcc 0.0432	BestValid 0.0503
	Epoch 550:	Loss 3.6906	TrainAcc 0.0610	ValidAcc 0.0571	TestAcc 0.0499	BestValid 0.0571
	Epoch 600:	Loss 3.6909	TrainAcc 0.0638	ValidAcc 0.0613	TestAcc 0.0550	BestValid 0.0613
	Epoch 650:	Loss 3.6897	TrainAcc 0.0640	ValidAcc 0.0591	TestAcc 0.0533	BestValid 0.0613
	Epoch 700:	Loss 3.6920	TrainAcc 0.0665	ValidAcc 0.0619	TestAcc 0.0556	BestValid 0.0619
	Epoch 750:	Loss 3.6892	TrainAcc 0.0717	ValidAcc 0.0701	TestAcc 0.0637	BestValid 0.0701
	Epoch 800:	Loss 3.6899	TrainAcc 0.0729	ValidAcc 0.0741	TestAcc 0.0685	BestValid 0.0741
	Epoch 850:	Loss 3.6897	TrainAcc 0.0747	ValidAcc 0.0813	TestAcc 0.0757	BestValid 0.0813
	Epoch 900:	Loss 3.6884	TrainAcc 0.0767	ValidAcc 0.0903	TestAcc 0.0853	BestValid 0.0903
	Epoch 950:	Loss 3.6889	TrainAcc 0.0803	ValidAcc 0.1026	TestAcc 0.0973	BestValid 0.1026
	Epoch 1000:	Loss 3.6903	TrainAcc 0.0828	ValidAcc 0.1084	TestAcc 0.1039	BestValid 0.1084
	Epoch 1050:	Loss 3.6890	TrainAcc 0.0992	ValidAcc 0.1564	TestAcc 0.1527	BestValid 0.1564
	Epoch 1100:	Loss 3.6900	TrainAcc 0.1016	ValidAcc 0.1611	TestAcc 0.1578	BestValid 0.1611
	Epoch 1150:	Loss 3.6898	TrainAcc 0.1005	ValidAcc 0.1610	TestAcc 0.1572	BestValid 0.1611
	Epoch 1200:	Loss 3.6891	TrainAcc 0.0998	ValidAcc 0.1602	TestAcc 0.1575	BestValid 0.1611
	Epoch 1250:	Loss 3.6892	TrainAcc 0.1020	ValidAcc 0.1691	TestAcc 0.1691	BestValid 0.1691
	Epoch 1300:	Loss 3.6920	TrainAcc 0.1010	ValidAcc 0.1657	TestAcc 0.1675	BestValid 0.1691
	Epoch 1350:	Loss 3.6886	TrainAcc 0.1010	ValidAcc 0.1685	TestAcc 0.1714	BestValid 0.1691
	Epoch 1400:	Loss 3.6888	TrainAcc 0.1042	ValidAcc 0.1791	TestAcc 0.1826	BestValid 0.1791
	Epoch 1450:	Loss 3.6893	TrainAcc 0.1024	ValidAcc 0.1796	TestAcc 0.1836	BestValid 0.1796
	Epoch 1500:	Loss 3.6891	TrainAcc 0.1033	ValidAcc 0.1813	TestAcc 0.1859	BestValid 0.1813
	Epoch 1550:	Loss 3.6892	TrainAcc 0.1061	ValidAcc 0.1890	TestAcc 0.1933	BestValid 0.1890
	Epoch 1600:	Loss 3.6903	TrainAcc 0.1063	ValidAcc 0.1885	TestAcc 0.1909	BestValid 0.1890
	Epoch 1650:	Loss 3.6904	TrainAcc 0.1079	ValidAcc 0.1921	TestAcc 0.1938	BestValid 0.1921
	Epoch 1700:	Loss 3.6894	TrainAcc 0.1060	ValidAcc 0.1884	TestAcc 0.1891	BestValid 0.1921
	Epoch 1750:	Loss 3.6892	TrainAcc 0.1056	ValidAcc 0.1868	TestAcc 0.1876	BestValid 0.1921
	Epoch 1800:	Loss 3.6897	TrainAcc 0.1074	ValidAcc 0.1898	TestAcc 0.1900	BestValid 0.1921
	Epoch 1850:	Loss 3.6898	TrainAcc 0.1075	ValidAcc 0.1881	TestAcc 0.1902	BestValid 0.1921
	Epoch 1900:	Loss 3.6900	TrainAcc 0.1049	ValidAcc 0.1827	TestAcc 0.1845	BestValid 0.1921
	Epoch 1950:	Loss 3.6886	TrainAcc 0.0983	ValidAcc 0.1716	TestAcc 0.1729	BestValid 0.1921
	Epoch 2000:	Loss 3.6893	TrainAcc 0.0998	ValidAcc 0.1778	TestAcc 0.1781	BestValid 0.1921
	Epoch 2050:	Loss 3.6891	TrainAcc 0.0973	ValidAcc 0.1707	TestAcc 0.1731	BestValid 0.1921
	Epoch 2100:	Loss 3.6895	TrainAcc 0.0966	ValidAcc 0.1703	TestAcc 0.1728	BestValid 0.1921
	Epoch 2150:	Loss 3.6888	TrainAcc 0.0943	ValidAcc 0.1639	TestAcc 0.1659	BestValid 0.1921
	Epoch 2200:	Loss 3.6902	TrainAcc 0.0914	ValidAcc 0.1586	TestAcc 0.1608	BestValid 0.1921
	Epoch 2250:	Loss 3.6887	TrainAcc 0.0892	ValidAcc 0.1510	TestAcc 0.1550	BestValid 0.1921
	Epoch 2300:	Loss 3.6893	TrainAcc 0.0856	ValidAcc 0.1418	TestAcc 0.1456	BestValid 0.1921
	Epoch 2350:	Loss 3.6899	TrainAcc 0.0863	ValidAcc 0.1427	TestAcc 0.1451	BestValid 0.1921
	Epoch 2400:	Loss 3.6886	TrainAcc 0.0859	ValidAcc 0.1405	TestAcc 0.1433	BestValid 0.1921
	Epoch 2450:	Loss 3.6894	TrainAcc 0.0820	ValidAcc 0.1309	TestAcc 0.1333	BestValid 0.1921
	Epoch 2500:	Loss 3.6900	TrainAcc 0.0779	ValidAcc 0.1276	TestAcc 0.1311	BestValid 0.1921
	Epoch 2550:	Loss 3.6884	TrainAcc 0.0582	ValidAcc 0.0616	TestAcc 0.0629	BestValid 0.1921
	Epoch 2600:	Loss 3.6887	TrainAcc 0.0451	ValidAcc 0.0277	TestAcc 0.0259	BestValid 0.1921
	Epoch 2650:	Loss 3.6884	TrainAcc 0.0438	ValidAcc 0.0233	TestAcc 0.0210	BestValid 0.1921
	Epoch 2700:	Loss 3.6902	TrainAcc 0.0423	ValidAcc 0.0195	TestAcc 0.0171	BestValid 0.1921
	Epoch 2750:	Loss 3.6867	TrainAcc 0.0397	ValidAcc 0.0178	TestAcc 0.0157	BestValid 0.1921
	Epoch 2800:	Loss 3.6862	TrainAcc 0.0687	ValidAcc 0.0360	TestAcc 0.0322	BestValid 0.1921
	Epoch 2850:	Loss 3.6748	TrainAcc 0.1787	ValidAcc 0.0762	TestAcc 0.0586	BestValid 0.1921
	Epoch 2900:	Loss 3.6137	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 2950:	Loss 3.4959	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3000:	Loss 3.3366	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3050:	Loss 3.1704	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3100:	Loss 3.0248	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3150:	Loss 2.9483	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3200:	Loss 2.9494	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3250:	Loss 2.9383	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3300:	Loss 2.9514	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3350:	Loss 2.9501	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3400:	Loss 2.9313	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3450:	Loss 2.9203	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3500:	Loss 2.9203	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3550:	Loss 2.9123	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3600:	Loss 2.9279	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3650:	Loss 2.9134	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3700:	Loss 2.9012	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3750:	Loss 2.9010	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3800:	Loss 2.9110	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3850:	Loss 2.8962	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3900:	Loss 2.8932	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 3950:	Loss 2.8952	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4000:	Loss 2.8814	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4050:	Loss 2.8797	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4100:	Loss 2.8811	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4150:	Loss 2.8804	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4200:	Loss 2.8795	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4250:	Loss 2.8714	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4300:	Loss 2.8691	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4350:	Loss 2.8846	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4400:	Loss 2.8776	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4450:	Loss 2.8764	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4500:	Loss 2.8803	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4550:	Loss 2.8902	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4600:	Loss 2.8878	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4650:	Loss 2.8668	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4700:	Loss 2.8765	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4750:	Loss 2.8704	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4800:	Loss 2.8955	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4850:	Loss 2.8692	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4900:	Loss 2.8618	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 4950:	Loss 2.8628	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
	Epoch 5000:	Loss 2.8548	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1921
****** Epoch Time (Excluding Evaluation Cost): 0.142 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 33.014 ms (Max: 35.645, Min: 20.145, Sum: 264.112)
Cluster-Wide Average, Compute: 66.123 ms (Max: 80.875, Min: 61.685, Sum: 528.987)
Cluster-Wide Average, Communication-Layer: 24.462 ms (Max: 29.380, Min: 17.016, Sum: 195.699)
Cluster-Wide Average, Bubble-Imbalance: 14.865 ms (Max: 20.935, Min: 7.707, Sum: 118.917)
Cluster-Wide Average, Communication-Graph: 0.434 ms (Max: 0.476, Min: 0.386, Sum: 3.473)
Cluster-Wide Average, Optimization: 0.095 ms (Max: 0.113, Min: 0.088, Sum: 0.761)
Cluster-Wide Average, Others: 3.168 ms (Max: 15.878, Min: 1.340, Sum: 25.345)
****** Breakdown Sum: 142.162 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.820 GB (Max: 5.016, Min: 3.622, Sum: 30.563)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 76.750 Gbps (Max: 92.535, Min: 61.581, Sum: 613.999)
Layer-level communication (cluster-wide, per-epoch): 1.766 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 1.766 GB
****** Accuracy Results ******
Highest valid_acc: 0.1921
Target test_acc: 0.1938
Epoch to reach the target acc: 1649
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
