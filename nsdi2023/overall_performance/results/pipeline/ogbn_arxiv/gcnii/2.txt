Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
DONE MPI INIT
Initialized node 3 on machine gnerv7
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 4 on machine gnerv8
Initialized node 7 on machine gnerv8

Initialized node 5 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
        It takes 0.050 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.049 seconds.
        It takes 0.054 seconds.
        It takes 0.051 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.058 seconds.
        It takes 0.057 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.865 Gbps (per GPU), 486.922 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.530 Gbps (per GPU), 484.236 Gbps (aggregated)
The layer-level communication performance: 60.523 Gbps (per GPU), 484.186 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.272 Gbps (per GPU), 482.174 Gbps (aggregated)
The layer-level communication performance: 60.247 Gbps (per GPU), 481.977 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.030 Gbps (per GPU), 480.243 Gbps (aggregated)
The layer-level communication performance: 59.972 Gbps (per GPU), 479.778 Gbps (aggregated)
The layer-level communication performance: 59.942 Gbps (per GPU), 479.539 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.590 Gbps (per GPU), 1268.719 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.596 Gbps (per GPU), 1268.766 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.590 Gbps (per GPU), 1268.719 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.572 Gbps (per GPU), 1268.575 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.590 Gbps (per GPU), 1268.719 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.572 Gbps (per GPU), 1268.575 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.584 Gbps (per GPU), 1268.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.590 Gbps (per GPU), 1268.719 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.378 Gbps (per GPU), 835.026 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.371 Gbps (per GPU), 834.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.378 Gbps (per GPU), 835.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.375 Gbps (per GPU), 834.999 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.378 Gbps (per GPU), 835.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.376 Gbps (per GPU), 835.006 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.358 Gbps (per GPU), 834.860 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.365 Gbps (per GPU), 834.922 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.785 Gbps (per GPU), 294.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.783 Gbps (per GPU), 294.267 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.783 Gbps (per GPU), 294.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.780 Gbps (per GPU), 294.241 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.782 Gbps (per GPU), 294.260 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.780 Gbps (per GPU), 294.241 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.778 Gbps (per GPU), 294.226 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.775 Gbps (per GPU), 294.204 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.33ms  0.46ms  0.61ms  1.87  5.55K  0.06M
 chk_1  0.33ms  0.47ms  0.62ms  1.89  5.75K  0.05M
 chk_2  0.32ms  0.47ms  0.62ms  1.93  5.20K  0.07M
 chk_3  0.33ms  0.47ms  0.62ms  1.90  5.57K  0.06M
 chk_4  0.32ms  0.46ms  0.61ms  1.90  5.05K  0.08M
 chk_5  0.31ms  0.48ms  0.63ms  2.02  4.73K  0.11M
 chk_6  0.32ms  0.46ms  0.61ms  1.91  4.98K  0.08M
 chk_7  0.32ms  0.47ms  0.61ms  1.93  5.01K  0.09M
 chk_8  0.33ms  0.46ms  0.62ms  1.88  5.73K  0.05M
 chk_9  0.31ms  0.46ms  0.61ms  1.98  4.54K  0.11M
chk_10  0.32ms  0.47ms  0.62ms  1.92  5.36K  0.07M
chk_11  0.35ms  0.47ms  0.63ms  1.80  5.39K  0.08M
chk_12  0.33ms  0.46ms  0.62ms  1.88  5.77K  0.05M
chk_13  0.32ms  0.46ms  0.62ms  1.90  5.43K  0.06M
chk_14  0.32ms  0.46ms  0.61ms  1.88  5.46K  0.06M
chk_15  0.33ms  0.46ms  0.62ms  1.87  5.88K  0.04M
chk_16  0.33ms  0.46ms  0.62ms  1.90  5.50K  0.06M
chk_17  0.32ms  0.48ms  0.63ms  2.00  4.86K  0.09M
chk_18  0.33ms  0.49ms  0.64ms  1.97  5.39K  0.07M
chk_19  0.32ms  0.47ms  0.62ms  1.93  5.20K  0.07M
chk_20  0.33ms  0.46ms  0.61ms  1.87  5.51K  0.06M
chk_21  0.33ms  0.46ms  0.61ms  1.85  5.81K  0.05M
chk_22  0.33ms  0.46ms  0.62ms  1.90  5.32K  0.07M
chk_23  0.33ms  0.48ms  0.64ms  1.96  5.39K  0.07M
chk_24  0.31ms  0.46ms  0.61ms  1.98  4.62K  0.11M
chk_25  0.32ms  0.47ms  0.62ms  1.93  5.04K  0.08M
chk_26  0.31ms  0.46ms  0.61ms  1.96  4.55K  0.11M
chk_27  0.32ms  0.45ms  0.61ms  1.88  5.30K  0.06M
chk_28  0.33ms  0.46ms  0.61ms  1.88  5.58K  0.06M
chk_29  0.32ms  0.48ms  0.63ms  1.98  4.98K  0.09M
chk_30  0.33ms  0.46ms  0.62ms  1.90  5.50K  0.07M
chk_31  0.33ms  0.46ms  0.62ms  1.90  5.38K  0.07M
   Avg  0.32  0.47  0.62
   Max  0.35  0.49  0.64
   Min  0.31  0.45  0.61
 Ratio  1.13  1.08  1.06
   Var  0.00  0.00  0.00
Profiling takes 0.679 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 5, starting model training...
Num Stages: 8 / 8
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 6.4523	TrainAcc 0.0197	ValidAcc 0.0103	TestAcc 0.0106	BestValid 0.0103
	Epoch 50:	Loss 3.7069	TrainAcc 0.0680	ValidAcc 0.0814	TestAcc 0.0964	BestValid 0.0814
	Epoch 100:	Loss 3.6982	TrainAcc 0.0661	ValidAcc 0.0564	TestAcc 0.0683	BestValid 0.0814
	Epoch 150:	Loss 3.6943	TrainAcc 0.0618	ValidAcc 0.0619	TestAcc 0.0775	BestValid 0.0814
	Epoch 200:	Loss 3.6925	TrainAcc 0.0467	ValidAcc 0.0430	TestAcc 0.0497	BestValid 0.0814
	Epoch 250:	Loss 3.6934	TrainAcc 0.0368	ValidAcc 0.0394	TestAcc 0.0451	BestValid 0.0814
	Epoch 300:	Loss 3.6915	TrainAcc 0.0409	ValidAcc 0.0420	TestAcc 0.0469	BestValid 0.0814
	Epoch 350:	Loss 3.6895	TrainAcc 0.0425	ValidAcc 0.0463	TestAcc 0.0515	BestValid 0.0814
	Epoch 400:	Loss 3.6892	TrainAcc 0.0396	ValidAcc 0.0418	TestAcc 0.0448	BestValid 0.0814
	Epoch 450:	Loss 3.6905	TrainAcc 0.0382	ValidAcc 0.0383	TestAcc 0.0389	BestValid 0.0814
	Epoch 500:	Loss 3.6904	TrainAcc 0.0371	ValidAcc 0.0396	TestAcc 0.0392	BestValid 0.0814
	Epoch 550:	Loss 3.6914	TrainAcc 0.0349	ValidAcc 0.0400	TestAcc 0.0394	BestValid 0.0814
	Epoch 600:	Loss 3.6917	TrainAcc 0.0367	ValidAcc 0.0407	TestAcc 0.0413	BestValid 0.0814
	Epoch 650:	Loss 3.6893	TrainAcc 0.0357	ValidAcc 0.0383	TestAcc 0.0401	BestValid 0.0814
	Epoch 700:	Loss 3.6909	TrainAcc 0.0391	ValidAcc 0.0372	TestAcc 0.0396	BestValid 0.0814
	Epoch 750:	Loss 3.6914	TrainAcc 0.0376	ValidAcc 0.0352	TestAcc 0.0381	BestValid 0.0814
	Epoch 800:	Loss 3.6918	TrainAcc 0.0373	ValidAcc 0.0382	TestAcc 0.0406	BestValid 0.0814
	Epoch 850:	Loss 3.6896	TrainAcc 0.0328	ValidAcc 0.0340	TestAcc 0.0359	BestValid 0.0814
	Epoch 900:	Loss 3.6910	TrainAcc 0.0324	ValidAcc 0.0314	TestAcc 0.0331	BestValid 0.0814
	Epoch 950:	Loss 3.6907	TrainAcc 0.0335	ValidAcc 0.0355	TestAcc 0.0353	BestValid 0.0814
	Epoch 1000:	Loss 3.6896	TrainAcc 0.0338	ValidAcc 0.0338	TestAcc 0.0338	BestValid 0.0814
	Epoch 1050:	Loss 3.6899	TrainAcc 0.0368	ValidAcc 0.0386	TestAcc 0.0405	BestValid 0.0814
	Epoch 1100:	Loss 3.6906	TrainAcc 0.0383	ValidAcc 0.0421	TestAcc 0.0460	BestValid 0.0814
	Epoch 1150:	Loss 3.6890	TrainAcc 0.0413	ValidAcc 0.0548	TestAcc 0.0592	BestValid 0.0814
	Epoch 1200:	Loss 3.6891	TrainAcc 0.0436	ValidAcc 0.0651	TestAcc 0.0689	BestValid 0.0814
	Epoch 1250:	Loss 3.6902	TrainAcc 0.0411	ValidAcc 0.0573	TestAcc 0.0633	BestValid 0.0814
	Epoch 1300:	Loss 3.6902	TrainAcc 0.0395	ValidAcc 0.0538	TestAcc 0.0592	BestValid 0.0814
	Epoch 1350:	Loss 3.6892	TrainAcc 0.0420	ValidAcc 0.0637	TestAcc 0.0664	BestValid 0.0814
	Epoch 1400:	Loss 3.6894	TrainAcc 0.0443	ValidAcc 0.0733	TestAcc 0.0741	BestValid 0.0814
	Epoch 1450:	Loss 3.6889	TrainAcc 0.0387	ValidAcc 0.0583	TestAcc 0.0606	BestValid 0.0814
	Epoch 1500:	Loss 3.6886	TrainAcc 0.0352	ValidAcc 0.0602	TestAcc 0.0615	BestValid 0.0814
	Epoch 1550:	Loss 3.6897	TrainAcc 0.0362	ValidAcc 0.0532	TestAcc 0.0565	BestValid 0.0814
	Epoch 1600:	Loss 3.6890	TrainAcc 0.0351	ValidAcc 0.0502	TestAcc 0.0537	BestValid 0.0814
	Epoch 1650:	Loss 3.6895	TrainAcc 0.0340	ValidAcc 0.0426	TestAcc 0.0451	BestValid 0.0814
	Epoch 1700:	Loss 3.6892	TrainAcc 0.0329	ValidAcc 0.0399	TestAcc 0.0443	BestValid 0.0814
	Epoch 1750:	Loss 3.6888	TrainAcc 0.0312	ValidAcc 0.0392	TestAcc 0.0437	BestValid 0.0814
	Epoch 1800:	Loss 3.6888	TrainAcc 0.0321	ValidAcc 0.0424	TestAcc 0.0466	BestValid 0.0814
	Epoch 1850:	Loss 3.6899	TrainAcc 0.0319	ValidAcc 0.0435	TestAcc 0.0481	BestValid 0.0814
	Epoch 1900:	Loss 3.6903	TrainAcc 0.0346	ValidAcc 0.0505	TestAcc 0.0550	BestValid 0.0814
	Epoch 1950:	Loss 3.6891	TrainAcc 0.0376	ValidAcc 0.0612	TestAcc 0.0655	BestValid 0.0814
	Epoch 2000:	Loss 3.6891	TrainAcc 0.0294	ValidAcc 0.0386	TestAcc 0.0410	BestValid 0.0814
	Epoch 2050:	Loss 3.6890	TrainAcc 0.0256	ValidAcc 0.0285	TestAcc 0.0290	BestValid 0.0814
	Epoch 2100:	Loss 3.6889	TrainAcc 0.0257	ValidAcc 0.0282	TestAcc 0.0286	BestValid 0.0814
	Epoch 2150:	Loss 3.6890	TrainAcc 0.0239	ValidAcc 0.0252	TestAcc 0.0259	BestValid 0.0814
	Epoch 2200:	Loss 3.6887	TrainAcc 0.0244	ValidAcc 0.0247	TestAcc 0.0251	BestValid 0.0814
	Epoch 2250:	Loss 3.6894	TrainAcc 0.0241	ValidAcc 0.0249	TestAcc 0.0253	BestValid 0.0814
	Epoch 2300:	Loss 3.6890	TrainAcc 0.0251	ValidAcc 0.0251	TestAcc 0.0255	BestValid 0.0814
	Epoch 2350:	Loss 3.6894	TrainAcc 0.0258	ValidAcc 0.0271	TestAcc 0.0284	BestValid 0.0814
	Epoch 2400:	Loss 3.6886	TrainAcc 0.0261	ValidAcc 0.0294	TestAcc 0.0304	BestValid 0.0814
	Epoch 2450:	Loss 3.6887	TrainAcc 0.0278	ValidAcc 0.0313	TestAcc 0.0319	BestValid 0.0814
	Epoch 2500:	Loss 3.6885	TrainAcc 0.0262	ValidAcc 0.0305	TestAcc 0.0312	BestValid 0.0814
	Epoch 2550:	Loss 3.6888	TrainAcc 0.0270	ValidAcc 0.0317	TestAcc 0.0315	BestValid 0.0814
	Epoch 2600:	Loss 3.6886	TrainAcc 0.0268	ValidAcc 0.0337	TestAcc 0.0331	BestValid 0.0814
	Epoch 2650:	Loss 3.6886	TrainAcc 0.0266	ValidAcc 0.0342	TestAcc 0.0336	BestValid 0.0814
	Epoch 2700:	Loss 3.6885	TrainAcc 0.1162	ValidAcc 0.0757	TestAcc 0.0624	BestValid 0.0814
	Epoch 2750:	Loss 3.6897	TrainAcc 0.1943	ValidAcc 0.1114	TestAcc 0.0923	BestValid 0.1114
	Epoch 2800:	Loss 3.6891	TrainAcc 0.2027	ValidAcc 0.1101	TestAcc 0.0956	BestValid 0.1114
	Epoch 2850:	Loss 3.6843	TrainAcc 0.1794	ValidAcc 0.0766	TestAcc 0.0589	BestValid 0.1114
	Epoch 2900:	Loss 3.6743	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0587	BestValid 0.1114
	Epoch 2950:	Loss 3.6377	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3000:	Loss 3.5481	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3050:	Loss 3.3095	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3100:	Loss 3.0824	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3150:	Loss 3.0043	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3200:	Loss 2.9804	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3250:	Loss 2.9733	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3300:	Loss 2.9692	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3350:	Loss 2.9694	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3400:	Loss 2.9593	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3450:	Loss 2.9502	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3500:	Loss 2.9493	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3550:	Loss 2.9446	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3600:	Loss 2.9258	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3650:	Loss 2.9289	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3700:	Loss 2.9284	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3750:	Loss 2.9329	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3800:	Loss 2.9190	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3850:	Loss 2.9137	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3900:	Loss 2.9149	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 3950:	Loss 2.9032	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4000:	Loss 2.9009	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4050:	Loss 2.9125	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4100:	Loss 2.8989	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4150:	Loss 2.9050	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4200:	Loss 2.9192	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4250:	Loss 2.9008	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4300:	Loss 2.9116	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4350:	Loss 2.9112	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4400:	Loss 2.9177	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4450:	Loss 2.9097	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4500:	Loss 2.9066	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4550:	Loss 2.9279	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4600:	Loss 2.8979	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4650:	Loss 2.9002	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4700:	Loss 2.8981	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4750:	Loss 2.9123	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4800:	Loss 2.8926	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4850:	Loss 2.8962	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4900:	Loss 2.8992	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 4950:	Loss 2.9016	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
	Epoch 5000:	Loss 2.9059	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.1114
****** Epoch Time (Excluding Evaluation Cost): 0.142 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 33.078 ms (Max: 35.687, Min: 20.305, Sum: 264.621)
Cluster-Wide Average, Compute: 66.627 ms (Max: 80.839, Min: 62.149, Sum: 533.012)
Cluster-Wide Average, Communication-Layer: 24.513 ms (Max: 29.419, Min: 17.048, Sum: 196.108)
Cluster-Wide Average, Bubble-Imbalance: 14.565 ms (Max: 20.722, Min: 7.907, Sum: 116.523)
Cluster-Wide Average, Communication-Graph: 0.456 ms (Max: 0.486, Min: 0.386, Sum: 3.646)
Cluster-Wide Average, Optimization: 0.096 ms (Max: 0.113, Min: 0.089, Sum: 0.765)
Cluster-Wide Average, Others: 3.170 ms (Max: 15.878, Min: 1.342, Sum: 25.359)
****** Breakdown Sum: 142.504 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.820 GB (Max: 5.016, Min: 3.622, Sum: 30.563)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 76.601 Gbps (Max: 92.685, Min: 61.637, Sum: 612.808)
Layer-level communication (cluster-wide, per-epoch): 1.766 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 1.766 GB
****** Accuracy Results ******
Highest valid_acc: 0.1114
Target test_acc: 0.0923
Epoch to reach the target acc: 2749
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
