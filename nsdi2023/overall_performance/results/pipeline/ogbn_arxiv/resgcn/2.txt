Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7

Initialized node 1 on machine gnerv7
DONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.067 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
        It takes 0.054 seconds.
        It takes 0.055 seconds.
        It takes 0.051 seconds.
        It takes 0.056 seconds.
        It takes 0.058 seconds.
        It takes 0.056 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.051 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.022 seconds.
        It takes 0.070 seconds.
Building the Label Vector...
        It takes 0.070 seconds.
Building the Label Vector...
        It takes 0.069 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
        It takes 0.028 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 61.305 Gbps (per GPU), 490.442 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 61.002 Gbps (per GPU), 488.014 Gbps (aggregated)
The layer-level communication performance: 61.144 Gbps (per GPU), 489.153 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.754 Gbps (per GPU), 486.031 Gbps (aggregated)
The layer-level communication performance: 60.872 Gbps (per GPU), 486.976 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.517 Gbps (per GPU), 484.135 Gbps (aggregated)
The layer-level communication performance: 60.466 Gbps (per GPU), 483.727 Gbps (aggregated)
The layer-level communication performance: 60.437 Gbps (per GPU), 483.496 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.105 Gbps (per GPU), 1264.843 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.102 Gbps (per GPU), 1264.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.106 Gbps (per GPU), 1264.845 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.978 Gbps (per GPU), 1263.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.106 Gbps (per GPU), 1264.845 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.055 Gbps (per GPU), 1264.440 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.978 Gbps (per GPU), 1263.820 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.016 Gbps (per GPU), 1264.128 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.769 Gbps (per GPU), 838.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.776 Gbps (per GPU), 838.204 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.777 Gbps (per GPU), 838.212 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.773 Gbps (per GPU), 838.183 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.769 Gbps (per GPU), 838.148 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.751 Gbps (per GPU), 838.009 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.768 Gbps (per GPU), 838.148 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.769 Gbps (per GPU), 838.148 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.477 Gbps (per GPU), 315.818 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.478 Gbps (per GPU), 315.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.478 Gbps (per GPU), 315.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.477 Gbps (per GPU), 315.817 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.477 Gbps (per GPU), 315.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.472 Gbps (per GPU), 315.772 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.473 Gbps (per GPU), 315.782 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.473 Gbps (per GPU), 315.782 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.49ms  1.06ms  1.84ms  3.75  5.55K  0.06M
 chk_1  0.51ms  1.07ms  1.86ms  3.62  5.75K  0.05M
 chk_2  0.50ms  1.05ms  1.80ms  3.60  5.20K  0.07M
 chk_3  0.50ms  1.07ms  1.84ms  3.65  5.57K  0.06M
 chk_4  0.50ms  1.03ms  1.77ms  3.57  5.05K  0.08M
 chk_5  0.52ms  1.05ms  1.78ms  3.42  4.73K  0.11M
 chk_6  0.49ms  1.03ms  1.77ms  3.60  4.98K  0.08M
 chk_7  0.50ms  1.04ms  1.78ms  3.55  5.01K  0.09M
 chk_8  0.50ms  1.07ms  1.85ms  3.71  5.73K  0.05M
 chk_9  0.50ms  1.01ms  1.73ms  3.50  4.54K  0.11M
chk_10  0.50ms  1.06ms  1.82ms  3.63  5.36K  0.07M
chk_11  0.50ms  1.07ms  1.83ms  3.65  5.39K  0.08M
chk_12  0.49ms  1.08ms  1.85ms  3.74  5.77K  0.05M
chk_13  0.50ms  1.06ms  1.82ms  3.67  5.43K  0.06M
chk_14  0.49ms  1.06ms  1.82ms  3.71  5.46K  0.06M
chk_15  0.50ms  1.08ms  1.87ms  3.72  5.88K  0.04M
chk_16  0.50ms  1.06ms  1.83ms  3.65  5.50K  0.06M
chk_17  0.51ms  1.05ms  1.79ms  3.48  4.86K  0.09M
chk_18  0.52ms  1.08ms  1.84ms  3.55  5.39K  0.07M
chk_19  0.50ms  1.05ms  1.80ms  3.59  5.20K  0.07M
chk_20  0.49ms  1.05ms  1.82ms  3.69  5.51K  0.06M
chk_21  0.49ms  1.06ms  1.85ms  3.79  5.81K  0.05M
chk_22  0.50ms  1.05ms  1.81ms  3.64  5.32K  0.07M
chk_23  0.51ms  1.07ms  1.84ms  3.58  5.39K  0.07M
chk_24  0.50ms  1.02ms  1.75ms  3.49  4.62K  0.11M
chk_25  0.50ms  1.04ms  1.78ms  3.57  5.04K  0.08M
chk_26  0.49ms  1.01ms  1.73ms  3.50  4.55K  0.11M
chk_27  0.49ms  1.04ms  1.80ms  3.68  5.30K  0.06M
chk_28  0.49ms  1.06ms  1.83ms  3.70  5.58K  0.06M
chk_29  0.51ms  1.04ms  1.79ms  3.53  4.98K  0.09M
chk_30  0.50ms  1.31ms  1.83ms  3.70  5.50K  0.07M
chk_31  0.50ms  1.07ms  1.82ms  3.66  5.38K  0.07M
   Avg  0.50  1.06  1.81
   Max  0.52  1.31  1.87
   Min  0.49  1.01  1.73
 Ratio  1.07  1.30  1.08
   Var  0.00  0.00  0.00
Profiling takes 1.352 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 48)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [48, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 152)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [152, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [256, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 256)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 6, starting model training...
Num Stages: 8 / 8
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [360, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [308, 360)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 48)...
+++++++++ Node 1 initializing the weights for op[48, 100)...
+++++++++ Node 2 initializing the weights for op[100, 152)...
+++++++++ Node 3 initializing the weights for op[152, 204)...
+++++++++ Node 4 initializing the weights for op[204, 256)...
+++++++++ Node 5 initializing the weights for op[256, 308)...
+++++++++ Node 6 initializing the weights for op[308, 360)...
+++++++++ Node 7 initializing the weights for op[360, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.1714	TrainAcc 0.1802	ValidAcc 0.0774	TestAcc 0.0594	BestValid 0.0774
	Epoch 50:	Loss 2.0345	TrainAcc 0.5136	ValidAcc 0.5413	TestAcc 0.5364	BestValid 0.5413
	Epoch 100:	Loss 1.6397	TrainAcc 0.6047	ValidAcc 0.6200	TestAcc 0.6173	BestValid 0.6200
	Epoch 150:	Loss 1.4525	TrainAcc 0.6478	ValidAcc 0.6534	TestAcc 0.6424	BestValid 0.6534
	Epoch 200:	Loss 1.3514	TrainAcc 0.6683	ValidAcc 0.6718	TestAcc 0.6647	BestValid 0.6718
	Epoch 250:	Loss 1.2855	TrainAcc 0.6840	ValidAcc 0.6855	TestAcc 0.6751	BestValid 0.6855
	Epoch 300:	Loss 1.2399	TrainAcc 0.6940	ValidAcc 0.6917	TestAcc 0.6789	BestValid 0.6917
	Epoch 350:	Loss 1.2081	TrainAcc 0.7003	ValidAcc 0.6969	TestAcc 0.6876	BestValid 0.6969
	Epoch 400:	Loss 1.1829	TrainAcc 0.7070	ValidAcc 0.7035	TestAcc 0.6949	BestValid 0.7035
	Epoch 450:	Loss 1.1624	TrainAcc 0.7115	ValidAcc 0.7071	TestAcc 0.6968	BestValid 0.7071
	Epoch 500:	Loss 1.1470	TrainAcc 0.7145	ValidAcc 0.7090	TestAcc 0.6983	BestValid 0.7090
	Epoch 550:	Loss 1.1338	TrainAcc 0.7175	ValidAcc 0.7111	TestAcc 0.6983	BestValid 0.7111
	Epoch 600:	Loss 1.1233	TrainAcc 0.7193	ValidAcc 0.7131	TestAcc 0.7063	BestValid 0.7131
	Epoch 650:	Loss 1.1089	TrainAcc 0.7228	ValidAcc 0.7139	TestAcc 0.7023	BestValid 0.7139
	Epoch 700:	Loss 1.0993	TrainAcc 0.7231	ValidAcc 0.7153	TestAcc 0.7049	BestValid 0.7153
	Epoch 750:	Loss 1.0907	TrainAcc 0.7256	ValidAcc 0.7165	TestAcc 0.7072	BestValid 0.7165
	Epoch 800:	Loss 1.0815	TrainAcc 0.7269	ValidAcc 0.7159	TestAcc 0.7051	BestValid 0.7165
	Epoch 850:	Loss 1.0762	TrainAcc 0.7290	ValidAcc 0.7174	TestAcc 0.7050	BestValid 0.7174
	Epoch 900:	Loss 1.0709	TrainAcc 0.7299	ValidAcc 0.7183	TestAcc 0.7081	BestValid 0.7183
	Epoch 950:	Loss 1.0624	TrainAcc 0.7312	ValidAcc 0.7202	TestAcc 0.7093	BestValid 0.7202
	Epoch 1000:	Loss 1.0578	TrainAcc 0.7317	ValidAcc 0.7195	TestAcc 0.7106	BestValid 0.7202
	Epoch 1050:	Loss 1.0518	TrainAcc 0.7325	ValidAcc 0.7196	TestAcc 0.7106	BestValid 0.7202
	Epoch 1100:	Loss 1.0461	TrainAcc 0.7334	ValidAcc 0.7193	TestAcc 0.7098	BestValid 0.7202
	Epoch 1150:	Loss 1.0410	TrainAcc 0.7338	ValidAcc 0.7192	TestAcc 0.7072	BestValid 0.7202
	Epoch 1200:	Loss 1.0358	TrainAcc 0.7352	ValidAcc 0.7215	TestAcc 0.7136	BestValid 0.7215
	Epoch 1250:	Loss 1.0276	TrainAcc 0.7368	ValidAcc 0.7211	TestAcc 0.7113	BestValid 0.7215
	Epoch 1300:	Loss 1.0296	TrainAcc 0.7368	ValidAcc 0.7213	TestAcc 0.7108	BestValid 0.7215
	Epoch 1350:	Loss 1.0206	TrainAcc 0.7383	ValidAcc 0.7226	TestAcc 0.7136	BestValid 0.7226
	Epoch 1400:	Loss 1.0246	TrainAcc 0.7397	ValidAcc 0.7234	TestAcc 0.7143	BestValid 0.7234
	Epoch 1450:	Loss 1.0184	TrainAcc 0.7391	ValidAcc 0.7219	TestAcc 0.7117	BestValid 0.7234
	Epoch 1500:	Loss 1.0143	TrainAcc 0.7395	ValidAcc 0.7197	TestAcc 0.7049	BestValid 0.7234
	Epoch 1550:	Loss 1.0105	TrainAcc 0.7400	ValidAcc 0.7206	TestAcc 0.7060	BestValid 0.7234
	Epoch 1600:	Loss 1.0065	TrainAcc 0.7408	ValidAcc 0.7218	TestAcc 0.7118	BestValid 0.7234
	Epoch 1650:	Loss 1.0044	TrainAcc 0.7427	ValidAcc 0.7246	TestAcc 0.7130	BestValid 0.7246
	Epoch 1700:	Loss 0.9964	TrainAcc 0.7427	ValidAcc 0.7221	TestAcc 0.7084	BestValid 0.7246
	Epoch 1750:	Loss 0.9968	TrainAcc 0.7442	ValidAcc 0.7252	TestAcc 0.7173	BestValid 0.7252
	Epoch 1800:	Loss 0.9875	TrainAcc 0.7435	ValidAcc 0.7210	TestAcc 0.7074	BestValid 0.7252
	Epoch 1850:	Loss 0.9878	TrainAcc 0.7447	ValidAcc 0.7247	TestAcc 0.7146	BestValid 0.7252
	Epoch 1900:	Loss 0.9871	TrainAcc 0.7455	ValidAcc 0.7241	TestAcc 0.7153	BestValid 0.7252
	Epoch 1950:	Loss 0.9833	TrainAcc 0.7471	ValidAcc 0.7258	TestAcc 0.7134	BestValid 0.7258
	Epoch 2000:	Loss 0.9817	TrainAcc 0.7457	ValidAcc 0.7239	TestAcc 0.7134	BestValid 0.7258
	Epoch 2050:	Loss 0.9747	TrainAcc 0.7481	ValidAcc 0.7265	TestAcc 0.7165	BestValid 0.7265
	Epoch 2100:	Loss 0.9781	TrainAcc 0.7453	ValidAcc 0.7183	TestAcc 0.7020	BestValid 0.7265
	Epoch 2150:	Loss 0.9764	TrainAcc 0.7479	ValidAcc 0.7260	TestAcc 0.7193	BestValid 0.7265
	Epoch 2200:	Loss 0.9688	TrainAcc 0.7489	ValidAcc 0.7253	TestAcc 0.7152	BestValid 0.7265
	Epoch 2250:	Loss 0.9653	TrainAcc 0.7484	ValidAcc 0.7253	TestAcc 0.7124	BestValid 0.7265
	Epoch 2300:	Loss 0.9634	TrainAcc 0.7501	ValidAcc 0.7256	TestAcc 0.7157	BestValid 0.7265
	Epoch 2350:	Loss 0.9609	TrainAcc 0.7491	ValidAcc 0.7263	TestAcc 0.7192	BestValid 0.7265
	Epoch 2400:	Loss 0.9574	TrainAcc 0.7506	ValidAcc 0.7262	TestAcc 0.7145	BestValid 0.7265
	Epoch 2450:	Loss 0.9556	TrainAcc 0.7511	ValidAcc 0.7274	TestAcc 0.7188	BestValid 0.7274
	Epoch 2500:	Loss 0.9540	TrainAcc 0.7487	ValidAcc 0.7249	TestAcc 0.7211	BestValid 0.7274
	Epoch 2550:	Loss 0.9517	TrainAcc 0.7525	ValidAcc 0.7264	TestAcc 0.7166	BestValid 0.7274
	Epoch 2600:	Loss 0.9530	TrainAcc 0.7520	ValidAcc 0.7250	TestAcc 0.7129	BestValid 0.7274
	Epoch 2650:	Loss 0.9507	TrainAcc 0.7520	ValidAcc 0.7266	TestAcc 0.7172	BestValid 0.7274
	Epoch 2700:	Loss 0.9466	TrainAcc 0.7527	ValidAcc 0.7265	TestAcc 0.7187	BestValid 0.7274
	Epoch 2750:	Loss 0.9425	TrainAcc 0.7523	ValidAcc 0.7252	TestAcc 0.7172	BestValid 0.7274
	Epoch 2800:	Loss 0.9384	TrainAcc 0.7533	ValidAcc 0.7249	TestAcc 0.7100	BestValid 0.7274
	Epoch 2850:	Loss 0.9419	TrainAcc 0.7538	ValidAcc 0.7270	TestAcc 0.7181	BestValid 0.7274
	Epoch 2900:	Loss 0.9375	TrainAcc 0.7525	ValidAcc 0.7226	TestAcc 0.7083	BestValid 0.7274
	Epoch 2950:	Loss 0.9378	TrainAcc 0.7544	ValidAcc 0.7269	TestAcc 0.7215	BestValid 0.7274
	Epoch 3000:	Loss 0.9320	TrainAcc 0.7546	ValidAcc 0.7272	TestAcc 0.7191	BestValid 0.7274
	Epoch 3050:	Loss 0.9314	TrainAcc 0.7554	ValidAcc 0.7265	TestAcc 0.7148	BestValid 0.7274
	Epoch 3100:	Loss 0.9315	TrainAcc 0.7558	ValidAcc 0.7283	TestAcc 0.7224	BestValid 0.7283
	Epoch 3150:	Loss 0.9294	TrainAcc 0.7558	ValidAcc 0.7291	TestAcc 0.7230	BestValid 0.7291
	Epoch 3200:	Loss 0.9259	TrainAcc 0.7562	ValidAcc 0.7255	TestAcc 0.7170	BestValid 0.7291
	Epoch 3250:	Loss 0.9270	TrainAcc 0.7571	ValidAcc 0.7271	TestAcc 0.7158	BestValid 0.7291
	Epoch 3300:	Loss 0.9225	TrainAcc 0.7567	ValidAcc 0.7290	TestAcc 0.7225	BestValid 0.7291
	Epoch 3350:	Loss 0.9220	TrainAcc 0.7587	ValidAcc 0.7283	TestAcc 0.7198	BestValid 0.7291
	Epoch 3400:	Loss 0.9217	TrainAcc 0.7577	ValidAcc 0.7289	TestAcc 0.7199	BestValid 0.7291
	Epoch 3450:	Loss 0.9169	TrainAcc 0.7588	ValidAcc 0.7285	TestAcc 0.7193	BestValid 0.7291
	Epoch 3500:	Loss 0.9134	TrainAcc 0.7570	ValidAcc 0.7262	TestAcc 0.7215	BestValid 0.7291
	Epoch 3550:	Loss 0.9141	TrainAcc 0.7591	ValidAcc 0.7285	TestAcc 0.7235	BestValid 0.7291
	Epoch 3600:	Loss 0.9158	TrainAcc 0.7595	ValidAcc 0.7292	TestAcc 0.7236	BestValid 0.7292
	Epoch 3650:	Loss 0.9138	TrainAcc 0.7585	ValidAcc 0.7282	TestAcc 0.7220	BestValid 0.7292
	Epoch 3700:	Loss 0.9100	TrainAcc 0.7598	ValidAcc 0.7285	TestAcc 0.7228	BestValid 0.7292
	Epoch 3750:	Loss 0.9079	TrainAcc 0.7602	ValidAcc 0.7231	TestAcc 0.7074	BestValid 0.7292
	Epoch 3800:	Loss 0.9073	TrainAcc 0.7606	ValidAcc 0.7280	TestAcc 0.7158	BestValid 0.7292
	Epoch 3850:	Loss 0.9067	TrainAcc 0.7607	ValidAcc 0.7277	TestAcc 0.7176	BestValid 0.7292
	Epoch 3900:	Loss 0.9056	TrainAcc 0.7627	ValidAcc 0.7292	TestAcc 0.7172	BestValid 0.7292
	Epoch 3950:	Loss 0.9030	TrainAcc 0.7621	ValidAcc 0.7286	TestAcc 0.7208	BestValid 0.7292
	Epoch 4000:	Loss 0.9000	TrainAcc 0.7623	ValidAcc 0.7302	TestAcc 0.7223	BestValid 0.7302
	Epoch 4050:	Loss 0.8987	TrainAcc 0.7628	ValidAcc 0.7284	TestAcc 0.7158	BestValid 0.7302
	Epoch 4100:	Loss 0.9040	TrainAcc 0.7608	ValidAcc 0.7285	TestAcc 0.7222	BestValid 0.7302
	Epoch 4150:	Loss 0.8941	TrainAcc 0.7624	ValidAcc 0.7270	TestAcc 0.7154	BestValid 0.7302
	Epoch 4200:	Loss 0.8964	TrainAcc 0.7645	ValidAcc 0.7285	TestAcc 0.7141	BestValid 0.7302
	Epoch 4250:	Loss 0.8952	TrainAcc 0.7640	ValidAcc 0.7283	TestAcc 0.7187	BestValid 0.7302
	Epoch 4300:	Loss 0.8926	TrainAcc 0.7644	ValidAcc 0.7294	TestAcc 0.7228	BestValid 0.7302
	Epoch 4350:	Loss 0.8900	TrainAcc 0.7626	ValidAcc 0.7281	TestAcc 0.7206	BestValid 0.7302
	Epoch 4400:	Loss 0.8898	TrainAcc 0.7629	ValidAcc 0.7257	TestAcc 0.7154	BestValid 0.7302
	Epoch 4450:	Loss 0.8895	TrainAcc 0.7635	ValidAcc 0.7281	TestAcc 0.7201	BestValid 0.7302
	Epoch 4500:	Loss 0.8893	TrainAcc 0.7652	ValidAcc 0.7295	TestAcc 0.7224	BestValid 0.7302
	Epoch 4550:	Loss 0.8851	TrainAcc 0.7648	ValidAcc 0.7292	TestAcc 0.7244	BestValid 0.7302
	Epoch 4600:	Loss 0.8833	TrainAcc 0.7659	ValidAcc 0.7314	TestAcc 0.7234	BestValid 0.7314
	Epoch 4650:	Loss 0.8839	TrainAcc 0.7660	ValidAcc 0.7307	TestAcc 0.7231	BestValid 0.7314
	Epoch 4700:	Loss 0.8821	TrainAcc 0.7662	ValidAcc 0.7310	TestAcc 0.7235	BestValid 0.7314
	Epoch 4750:	Loss 0.8797	TrainAcc 0.7658	ValidAcc 0.7299	TestAcc 0.7228	BestValid 0.7314
	Epoch 4800:	Loss 0.8813	TrainAcc 0.7661	ValidAcc 0.7305	TestAcc 0.7231	BestValid 0.7314
	Epoch 4850:	Loss 0.8808	TrainAcc 0.7662	ValidAcc 0.7282	TestAcc 0.7159	BestValid 0.7314
	Epoch 4900:	Loss 0.8783	TrainAcc 0.7671	ValidAcc 0.7314	TestAcc 0.7262	BestValid 0.7314
	Epoch 4950:	Loss 0.8811	TrainAcc 0.7671	ValidAcc 0.7310	TestAcc 0.7211	BestValid 0.7314
	Epoch 5000:	Loss 0.8769	TrainAcc 0.7684	ValidAcc 0.7301	TestAcc 0.7196	BestValid 0.7314
****** Epoch Time (Excluding Evaluation Cost): 0.223 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 45.390 ms (Max: 50.127, Min: 30.321, Sum: 363.116)
Cluster-Wide Average, Compute: 130.606 ms (Max: 163.320, Min: 115.480, Sum: 1044.845)
Cluster-Wide Average, Communication-Layer: 15.558 ms (Max: 17.695, Min: 11.041, Sum: 124.463)
Cluster-Wide Average, Bubble-Imbalance: 27.446 ms (Max: 43.382, Min: 1.688, Sum: 219.568)
Cluster-Wide Average, Communication-Graph: 0.484 ms (Max: 0.622, Min: 0.433, Sum: 3.868)
Cluster-Wide Average, Optimization: 0.257 ms (Max: 0.285, Min: 0.247, Sum: 2.057)
Cluster-Wide Average, Others: 3.473 ms (Max: 16.161, Min: 1.636, Sum: 27.783)
****** Breakdown Sum: 223.212 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.548 GB (Max: 5.089, Min: 4.385, Sum: 36.388)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 59.927 Gbps (Max: 66.576, Min: 47.394, Sum: 479.412)
Layer-level communication (cluster-wide, per-epoch): 0.883 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.883 GB
****** Accuracy Results ******
Highest valid_acc: 0.7314
Target test_acc: 0.7234
Epoch to reach the target acc: 4599
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
