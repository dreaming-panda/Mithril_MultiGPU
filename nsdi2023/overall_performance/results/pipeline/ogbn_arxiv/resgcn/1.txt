Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 4 on machine gnerv8

Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.047 seconds.
        It takes 0.049 seconds.
        It takes 0.051 seconds.
        It takes 0.052 seconds.
        It takes 0.050 seconds.
Building the Feature Vector...
        It takes 0.055 seconds.
        It takes 0.053 seconds.
        It takes 0.055 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.050 seconds.
Building the Label Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.021 seconds.
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.024 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.791 Gbps (per GPU), 486.331 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.484 Gbps (per GPU), 483.870 Gbps (aggregated)
The layer-level communication performance: 60.478 Gbps (per GPU), 483.823 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.224 Gbps (per GPU), 481.792 Gbps (aggregated)
The layer-level communication performance: 60.197 Gbps (per GPU), 481.576 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.988 Gbps (per GPU), 479.903 Gbps (aggregated)
The layer-level communication performance: 59.934 Gbps (per GPU), 479.471 Gbps (aggregated)
The layer-level communication performance: 59.899 Gbps (per GPU), 479.195 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 145.522 Gbps (per GPU), 1164.175 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 145.519 Gbps (per GPU), 1164.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 145.681 Gbps (per GPU), 1165.449 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 152.929 Gbps (per GPU), 1223.431 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 145.524 Gbps (per GPU), 1164.196 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 152.915 Gbps (per GPU), 1223.322 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 145.522 Gbps (per GPU), 1164.175 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 145.446 Gbps (per GPU), 1163.569 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 94.683 Gbps (per GPU), 757.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.678 Gbps (per GPU), 757.425 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.685 Gbps (per GPU), 757.476 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.681 Gbps (per GPU), 757.448 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.665 Gbps (per GPU), 757.322 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.681 Gbps (per GPU), 757.448 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.668 Gbps (per GPU), 757.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.678 Gbps (per GPU), 757.425 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.661 Gbps (per GPU), 309.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.660 Gbps (per GPU), 309.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.662 Gbps (per GPU), 309.298 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.661 Gbps (per GPU), 309.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.662 Gbps (per GPU), 309.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.662 Gbps (per GPU), 309.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.656 Gbps (per GPU), 309.251 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.659 Gbps (per GPU), 309.275 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.49ms  1.05ms  1.82ms  3.73  5.55K  0.06M
 chk_1  0.51ms  1.07ms  1.84ms  3.60  5.75K  0.05M
 chk_2  0.50ms  1.04ms  1.80ms  3.62  5.20K  0.07M
 chk_3  0.50ms  1.06ms  1.83ms  3.66  5.57K  0.06M
 chk_4  0.49ms  1.03ms  1.77ms  3.59  5.05K  0.08M
 chk_5  0.52ms  1.05ms  1.78ms  3.42  4.73K  0.11M
 chk_6  0.49ms  1.03ms  1.76ms  3.59  4.98K  0.08M
 chk_7  0.50ms  1.04ms  1.77ms  3.55  5.01K  0.09M
 chk_8  0.49ms  1.06ms  1.84ms  3.72  5.73K  0.05M
 chk_9  0.49ms  1.01ms  1.72ms  3.48  4.54K  0.11M
chk_10  0.50ms  1.06ms  1.81ms  3.65  5.36K  0.07M
chk_11  0.50ms  1.06ms  1.82ms  3.63  5.39K  0.08M
chk_12  0.49ms  1.07ms  1.84ms  3.74  5.77K  0.05M
chk_13  0.50ms  1.06ms  1.81ms  3.66  5.43K  0.06M
chk_14  0.49ms  1.05ms  1.81ms  3.67  5.46K  0.06M
chk_15  0.49ms  1.07ms  1.86ms  3.76  5.88K  0.04M
chk_16  0.77ms  1.06ms  1.82ms  2.36  5.50K  0.06M
chk_17  0.51ms  1.05ms  1.78ms  3.46  4.86K  0.09M
chk_18  0.52ms  1.08ms  1.83ms  3.52  5.39K  0.07M
chk_19  0.50ms  1.05ms  1.79ms  3.57  5.20K  0.07M
chk_20  0.49ms  1.05ms  1.81ms  3.68  5.51K  0.06M
chk_21  0.49ms  1.06ms  1.84ms  3.78  5.81K  0.05M
chk_22  0.50ms  1.05ms  1.80ms  3.64  5.32K  0.07M
chk_23  0.51ms  1.07ms  1.82ms  3.56  5.39K  0.07M
chk_24  0.50ms  1.02ms  1.74ms  3.48  4.62K  0.11M
chk_25  0.50ms  1.04ms  1.77ms  3.55  5.04K  0.08M
chk_26  0.49ms  1.01ms  1.72ms  3.49  4.55K  0.11M
chk_27  0.49ms  1.04ms  1.79ms  3.67  5.30K  0.06M
chk_28  0.49ms  1.05ms  1.82ms  3.69  5.58K  0.06M
chk_29  0.51ms  1.05ms  1.78ms  3.52  4.98K  0.09M
chk_30  0.50ms  1.06ms  1.82ms  3.67  5.50K  0.07M
chk_31  0.50ms  1.06ms  1.82ms  3.65  5.38K  0.07M
   Avg  0.51  1.05  1.80
   Max  0.77  1.08  1.86
   Min  0.49  1.01  1.72
 Ratio  1.58  1.07  1.08
   Var  0.00  0.00  0.00
Profiling takes 1.345 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 48)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 256)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 1, starting model training...
Num Stages: 8 / 8
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [256, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [48, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [308, 360)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 3, starting model training...
Num Stages: 8 / 8
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [360, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 152)
*** Node 2, constructing the helper classes...
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [152, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 169343
Node 2, Local Vertex Begin: 0, Num Local Vertices: 169343
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 48)...
+++++++++ Node 5 initializing the weights for op[256, 308)...
+++++++++ Node 1 initializing the weights for op[48, 100)...
+++++++++ Node 4 initializing the weights for op[204, 256)...
+++++++++ Node 2 initializing the weights for op[100, 152)...
+++++++++ Node 6 initializing the weights for op[308, 360)...
+++++++++ Node 3 initializing the weights for op[152, 204)...
+++++++++ Node 7 initializing the weights for op[360, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.0937	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 50:	Loss 2.1279	TrainAcc 0.4748	ValidAcc 0.4884	TestAcc 0.4637	BestValid 0.4884
	Epoch 100:	Loss 1.6888	TrainAcc 0.5920	ValidAcc 0.6129	TestAcc 0.6152	BestValid 0.6129
	Epoch 150:	Loss 1.4597	TrainAcc 0.6455	ValidAcc 0.6563	TestAcc 0.6543	BestValid 0.6563
	Epoch 200:	Loss 1.3494	TrainAcc 0.6685	ValidAcc 0.6717	TestAcc 0.6638	BestValid 0.6717
	Epoch 250:	Loss 1.2812	TrainAcc 0.6835	ValidAcc 0.6854	TestAcc 0.6809	BestValid 0.6854
	Epoch 300:	Loss 1.2416	TrainAcc 0.6936	ValidAcc 0.6927	TestAcc 0.6854	BestValid 0.6927
	Epoch 350:	Loss 1.2062	TrainAcc 0.7015	ValidAcc 0.6983	TestAcc 0.6876	BestValid 0.6983
	Epoch 400:	Loss 1.1876	TrainAcc 0.7059	ValidAcc 0.7020	TestAcc 0.6926	BestValid 0.7020
	Epoch 450:	Loss 1.1681	TrainAcc 0.7100	ValidAcc 0.7050	TestAcc 0.6927	BestValid 0.7050
	Epoch 500:	Loss 1.1509	TrainAcc 0.7133	ValidAcc 0.7084	TestAcc 0.6961	BestValid 0.7084
	Epoch 550:	Loss 1.1397	TrainAcc 0.7163	ValidAcc 0.7084	TestAcc 0.6943	BestValid 0.7084
	Epoch 600:	Loss 1.1249	TrainAcc 0.7173	ValidAcc 0.7091	TestAcc 0.7002	BestValid 0.7091
	Epoch 650:	Loss 1.1154	TrainAcc 0.7190	ValidAcc 0.7066	TestAcc 0.6944	BestValid 0.7091
	Epoch 700:	Loss 1.1048	TrainAcc 0.7231	ValidAcc 0.7132	TestAcc 0.7017	BestValid 0.7132
	Epoch 750:	Loss 1.0979	TrainAcc 0.7237	ValidAcc 0.7129	TestAcc 0.7005	BestValid 0.7132
	Epoch 800:	Loss 1.0891	TrainAcc 0.7253	ValidAcc 0.7130	TestAcc 0.7032	BestValid 0.7132
	Epoch 850:	Loss 1.0783	TrainAcc 0.7265	ValidAcc 0.7143	TestAcc 0.7015	BestValid 0.7143
	Epoch 900:	Loss 1.0743	TrainAcc 0.7280	ValidAcc 0.7157	TestAcc 0.7044	BestValid 0.7157
	Epoch 950:	Loss 1.0641	TrainAcc 0.7289	ValidAcc 0.7172	TestAcc 0.7099	BestValid 0.7172
	Epoch 1000:	Loss 1.0615	TrainAcc 0.7302	ValidAcc 0.7157	TestAcc 0.7043	BestValid 0.7172
	Epoch 1050:	Loss 1.0543	TrainAcc 0.7310	ValidAcc 0.7183	TestAcc 0.7077	BestValid 0.7183
	Epoch 1100:	Loss 1.0474	TrainAcc 0.7319	ValidAcc 0.7200	TestAcc 0.7097	BestValid 0.7200
	Epoch 1150:	Loss 1.0431	TrainAcc 0.7328	ValidAcc 0.7178	TestAcc 0.7052	BestValid 0.7200
	Epoch 1200:	Loss 1.0415	TrainAcc 0.7338	ValidAcc 0.7190	TestAcc 0.7090	BestValid 0.7200
	Epoch 1250:	Loss 1.0315	TrainAcc 0.7348	ValidAcc 0.7197	TestAcc 0.7086	BestValid 0.7200
	Epoch 1300:	Loss 1.0299	TrainAcc 0.7357	ValidAcc 0.7206	TestAcc 0.7091	BestValid 0.7206
	Epoch 1350:	Loss 1.0314	TrainAcc 0.7365	ValidAcc 0.7206	TestAcc 0.7089	BestValid 0.7206
	Epoch 1400:	Loss 1.0228	TrainAcc 0.7371	ValidAcc 0.7230	TestAcc 0.7130	BestValid 0.7230
	Epoch 1450:	Loss 1.0153	TrainAcc 0.7386	ValidAcc 0.7214	TestAcc 0.7114	BestValid 0.7230
	Epoch 1500:	Loss 1.0162	TrainAcc 0.7396	ValidAcc 0.7239	TestAcc 0.7130	BestValid 0.7239
	Epoch 1550:	Loss 1.0069	TrainAcc 0.7401	ValidAcc 0.7224	TestAcc 0.7101	BestValid 0.7239
	Epoch 1600:	Loss 1.0071	TrainAcc 0.7395	ValidAcc 0.7178	TestAcc 0.7033	BestValid 0.7239
	Epoch 1650:	Loss 1.0038	TrainAcc 0.7407	ValidAcc 0.7206	TestAcc 0.7111	BestValid 0.7239
	Epoch 1700:	Loss 1.0006	TrainAcc 0.7422	ValidAcc 0.7219	TestAcc 0.7090	BestValid 0.7239
	Epoch 1750:	Loss 0.9987	TrainAcc 0.7429	ValidAcc 0.7238	TestAcc 0.7114	BestValid 0.7239
	Epoch 1800:	Loss 0.9943	TrainAcc 0.7423	ValidAcc 0.7219	TestAcc 0.7123	BestValid 0.7239
	Epoch 1850:	Loss 0.9916	TrainAcc 0.7442	ValidAcc 0.7241	TestAcc 0.7127	BestValid 0.7241
	Epoch 1900:	Loss 0.9822	TrainAcc 0.7442	ValidAcc 0.7246	TestAcc 0.7141	BestValid 0.7246
	Epoch 1950:	Loss 0.9855	TrainAcc 0.7452	ValidAcc 0.7231	TestAcc 0.7120	BestValid 0.7246
	Epoch 2000:	Loss 0.9834	TrainAcc 0.7456	ValidAcc 0.7221	TestAcc 0.7096	BestValid 0.7246
	Epoch 2050:	Loss 0.9776	TrainAcc 0.7463	ValidAcc 0.7258	TestAcc 0.7175	BestValid 0.7258
	Epoch 2100:	Loss 0.9779	TrainAcc 0.7472	ValidAcc 0.7256	TestAcc 0.7148	BestValid 0.7258
	Epoch 2150:	Loss 0.9768	TrainAcc 0.7472	ValidAcc 0.7252	TestAcc 0.7139	BestValid 0.7258
	Epoch 2200:	Loss 0.9683	TrainAcc 0.7462	ValidAcc 0.7248	TestAcc 0.7148	BestValid 0.7258
	Epoch 2250:	Loss 0.9632	TrainAcc 0.7473	ValidAcc 0.7262	TestAcc 0.7218	BestValid 0.7262
	Epoch 2300:	Loss 0.9651	TrainAcc 0.7486	ValidAcc 0.7261	TestAcc 0.7168	BestValid 0.7262
	Epoch 2350:	Loss 0.9626	TrainAcc 0.7497	ValidAcc 0.7266	TestAcc 0.7183	BestValid 0.7266
	Epoch 2400:	Loss 0.9611	TrainAcc 0.7497	ValidAcc 0.7252	TestAcc 0.7156	BestValid 0.7266
	Epoch 2450:	Loss 0.9561	TrainAcc 0.7497	ValidAcc 0.7227	TestAcc 0.7069	BestValid 0.7266
	Epoch 2500:	Loss 0.9551	TrainAcc 0.7500	ValidAcc 0.7267	TestAcc 0.7194	BestValid 0.7267
	Epoch 2550:	Loss 0.9537	TrainAcc 0.7504	ValidAcc 0.7267	TestAcc 0.7196	BestValid 0.7267
	Epoch 2600:	Loss 0.9525	TrainAcc 0.7516	ValidAcc 0.7281	TestAcc 0.7198	BestValid 0.7281
	Epoch 2650:	Loss 0.9531	TrainAcc 0.7519	ValidAcc 0.7270	TestAcc 0.7175	BestValid 0.7281
	Epoch 2700:	Loss 0.9476	TrainAcc 0.7518	ValidAcc 0.7243	TestAcc 0.7098	BestValid 0.7281
	Epoch 2750:	Loss 0.9433	TrainAcc 0.7523	ValidAcc 0.7252	TestAcc 0.7125	BestValid 0.7281
	Epoch 2800:	Loss 0.9422	TrainAcc 0.7532	ValidAcc 0.7272	TestAcc 0.7189	BestValid 0.7281
	Epoch 2850:	Loss 0.9414	TrainAcc 0.7524	ValidAcc 0.7247	TestAcc 0.7104	BestValid 0.7281
	Epoch 2900:	Loss 0.9397	TrainAcc 0.7524	ValidAcc 0.7262	TestAcc 0.7187	BestValid 0.7281
	Epoch 2950:	Loss 0.9401	TrainAcc 0.7523	ValidAcc 0.7175	TestAcc 0.7016	BestValid 0.7281
	Epoch 3000:	Loss 0.9340	TrainAcc 0.7545	ValidAcc 0.7249	TestAcc 0.7108	BestValid 0.7281
	Epoch 3050:	Loss 0.9345	TrainAcc 0.7546	ValidAcc 0.7279	TestAcc 0.7195	BestValid 0.7281
	Epoch 3100:	Loss 0.9332	TrainAcc 0.7550	ValidAcc 0.7256	TestAcc 0.7129	BestValid 0.7281
	Epoch 3150:	Loss 0.9300	TrainAcc 0.7561	ValidAcc 0.7273	TestAcc 0.7154	BestValid 0.7281
	Epoch 3200:	Loss 0.9283	TrainAcc 0.7558	ValidAcc 0.7281	TestAcc 0.7196	BestValid 0.7281
	Epoch 3250:	Loss 0.9234	TrainAcc 0.7570	ValidAcc 0.7280	TestAcc 0.7202	BestValid 0.7281
	Epoch 3300:	Loss 0.9244	TrainAcc 0.7570	ValidAcc 0.7249	TestAcc 0.7136	BestValid 0.7281
	Epoch 3350:	Loss 0.9222	TrainAcc 0.7570	ValidAcc 0.7260	TestAcc 0.7149	BestValid 0.7281
	Epoch 3400:	Loss 0.9240	TrainAcc 0.7579	ValidAcc 0.7272	TestAcc 0.7165	BestValid 0.7281
	Epoch 3450:	Loss 0.9184	TrainAcc 0.7576	ValidAcc 0.7276	TestAcc 0.7156	BestValid 0.7281
	Epoch 3500:	Loss 0.9155	TrainAcc 0.7580	ValidAcc 0.7285	TestAcc 0.7192	BestValid 0.7285
	Epoch 3550:	Loss 0.9179	TrainAcc 0.7588	ValidAcc 0.7294	TestAcc 0.7213	BestValid 0.7294
	Epoch 3600:	Loss 0.9125	TrainAcc 0.7589	ValidAcc 0.7230	TestAcc 0.7071	BestValid 0.7294
	Epoch 3650:	Loss 0.9114	TrainAcc 0.7590	ValidAcc 0.7279	TestAcc 0.7180	BestValid 0.7294
	Epoch 3700:	Loss 0.9109	TrainAcc 0.7598	ValidAcc 0.7294	TestAcc 0.7217	BestValid 0.7294
	Epoch 3750:	Loss 0.9106	TrainAcc 0.7592	ValidAcc 0.7290	TestAcc 0.7216	BestValid 0.7294
	Epoch 3800:	Loss 0.9091	TrainAcc 0.7598	ValidAcc 0.7280	TestAcc 0.7147	BestValid 0.7294
	Epoch 3850:	Loss 0.9087	TrainAcc 0.7604	ValidAcc 0.7257	TestAcc 0.7108	BestValid 0.7294
	Epoch 3900:	Loss 0.9050	TrainAcc 0.7604	ValidAcc 0.7294	TestAcc 0.7200	BestValid 0.7294
	Epoch 3950:	Loss 0.9026	TrainAcc 0.7608	ValidAcc 0.7274	TestAcc 0.7188	BestValid 0.7294
	Epoch 4000:	Loss 0.9042	TrainAcc 0.7612	ValidAcc 0.7292	TestAcc 0.7221	BestValid 0.7294
	Epoch 4050:	Loss 0.9006	TrainAcc 0.7599	ValidAcc 0.7254	TestAcc 0.7106	BestValid 0.7294
	Epoch 4100:	Loss 0.9001	TrainAcc 0.7622	ValidAcc 0.7258	TestAcc 0.7096	BestValid 0.7294
	Epoch 4150:	Loss 0.8991	TrainAcc 0.7613	ValidAcc 0.7294	TestAcc 0.7204	BestValid 0.7294
	Epoch 4200:	Loss 0.8957	TrainAcc 0.7622	ValidAcc 0.7291	TestAcc 0.7181	BestValid 0.7294
	Epoch 4250:	Loss 0.8953	TrainAcc 0.7633	ValidAcc 0.7292	TestAcc 0.7186	BestValid 0.7294
	Epoch 4300:	Loss 0.8976	TrainAcc 0.7622	ValidAcc 0.7240	TestAcc 0.7079	BestValid 0.7294
	Epoch 4350:	Loss 0.8926	TrainAcc 0.7648	ValidAcc 0.7296	TestAcc 0.7214	BestValid 0.7296
	Epoch 4400:	Loss 0.8935	TrainAcc 0.7641	ValidAcc 0.7297	TestAcc 0.7218	BestValid 0.7297
	Epoch 4450:	Loss 0.8923	TrainAcc 0.7636	ValidAcc 0.7304	TestAcc 0.7219	BestValid 0.7304
	Epoch 4500:	Loss 0.8908	TrainAcc 0.7637	ValidAcc 0.7250	TestAcc 0.7113	BestValid 0.7304
	Epoch 4550:	Loss 0.8881	TrainAcc 0.7642	ValidAcc 0.7295	TestAcc 0.7214	BestValid 0.7304
	Epoch 4600:	Loss 0.8887	TrainAcc 0.7656	ValidAcc 0.7288	TestAcc 0.7202	BestValid 0.7304
	Epoch 4650:	Loss 0.8839	TrainAcc 0.7650	ValidAcc 0.7278	TestAcc 0.7156	BestValid 0.7304
	Epoch 4700:	Loss 0.8808	TrainAcc 0.7654	ValidAcc 0.7311	TestAcc 0.7207	BestValid 0.7311
	Epoch 4750:	Loss 0.8836	TrainAcc 0.7666	ValidAcc 0.7313	TestAcc 0.7212	BestValid 0.7313
	Epoch 4800:	Loss 0.8816	TrainAcc 0.7663	ValidAcc 0.7302	TestAcc 0.7224	BestValid 0.7313
	Epoch 4850:	Loss 0.8806	TrainAcc 0.7667	ValidAcc 0.7270	TestAcc 0.7150	BestValid 0.7313
	Epoch 4900:	Loss 0.8774	TrainAcc 0.7635	ValidAcc 0.7293	TestAcc 0.7266	BestValid 0.7313
	Epoch 4950:	Loss 0.8785	TrainAcc 0.7657	ValidAcc 0.7289	TestAcc 0.7168	BestValid 0.7313
	Epoch 5000:	Loss 0.8757	TrainAcc 0.7666	ValidAcc 0.7252	TestAcc 0.7104	BestValid 0.7313
****** Epoch Time (Excluding Evaluation Cost): 0.224 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 45.568 ms (Max: 50.398, Min: 30.593, Sum: 364.547)
Cluster-Wide Average, Compute: 131.892 ms (Max: 164.022, Min: 115.183, Sum: 1055.133)
Cluster-Wide Average, Communication-Layer: 15.552 ms (Max: 17.609, Min: 11.049, Sum: 124.416)
Cluster-Wide Average, Bubble-Imbalance: 27.260 ms (Max: 44.872, Min: 1.991, Sum: 218.082)
Cluster-Wide Average, Communication-Graph: 0.509 ms (Max: 0.666, Min: 0.406, Sum: 4.074)
Cluster-Wide Average, Optimization: 0.261 ms (Max: 0.287, Min: 0.248, Sum: 2.084)
Cluster-Wide Average, Others: 3.480 ms (Max: 16.166, Min: 1.624, Sum: 27.840)
****** Breakdown Sum: 224.522 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.548 GB (Max: 5.089, Min: 4.385, Sum: 36.388)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 59.944 Gbps (Max: 66.316, Min: 47.568, Sum: 479.548)
Layer-level communication (cluster-wide, per-epoch): 0.883 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.883 GB
****** Accuracy Results ******
Highest valid_acc: 0.7313
Target test_acc: 0.7212
Epoch to reach the target acc: 4749
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
