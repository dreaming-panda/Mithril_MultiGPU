Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...Building the CSR structure...

        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.014 seconds.
Building the Feature Vector...
        It takes 0.011 seconds.
        It takes 0.010 seconds.
        It takes 0.013 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.413 Gbps (per GPU), 451.301 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.162 Gbps (per GPU), 449.296 Gbps (aggregated)
The layer-level communication performance: 56.153 Gbps (per GPU), 449.226 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.930 Gbps (per GPU), 447.442 Gbps (aggregated)
The layer-level communication performance: 55.901 Gbps (per GPU), 447.210 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.713 Gbps (per GPU), 445.704 Gbps (aggregated)
The layer-level communication performance: 55.670 Gbps (per GPU), 445.360 Gbps (aggregated)
The layer-level communication performance: 55.643 Gbps (per GPU), 445.144 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 153.778 Gbps (per GPU), 1230.227 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 153.767 Gbps (per GPU), 1230.136 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 153.716 Gbps (per GPU), 1229.731 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 153.773 Gbps (per GPU), 1230.182 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 153.795 Gbps (per GPU), 1230.362 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 153.784 Gbps (per GPU), 1230.272 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 153.719 Gbps (per GPU), 1229.753 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 153.778 Gbps (per GPU), 1230.227 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 98.709 Gbps (per GPU), 789.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.710 Gbps (per GPU), 789.677 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.708 Gbps (per GPU), 789.665 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.706 Gbps (per GPU), 789.646 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.707 Gbps (per GPU), 789.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.707 Gbps (per GPU), 789.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.709 Gbps (per GPU), 789.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.707 Gbps (per GPU), 789.652 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.470 Gbps (per GPU), 267.764 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.471 Gbps (per GPU), 267.766 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.470 Gbps (per GPU), 267.757 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.471 Gbps (per GPU), 267.768 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.465 Gbps (per GPU), 267.723 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.468 Gbps (per GPU), 267.746 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.465 Gbps (per GPU), 267.724 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.465 Gbps (per GPU), 267.719 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.35ms  0.43ms  0.56ms  1.59  0.08K  0.02M
 chk_1  0.44ms  0.46ms  0.60ms  1.36  0.16K  0.01M
 chk_2  0.40ms  0.44ms  0.58ms  1.45  0.13K  0.01M
 chk_3  0.46ms  0.50ms  0.64ms  1.38  0.18K  0.01M
 chk_4  0.44ms  0.46ms  0.60ms  1.37  0.15K  0.01M
 chk_5  0.52ms  0.51ms  0.65ms  1.28  0.25K  0.01M
 chk_6  0.36ms  0.43ms  0.57ms  1.59  0.10K  0.02M
 chk_7  0.40ms  0.45ms  0.59ms  1.47  0.11K  0.02M
 chk_8  0.51ms  0.50ms  0.64ms  1.29  0.23K  0.01M
 chk_9  0.43ms  0.46ms  0.60ms  1.38  0.14K  0.01M
chk_10  0.50ms  0.50ms  0.64ms  1.29  0.20K  0.01M
chk_11  0.36ms  0.43ms  0.57ms  1.58  0.09K  0.02M
chk_12  0.45ms  0.46ms  0.60ms  1.35  0.16K  0.01M
chk_13  0.45ms  0.46ms  0.60ms  1.35  0.16K  0.01M
chk_14  0.44ms  0.46ms  0.60ms  1.37  0.14K  0.01M
chk_15  0.50ms  0.50ms  0.64ms  1.28  0.21K  0.01M
chk_16  0.46ms  0.50ms  0.64ms  1.38  0.18K  0.01M
chk_17  0.57ms  0.50ms  0.65ms  1.29  0.29K  0.01M
chk_18  0.58ms  0.50ms  0.65ms  1.29  0.31K  0.00M
chk_19  0.40ms  0.44ms  0.58ms  1.46  0.13K  0.01M
chk_20  0.40ms  0.44ms  0.58ms  1.44  0.13K  0.01M
chk_21  0.46ms  0.49ms  0.64ms  1.38  0.18K  0.01M
chk_22  0.40ms  0.43ms  0.57ms  1.44  0.13K  0.01M
chk_23  0.46ms  0.49ms  0.64ms  1.39  0.16K  0.01M
chk_24  0.36ms  0.43ms  0.57ms  1.58  0.09K  0.02M
chk_25  0.36ms  0.43ms  0.57ms  1.59  0.09K  0.02M
chk_26  0.46ms  0.49ms  0.64ms  1.38  0.18K  0.01M
chk_27  0.40ms  0.44ms  0.58ms  1.45  0.13K  0.01M
chk_28  0.46ms  0.49ms  0.64ms  1.38  0.17K  0.01M
chk_29  0.44ms  0.46ms  0.61ms  1.36  0.15K  0.01M
chk_30  0.52ms  0.50ms  0.64ms  1.29  0.24K  0.01M
chk_31  0.50ms  0.49ms  0.63ms  1.28  0.20K  0.01M
   Avg  0.45  0.47  0.61
   Max  0.58  0.51  0.65
   Min  0.35  0.43  0.56
 Ratio  1.65  1.18  1.15
   Var  0.00  0.00  0.00
Profiling takes 0.687 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 74.076 ms
Partition 0 [0, 5) has cost: 74.076 ms
Partition 1 [5, 9) has cost: 59.832 ms
Partition 2 [9, 13) has cost: 59.832 ms
Partition 3 [13, 17) has cost: 59.832 ms
Partition 4 [17, 21) has cost: 59.832 ms
Partition 5 [21, 25) has cost: 59.832 ms
Partition 6 [25, 29) has cost: 59.832 ms
Partition 7 [29, 33) has cost: 64.351 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 32.838 ms
GPU 0, Compute+Comm Time: 27.049 ms, Bubble Time: 5.462 ms, Imbalance Overhead: 0.328 ms
GPU 1, Compute+Comm Time: 22.701 ms, Bubble Time: 5.443 ms, Imbalance Overhead: 4.695 ms
GPU 2, Compute+Comm Time: 22.701 ms, Bubble Time: 5.506 ms, Imbalance Overhead: 4.631 ms
GPU 3, Compute+Comm Time: 22.701 ms, Bubble Time: 5.521 ms, Imbalance Overhead: 4.616 ms
GPU 4, Compute+Comm Time: 22.701 ms, Bubble Time: 5.631 ms, Imbalance Overhead: 4.507 ms
GPU 5, Compute+Comm Time: 22.701 ms, Bubble Time: 5.701 ms, Imbalance Overhead: 4.437 ms
GPU 6, Compute+Comm Time: 22.701 ms, Bubble Time: 5.825 ms, Imbalance Overhead: 4.312 ms
GPU 7, Compute+Comm Time: 23.969 ms, Bubble Time: 5.823 ms, Imbalance Overhead: 3.047 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 64.488 ms
GPU 0, Compute+Comm Time: 46.283 ms, Bubble Time: 11.331 ms, Imbalance Overhead: 6.874 ms
GPU 1, Compute+Comm Time: 43.032 ms, Bubble Time: 11.357 ms, Imbalance Overhead: 10.099 ms
GPU 2, Compute+Comm Time: 43.032 ms, Bubble Time: 11.075 ms, Imbalance Overhead: 10.382 ms
GPU 3, Compute+Comm Time: 43.032 ms, Bubble Time: 10.987 ms, Imbalance Overhead: 10.469 ms
GPU 4, Compute+Comm Time: 43.032 ms, Bubble Time: 10.754 ms, Imbalance Overhead: 10.703 ms
GPU 5, Compute+Comm Time: 43.032 ms, Bubble Time: 10.757 ms, Imbalance Overhead: 10.700 ms
GPU 6, Compute+Comm Time: 43.032 ms, Bubble Time: 10.663 ms, Imbalance Overhead: 10.794 ms
GPU 7, Compute+Comm Time: 52.928 ms, Bubble Time: 10.758 ms, Imbalance Overhead: 0.802 ms
The estimated cost of the whole pipeline: 102.193 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 133.908 ms
Partition 0 [0, 9) has cost: 133.908 ms
Partition 1 [9, 17) has cost: 119.665 ms
Partition 2 [17, 25) has cost: 119.665 ms
Partition 3 [25, 33) has cost: 124.184 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 33.743 ms
GPU 0, Compute+Comm Time: 27.990 ms, Bubble Time: 5.124 ms, Imbalance Overhead: 0.629 ms
GPU 1, Compute+Comm Time: 25.686 ms, Bubble Time: 5.181 ms, Imbalance Overhead: 2.877 ms
GPU 2, Compute+Comm Time: 25.686 ms, Bubble Time: 5.389 ms, Imbalance Overhead: 2.669 ms
GPU 3, Compute+Comm Time: 26.321 ms, Bubble Time: 5.612 ms, Imbalance Overhead: 1.810 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 63.617 ms
GPU 0, Compute+Comm Time: 48.893 ms, Bubble Time: 10.604 ms, Imbalance Overhead: 4.119 ms
GPU 1, Compute+Comm Time: 47.243 ms, Bubble Time: 10.144 ms, Imbalance Overhead: 6.229 ms
GPU 2, Compute+Comm Time: 47.243 ms, Bubble Time: 9.769 ms, Imbalance Overhead: 6.605 ms
GPU 3, Compute+Comm Time: 52.599 ms, Bubble Time: 9.653 ms, Imbalance Overhead: 1.365 ms
    The estimated cost with 2 DP ways is 102.228 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 253.573 ms
Partition 0 [0, 17) has cost: 253.573 ms
Partition 1 [17, 33) has cost: 243.848 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 45.175 ms
GPU 0, Compute+Comm Time: 39.344 ms, Bubble Time: 4.597 ms, Imbalance Overhead: 1.234 ms
GPU 1, Compute+Comm Time: 38.449 ms, Bubble Time: 4.844 ms, Imbalance Overhead: 1.882 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 72.744 ms
GPU 0, Compute+Comm Time: 61.991 ms, Bubble Time: 7.900 ms, Imbalance Overhead: 2.853 ms
GPU 1, Compute+Comm Time: 64.073 ms, Bubble Time: 7.529 ms, Imbalance Overhead: 1.142 ms
    The estimated cost with 4 DP ways is 123.814 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 497.421 ms
Partition 0 [0, 33) has cost: 497.421 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 131.994 ms
GPU 0, Compute+Comm Time: 131.994 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 156.346 ms
GPU 0, Compute+Comm Time: 156.346 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 302.757 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 5201
Node 4, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.7372	TrainAcc 0.1999	ValidAcc 0.2025	TestAcc 0.1979	BestValid 0.2025
	Epoch 50:	Loss 1.5053	TrainAcc 0.4848	ValidAcc 0.3053	TestAcc 0.3112	BestValid 0.3053
	Epoch 100:	Loss 1.2739	TrainAcc 0.5729	ValidAcc 0.3185	TestAcc 0.3026	BestValid 0.3185
	Epoch 150:	Loss 1.1565	TrainAcc 0.6647	ValidAcc 0.3257	TestAcc 0.3420	BestValid 0.3257
	Epoch 200:	Loss 1.1118	TrainAcc 0.7111	ValidAcc 0.3233	TestAcc 0.3449	BestValid 0.3257
	Epoch 250:	Loss 1.0227	TrainAcc 0.7328	ValidAcc 0.3263	TestAcc 0.3420	BestValid 0.3263
	Epoch 300:	Loss 1.0566	TrainAcc 0.7652	ValidAcc 0.3263	TestAcc 0.3362	BestValid 0.3263
	Epoch 350:	Loss 1.0470	TrainAcc 0.7712	ValidAcc 0.3179	TestAcc 0.3353	BestValid 0.3263
	Epoch 400:	Loss 0.9557	TrainAcc 0.7973	ValidAcc 0.3245	TestAcc 0.3410	BestValid 0.3263
	Epoch 450:	Loss 0.9027	TrainAcc 0.8073	ValidAcc 0.3341	TestAcc 0.3324	BestValid 0.3341
	Epoch 500:	Loss 0.8754	TrainAcc 0.8221	ValidAcc 0.3305	TestAcc 0.3410	BestValid 0.3341
	Epoch 550:	Loss 0.8490	TrainAcc 0.8365	ValidAcc 0.3323	TestAcc 0.3401	BestValid 0.3341
	Epoch 600:	Loss 0.8014	TrainAcc 0.8518	ValidAcc 0.3323	TestAcc 0.3458	BestValid 0.3341
	Epoch 650:	Loss 0.7923	TrainAcc 0.8486	ValidAcc 0.3371	TestAcc 0.3372	BestValid 0.3371
	Epoch 700:	Loss 0.8017	TrainAcc 0.8550	ValidAcc 0.3383	TestAcc 0.3372	BestValid 0.3383
	Epoch 750:	Loss 0.7799	TrainAcc 0.8750	ValidAcc 0.3431	TestAcc 0.3449	BestValid 0.3431
	Epoch 800:	Loss 0.7650	TrainAcc 0.8814	ValidAcc 0.3377	TestAcc 0.3420	BestValid 0.3431
	Epoch 850:	Loss 0.7575	TrainAcc 0.8902	ValidAcc 0.3395	TestAcc 0.3458	BestValid 0.3431
	Epoch 900:	Loss 0.7416	TrainAcc 0.8974	ValidAcc 0.3438	TestAcc 0.3468	BestValid 0.3438
	Epoch 950:	Loss 0.7160	TrainAcc 0.8878	ValidAcc 0.3371	TestAcc 0.3391	BestValid 0.3438
	Epoch 1000:	Loss 0.6895	TrainAcc 0.9018	ValidAcc 0.3377	TestAcc 0.3477	BestValid 0.3438
	Epoch 1050:	Loss 0.6804	TrainAcc 0.9083	ValidAcc 0.3413	TestAcc 0.3477	BestValid 0.3438
	Epoch 1100:	Loss 0.6885	TrainAcc 0.9191	ValidAcc 0.3498	TestAcc 0.3535	BestValid 0.3498
	Epoch 1150:	Loss 0.6344	TrainAcc 0.9183	ValidAcc 0.3431	TestAcc 0.3506	BestValid 0.3498
	Epoch 1200:	Loss 0.6205	TrainAcc 0.9315	ValidAcc 0.3450	TestAcc 0.3525	BestValid 0.3498
	Epoch 1250:	Loss 0.6257	TrainAcc 0.9311	ValidAcc 0.3413	TestAcc 0.3535	BestValid 0.3498
	Epoch 1300:	Loss 0.5779	TrainAcc 0.9311	ValidAcc 0.3419	TestAcc 0.3506	BestValid 0.3498
	Epoch 1350:	Loss 0.6175	TrainAcc 0.9419	ValidAcc 0.3450	TestAcc 0.3583	BestValid 0.3498
	Epoch 1400:	Loss 0.5870	TrainAcc 0.9447	ValidAcc 0.3468	TestAcc 0.3573	BestValid 0.3498
	Epoch 1450:	Loss 0.5770	TrainAcc 0.9455	ValidAcc 0.3468	TestAcc 0.3583	BestValid 0.3498
	Epoch 1500:	Loss 0.5612	TrainAcc 0.9467	ValidAcc 0.3450	TestAcc 0.3516	BestValid 0.3498
	Epoch 1550:	Loss 0.5528	TrainAcc 0.9495	ValidAcc 0.3450	TestAcc 0.3525	BestValid 0.3498
	Epoch 1600:	Loss 0.5052	TrainAcc 0.9511	ValidAcc 0.3516	TestAcc 0.3477	BestValid 0.3516
	Epoch 1650:	Loss 0.5952	TrainAcc 0.9583	ValidAcc 0.3540	TestAcc 0.3641	BestValid 0.3540
	Epoch 1700:	Loss 0.5177	TrainAcc 0.9567	ValidAcc 0.3462	TestAcc 0.3660	BestValid 0.3540
	Epoch 1750:	Loss 0.5005	TrainAcc 0.9619	ValidAcc 0.3528	TestAcc 0.3650	BestValid 0.3540
	Epoch 1800:	Loss 0.4716	TrainAcc 0.9627	ValidAcc 0.3474	TestAcc 0.3650	BestValid 0.3540
	Epoch 1850:	Loss 0.4623	TrainAcc 0.9647	ValidAcc 0.3576	TestAcc 0.3679	BestValid 0.3576
	Epoch 1900:	Loss 0.4484	TrainAcc 0.9639	ValidAcc 0.3588	TestAcc 0.3660	BestValid 0.3588
	Epoch 1950:	Loss 0.4862	TrainAcc 0.9651	ValidAcc 0.3528	TestAcc 0.3622	BestValid 0.3588
	Epoch 2000:	Loss 0.4375	TrainAcc 0.9696	ValidAcc 0.3582	TestAcc 0.3622	BestValid 0.3588
	Epoch 2050:	Loss 0.4531	TrainAcc 0.9704	ValidAcc 0.3564	TestAcc 0.3670	BestValid 0.3588
	Epoch 2100:	Loss 0.4254	TrainAcc 0.9708	ValidAcc 0.3564	TestAcc 0.3650	BestValid 0.3588
	Epoch 2150:	Loss 0.4051	TrainAcc 0.9696	ValidAcc 0.3582	TestAcc 0.3573	BestValid 0.3588
	Epoch 2200:	Loss 0.4184	TrainAcc 0.9716	ValidAcc 0.3594	TestAcc 0.3670	BestValid 0.3594
	Epoch 2250:	Loss 0.4050	TrainAcc 0.9756	ValidAcc 0.3552	TestAcc 0.3766	BestValid 0.3594
	Epoch 2300:	Loss 0.3946	TrainAcc 0.9768	ValidAcc 0.3570	TestAcc 0.3727	BestValid 0.3594
	Epoch 2350:	Loss 0.4071	TrainAcc 0.9764	ValidAcc 0.3624	TestAcc 0.3727	BestValid 0.3624
	Epoch 2400:	Loss 0.3730	TrainAcc 0.9764	ValidAcc 0.3618	TestAcc 0.3583	BestValid 0.3624
	Epoch 2450:	Loss 0.3683	TrainAcc 0.9728	ValidAcc 0.3582	TestAcc 0.3650	BestValid 0.3624
	Epoch 2500:	Loss 0.3551	TrainAcc 0.9780	ValidAcc 0.3618	TestAcc 0.3650	BestValid 0.3624
	Epoch 2550:	Loss 0.3941	TrainAcc 0.9800	ValidAcc 0.3582	TestAcc 0.3775	BestValid 0.3624
	Epoch 2600:	Loss 0.3466	TrainAcc 0.9772	ValidAcc 0.3648	TestAcc 0.3631	BestValid 0.3648
	Epoch 2650:	Loss 0.3447	TrainAcc 0.9736	ValidAcc 0.3534	TestAcc 0.3660	BestValid 0.3648
	Epoch 2700:	Loss 0.3292	TrainAcc 0.9812	ValidAcc 0.3618	TestAcc 0.3698	BestValid 0.3648
	Epoch 2750:	Loss 0.3510	TrainAcc 0.9808	ValidAcc 0.3648	TestAcc 0.3718	BestValid 0.3648
	Epoch 2800:	Loss 0.3213	TrainAcc 0.9828	ValidAcc 0.3624	TestAcc 0.3708	BestValid 0.3648
	Epoch 2850:	Loss 0.3293	TrainAcc 0.9816	ValidAcc 0.3540	TestAcc 0.3727	BestValid 0.3648
	Epoch 2900:	Loss 0.3571	TrainAcc 0.9824	ValidAcc 0.3624	TestAcc 0.3718	BestValid 0.3648
	Epoch 2950:	Loss 0.3200	TrainAcc 0.9800	ValidAcc 0.3648	TestAcc 0.3612	BestValid 0.3648
	Epoch 3000:	Loss 0.3725	TrainAcc 0.9824	ValidAcc 0.3636	TestAcc 0.3679	BestValid 0.3648
	Epoch 3050:	Loss 0.2982	TrainAcc 0.9836	ValidAcc 0.3552	TestAcc 0.3737	BestValid 0.3648
	Epoch 3100:	Loss 0.3229	TrainAcc 0.9840	ValidAcc 0.3654	TestAcc 0.3718	BestValid 0.3654
	Epoch 3150:	Loss 0.3127	TrainAcc 0.9832	ValidAcc 0.3720	TestAcc 0.3756	BestValid 0.3720
	Epoch 3200:	Loss 0.3233	TrainAcc 0.9840	ValidAcc 0.3612	TestAcc 0.3785	BestValid 0.3720
	Epoch 3250:	Loss 0.3086	TrainAcc 0.9860	ValidAcc 0.3618	TestAcc 0.3823	BestValid 0.3720
	Epoch 3300:	Loss 0.3189	TrainAcc 0.9836	ValidAcc 0.3624	TestAcc 0.3679	BestValid 0.3720
	Epoch 3350:	Loss 0.2771	TrainAcc 0.9852	ValidAcc 0.3666	TestAcc 0.3679	BestValid 0.3720
	Epoch 3400:	Loss 0.3144	TrainAcc 0.9844	ValidAcc 0.3660	TestAcc 0.3746	BestValid 0.3720
	Epoch 3450:	Loss 0.2825	TrainAcc 0.9856	ValidAcc 0.3618	TestAcc 0.3823	BestValid 0.3720
	Epoch 3500:	Loss 0.2666	TrainAcc 0.9836	ValidAcc 0.3642	TestAcc 0.3689	BestValid 0.3720
	Epoch 3550:	Loss 0.2395	TrainAcc 0.9888	ValidAcc 0.3606	TestAcc 0.3756	BestValid 0.3720
	Epoch 3600:	Loss 0.2836	TrainAcc 0.9824	ValidAcc 0.3648	TestAcc 0.3612	BestValid 0.3720
	Epoch 3650:	Loss 0.2503	TrainAcc 0.9848	ValidAcc 0.3642	TestAcc 0.3689	BestValid 0.3720
	Epoch 3700:	Loss 0.2497	TrainAcc 0.9872	ValidAcc 0.3612	TestAcc 0.3833	BestValid 0.3720
	Epoch 3750:	Loss 0.2423	TrainAcc 0.9872	ValidAcc 0.3624	TestAcc 0.3814	BestValid 0.3720
	Epoch 3800:	Loss 0.2448	TrainAcc 0.9856	ValidAcc 0.3600	TestAcc 0.3804	BestValid 0.3720
	Epoch 3850:	Loss 0.2760	TrainAcc 0.9868	ValidAcc 0.3624	TestAcc 0.3766	BestValid 0.3720
	Epoch 3900:	Loss 0.2466	TrainAcc 0.9856	ValidAcc 0.3630	TestAcc 0.3766	BestValid 0.3720
	Epoch 3950:	Loss 0.2500	TrainAcc 0.9884	ValidAcc 0.3630	TestAcc 0.3766	BestValid 0.3720
	Epoch 4000:	Loss 0.2373	TrainAcc 0.9872	ValidAcc 0.3612	TestAcc 0.3794	BestValid 0.3720
	Epoch 4050:	Loss 0.3498	TrainAcc 0.9884	ValidAcc 0.3672	TestAcc 0.3785	BestValid 0.3720
	Epoch 4100:	Loss 0.2814	TrainAcc 0.9876	ValidAcc 0.3684	TestAcc 0.3679	BestValid 0.3720
	Epoch 4150:	Loss 0.2241	TrainAcc 0.9880	ValidAcc 0.3654	TestAcc 0.3756	BestValid 0.3720
	Epoch 4200:	Loss 0.2424	TrainAcc 0.9880	ValidAcc 0.3678	TestAcc 0.3756	BestValid 0.3720
	Epoch 4250:	Loss 0.2346	TrainAcc 0.9888	ValidAcc 0.3672	TestAcc 0.3814	BestValid 0.3720
	Epoch 4300:	Loss 0.2701	TrainAcc 0.9908	ValidAcc 0.3678	TestAcc 0.3794	BestValid 0.3720
	Epoch 4350:	Loss 0.2158	TrainAcc 0.9908	ValidAcc 0.3702	TestAcc 0.3756	BestValid 0.3720
	Epoch 4400:	Loss 0.2217	TrainAcc 0.9876	ValidAcc 0.3666	TestAcc 0.3670	BestValid 0.3720
	Epoch 4450:	Loss 0.2079	TrainAcc 0.9872	ValidAcc 0.3636	TestAcc 0.3698	BestValid 0.3720
	Epoch 4500:	Loss 0.2211	TrainAcc 0.9888	ValidAcc 0.3654	TestAcc 0.3785	BestValid 0.3720
	Epoch 4550:	Loss 0.2107	TrainAcc 0.9888	ValidAcc 0.3678	TestAcc 0.3775	BestValid 0.3720
	Epoch 4600:	Loss 0.2501	TrainAcc 0.9868	ValidAcc 0.3678	TestAcc 0.3718	BestValid 0.3720
	Epoch 4650:	Loss 0.2099	TrainAcc 0.9904	ValidAcc 0.3690	TestAcc 0.3746	BestValid 0.3720
	Epoch 4700:	Loss 0.2068	TrainAcc 0.9916	ValidAcc 0.3756	TestAcc 0.3794	BestValid 0.3756
	Epoch 4750:	Loss 0.2062	TrainAcc 0.9848	ValidAcc 0.3672	TestAcc 0.3794	BestValid 0.3756
	Epoch 4800:	Loss 0.2266	TrainAcc 0.9892	ValidAcc 0.3690	TestAcc 0.3833	BestValid 0.3756
	Epoch 4850:	Loss 0.2069	TrainAcc 0.9892	ValidAcc 0.3684	TestAcc 0.3766	BestValid 0.3756
	Epoch 4900:	Loss 0.2000	TrainAcc 0.9888	ValidAcc 0.3786	TestAcc 0.3727	BestValid 0.3786
	Epoch 4950:	Loss 0.2063	TrainAcc 0.9888	ValidAcc 0.3774	TestAcc 0.3785	BestValid 0.3786
	Epoch 5000:	Loss 0.2217	TrainAcc 0.9860	ValidAcc 0.3684	TestAcc 0.3900	BestValid 0.3786
****** Epoch Time (Excluding Evaluation Cost): 0.108 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 18.436 ms (Max: 19.580, Min: 17.014, Sum: 147.488)
Cluster-Wide Average, Compute: 62.657 ms (Max: 76.190, Min: 58.772, Sum: 501.258)
Cluster-Wide Average, Communication-Layer: 11.578 ms (Max: 13.241, Min: 9.173, Sum: 92.622)
Cluster-Wide Average, Bubble-Imbalance: 12.894 ms (Max: 16.278, Min: 2.468, Sum: 103.152)
Cluster-Wide Average, Communication-Graph: 0.480 ms (Max: 0.533, Min: 0.426, Sum: 3.841)
Cluster-Wide Average, Optimization: 1.027 ms (Max: 1.462, Min: 0.951, Sum: 8.218)
Cluster-Wide Average, Others: 0.944 ms (Max: 1.793, Min: 0.647, Sum: 7.551)
****** Breakdown Sum: 108.016 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.080 GB (Max: 4.061, Min: 2.926, Sum: 24.640)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 49.556 Gbps (Max: 59.863, Min: 34.809, Sum: 396.445)
Layer-level communication (cluster-wide, per-epoch): 0.543 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.543 GB
****** Accuracy Results ******
Highest valid_acc: 0.3786
Target test_acc: 0.3727
Epoch to reach the target acc: 4899
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
