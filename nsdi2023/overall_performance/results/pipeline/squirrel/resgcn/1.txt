Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INITDONE MPI INIT
Initialized node 4 on machine gnerv8

Initialized node 7 on machine gnerv8
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 3 on machine gnerv4
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4

Initialized node 1 on machine gnerv4
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.018 seconds.
        It takes 0.018 seconds.
        It takes 0.018 seconds.
        It takes 0.018 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.096 seconds.
        It takes 0.097 seconds.
        It takes 0.097 seconds.
        It takes 0.097 seconds.
        It takes 0.098 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.003 seconds.
        It takes 0.003 seconds.
        It takes 0.099 seconds.
        It takes 0.003 seconds.
Building the Label Vector...        It takes 0.003 seconds.

The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.000 seconds.
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
5201, 401907, 401907
Number of vertices per chunk: 163
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.413 Gbps (per GPU), 451.307 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.144 Gbps (per GPU), 449.155 Gbps (aggregated)
The layer-level communication performance: 56.139 Gbps (per GPU), 449.113 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.926 Gbps (per GPU), 447.410 Gbps (aggregated)
The layer-level communication performance: 55.898 Gbps (per GPU), 447.183 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.703 Gbps (per GPU), 445.623 Gbps (aggregated)
The layer-level communication performance: 55.661 Gbps (per GPU), 445.287 Gbps (aggregated)
The layer-level communication performance: 55.629 Gbps (per GPU), 445.034 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.923 Gbps (per GPU), 1279.387 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.908 Gbps (per GPU), 1279.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.877 Gbps (per GPU), 1279.013 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.908 Gbps (per GPU), 1279.266 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.926 Gbps (per GPU), 1279.409 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.923 Gbps (per GPU), 1279.384 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.865 Gbps (per GPU), 1278.922 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.926 Gbps (per GPU), 1279.409 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.980 Gbps (per GPU), 815.840 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.978 Gbps (per GPU), 815.821 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.980 Gbps (per GPU), 815.840 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.977 Gbps (per GPU), 815.814 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.973 Gbps (per GPU), 815.788 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.981 Gbps (per GPU), 815.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.983 Gbps (per GPU), 815.867 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.981 Gbps (per GPU), 815.847 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.654 Gbps (per GPU), 301.235 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.656 Gbps (per GPU), 301.249 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.655 Gbps (per GPU), 301.241 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.656 Gbps (per GPU), 301.245 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.649 Gbps (per GPU), 301.193 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.652 Gbps (per GPU), 301.214 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.648 Gbps (per GPU), 301.180 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.650 Gbps (per GPU), 301.198 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.97ms  0.75ms  0.88ms  1.30  0.08K  0.02M
 chk_1  1.08ms  0.78ms  0.91ms  1.40  0.16K  0.01M
 chk_2  1.01ms  0.75ms  0.88ms  1.34  0.13K  0.01M
 chk_3  1.12ms  0.81ms  0.94ms  1.38  0.18K  0.01M
 chk_4  1.07ms  0.78ms  0.91ms  1.38  0.15K  0.01M
 chk_5  1.19ms  0.83ms  0.97ms  1.43  0.25K  0.01M
 chk_6  0.98ms  0.75ms  0.88ms  1.30  0.10K  0.02M
 chk_7  1.01ms  0.76ms  0.89ms  1.33  0.11K  0.02M
 chk_8  1.18ms  0.83ms  0.96ms  1.43  0.23K  0.01M
 chk_9  1.07ms  0.78ms  0.91ms  1.37  0.14K  0.01M
chk_10  1.16ms  0.82ms  0.95ms  1.43  0.20K  0.01M
chk_11  0.98ms  0.75ms  0.88ms  1.30  0.09K  0.02M
chk_12  1.08ms  0.78ms  0.91ms  1.38  0.16K  0.01M
chk_13  1.07ms  0.78ms  0.91ms  1.38  0.16K  0.01M
chk_14  1.07ms  0.78ms  0.90ms  1.37  0.14K  0.01M
chk_15  1.17ms  0.82ms  0.96ms  1.42  0.21K  0.01M
chk_16  1.12ms  0.81ms  0.94ms  1.39  0.18K  0.01M
chk_17  1.24ms  0.83ms  0.97ms  1.49  0.29K  0.01M
chk_18  1.25ms  0.83ms  0.97ms  1.50  0.31K  0.00M
chk_19  1.01ms  0.76ms  0.88ms  1.33  0.13K  0.01M
chk_20  1.01ms  0.76ms  0.88ms  1.33  0.13K  0.01M
chk_21  1.12ms  0.81ms  0.94ms  1.39  0.18K  0.01M
chk_22  1.01ms  0.76ms  0.88ms  1.33  0.13K  0.01M
chk_23  1.11ms  0.80ms  0.93ms  1.38  0.16K  0.01M
chk_24  0.97ms  0.75ms  0.88ms  1.29  0.09K  0.02M
chk_25  0.98ms  0.76ms  0.88ms  1.29  0.09K  0.02M
chk_26  1.12ms  0.81ms  0.94ms  1.39  0.18K  0.01M
chk_27  1.01ms  0.76ms  0.88ms  1.33  0.13K  0.01M
chk_28  1.12ms  0.80ms  0.94ms  1.39  0.17K  0.01M
chk_29  1.08ms  0.78ms  0.91ms  1.38  0.15K  0.01M
chk_30  1.18ms  0.83ms  0.96ms  1.42  0.24K  0.01M
chk_31  1.16ms  0.81ms  0.95ms  1.43  0.20K  0.01M
   Avg  1.08  0.79  0.92
   Max  1.25  0.83  0.97
   Min  0.97  0.75  0.88
 Ratio  1.28  1.11  1.11
   Var  0.01  0.00  0.00
Profiling takes 1.119 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 0 owns the model-level partition [0, 35)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 1, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 1 owns the model-level partition [35, 67)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 2, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 2 owns the model-level partition [67, 99)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 5, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 5 owns the model-level partition [163, 195)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [227, 262)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 5201
Node 4, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 4, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 4 owns the model-level partition [131, 163)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 6, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 6 owns the model-level partition [195, 227)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 3, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 3 owns the model-level partition [99, 131)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 35)...
+++++++++ Node 2 initializing the weights for op[67, 99)...
+++++++++ Node 3 initializing the weights for op[99, 131)...
+++++++++ Node 1 initializing the weights for op[35, 67)...
+++++++++ Node 4 initializing the weights for op[131, 163)...
+++++++++ Node 5 initializing the weights for op[163, 195)...
+++++++++ Node 6 initializing the weights for op[195, 227)...
+++++++++ Node 7 initializing the weights for op[227, 262)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2180	TrainAcc 0.2412	ValidAcc 0.2356	TestAcc 0.2152	BestValid 0.2356
	Epoch 50:	Loss 0.7057	TrainAcc 0.5457	ValidAcc 0.3383	TestAcc 0.3362	BestValid 0.3383
	Epoch 100:	Loss 0.3677	TrainAcc 0.4175	ValidAcc 0.2590	TestAcc 0.2805	BestValid 0.3383
	Epoch 150:	Loss 0.2318	TrainAcc 0.5349	ValidAcc 0.3335	TestAcc 0.3401	BestValid 0.3383
	Epoch 200:	Loss 0.1794	TrainAcc 0.6162	ValidAcc 0.3510	TestAcc 0.3497	BestValid 0.3510
	Epoch 250:	Loss 0.1241	TrainAcc 0.6474	ValidAcc 0.3666	TestAcc 0.3650	BestValid 0.3666
	Epoch 300:	Loss 0.1090	TrainAcc 0.6999	ValidAcc 0.3978	TestAcc 0.3842	BestValid 0.3978
	Epoch 350:	Loss 0.0725	TrainAcc 0.7692	ValidAcc 0.4195	TestAcc 0.3939	BestValid 0.4195
	Epoch 400:	Loss 0.0588	TrainAcc 0.7796	ValidAcc 0.4435	TestAcc 0.4265	BestValid 0.4435
	Epoch 450:	Loss 0.0494	TrainAcc 0.7648	ValidAcc 0.4225	TestAcc 0.4102	BestValid 0.4435
	Epoch 500:	Loss 0.0475	TrainAcc 0.8037	ValidAcc 0.4447	TestAcc 0.4419	BestValid 0.4447
	Epoch 550:	Loss 0.0420	TrainAcc 0.8289	ValidAcc 0.4477	TestAcc 0.4448	BestValid 0.4477
	Epoch 600:	Loss 0.0434	TrainAcc 0.8321	ValidAcc 0.4477	TestAcc 0.4390	BestValid 0.4477
	Epoch 650:	Loss 0.0572	TrainAcc 0.6907	ValidAcc 0.3558	TestAcc 0.3564	BestValid 0.4477
	Epoch 700:	Loss 0.0404	TrainAcc 0.6919	ValidAcc 0.3564	TestAcc 0.3554	BestValid 0.4477
	Epoch 750:	Loss 0.0377	TrainAcc 0.7256	ValidAcc 0.3624	TestAcc 0.3708	BestValid 0.4477
	Epoch 800:	Loss 0.0273	TrainAcc 0.7019	ValidAcc 0.3726	TestAcc 0.3718	BestValid 0.4477
	Epoch 850:	Loss 0.0216	TrainAcc 0.7484	ValidAcc 0.4056	TestAcc 0.4121	BestValid 0.4477
	Epoch 900:	Loss 0.0228	TrainAcc 0.7520	ValidAcc 0.4032	TestAcc 0.4150	BestValid 0.4477
	Epoch 950:	Loss 0.0244	TrainAcc 0.7873	ValidAcc 0.4123	TestAcc 0.4246	BestValid 0.4477
	Epoch 1000:	Loss 0.0257	TrainAcc 0.7584	ValidAcc 0.3942	TestAcc 0.3967	BestValid 0.4477
	Epoch 1050:	Loss 0.0206	TrainAcc 0.7360	ValidAcc 0.3954	TestAcc 0.4102	BestValid 0.4477
	Epoch 1100:	Loss 0.0119	TrainAcc 0.7965	ValidAcc 0.4267	TestAcc 0.4236	BestValid 0.4477
	Epoch 1150:	Loss 0.0190	TrainAcc 0.7736	ValidAcc 0.4056	TestAcc 0.4179	BestValid 0.4477
	Epoch 1200:	Loss 0.0203	TrainAcc 0.7829	ValidAcc 0.4117	TestAcc 0.4169	BestValid 0.4477
	Epoch 1250:	Loss 0.0199	TrainAcc 0.8165	ValidAcc 0.4303	TestAcc 0.4284	BestValid 0.4477
	Epoch 1300:	Loss 0.0127	TrainAcc 0.7664	ValidAcc 0.4056	TestAcc 0.4207	BestValid 0.4477
	Epoch 1350:	Loss 0.0177	TrainAcc 0.7692	ValidAcc 0.4093	TestAcc 0.4304	BestValid 0.4477
	Epoch 1400:	Loss 0.0132	TrainAcc 0.7556	ValidAcc 0.4062	TestAcc 0.4198	BestValid 0.4477
	Epoch 1450:	Loss 0.0179	TrainAcc 0.8141	ValidAcc 0.4429	TestAcc 0.4419	BestValid 0.4477
	Epoch 1500:	Loss 0.0136	TrainAcc 0.8129	ValidAcc 0.4273	TestAcc 0.4246	BestValid 0.4477
	Epoch 1550:	Loss 0.0125	TrainAcc 0.8013	ValidAcc 0.4345	TestAcc 0.4323	BestValid 0.4477
	Epoch 1600:	Loss 0.0108	TrainAcc 0.7917	ValidAcc 0.4219	TestAcc 0.4342	BestValid 0.4477
	Epoch 1650:	Loss 0.0121	TrainAcc 0.7688	ValidAcc 0.4261	TestAcc 0.4284	BestValid 0.4477
	Epoch 1700:	Loss 0.0100	TrainAcc 0.7861	ValidAcc 0.4219	TestAcc 0.4236	BestValid 0.4477
	Epoch 1750:	Loss 0.0089	TrainAcc 0.7965	ValidAcc 0.4267	TestAcc 0.4294	BestValid 0.4477
	Epoch 1800:	Loss 0.0079	TrainAcc 0.8069	ValidAcc 0.4357	TestAcc 0.4256	BestValid 0.4477
	Epoch 1850:	Loss 0.0122	TrainAcc 0.8053	ValidAcc 0.4357	TestAcc 0.4265	BestValid 0.4477
	Epoch 1900:	Loss 0.0061	TrainAcc 0.8013	ValidAcc 0.4333	TestAcc 0.4256	BestValid 0.4477
	Epoch 1950:	Loss 0.0098	TrainAcc 0.8141	ValidAcc 0.4381	TestAcc 0.4294	BestValid 0.4477
	Epoch 2000:	Loss 0.0071	TrainAcc 0.8077	ValidAcc 0.4255	TestAcc 0.4217	BestValid 0.4477
	Epoch 2050:	Loss 0.0101	TrainAcc 0.8273	ValidAcc 0.4459	TestAcc 0.4323	BestValid 0.4477
	Epoch 2100:	Loss 0.0103	TrainAcc 0.8213	ValidAcc 0.4393	TestAcc 0.4284	BestValid 0.4477
	Epoch 2150:	Loss 0.0121	TrainAcc 0.8141	ValidAcc 0.4399	TestAcc 0.4380	BestValid 0.4477
	Epoch 2200:	Loss 0.0092	TrainAcc 0.8057	ValidAcc 0.4339	TestAcc 0.4227	BestValid 0.4477
	Epoch 2250:	Loss 0.0107	TrainAcc 0.8237	ValidAcc 0.4489	TestAcc 0.4294	BestValid 0.4489
	Epoch 2300:	Loss 0.0100	TrainAcc 0.8073	ValidAcc 0.4453	TestAcc 0.4323	BestValid 0.4489
	Epoch 2350:	Loss 0.0070	TrainAcc 0.8177	ValidAcc 0.4339	TestAcc 0.4313	BestValid 0.4489
	Epoch 2400:	Loss 0.0054	TrainAcc 0.8085	ValidAcc 0.4405	TestAcc 0.4265	BestValid 0.4489
	Epoch 2450:	Loss 0.0058	TrainAcc 0.8289	ValidAcc 0.4441	TestAcc 0.4342	BestValid 0.4489
	Epoch 2500:	Loss 0.0049	TrainAcc 0.8438	ValidAcc 0.4447	TestAcc 0.4332	BestValid 0.4489
	Epoch 2550:	Loss 0.0046	TrainAcc 0.8385	ValidAcc 0.4405	TestAcc 0.4332	BestValid 0.4489
	Epoch 2600:	Loss 0.0119	TrainAcc 0.8325	ValidAcc 0.4429	TestAcc 0.4332	BestValid 0.4489
	Epoch 2650:	Loss 0.0086	TrainAcc 0.7620	ValidAcc 0.4129	TestAcc 0.4236	BestValid 0.4489
	Epoch 2700:	Loss 0.0050	TrainAcc 0.7592	ValidAcc 0.4129	TestAcc 0.4207	BestValid 0.4489
	Epoch 2750:	Loss 0.0044	TrainAcc 0.7945	ValidAcc 0.4285	TestAcc 0.4227	BestValid 0.4489
	Epoch 2800:	Loss 0.0056	TrainAcc 0.8129	ValidAcc 0.4333	TestAcc 0.4246	BestValid 0.4489
	Epoch 2850:	Loss 0.0047	TrainAcc 0.8277	ValidAcc 0.4363	TestAcc 0.4323	BestValid 0.4489
	Epoch 2900:	Loss 0.0095	TrainAcc 0.8273	ValidAcc 0.4477	TestAcc 0.4236	BestValid 0.4489
	Epoch 2950:	Loss 0.0088	TrainAcc 0.7985	ValidAcc 0.4309	TestAcc 0.4265	BestValid 0.4489
	Epoch 3000:	Loss 0.0048	TrainAcc 0.7917	ValidAcc 0.4315	TestAcc 0.4265	BestValid 0.4489
	Epoch 3050:	Loss 0.0082	TrainAcc 0.8173	ValidAcc 0.4411	TestAcc 0.4246	BestValid 0.4489
	Epoch 3100:	Loss 0.0099	TrainAcc 0.8273	ValidAcc 0.4453	TestAcc 0.4342	BestValid 0.4489
	Epoch 3150:	Loss 0.0047	TrainAcc 0.8069	ValidAcc 0.4333	TestAcc 0.4284	BestValid 0.4489
	Epoch 3200:	Loss 0.0138	TrainAcc 0.8169	ValidAcc 0.4291	TestAcc 0.4313	BestValid 0.4489
	Epoch 3250:	Loss 0.0064	TrainAcc 0.8389	ValidAcc 0.4399	TestAcc 0.4352	BestValid 0.4489
	Epoch 3300:	Loss 0.0083	TrainAcc 0.8454	ValidAcc 0.4333	TestAcc 0.4227	BestValid 0.4489
	Epoch 3350:	Loss 0.0047	TrainAcc 0.7825	ValidAcc 0.3972	TestAcc 0.3967	BestValid 0.4489
	Epoch 3400:	Loss 0.0046	TrainAcc 0.8674	ValidAcc 0.4567	TestAcc 0.4352	BestValid 0.4567
	Epoch 3450:	Loss 0.0071	TrainAcc 0.8670	ValidAcc 0.4489	TestAcc 0.4409	BestValid 0.4567
	Epoch 3500:	Loss 0.0092	TrainAcc 0.8770	ValidAcc 0.4561	TestAcc 0.4371	BestValid 0.4567
	Epoch 3550:	Loss 0.0064	TrainAcc 0.8373	ValidAcc 0.4519	TestAcc 0.4275	BestValid 0.4567
	Epoch 3600:	Loss 0.0043	TrainAcc 0.8690	ValidAcc 0.4561	TestAcc 0.4380	BestValid 0.4567
	Epoch 3650:	Loss 0.0053	TrainAcc 0.8758	ValidAcc 0.4441	TestAcc 0.4342	BestValid 0.4567
	Epoch 3700:	Loss 0.0080	TrainAcc 0.8782	ValidAcc 0.4609	TestAcc 0.4467	BestValid 0.4609
	Epoch 3750:	Loss 0.0047	TrainAcc 0.8498	ValidAcc 0.4621	TestAcc 0.4409	BestValid 0.4621
	Epoch 3800:	Loss 0.0040	TrainAcc 0.8570	ValidAcc 0.4543	TestAcc 0.4371	BestValid 0.4621
	Epoch 3850:	Loss 0.0039	TrainAcc 0.8257	ValidAcc 0.4285	TestAcc 0.4217	BestValid 0.4621
	Epoch 3900:	Loss 0.0044	TrainAcc 0.8173	ValidAcc 0.4345	TestAcc 0.4294	BestValid 0.4621
	Epoch 3950:	Loss 0.0030	TrainAcc 0.8005	ValidAcc 0.4129	TestAcc 0.4073	BestValid 0.4621
	Epoch 4000:	Loss 0.0066	TrainAcc 0.8121	ValidAcc 0.4219	TestAcc 0.4179	BestValid 0.4621
	Epoch 4050:	Loss 0.0028	TrainAcc 0.8409	ValidAcc 0.4465	TestAcc 0.4342	BestValid 0.4621
	Epoch 4100:	Loss 0.0048	TrainAcc 0.8093	ValidAcc 0.4207	TestAcc 0.4207	BestValid 0.4621
	Epoch 4150:	Loss 0.0029	TrainAcc 0.8353	ValidAcc 0.4453	TestAcc 0.4361	BestValid 0.4621
	Epoch 4200:	Loss 0.0034	TrainAcc 0.8618	ValidAcc 0.4459	TestAcc 0.4371	BestValid 0.4621
	Epoch 4250:	Loss 0.0027	TrainAcc 0.8906	ValidAcc 0.4688	TestAcc 0.4505	BestValid 0.4688
	Epoch 4300:	Loss 0.0021	TrainAcc 0.8802	ValidAcc 0.4543	TestAcc 0.4428	BestValid 0.4688
	Epoch 4350:	Loss 0.0066	TrainAcc 0.8433	ValidAcc 0.4429	TestAcc 0.4438	BestValid 0.4688
	Epoch 4400:	Loss 0.0068	TrainAcc 0.8181	ValidAcc 0.4189	TestAcc 0.4236	BestValid 0.4688
	Epoch 4450:	Loss 0.0045	TrainAcc 0.8369	ValidAcc 0.4369	TestAcc 0.4419	BestValid 0.4688
	Epoch 4500:	Loss 0.0042	TrainAcc 0.8289	ValidAcc 0.4297	TestAcc 0.4323	BestValid 0.4688
	Epoch 4550:	Loss 0.0037	TrainAcc 0.9006	ValidAcc 0.4681	TestAcc 0.4476	BestValid 0.4688
	Epoch 4600:	Loss 0.0034	TrainAcc 0.8381	ValidAcc 0.4453	TestAcc 0.4313	BestValid 0.4688
	Epoch 4650:	Loss 0.0046	TrainAcc 0.8265	ValidAcc 0.4303	TestAcc 0.4284	BestValid 0.4688
	Epoch 4700:	Loss 0.0021	TrainAcc 0.8421	ValidAcc 0.4363	TestAcc 0.4332	BestValid 0.4688
	Epoch 4750:	Loss 0.0021	TrainAcc 0.7925	ValidAcc 0.4026	TestAcc 0.3919	BestValid 0.4688
	Epoch 4800:	Loss 0.0055	TrainAcc 0.8417	ValidAcc 0.4213	TestAcc 0.4227	BestValid 0.4688
	Epoch 4850:	Loss 0.0021	TrainAcc 0.8121	ValidAcc 0.4267	TestAcc 0.4150	BestValid 0.4688
	Epoch 4900:	Loss 0.0068	TrainAcc 0.8225	ValidAcc 0.4291	TestAcc 0.4188	BestValid 0.4688
	Epoch 4950:	Loss 0.0059	TrainAcc 0.8365	ValidAcc 0.4315	TestAcc 0.4284	BestValid 0.4688
	Epoch 5000:	Loss 0.0017	TrainAcc 0.8241	ValidAcc 0.4213	TestAcc 0.4159	BestValid 0.4688
****** Epoch Time (Excluding Evaluation Cost): 0.148 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 25.625 ms (Max: 26.546, Min: 24.511, Sum: 205.002)
Cluster-Wide Average, Compute: 100.177 ms (Max: 111.311, Min: 96.172, Sum: 801.414)
Cluster-Wide Average, Communication-Layer: 7.257 ms (Max: 8.150, Min: 5.377, Sum: 58.054)
Cluster-Wide Average, Bubble-Imbalance: 11.836 ms (Max: 14.480, Min: 3.308, Sum: 94.691)
Cluster-Wide Average, Communication-Graph: 0.554 ms (Max: 0.670, Min: 0.489, Sum: 4.429)
Cluster-Wide Average, Optimization: 1.033 ms (Max: 1.463, Min: 0.963, Sum: 8.264)
Cluster-Wide Average, Others: 0.966 ms (Max: 1.853, Min: 0.657, Sum: 7.728)
****** Breakdown Sum: 147.448 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.014 GB (Max: 3.547, Min: 2.924, Sum: 24.112)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 39.429 Gbps (Max: 44.606, Min: 30.455, Sum: 315.431)
Layer-level communication (cluster-wide, per-epoch): 0.271 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.271 GB
****** Accuracy Results ******
Highest valid_acc: 0.4688
Target test_acc: 0.4505
Epoch to reach the target acc: 4249
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
