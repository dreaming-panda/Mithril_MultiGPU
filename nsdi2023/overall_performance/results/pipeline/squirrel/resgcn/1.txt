Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
DONE MPI INITDONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT

Initialized node 7 on machine gnerv8
Initialized node 6 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.006 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.006 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.016 seconds.
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 163
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.649 Gbps (per GPU), 485.195 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.344 Gbps (per GPU), 482.750 Gbps (aggregated)
The layer-level communication performance: 60.343 Gbps (per GPU), 482.745 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.082 Gbps (per GPU), 480.652 Gbps (aggregated)
The layer-level communication performance: 60.047 Gbps (per GPU), 480.379 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.829 Gbps (per GPU), 478.633 Gbps (aggregated)
The layer-level communication performance: 59.782 Gbps (per GPU), 478.253 Gbps (aggregated)
The layer-level communication performance: 59.749 Gbps (per GPU), 477.989 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.823 Gbps (per GPU), 1262.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.811 Gbps (per GPU), 1262.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.820 Gbps (per GPU), 1262.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.814 Gbps (per GPU), 1262.515 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.823 Gbps (per GPU), 1262.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.823 Gbps (per GPU), 1262.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.790 Gbps (per GPU), 1262.323 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 2): 154.526 Gbps (per GPU), 1236.209 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.191 Gbps (per GPU), 801.529 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.190 Gbps (per GPU), 801.523 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.186 Gbps (per GPU), 801.485 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.190 Gbps (per GPU), 801.523 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.186 Gbps (per GPU), 801.491 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.189 Gbps (per GPU), 801.510 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.184 Gbps (per GPU), 801.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.188 Gbps (per GPU), 801.504 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.088 Gbps (per GPU), 304.705 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.089 Gbps (per GPU), 304.710 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.087 Gbps (per GPU), 304.695 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.083 Gbps (per GPU), 304.664 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.087 Gbps (per GPU), 304.697 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.082 Gbps (per GPU), 304.655 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.077 Gbps (per GPU), 304.613 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.075 Gbps (per GPU), 304.601 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.44ms  0.73ms  1.27ms  2.89  0.08K  0.02M
 chk_1  0.55ms  0.77ms  1.35ms  2.47  0.16K  0.01M
 chk_2  0.47ms  0.73ms  1.29ms  2.72  0.13K  0.01M
 chk_3  0.57ms  0.81ms  1.39ms  2.45  0.18K  0.01M
 chk_4  0.52ms  0.76ms  1.34ms  2.58  0.15K  0.01M
 chk_5  0.60ms  0.83ms  1.41ms  2.34  0.25K  0.01M
 chk_6  0.45ms  0.73ms  1.28ms  2.83  0.10K  0.02M
 chk_7  0.48ms  0.74ms  1.29ms  2.71  0.11K  0.02M
 chk_8  0.59ms  0.82ms  1.41ms  2.37  0.23K  0.01M
 chk_9  0.51ms  0.76ms  1.33ms  2.59  0.14K  0.01M
chk_10  0.58ms  0.81ms  1.39ms  2.38  0.20K  0.01M
chk_11  0.45ms  0.73ms  1.28ms  2.84  0.09K  0.02M
chk_12  0.52ms  0.77ms  1.35ms  2.60  0.16K  0.01M
chk_13  0.52ms  0.77ms  1.35ms  2.60  0.16K  0.01M
chk_14  0.51ms  0.77ms  1.34ms  2.60  0.14K  0.01M
chk_15  0.59ms  0.82ms  1.40ms  2.39  0.21K  0.01M
chk_16  0.57ms  0.81ms  1.39ms  2.46  0.18K  0.01M
chk_17  0.63ms  0.83ms  1.44ms  2.30  0.29K  0.01M
chk_18  0.63ms  0.83ms  1.45ms  2.30  0.31K  0.00M
chk_19  0.47ms  0.72ms  1.30ms  2.76  0.13K  0.01M
chk_20  0.47ms  0.73ms  1.30ms  2.74  0.13K  0.01M
chk_21  0.56ms  0.80ms  1.39ms  2.47  0.18K  0.01M
chk_22  0.47ms  0.72ms  1.29ms  2.74  0.13K  0.01M
chk_23  0.56ms  0.80ms  1.38ms  2.47  0.16K  0.01M
chk_24  0.45ms  0.73ms  1.28ms  2.84  0.09K  0.02M
chk_25  0.45ms  0.73ms  1.28ms  2.83  0.09K  0.02M
chk_26  0.56ms  0.80ms  1.39ms  2.46  0.18K  0.01M
chk_27  0.47ms  0.73ms  1.30ms  2.75  0.13K  0.01M
chk_28  0.56ms  0.80ms  1.38ms  2.48  0.17K  0.01M
chk_29  0.52ms  0.76ms  1.35ms  2.60  0.15K  0.01M
chk_30  0.60ms  0.82ms  1.42ms  2.36  0.24K  0.01M
chk_31  0.58ms  0.80ms  1.39ms  2.39  0.20K  0.01M
   Avg  0.53  0.77  1.35
   Max  0.63  0.83  1.45
   Min  0.44  0.72  1.27
 Ratio  1.43  1.15  1.14
   Var  0.00  0.00  0.00
Profiling takes 1.068 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 27)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [27, 55)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [55, 83)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [83, 111)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [139, 167)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [167, 195)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [195, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 5201
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [111, 139)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[167, 195)...
+++++++++ Node 7 initializing the weights for op[195, 229)...
+++++++++ Node 5 initializing the weights for op[139, 167)...
+++++++++ Node 0 initializing the weights for op[0, 27)...
+++++++++ Node 4 initializing the weights for op[111, 139)...
+++++++++ Node 3 initializing the weights for op[83, 111)...
+++++++++ Node 2 initializing the weights for op[55, 83)...
+++++++++ Node 1 initializing the weights for op[27, 55)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.0137	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 1.7360	TrainAcc 0.2043	ValidAcc 0.2025	TestAcc 0.1979	BestValid 0.2025
	Epoch 100:	Loss 1.6923	TrainAcc 0.2396	ValidAcc 0.2200	TestAcc 0.2382	BestValid 0.2200
	Epoch 150:	Loss 1.6341	TrainAcc 0.2532	ValidAcc 0.2440	TestAcc 0.2517	BestValid 0.2440
	Epoch 200:	Loss 1.5987	TrainAcc 0.2612	ValidAcc 0.2656	TestAcc 0.2747	BestValid 0.2656
	Epoch 250:	Loss 1.5730	TrainAcc 0.2668	ValidAcc 0.2788	TestAcc 0.2834	BestValid 0.2788
	Epoch 300:	Loss 1.5576	TrainAcc 0.2768	ValidAcc 0.2740	TestAcc 0.2824	BestValid 0.2788
	Epoch 350:	Loss 1.4602	TrainAcc 0.2845	ValidAcc 0.2650	TestAcc 0.2872	BestValid 0.2788
	Epoch 400:	Loss 1.4228	TrainAcc 0.3257	ValidAcc 0.2891	TestAcc 0.2968	BestValid 0.2891
	Epoch 450:	Loss 1.3339	TrainAcc 0.3594	ValidAcc 0.2975	TestAcc 0.3045	BestValid 0.2975
	Epoch 500:	Loss 1.2619	TrainAcc 0.4655	ValidAcc 0.3624	TestAcc 0.3746	BestValid 0.3624
	Epoch 550:	Loss 1.2073	TrainAcc 0.4932	ValidAcc 0.3840	TestAcc 0.3842	BestValid 0.3840
	Epoch 600:	Loss 1.3016	TrainAcc 0.4692	ValidAcc 0.3756	TestAcc 0.3823	BestValid 0.3840
	Epoch 650:	Loss 1.1221	TrainAcc 0.5012	ValidAcc 0.3756	TestAcc 0.3890	BestValid 0.3840
	Epoch 700:	Loss 1.0660	TrainAcc 0.5429	ValidAcc 0.3810	TestAcc 0.4044	BestValid 0.3840
	Epoch 750:	Loss 1.0041	TrainAcc 0.5821	ValidAcc 0.4189	TestAcc 0.4313	BestValid 0.4189
	Epoch 800:	Loss 0.9262	TrainAcc 0.6058	ValidAcc 0.4285	TestAcc 0.4496	BestValid 0.4285
	Epoch 850:	Loss 0.8669	TrainAcc 0.6542	ValidAcc 0.4297	TestAcc 0.4717	BestValid 0.4297
	Epoch 900:	Loss 0.8192	TrainAcc 0.6026	ValidAcc 0.4141	TestAcc 0.4380	BestValid 0.4297
	Epoch 950:	Loss 0.7459	TrainAcc 0.6991	ValidAcc 0.4465	TestAcc 0.4909	BestValid 0.4465
	Epoch 1000:	Loss 0.7305	TrainAcc 0.7304	ValidAcc 0.4778	TestAcc 0.4957	BestValid 0.4778
	Epoch 1050:	Loss 0.6954	TrainAcc 0.6971	ValidAcc 0.4417	TestAcc 0.4870	BestValid 0.4778
	Epoch 1100:	Loss 0.6401	TrainAcc 0.7468	ValidAcc 0.4820	TestAcc 0.4947	BestValid 0.4820
	Epoch 1150:	Loss 0.6121	TrainAcc 0.7376	ValidAcc 0.4537	TestAcc 0.4803	BestValid 0.4820
	Epoch 1200:	Loss 0.6004	TrainAcc 0.7656	ValidAcc 0.4718	TestAcc 0.5082	BestValid 0.4820
	Epoch 1250:	Loss 0.5534	TrainAcc 0.7969	ValidAcc 0.4952	TestAcc 0.5130	BestValid 0.4952
	Epoch 1300:	Loss 0.5122	TrainAcc 0.8241	ValidAcc 0.4934	TestAcc 0.5360	BestValid 0.4952
	Epoch 1350:	Loss 0.5532	TrainAcc 0.7945	ValidAcc 0.4868	TestAcc 0.5130	BestValid 0.4952
	Epoch 1400:	Loss 0.4681	TrainAcc 0.8185	ValidAcc 0.4898	TestAcc 0.5197	BestValid 0.4952
	Epoch 1450:	Loss 0.4910	TrainAcc 0.8329	ValidAcc 0.5024	TestAcc 0.5274	BestValid 0.5024
	Epoch 1500:	Loss 0.4637	TrainAcc 0.8345	ValidAcc 0.5018	TestAcc 0.5351	BestValid 0.5024
	Epoch 1550:	Loss 0.4530	TrainAcc 0.8590	ValidAcc 0.5138	TestAcc 0.5351	BestValid 0.5138
	Epoch 1600:	Loss 0.3925	TrainAcc 0.8586	ValidAcc 0.5036	TestAcc 0.5418	BestValid 0.5138
	Epoch 1650:	Loss 0.3716	TrainAcc 0.8365	ValidAcc 0.4868	TestAcc 0.5043	BestValid 0.5138
	Epoch 1700:	Loss 0.3646	TrainAcc 0.8658	ValidAcc 0.5102	TestAcc 0.5370	BestValid 0.5138
	Epoch 1750:	Loss 0.3730	TrainAcc 0.8117	ValidAcc 0.4838	TestAcc 0.5034	BestValid 0.5138
	Epoch 1800:	Loss 0.3828	TrainAcc 0.8245	ValidAcc 0.4832	TestAcc 0.5120	BestValid 0.5138
	Epoch 1850:	Loss 0.3275	TrainAcc 0.8706	ValidAcc 0.5084	TestAcc 0.5303	BestValid 0.5138
	Epoch 1900:	Loss 0.3277	TrainAcc 0.8838	ValidAcc 0.5258	TestAcc 0.5370	BestValid 0.5258
	Epoch 1950:	Loss 0.3038	TrainAcc 0.9127	ValidAcc 0.5367	TestAcc 0.5543	BestValid 0.5367
	Epoch 2000:	Loss 0.3550	TrainAcc 0.8910	ValidAcc 0.5222	TestAcc 0.5418	BestValid 0.5367
	Epoch 2050:	Loss 0.3026	TrainAcc 0.8986	ValidAcc 0.5108	TestAcc 0.5264	BestValid 0.5367
	Epoch 2100:	Loss 0.2730	TrainAcc 0.8978	ValidAcc 0.5264	TestAcc 0.5341	BestValid 0.5367
	Epoch 2150:	Loss 0.2856	TrainAcc 0.8910	ValidAcc 0.5198	TestAcc 0.5207	BestValid 0.5367
	Epoch 2200:	Loss 0.2716	TrainAcc 0.9227	ValidAcc 0.5294	TestAcc 0.5351	BestValid 0.5367
	Epoch 2250:	Loss 0.2625	TrainAcc 0.9155	ValidAcc 0.5306	TestAcc 0.5322	BestValid 0.5367
	Epoch 2300:	Loss 0.2484	TrainAcc 0.9151	ValidAcc 0.5210	TestAcc 0.5235	BestValid 0.5367
	Epoch 2350:	Loss 0.2742	TrainAcc 0.8774	ValidAcc 0.5072	TestAcc 0.5091	BestValid 0.5367
	Epoch 2400:	Loss 0.2513	TrainAcc 0.9058	ValidAcc 0.5156	TestAcc 0.5149	BestValid 0.5367
	Epoch 2450:	Loss 0.2229	TrainAcc 0.8922	ValidAcc 0.5126	TestAcc 0.5274	BestValid 0.5367
	Epoch 2500:	Loss 0.2165	TrainAcc 0.9147	ValidAcc 0.5294	TestAcc 0.5178	BestValid 0.5367
	Epoch 2550:	Loss 0.2256	TrainAcc 0.9227	ValidAcc 0.5355	TestAcc 0.5370	BestValid 0.5367
	Epoch 2600:	Loss 0.2622	TrainAcc 0.9211	ValidAcc 0.5288	TestAcc 0.5331	BestValid 0.5367
	Epoch 2650:	Loss 0.2061	TrainAcc 0.9203	ValidAcc 0.5294	TestAcc 0.5322	BestValid 0.5367
	Epoch 2700:	Loss 0.2038	TrainAcc 0.9363	ValidAcc 0.5379	TestAcc 0.5351	BestValid 0.5379
	Epoch 2750:	Loss 0.2420	TrainAcc 0.8914	ValidAcc 0.5132	TestAcc 0.5207	BestValid 0.5379
	Epoch 2800:	Loss 0.1990	TrainAcc 0.9455	ValidAcc 0.5475	TestAcc 0.5437	BestValid 0.5475
	Epoch 2850:	Loss 0.1823	TrainAcc 0.9387	ValidAcc 0.5385	TestAcc 0.5312	BestValid 0.5475
	Epoch 2900:	Loss 0.1880	TrainAcc 0.9307	ValidAcc 0.5216	TestAcc 0.5331	BestValid 0.5475
	Epoch 2950:	Loss 0.1758	TrainAcc 0.9207	ValidAcc 0.5150	TestAcc 0.5245	BestValid 0.5475
	Epoch 3000:	Loss 0.2095	TrainAcc 0.8918	ValidAcc 0.5072	TestAcc 0.5005	BestValid 0.5475
	Epoch 3050:	Loss 0.1623	TrainAcc 0.9427	ValidAcc 0.5415	TestAcc 0.5341	BestValid 0.5475
	Epoch 3100:	Loss 0.1586	TrainAcc 0.9455	ValidAcc 0.5355	TestAcc 0.5456	BestValid 0.5475
	Epoch 3150:	Loss 0.1489	TrainAcc 0.9463	ValidAcc 0.5300	TestAcc 0.5427	BestValid 0.5475
	Epoch 3200:	Loss 0.1632	TrainAcc 0.9335	ValidAcc 0.5379	TestAcc 0.5447	BestValid 0.5475
	Epoch 3250:	Loss 0.1701	TrainAcc 0.9391	ValidAcc 0.5391	TestAcc 0.5427	BestValid 0.5475
	Epoch 3300:	Loss 0.1622	TrainAcc 0.9503	ValidAcc 0.5439	TestAcc 0.5533	BestValid 0.5475
	Epoch 3350:	Loss 0.1349	TrainAcc 0.9627	ValidAcc 0.5547	TestAcc 0.5610	BestValid 0.5547
	Epoch 3400:	Loss 0.1348	TrainAcc 0.9515	ValidAcc 0.5379	TestAcc 0.5399	BestValid 0.5547
	Epoch 3450:	Loss 0.1197	TrainAcc 0.9635	ValidAcc 0.5451	TestAcc 0.5533	BestValid 0.5547
	Epoch 3500:	Loss 0.2093	TrainAcc 0.9299	ValidAcc 0.5204	TestAcc 0.5226	BestValid 0.5547
	Epoch 3550:	Loss 0.1362	TrainAcc 0.9543	ValidAcc 0.5367	TestAcc 0.5456	BestValid 0.5547
	Epoch 3600:	Loss 0.1391	TrainAcc 0.9595	ValidAcc 0.5385	TestAcc 0.5370	BestValid 0.5547
	Epoch 3650:	Loss 0.1546	TrainAcc 0.9331	ValidAcc 0.5186	TestAcc 0.5159	BestValid 0.5547
	Epoch 3700:	Loss 0.1172	TrainAcc 0.9571	ValidAcc 0.5475	TestAcc 0.5495	BestValid 0.5547
	Epoch 3750:	Loss 0.1223	TrainAcc 0.9495	ValidAcc 0.5493	TestAcc 0.5379	BestValid 0.5547
	Epoch 3800:	Loss 0.1281	TrainAcc 0.9551	ValidAcc 0.5331	TestAcc 0.5418	BestValid 0.5547
	Epoch 3850:	Loss 0.1199	TrainAcc 0.9623	ValidAcc 0.5505	TestAcc 0.5543	BestValid 0.5547
	Epoch 3900:	Loss 0.1233	TrainAcc 0.9531	ValidAcc 0.5331	TestAcc 0.5379	BestValid 0.5547
	Epoch 3950:	Loss 0.1109	TrainAcc 0.9627	ValidAcc 0.5439	TestAcc 0.5504	BestValid 0.5547
	Epoch 4000:	Loss 0.1028	TrainAcc 0.9547	ValidAcc 0.5469	TestAcc 0.5418	BestValid 0.5547
	Epoch 4050:	Loss 0.1134	TrainAcc 0.9111	ValidAcc 0.4976	TestAcc 0.5091	BestValid 0.5547
	Epoch 4100:	Loss 0.1072	TrainAcc 0.9459	ValidAcc 0.5397	TestAcc 0.5427	BestValid 0.5547
	Epoch 4150:	Loss 0.1065	TrainAcc 0.9535	ValidAcc 0.5469	TestAcc 0.5581	BestValid 0.5547
	Epoch 4200:	Loss 0.1051	TrainAcc 0.9555	ValidAcc 0.5433	TestAcc 0.5533	BestValid 0.5547
	Epoch 4250:	Loss 0.1009	TrainAcc 0.9736	ValidAcc 0.5457	TestAcc 0.5524	BestValid 0.5547
	Epoch 4300:	Loss 0.0919	TrainAcc 0.9535	ValidAcc 0.5246	TestAcc 0.5437	BestValid 0.5547
	Epoch 4350:	Loss 0.0982	TrainAcc 0.9700	ValidAcc 0.5355	TestAcc 0.5437	BestValid 0.5547
	Epoch 4400:	Loss 0.0922	TrainAcc 0.9563	ValidAcc 0.5228	TestAcc 0.5399	BestValid 0.5547
	Epoch 4450:	Loss 0.1158	TrainAcc 0.9515	ValidAcc 0.5258	TestAcc 0.5456	BestValid 0.5547
	Epoch 4500:	Loss 0.1043	TrainAcc 0.9732	ValidAcc 0.5493	TestAcc 0.5543	BestValid 0.5547
	Epoch 4550:	Loss 0.1065	TrainAcc 0.9407	ValidAcc 0.5162	TestAcc 0.5255	BestValid 0.5547
	Epoch 4600:	Loss 0.0873	TrainAcc 0.9531	ValidAcc 0.5246	TestAcc 0.5476	BestValid 0.5547
	Epoch 4650:	Loss 0.0949	TrainAcc 0.9575	ValidAcc 0.5415	TestAcc 0.5591	BestValid 0.5547
	Epoch 4700:	Loss 0.0861	TrainAcc 0.9744	ValidAcc 0.5469	TestAcc 0.5504	BestValid 0.5547
	Epoch 4750:	Loss 0.0787	TrainAcc 0.9563	ValidAcc 0.5198	TestAcc 0.5360	BestValid 0.5547
	Epoch 4800:	Loss 0.0826	TrainAcc 0.9724	ValidAcc 0.5511	TestAcc 0.5610	BestValid 0.5547
	Epoch 4850:	Loss 0.0917	TrainAcc 0.9748	ValidAcc 0.5409	TestAcc 0.5572	BestValid 0.5547
	Epoch 4900:	Loss 0.0806	TrainAcc 0.9800	ValidAcc 0.5487	TestAcc 0.5514	BestValid 0.5547
	Epoch 4950:	Loss 0.0854	TrainAcc 0.9740	ValidAcc 0.5457	TestAcc 0.5629	BestValid 0.5547
	Epoch 5000:	Loss 0.0754	TrainAcc 0.9788	ValidAcc 0.5511	TestAcc 0.5514	BestValid 0.5547
****** Epoch Time (Excluding Evaluation Cost): 0.142 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 24.450 ms (Max: 25.660, Min: 22.338, Sum: 195.599)
Cluster-Wide Average, Compute: 92.465 ms (Max: 109.065, Min: 88.868, Sum: 739.716)
Cluster-Wide Average, Communication-Layer: 7.279 ms (Max: 8.156, Min: 5.434, Sum: 58.236)
Cluster-Wide Average, Bubble-Imbalance: 15.243 ms (Max: 18.874, Min: 1.680, Sum: 121.942)
Cluster-Wide Average, Communication-Graph: 0.553 ms (Max: 0.602, Min: 0.504, Sum: 4.421)
Cluster-Wide Average, Optimization: 1.036 ms (Max: 1.465, Min: 0.965, Sum: 8.287)
Cluster-Wide Average, Others: 1.038 ms (Max: 2.015, Min: 0.660, Sum: 8.301)
****** Breakdown Sum: 142.063 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.098 GB (Max: 3.553, Min: 2.979, Sum: 24.780)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 39.304 Gbps (Max: 44.639, Min: 30.303, Sum: 314.431)
Layer-level communication (cluster-wide, per-epoch): 0.271 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.271 GB
****** Accuracy Results ******
Highest valid_acc: 0.5547
Target test_acc: 0.5610
Epoch to reach the target acc: 3349
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
