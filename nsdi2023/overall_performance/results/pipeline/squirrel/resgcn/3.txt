Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv8
Initialized node 4 on machine gnerv8

DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 7 on machine gnerv8
Initialized node 1 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.011 seconds.
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.012 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.013 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.029 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
5201, 401907, 401907
Number of vertices per chunk: 163
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 61.177 Gbps (per GPU), 489.412 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.855 Gbps (per GPU), 486.837 Gbps (aggregated)
The layer-level communication performance: 60.847 Gbps (per GPU), 486.777 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.581 Gbps (per GPU), 484.646 Gbps (aggregated)
The layer-level communication performance: 60.541 Gbps (per GPU), 484.329 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.311 Gbps (per GPU), 482.490 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.261 Gbps (per GPU), 482.090 Gbps (aggregated)
The layer-level communication performance: 60.226 Gbps (per GPU), 481.812 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.556 Gbps (per GPU), 1260.450 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.556 Gbps (per GPU), 1260.450 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.568 Gbps (per GPU), 1260.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.524 Gbps (per GPU), 1260.189 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.574 Gbps (per GPU), 1260.592 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.547 Gbps (per GPU), 1260.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.568 Gbps (per GPU), 1260.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.565 Gbps (per GPU), 1260.519 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.080 Gbps (per GPU), 832.644 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.079 Gbps (per GPU), 832.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.079 Gbps (per GPU), 832.631 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.079 Gbps (per GPU), 832.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.080 Gbps (per GPU), 832.644 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.080 Gbps (per GPU), 832.637 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.077 Gbps (per GPU), 832.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.078 Gbps (per GPU), 832.623 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.780 Gbps (per GPU), 302.241 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.779 Gbps (per GPU), 302.229 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.780 Gbps (per GPU), 302.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.774 Gbps (per GPU), 302.196 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.771 Gbps (per GPU), 302.171 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.771 Gbps (per GPU), 302.171 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.772 Gbps (per GPU), 302.176 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.767 Gbps (per GPU), 302.136 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.45ms  0.73ms  1.27ms  2.86  0.08K  0.02M
 chk_1  0.54ms  0.77ms  1.35ms  2.48  0.16K  0.01M
 chk_2  0.48ms  0.73ms  1.29ms  2.70  0.13K  0.01M
 chk_3  0.57ms  0.80ms  1.38ms  2.41  0.18K  0.01M
 chk_4  0.53ms  0.76ms  1.35ms  2.54  0.15K  0.01M
 chk_5  0.61ms  0.82ms  1.42ms  2.30  0.25K  0.01M
 chk_6  0.45ms  0.73ms  1.28ms  2.83  0.10K  0.02M
 chk_7  0.48ms  0.74ms  1.29ms  2.71  0.11K  0.02M
 chk_8  0.59ms  0.82ms  1.41ms  2.38  0.23K  0.01M
 chk_9  0.51ms  0.76ms  1.34ms  2.61  0.14K  0.01M
chk_10  0.59ms  0.81ms  1.38ms  2.36  0.20K  0.01M
chk_11  0.45ms  0.73ms  1.28ms  2.83  0.09K  0.02M
chk_12  0.52ms  0.77ms  1.35ms  2.60  0.16K  0.01M
chk_13  0.52ms  0.77ms  1.34ms  2.58  0.16K  0.01M
chk_14  0.52ms  0.76ms  1.34ms  2.59  0.14K  0.01M
chk_15  0.59ms  0.82ms  1.40ms  2.38  0.21K  0.01M
chk_16  0.57ms  0.80ms  1.39ms  2.45  0.18K  0.01M
chk_17  0.63ms  0.82ms  1.44ms  2.29  0.29K  0.01M
chk_18  0.63ms  0.83ms  1.45ms  2.29  0.31K  0.00M
chk_19  0.47ms  0.73ms  1.29ms  2.75  0.13K  0.01M
chk_20  0.47ms  0.73ms  1.30ms  2.75  0.13K  0.01M
chk_21  0.56ms  0.80ms  1.38ms  2.45  0.18K  0.01M
chk_22  0.47ms  0.73ms  1.29ms  2.75  0.13K  0.01M
chk_23  0.56ms  0.80ms  1.38ms  2.47  0.16K  0.01M
chk_24  0.45ms  0.72ms  1.28ms  2.85  0.09K  0.02M
chk_25  0.45ms  0.73ms  1.28ms  2.82  0.09K  0.02M
chk_26  0.56ms  0.80ms  1.38ms  2.46  0.18K  0.01M
chk_27  0.47ms  0.73ms  1.29ms  2.74  0.13K  0.01M
chk_28  0.56ms  0.80ms  1.38ms  2.46  0.17K  0.01M
chk_29  0.52ms  0.76ms  1.34ms  2.60  0.15K  0.01M
chk_30  0.60ms  0.82ms  1.42ms  2.37  0.24K  0.01M
chk_31  0.58ms  0.80ms  1.38ms  2.39  0.20K  0.01M
   Avg  0.53  0.77  1.35
   Max  0.63  0.83  1.45
   Min  0.45  0.72  1.27
 Ratio  1.42  1.15  1.14
   Var  0.00  0.00  0.00
Profiling takes 1.068 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 27)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [27, 55)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 3, starting model training...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [139, 167)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 5201
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [55, 83)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [167, 195)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 5201
Num Stages: 8 / 8
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [195, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 5201
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [111, 139)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 5201
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [83, 111)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 27)...
+++++++++ Node 1 initializing the weights for op[27, 55)...
+++++++++ Node 4 initializing the weights for op[111, 139)...
+++++++++ Node 5 initializing the weights for op[139, 167)...
+++++++++ Node 7 initializing the weights for op[195, 229)...
+++++++++ Node 2 initializing the weights for op[55, 83)...
+++++++++ Node 3 initializing the weights for op[83, 111)...
+++++++++ Node 6 initializing the weights for op[167, 195)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.0206	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2055
	Epoch 50:	Loss 1.7451	TrainAcc 0.1979	ValidAcc 0.2043	TestAcc 0.1979	BestValid 0.2055
	Epoch 100:	Loss 1.6803	TrainAcc 0.2536	ValidAcc 0.2338	TestAcc 0.2498	BestValid 0.2338
	Epoch 150:	Loss 1.6233	TrainAcc 0.2644	ValidAcc 0.2644	TestAcc 0.2728	BestValid 0.2644
	Epoch 200:	Loss 1.5766	TrainAcc 0.2612	ValidAcc 0.2590	TestAcc 0.2709	BestValid 0.2644
	Epoch 250:	Loss 1.5358	TrainAcc 0.2532	ValidAcc 0.2512	TestAcc 0.2757	BestValid 0.2644
	Epoch 300:	Loss 1.4832	TrainAcc 0.3149	ValidAcc 0.2903	TestAcc 0.2988	BestValid 0.2903
	Epoch 350:	Loss 1.3697	TrainAcc 0.4107	ValidAcc 0.3311	TestAcc 0.3247	BestValid 0.3311
	Epoch 400:	Loss 1.3205	TrainAcc 0.4355	ValidAcc 0.3438	TestAcc 0.3554	BestValid 0.3438
	Epoch 450:	Loss 1.2501	TrainAcc 0.4679	ValidAcc 0.3606	TestAcc 0.3708	BestValid 0.3606
	Epoch 500:	Loss 1.1986	TrainAcc 0.4355	ValidAcc 0.3299	TestAcc 0.3324	BestValid 0.3606
	Epoch 550:	Loss 1.1452	TrainAcc 0.5064	ValidAcc 0.3618	TestAcc 0.3842	BestValid 0.3618
	Epoch 600:	Loss 1.1953	TrainAcc 0.4343	ValidAcc 0.3462	TestAcc 0.3775	BestValid 0.3618
	Epoch 650:	Loss 1.0739	TrainAcc 0.5304	ValidAcc 0.3642	TestAcc 0.4035	BestValid 0.3642
	Epoch 700:	Loss 0.9715	TrainAcc 0.6198	ValidAcc 0.4087	TestAcc 0.4563	BestValid 0.4087
	Epoch 750:	Loss 0.9775	TrainAcc 0.6230	ValidAcc 0.4111	TestAcc 0.4601	BestValid 0.4111
	Epoch 800:	Loss 0.8569	TrainAcc 0.6538	ValidAcc 0.4453	TestAcc 0.4707	BestValid 0.4453
	Epoch 850:	Loss 0.8288	TrainAcc 0.6959	ValidAcc 0.4543	TestAcc 0.4890	BestValid 0.4543
	Epoch 900:	Loss 0.7753	TrainAcc 0.6887	ValidAcc 0.4567	TestAcc 0.4630	BestValid 0.4567
	Epoch 950:	Loss 0.7562	TrainAcc 0.6811	ValidAcc 0.4351	TestAcc 0.4649	BestValid 0.4567
	Epoch 1000:	Loss 0.6745	TrainAcc 0.7332	ValidAcc 0.4651	TestAcc 0.4803	BestValid 0.4651
	Epoch 1050:	Loss 0.6409	TrainAcc 0.7865	ValidAcc 0.4892	TestAcc 0.5024	BestValid 0.4892
	Epoch 1100:	Loss 0.6150	TrainAcc 0.7804	ValidAcc 0.4808	TestAcc 0.5130	BestValid 0.4892
	Epoch 1150:	Loss 0.5955	TrainAcc 0.7736	ValidAcc 0.4633	TestAcc 0.4938	BestValid 0.4892
	Epoch 1200:	Loss 0.5978	TrainAcc 0.7756	ValidAcc 0.4669	TestAcc 0.4986	BestValid 0.4892
	Epoch 1250:	Loss 0.5656	TrainAcc 0.7845	ValidAcc 0.4609	TestAcc 0.4880	BestValid 0.4892
	Epoch 1300:	Loss 0.5150	TrainAcc 0.8157	ValidAcc 0.4802	TestAcc 0.5120	BestValid 0.4892
	Epoch 1350:	Loss 0.4956	TrainAcc 0.7909	ValidAcc 0.4633	TestAcc 0.5091	BestValid 0.4892
	Epoch 1400:	Loss 0.4442	TrainAcc 0.8474	ValidAcc 0.5048	TestAcc 0.5370	BestValid 0.5048
	Epoch 1450:	Loss 0.4774	TrainAcc 0.7256	ValidAcc 0.4213	TestAcc 0.4534	BestValid 0.5048
	Epoch 1500:	Loss 0.4312	TrainAcc 0.8289	ValidAcc 0.4964	TestAcc 0.5226	BestValid 0.5048
	Epoch 1550:	Loss 0.4336	TrainAcc 0.7969	ValidAcc 0.4669	TestAcc 0.4918	BestValid 0.5048
	Epoch 1600:	Loss 0.4165	TrainAcc 0.8438	ValidAcc 0.4916	TestAcc 0.5168	BestValid 0.5048
	Epoch 1650:	Loss 0.3615	TrainAcc 0.8498	ValidAcc 0.4922	TestAcc 0.5245	BestValid 0.5048
	Epoch 1700:	Loss 0.3638	TrainAcc 0.8650	ValidAcc 0.4994	TestAcc 0.5437	BestValid 0.5048
	Epoch 1750:	Loss 0.3449	TrainAcc 0.8898	ValidAcc 0.5258	TestAcc 0.5485	BestValid 0.5258
	Epoch 1800:	Loss 0.3920	TrainAcc 0.8217	ValidAcc 0.4994	TestAcc 0.5274	BestValid 0.5258
	Epoch 1850:	Loss 0.3293	TrainAcc 0.9143	ValidAcc 0.5385	TestAcc 0.5706	BestValid 0.5385
	Epoch 1900:	Loss 0.3239	TrainAcc 0.8950	ValidAcc 0.5192	TestAcc 0.5524	BestValid 0.5385
	Epoch 1950:	Loss 0.3072	TrainAcc 0.9038	ValidAcc 0.5427	TestAcc 0.5639	BestValid 0.5427
	Epoch 2000:	Loss 0.2940	TrainAcc 0.8770	ValidAcc 0.5132	TestAcc 0.5379	BestValid 0.5427
	Epoch 2050:	Loss 0.2977	TrainAcc 0.8261	ValidAcc 0.4712	TestAcc 0.4909	BestValid 0.5427
	Epoch 2100:	Loss 0.2884	TrainAcc 0.8862	ValidAcc 0.5246	TestAcc 0.5514	BestValid 0.5427
	Epoch 2150:	Loss 0.2752	TrainAcc 0.8506	ValidAcc 0.4946	TestAcc 0.5082	BestValid 0.5427
	Epoch 2200:	Loss 0.2537	TrainAcc 0.9014	ValidAcc 0.5258	TestAcc 0.5562	BestValid 0.5427
	Epoch 2250:	Loss 0.2387	TrainAcc 0.9287	ValidAcc 0.5331	TestAcc 0.5600	BestValid 0.5427
	Epoch 2300:	Loss 0.2288	TrainAcc 0.9371	ValidAcc 0.5427	TestAcc 0.5706	BestValid 0.5427
	Epoch 2350:	Loss 0.2493	TrainAcc 0.8413	ValidAcc 0.4627	TestAcc 0.5024	BestValid 0.5427
	Epoch 2400:	Loss 0.2494	TrainAcc 0.9343	ValidAcc 0.5337	TestAcc 0.5629	BestValid 0.5427
	Epoch 2450:	Loss 0.2127	TrainAcc 0.9255	ValidAcc 0.5234	TestAcc 0.5562	BestValid 0.5427
	Epoch 2500:	Loss 0.2178	TrainAcc 0.9191	ValidAcc 0.5294	TestAcc 0.5533	BestValid 0.5427
	Epoch 2550:	Loss 0.2037	TrainAcc 0.9243	ValidAcc 0.5222	TestAcc 0.5466	BestValid 0.5427
	Epoch 2600:	Loss 0.2393	TrainAcc 0.9038	ValidAcc 0.5216	TestAcc 0.5293	BestValid 0.5427
	Epoch 2650:	Loss 0.1930	TrainAcc 0.9343	ValidAcc 0.5288	TestAcc 0.5629	BestValid 0.5427
	Epoch 2700:	Loss 0.2167	TrainAcc 0.9179	ValidAcc 0.5282	TestAcc 0.5476	BestValid 0.5427
	Epoch 2750:	Loss 0.1986	TrainAcc 0.9339	ValidAcc 0.5469	TestAcc 0.5648	BestValid 0.5469
	Epoch 2800:	Loss 0.1985	TrainAcc 0.8966	ValidAcc 0.5030	TestAcc 0.5274	BestValid 0.5469
	Epoch 2850:	Loss 0.1687	TrainAcc 0.9451	ValidAcc 0.5499	TestAcc 0.5687	BestValid 0.5499
	Epoch 2900:	Loss 0.1942	TrainAcc 0.8694	ValidAcc 0.4862	TestAcc 0.5293	BestValid 0.5499
	Epoch 2950:	Loss 0.1760	TrainAcc 0.9399	ValidAcc 0.5463	TestAcc 0.5744	BestValid 0.5499
	Epoch 3000:	Loss 0.1910	TrainAcc 0.9379	ValidAcc 0.5306	TestAcc 0.5658	BestValid 0.5499
	Epoch 3050:	Loss 0.1632	TrainAcc 0.8862	ValidAcc 0.5228	TestAcc 0.5312	BestValid 0.5499
	Epoch 3100:	Loss 0.1466	TrainAcc 0.9511	ValidAcc 0.5306	TestAcc 0.5716	BestValid 0.5499
	Epoch 3150:	Loss 0.1615	TrainAcc 0.9423	ValidAcc 0.5403	TestAcc 0.5706	BestValid 0.5499
	Epoch 3200:	Loss 0.1536	TrainAcc 0.9075	ValidAcc 0.5090	TestAcc 0.5437	BestValid 0.5499
	Epoch 3250:	Loss 0.1844	TrainAcc 0.9367	ValidAcc 0.5331	TestAcc 0.5629	BestValid 0.5499
	Epoch 3300:	Loss 0.1523	TrainAcc 0.9555	ValidAcc 0.5487	TestAcc 0.5735	BestValid 0.5499
	Epoch 3350:	Loss 0.1469	TrainAcc 0.9387	ValidAcc 0.5312	TestAcc 0.5716	BestValid 0.5499
	Epoch 3400:	Loss 0.1325	TrainAcc 0.9491	ValidAcc 0.5385	TestAcc 0.5639	BestValid 0.5499
	Epoch 3450:	Loss 0.1296	TrainAcc 0.9423	ValidAcc 0.5403	TestAcc 0.5629	BestValid 0.5499
	Epoch 3500:	Loss 0.1525	TrainAcc 0.9247	ValidAcc 0.5319	TestAcc 0.5437	BestValid 0.5499
	Epoch 3550:	Loss 0.1291	TrainAcc 0.9483	ValidAcc 0.5379	TestAcc 0.5572	BestValid 0.5499
	Epoch 3600:	Loss 0.1232	TrainAcc 0.9439	ValidAcc 0.5355	TestAcc 0.5572	BestValid 0.5499
	Epoch 3650:	Loss 0.1160	TrainAcc 0.9643	ValidAcc 0.5553	TestAcc 0.5648	BestValid 0.5553
	Epoch 3700:	Loss 0.1184	TrainAcc 0.9383	ValidAcc 0.5162	TestAcc 0.5572	BestValid 0.5553
	Epoch 3750:	Loss 0.1564	TrainAcc 0.8550	ValidAcc 0.4724	TestAcc 0.4976	BestValid 0.5553
	Epoch 3800:	Loss 0.1356	TrainAcc 0.9692	ValidAcc 0.5481	TestAcc 0.5764	BestValid 0.5553
	Epoch 3850:	Loss 0.1165	TrainAcc 0.9635	ValidAcc 0.5463	TestAcc 0.5639	BestValid 0.5553
	Epoch 3900:	Loss 0.1005	TrainAcc 0.9607	ValidAcc 0.5403	TestAcc 0.5648	BestValid 0.5553
	Epoch 3950:	Loss 0.1201	TrainAcc 0.9571	ValidAcc 0.5403	TestAcc 0.5706	BestValid 0.5553
	Epoch 4000:	Loss 0.1129	TrainAcc 0.9663	ValidAcc 0.5553	TestAcc 0.5744	BestValid 0.5553
	Epoch 4050:	Loss 0.1253	TrainAcc 0.9331	ValidAcc 0.5252	TestAcc 0.5485	BestValid 0.5553
	Epoch 4100:	Loss 0.1108	TrainAcc 0.9575	ValidAcc 0.5312	TestAcc 0.5600	BestValid 0.5553
	Epoch 4150:	Loss 0.1178	TrainAcc 0.9539	ValidAcc 0.5409	TestAcc 0.5639	BestValid 0.5553
	Epoch 4200:	Loss 0.1386	TrainAcc 0.9251	ValidAcc 0.5078	TestAcc 0.5197	BestValid 0.5553
	Epoch 4250:	Loss 0.0901	TrainAcc 0.9744	ValidAcc 0.5499	TestAcc 0.5754	BestValid 0.5553
	Epoch 4300:	Loss 0.1040	TrainAcc 0.9495	ValidAcc 0.5282	TestAcc 0.5591	BestValid 0.5553
	Epoch 4350:	Loss 0.1230	TrainAcc 0.9607	ValidAcc 0.5421	TestAcc 0.5629	BestValid 0.5553
	Epoch 4400:	Loss 0.0939	TrainAcc 0.9663	ValidAcc 0.5457	TestAcc 0.5706	BestValid 0.5553
	Epoch 4450:	Loss 0.0898	TrainAcc 0.9235	ValidAcc 0.5078	TestAcc 0.5245	BestValid 0.5553
	Epoch 4500:	Loss 0.0880	TrainAcc 0.9635	ValidAcc 0.5427	TestAcc 0.5754	BestValid 0.5553
	Epoch 4550:	Loss 0.0928	TrainAcc 0.9547	ValidAcc 0.5523	TestAcc 0.5600	BestValid 0.5553
	Epoch 4600:	Loss 0.0824	TrainAcc 0.9663	ValidAcc 0.5421	TestAcc 0.5600	BestValid 0.5553
	Epoch 4650:	Loss 0.0838	TrainAcc 0.9607	ValidAcc 0.5373	TestAcc 0.5504	BestValid 0.5553
	Epoch 4700:	Loss 0.0896	TrainAcc 0.9491	ValidAcc 0.5439	TestAcc 0.5648	BestValid 0.5553
	Epoch 4750:	Loss 0.0951	TrainAcc 0.9720	ValidAcc 0.5433	TestAcc 0.5668	BestValid 0.5553
	Epoch 4800:	Loss 0.0823	TrainAcc 0.9663	ValidAcc 0.5433	TestAcc 0.5552	BestValid 0.5553
	Epoch 4850:	Loss 0.0683	TrainAcc 0.9683	ValidAcc 0.5475	TestAcc 0.5735	BestValid 0.5553
	Epoch 4900:	Loss 0.0824	TrainAcc 0.9728	ValidAcc 0.5553	TestAcc 0.5677	BestValid 0.5553
	Epoch 4950:	Loss 0.0661	TrainAcc 0.9692	ValidAcc 0.5517	TestAcc 0.5639	BestValid 0.5553
	Epoch 5000:	Loss 0.0697	TrainAcc 0.9776	ValidAcc 0.5607	TestAcc 0.5773	BestValid 0.5607
****** Epoch Time (Excluding Evaluation Cost): 0.137 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 23.661 ms (Max: 24.375, Min: 22.255, Sum: 189.286)
Cluster-Wide Average, Compute: 91.239 ms (Max: 103.046, Min: 87.172, Sum: 729.912)
Cluster-Wide Average, Communication-Layer: 7.245 ms (Max: 8.185, Min: 5.381, Sum: 57.960)
Cluster-Wide Average, Bubble-Imbalance: 11.998 ms (Max: 15.404, Min: 2.712, Sum: 95.982)
Cluster-Wide Average, Communication-Graph: 0.524 ms (Max: 0.577, Min: 0.458, Sum: 4.190)
Cluster-Wide Average, Optimization: 1.033 ms (Max: 1.464, Min: 0.963, Sum: 8.266)
Cluster-Wide Average, Others: 0.921 ms (Max: 1.774, Min: 0.644, Sum: 7.367)
****** Breakdown Sum: 136.620 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.098 GB (Max: 3.555, Min: 2.979, Sum: 24.784)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 39.501 Gbps (Max: 45.039, Min: 30.536, Sum: 316.005)
Layer-level communication (cluster-wide, per-epoch): 0.271 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.271 GB
****** Accuracy Results ******
Highest valid_acc: 0.5607
Target test_acc: 0.5773
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
