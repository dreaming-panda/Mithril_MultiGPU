Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 1 on machine gnerv2

DONE MPI INIT
Initialized node 2 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.010 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.030 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.035 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.035 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.374 Gbps (per GPU), 474.990 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.074 Gbps (per GPU), 472.593 Gbps (aggregated)
The layer-level communication performance: 59.073 Gbps (per GPU), 472.587 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.813 Gbps (per GPU), 470.501 Gbps (aggregated)
The layer-level communication performance: 58.776 Gbps (per GPU), 470.209 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.579 Gbps (per GPU), 468.629 Gbps (aggregated)
The layer-level communication performance: 58.531 Gbps (per GPU), 468.247 Gbps (aggregated)
The layer-level communication performance: 58.500 Gbps (per GPU), 467.996 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.691 Gbps (per GPU), 1253.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.691 Gbps (per GPU), 1253.531 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.688 Gbps (per GPU), 1253.507 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.609 Gbps (per GPU), 1252.873 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.685 Gbps (per GPU), 1253.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.609 Gbps (per GPU), 1252.873 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.688 Gbps (per GPU), 1253.504 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.685 Gbps (per GPU), 1253.481 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.065 Gbps (per GPU), 808.521 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.068 Gbps (per GPU), 808.541 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.065 Gbps (per GPU), 808.521 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.067 Gbps (per GPU), 808.534 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.064 Gbps (per GPU), 808.515 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.069 Gbps (per GPU), 808.553 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.064 Gbps (per GPU), 808.508 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.067 Gbps (per GPU), 808.534 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.171 Gbps (per GPU), 281.371 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.171 Gbps (per GPU), 281.368 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.171 Gbps (per GPU), 281.371 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.165 Gbps (per GPU), 281.318 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.166 Gbps (per GPU), 281.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.165 Gbps (per GPU), 281.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.166 Gbps (per GPU), 281.330 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.164 Gbps (per GPU), 281.314 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.63ms  0.93ms  1.51ms  2.40  0.08K  0.02M
 chk_1  0.74ms  1.02ms  1.64ms  2.22  0.16K  0.01M
 chk_2  0.66ms  0.96ms  1.55ms  2.33  0.13K  0.01M
 chk_3  0.82ms  1.09ms  1.70ms  2.07  0.18K  0.01M
 chk_4  0.73ms  1.01ms  1.63ms  2.23  0.15K  0.01M
 chk_5  0.85ms  1.11ms  1.74ms  2.04  0.25K  0.01M
 chk_6  0.61ms  0.93ms  1.51ms  2.46  0.10K  0.02M
 chk_7  0.66ms  0.95ms  1.54ms  2.36  0.11K  0.02M
 chk_8  0.83ms  1.10ms  1.73ms  2.08  0.23K  0.01M
 chk_9  0.72ms  1.00ms  1.62ms  2.23  0.14K  0.01M
chk_10  0.82ms  1.08ms  1.69ms  2.07  0.20K  0.01M
chk_11  0.62ms  0.92ms  1.52ms  2.46  0.09K  0.02M
chk_12  0.73ms  1.01ms  1.63ms  2.24  0.16K  0.01M
chk_13  0.73ms  1.01ms  1.63ms  2.24  0.16K  0.01M
chk_14  0.72ms  1.01ms  1.62ms  2.25  0.14K  0.01M
chk_15  0.82ms  1.09ms  1.72ms  2.09  0.21K  0.01M
chk_16  0.81ms  1.08ms  1.70ms  2.11  0.18K  0.01M
chk_17  0.89ms  1.14ms  1.80ms  2.01  0.29K  0.01M
chk_18  0.90ms  1.15ms  1.81ms  2.00  0.31K  0.00M
chk_19  0.65ms  0.95ms  1.55ms  2.39  0.13K  0.01M
chk_20  0.65ms  0.95ms  1.55ms  2.38  0.13K  0.01M
chk_21  0.80ms  1.08ms  1.70ms  2.12  0.18K  0.01M
chk_22  0.65ms  0.96ms  1.55ms  2.38  0.13K  0.01M
chk_23  0.79ms  1.07ms  1.69ms  2.13  0.16K  0.01M
chk_24  0.61ms  0.92ms  1.51ms  2.47  0.09K  0.02M
chk_25  0.62ms  0.93ms  1.52ms  2.45  0.09K  0.02M
chk_26  0.80ms  1.07ms  1.69ms  2.11  0.18K  0.01M
chk_27  0.65ms  0.94ms  1.55ms  2.38  0.13K  0.01M
chk_28  0.80ms  1.07ms  1.69ms  2.12  0.17K  0.01M
chk_29  0.73ms  1.01ms  1.63ms  2.24  0.15K  0.01M
chk_30  0.85ms  1.10ms  1.74ms  2.06  0.24K  0.01M
chk_31  0.82ms  1.07ms  1.70ms  2.07  0.20K  0.01M
   Avg  0.74  1.02  1.64
   Max  0.90  1.15  1.81
   Min  0.61  0.92  1.51
 Ratio  1.47  1.24  1.20
   Var  0.01  0.00  0.01
Profiling takes 1.339 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 48)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [48, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [256, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 152)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 5201
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 256)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [152, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [308, 360)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [360, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 48)...
+++++++++ Node 1 initializing the weights for op[48, 100)...
+++++++++ Node 2 initializing the weights for op[100, 152)...
+++++++++ Node 5 initializing the weights for op[256, 308)...
+++++++++ Node 4 initializing the weights for op[204, 256)...
+++++++++ Node 3 initializing the weights for op[152, 204)...
+++++++++ Node 6 initializing the weights for op[308, 360)...
+++++++++ Node 7 initializing the weights for op[360, 421)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.1003	TrainAcc 0.1979	ValidAcc 0.2043	TestAcc 0.1979	BestValid 0.2043
	Epoch 50:	Loss 1.6699	TrainAcc 0.3494	ValidAcc 0.3179	TestAcc 0.3208	BestValid 0.3179
	Epoch 100:	Loss 1.2168	TrainAcc 0.5329	ValidAcc 0.3594	TestAcc 0.3631	BestValid 0.3594
	Epoch 150:	Loss 0.7482	TrainAcc 0.7873	ValidAcc 0.3401	TestAcc 0.3612	BestValid 0.3594
	Epoch 200:	Loss 0.3302	TrainAcc 0.9407	ValidAcc 0.3516	TestAcc 0.3718	BestValid 0.3594
	Epoch 250:	Loss 0.0979	TrainAcc 0.9924	ValidAcc 0.3534	TestAcc 0.3554	BestValid 0.3594
	Epoch 300:	Loss 0.0463	TrainAcc 0.9956	ValidAcc 0.3648	TestAcc 0.3679	BestValid 0.3648
	Epoch 350:	Loss 0.0315	TrainAcc 0.9976	ValidAcc 0.3600	TestAcc 0.3708	BestValid 0.3648
	Epoch 400:	Loss 0.0235	TrainAcc 0.9976	ValidAcc 0.3654	TestAcc 0.3660	BestValid 0.3654
	Epoch 450:	Loss 0.0215	TrainAcc 0.9976	ValidAcc 0.3678	TestAcc 0.3775	BestValid 0.3678
	Epoch 500:	Loss 0.0127	TrainAcc 0.9976	ValidAcc 0.3666	TestAcc 0.3756	BestValid 0.3678
	Epoch 550:	Loss 0.0184	TrainAcc 0.9980	ValidAcc 0.3672	TestAcc 0.3727	BestValid 0.3678
	Epoch 600:	Loss 0.0121	TrainAcc 0.9980	ValidAcc 0.3642	TestAcc 0.3775	BestValid 0.3678
	Epoch 650:	Loss 0.0114	TrainAcc 0.9980	ValidAcc 0.3672	TestAcc 0.3766	BestValid 0.3678
	Epoch 700:	Loss 0.0053	TrainAcc 0.9984	ValidAcc 0.3696	TestAcc 0.3804	BestValid 0.3696
	Epoch 750:	Loss 0.0087	TrainAcc 0.9980	ValidAcc 0.3732	TestAcc 0.3871	BestValid 0.3732
	Epoch 800:	Loss 0.0067	TrainAcc 0.9984	ValidAcc 0.3636	TestAcc 0.3727	BestValid 0.3732
	Epoch 850:	Loss 0.0072	TrainAcc 0.9980	ValidAcc 0.3708	TestAcc 0.3775	BestValid 0.3732
	Epoch 900:	Loss 0.0087	TrainAcc 0.9984	ValidAcc 0.3666	TestAcc 0.3727	BestValid 0.3732
	Epoch 950:	Loss 0.0050	TrainAcc 0.9988	ValidAcc 0.3648	TestAcc 0.3718	BestValid 0.3732
	Epoch 1000:	Loss 0.0041	TrainAcc 0.9980	ValidAcc 0.3624	TestAcc 0.3718	BestValid 0.3732
	Epoch 1050:	Loss 0.0044	TrainAcc 0.9992	ValidAcc 0.3606	TestAcc 0.3679	BestValid 0.3732
	Epoch 1100:	Loss 0.0033	TrainAcc 0.9988	ValidAcc 0.3546	TestAcc 0.3698	BestValid 0.3732
	Epoch 1150:	Loss 0.0080	TrainAcc 0.9992	ValidAcc 0.3660	TestAcc 0.3679	BestValid 0.3732
	Epoch 1200:	Loss 0.0030	TrainAcc 0.9992	ValidAcc 0.3702	TestAcc 0.3852	BestValid 0.3732
	Epoch 1250:	Loss 0.0174	TrainAcc 0.9988	ValidAcc 0.3762	TestAcc 0.3775	BestValid 0.3762
	Epoch 1300:	Loss 0.0055	TrainAcc 0.9984	ValidAcc 0.3750	TestAcc 0.3823	BestValid 0.3762
	Epoch 1350:	Loss 0.0047	TrainAcc 0.9992	ValidAcc 0.3816	TestAcc 0.3766	BestValid 0.3816
	Epoch 1400:	Loss 0.0030	TrainAcc 0.9992	ValidAcc 0.3762	TestAcc 0.3814	BestValid 0.3816
	Epoch 1450:	Loss 0.0066	TrainAcc 0.9992	ValidAcc 0.3696	TestAcc 0.3775	BestValid 0.3816
	Epoch 1500:	Loss 0.0015	TrainAcc 0.9988	ValidAcc 0.3684	TestAcc 0.3727	BestValid 0.3816
	Epoch 1550:	Loss 0.0063	TrainAcc 0.9980	ValidAcc 0.3756	TestAcc 0.3727	BestValid 0.3816
	Epoch 1600:	Loss 0.0043	TrainAcc 0.9996	ValidAcc 0.3714	TestAcc 0.3737	BestValid 0.3816
	Epoch 1650:	Loss 0.0021	TrainAcc 0.9988	ValidAcc 0.3690	TestAcc 0.3698	BestValid 0.3816
	Epoch 1700:	Loss 0.0030	TrainAcc 0.9996	ValidAcc 0.3648	TestAcc 0.3698	BestValid 0.3816
	Epoch 1750:	Loss 0.0042	TrainAcc 0.9992	ValidAcc 0.3720	TestAcc 0.3804	BestValid 0.3816
	Epoch 1800:	Loss 0.0023	TrainAcc 0.9996	ValidAcc 0.3696	TestAcc 0.3785	BestValid 0.3816
	Epoch 1850:	Loss 0.0018	TrainAcc 0.9992	ValidAcc 0.3714	TestAcc 0.3814	BestValid 0.3816
	Epoch 1900:	Loss 0.0039	TrainAcc 0.9988	ValidAcc 0.3708	TestAcc 0.3718	BestValid 0.3816
	Epoch 1950:	Loss 0.0034	TrainAcc 0.9992	ValidAcc 0.3648	TestAcc 0.3823	BestValid 0.3816
	Epoch 2000:	Loss 0.0006	TrainAcc 0.9996	ValidAcc 0.3612	TestAcc 0.3775	BestValid 0.3816
	Epoch 2050:	Loss 0.0020	TrainAcc 0.9992	ValidAcc 0.3660	TestAcc 0.3814	BestValid 0.3816
	Epoch 2100:	Loss 0.0023	TrainAcc 0.9988	ValidAcc 0.3672	TestAcc 0.3622	BestValid 0.3816
	Epoch 2150:	Loss 0.0013	TrainAcc 0.9992	ValidAcc 0.3666	TestAcc 0.3708	BestValid 0.3816
	Epoch 2200:	Loss 0.0011	TrainAcc 0.9996	ValidAcc 0.3744	TestAcc 0.3727	BestValid 0.3816
	Epoch 2250:	Loss 0.0018	TrainAcc 0.9980	ValidAcc 0.3648	TestAcc 0.3689	BestValid 0.3816
	Epoch 2300:	Loss 0.0020	TrainAcc 0.9996	ValidAcc 0.3672	TestAcc 0.3689	BestValid 0.3816
	Epoch 2350:	Loss 0.0013	TrainAcc 1.0000	ValidAcc 0.3708	TestAcc 0.3660	BestValid 0.3816
	Epoch 2400:	Loss 0.0008	TrainAcc 0.9996	ValidAcc 0.3750	TestAcc 0.3612	BestValid 0.3816
	Epoch 2450:	Loss 0.0005	TrainAcc 1.0000	ValidAcc 0.3816	TestAcc 0.3641	BestValid 0.3816
	Epoch 2500:	Loss 0.0008	TrainAcc 0.9996	ValidAcc 0.3816	TestAcc 0.3660	BestValid 0.3816
	Epoch 2550:	Loss 0.0045	TrainAcc 0.9988	ValidAcc 0.3822	TestAcc 0.3794	BestValid 0.3822
	Epoch 2600:	Loss 0.0028	TrainAcc 1.0000	ValidAcc 0.3798	TestAcc 0.3622	BestValid 0.3822
	Epoch 2650:	Loss 0.0010	TrainAcc 0.9996	ValidAcc 0.3744	TestAcc 0.3573	BestValid 0.3822
	Epoch 2700:	Loss 0.0066	TrainAcc 0.9996	ValidAcc 0.3600	TestAcc 0.3612	BestValid 0.3822
	Epoch 2750:	Loss 0.0045	TrainAcc 0.9992	ValidAcc 0.3708	TestAcc 0.3650	BestValid 0.3822
	Epoch 2800:	Loss 0.0020	TrainAcc 0.9996	ValidAcc 0.3774	TestAcc 0.3641	BestValid 0.3822
	Epoch 2850:	Loss 0.0049	TrainAcc 0.9992	ValidAcc 0.3822	TestAcc 0.3698	BestValid 0.3822
	Epoch 2900:	Loss 0.0005	TrainAcc 0.9996	ValidAcc 0.3708	TestAcc 0.3650	BestValid 0.3822
	Epoch 2950:	Loss 0.0006	TrainAcc 1.0000	ValidAcc 0.3720	TestAcc 0.3660	BestValid 0.3822
	Epoch 3000:	Loss 0.0039	TrainAcc 0.9992	ValidAcc 0.3774	TestAcc 0.3689	BestValid 0.3822
	Epoch 3050:	Loss 0.0007	TrainAcc 0.9996	ValidAcc 0.3762	TestAcc 0.3689	BestValid 0.3822
	Epoch 3100:	Loss 0.0005	TrainAcc 1.0000	ValidAcc 0.3774	TestAcc 0.3698	BestValid 0.3822
	Epoch 3150:	Loss 0.0013	TrainAcc 0.9992	ValidAcc 0.3780	TestAcc 0.3814	BestValid 0.3822
	Epoch 3200:	Loss 0.0013	TrainAcc 0.9988	ValidAcc 0.3690	TestAcc 0.3631	BestValid 0.3822
	Epoch 3250:	Loss 0.0012	TrainAcc 0.9992	ValidAcc 0.3786	TestAcc 0.3660	BestValid 0.3822
	Epoch 3300:	Loss 0.0004	TrainAcc 1.0000	ValidAcc 0.3720	TestAcc 0.3660	BestValid 0.3822
	Epoch 3350:	Loss 0.0006	TrainAcc 0.9996	ValidAcc 0.3864	TestAcc 0.3698	BestValid 0.3864
	Epoch 3400:	Loss 0.0038	TrainAcc 0.9992	ValidAcc 0.3894	TestAcc 0.3718	BestValid 0.3894
	Epoch 3450:	Loss 0.0021	TrainAcc 0.9996	ValidAcc 0.3828	TestAcc 0.3641	BestValid 0.3894
	Epoch 3500:	Loss 0.0006	TrainAcc 0.9996	ValidAcc 0.3876	TestAcc 0.3785	BestValid 0.3894
	Epoch 3550:	Loss 0.0014	TrainAcc 1.0000	ValidAcc 0.3858	TestAcc 0.3775	BestValid 0.3894
	Epoch 3600:	Loss 0.0004	TrainAcc 1.0000	ValidAcc 0.3864	TestAcc 0.3679	BestValid 0.3894
	Epoch 3650:	Loss 0.0003	TrainAcc 1.0000	ValidAcc 0.3900	TestAcc 0.3785	BestValid 0.3900
	Epoch 3700:	Loss 0.0008	TrainAcc 1.0000	ValidAcc 0.3876	TestAcc 0.3746	BestValid 0.3900
	Epoch 3750:	Loss 0.0006	TrainAcc 0.9996	ValidAcc 0.3870	TestAcc 0.3708	BestValid 0.3900
	Epoch 3800:	Loss 0.0005	TrainAcc 1.0000	ValidAcc 0.3810	TestAcc 0.3804	BestValid 0.3900
	Epoch 3850:	Loss 0.0002	TrainAcc 1.0000	ValidAcc 0.3774	TestAcc 0.3698	BestValid 0.3900
	Epoch 3900:	Loss 0.0005	TrainAcc 0.9996	ValidAcc 0.3774	TestAcc 0.3612	BestValid 0.3900
	Epoch 3950:	Loss 0.0005	TrainAcc 1.0000	ValidAcc 0.3798	TestAcc 0.3766	BestValid 0.3900
	Epoch 4000:	Loss 0.0002	TrainAcc 1.0000	ValidAcc 0.3756	TestAcc 0.3746	BestValid 0.3900
	Epoch 4050:	Loss 0.0002	TrainAcc 0.9996	ValidAcc 0.3894	TestAcc 0.3756	BestValid 0.3900
	Epoch 4100:	Loss 0.0002	TrainAcc 1.0000	ValidAcc 0.3960	TestAcc 0.3737	BestValid 0.3960
	Epoch 4150:	Loss 0.0009	TrainAcc 0.9996	ValidAcc 0.3852	TestAcc 0.3785	BestValid 0.3960
	Epoch 4200:	Loss 0.0003	TrainAcc 1.0000	ValidAcc 0.3894	TestAcc 0.3718	BestValid 0.3960
	Epoch 4250:	Loss 0.0001	TrainAcc 1.0000	ValidAcc 0.3870	TestAcc 0.3631	BestValid 0.3960
	Epoch 4300:	Loss 0.0002	TrainAcc 1.0000	ValidAcc 0.3804	TestAcc 0.3670	BestValid 0.3960
	Epoch 4350:	Loss 0.0001	TrainAcc 1.0000	ValidAcc 0.3888	TestAcc 0.3718	BestValid 0.3960
	Epoch 4400:	Loss 0.0000	TrainAcc 1.0000	ValidAcc 0.3822	TestAcc 0.3698	BestValid 0.3960
	Epoch 4450:	Loss 0.0052	TrainAcc 0.9988	ValidAcc 0.3762	TestAcc 0.3660	BestValid 0.3960
	Epoch 4500:	Loss 0.0004	TrainAcc 1.0000	ValidAcc 0.3732	TestAcc 0.3612	BestValid 0.3960
	Epoch 4550:	Loss 0.0002	TrainAcc 1.0000	ValidAcc 0.3804	TestAcc 0.3670	BestValid 0.3960
	Epoch 4600:	Loss 0.0006	TrainAcc 1.0000	ValidAcc 0.3774	TestAcc 0.3612	BestValid 0.3960
	Epoch 4650:	Loss 0.0029	TrainAcc 0.9992	ValidAcc 0.3882	TestAcc 0.3785	BestValid 0.3960
	Epoch 4700:	Loss 0.0002	TrainAcc 1.0000	ValidAcc 0.3918	TestAcc 0.3746	BestValid 0.3960
	Epoch 4750:	Loss 0.0004	TrainAcc 0.9996	ValidAcc 0.3948	TestAcc 0.3823	BestValid 0.3960
	Epoch 4800:	Loss 0.0003	TrainAcc 1.0000	ValidAcc 0.3888	TestAcc 0.3775	BestValid 0.3960
	Epoch 4850:	Loss 0.0049	TrainAcc 1.0000	ValidAcc 0.3822	TestAcc 0.3698	BestValid 0.3960
	Epoch 4900:	Loss 0.0010	TrainAcc 1.0000	ValidAcc 0.3864	TestAcc 0.3737	BestValid 0.3960
	Epoch 4950:	Loss 0.0000	TrainAcc 1.0000	ValidAcc 0.3888	TestAcc 0.3766	BestValid 0.3960
	Epoch 5000:	Loss 0.0006	TrainAcc 0.9996	ValidAcc 0.3816	TestAcc 0.3756	BestValid 0.3960
****** Epoch Time (Excluding Evaluation Cost): 0.184 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 31.698 ms (Max: 32.561, Min: 30.297, Sum: 253.581)
Cluster-Wide Average, Compute: 125.297 ms (Max: 138.820, Min: 121.700, Sum: 1002.378)
Cluster-Wide Average, Communication-Layer: 7.390 ms (Max: 8.297, Min: 5.395, Sum: 59.117)
Cluster-Wide Average, Bubble-Imbalance: 15.878 ms (Max: 19.822, Min: 5.007, Sum: 127.026)
Cluster-Wide Average, Communication-Graph: 0.550 ms (Max: 0.698, Min: 0.479, Sum: 4.398)
Cluster-Wide Average, Optimization: 2.164 ms (Max: 2.538, Min: 2.083, Sum: 17.311)
Cluster-Wide Average, Others: 1.292 ms (Max: 2.074, Min: 1.074, Sum: 10.336)
****** Breakdown Sum: 184.269 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.723 GB (Max: 4.159, Min: 3.604, Sum: 29.784)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 38.711 Gbps (Max: 42.439, Min: 30.132, Sum: 309.689)
Layer-level communication (cluster-wide, per-epoch): 0.271 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.271 GB
****** Accuracy Results ******
Highest valid_acc: 0.3960
Target test_acc: 0.3737
Epoch to reach the target acc: 4099
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
