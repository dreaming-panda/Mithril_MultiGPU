Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
DONE MPI INIT
Initialized node 0 on machine gnerv4

Initialized node 1 on machine gnerv4
DONE MPI INIT
DONE MPI INIT
Initialized node 4 on machine gnerv8
Initialized node 5 on machine gnerv8
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv8

Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.014 seconds.
        It takes 0.009 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.015 seconds.
        It takes 0.019 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.030 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.626 Gbps (per GPU), 485.005 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.311 Gbps (per GPU), 482.488 Gbps (aggregated)
The layer-level communication performance: 60.510 Gbps (per GPU), 484.079 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.080 Gbps (per GPU), 480.638 Gbps (aggregated)
The layer-level communication performance: 60.047 Gbps (per GPU), 480.379 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.810 Gbps (per GPU), 478.479 Gbps (aggregated)
The layer-level communication performance: 59.764 Gbps (per GPU), 478.116 Gbps (aggregated)
The layer-level communication performance: 59.730 Gbps (per GPU), 477.841 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.474 Gbps (per GPU), 1283.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.449 Gbps (per GPU), 1283.594 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.419 Gbps (per GPU), 1283.349 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.449 Gbps (per GPU), 1283.594 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.471 Gbps (per GPU), 1283.766 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.437 Gbps (per GPU), 1283.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.422 Gbps (per GPU), 1283.375 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.446 Gbps (per GPU), 1283.569 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.648 Gbps (per GPU), 837.186 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.644 Gbps (per GPU), 837.152 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.648 Gbps (per GPU), 837.186 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.644 Gbps (per GPU), 837.152 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.650 Gbps (per GPU), 837.200 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.644 Gbps (per GPU), 837.152 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.647 Gbps (per GPU), 837.179 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.646 Gbps (per GPU), 837.166 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.920 Gbps (per GPU), 303.362 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.918 Gbps (per GPU), 303.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.918 Gbps (per GPU), 303.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.917 Gbps (per GPU), 303.334 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.914 Gbps (per GPU), 303.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.918 Gbps (per GPU), 303.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.914 Gbps (per GPU), 303.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.915 Gbps (per GPU), 303.317 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.43ms  0.70ms  1.21ms  2.80  0.08K  0.02M
 chk_1  0.53ms  0.74ms  1.29ms  2.42  0.16K  0.01M
 chk_2  0.46ms  0.69ms  1.23ms  2.66  0.13K  0.01M
 chk_3  0.56ms  0.77ms  1.32ms  2.36  0.18K  0.01M
 chk_4  0.52ms  0.73ms  1.28ms  2.47  0.15K  0.01M
 chk_5  0.60ms  0.79ms  1.35ms  2.25  0.25K  0.01M
 chk_6  0.44ms  0.70ms  1.22ms  2.77  0.10K  0.02M
 chk_7  0.47ms  0.71ms  1.23ms  2.62  0.11K  0.02M
 chk_8  0.59ms  0.79ms  1.35ms  2.29  0.23K  0.01M
 chk_9  0.50ms  0.73ms  1.27ms  2.53  0.14K  0.01M
chk_10  0.58ms  0.78ms  1.33ms  2.30  0.20K  0.01M
chk_11  0.44ms  0.70ms  1.22ms  2.75  0.09K  0.02M
chk_12  0.51ms  0.73ms  1.29ms  2.52  0.16K  0.01M
chk_13  0.51ms  0.73ms  1.29ms  2.52  0.16K  0.01M
chk_14  0.51ms  0.73ms  1.28ms  2.52  0.14K  0.01M
chk_15  0.58ms  0.78ms  1.33ms  2.30  0.21K  0.01M
chk_16  0.56ms  0.78ms  1.32ms  2.36  0.18K  0.01M
chk_17  0.62ms  0.80ms  1.38ms  2.23  0.29K  0.01M
chk_18  0.62ms  0.80ms  1.39ms  2.24  0.31K  0.00M
chk_19  0.46ms  0.70ms  1.24ms  2.68  0.13K  0.01M
chk_20  0.46ms  0.69ms  1.24ms  2.67  0.13K  0.01M
chk_21  0.55ms  0.77ms  1.32ms  2.38  0.18K  0.01M
chk_22  0.46ms  0.69ms  1.23ms  2.67  0.13K  0.01M
chk_23  0.55ms  0.76ms  1.32ms  2.40  0.16K  0.01M
chk_24  0.44ms  0.69ms  1.22ms  2.78  0.09K  0.02M
chk_25  0.44ms  0.69ms  1.22ms  2.74  0.09K  0.02M
chk_26  0.55ms  0.77ms  1.32ms  2.39  0.18K  0.01M
chk_27  0.46ms  0.70ms  1.23ms  2.67  0.13K  0.01M
chk_28  0.55ms  0.76ms  1.32ms  2.40  0.17K  0.01M
chk_29  0.51ms  0.73ms  1.29ms  2.54  0.15K  0.01M
chk_30  0.59ms  0.78ms  1.36ms  2.30  0.24K  0.01M
chk_31  0.57ms  0.77ms  1.33ms  2.32  0.20K  0.01M
   Avg  0.52  0.74  1.29
   Max  0.62  0.80  1.39
   Min  0.43  0.69  1.21
 Ratio  1.44  1.16  1.15
   Var  0.00  0.00  0.00
Profiling takes 1.031 s
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 27)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [27, 55)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [83, 111)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [55, 83)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [139, 167)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [167, 195)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [195, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 5201
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [111, 139)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 5201
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[55, 83)...
+++++++++ Node 0 initializing the weights for op[0, 27)...
+++++++++ Node 6 initializing the weights for op[167, 195)...
+++++++++ Node 1 initializing the weights for op[27, 55)...
+++++++++ Node 3 initializing the weights for op[83, 111)...
+++++++++ Node 4 initializing the weights for op[111, 139)...
+++++++++ Node 5 initializing the weights for op[139, 167)...
+++++++++ Node 7 initializing the weights for op[195, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 1, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...



*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.0663	TrainAcc 0.2059	ValidAcc 0.1953	TestAcc 0.1931	BestValid 0.1953
	Epoch 50:	Loss 1.7483	TrainAcc 0.2031	ValidAcc 0.2049	TestAcc 0.2008	BestValid 0.2049
	Epoch 100:	Loss 1.6985	TrainAcc 0.2151	ValidAcc 0.2067	TestAcc 0.2113	BestValid 0.2067
	Epoch 150:	Loss 1.6587	TrainAcc 0.2300	ValidAcc 0.2115	TestAcc 0.2450	BestValid 0.2115
	Epoch 200:	Loss 1.5955	TrainAcc 0.2704	ValidAcc 0.2650	TestAcc 0.2767	BestValid 0.2650
	Epoch 250:	Loss 1.5663	TrainAcc 0.2600	ValidAcc 0.2680	TestAcc 0.2728	BestValid 0.2680
	Epoch 300:	Loss 1.5630	TrainAcc 0.2845	ValidAcc 0.2770	TestAcc 0.2757	BestValid 0.2770
	Epoch 350:	Loss 1.4884	TrainAcc 0.2732	ValidAcc 0.2650	TestAcc 0.2939	BestValid 0.2770
	Epoch 400:	Loss 1.4389	TrainAcc 0.3085	ValidAcc 0.2825	TestAcc 0.2930	BestValid 0.2825
	Epoch 450:	Loss 1.3569	TrainAcc 0.3494	ValidAcc 0.3041	TestAcc 0.3141	BestValid 0.3041
	Epoch 500:	Loss 1.3073	TrainAcc 0.4155	ValidAcc 0.3359	TestAcc 0.3372	BestValid 0.3359
	Epoch 550:	Loss 1.2256	TrainAcc 0.4768	ValidAcc 0.3522	TestAcc 0.3708	BestValid 0.3522
	Epoch 600:	Loss 1.1717	TrainAcc 0.5204	ValidAcc 0.3882	TestAcc 0.4073	BestValid 0.3882
	Epoch 650:	Loss 1.1155	TrainAcc 0.5208	ValidAcc 0.3666	TestAcc 0.3958	BestValid 0.3882
	Epoch 700:	Loss 1.0427	TrainAcc 0.5877	ValidAcc 0.4165	TestAcc 0.4236	BestValid 0.4165
	Epoch 750:	Loss 0.9858	TrainAcc 0.6210	ValidAcc 0.4411	TestAcc 0.4419	BestValid 0.4411
	Epoch 800:	Loss 0.9421	TrainAcc 0.6294	ValidAcc 0.4297	TestAcc 0.4524	BestValid 0.4411
	Epoch 850:	Loss 0.8561	TrainAcc 0.6683	ValidAcc 0.4447	TestAcc 0.4793	BestValid 0.4447
	Epoch 900:	Loss 0.8655	TrainAcc 0.6094	ValidAcc 0.4026	TestAcc 0.4256	BestValid 0.4447
	Epoch 950:	Loss 0.7808	TrainAcc 0.7095	ValidAcc 0.4597	TestAcc 0.4880	BestValid 0.4597
	Epoch 1000:	Loss 0.7711	TrainAcc 0.6999	ValidAcc 0.4561	TestAcc 0.4813	BestValid 0.4597
	Epoch 1050:	Loss 0.6924	TrainAcc 0.7408	ValidAcc 0.4772	TestAcc 0.5082	BestValid 0.4772
	Epoch 1100:	Loss 0.6876	TrainAcc 0.6927	ValidAcc 0.4543	TestAcc 0.4745	BestValid 0.4772
	Epoch 1150:	Loss 0.6266	TrainAcc 0.7716	ValidAcc 0.5012	TestAcc 0.5062	BestValid 0.5012
	Epoch 1200:	Loss 0.6033	TrainAcc 0.7568	ValidAcc 0.4910	TestAcc 0.5139	BestValid 0.5012
	Epoch 1250:	Loss 0.6091	TrainAcc 0.8025	ValidAcc 0.4958	TestAcc 0.5062	BestValid 0.5012
	Epoch 1300:	Loss 0.5541	TrainAcc 0.7768	ValidAcc 0.4742	TestAcc 0.4918	BestValid 0.5012
	Epoch 1350:	Loss 0.5763	TrainAcc 0.7720	ValidAcc 0.4886	TestAcc 0.4947	BestValid 0.5012
	Epoch 1400:	Loss 0.5106	TrainAcc 0.8189	ValidAcc 0.5138	TestAcc 0.5101	BestValid 0.5138
	Epoch 1450:	Loss 0.4926	TrainAcc 0.8381	ValidAcc 0.5246	TestAcc 0.5235	BestValid 0.5246
	Epoch 1500:	Loss 0.5121	TrainAcc 0.7660	ValidAcc 0.4706	TestAcc 0.4717	BestValid 0.5246
	Epoch 1550:	Loss 0.4524	TrainAcc 0.8618	ValidAcc 0.5300	TestAcc 0.5389	BestValid 0.5300
	Epoch 1600:	Loss 0.4600	TrainAcc 0.8538	ValidAcc 0.5282	TestAcc 0.5418	BestValid 0.5300
	Epoch 1650:	Loss 0.4339	TrainAcc 0.7897	ValidAcc 0.4675	TestAcc 0.4832	BestValid 0.5300
	Epoch 1700:	Loss 0.4101	TrainAcc 0.8245	ValidAcc 0.4940	TestAcc 0.5053	BestValid 0.5300
	Epoch 1750:	Loss 0.4400	TrainAcc 0.7808	ValidAcc 0.4772	TestAcc 0.4851	BestValid 0.5300
	Epoch 1800:	Loss 0.5565	TrainAcc 0.7792	ValidAcc 0.4694	TestAcc 0.4899	BestValid 0.5300
	Epoch 1850:	Loss 0.3829	TrainAcc 0.8674	ValidAcc 0.5204	TestAcc 0.5312	BestValid 0.5300
	Epoch 1900:	Loss 0.3435	TrainAcc 0.9054	ValidAcc 0.5373	TestAcc 0.5610	BestValid 0.5373
	Epoch 1950:	Loss 0.3457	TrainAcc 0.8826	ValidAcc 0.5240	TestAcc 0.5293	BestValid 0.5373
	Epoch 2000:	Loss 0.3152	TrainAcc 0.8726	ValidAcc 0.5132	TestAcc 0.5303	BestValid 0.5373
	Epoch 2050:	Loss 0.3675	TrainAcc 0.8522	ValidAcc 0.5036	TestAcc 0.5264	BestValid 0.5373
	Epoch 2100:	Loss 0.3188	TrainAcc 0.8770	ValidAcc 0.5120	TestAcc 0.5226	BestValid 0.5373
	Epoch 2150:	Loss 0.3214	TrainAcc 0.9038	ValidAcc 0.5331	TestAcc 0.5418	BestValid 0.5373
	Epoch 2200:	Loss 0.2985	TrainAcc 0.8914	ValidAcc 0.5391	TestAcc 0.5533	BestValid 0.5391
	Epoch 2250:	Loss 0.2770	TrainAcc 0.9199	ValidAcc 0.5397	TestAcc 0.5476	BestValid 0.5397
	Epoch 2300:	Loss 0.2363	TrainAcc 0.9263	ValidAcc 0.5385	TestAcc 0.5639	BestValid 0.5397
	Epoch 2350:	Loss 0.2670	TrainAcc 0.9034	ValidAcc 0.5403	TestAcc 0.5514	BestValid 0.5403
	Epoch 2400:	Loss 0.2472	TrainAcc 0.9143	ValidAcc 0.5325	TestAcc 0.5456	BestValid 0.5403
	Epoch 2450:	Loss 0.2818	TrainAcc 0.8722	ValidAcc 0.5144	TestAcc 0.5427	BestValid 0.5403
	Epoch 2500:	Loss 0.2833	TrainAcc 0.8858	ValidAcc 0.5264	TestAcc 0.5427	BestValid 0.5403
	Epoch 2550:	Loss 0.2314	TrainAcc 0.9255	ValidAcc 0.5439	TestAcc 0.5591	BestValid 0.5439
	Epoch 2600:	Loss 0.3383	TrainAcc 0.9159	ValidAcc 0.5499	TestAcc 0.5581	BestValid 0.5499
	Epoch 2650:	Loss 0.2689	TrainAcc 0.8934	ValidAcc 0.5138	TestAcc 0.5399	BestValid 0.5499
	Epoch 2700:	Loss 0.2299	TrainAcc 0.8894	ValidAcc 0.5162	TestAcc 0.5235	BestValid 0.5499
	Epoch 2750:	Loss 0.2269	TrainAcc 0.9171	ValidAcc 0.5427	TestAcc 0.5524	BestValid 0.5499
	Epoch 2800:	Loss 0.2035	TrainAcc 0.9227	ValidAcc 0.5379	TestAcc 0.5427	BestValid 0.5499
	Epoch 2850:	Loss 0.1959	TrainAcc 0.9451	ValidAcc 0.5571	TestAcc 0.5620	BestValid 0.5571
	Epoch 2900:	Loss 0.1970	TrainAcc 0.9058	ValidAcc 0.5162	TestAcc 0.5312	BestValid 0.5571
	Epoch 2950:	Loss 0.2130	TrainAcc 0.9095	ValidAcc 0.5331	TestAcc 0.5370	BestValid 0.5571
	Epoch 3000:	Loss 0.2159	TrainAcc 0.8602	ValidAcc 0.4844	TestAcc 0.5024	BestValid 0.5571
	Epoch 3050:	Loss 0.1838	TrainAcc 0.9143	ValidAcc 0.5300	TestAcc 0.5389	BestValid 0.5571
	Epoch 3100:	Loss 0.1761	TrainAcc 0.9415	ValidAcc 0.5487	TestAcc 0.5591	BestValid 0.5571
	Epoch 3150:	Loss 0.1628	TrainAcc 0.9519	ValidAcc 0.5505	TestAcc 0.5620	BestValid 0.5571
	Epoch 3200:	Loss 0.1721	TrainAcc 0.9479	ValidAcc 0.5439	TestAcc 0.5572	BestValid 0.5571
	Epoch 3250:	Loss 0.1665	TrainAcc 0.8730	ValidAcc 0.4880	TestAcc 0.5024	BestValid 0.5571
	Epoch 3300:	Loss 0.1851	TrainAcc 0.9183	ValidAcc 0.5204	TestAcc 0.5341	BestValid 0.5571
	Epoch 3350:	Loss 0.1619	TrainAcc 0.9455	ValidAcc 0.5511	TestAcc 0.5677	BestValid 0.5571
	Epoch 3400:	Loss 0.1560	TrainAcc 0.9483	ValidAcc 0.5565	TestAcc 0.5639	BestValid 0.5571
	Epoch 3450:	Loss 0.1554	TrainAcc 0.9423	ValidAcc 0.5246	TestAcc 0.5379	BestValid 0.5571
	Epoch 3500:	Loss 0.1463	TrainAcc 0.9303	ValidAcc 0.5210	TestAcc 0.5418	BestValid 0.5571
	Epoch 3550:	Loss 0.1658	TrainAcc 0.9507	ValidAcc 0.5487	TestAcc 0.5629	BestValid 0.5571
	Epoch 3600:	Loss 0.1409	TrainAcc 0.9475	ValidAcc 0.5643	TestAcc 0.5716	BestValid 0.5643
	Epoch 3650:	Loss 0.1424	TrainAcc 0.9419	ValidAcc 0.5439	TestAcc 0.5620	BestValid 0.5643
	Epoch 3700:	Loss 0.1415	TrainAcc 0.9499	ValidAcc 0.5487	TestAcc 0.5591	BestValid 0.5643
	Epoch 3750:	Loss 0.1537	TrainAcc 0.9603	ValidAcc 0.5583	TestAcc 0.5677	BestValid 0.5643
	Epoch 3800:	Loss 0.1424	TrainAcc 0.9375	ValidAcc 0.5379	TestAcc 0.5466	BestValid 0.5643
	Epoch 3850:	Loss 0.1400	TrainAcc 0.9099	ValidAcc 0.5252	TestAcc 0.5312	BestValid 0.5643
	Epoch 3900:	Loss 0.1186	TrainAcc 0.9595	ValidAcc 0.5685	TestAcc 0.5658	BestValid 0.5685
	Epoch 3950:	Loss 0.1284	TrainAcc 0.9483	ValidAcc 0.5433	TestAcc 0.5572	BestValid 0.5685
	Epoch 4000:	Loss 0.1271	TrainAcc 0.9688	ValidAcc 0.5709	TestAcc 0.5706	BestValid 0.5709
	Epoch 4050:	Loss 0.1195	TrainAcc 0.9463	ValidAcc 0.5343	TestAcc 0.5562	BestValid 0.5709
	Epoch 4100:	Loss 0.1300	TrainAcc 0.9543	ValidAcc 0.5433	TestAcc 0.5504	BestValid 0.5709
	Epoch 4150:	Loss 0.1432	TrainAcc 0.9171	ValidAcc 0.5216	TestAcc 0.5370	BestValid 0.5709
	Epoch 4200:	Loss 0.1247	TrainAcc 0.9379	ValidAcc 0.5421	TestAcc 0.5648	BestValid 0.5709
	Epoch 4250:	Loss 0.1175	TrainAcc 0.9387	ValidAcc 0.5397	TestAcc 0.5572	BestValid 0.5709
	Epoch 4300:	Loss 0.1097	TrainAcc 0.9499	ValidAcc 0.5361	TestAcc 0.5514	BestValid 0.5709
	Epoch 4350:	Loss 0.1112	TrainAcc 0.9591	ValidAcc 0.5367	TestAcc 0.5706	BestValid 0.5709
	Epoch 4400:	Loss 0.1056	TrainAcc 0.9611	ValidAcc 0.5523	TestAcc 0.5600	BestValid 0.5709
	Epoch 4450:	Loss 0.1163	TrainAcc 0.9575	ValidAcc 0.5517	TestAcc 0.5696	BestValid 0.5709
	Epoch 4500:	Loss 0.1013	TrainAcc 0.9479	ValidAcc 0.5385	TestAcc 0.5581	BestValid 0.5709
	Epoch 4550:	Loss 0.1133	TrainAcc 0.9611	ValidAcc 0.5511	TestAcc 0.5658	BestValid 0.5709
	Epoch 4600:	Loss 0.0894	TrainAcc 0.9595	ValidAcc 0.5553	TestAcc 0.5677	BestValid 0.5709
	Epoch 4650:	Loss 0.1051	TrainAcc 0.9736	ValidAcc 0.5595	TestAcc 0.5841	BestValid 0.5709
	Epoch 4700:	Loss 0.0870	TrainAcc 0.9635	ValidAcc 0.5457	TestAcc 0.5620	BestValid 0.5709
	Epoch 4750:	Loss 0.0931	TrainAcc 0.9579	ValidAcc 0.5445	TestAcc 0.5629	BestValid 0.5709
	Epoch 4800:	Loss 0.0943	TrainAcc 0.9571	ValidAcc 0.5445	TestAcc 0.5533	BestValid 0.5709
	Epoch 4850:	Loss 0.1031	TrainAcc 0.9235	ValidAcc 0.5300	TestAcc 0.5456	BestValid 0.5709
	Epoch 4900:	Loss 0.1062	TrainAcc 0.9395	ValidAcc 0.5216	TestAcc 0.5427	BestValid 0.5709
	Epoch 4950:	Loss 0.0970	TrainAcc 0.9756	ValidAcc 0.5667	TestAcc 0.5735	BestValid 0.5709
	Epoch 5000:	Loss 0.1438	TrainAcc 0.9034	ValidAcc 0.5066	TestAcc 0.5235	BestValid 0.5709
****** Epoch Time (Excluding Evaluation Cost): 0.137 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 23.553 ms (Max: 24.416, Min: 22.174, Sum: 188.425)
Cluster-Wide Average, Compute: 91.190 ms (Max: 103.100, Min: 85.851, Sum: 729.519)
Cluster-Wide Average, Communication-Layer: 7.251 ms (Max: 8.165, Min: 5.371, Sum: 58.011)
Cluster-Wide Average, Bubble-Imbalance: 12.107 ms (Max: 18.296, Min: 2.679, Sum: 96.856)
Cluster-Wide Average, Communication-Graph: 0.513 ms (Max: 0.583, Min: 0.460, Sum: 4.101)
Cluster-Wide Average, Optimization: 1.033 ms (Max: 1.462, Min: 0.965, Sum: 8.261)
Cluster-Wide Average, Others: 0.905 ms (Max: 1.737, Min: 0.633, Sum: 7.240)
****** Breakdown Sum: 136.552 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.098 GB (Max: 3.555, Min: 2.979, Sum: 24.782)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 39.456 Gbps (Max: 44.083, Min: 30.980, Sum: 315.645)
Layer-level communication (cluster-wide, per-epoch): 0.271 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.271 GB
****** Accuracy Results ******
Highest valid_acc: 0.5709
Target test_acc: 0.5706
Epoch to reach the target acc: 3999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
