Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
        It takes 0.023 seconds.
        It takes 0.019 seconds.
        It takes 0.028 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.034 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.836 Gbps (per GPU), 486.685 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.524 Gbps (per GPU), 484.193 Gbps (aggregated)
The layer-level communication performance: 60.531 Gbps (per GPU), 484.251 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.255 Gbps (per GPU), 482.040 Gbps (aggregated)
The layer-level communication performance: 60.224 Gbps (per GPU), 481.795 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.985 Gbps (per GPU), 479.884 Gbps (aggregated)
The layer-level communication performance: 59.904 Gbps (per GPU), 479.228 Gbps (aggregated)
The layer-level communication performance: 59.937 Gbps (per GPU), 479.492 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.111 Gbps (per GPU), 1256.885 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.137 Gbps (per GPU), 1257.096 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.111 Gbps (per GPU), 1256.885 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.134 Gbps (per GPU), 1257.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.128 Gbps (per GPU), 1257.026 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.131 Gbps (per GPU), 1257.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.120 Gbps (per GPU), 1256.960 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.131 Gbps (per GPU), 1257.050 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.313 Gbps (per GPU), 818.507 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.313 Gbps (per GPU), 818.500 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.313 Gbps (per GPU), 818.501 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.305 Gbps (per GPU), 818.441 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.314 Gbps (per GPU), 818.508 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.313 Gbps (per GPU), 818.507 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.310 Gbps (per GPU), 818.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.284 Gbps (per GPU), 818.268 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.204 Gbps (per GPU), 257.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.202 Gbps (per GPU), 257.617 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.202 Gbps (per GPU), 257.618 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.202 Gbps (per GPU), 257.617 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.200 Gbps (per GPU), 257.600 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.199 Gbps (per GPU), 257.593 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.197 Gbps (per GPU), 257.575 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.198 Gbps (per GPU), 257.586 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.34ms  0.51ms  1.49  2.81K  0.03M
 chk_1  0.40ms  0.35ms  0.51ms  1.46  2.82K  0.03M
 chk_2  0.40ms  0.35ms  0.51ms  1.46  2.80K  0.03M
 chk_3  0.40ms  0.35ms  0.51ms  1.46  2.80K  0.03M
 chk_4  0.40ms  0.35ms  0.51ms  1.46  2.82K  0.03M
 chk_5  0.40ms  0.35ms  0.51ms  1.45  2.75K  0.03M
 chk_6  0.40ms  0.34ms  0.50ms  1.46  2.71K  0.03M
 chk_7  0.40ms  0.35ms  0.51ms  1.46  2.76K  0.03M
 chk_8  0.40ms  0.35ms  0.51ms  1.46  2.79K  0.03M
 chk_9  0.40ms  0.35ms  0.50ms  1.46  2.81K  0.03M
chk_10  0.41ms  0.35ms  0.50ms  1.46  2.81K  0.03M
chk_11  0.40ms  0.35ms  0.51ms  1.45  2.74K  0.03M
chk_12  0.40ms  0.35ms  0.51ms  1.46  2.76K  0.03M
chk_13  0.40ms  0.35ms  0.51ms  1.45  2.75K  0.03M
chk_14  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_15  0.40ms  0.35ms  0.50ms  1.46  2.77K  0.03M
chk_16  0.40ms  0.34ms  0.50ms  1.46  2.78K  0.03M
chk_17  0.40ms  0.35ms  0.51ms  1.46  2.79K  0.03M
chk_18  0.41ms  0.35ms  0.51ms  1.46  2.82K  0.03M
chk_19  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_20  0.40ms  0.35ms  0.51ms  1.46  2.77K  0.03M
chk_21  0.41ms  0.35ms  0.51ms  1.46  2.84K  0.02M
chk_22  0.40ms  0.35ms  0.51ms  1.46  2.78K  0.03M
chk_23  0.41ms  0.35ms  0.51ms  1.46  2.80K  0.03M
chk_24  0.40ms  0.35ms  0.50ms  1.46  2.80K  0.03M
chk_25  0.41ms  0.35ms  0.50ms  1.45  2.81K  0.03M
chk_26  0.41ms  0.35ms  0.51ms  1.46  2.81K  0.03M
chk_27  0.40ms  0.35ms  0.51ms  1.45  2.79K  0.03M
chk_28  0.40ms  0.35ms  0.51ms  1.45  2.77K  0.03M
chk_29  0.40ms  0.35ms  0.50ms  1.46  2.77K  0.03M
chk_30  0.41ms  0.35ms  0.51ms  1.46  2.80K  0.03M
chk_31  0.40ms  0.35ms  0.51ms  1.45  2.78K  0.03M
   Avg  0.40  0.35  0.51
   Max  0.41  0.35  0.51
   Min  0.40  0.34  0.50
 Ratio  1.03  1.03  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.594 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 55.594 ms
Partition 0 [0, 4) has cost: 46.264 ms
Partition 1 [4, 8) has cost: 44.475 ms
Partition 2 [8, 12) has cost: 44.475 ms
Partition 3 [12, 16) has cost: 44.475 ms
Partition 4 [16, 20) has cost: 44.475 ms
Partition 5 [20, 24) has cost: 44.475 ms
Partition 6 [24, 29) has cost: 55.594 ms
Partition 7 [29, 33) has cost: 49.578 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 28.283 ms
GPU 0, Compute+Comm Time: 20.243 ms, Bubble Time: 5.138 ms, Imbalance Overhead: 2.901 ms
GPU 1, Compute+Comm Time: 20.000 ms, Bubble Time: 5.029 ms, Imbalance Overhead: 3.254 ms
GPU 2, Compute+Comm Time: 20.000 ms, Bubble Time: 4.906 ms, Imbalance Overhead: 3.376 ms
GPU 3, Compute+Comm Time: 20.000 ms, Bubble Time: 4.786 ms, Imbalance Overhead: 3.497 ms
GPU 4, Compute+Comm Time: 20.000 ms, Bubble Time: 4.679 ms, Imbalance Overhead: 3.604 ms
GPU 5, Compute+Comm Time: 20.000 ms, Bubble Time: 4.569 ms, Imbalance Overhead: 3.714 ms
GPU 6, Compute+Comm Time: 23.826 ms, Bubble Time: 4.457 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 20.928 ms, Bubble Time: 4.539 ms, Imbalance Overhead: 2.815 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.991 ms
GPU 0, Compute+Comm Time: 38.039 ms, Bubble Time: 7.910 ms, Imbalance Overhead: 3.042 ms
GPU 1, Compute+Comm Time: 41.157 ms, Bubble Time: 7.834 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 33.865 ms, Bubble Time: 8.021 ms, Imbalance Overhead: 7.105 ms
GPU 3, Compute+Comm Time: 33.865 ms, Bubble Time: 8.212 ms, Imbalance Overhead: 6.914 ms
GPU 4, Compute+Comm Time: 33.865 ms, Bubble Time: 8.376 ms, Imbalance Overhead: 6.750 ms
GPU 5, Compute+Comm Time: 33.865 ms, Bubble Time: 8.562 ms, Imbalance Overhead: 6.564 ms
GPU 6, Compute+Comm Time: 33.865 ms, Bubble Time: 8.752 ms, Imbalance Overhead: 6.374 ms
GPU 7, Compute+Comm Time: 35.410 ms, Bubble Time: 8.940 ms, Imbalance Overhead: 4.641 ms
The estimated cost of the whole pipeline: 81.137 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 100.070 ms
Partition 0 [0, 8) has cost: 90.739 ms
Partition 1 [8, 16) has cost: 88.951 ms
Partition 2 [16, 25) has cost: 100.070 ms
Partition 3 [25, 33) has cost: 94.053 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.542 ms
GPU 0, Compute+Comm Time: 23.387 ms, Bubble Time: 4.773 ms, Imbalance Overhead: 2.382 ms
GPU 1, Compute+Comm Time: 23.723 ms, Bubble Time: 4.598 ms, Imbalance Overhead: 2.222 ms
GPU 2, Compute+Comm Time: 26.102 ms, Bubble Time: 4.440 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 24.188 ms, Bubble Time: 4.551 ms, Imbalance Overhead: 1.804 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 49.001 ms
GPU 0, Compute+Comm Time: 39.749 ms, Bubble Time: 7.315 ms, Imbalance Overhead: 1.937 ms
GPU 1, Compute+Comm Time: 41.776 ms, Bubble Time: 7.225 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 37.656 ms, Bubble Time: 7.450 ms, Imbalance Overhead: 3.895 ms
GPU 3, Compute+Comm Time: 37.970 ms, Bubble Time: 7.726 ms, Imbalance Overhead: 3.305 ms
    The estimated cost with 2 DP ways is 83.520 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 190.808 ms
Partition 0 [0, 17) has cost: 190.808 ms
Partition 1 [17, 33) has cost: 183.004 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.725 ms
GPU 0, Compute+Comm Time: 38.082 ms, Bubble Time: 4.643 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 37.289 ms, Bubble Time: 4.740 ms, Imbalance Overhead: 0.695 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 59.753 ms
GPU 0, Compute+Comm Time: 52.084 ms, Bubble Time: 6.625 ms, Imbalance Overhead: 1.045 ms
GPU 1, Compute+Comm Time: 53.258 ms, Bubble Time: 6.495 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 107.602 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 373.812 ms
Partition 0 [0, 33) has cost: 373.812 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 141.802 ms
GPU 0, Compute+Comm Time: 141.802 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 156.780 ms
GPU 0, Compute+Comm Time: 156.780 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 313.511 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2731	TrainAcc 0.4199	ValidAcc 0.4178	TestAcc 0.4207	BestValid 0.4178
	Epoch 50:	Loss 1.6186	TrainAcc 0.4429	ValidAcc 0.4449	TestAcc 0.4427	BestValid 0.4449
	Epoch 100:	Loss 1.5906	TrainAcc 0.4601	ValidAcc 0.4603	TestAcc 0.4609	BestValid 0.4603
	Epoch 150:	Loss 1.5687	TrainAcc 0.4454	ValidAcc 0.4459	TestAcc 0.4450	BestValid 0.4603
	Epoch 200:	Loss 1.5538	TrainAcc 0.4552	ValidAcc 0.4537	TestAcc 0.4542	BestValid 0.4603
	Epoch 250:	Loss 1.5468	TrainAcc 0.4647	ValidAcc 0.4623	TestAcc 0.4642	BestValid 0.4623
	Epoch 300:	Loss 1.5361	TrainAcc 0.4647	ValidAcc 0.4619	TestAcc 0.4639	BestValid 0.4623
	Epoch 350:	Loss 1.5254	TrainAcc 0.4880	ValidAcc 0.4864	TestAcc 0.4848	BestValid 0.4864
	Epoch 400:	Loss 1.5171	TrainAcc 0.4925	ValidAcc 0.4905	TestAcc 0.4884	BestValid 0.4905
	Epoch 450:	Loss 1.5023	TrainAcc 0.4648	ValidAcc 0.4569	TestAcc 0.4577	BestValid 0.4905
	Epoch 500:	Loss 1.4963	TrainAcc 0.4504	ValidAcc 0.4425	TestAcc 0.4409	BestValid 0.4905
	Epoch 550:	Loss 1.4880	TrainAcc 0.4161	ValidAcc 0.4091	TestAcc 0.4071	BestValid 0.4905
	Epoch 600:	Loss 1.4874	TrainAcc 0.3691	ValidAcc 0.3604	TestAcc 0.3606	BestValid 0.4905
	Epoch 650:	Loss 1.4832	TrainAcc 0.4741	ValidAcc 0.4665	TestAcc 0.4677	BestValid 0.4905
	Epoch 700:	Loss 1.4731	TrainAcc 0.4537	ValidAcc 0.4454	TestAcc 0.4449	BestValid 0.4905
	Epoch 750:	Loss 1.4745	TrainAcc 0.4188	ValidAcc 0.4086	TestAcc 0.4087	BestValid 0.4905
	Epoch 800:	Loss 1.4660	TrainAcc 0.4660	ValidAcc 0.4563	TestAcc 0.4560	BestValid 0.4905
	Epoch 850:	Loss 1.4664	TrainAcc 0.5076	ValidAcc 0.4965	TestAcc 0.4988	BestValid 0.4965
	Epoch 900:	Loss 1.4590	TrainAcc 0.4889	ValidAcc 0.4781	TestAcc 0.4803	BestValid 0.4965
	Epoch 950:	Loss 1.4566	TrainAcc 0.4791	ValidAcc 0.4706	TestAcc 0.4696	BestValid 0.4965
	Epoch 1000:	Loss 1.4568	TrainAcc 0.4654	ValidAcc 0.4572	TestAcc 0.4543	BestValid 0.4965
	Epoch 1050:	Loss 1.4549	TrainAcc 0.5135	ValidAcc 0.5069	TestAcc 0.5045	BestValid 0.5069
	Epoch 1100:	Loss 1.4500	TrainAcc 0.5171	ValidAcc 0.5081	TestAcc 0.5089	BestValid 0.5081
	Epoch 1150:	Loss 1.4466	TrainAcc 0.5176	ValidAcc 0.5090	TestAcc 0.5097	BestValid 0.5090
	Epoch 1200:	Loss 1.4487	TrainAcc 0.5154	ValidAcc 0.5061	TestAcc 0.5056	BestValid 0.5090
	Epoch 1250:	Loss 1.4456	TrainAcc 0.5193	ValidAcc 0.5105	TestAcc 0.5118	BestValid 0.5105
	Epoch 1300:	Loss 1.4424	TrainAcc 0.5173	ValidAcc 0.5074	TestAcc 0.5056	BestValid 0.5105
	Epoch 1350:	Loss 1.4416	TrainAcc 0.5186	ValidAcc 0.5099	TestAcc 0.5107	BestValid 0.5105
	Epoch 1400:	Loss 1.4371	TrainAcc 0.5199	ValidAcc 0.5117	TestAcc 0.5115	BestValid 0.5117
	Epoch 1450:	Loss 1.4355	TrainAcc 0.5229	ValidAcc 0.5136	TestAcc 0.5132	BestValid 0.5136
	Epoch 1500:	Loss 1.4357	TrainAcc 0.5203	ValidAcc 0.5114	TestAcc 0.5107	BestValid 0.5136
	Epoch 1550:	Loss 1.4312	TrainAcc 0.5178	ValidAcc 0.5052	TestAcc 0.5054	BestValid 0.5136
	Epoch 1600:	Loss 1.4347	TrainAcc 0.5239	ValidAcc 0.5140	TestAcc 0.5120	BestValid 0.5140
	Epoch 1650:	Loss 1.4317	TrainAcc 0.5214	ValidAcc 0.5121	TestAcc 0.5114	BestValid 0.5140
	Epoch 1700:	Loss 1.4285	TrainAcc 0.5183	ValidAcc 0.5047	TestAcc 0.5054	BestValid 0.5140
	Epoch 1750:	Loss 1.4288	TrainAcc 0.5203	ValidAcc 0.5073	TestAcc 0.5063	BestValid 0.5140
	Epoch 1800:	Loss 1.4243	TrainAcc 0.5098	ValidAcc 0.4970	TestAcc 0.4952	BestValid 0.5140
	Epoch 1850:	Loss 1.4274	TrainAcc 0.5139	ValidAcc 0.5009	TestAcc 0.5004	BestValid 0.5140
	Epoch 1900:	Loss 1.4225	TrainAcc 0.5212	ValidAcc 0.5082	TestAcc 0.5077	BestValid 0.5140
	Epoch 1950:	Loss 1.4219	TrainAcc 0.4982	ValidAcc 0.4829	TestAcc 0.4844	BestValid 0.5140
	Epoch 2000:	Loss 1.4181	TrainAcc 0.5158	ValidAcc 0.5021	TestAcc 0.5026	BestValid 0.5140
	Epoch 2050:	Loss 1.4174	TrainAcc 0.5247	ValidAcc 0.5125	TestAcc 0.5128	BestValid 0.5140
	Epoch 2100:	Loss 1.4177	TrainAcc 0.5239	ValidAcc 0.5107	TestAcc 0.5112	BestValid 0.5140
	Epoch 2150:	Loss 1.4211	TrainAcc 0.4996	ValidAcc 0.4883	TestAcc 0.4863	BestValid 0.5140
	Epoch 2200:	Loss 1.4165	TrainAcc 0.4973	ValidAcc 0.4859	TestAcc 0.4836	BestValid 0.5140
	Epoch 2250:	Loss 1.4143	TrainAcc 0.5158	ValidAcc 0.5007	TestAcc 0.5009	BestValid 0.5140
	Epoch 2300:	Loss 1.4128	TrainAcc 0.5217	ValidAcc 0.5056	TestAcc 0.5054	BestValid 0.5140
	Epoch 2350:	Loss 1.4123	TrainAcc 0.5160	ValidAcc 0.4988	TestAcc 0.4998	BestValid 0.5140
	Epoch 2400:	Loss 1.4100	TrainAcc 0.5052	ValidAcc 0.4905	TestAcc 0.4898	BestValid 0.5140
	Epoch 2450:	Loss 1.4081	TrainAcc 0.5109	ValidAcc 0.4962	TestAcc 0.4951	BestValid 0.5140
	Epoch 2500:	Loss 1.4060	TrainAcc 0.5234	ValidAcc 0.5068	TestAcc 0.5063	BestValid 0.5140
	Epoch 2550:	Loss 1.4043	TrainAcc 0.5155	ValidAcc 0.4971	TestAcc 0.4981	BestValid 0.5140
	Epoch 2600:	Loss 1.4029	TrainAcc 0.5142	ValidAcc 0.4985	TestAcc 0.4969	BestValid 0.5140
	Epoch 2650:	Loss 1.4021	TrainAcc 0.5163	ValidAcc 0.4999	TestAcc 0.4982	BestValid 0.5140
	Epoch 2700:	Loss 1.4047	TrainAcc 0.4977	ValidAcc 0.4845	TestAcc 0.4837	BestValid 0.5140
	Epoch 2750:	Loss 1.4017	TrainAcc 0.5161	ValidAcc 0.4989	TestAcc 0.4976	BestValid 0.5140
	Epoch 2800:	Loss 1.4053	TrainAcc 0.5120	ValidAcc 0.4948	TestAcc 0.4927	BestValid 0.5140
	Epoch 2850:	Loss 1.3987	TrainAcc 0.5150	ValidAcc 0.4971	TestAcc 0.4963	BestValid 0.5140
	Epoch 2900:	Loss 1.3967	TrainAcc 0.5201	ValidAcc 0.5016	TestAcc 0.5006	BestValid 0.5140
	Epoch 2950:	Loss 1.3951	TrainAcc 0.5219	ValidAcc 0.5026	TestAcc 0.5015	BestValid 0.5140
	Epoch 3000:	Loss 1.3974	TrainAcc 0.5199	ValidAcc 0.5011	TestAcc 0.4994	BestValid 0.5140
	Epoch 3050:	Loss 1.3921	TrainAcc 0.5064	ValidAcc 0.4908	TestAcc 0.4889	BestValid 0.5140
	Epoch 3100:	Loss 1.3895	TrainAcc 0.4825	ValidAcc 0.4691	TestAcc 0.4689	BestValid 0.5140
	Epoch 3150:	Loss 1.3888	TrainAcc 0.5185	ValidAcc 0.4986	TestAcc 0.4968	BestValid 0.5140
	Epoch 3200:	Loss 1.3867	TrainAcc 0.5232	ValidAcc 0.5025	TestAcc 0.5006	BestValid 0.5140
	Epoch 3250:	Loss 1.3835	TrainAcc 0.5120	ValidAcc 0.4951	TestAcc 0.4920	BestValid 0.5140
	Epoch 3300:	Loss 1.3856	TrainAcc 0.5270	ValidAcc 0.5048	TestAcc 0.5031	BestValid 0.5140
	Epoch 3350:	Loss 1.3858	TrainAcc 0.5165	ValidAcc 0.4975	TestAcc 0.4953	BestValid 0.5140
	Epoch 3400:	Loss 1.3813	TrainAcc 0.5340	ValidAcc 0.5121	TestAcc 0.5121	BestValid 0.5140
	Epoch 3450:	Loss 1.3819	TrainAcc 0.5323	ValidAcc 0.5100	TestAcc 0.5101	BestValid 0.5140
	Epoch 3500:	Loss 1.3797	TrainAcc 0.5356	ValidAcc 0.5118	TestAcc 0.5114	BestValid 0.5140
	Epoch 3550:	Loss 1.3802	TrainAcc 0.5220	ValidAcc 0.5008	TestAcc 0.4982	BestValid 0.5140
	Epoch 3600:	Loss 1.3747	TrainAcc 0.5331	ValidAcc 0.5092	TestAcc 0.5072	BestValid 0.5140
	Epoch 3650:	Loss 1.3779	TrainAcc 0.5227	ValidAcc 0.5006	TestAcc 0.4986	BestValid 0.5140
	Epoch 3700:	Loss 1.3718	TrainAcc 0.5326	ValidAcc 0.5081	TestAcc 0.5058	BestValid 0.5140
	Epoch 3750:	Loss 1.3716	TrainAcc 0.5336	ValidAcc 0.5092	TestAcc 0.5076	BestValid 0.5140
	Epoch 3800:	Loss 1.3723	TrainAcc 0.5254	ValidAcc 0.5013	TestAcc 0.4990	BestValid 0.5140
	Epoch 3850:	Loss 1.3697	TrainAcc 0.5183	ValidAcc 0.4953	TestAcc 0.4937	BestValid 0.5140
	Epoch 3900:	Loss 1.3711	TrainAcc 0.5370	ValidAcc 0.5108	TestAcc 0.5084	BestValid 0.5140
	Epoch 3950:	Loss 1.3708	TrainAcc 0.5276	ValidAcc 0.5026	TestAcc 0.5002	BestValid 0.5140
	Epoch 4000:	Loss 1.3667	TrainAcc 0.5377	ValidAcc 0.5118	TestAcc 0.5090	BestValid 0.5140
	Epoch 4050:	Loss 1.3690	TrainAcc 0.5338	ValidAcc 0.5066	TestAcc 0.5034	BestValid 0.5140
	Epoch 4100:	Loss 1.3621	TrainAcc 0.5317	ValidAcc 0.5044	TestAcc 0.5024	BestValid 0.5140
	Epoch 4150:	Loss 1.3603	TrainAcc 0.5480	ValidAcc 0.5196	TestAcc 0.5191	BestValid 0.5196
	Epoch 4200:	Loss 1.3584	TrainAcc 0.5367	ValidAcc 0.5069	TestAcc 0.5058	BestValid 0.5196
	Epoch 4250:	Loss 1.3633	TrainAcc 0.5417	ValidAcc 0.5128	TestAcc 0.5115	BestValid 0.5196
	Epoch 4300:	Loss 1.3604	TrainAcc 0.5474	ValidAcc 0.5183	TestAcc 0.5165	BestValid 0.5196
	Epoch 4350:	Loss 1.3574	TrainAcc 0.5471	ValidAcc 0.5155	TestAcc 0.5145	BestValid 0.5196
	Epoch 4400:	Loss 1.3573	TrainAcc 0.5347	ValidAcc 0.5059	TestAcc 0.5033	BestValid 0.5196
	Epoch 4450:	Loss 1.3592	TrainAcc 0.5364	ValidAcc 0.5045	TestAcc 0.5037	BestValid 0.5196
	Epoch 4500:	Loss 1.3546	TrainAcc 0.5428	ValidAcc 0.5105	TestAcc 0.5102	BestValid 0.5196
	Epoch 4550:	Loss 1.3543	TrainAcc 0.5508	ValidAcc 0.5169	TestAcc 0.5171	BestValid 0.5196
	Epoch 4600:	Loss 1.3542	TrainAcc 0.5512	ValidAcc 0.5177	TestAcc 0.5174	BestValid 0.5196
	Epoch 4650:	Loss 1.3524	TrainAcc 0.5455	ValidAcc 0.5114	TestAcc 0.5106	BestValid 0.5196
	Epoch 4700:	Loss 1.3527	TrainAcc 0.5585	ValidAcc 0.5255	TestAcc 0.5228	BestValid 0.5255
	Epoch 4750:	Loss 1.3448	TrainAcc 0.5423	ValidAcc 0.5080	TestAcc 0.5065	BestValid 0.5255
	Epoch 4800:	Loss 1.3500	TrainAcc 0.5501	ValidAcc 0.5130	TestAcc 0.5135	BestValid 0.5255
	Epoch 4850:	Loss 1.3442	TrainAcc 0.5320	ValidAcc 0.5008	TestAcc 0.5003	BestValid 0.5255
	Epoch 4900:	Loss 1.3501	TrainAcc 0.5347	ValidAcc 0.5025	TestAcc 0.5011	BestValid 0.5255
	Epoch 4950:	Loss 1.3452	TrainAcc 0.5415	ValidAcc 0.5069	TestAcc 0.5054	BestValid 0.5255
	Epoch 5000:	Loss 1.3450	TrainAcc 0.5517	ValidAcc 0.5147	TestAcc 0.5141	BestValid 0.5255
****** Epoch Time (Excluding Evaluation Cost): 0.117 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.265 ms (Max: 17.913, Min: 16.557, Sum: 138.122)
Cluster-Wide Average, Compute: 51.710 ms (Max: 56.466, Min: 47.617, Sum: 413.677)
Cluster-Wide Average, Communication-Layer: 11.356 ms (Max: 13.535, Min: 8.337, Sum: 90.845)
Cluster-Wide Average, Bubble-Imbalance: 5.576 ms (Max: 8.181, Min: 1.387, Sum: 44.610)
Cluster-Wide Average, Communication-Graph: 27.107 ms (Max: 28.177, Min: 26.002, Sum: 216.857)
Cluster-Wide Average, Optimization: 1.979 ms (Max: 2.687, Min: 1.618, Sum: 15.833)
Cluster-Wide Average, Others: 2.219 ms (Max: 3.729, Min: 1.455, Sum: 17.755)
****** Breakdown Sum: 117.212 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.547 GB (Max: 4.755, Min: 3.292, Sum: 28.380)
Cluster-Wide Average, Graph-Level Communication Throughput: 97.592 Gbps (Max: 103.412, Min: 89.953, Sum: 780.738)
Cluster-Wide Average, Layer-Level Communication Throughput: 37.347 Gbps (Max: 49.749, Min: 23.894, Sum: 298.779)
Layer-level communication (cluster-wide, per-epoch): 0.399 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 2.195 GB
****** Accuracy Results ******
Highest valid_acc: 0.5255
Target test_acc: 0.5228
Epoch to reach the target acc: 4699
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
