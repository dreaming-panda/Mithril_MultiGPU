Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.032 seconds.
Building the CSC structure...
        It takes 0.032 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.023 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.116 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.722 Gbps (per GPU), 485.773 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.429 Gbps (per GPU), 483.435 Gbps (aggregated)
The layer-level communication performance: 60.421 Gbps (per GPU), 483.366 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.127 Gbps (per GPU), 481.015 Gbps (aggregated)
The layer-level communication performance: 60.158 Gbps (per GPU), 481.261 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.916 Gbps (per GPU), 479.324 Gbps (aggregated)
The layer-level communication performance: 59.866 Gbps (per GPU), 478.931 Gbps (aggregated)
The layer-level communication performance: 59.834 Gbps (per GPU), 478.674 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.170 Gbps (per GPU), 1257.356 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.167 Gbps (per GPU), 1257.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.152 Gbps (per GPU), 1257.215 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.117 Gbps (per GPU), 1256.932 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.155 Gbps (per GPU), 1257.238 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.181 Gbps (per GPU), 1257.450 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.164 Gbps (per GPU), 1257.309 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.120 Gbps (per GPU), 1256.959 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 96.941 Gbps (per GPU), 775.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.953 Gbps (per GPU), 775.622 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.945 Gbps (per GPU), 775.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.944 Gbps (per GPU), 775.550 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.945 Gbps (per GPU), 775.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.924 Gbps (per GPU), 775.395 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.866 Gbps (per GPU), 774.930 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.927 Gbps (per GPU), 775.413 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.146 Gbps (per GPU), 257.164 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.143 Gbps (per GPU), 257.143 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.143 Gbps (per GPU), 257.145 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.141 Gbps (per GPU), 257.126 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.142 Gbps (per GPU), 257.140 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.139 Gbps (per GPU), 257.108 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.139 Gbps (per GPU), 257.110 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.137 Gbps (per GPU), 257.095 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.42ms  0.36ms  0.52ms  1.47  2.81K  0.03M
 chk_1  0.42ms  0.36ms  0.52ms  1.46  2.82K  0.03M
 chk_2  0.41ms  0.36ms  0.52ms  1.46  2.80K  0.03M
 chk_3  0.41ms  0.36ms  0.52ms  1.46  2.80K  0.03M
 chk_4  0.41ms  0.36ms  0.52ms  1.46  2.82K  0.03M
 chk_5  0.41ms  0.36ms  0.53ms  1.46  2.75K  0.03M
 chk_6  0.41ms  0.36ms  0.52ms  1.46  2.71K  0.03M
 chk_7  0.41ms  0.36ms  0.52ms  1.46  2.76K  0.03M
 chk_8  0.41ms  0.36ms  0.52ms  1.46  2.79K  0.03M
 chk_9  0.41ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_10  0.41ms  0.36ms  0.52ms  1.45  2.81K  0.03M
chk_11  0.41ms  0.36ms  0.53ms  1.44  2.74K  0.03M
chk_12  0.41ms  0.36ms  0.53ms  1.46  2.76K  0.03M
chk_13  0.41ms  0.36ms  0.52ms  1.45  2.75K  0.03M
chk_14  0.41ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_15  0.41ms  0.36ms  0.52ms  1.46  2.77K  0.03M
chk_16  0.41ms  0.36ms  0.52ms  1.46  2.78K  0.03M
chk_17  0.41ms  0.36ms  0.53ms  1.46  2.79K  0.03M
chk_18  0.42ms  0.36ms  0.53ms  1.46  2.82K  0.03M
chk_19  0.41ms  0.35ms  0.52ms  1.47  2.81K  0.03M
chk_20  0.41ms  0.36ms  0.53ms  1.46  2.77K  0.03M
chk_21  0.42ms  0.36ms  0.52ms  1.45  2.84K  0.02M
chk_22  0.41ms  0.36ms  0.52ms  1.46  2.78K  0.03M
chk_23  0.41ms  0.36ms  0.52ms  1.46  2.80K  0.03M
chk_24  0.41ms  0.36ms  0.52ms  1.45  2.80K  0.03M
chk_25  0.41ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_26  0.41ms  0.36ms  0.52ms  1.46  2.81K  0.03M
chk_27  0.41ms  0.37ms  0.53ms  1.43  2.79K  0.03M
chk_28  0.41ms  0.36ms  0.52ms  1.45  2.77K  0.03M
chk_29  0.41ms  0.36ms  0.52ms  1.47  2.77K  0.03M
chk_30  0.41ms  0.36ms  0.53ms  1.46  2.80K  0.03M
chk_31  0.41ms  0.36ms  0.53ms  1.45  2.78K  0.03M
   Avg  0.41  0.36  0.52
   Max  0.42  0.37  0.53
   Min  0.41  0.35  0.52
 Ratio  1.03  1.05  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.609 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 57.429 ms
Partition 0 [0, 4) has cost: 47.652 ms
Partition 1 [4, 8) has cost: 45.943 ms
Partition 2 [8, 12) has cost: 45.943 ms
Partition 3 [12, 16) has cost: 45.943 ms
Partition 4 [16, 20) has cost: 45.943 ms
Partition 5 [20, 24) has cost: 45.943 ms
Partition 6 [24, 29) has cost: 57.429 ms
Partition 7 [29, 33) has cost: 51.201 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 29.122 ms
GPU 0, Compute+Comm Time: 20.776 ms, Bubble Time: 5.344 ms, Imbalance Overhead: 3.001 ms
GPU 1, Compute+Comm Time: 20.575 ms, Bubble Time: 5.228 ms, Imbalance Overhead: 3.318 ms
GPU 2, Compute+Comm Time: 20.575 ms, Bubble Time: 5.060 ms, Imbalance Overhead: 3.486 ms
GPU 3, Compute+Comm Time: 20.575 ms, Bubble Time: 4.938 ms, Imbalance Overhead: 3.609 ms
GPU 4, Compute+Comm Time: 20.575 ms, Bubble Time: 4.830 ms, Imbalance Overhead: 3.717 ms
GPU 5, Compute+Comm Time: 20.575 ms, Bubble Time: 4.711 ms, Imbalance Overhead: 3.836 ms
GPU 6, Compute+Comm Time: 24.543 ms, Bubble Time: 4.579 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 21.572 ms, Bubble Time: 4.659 ms, Imbalance Overhead: 2.890 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.321 ms
GPU 0, Compute+Comm Time: 39.035 ms, Bubble Time: 8.113 ms, Imbalance Overhead: 3.173 ms
GPU 1, Compute+Comm Time: 42.293 ms, Bubble Time: 8.028 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 34.775 ms, Bubble Time: 8.218 ms, Imbalance Overhead: 7.328 ms
GPU 3, Compute+Comm Time: 34.775 ms, Bubble Time: 8.413 ms, Imbalance Overhead: 7.133 ms
GPU 4, Compute+Comm Time: 34.775 ms, Bubble Time: 8.593 ms, Imbalance Overhead: 6.953 ms
GPU 5, Compute+Comm Time: 34.775 ms, Bubble Time: 8.787 ms, Imbalance Overhead: 6.759 ms
GPU 6, Compute+Comm Time: 34.775 ms, Bubble Time: 8.976 ms, Imbalance Overhead: 6.569 ms
GPU 7, Compute+Comm Time: 36.283 ms, Bubble Time: 9.168 ms, Imbalance Overhead: 4.870 ms
The estimated cost of the whole pipeline: 83.415 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 103.372 ms
Partition 0 [0, 8) has cost: 93.596 ms
Partition 1 [8, 16) has cost: 91.887 ms
Partition 2 [16, 25) has cost: 103.372 ms
Partition 3 [25, 33) has cost: 97.144 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.369 ms
GPU 0, Compute+Comm Time: 23.984 ms, Bubble Time: 4.926 ms, Imbalance Overhead: 2.460 ms
GPU 1, Compute+Comm Time: 24.348 ms, Bubble Time: 4.747 ms, Imbalance Overhead: 2.274 ms
GPU 2, Compute+Comm Time: 26.804 ms, Bubble Time: 4.566 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 24.843 ms, Bubble Time: 4.662 ms, Imbalance Overhead: 1.865 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.168 ms
GPU 0, Compute+Comm Time: 40.675 ms, Bubble Time: 7.505 ms, Imbalance Overhead: 1.988 ms
GPU 1, Compute+Comm Time: 42.771 ms, Bubble Time: 7.397 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 38.542 ms, Bubble Time: 7.616 ms, Imbalance Overhead: 4.011 ms
GPU 3, Compute+Comm Time: 38.843 ms, Bubble Time: 7.888 ms, Imbalance Overhead: 3.438 ms
    The estimated cost with 2 DP ways is 85.614 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 196.968 ms
Partition 0 [0, 17) has cost: 196.968 ms
Partition 1 [17, 33) has cost: 189.030 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 44.650 ms
GPU 0, Compute+Comm Time: 39.741 ms, Bubble Time: 4.869 ms, Imbalance Overhead: 0.040 ms
GPU 1, Compute+Comm Time: 38.937 ms, Bubble Time: 4.927 ms, Imbalance Overhead: 0.786 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.864 ms
GPU 0, Compute+Comm Time: 53.950 ms, Bubble Time: 6.874 ms, Imbalance Overhead: 1.040 ms
GPU 1, Compute+Comm Time: 55.155 ms, Bubble Time: 6.709 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 111.840 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 385.998 ms
Partition 0 [0, 33) has cost: 385.998 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 142.833 ms
GPU 0, Compute+Comm Time: 142.833 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 158.007 ms
GPU 0, Compute+Comm Time: 158.007 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 315.881 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
Num Stages: 4 / 4
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9951	TrainAcc 0.4221	ValidAcc 0.4246	TestAcc 0.4233	BestValid 0.4246
	Epoch 50:	Loss 1.6069	TrainAcc 0.4741	ValidAcc 0.4757	TestAcc 0.4750	BestValid 0.4757
	Epoch 100:	Loss 1.5758	TrainAcc 0.4413	ValidAcc 0.4426	TestAcc 0.4423	BestValid 0.4757
	Epoch 150:	Loss 1.5527	TrainAcc 0.4704	ValidAcc 0.4698	TestAcc 0.4695	BestValid 0.4757
	Epoch 200:	Loss 1.5394	TrainAcc 0.4641	ValidAcc 0.4607	TestAcc 0.4572	BestValid 0.4757
	Epoch 250:	Loss 1.5228	TrainAcc 0.4550	ValidAcc 0.4502	TestAcc 0.4465	BestValid 0.4757
	Epoch 300:	Loss 1.5074	TrainAcc 0.4975	ValidAcc 0.4936	TestAcc 0.4953	BestValid 0.4936
	Epoch 350:	Loss 1.4923	TrainAcc 0.3563	ValidAcc 0.3492	TestAcc 0.3491	BestValid 0.4936
	Epoch 400:	Loss 1.4843	TrainAcc 0.3477	ValidAcc 0.3408	TestAcc 0.3419	BestValid 0.4936
	Epoch 450:	Loss 1.4794	TrainAcc 0.3702	ValidAcc 0.3594	TestAcc 0.3600	BestValid 0.4936
	Epoch 500:	Loss 1.4695	TrainAcc 0.4005	ValidAcc 0.3872	TestAcc 0.3885	BestValid 0.4936
	Epoch 550:	Loss 1.4660	TrainAcc 0.4824	ValidAcc 0.4692	TestAcc 0.4715	BestValid 0.4936
	Epoch 600:	Loss 1.4573	TrainAcc 0.4597	ValidAcc 0.4494	TestAcc 0.4479	BestValid 0.4936
	Epoch 650:	Loss 1.4539	TrainAcc 0.5070	ValidAcc 0.4972	TestAcc 0.4972	BestValid 0.4972
	Epoch 700:	Loss 1.4516	TrainAcc 0.4951	ValidAcc 0.4834	TestAcc 0.4847	BestValid 0.4972
	Epoch 750:	Loss 1.4461	TrainAcc 0.4951	ValidAcc 0.4818	TestAcc 0.4844	BestValid 0.4972
	Epoch 800:	Loss 1.4442	TrainAcc 0.4824	ValidAcc 0.4718	TestAcc 0.4722	BestValid 0.4972
	Epoch 850:	Loss 1.4449	TrainAcc 0.5102	ValidAcc 0.4999	TestAcc 0.5005	BestValid 0.4999
	Epoch 900:	Loss 1.4399	TrainAcc 0.5074	ValidAcc 0.4951	TestAcc 0.4966	BestValid 0.4999
	Epoch 950:	Loss 1.4375	TrainAcc 0.5196	ValidAcc 0.5076	TestAcc 0.5088	BestValid 0.5076
	Epoch 1000:	Loss 1.4357	TrainAcc 0.5186	ValidAcc 0.5064	TestAcc 0.5044	BestValid 0.5076
	Epoch 1050:	Loss 1.4326	TrainAcc 0.5194	ValidAcc 0.5068	TestAcc 0.5054	BestValid 0.5076
	Epoch 1100:	Loss 1.4332	TrainAcc 0.5212	ValidAcc 0.5091	TestAcc 0.5085	BestValid 0.5091
	Epoch 1150:	Loss 1.4318	TrainAcc 0.5224	ValidAcc 0.5099	TestAcc 0.5094	BestValid 0.5099
	Epoch 1200:	Loss 1.4265	TrainAcc 0.5025	ValidAcc 0.4875	TestAcc 0.4897	BestValid 0.5099
	Epoch 1250:	Loss 1.4268	TrainAcc 0.5240	ValidAcc 0.5122	TestAcc 0.5104	BestValid 0.5122
	Epoch 1300:	Loss 1.4230	TrainAcc 0.5175	ValidAcc 0.5012	TestAcc 0.5011	BestValid 0.5122
	Epoch 1350:	Loss 1.4231	TrainAcc 0.5215	ValidAcc 0.5063	TestAcc 0.5056	BestValid 0.5122
	Epoch 1400:	Loss 1.4214	TrainAcc 0.5136	ValidAcc 0.4975	TestAcc 0.4966	BestValid 0.5122
	Epoch 1450:	Loss 1.4179	TrainAcc 0.5273	ValidAcc 0.5129	TestAcc 0.5114	BestValid 0.5129
	Epoch 1500:	Loss 1.4168	TrainAcc 0.5211	ValidAcc 0.5030	TestAcc 0.5028	BestValid 0.5129
	Epoch 1550:	Loss 1.4145	TrainAcc 0.5203	ValidAcc 0.5028	TestAcc 0.5024	BestValid 0.5129
	Epoch 1600:	Loss 1.4142	TrainAcc 0.5189	ValidAcc 0.4996	TestAcc 0.5004	BestValid 0.5129
	Epoch 1650:	Loss 1.4127	TrainAcc 0.5257	ValidAcc 0.5098	TestAcc 0.5084	BestValid 0.5129
	Epoch 1700:	Loss 1.4107	TrainAcc 0.5239	ValidAcc 0.5069	TestAcc 0.5065	BestValid 0.5129
	Epoch 1750:	Loss 1.4121	TrainAcc 0.5270	ValidAcc 0.5090	TestAcc 0.5080	BestValid 0.5129
	Epoch 1800:	Loss 1.4055	TrainAcc 0.5324	ValidAcc 0.5142	TestAcc 0.5133	BestValid 0.5142
	Epoch 1850:	Loss 1.4066	TrainAcc 0.5114	ValidAcc 0.4949	TestAcc 0.4942	BestValid 0.5142
	Epoch 1900:	Loss 1.4016	TrainAcc 0.4940	ValidAcc 0.4815	TestAcc 0.4796	BestValid 0.5142
	Epoch 1950:	Loss 1.4012	TrainAcc 0.5089	ValidAcc 0.4920	TestAcc 0.4927	BestValid 0.5142
	Epoch 2000:	Loss 1.4027	TrainAcc 0.5184	ValidAcc 0.4991	TestAcc 0.4991	BestValid 0.5142
	Epoch 2050:	Loss 1.3988	TrainAcc 0.4910	ValidAcc 0.4758	TestAcc 0.4754	BestValid 0.5142
	Epoch 2100:	Loss 1.3992	TrainAcc 0.5101	ValidAcc 0.4918	TestAcc 0.4929	BestValid 0.5142
	Epoch 2150:	Loss 1.4002	TrainAcc 0.5112	ValidAcc 0.4926	TestAcc 0.4930	BestValid 0.5142
	Epoch 2200:	Loss 1.3986	TrainAcc 0.5308	ValidAcc 0.5094	TestAcc 0.5090	BestValid 0.5142
	Epoch 2250:	Loss 1.4005	TrainAcc 0.5122	ValidAcc 0.4926	TestAcc 0.4926	BestValid 0.5142
	Epoch 2300:	Loss 1.3961	TrainAcc 0.5103	ValidAcc 0.4904	TestAcc 0.4912	BestValid 0.5142
	Epoch 2350:	Loss 1.3932	TrainAcc 0.4907	ValidAcc 0.4726	TestAcc 0.4729	BestValid 0.5142
	Epoch 2400:	Loss 1.3919	TrainAcc 0.5115	ValidAcc 0.4913	TestAcc 0.4914	BestValid 0.5142
	Epoch 2450:	Loss 1.3889	TrainAcc 0.5177	ValidAcc 0.4944	TestAcc 0.4949	BestValid 0.5142
	Epoch 2500:	Loss 1.3880	TrainAcc 0.5306	ValidAcc 0.5050	TestAcc 0.5037	BestValid 0.5142
	Epoch 2550:	Loss 1.3880	TrainAcc 0.5310	ValidAcc 0.5062	TestAcc 0.5071	BestValid 0.5142
	Epoch 2600:	Loss 1.3890	TrainAcc 0.4934	ValidAcc 0.4748	TestAcc 0.4748	BestValid 0.5142
	Epoch 2650:	Loss 1.3864	TrainAcc 0.5078	ValidAcc 0.4874	TestAcc 0.4867	BestValid 0.5142
	Epoch 2700:	Loss 1.3855	TrainAcc 0.5090	ValidAcc 0.4877	TestAcc 0.4874	BestValid 0.5142
	Epoch 2750:	Loss 1.3813	TrainAcc 0.4996	ValidAcc 0.4803	TestAcc 0.4796	BestValid 0.5142
	Epoch 2800:	Loss 1.3807	TrainAcc 0.5194	ValidAcc 0.4933	TestAcc 0.4937	BestValid 0.5142
	Epoch 2850:	Loss 1.3800	TrainAcc 0.4840	ValidAcc 0.4662	TestAcc 0.4656	BestValid 0.5142
	Epoch 2900:	Loss 1.3778	TrainAcc 0.5157	ValidAcc 0.4922	TestAcc 0.4917	BestValid 0.5142
	Epoch 2950:	Loss 1.3760	TrainAcc 0.5304	ValidAcc 0.5018	TestAcc 0.5027	BestValid 0.5142
	Epoch 3000:	Loss 1.3751	TrainAcc 0.5019	ValidAcc 0.4795	TestAcc 0.4798	BestValid 0.5142
	Epoch 3050:	Loss 1.3763	TrainAcc 0.5026	ValidAcc 0.4782	TestAcc 0.4772	BestValid 0.5142
	Epoch 3100:	Loss 1.3776	TrainAcc 0.5139	ValidAcc 0.4894	TestAcc 0.4888	BestValid 0.5142
	Epoch 3150:	Loss 1.3746	TrainAcc 0.5145	ValidAcc 0.4871	TestAcc 0.4860	BestValid 0.5142
	Epoch 3200:	Loss 1.3725	TrainAcc 0.5060	ValidAcc 0.4819	TestAcc 0.4814	BestValid 0.5142
	Epoch 3250:	Loss 1.3716	TrainAcc 0.5155	ValidAcc 0.4905	TestAcc 0.4898	BestValid 0.5142
	Epoch 3300:	Loss 1.3712	TrainAcc 0.4949	ValidAcc 0.4729	TestAcc 0.4724	BestValid 0.5142
	Epoch 3350:	Loss 1.3678	TrainAcc 0.5367	ValidAcc 0.5031	TestAcc 0.5038	BestValid 0.5142
	Epoch 3400:	Loss 1.3642	TrainAcc 0.5255	ValidAcc 0.4947	TestAcc 0.4952	BestValid 0.5142
	Epoch 3450:	Loss 1.3669	TrainAcc 0.5229	ValidAcc 0.4939	TestAcc 0.4945	BestValid 0.5142
	Epoch 3500:	Loss 1.3670	TrainAcc 0.5077	ValidAcc 0.4819	TestAcc 0.4803	BestValid 0.5142
	Epoch 3550:	Loss 1.3653	TrainAcc 0.5008	ValidAcc 0.4766	TestAcc 0.4777	BestValid 0.5142
	Epoch 3600:	Loss 1.3652	TrainAcc 0.4991	ValidAcc 0.4754	TestAcc 0.4760	BestValid 0.5142
	Epoch 3650:	Loss 1.3665	TrainAcc 0.4973	ValidAcc 0.4712	TestAcc 0.4717	BestValid 0.5142
	Epoch 3700:	Loss 1.3614	TrainAcc 0.5112	ValidAcc 0.4814	TestAcc 0.4807	BestValid 0.5142
	Epoch 3750:	Loss 1.3638	TrainAcc 0.4985	ValidAcc 0.4715	TestAcc 0.4717	BestValid 0.5142
	Epoch 3800:	Loss 1.3576	TrainAcc 0.5117	ValidAcc 0.4824	TestAcc 0.4825	BestValid 0.5142
	Epoch 3850:	Loss 1.3647	TrainAcc 0.5163	ValidAcc 0.4876	TestAcc 0.4873	BestValid 0.5142
	Epoch 3900:	Loss 1.3569	TrainAcc 0.4848	ValidAcc 0.4644	TestAcc 0.4639	BestValid 0.5142
	Epoch 3950:	Loss 1.3579	TrainAcc 0.4841	ValidAcc 0.4634	TestAcc 0.4630	BestValid 0.5142
	Epoch 4000:	Loss 1.3604	TrainAcc 0.5127	ValidAcc 0.4848	TestAcc 0.4846	BestValid 0.5142
	Epoch 4050:	Loss 1.3573	TrainAcc 0.5174	ValidAcc 0.4852	TestAcc 0.4864	BestValid 0.5142
	Epoch 4100:	Loss 1.3532	TrainAcc 0.5362	ValidAcc 0.4994	TestAcc 0.4971	BestValid 0.5142
	Epoch 4150:	Loss 1.3536	TrainAcc 0.5127	ValidAcc 0.4818	TestAcc 0.4813	BestValid 0.5142
	Epoch 4200:	Loss 1.3521	TrainAcc 0.5052	ValidAcc 0.4754	TestAcc 0.4735	BestValid 0.5142
	Epoch 4250:	Loss 1.3500	TrainAcc 0.4787	ValidAcc 0.4563	TestAcc 0.4562	BestValid 0.5142
	Epoch 4300:	Loss 1.3493	TrainAcc 0.4914	ValidAcc 0.4680	TestAcc 0.4676	BestValid 0.5142
	Epoch 4350:	Loss 1.3509	TrainAcc 0.5258	ValidAcc 0.4917	TestAcc 0.4921	BestValid 0.5142
	Epoch 4400:	Loss 1.3503	TrainAcc 0.5144	ValidAcc 0.4814	TestAcc 0.4792	BestValid 0.5142
	Epoch 4450:	Loss 1.3438	TrainAcc 0.5345	ValidAcc 0.4977	TestAcc 0.4986	BestValid 0.5142
	Epoch 4500:	Loss 1.3492	TrainAcc 0.5199	ValidAcc 0.4861	TestAcc 0.4840	BestValid 0.5142
	Epoch 4550:	Loss 1.3492	TrainAcc 0.5241	ValidAcc 0.4856	TestAcc 0.4850	BestValid 0.5142
	Epoch 4600:	Loss 1.3426	TrainAcc 0.5374	ValidAcc 0.4995	TestAcc 0.4960	BestValid 0.5142
	Epoch 4650:	Loss 1.3437	TrainAcc 0.5153	ValidAcc 0.4813	TestAcc 0.4795	BestValid 0.5142
	Epoch 4700:	Loss 1.3450	TrainAcc 0.4877	ValidAcc 0.4607	TestAcc 0.4592	BestValid 0.5142
	Epoch 4750:	Loss 1.3449	TrainAcc 0.5320	ValidAcc 0.4947	TestAcc 0.4937	BestValid 0.5142
	Epoch 4800:	Loss 1.3398	TrainAcc 0.5452	ValidAcc 0.5036	TestAcc 0.5021	BestValid 0.5142
	Epoch 4850:	Loss 1.3411	TrainAcc 0.5248	ValidAcc 0.4885	TestAcc 0.4877	BestValid 0.5142
	Epoch 4900:	Loss 1.3373	TrainAcc 0.5204	ValidAcc 0.4844	TestAcc 0.4840	BestValid 0.5142
	Epoch 4950:	Loss 1.3381	TrainAcc 0.5298	ValidAcc 0.4922	TestAcc 0.4932	BestValid 0.5142
	Epoch 5000:	Loss 1.3399	TrainAcc 0.5253	ValidAcc 0.4886	TestAcc 0.4890	BestValid 0.5142
****** Epoch Time (Excluding Evaluation Cost): 0.117 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.212 ms (Max: 17.806, Min: 16.255, Sum: 137.693)
Cluster-Wide Average, Compute: 51.631 ms (Max: 56.741, Min: 48.760, Sum: 413.046)
Cluster-Wide Average, Communication-Layer: 11.452 ms (Max: 13.548, Min: 8.578, Sum: 91.614)
Cluster-Wide Average, Bubble-Imbalance: 6.053 ms (Max: 8.103, Min: 1.492, Sum: 48.427)
Cluster-Wide Average, Communication-Graph: 27.137 ms (Max: 29.190, Min: 26.172, Sum: 217.097)
Cluster-Wide Average, Optimization: 1.948 ms (Max: 2.612, Min: 1.572, Sum: 15.586)
Cluster-Wide Average, Others: 2.207 ms (Max: 3.702, Min: 1.465, Sum: 17.653)
****** Breakdown Sum: 117.640 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.547 GB (Max: 4.755, Min: 3.292, Sum: 28.380)
Cluster-Wide Average, Graph-Level Communication Throughput: 97.158 Gbps (Max: 100.205, Min: 88.274, Sum: 777.261)
Cluster-Wide Average, Layer-Level Communication Throughput: 36.977 Gbps (Max: 49.344, Min: 23.886, Sum: 295.817)
Layer-level communication (cluster-wide, per-epoch): 0.399 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 2.195 GB
****** Accuracy Results ******
Highest valid_acc: 0.5142
Target test_acc: 0.5133
Epoch to reach the target acc: 1799
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
