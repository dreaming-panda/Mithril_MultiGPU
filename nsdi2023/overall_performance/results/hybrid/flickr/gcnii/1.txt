Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.018 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.022 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.117 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.312 Gbps (per GPU), 458.499 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.063 Gbps (per GPU), 456.501 Gbps (aggregated)
The layer-level communication performance: 57.066 Gbps (per GPU), 456.530 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.812 Gbps (per GPU), 454.498 Gbps (aggregated)
The layer-level communication performance: 56.843 Gbps (per GPU), 454.743 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.632 Gbps (per GPU), 453.053 Gbps (aggregated)
The layer-level communication performance: 56.586 Gbps (per GPU), 452.686 Gbps (aggregated)
The layer-level communication performance: 56.557 Gbps (per GPU), 452.453 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 124.336 Gbps (per GPU), 994.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 124.432 Gbps (per GPU), 995.459 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 124.340 Gbps (per GPU), 994.721 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 124.371 Gbps (per GPU), 994.972 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 124.349 Gbps (per GPU), 994.795 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 124.384 Gbps (per GPU), 995.075 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 124.336 Gbps (per GPU), 994.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 124.324 Gbps (per GPU), 994.589 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.310 Gbps (per GPU), 818.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.311 Gbps (per GPU), 818.487 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.309 Gbps (per GPU), 818.474 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.308 Gbps (per GPU), 818.461 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.308 Gbps (per GPU), 818.468 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.301 Gbps (per GPU), 818.407 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.310 Gbps (per GPU), 818.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.279 Gbps (per GPU), 818.234 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.463 Gbps (per GPU), 267.705 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.472 Gbps (per GPU), 267.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.464 Gbps (per GPU), 267.713 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.462 Gbps (per GPU), 267.694 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.465 Gbps (per GPU), 267.716 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.465 Gbps (per GPU), 267.716 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.460 Gbps (per GPU), 267.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.462 Gbps (per GPU), 267.694 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
 chk_1  0.40ms  0.34ms  0.50ms  1.46  2.82K  0.03M
 chk_2  0.40ms  0.34ms  0.50ms  1.46  2.80K  0.03M
 chk_3  0.40ms  0.34ms  0.50ms  1.46  2.80K  0.03M
 chk_4  0.40ms  0.34ms  0.50ms  1.46  2.82K  0.03M
 chk_5  0.40ms  0.35ms  0.51ms  1.45  2.75K  0.03M
 chk_6  0.40ms  0.34ms  0.50ms  1.46  2.71K  0.03M
 chk_7  0.40ms  0.34ms  0.50ms  1.46  2.76K  0.03M
 chk_8  0.40ms  0.34ms  0.50ms  1.46  2.79K  0.03M
 chk_9  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_10  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_11  0.40ms  0.35ms  0.51ms  1.45  2.74K  0.03M
chk_12  0.40ms  0.35ms  0.50ms  1.46  2.76K  0.03M
chk_13  0.40ms  0.35ms  0.50ms  1.45  2.75K  0.03M
chk_14  0.40ms  0.34ms  0.50ms  1.45  2.81K  0.03M
chk_15  0.40ms  0.34ms  0.50ms  1.46  2.77K  0.03M
chk_16  0.40ms  0.34ms  0.50ms  1.46  2.78K  0.03M
chk_17  0.40ms  0.35ms  0.51ms  1.45  2.79K  0.03M
chk_18  0.41ms  0.35ms  0.51ms  1.45  2.82K  0.03M
chk_19  0.40ms  0.34ms  0.50ms  1.47  2.81K  0.03M
chk_20  0.40ms  0.35ms  0.50ms  1.45  2.77K  0.03M
chk_21  0.41ms  0.35ms  0.50ms  1.45  2.84K  0.02M
chk_22  0.40ms  0.34ms  0.50ms  1.46  2.78K  0.03M
chk_23  0.40ms  0.34ms  0.50ms  1.46  2.80K  0.03M
chk_24  0.40ms  0.34ms  0.50ms  1.46  2.80K  0.03M
chk_25  0.40ms  0.34ms  0.50ms  1.47  2.81K  0.03M
chk_26  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_27  0.40ms  0.35ms  0.51ms  1.45  2.79K  0.03M
chk_28  0.40ms  0.35ms  0.50ms  1.46  2.77K  0.03M
chk_29  0.40ms  0.34ms  0.50ms  1.46  2.77K  0.03M
chk_30  0.41ms  0.35ms  0.51ms  1.46  2.80K  0.03M
chk_31  0.40ms  0.35ms  0.51ms  1.45  2.78K  0.03M
   Avg  0.40  0.34  0.50
   Max  0.41  0.35  0.51
   Min  0.40  0.34  0.50
 Ratio  1.03  1.04  1.02
   Var  0.00  0.00  0.00
Profiling takes 0.589 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 55.064 ms
Partition 0 [0, 4) has cost: 45.922 ms
Partition 1 [4, 8) has cost: 44.051 ms
Partition 2 [8, 12) has cost: 44.051 ms
Partition 3 [12, 16) has cost: 44.051 ms
Partition 4 [16, 20) has cost: 44.051 ms
Partition 5 [20, 24) has cost: 44.051 ms
Partition 6 [24, 29) has cost: 55.064 ms
Partition 7 [29, 33) has cost: 49.105 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 28.405 ms
GPU 0, Compute+Comm Time: 20.397 ms, Bubble Time: 5.154 ms, Imbalance Overhead: 2.854 ms
GPU 1, Compute+Comm Time: 20.125 ms, Bubble Time: 5.047 ms, Imbalance Overhead: 3.233 ms
GPU 2, Compute+Comm Time: 20.125 ms, Bubble Time: 4.930 ms, Imbalance Overhead: 3.349 ms
GPU 3, Compute+Comm Time: 20.125 ms, Bubble Time: 4.818 ms, Imbalance Overhead: 3.461 ms
GPU 4, Compute+Comm Time: 20.125 ms, Bubble Time: 4.716 ms, Imbalance Overhead: 3.564 ms
GPU 5, Compute+Comm Time: 20.125 ms, Bubble Time: 4.607 ms, Imbalance Overhead: 3.673 ms
GPU 6, Compute+Comm Time: 23.911 ms, Bubble Time: 4.494 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 21.056 ms, Bubble Time: 4.577 ms, Imbalance Overhead: 2.772 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.971 ms
GPU 0, Compute+Comm Time: 38.015 ms, Bubble Time: 7.928 ms, Imbalance Overhead: 3.028 ms
GPU 1, Compute+Comm Time: 41.120 ms, Bubble Time: 7.852 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 33.892 ms, Bubble Time: 8.039 ms, Imbalance Overhead: 7.040 ms
GPU 3, Compute+Comm Time: 33.892 ms, Bubble Time: 8.221 ms, Imbalance Overhead: 6.858 ms
GPU 4, Compute+Comm Time: 33.892 ms, Bubble Time: 8.384 ms, Imbalance Overhead: 6.695 ms
GPU 5, Compute+Comm Time: 33.892 ms, Bubble Time: 8.569 ms, Imbalance Overhead: 6.510 ms
GPU 6, Compute+Comm Time: 33.892 ms, Bubble Time: 8.755 ms, Imbalance Overhead: 6.324 ms
GPU 7, Compute+Comm Time: 35.492 ms, Bubble Time: 8.938 ms, Imbalance Overhead: 4.541 ms
The estimated cost of the whole pipeline: 81.245 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 99.115 ms
Partition 0 [0, 8) has cost: 89.973 ms
Partition 1 [8, 16) has cost: 88.102 ms
Partition 2 [16, 25) has cost: 99.115 ms
Partition 3 [25, 33) has cost: 93.156 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.914 ms
GPU 0, Compute+Comm Time: 24.359 ms, Bubble Time: 4.990 ms, Imbalance Overhead: 2.564 ms
GPU 1, Compute+Comm Time: 24.799 ms, Bubble Time: 4.801 ms, Imbalance Overhead: 2.314 ms
GPU 2, Compute+Comm Time: 27.276 ms, Bubble Time: 4.638 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 25.265 ms, Bubble Time: 4.754 ms, Imbalance Overhead: 1.894 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.262 ms
GPU 0, Compute+Comm Time: 40.707 ms, Bubble Time: 7.510 ms, Imbalance Overhead: 2.044 ms
GPU 1, Compute+Comm Time: 42.851 ms, Bubble Time: 7.412 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 38.643 ms, Bubble Time: 7.657 ms, Imbalance Overhead: 3.962 ms
GPU 3, Compute+Comm Time: 38.863 ms, Bubble Time: 7.932 ms, Imbalance Overhead: 3.467 ms
    The estimated cost with 2 DP ways is 86.285 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 189.088 ms
Partition 0 [0, 17) has cost: 189.088 ms
Partition 1 [17, 33) has cost: 181.258 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.837 ms
GPU 0, Compute+Comm Time: 38.176 ms, Bubble Time: 4.660 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 37.387 ms, Bubble Time: 4.748 ms, Imbalance Overhead: 0.702 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 59.825 ms
GPU 0, Compute+Comm Time: 52.126 ms, Bubble Time: 6.625 ms, Imbalance Overhead: 1.075 ms
GPU 1, Compute+Comm Time: 53.313 ms, Bubble Time: 6.512 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 107.795 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 370.346 ms
Partition 0 [0, 33) has cost: 370.346 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 136.826 ms
GPU 0, Compute+Comm Time: 136.826 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 151.795 ms
GPU 0, Compute+Comm Time: 151.795 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 303.052 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9451	TrainAcc 0.4173	ValidAcc 0.4197	TestAcc 0.4201	BestValid 0.4197
	Epoch 50:	Loss 1.6109	TrainAcc 0.4627	ValidAcc 0.4624	TestAcc 0.4628	BestValid 0.4624
	Epoch 100:	Loss 1.5716	TrainAcc 0.4344	ValidAcc 0.4351	TestAcc 0.4361	BestValid 0.4624
	Epoch 150:	Loss 1.5438	TrainAcc 0.4579	ValidAcc 0.4568	TestAcc 0.4568	BestValid 0.4624
	Epoch 200:	Loss 1.5267	TrainAcc 0.4916	ValidAcc 0.4875	TestAcc 0.4877	BestValid 0.4875
	Epoch 250:	Loss 1.5100	TrainAcc 0.4789	ValidAcc 0.4766	TestAcc 0.4753	BestValid 0.4875
	Epoch 300:	Loss 1.4939	TrainAcc 0.4552	ValidAcc 0.4477	TestAcc 0.4470	BestValid 0.4875
	Epoch 350:	Loss 1.4810	TrainAcc 0.3260	ValidAcc 0.3203	TestAcc 0.3165	BestValid 0.4875
	Epoch 400:	Loss 1.4760	TrainAcc 0.3677	ValidAcc 0.3584	TestAcc 0.3586	BestValid 0.4875
	Epoch 450:	Loss 1.4680	TrainAcc 0.3822	ValidAcc 0.3707	TestAcc 0.3699	BestValid 0.4875
	Epoch 500:	Loss 1.4632	TrainAcc 0.4986	ValidAcc 0.4889	TestAcc 0.4908	BestValid 0.4889
	Epoch 550:	Loss 1.4633	TrainAcc 0.4426	ValidAcc 0.4307	TestAcc 0.4321	BestValid 0.4889
	Epoch 600:	Loss 1.4550	TrainAcc 0.4895	ValidAcc 0.4812	TestAcc 0.4806	BestValid 0.4889
	Epoch 650:	Loss 1.4578	TrainAcc 0.4414	ValidAcc 0.4275	TestAcc 0.4302	BestValid 0.4889
	Epoch 700:	Loss 1.4542	TrainAcc 0.4829	ValidAcc 0.4728	TestAcc 0.4730	BestValid 0.4889
	Epoch 750:	Loss 1.4454	TrainAcc 0.4906	ValidAcc 0.4788	TestAcc 0.4794	BestValid 0.4889
	Epoch 800:	Loss 1.4429	TrainAcc 0.5061	ValidAcc 0.4946	TestAcc 0.4941	BestValid 0.4946
	Epoch 850:	Loss 1.4455	TrainAcc 0.5157	ValidAcc 0.5038	TestAcc 0.5060	BestValid 0.5038
	Epoch 900:	Loss 1.4411	TrainAcc 0.5135	ValidAcc 0.5004	TestAcc 0.5011	BestValid 0.5038
	Epoch 950:	Loss 1.4367	TrainAcc 0.5190	ValidAcc 0.5069	TestAcc 0.5103	BestValid 0.5069
	Epoch 1000:	Loss 1.4365	TrainAcc 0.5190	ValidAcc 0.5058	TestAcc 0.5073	BestValid 0.5069
	Epoch 1050:	Loss 1.4369	TrainAcc 0.5136	ValidAcc 0.4996	TestAcc 0.5000	BestValid 0.5069
	Epoch 1100:	Loss 1.4309	TrainAcc 0.5201	ValidAcc 0.5063	TestAcc 0.5104	BestValid 0.5069
	Epoch 1150:	Loss 1.4315	TrainAcc 0.5233	ValidAcc 0.5099	TestAcc 0.5096	BestValid 0.5099
	Epoch 1200:	Loss 1.4280	TrainAcc 0.5257	ValidAcc 0.5109	TestAcc 0.5141	BestValid 0.5109
	Epoch 1250:	Loss 1.4281	TrainAcc 0.5245	ValidAcc 0.5097	TestAcc 0.5123	BestValid 0.5109
	Epoch 1300:	Loss 1.4250	TrainAcc 0.5256	ValidAcc 0.5105	TestAcc 0.5129	BestValid 0.5109
	Epoch 1350:	Loss 1.4239	TrainAcc 0.5256	ValidAcc 0.5108	TestAcc 0.5119	BestValid 0.5109
	Epoch 1400:	Loss 1.4217	TrainAcc 0.5264	ValidAcc 0.5119	TestAcc 0.5130	BestValid 0.5119
	Epoch 1450:	Loss 1.4218	TrainAcc 0.5116	ValidAcc 0.4937	TestAcc 0.4967	BestValid 0.5119
	Epoch 1500:	Loss 1.4166	TrainAcc 0.5258	ValidAcc 0.5099	TestAcc 0.5112	BestValid 0.5119
	Epoch 1550:	Loss 1.4162	TrainAcc 0.5280	ValidAcc 0.5111	TestAcc 0.5140	BestValid 0.5119
	Epoch 1600:	Loss 1.4169	TrainAcc 0.5249	ValidAcc 0.5084	TestAcc 0.5102	BestValid 0.5119
	Epoch 1650:	Loss 1.4185	TrainAcc 0.5283	ValidAcc 0.5106	TestAcc 0.5116	BestValid 0.5119
	Epoch 1700:	Loss 1.4130	TrainAcc 0.5296	ValidAcc 0.5119	TestAcc 0.5140	BestValid 0.5119
	Epoch 1750:	Loss 1.4128	TrainAcc 0.5333	ValidAcc 0.5160	TestAcc 0.5170	BestValid 0.5160
	Epoch 1800:	Loss 1.4120	TrainAcc 0.5305	ValidAcc 0.5118	TestAcc 0.5121	BestValid 0.5160
	Epoch 1850:	Loss 1.4097	TrainAcc 0.5336	ValidAcc 0.5130	TestAcc 0.5150	BestValid 0.5160
	Epoch 1900:	Loss 1.4069	TrainAcc 0.5247	ValidAcc 0.5052	TestAcc 0.5068	BestValid 0.5160
	Epoch 1950:	Loss 1.4049	TrainAcc 0.5246	ValidAcc 0.5056	TestAcc 0.5068	BestValid 0.5160
	Epoch 2000:	Loss 1.4041	TrainAcc 0.5237	ValidAcc 0.5057	TestAcc 0.5053	BestValid 0.5160
	Epoch 2050:	Loss 1.4061	TrainAcc 0.5355	ValidAcc 0.5138	TestAcc 0.5152	BestValid 0.5160
	Epoch 2100:	Loss 1.4052	TrainAcc 0.5358	ValidAcc 0.5151	TestAcc 0.5145	BestValid 0.5160
	Epoch 2150:	Loss 1.4003	TrainAcc 0.5219	ValidAcc 0.5047	TestAcc 0.5028	BestValid 0.5160
	Epoch 2200:	Loss 1.3994	TrainAcc 0.5307	ValidAcc 0.5098	TestAcc 0.5097	BestValid 0.5160
	Epoch 2250:	Loss 1.3993	TrainAcc 0.5332	ValidAcc 0.5102	TestAcc 0.5121	BestValid 0.5160
	Epoch 2300:	Loss 1.3935	TrainAcc 0.5214	ValidAcc 0.5006	TestAcc 0.5001	BestValid 0.5160
	Epoch 2350:	Loss 1.3969	TrainAcc 0.5305	ValidAcc 0.5108	TestAcc 0.5104	BestValid 0.5160
	Epoch 2400:	Loss 1.3950	TrainAcc 0.5329	ValidAcc 0.5086	TestAcc 0.5093	BestValid 0.5160
	Epoch 2450:	Loss 1.3911	TrainAcc 0.5232	ValidAcc 0.5035	TestAcc 0.5013	BestValid 0.5160
	Epoch 2500:	Loss 1.3905	TrainAcc 0.5346	ValidAcc 0.5099	TestAcc 0.5103	BestValid 0.5160
	Epoch 2550:	Loss 1.3887	TrainAcc 0.5310	ValidAcc 0.5089	TestAcc 0.5076	BestValid 0.5160
	Epoch 2600:	Loss 1.3847	TrainAcc 0.5354	ValidAcc 0.5071	TestAcc 0.5084	BestValid 0.5160
	Epoch 2650:	Loss 1.3883	TrainAcc 0.5287	ValidAcc 0.5066	TestAcc 0.5052	BestValid 0.5160
	Epoch 2700:	Loss 1.3827	TrainAcc 0.5156	ValidAcc 0.4946	TestAcc 0.4943	BestValid 0.5160
	Epoch 2750:	Loss 1.3804	TrainAcc 0.5255	ValidAcc 0.5028	TestAcc 0.5024	BestValid 0.5160
	Epoch 2800:	Loss 1.3837	TrainAcc 0.5152	ValidAcc 0.4912	TestAcc 0.4920	BestValid 0.5160
	Epoch 2850:	Loss 1.3797	TrainAcc 0.5224	ValidAcc 0.4985	TestAcc 0.4978	BestValid 0.5160
	Epoch 2900:	Loss 1.3791	TrainAcc 0.5254	ValidAcc 0.4996	TestAcc 0.4997	BestValid 0.5160
	Epoch 2950:	Loss 1.3769	TrainAcc 0.5421	ValidAcc 0.5130	TestAcc 0.5132	BestValid 0.5160
	Epoch 3000:	Loss 1.3747	TrainAcc 0.5347	ValidAcc 0.5060	TestAcc 0.5065	BestValid 0.5160
	Epoch 3050:	Loss 1.3800	TrainAcc 0.5267	ValidAcc 0.4995	TestAcc 0.4977	BestValid 0.5160
	Epoch 3100:	Loss 1.3769	TrainAcc 0.5254	ValidAcc 0.4982	TestAcc 0.4981	BestValid 0.5160
	Epoch 3150:	Loss 1.3753	TrainAcc 0.5301	ValidAcc 0.5019	TestAcc 0.5006	BestValid 0.5160
	Epoch 3200:	Loss 1.3688	TrainAcc 0.5375	ValidAcc 0.5099	TestAcc 0.5094	BestValid 0.5160
	Epoch 3250:	Loss 1.3715	TrainAcc 0.5423	ValidAcc 0.5107	TestAcc 0.5106	BestValid 0.5160
	Epoch 3300:	Loss 1.3702	TrainAcc 0.5230	ValidAcc 0.4966	TestAcc 0.4967	BestValid 0.5160
	Epoch 3350:	Loss 1.3693	TrainAcc 0.5459	ValidAcc 0.5134	TestAcc 0.5132	BestValid 0.5160
	Epoch 3400:	Loss 1.3703	TrainAcc 0.5280	ValidAcc 0.4983	TestAcc 0.4973	BestValid 0.5160
	Epoch 3450:	Loss 1.3669	TrainAcc 0.5316	ValidAcc 0.5014	TestAcc 0.5008	BestValid 0.5160
	Epoch 3500:	Loss 1.3669	TrainAcc 0.5174	ValidAcc 0.4918	TestAcc 0.4930	BestValid 0.5160
	Epoch 3550:	Loss 1.3661	TrainAcc 0.5352	ValidAcc 0.5040	TestAcc 0.5032	BestValid 0.5160
	Epoch 3600:	Loss 1.3618	TrainAcc 0.5409	ValidAcc 0.5055	TestAcc 0.5060	BestValid 0.5160
	Epoch 3650:	Loss 1.3591	TrainAcc 0.5405	ValidAcc 0.5068	TestAcc 0.5062	BestValid 0.5160
	Epoch 3700:	Loss 1.3603	TrainAcc 0.5352	ValidAcc 0.5048	TestAcc 0.5037	BestValid 0.5160
	Epoch 3750:	Loss 1.3597	TrainAcc 0.5395	ValidAcc 0.5069	TestAcc 0.5053	BestValid 0.5160
	Epoch 3800:	Loss 1.3596	TrainAcc 0.5362	ValidAcc 0.5009	TestAcc 0.4992	BestValid 0.5160
	Epoch 3850:	Loss 1.3541	TrainAcc 0.5182	ValidAcc 0.4878	TestAcc 0.4891	BestValid 0.5160
	Epoch 3900:	Loss 1.3569	TrainAcc 0.5299	ValidAcc 0.4981	TestAcc 0.4980	BestValid 0.5160
	Epoch 3950:	Loss 1.3551	TrainAcc 0.5483	ValidAcc 0.5115	TestAcc 0.5106	BestValid 0.5160
	Epoch 4000:	Loss 1.3536	TrainAcc 0.5389	ValidAcc 0.5017	TestAcc 0.5005	BestValid 0.5160
	Epoch 4050:	Loss 1.3534	TrainAcc 0.5198	ValidAcc 0.4875	TestAcc 0.4879	BestValid 0.5160
	Epoch 4100:	Loss 1.3525	TrainAcc 0.5334	ValidAcc 0.4968	TestAcc 0.4969	BestValid 0.5160
	Epoch 4150:	Loss 1.3520	TrainAcc 0.5347	ValidAcc 0.5005	TestAcc 0.4998	BestValid 0.5160
	Epoch 4200:	Loss 1.3537	TrainAcc 0.5496	ValidAcc 0.5117	TestAcc 0.5097	BestValid 0.5160
	Epoch 4250:	Loss 1.3485	TrainAcc 0.5357	ValidAcc 0.4992	TestAcc 0.4990	BestValid 0.5160
	Epoch 4300:	Loss 1.3450	TrainAcc 0.5319	ValidAcc 0.4949	TestAcc 0.4954	BestValid 0.5160
	Epoch 4350:	Loss 1.3428	TrainAcc 0.5415	ValidAcc 0.5039	TestAcc 0.5028	BestValid 0.5160
	Epoch 4400:	Loss 1.3489	TrainAcc 0.5278	ValidAcc 0.4944	TestAcc 0.4938	BestValid 0.5160
	Epoch 4450:	Loss 1.3431	TrainAcc 0.5310	ValidAcc 0.4954	TestAcc 0.4945	BestValid 0.5160
	Epoch 4500:	Loss 1.3480	TrainAcc 0.5360	ValidAcc 0.4991	TestAcc 0.4977	BestValid 0.5160
	Epoch 4550:	Loss 1.3430	TrainAcc 0.5458	ValidAcc 0.5060	TestAcc 0.5040	BestValid 0.5160
	Epoch 4600:	Loss 1.3422	TrainAcc 0.5362	ValidAcc 0.4970	TestAcc 0.4972	BestValid 0.5160
	Epoch 4650:	Loss 1.3444	TrainAcc 0.5476	ValidAcc 0.5080	TestAcc 0.5065	BestValid 0.5160
	Epoch 4700:	Loss 1.3411	TrainAcc 0.5606	ValidAcc 0.5158	TestAcc 0.5136	BestValid 0.5160
	Epoch 4750:	Loss 1.3395	TrainAcc 0.5374	ValidAcc 0.4996	TestAcc 0.4989	BestValid 0.5160
	Epoch 4800:	Loss 1.3421	TrainAcc 0.5579	ValidAcc 0.5156	TestAcc 0.5123	BestValid 0.5160
	Epoch 4850:	Loss 1.3358	TrainAcc 0.5361	ValidAcc 0.4959	TestAcc 0.4957	BestValid 0.5160
	Epoch 4900:	Loss 1.3372	TrainAcc 0.5492	ValidAcc 0.5067	TestAcc 0.5047	BestValid 0.5160
	Epoch 4950:	Loss 1.3421	TrainAcc 0.5474	ValidAcc 0.5044	TestAcc 0.5039	BestValid 0.5160
	Epoch 5000:	Loss 1.3365	TrainAcc 0.5616	ValidAcc 0.5148	TestAcc 0.5129	BestValid 0.5160
****** Epoch Time (Excluding Evaluation Cost): 0.114 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 16.771 ms (Max: 17.493, Min: 16.169, Sum: 134.167)
Cluster-Wide Average, Compute: 51.282 ms (Max: 54.502, Min: 47.939, Sum: 410.253)
Cluster-Wide Average, Communication-Layer: 11.451 ms (Max: 13.425, Min: 8.569, Sum: 91.609)
Cluster-Wide Average, Bubble-Imbalance: 4.348 ms (Max: 7.554, Min: 1.344, Sum: 34.784)
Cluster-Wide Average, Communication-Graph: 26.673 ms (Max: 27.504, Min: 26.213, Sum: 213.383)
Cluster-Wide Average, Optimization: 1.908 ms (Max: 2.506, Min: 1.590, Sum: 15.268)
Cluster-Wide Average, Others: 2.133 ms (Max: 3.567, Min: 1.452, Sum: 17.066)
****** Breakdown Sum: 114.566 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.547 GB (Max: 4.755, Min: 3.292, Sum: 28.380)
Cluster-Wide Average, Graph-Level Communication Throughput: 99.080 Gbps (Max: 101.607, Min: 97.201, Sum: 792.639)
Cluster-Wide Average, Layer-Level Communication Throughput: 37.021 Gbps (Max: 49.312, Min: 23.629, Sum: 296.170)
Layer-level communication (cluster-wide, per-epoch): 0.399 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 2.195 GB
****** Accuracy Results ******
Highest valid_acc: 0.5160
Target test_acc: 0.5170
Epoch to reach the target acc: 1749
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
