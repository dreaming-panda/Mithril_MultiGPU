Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.020 seconds.
        It takes 0.019 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Feature Vector...
        It takes 0.025 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.009 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.458 Gbps (per GPU), 483.660 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.161 Gbps (per GPU), 481.291 Gbps (aggregated)
The layer-level communication performance: 60.156 Gbps (per GPU), 481.246 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.904 Gbps (per GPU), 479.234 Gbps (aggregated)
The layer-level communication performance: 59.872 Gbps (per GPU), 478.978 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.662 Gbps (per GPU), 477.293 Gbps (aggregated)
The layer-level communication performance: 59.608 Gbps (per GPU), 476.868 Gbps (aggregated)
The layer-level communication performance: 59.577 Gbps (per GPU), 476.618 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.058 Gbps (per GPU), 1256.461 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.037 Gbps (per GPU), 1256.297 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.993 Gbps (per GPU), 1255.944 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.037 Gbps (per GPU), 1256.297 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.072 Gbps (per GPU), 1256.578 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.052 Gbps (per GPU), 1256.414 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.008 Gbps (per GPU), 1256.062 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.052 Gbps (per GPU), 1256.414 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.609 Gbps (per GPU), 812.869 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.608 Gbps (per GPU), 812.863 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.609 Gbps (per GPU), 812.875 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.608 Gbps (per GPU), 812.863 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.605 Gbps (per GPU), 812.842 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.605 Gbps (per GPU), 812.836 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.608 Gbps (per GPU), 812.863 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.604 Gbps (per GPU), 812.830 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.209 Gbps (per GPU), 273.673 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.209 Gbps (per GPU), 273.675 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.208 Gbps (per GPU), 273.666 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.208 Gbps (per GPU), 273.668 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.205 Gbps (per GPU), 273.636 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.205 Gbps (per GPU), 273.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.206 Gbps (per GPU), 273.648 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.206 Gbps (per GPU), 273.648 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.31ms  0.26ms  0.21ms  1.47  2.81K  0.03M
 chk_1  0.31ms  0.26ms  0.21ms  1.47  2.82K  0.03M
 chk_2  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
 chk_3  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
 chk_4  0.31ms  0.26ms  0.21ms  1.46  2.82K  0.03M
 chk_5  0.32ms  0.26ms  0.22ms  1.44  2.75K  0.03M
 chk_6  0.30ms  0.25ms  0.21ms  1.46  2.71K  0.03M
 chk_7  0.31ms  0.26ms  0.21ms  1.47  2.76K  0.03M
 chk_8  0.31ms  0.26ms  0.21ms  1.45  2.79K  0.03M
 chk_9  0.31ms  0.26ms  0.21ms  1.46  2.81K  0.03M
chk_10  0.31ms  0.25ms  0.21ms  1.47  2.81K  0.03M
chk_11  0.31ms  0.26ms  0.22ms  1.44  2.74K  0.03M
chk_12  0.31ms  0.26ms  0.22ms  1.45  2.76K  0.03M
chk_13  0.31ms  0.26ms  0.21ms  1.46  2.75K  0.03M
chk_14  0.31ms  0.26ms  0.21ms  1.47  2.81K  0.03M
chk_15  0.31ms  0.26ms  0.21ms  1.46  2.77K  0.03M
chk_16  0.31ms  0.26ms  0.21ms  1.47  2.78K  0.03M
chk_17  0.31ms  0.26ms  0.21ms  1.45  2.79K  0.03M
chk_18  0.31ms  0.26ms  0.21ms  1.47  2.82K  0.03M
chk_19  0.30ms  0.25ms  0.21ms  1.47  2.81K  0.03M
chk_20  0.31ms  0.26ms  0.21ms  1.46  2.77K  0.03M
chk_21  0.31ms  0.26ms  0.21ms  1.47  2.84K  0.02M
chk_22  0.31ms  0.26ms  0.21ms  1.45  2.78K  0.03M
chk_23  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
chk_24  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
chk_25  0.31ms  0.25ms  0.21ms  1.48  2.81K  0.03M
chk_26  0.30ms  0.25ms  0.21ms  1.47  2.81K  0.03M
chk_27  0.31ms  0.26ms  0.21ms  1.46  2.79K  0.03M
chk_28  0.31ms  0.26ms  0.21ms  1.46  2.77K  0.03M
chk_29  0.31ms  0.26ms  0.21ms  1.45  2.77K  0.03M
chk_30  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
chk_31  0.31ms  0.26ms  0.21ms  1.47  2.78K  0.03M
   Avg  0.31  0.26  0.21
   Max  0.32  0.26  0.22
   Min  0.30  0.25  0.21
 Ratio  1.04  1.05  1.06
   Var  0.00  0.00  0.00
Profiling takes 0.380 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 34.572 ms
Partition 0 [0, 4) has cost: 34.572 ms
Partition 1 [4, 8) has cost: 32.909 ms
Partition 2 [8, 12) has cost: 32.909 ms
Partition 3 [12, 16) has cost: 32.909 ms
Partition 4 [16, 20) has cost: 32.909 ms
Partition 5 [20, 24) has cost: 32.909 ms
Partition 6 [24, 28) has cost: 32.909 ms
Partition 7 [28, 32) has cost: 31.451 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 20.806 ms
GPU 0, Compute+Comm Time: 17.239 ms, Bubble Time: 3.567 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 16.417 ms, Bubble Time: 3.593 ms, Imbalance Overhead: 0.796 ms
GPU 2, Compute+Comm Time: 16.417 ms, Bubble Time: 3.623 ms, Imbalance Overhead: 0.766 ms
GPU 3, Compute+Comm Time: 16.417 ms, Bubble Time: 3.650 ms, Imbalance Overhead: 0.738 ms
GPU 4, Compute+Comm Time: 16.417 ms, Bubble Time: 3.675 ms, Imbalance Overhead: 0.714 ms
GPU 5, Compute+Comm Time: 16.417 ms, Bubble Time: 3.701 ms, Imbalance Overhead: 0.687 ms
GPU 6, Compute+Comm Time: 16.417 ms, Bubble Time: 3.732 ms, Imbalance Overhead: 0.656 ms
GPU 7, Compute+Comm Time: 15.273 ms, Bubble Time: 3.783 ms, Imbalance Overhead: 1.749 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 32.481 ms
GPU 0, Compute+Comm Time: 25.626 ms, Bubble Time: 5.877 ms, Imbalance Overhead: 0.978 ms
GPU 1, Compute+Comm Time: 25.940 ms, Bubble Time: 5.850 ms, Imbalance Overhead: 0.690 ms
GPU 2, Compute+Comm Time: 25.940 ms, Bubble Time: 5.810 ms, Imbalance Overhead: 0.731 ms
GPU 3, Compute+Comm Time: 25.940 ms, Bubble Time: 5.781 ms, Imbalance Overhead: 0.759 ms
GPU 4, Compute+Comm Time: 25.940 ms, Bubble Time: 5.756 ms, Imbalance Overhead: 0.785 ms
GPU 5, Compute+Comm Time: 25.940 ms, Bubble Time: 5.734 ms, Imbalance Overhead: 0.806 ms
GPU 6, Compute+Comm Time: 25.940 ms, Bubble Time: 5.708 ms, Imbalance Overhead: 0.832 ms
GPU 7, Compute+Comm Time: 26.781 ms, Bubble Time: 5.687 ms, Imbalance Overhead: 0.013 ms
The estimated cost of the whole pipeline: 55.951 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 67.481 ms
Partition 0 [0, 8) has cost: 67.481 ms
Partition 1 [8, 16) has cost: 65.819 ms
Partition 2 [16, 24) has cost: 65.819 ms
Partition 3 [24, 32) has cost: 64.360 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 24.264 ms
GPU 0, Compute+Comm Time: 20.531 ms, Bubble Time: 3.729 ms, Imbalance Overhead: 0.004 ms
GPU 1, Compute+Comm Time: 20.117 ms, Bubble Time: 3.753 ms, Imbalance Overhead: 0.394 ms
GPU 2, Compute+Comm Time: 20.117 ms, Bubble Time: 3.793 ms, Imbalance Overhead: 0.354 ms
GPU 3, Compute+Comm Time: 19.548 ms, Bubble Time: 3.870 ms, Imbalance Overhead: 0.846 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 35.737 ms
GPU 0, Compute+Comm Time: 29.549 ms, Bubble Time: 5.640 ms, Imbalance Overhead: 0.548 ms
GPU 1, Compute+Comm Time: 29.707 ms, Bubble Time: 5.601 ms, Imbalance Overhead: 0.428 ms
GPU 2, Compute+Comm Time: 29.707 ms, Bubble Time: 5.565 ms, Imbalance Overhead: 0.465 ms
GPU 3, Compute+Comm Time: 30.126 ms, Bubble Time: 5.551 ms, Imbalance Overhead: 0.060 ms
    The estimated cost with 2 DP ways is 63.001 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 133.300 ms
Partition 0 [0, 16) has cost: 133.300 ms
Partition 1 [16, 32) has cost: 130.179 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 37.906 ms
GPU 0, Compute+Comm Time: 33.773 ms, Bubble Time: 4.134 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 33.281 ms, Bubble Time: 4.212 ms, Imbalance Overhead: 0.413 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.840 ms
GPU 0, Compute+Comm Time: 43.140 ms, Bubble Time: 5.404 ms, Imbalance Overhead: 0.296 ms
GPU 1, Compute+Comm Time: 43.428 ms, Bubble Time: 5.357 ms, Imbalance Overhead: 0.054 ms
    The estimated cost with 4 DP ways is 91.084 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 263.479 ms
Partition 0 [0, 32) has cost: 263.479 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 130.132 ms
GPU 0, Compute+Comm Time: 130.132 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 140.008 ms
GPU 0, Compute+Comm Time: 140.008 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 283.646 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 121)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [81, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 81)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [121, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [41, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [121, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 41)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 41)...
+++++++++ Node 7 initializing the weights for op[121, 160)...
+++++++++ Node 2 initializing the weights for op[41, 81)...
+++++++++ Node 5 initializing the weights for op[81, 121)...
+++++++++ Node 3 initializing the weights for op[41, 81)...
+++++++++ Node 6 initializing the weights for op[121, 160)...
+++++++++ Node 0 initializing the weights for op[0, 41)...
+++++++++ Node 4 initializing the weights for op[81, 121)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9459	TrainAcc 0.0545	ValidAcc 0.0549	TestAcc 0.0559	BestValid 0.0549
	Epoch 50:	Loss 1.8167	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6300	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.6203	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.6155	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.6211	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.6120	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.6168	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.6076	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5950	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5953	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5847	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5846	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5859	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5841	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5811	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5737	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5715	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5731	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 950:	Loss 1.5672	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5711	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5671	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5601	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5627	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5665	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5618	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5601	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5625	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5598	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5591	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5560	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5591	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5581	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5588	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5571	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5576	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5560	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5545	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5545	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1950:	Loss 1.5516	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2000:	Loss 1.5569	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5539	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5528	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5528	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5542	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5534	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5548	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5554	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2400:	Loss 1.5524	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5529	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5518	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5526	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2600:	Loss 1.5525	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5491	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5520	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5526	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5495	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5525	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5513	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5526	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5527	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5508	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5490	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5532	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5544	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5517	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5516	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5514	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5534	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5496	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5510	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5511	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5503	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5532	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5530	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5506	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5518	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5505	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5527	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5502	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5495	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5499	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5475	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5506	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5494	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5496	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5499	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5524	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5494	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5490	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5493	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5486	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5473	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5492	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5480	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.5499	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5479	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5489	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5501	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5500	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5482	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.089 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 13.089 ms (Max: 13.557, Min: 11.969, Sum: 104.715)
Cluster-Wide Average, Compute: 37.098 ms (Max: 38.315, Min: 35.959, Sum: 296.784)
Cluster-Wide Average, Communication-Layer: 6.490 ms (Max: 7.609, Min: 5.666, Sum: 51.919)
Cluster-Wide Average, Bubble-Imbalance: 2.534 ms (Max: 3.282, Min: 1.162, Sum: 20.273)
Cluster-Wide Average, Communication-Graph: 26.543 ms (Max: 27.541, Min: 25.960, Sum: 212.345)
Cluster-Wide Average, Optimization: 1.567 ms (Max: 1.701, Min: 1.332, Sum: 12.536)
Cluster-Wide Average, Others: 1.917 ms (Max: 3.128, Min: 1.497, Sum: 15.334)
****** Breakdown Sum: 89.238 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.266 GB (Max: 3.921, Min: 3.036, Sum: 26.130)
Cluster-Wide Average, Graph-Level Communication Throughput: 100.512 Gbps (Max: 101.931, Min: 98.548, Sum: 804.094)
Cluster-Wide Average, Layer-Level Communication Throughput: 32.214 Gbps (Max: 41.227, Min: 23.701, Sum: 257.712)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 1.995 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
