Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
        It takes 0.032 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
        It takes 0.020 seconds.
        It takes 0.023 seconds.
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.026 seconds.
Building the Feature Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
        It takes 0.109 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
Number of vertices per chunk: 2790
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.227 Gbps (per GPU), 481.817 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.916 Gbps (per GPU), 479.324 Gbps (aggregated)
The layer-level communication performance: 59.918 Gbps (per GPU), 479.346 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.644 Gbps (per GPU), 477.148 Gbps (aggregated)
The layer-level communication performance: 59.663 Gbps (per GPU), 477.304 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.416 Gbps (per GPU), 475.327 Gbps (aggregated)
The layer-level communication performance: 59.336 Gbps (per GPU), 474.688 Gbps (aggregated)
The layer-level communication performance: 59.369 Gbps (per GPU), 474.953 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.291 Gbps (per GPU), 1266.324 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.300 Gbps (per GPU), 1266.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.291 Gbps (per GPU), 1266.324 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.249 Gbps (per GPU), 1265.990 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.285 Gbps (per GPU), 1266.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.297 Gbps (per GPU), 1266.372 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.294 Gbps (per GPU), 1266.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.258 Gbps (per GPU), 1266.062 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.050 Gbps (per GPU), 808.398 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.054 Gbps (per GPU), 808.430 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.054 Gbps (per GPU), 808.430 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.045 Gbps (per GPU), 808.360 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.051 Gbps (per GPU), 808.411 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.051 Gbps (per GPU), 808.411 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.055 Gbps (per GPU), 808.437 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.027 Gbps (per GPU), 808.216 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.966 Gbps (per GPU), 247.729 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.968 Gbps (per GPU), 247.741 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.962 Gbps (per GPU), 247.699 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.963 Gbps (per GPU), 247.707 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.964 Gbps (per GPU), 247.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.964 Gbps (per GPU), 247.716 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.961 Gbps (per GPU), 247.687 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.959 Gbps (per GPU), 247.671 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.31ms  0.26ms  0.21ms  1.46  2.81K  0.03M
 chk_1  0.31ms  0.26ms  0.22ms  1.45  2.82K  0.03M
 chk_2  0.31ms  0.26ms  0.21ms  1.45  2.80K  0.03M
 chk_3  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
 chk_4  0.31ms  0.26ms  0.22ms  1.46  2.82K  0.03M
 chk_5  0.32ms  0.27ms  0.22ms  1.44  2.75K  0.03M
 chk_6  0.30ms  0.25ms  0.21ms  1.46  2.71K  0.03M
 chk_7  0.31ms  0.26ms  0.21ms  1.46  2.76K  0.03M
 chk_8  0.31ms  0.26ms  0.21ms  1.45  2.79K  0.03M
 chk_9  0.31ms  0.26ms  0.21ms  1.46  2.81K  0.03M
chk_10  0.31ms  0.25ms  0.21ms  1.47  2.81K  0.03M
chk_11  0.31ms  0.26ms  0.22ms  1.44  2.74K  0.03M
chk_12  0.31ms  0.26ms  0.22ms  1.46  2.76K  0.03M
chk_13  0.31ms  0.26ms  0.21ms  1.45  2.75K  0.03M
chk_14  0.31ms  0.26ms  0.21ms  1.45  2.81K  0.03M
chk_15  0.31ms  0.26ms  0.21ms  1.45  2.77K  0.03M
chk_16  0.31ms  0.26ms  0.21ms  1.45  2.78K  0.03M
chk_17  0.31ms  0.26ms  0.22ms  1.45  2.79K  0.03M
chk_18  0.31ms  0.26ms  0.21ms  1.46  2.82K  0.03M
chk_19  0.31ms  0.25ms  0.21ms  1.46  2.81K  0.03M
chk_20  0.31ms  0.26ms  0.22ms  1.45  2.77K  0.03M
chk_21  0.31ms  0.26ms  0.21ms  1.47  2.84K  0.02M
chk_22  0.31ms  0.26ms  0.21ms  1.45  2.78K  0.03M
chk_23  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
chk_24  0.31ms  0.26ms  0.21ms  1.46  2.80K  0.03M
chk_25  0.31ms  0.26ms  0.21ms  1.47  2.81K  0.03M
chk_26  0.31ms  0.26ms  0.21ms  1.46  2.81K  0.03M
chk_27  0.31ms  0.26ms  0.22ms  1.45  2.79K  0.03M
chk_28  0.31ms  0.26ms  0.22ms  1.44  2.77K  0.03M
chk_29  0.31ms  0.26ms  0.21ms  1.45  2.77K  0.03M
chk_30  0.31ms  0.26ms  0.22ms  1.46  2.80K  0.03M
chk_31  0.31ms  0.26ms  0.21ms  1.45  2.78K  0.03M
   Avg  0.31  0.26  0.21
   Max  0.32  0.27  0.22
   Min  0.30  0.25  0.21
 Ratio  1.04  1.05  1.06
   Var  0.00  0.00  0.00
Profiling takes 0.384 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 34.835 ms
Partition 0 [0, 4) has cost: 34.835 ms
Partition 1 [4, 8) has cost: 33.176 ms
Partition 2 [8, 12) has cost: 33.176 ms
Partition 3 [12, 16) has cost: 33.176 ms
Partition 4 [16, 20) has cost: 33.176 ms
Partition 5 [20, 24) has cost: 33.176 ms
Partition 6 [24, 28) has cost: 33.176 ms
Partition 7 [28, 32) has cost: 31.721 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 20.970 ms
GPU 0, Compute+Comm Time: 17.365 ms, Bubble Time: 3.606 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 16.539 ms, Bubble Time: 3.626 ms, Imbalance Overhead: 0.806 ms
GPU 2, Compute+Comm Time: 16.539 ms, Bubble Time: 3.653 ms, Imbalance Overhead: 0.778 ms
GPU 3, Compute+Comm Time: 16.539 ms, Bubble Time: 3.677 ms, Imbalance Overhead: 0.755 ms
GPU 4, Compute+Comm Time: 16.539 ms, Bubble Time: 3.697 ms, Imbalance Overhead: 0.734 ms
GPU 5, Compute+Comm Time: 16.539 ms, Bubble Time: 3.725 ms, Imbalance Overhead: 0.707 ms
GPU 6, Compute+Comm Time: 16.539 ms, Bubble Time: 3.757 ms, Imbalance Overhead: 0.675 ms
GPU 7, Compute+Comm Time: 15.395 ms, Bubble Time: 3.806 ms, Imbalance Overhead: 1.770 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 32.700 ms
GPU 0, Compute+Comm Time: 25.811 ms, Bubble Time: 5.914 ms, Imbalance Overhead: 0.975 ms
GPU 1, Compute+Comm Time: 26.121 ms, Bubble Time: 5.888 ms, Imbalance Overhead: 0.691 ms
GPU 2, Compute+Comm Time: 26.121 ms, Bubble Time: 5.848 ms, Imbalance Overhead: 0.732 ms
GPU 3, Compute+Comm Time: 26.121 ms, Bubble Time: 5.819 ms, Imbalance Overhead: 0.760 ms
GPU 4, Compute+Comm Time: 26.121 ms, Bubble Time: 5.799 ms, Imbalance Overhead: 0.780 ms
GPU 5, Compute+Comm Time: 26.121 ms, Bubble Time: 5.776 ms, Imbalance Overhead: 0.803 ms
GPU 6, Compute+Comm Time: 26.121 ms, Bubble Time: 5.750 ms, Imbalance Overhead: 0.829 ms
GPU 7, Compute+Comm Time: 26.955 ms, Bubble Time: 5.731 ms, Imbalance Overhead: 0.015 ms
The estimated cost of the whole pipeline: 56.354 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 68.011 ms
Partition 0 [0, 8) has cost: 68.011 ms
Partition 1 [8, 16) has cost: 66.351 ms
Partition 2 [16, 24) has cost: 66.351 ms
Partition 3 [24, 32) has cost: 64.897 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 24.397 ms
GPU 0, Compute+Comm Time: 20.640 ms, Bubble Time: 3.754 ms, Imbalance Overhead: 0.003 ms
GPU 1, Compute+Comm Time: 20.227 ms, Bubble Time: 3.767 ms, Imbalance Overhead: 0.403 ms
GPU 2, Compute+Comm Time: 20.227 ms, Bubble Time: 3.805 ms, Imbalance Overhead: 0.366 ms
GPU 3, Compute+Comm Time: 19.653 ms, Bubble Time: 3.873 ms, Imbalance Overhead: 0.871 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 35.890 ms
GPU 0, Compute+Comm Time: 29.683 ms, Bubble Time: 5.682 ms, Imbalance Overhead: 0.525 ms
GPU 1, Compute+Comm Time: 29.839 ms, Bubble Time: 5.643 ms, Imbalance Overhead: 0.408 ms
GPU 2, Compute+Comm Time: 29.839 ms, Bubble Time: 5.611 ms, Imbalance Overhead: 0.440 ms
GPU 3, Compute+Comm Time: 30.255 ms, Bubble Time: 5.587 ms, Imbalance Overhead: 0.047 ms
    The estimated cost with 2 DP ways is 63.301 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 134.362 ms
Partition 0 [0, 16) has cost: 134.362 ms
Partition 1 [16, 32) has cost: 131.248 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 38.149 ms
GPU 0, Compute+Comm Time: 33.985 ms, Bubble Time: 4.164 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 33.491 ms, Bubble Time: 4.236 ms, Imbalance Overhead: 0.422 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 49.105 ms
GPU 0, Compute+Comm Time: 43.359 ms, Bubble Time: 5.441 ms, Imbalance Overhead: 0.305 ms
GPU 1, Compute+Comm Time: 43.645 ms, Bubble Time: 5.398 ms, Imbalance Overhead: 0.062 ms
    The estimated cost with 4 DP ways is 91.617 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 265.610 ms
Partition 0 [0, 32) has cost: 265.610 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 142.612 ms
GPU 0, Compute+Comm Time: 142.612 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 152.413 ms
GPU 0, Compute+Comm Time: 152.413 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 309.777 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 41)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [81, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [121, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 81)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [121, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [41, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 121)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[121, 160)...
+++++++++ Node 0 initializing the weights for op[0, 41)...
+++++++++ Node 4 initializing the weights for op[81, 121)...
+++++++++ Node 1 initializing the weights for op[0, 41)...
+++++++++ Node 2 initializing the weights for op[41, 81)...
+++++++++ Node 3 initializing the weights for op[41, 81)...
+++++++++ Node 5 initializing the weights for op[81, 121)...
+++++++++ Node 7 initializing the weights for op[121, 160)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9459	TrainAcc 0.0968	ValidAcc 0.0952	TestAcc 0.0924	BestValid 0.0952
	Epoch 50:	Loss 1.9458	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6490	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.6423	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.6262	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.6206	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.6229	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.6270	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.6188	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.6173	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.6152	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.6108	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.6145	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.6129	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.6056	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5853	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5844	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5840	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5847	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 950:	Loss 1.5818	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5790	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5835	TrainAcc 0.2583	ValidAcc 0.2580	TestAcc 0.2547	BestValid 0.4239
	Epoch 1100:	Loss 1.5768	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5746	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5772	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5713	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5694	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5682	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5662	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5736	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5766	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5682	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5713	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5638	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5630	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5626	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5646	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5680	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5626	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1950:	Loss 1.5618	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2000:	Loss 1.5573	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5656	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5632	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5615	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5602	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5563	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5587	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5578	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2400:	Loss 1.5563	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5556	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5544	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5551	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2600:	Loss 1.5568	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5558	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5563	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5529	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5571	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5546	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5573	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5528	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5559	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5567	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5524	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5520	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5544	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5548	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5554	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5545	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5544	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5569	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5540	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5553	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5515	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5508	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5556	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5554	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5529	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5532	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5539	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5531	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5533	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5557	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.5531	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.5531	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.5528	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.5557	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.5531	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.5517	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.5527	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.5512	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.5540	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.5547	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.5573	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.5552	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.5588	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.5592	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.5561	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.5546	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.5617	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.5575	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.5580	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.090 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 13.175 ms (Max: 13.405, Min: 12.593, Sum: 105.401)
Cluster-Wide Average, Compute: 37.321 ms (Max: 38.065, Min: 36.323, Sum: 298.565)
Cluster-Wide Average, Communication-Layer: 6.523 ms (Max: 7.622, Min: 5.687, Sum: 52.182)
Cluster-Wide Average, Bubble-Imbalance: 3.025 ms (Max: 4.519, Min: 1.809, Sum: 24.201)
Cluster-Wide Average, Communication-Graph: 26.870 ms (Max: 28.154, Min: 25.986, Sum: 214.961)
Cluster-Wide Average, Optimization: 1.579 ms (Max: 1.657, Min: 1.440, Sum: 12.635)
Cluster-Wide Average, Others: 1.914 ms (Max: 3.112, Min: 1.493, Sum: 15.313)
****** Breakdown Sum: 90.407 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.266 GB (Max: 3.921, Min: 3.036, Sum: 26.130)
Cluster-Wide Average, Graph-Level Communication Throughput: 99.259 Gbps (Max: 101.083, Min: 94.122, Sum: 794.069)
Cluster-Wide Average, Layer-Level Communication Throughput: 32.047 Gbps (Max: 41.116, Min: 23.649, Sum: 256.374)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 1.995 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 49
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
