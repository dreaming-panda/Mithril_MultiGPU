Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.016 seconds.
        It takes 0.016 seconds.
        It takes 0.021 seconds.
        It takes 0.017 seconds.
        It takes 0.020 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.112 seconds.
Building the Label Vector...
        It takes 0.112 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.969 Gbps (per GPU), 471.751 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.684 Gbps (per GPU), 469.473 Gbps (aggregated)
The layer-level communication performance: 58.688 Gbps (per GPU), 469.505 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.442 Gbps (per GPU), 467.538 Gbps (aggregated)
The layer-level communication performance: 58.413 Gbps (per GPU), 467.305 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.195 Gbps (per GPU), 465.557 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.151 Gbps (per GPU), 465.212 Gbps (aggregated)
The layer-level communication performance: 58.122 Gbps (per GPU), 464.975 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.903 Gbps (per GPU), 1263.226 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.906 Gbps (per GPU), 1263.249 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.898 Gbps (per GPU), 1263.183 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.903 Gbps (per GPU), 1263.226 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.894 Gbps (per GPU), 1263.154 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.903 Gbps (per GPU), 1263.226 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.897 Gbps (per GPU), 1263.178 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.903 Gbps (per GPU), 1263.225 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.163 Gbps (per GPU), 809.308 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.168 Gbps (per GPU), 809.347 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.166 Gbps (per GPU), 809.327 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.169 Gbps (per GPU), 809.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.168 Gbps (per GPU), 809.347 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.165 Gbps (per GPU), 809.321 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.146 Gbps (per GPU), 809.171 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.165 Gbps (per GPU), 809.321 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.508 Gbps (per GPU), 268.065 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.507 Gbps (per GPU), 268.053 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.509 Gbps (per GPU), 268.068 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.505 Gbps (per GPU), 268.040 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.505 Gbps (per GPU), 268.040 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.500 Gbps (per GPU), 267.997 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.501 Gbps (per GPU), 268.005 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.505 Gbps (per GPU), 268.037 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.68ms  0.38ms  0.36ms  1.92  2.81K  0.03M
 chk_1  0.68ms  0.38ms  0.36ms  1.91  2.82K  0.03M
 chk_2  0.68ms  0.38ms  0.35ms  1.92  2.80K  0.03M
 chk_3  0.68ms  0.39ms  0.35ms  1.93  2.80K  0.03M
 chk_4  0.68ms  0.39ms  0.36ms  1.90  2.82K  0.03M
 chk_5  0.68ms  0.39ms  0.36ms  1.88  2.75K  0.03M
 chk_6  0.67ms  0.38ms  0.35ms  1.92  2.71K  0.03M
 chk_7  0.68ms  0.39ms  0.36ms  1.91  2.76K  0.03M
 chk_8  0.68ms  0.38ms  0.36ms  1.90  2.79K  0.03M
 chk_9  0.68ms  0.38ms  0.36ms  1.91  2.81K  0.03M
chk_10  0.68ms  0.38ms  0.35ms  1.94  2.81K  0.03M
chk_11  0.68ms  0.39ms  0.37ms  1.85  2.74K  0.03M
chk_12  0.68ms  0.39ms  0.36ms  1.90  2.76K  0.03M
chk_13  0.68ms  0.39ms  0.35ms  1.92  2.75K  0.03M
chk_14  0.68ms  0.38ms  0.36ms  1.91  2.81K  0.03M
chk_15  0.68ms  0.38ms  0.35ms  1.91  2.77K  0.03M
chk_16  0.68ms  0.38ms  0.35ms  1.92  2.78K  0.03M
chk_17  0.68ms  0.39ms  0.36ms  1.90  2.79K  0.03M
chk_18  0.68ms  0.39ms  0.36ms  1.92  2.82K  0.03M
chk_19  0.68ms  0.38ms  0.35ms  1.92  2.81K  0.03M
chk_20  0.68ms  0.39ms  0.36ms  1.91  2.77K  0.03M
chk_21  0.68ms  0.39ms  0.36ms  1.92  2.84K  0.02M
chk_22  0.68ms  0.39ms  0.36ms  1.91  2.78K  0.03M
chk_23  0.68ms  0.38ms  0.36ms  1.91  2.80K  0.03M
chk_24  0.68ms  0.39ms  0.36ms  1.90  2.80K  0.03M
chk_25  0.68ms  0.38ms  0.35ms  1.92  2.81K  0.03M
chk_26  0.68ms  0.38ms  0.35ms  1.92  2.81K  0.03M
chk_27  0.68ms  0.39ms  0.36ms  1.92  2.79K  0.03M
chk_28  0.68ms  0.39ms  0.36ms  1.91  2.77K  0.03M
chk_29  0.68ms  0.38ms  0.36ms  1.91  2.77K  0.03M
chk_30  0.68ms  0.39ms  0.36ms  1.92  2.80K  0.03M
chk_31  0.68ms  0.39ms  0.35ms  1.92  2.78K  0.03M
   Avg  0.68  0.39  0.36
   Max  0.68  0.39  0.37
   Min  0.67  0.38  0.35
 Ratio  1.02  1.03  1.05
   Var  0.00  0.00  0.00
Profiling takes 0.630 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 58.745 ms
Partition 0 [0, 4) has cost: 58.745 ms
Partition 1 [4, 8) has cost: 49.300 ms
Partition 2 [8, 12) has cost: 49.300 ms
Partition 3 [12, 16) has cost: 49.300 ms
Partition 4 [16, 20) has cost: 49.300 ms
Partition 5 [20, 24) has cost: 49.300 ms
Partition 6 [24, 28) has cost: 49.300 ms
Partition 7 [28, 32) has cost: 48.367 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 28.877 ms
GPU 0, Compute+Comm Time: 24.235 ms, Bubble Time: 4.642 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 21.180 ms, Bubble Time: 4.726 ms, Imbalance Overhead: 2.971 ms
GPU 2, Compute+Comm Time: 21.180 ms, Bubble Time: 4.813 ms, Imbalance Overhead: 2.884 ms
GPU 3, Compute+Comm Time: 21.180 ms, Bubble Time: 4.897 ms, Imbalance Overhead: 2.800 ms
GPU 4, Compute+Comm Time: 21.180 ms, Bubble Time: 4.984 ms, Imbalance Overhead: 2.713 ms
GPU 5, Compute+Comm Time: 21.180 ms, Bubble Time: 5.072 ms, Imbalance Overhead: 2.625 ms
GPU 6, Compute+Comm Time: 21.180 ms, Bubble Time: 5.170 ms, Imbalance Overhead: 2.527 ms
GPU 7, Compute+Comm Time: 19.828 ms, Bubble Time: 5.293 ms, Imbalance Overhead: 3.756 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 52.562 ms
GPU 0, Compute+Comm Time: 38.225 ms, Bubble Time: 9.652 ms, Imbalance Overhead: 4.685 ms
GPU 1, Compute+Comm Time: 37.806 ms, Bubble Time: 9.493 ms, Imbalance Overhead: 5.263 ms
GPU 2, Compute+Comm Time: 37.806 ms, Bubble Time: 9.310 ms, Imbalance Overhead: 5.447 ms
GPU 3, Compute+Comm Time: 37.806 ms, Bubble Time: 9.114 ms, Imbalance Overhead: 5.642 ms
GPU 4, Compute+Comm Time: 37.806 ms, Bubble Time: 8.929 ms, Imbalance Overhead: 5.827 ms
GPU 5, Compute+Comm Time: 37.806 ms, Bubble Time: 8.748 ms, Imbalance Overhead: 6.008 ms
GPU 6, Compute+Comm Time: 37.806 ms, Bubble Time: 8.556 ms, Imbalance Overhead: 6.200 ms
GPU 7, Compute+Comm Time: 44.196 ms, Bubble Time: 8.367 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 85.512 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 108.045 ms
Partition 0 [0, 8) has cost: 108.045 ms
Partition 1 [8, 16) has cost: 98.600 ms
Partition 2 [16, 24) has cost: 98.600 ms
Partition 3 [24, 32) has cost: 97.667 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.056 ms
GPU 0, Compute+Comm Time: 26.426 ms, Bubble Time: 4.630 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 24.901 ms, Bubble Time: 4.701 ms, Imbalance Overhead: 1.454 ms
GPU 2, Compute+Comm Time: 24.901 ms, Bubble Time: 4.821 ms, Imbalance Overhead: 1.334 ms
GPU 3, Compute+Comm Time: 24.228 ms, Bubble Time: 4.972 ms, Imbalance Overhead: 1.855 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 52.591 ms
GPU 0, Compute+Comm Time: 41.763 ms, Bubble Time: 8.413 ms, Imbalance Overhead: 2.416 ms
GPU 1, Compute+Comm Time: 41.555 ms, Bubble Time: 8.205 ms, Imbalance Overhead: 2.832 ms
GPU 2, Compute+Comm Time: 41.555 ms, Bubble Time: 8.015 ms, Imbalance Overhead: 3.021 ms
GPU 3, Compute+Comm Time: 44.756 ms, Bubble Time: 7.835 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 87.830 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 206.644 ms
Partition 0 [0, 16) has cost: 206.644 ms
Partition 1 [16, 32) has cost: 196.267 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 43.941 ms
GPU 0, Compute+Comm Time: 39.205 ms, Bubble Time: 4.736 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.112 ms, Bubble Time: 4.872 ms, Imbalance Overhead: 0.956 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 63.629 ms
GPU 0, Compute+Comm Time: 55.249 ms, Bubble Time: 7.071 ms, Imbalance Overhead: 1.308 ms
GPU 1, Compute+Comm Time: 56.748 ms, Bubble Time: 6.881 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 112.948 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 402.912 ms
Partition 0 [0, 32) has cost: 402.912 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 137.547 ms
GPU 0, Compute+Comm Time: 137.547 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 154.878 ms
GPU 0, Compute+Comm Time: 154.878 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 307.046 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[130, 194)...
+++++++++ Node 1 initializing the weights for op[0, 66)...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 6 initializing the weights for op[194, 257)...
+++++++++ Node 2 initializing the weights for op[66, 130)...
+++++++++ Node 7 initializing the weights for op[194, 257)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
+++++++++ Node 0 initializing the weights for op[0, 66)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.6250	TrainAcc 0.0602	ValidAcc 0.0592	TestAcc 0.0612	BestValid 0.0592
	Epoch 50:	Loss 1.7840	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6177	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5927	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5857	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5855	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5868	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5858	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5833	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5848	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5823	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5806	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5801	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5782	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5786	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5767	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5784	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5763	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5775	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5784	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5783	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5768	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5753	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5774	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5763	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5767	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5760	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5760	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5778	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5752	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5760	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5762	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5760	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5745	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5752	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5755	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5780	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5762	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5749	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1950:	Loss 1.5741	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2000:	Loss 1.5751	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5745	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5751	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5751	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5746	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5751	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5733	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5743	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2400:	Loss 1.5762	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5735	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5748	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5739	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2600:	Loss 1.5748	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5742	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5732	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5736	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5698	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5662	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5622	TrainAcc 0.4202	ValidAcc 0.4217	TestAcc 0.4216	BestValid 0.4239
	Epoch 2950:	Loss 1.5563	TrainAcc 0.3990	ValidAcc 0.4034	TestAcc 0.4019	BestValid 0.4239
	Epoch 3000:	Loss 1.5526	TrainAcc 0.4004	ValidAcc 0.4021	TestAcc 0.4022	BestValid 0.4239
	Epoch 3050:	Loss 1.5517	TrainAcc 0.3879	ValidAcc 0.3906	TestAcc 0.3913	BestValid 0.4239
	Epoch 3100:	Loss 1.5494	TrainAcc 0.4188	ValidAcc 0.4211	TestAcc 0.4207	BestValid 0.4239
	Epoch 3150:	Loss 1.5469	TrainAcc 0.3840	ValidAcc 0.3854	TestAcc 0.3865	BestValid 0.4239
	Epoch 3200:	Loss 1.5438	TrainAcc 0.4182	ValidAcc 0.4201	TestAcc 0.4196	BestValid 0.4239
	Epoch 3250:	Loss 1.5418	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5399	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.5393	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.5367	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.5345	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.5343	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.5342	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.5330	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.5314	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.5309	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.5321	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.5332	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.5309	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.5285	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.5286	TrainAcc 0.4216	ValidAcc 0.4238	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.5271	TrainAcc 0.4216	ValidAcc 0.4237	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.5263	TrainAcc 0.4222	ValidAcc 0.4245	TestAcc 0.4240	BestValid 0.4245
	Epoch 4100:	Loss 1.5263	TrainAcc 0.4215	ValidAcc 0.4238	TestAcc 0.4233	BestValid 0.4245
	Epoch 4150:	Loss 1.5285	TrainAcc 0.4214	ValidAcc 0.4239	TestAcc 0.4222	BestValid 0.4245
	Epoch 4200:	Loss 1.5247	TrainAcc 0.4221	ValidAcc 0.4265	TestAcc 0.4246	BestValid 0.4265
	Epoch 4250:	Loss 1.5258	TrainAcc 0.4214	ValidAcc 0.4233	TestAcc 0.4230	BestValid 0.4265
	Epoch 4300:	Loss 1.5252	TrainAcc 0.4212	ValidAcc 0.4236	TestAcc 0.4229	BestValid 0.4265
	Epoch 4350:	Loss 1.5256	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4400:	Loss 1.5246	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4450:	Loss 1.5239	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4500:	Loss 1.5211	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4550:	Loss 1.5203	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4600:	Loss 1.5196	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4650:	Loss 1.5193	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4700:	Loss 1.5162	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4750:	Loss 1.5151	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4800:	Loss 1.5159	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4850:	Loss 1.5147	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4900:	Loss 1.5086	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 4950:	Loss 1.5116	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
	Epoch 5000:	Loss 1.5087	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4265
****** Epoch Time (Excluding Evaluation Cost): 0.125 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 18.622 ms (Max: 18.755, Min: 18.493, Sum: 148.974)
Cluster-Wide Average, Compute: 64.757 ms (Max: 67.340, Min: 62.290, Sum: 518.058)
Cluster-Wide Average, Communication-Layer: 6.536 ms (Max: 7.534, Min: 5.728, Sum: 52.285)
Cluster-Wide Average, Bubble-Imbalance: 3.519 ms (Max: 6.407, Min: 2.307, Sum: 28.152)
Cluster-Wide Average, Communication-Graph: 27.097 ms (Max: 28.035, Min: 26.147, Sum: 216.774)
Cluster-Wide Average, Optimization: 2.388 ms (Max: 2.826, Min: 1.829, Sum: 19.102)
Cluster-Wide Average, Others: 2.235 ms (Max: 3.767, Min: 1.574, Sum: 17.884)
****** Breakdown Sum: 125.154 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.137 GB (Max: 4.034, Min: 2.854, Sum: 25.099)
Cluster-Wide Average, Graph-Level Communication Throughput: 98.325 Gbps (Max: 100.277, Min: 96.129, Sum: 786.598)
Cluster-Wide Average, Layer-Level Communication Throughput: 31.981 Gbps (Max: 40.723, Min: 23.580, Sum: 255.850)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 1.998 GB
****** Accuracy Results ******
Highest valid_acc: 0.4265
Target test_acc: 0.4246
Epoch to reach the target acc: 4199
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
