Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.016 seconds.
        It takes 0.019 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.018 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.106 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.113 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.012 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.485 Gbps (per GPU), 475.882 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.204 Gbps (per GPU), 473.631 Gbps (aggregated)
The layer-level communication performance: 59.191 Gbps (per GPU), 473.526 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.942 Gbps (per GPU), 471.533 Gbps (aggregated)
The layer-level communication performance: 58.912 Gbps (per GPU), 471.297 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.697 Gbps (per GPU), 469.575 Gbps (aggregated)
The layer-level communication performance: 58.651 Gbps (per GPU), 469.210 Gbps (aggregated)
The layer-level communication performance: 58.619 Gbps (per GPU), 468.950 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 152.023 Gbps (per GPU), 1216.181 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 152.025 Gbps (per GPU), 1216.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.041 Gbps (per GPU), 1240.326 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 152.020 Gbps (per GPU), 1216.159 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 152.014 Gbps (per GPU), 1216.115 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 152.006 Gbps (per GPU), 1216.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 152.017 Gbps (per GPU), 1216.137 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 152.006 Gbps (per GPU), 1216.049 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 96.006 Gbps (per GPU), 768.047 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.007 Gbps (per GPU), 768.053 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.009 Gbps (per GPU), 768.070 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.007 Gbps (per GPU), 768.059 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.010 Gbps (per GPU), 768.076 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.010 Gbps (per GPU), 768.082 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.010 Gbps (per GPU), 768.082 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 96.010 Gbps (per GPU), 768.076 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.305 Gbps (per GPU), 250.442 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.304 Gbps (per GPU), 250.433 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.304 Gbps (per GPU), 250.431 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.300 Gbps (per GPU), 250.398 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.299 Gbps (per GPU), 250.395 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.300 Gbps (per GPU), 250.401 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.303 Gbps (per GPU), 250.427 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.294 Gbps (per GPU), 250.354 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.70ms  0.40ms  0.37ms  1.87  2.81K  0.03M
 chk_1  0.70ms  0.40ms  0.37ms  1.88  2.82K  0.03M
 chk_2  0.70ms  0.40ms  0.37ms  1.88  2.80K  0.03M
 chk_3  0.70ms  0.40ms  0.37ms  1.88  2.80K  0.03M
 chk_4  0.70ms  0.40ms  0.37ms  1.88  2.82K  0.03M
 chk_5  0.70ms  0.40ms  0.38ms  1.86  2.75K  0.03M
 chk_6  0.69ms  0.39ms  0.37ms  1.88  2.71K  0.03M
 chk_7  0.70ms  0.40ms  0.37ms  1.88  2.76K  0.03M
 chk_8  0.70ms  0.40ms  0.37ms  1.87  2.79K  0.03M
 chk_9  0.70ms  0.40ms  0.37ms  1.88  2.81K  0.03M
chk_10  0.70ms  0.40ms  0.37ms  1.90  2.81K  0.03M
chk_11  0.70ms  0.40ms  0.38ms  1.86  2.74K  0.03M
chk_12  0.70ms  0.40ms  0.37ms  1.87  2.76K  0.03M
chk_13  0.70ms  0.40ms  0.37ms  1.88  2.75K  0.03M
chk_14  0.70ms  0.40ms  0.37ms  1.89  2.81K  0.03M
chk_15  0.70ms  0.40ms  0.37ms  1.88  2.77K  0.03M
chk_16  0.70ms  0.40ms  0.37ms  1.88  2.78K  0.03M
chk_17  0.70ms  0.40ms  0.37ms  1.88  2.79K  0.03M
chk_18  0.70ms  0.40ms  0.37ms  1.89  2.82K  0.03M
chk_19  0.69ms  0.39ms  0.37ms  1.89  2.81K  0.03M
chk_20  0.70ms  0.40ms  0.37ms  1.88  2.77K  0.03M
chk_21  0.71ms  0.40ms  0.37ms  1.90  2.84K  0.02M
chk_22  0.70ms  0.40ms  0.37ms  1.88  2.78K  0.03M
chk_23  0.70ms  0.40ms  0.37ms  1.88  2.80K  0.03M
chk_24  0.70ms  0.40ms  0.37ms  1.87  2.80K  0.03M
chk_25  0.70ms  0.40ms  0.37ms  1.89  2.81K  0.03M
chk_26  0.70ms  0.40ms  0.37ms  1.89  2.81K  0.03M
chk_27  0.70ms  0.40ms  0.37ms  1.89  2.79K  0.03M
chk_28  0.70ms  0.40ms  0.37ms  1.87  2.77K  0.03M
chk_29  0.70ms  0.40ms  0.37ms  1.86  2.77K  0.03M
chk_30  0.70ms  0.40ms  0.37ms  1.88  2.80K  0.03M
chk_31  0.70ms  0.40ms  0.37ms  1.88  2.78K  0.03M
   Avg  0.70  0.40  0.37
   Max  0.71  0.40  0.38
   Min  0.69  0.39  0.37
 Ratio  1.03  1.03  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.651 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 60.793 ms
Partition 0 [0, 4) has cost: 60.793 ms
Partition 1 [4, 8) has cost: 51.219 ms
Partition 2 [8, 12) has cost: 51.219 ms
Partition 3 [12, 16) has cost: 51.219 ms
Partition 4 [16, 20) has cost: 51.219 ms
Partition 5 [20, 24) has cost: 51.219 ms
Partition 6 [24, 28) has cost: 51.219 ms
Partition 7 [28, 32) has cost: 50.321 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 29.718 ms
GPU 0, Compute+Comm Time: 24.962 ms, Bubble Time: 4.756 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 21.841 ms, Bubble Time: 4.847 ms, Imbalance Overhead: 3.030 ms
GPU 2, Compute+Comm Time: 21.841 ms, Bubble Time: 4.943 ms, Imbalance Overhead: 2.934 ms
GPU 3, Compute+Comm Time: 21.841 ms, Bubble Time: 5.042 ms, Imbalance Overhead: 2.835 ms
GPU 4, Compute+Comm Time: 21.841 ms, Bubble Time: 5.135 ms, Imbalance Overhead: 2.742 ms
GPU 5, Compute+Comm Time: 21.841 ms, Bubble Time: 5.233 ms, Imbalance Overhead: 2.644 ms
GPU 6, Compute+Comm Time: 21.841 ms, Bubble Time: 5.335 ms, Imbalance Overhead: 2.542 ms
GPU 7, Compute+Comm Time: 20.490 ms, Bubble Time: 5.460 ms, Imbalance Overhead: 3.768 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 54.072 ms
GPU 0, Compute+Comm Time: 39.434 ms, Bubble Time: 9.929 ms, Imbalance Overhead: 4.709 ms
GPU 1, Compute+Comm Time: 38.981 ms, Bubble Time: 9.776 ms, Imbalance Overhead: 5.315 ms
GPU 2, Compute+Comm Time: 38.981 ms, Bubble Time: 9.589 ms, Imbalance Overhead: 5.502 ms
GPU 3, Compute+Comm Time: 38.981 ms, Bubble Time: 9.386 ms, Imbalance Overhead: 5.705 ms
GPU 4, Compute+Comm Time: 38.981 ms, Bubble Time: 9.203 ms, Imbalance Overhead: 5.888 ms
GPU 5, Compute+Comm Time: 38.981 ms, Bubble Time: 9.026 ms, Imbalance Overhead: 6.065 ms
GPU 6, Compute+Comm Time: 38.981 ms, Bubble Time: 8.832 ms, Imbalance Overhead: 6.259 ms
GPU 7, Compute+Comm Time: 45.434 ms, Bubble Time: 8.638 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 87.980 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 112.013 ms
Partition 0 [0, 8) has cost: 112.013 ms
Partition 1 [8, 16) has cost: 102.439 ms
Partition 2 [16, 24) has cost: 102.439 ms
Partition 3 [24, 32) has cost: 101.541 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 32.013 ms
GPU 0, Compute+Comm Time: 27.242 ms, Bubble Time: 4.771 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 25.679 ms, Bubble Time: 4.857 ms, Imbalance Overhead: 1.477 ms
GPU 2, Compute+Comm Time: 25.679 ms, Bubble Time: 4.968 ms, Imbalance Overhead: 1.365 ms
GPU 3, Compute+Comm Time: 25.006 ms, Bubble Time: 5.125 ms, Imbalance Overhead: 1.882 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 54.199 ms
GPU 0, Compute+Comm Time: 43.098 ms, Bubble Time: 8.658 ms, Imbalance Overhead: 2.444 ms
GPU 1, Compute+Comm Time: 42.871 ms, Bubble Time: 8.463 ms, Imbalance Overhead: 2.865 ms
GPU 2, Compute+Comm Time: 42.871 ms, Bubble Time: 8.279 ms, Imbalance Overhead: 3.049 ms
GPU 3, Compute+Comm Time: 46.105 ms, Bubble Time: 8.095 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 90.523 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 214.452 ms
Partition 0 [0, 16) has cost: 214.452 ms
Partition 1 [16, 32) has cost: 203.980 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 45.725 ms
GPU 0, Compute+Comm Time: 40.789 ms, Bubble Time: 4.936 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 39.670 ms, Bubble Time: 5.076 ms, Imbalance Overhead: 0.980 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 66.024 ms
GPU 0, Compute+Comm Time: 57.359 ms, Bubble Time: 7.327 ms, Imbalance Overhead: 1.338 ms
GPU 1, Compute+Comm Time: 58.868 ms, Bubble Time: 7.156 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 117.337 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 418.432 ms
Partition 0 [0, 32) has cost: 418.432 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 146.748 ms
GPU 0, Compute+Comm Time: 146.748 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 164.651 ms
GPU 0, Compute+Comm Time: 164.651 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 326.969 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 66)...
+++++++++ Node 5 initializing the weights for op[130, 194)...
+++++++++ Node 2 initializing the weights for op[66, 130)...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 0 initializing the weights for op[0, 66)...
+++++++++ Node 6 initializing the weights for op[194, 257)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
+++++++++ Node 7 initializing the weights for op[194, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 6.2287	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 50:	Loss 1.6205	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6231	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5990	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5894	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5913	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5903	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5894	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5861	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5856	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5866	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5878	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5826	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5841	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5848	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5849	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5851	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5817	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5787	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5755	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5711	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1050:	Loss 1.5646	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1100:	Loss 1.5595	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1150:	Loss 1.5541	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1200:	Loss 1.5500	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5459	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1300:	Loss 1.5442	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5433	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5419	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5413	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5394	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5398	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5404	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1650:	Loss 1.5382	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1700:	Loss 1.5378	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1750:	Loss 1.5376	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1800:	Loss 1.5380	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1850:	Loss 1.5377	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1900:	Loss 1.5354	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1950:	Loss 1.5362	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2000:	Loss 1.5362	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2050:	Loss 1.5350	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2100:	Loss 1.5344	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2150:	Loss 1.5353	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2200:	Loss 1.5320	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2250:	Loss 1.5332	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2300:	Loss 1.5340	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2350:	Loss 1.5312	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2400:	Loss 1.5299	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2450:	Loss 1.5298	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2500:	Loss 1.5291	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2550:	Loss 1.5289	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2600:	Loss 1.5271	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2650:	Loss 1.5269	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2700:	Loss 1.5239	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2750:	Loss 1.5231	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2800:	Loss 1.5241	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2850:	Loss 1.5238	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2900:	Loss 1.5258	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 2950:	Loss 1.5220	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3000:	Loss 1.5191	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3050:	Loss 1.5209	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3100:	Loss 1.5140	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3150:	Loss 1.5129	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3200:	Loss 1.5047	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3250:	Loss 1.5030	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3300:	Loss 1.5002	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3350:	Loss 1.4971	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3400:	Loss 1.4947	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3450:	Loss 1.4954	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3500:	Loss 1.4923	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3550:	Loss 1.4894	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3600:	Loss 1.4895	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3650:	Loss 1.4898	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3700:	Loss 1.4843	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3750:	Loss 1.4863	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3800:	Loss 1.4866	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3850:	Loss 1.4838	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3900:	Loss 1.4886	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 3950:	Loss 1.4875	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4000:	Loss 1.4839	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4050:	Loss 1.4845	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4100:	Loss 1.4815	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4150:	Loss 1.4801	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4200:	Loss 1.4852	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4250:	Loss 1.4779	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4300:	Loss 1.4765	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4350:	Loss 1.4804	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4400:	Loss 1.4801	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4450:	Loss 1.4777	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4500:	Loss 1.4753	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4550:	Loss 1.4813	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4600:	Loss 1.4768	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4650:	Loss 1.4825	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4700:	Loss 1.4812	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4750:	Loss 1.4756	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4800:	Loss 1.4765	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4850:	Loss 1.4781	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4900:	Loss 1.4793	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 4950:	Loss 1.4789	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 5000:	Loss 1.4782	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
****** Epoch Time (Excluding Evaluation Cost): 0.127 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 19.156 ms (Max: 19.694, Min: 18.691, Sum: 153.248)
Cluster-Wide Average, Compute: 65.400 ms (Max: 69.981, Min: 62.449, Sum: 523.197)
Cluster-Wide Average, Communication-Layer: 6.522 ms (Max: 7.543, Min: 5.601, Sum: 52.176)
Cluster-Wide Average, Bubble-Imbalance: 4.558 ms (Max: 7.737, Min: 1.309, Sum: 36.465)
Cluster-Wide Average, Communication-Graph: 27.357 ms (Max: 28.312, Min: 26.470, Sum: 218.858)
Cluster-Wide Average, Optimization: 2.350 ms (Max: 2.867, Min: 2.102, Sum: 18.796)
Cluster-Wide Average, Others: 2.217 ms (Max: 3.607, Min: 1.589, Sum: 17.732)
****** Breakdown Sum: 127.559 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.137 GB (Max: 4.034, Min: 2.854, Sum: 25.099)
Cluster-Wide Average, Graph-Level Communication Throughput: 97.532 Gbps (Max: 102.038, Min: 90.686, Sum: 780.256)
Cluster-Wide Average, Layer-Level Communication Throughput: 32.023 Gbps (Max: 40.417, Min: 23.556, Sum: 256.184)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 1.998 GB
****** Accuracy Results ******
Highest valid_acc: 0.4239
Target test_acc: 0.4234
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
