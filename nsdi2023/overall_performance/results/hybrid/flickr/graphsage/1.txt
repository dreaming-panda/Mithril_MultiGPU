Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the Feature Vector...
        It takes 0.019 seconds.
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.114 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.902 Gbps (per GPU), 487.215 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.581 Gbps (per GPU), 484.650 Gbps (aggregated)
The layer-level communication performance: 60.586 Gbps (per GPU), 484.689 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.300 Gbps (per GPU), 482.399 Gbps (aggregated)
The layer-level communication performance: 60.333 Gbps (per GPU), 482.662 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.045 Gbps (per GPU), 480.362 Gbps (aggregated)
The layer-level communication performance: 60.001 Gbps (per GPU), 480.005 Gbps (aggregated)
The layer-level communication performance: 59.966 Gbps (per GPU), 479.729 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.299 Gbps (per GPU), 1258.394 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.290 Gbps (per GPU), 1258.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.293 Gbps (per GPU), 1258.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.397 Gbps (per GPU), 1259.173 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.297 Gbps (per GPU), 1258.373 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.290 Gbps (per GPU), 1258.320 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.294 Gbps (per GPU), 1258.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.340 Gbps (per GPU), 1258.724 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 97.460 Gbps (per GPU), 779.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.460 Gbps (per GPU), 779.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.460 Gbps (per GPU), 779.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.457 Gbps (per GPU), 779.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.456 Gbps (per GPU), 779.647 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.459 Gbps (per GPU), 779.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.457 Gbps (per GPU), 779.653 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.401 Gbps (per GPU), 779.206 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.340 Gbps (per GPU), 266.722 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.343 Gbps (per GPU), 266.745 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.337 Gbps (per GPU), 266.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.343 Gbps (per GPU), 266.741 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.336 Gbps (per GPU), 266.685 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.339 Gbps (per GPU), 266.715 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.331 Gbps (per GPU), 266.647 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.340 Gbps (per GPU), 266.724 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.69ms  0.38ms  0.36ms  1.92  2.81K  0.03M
 chk_1  0.69ms  0.39ms  0.36ms  1.91  2.82K  0.03M
 chk_2  0.68ms  0.39ms  0.35ms  1.92  2.80K  0.03M
 chk_3  0.68ms  0.39ms  0.36ms  1.92  2.80K  0.03M
 chk_4  0.69ms  0.39ms  0.36ms  1.90  2.82K  0.03M
 chk_5  0.69ms  0.39ms  0.36ms  1.88  2.75K  0.03M
 chk_6  0.67ms  0.38ms  0.35ms  1.91  2.71K  0.03M
 chk_7  0.68ms  0.39ms  0.35ms  1.92  2.76K  0.03M
 chk_8  0.68ms  0.39ms  0.36ms  1.90  2.79K  0.03M
 chk_9  0.68ms  0.38ms  0.36ms  1.91  2.81K  0.03M
chk_10  0.68ms  0.39ms  0.35ms  1.93  2.81K  0.03M
chk_11  0.68ms  0.39ms  0.36ms  1.90  2.74K  0.03M
chk_12  0.68ms  0.39ms  0.36ms  1.91  2.76K  0.03M
chk_13  0.68ms  0.39ms  0.36ms  1.91  2.75K  0.03M
chk_14  0.68ms  0.39ms  0.36ms  1.92  2.81K  0.03M
chk_15  0.68ms  0.39ms  0.36ms  1.92  2.77K  0.03M
chk_16  0.68ms  0.38ms  0.35ms  1.92  2.78K  0.03M
chk_17  0.69ms  0.39ms  0.36ms  1.91  2.79K  0.03M
chk_18  0.69ms  0.39ms  0.36ms  1.93  2.82K  0.03M
chk_19  0.68ms  0.38ms  0.35ms  1.93  2.81K  0.03M
chk_20  0.68ms  0.39ms  0.36ms  1.92  2.77K  0.03M
chk_21  0.69ms  0.39ms  0.36ms  1.93  2.84K  0.02M
chk_22  0.70ms  0.39ms  0.36ms  1.97  2.78K  0.03M
chk_23  0.69ms  0.39ms  0.36ms  1.91  2.80K  0.03M
chk_24  0.68ms  0.39ms  0.36ms  1.90  2.80K  0.03M
chk_25  0.68ms  0.38ms  0.36ms  1.92  2.81K  0.03M
chk_26  0.69ms  0.39ms  0.36ms  1.93  2.81K  0.03M
chk_27  0.69ms  0.39ms  0.36ms  1.91  2.79K  0.03M
chk_28  0.69ms  0.39ms  0.36ms  1.91  2.77K  0.03M
chk_29  0.68ms  0.39ms  0.36ms  1.90  2.77K  0.03M
chk_30  0.69ms  0.39ms  0.36ms  1.91  2.80K  0.03M
chk_31  0.68ms  0.39ms  0.36ms  1.91  2.78K  0.03M
   Avg  0.68  0.39  0.36
   Max  0.70  0.39  0.36
   Min  0.67  0.38  0.35
 Ratio  1.05  1.03  1.04
   Var  0.00  0.00  0.00
Profiling takes 0.633 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 59.104 ms
Partition 0 [0, 4) has cost: 59.104 ms
Partition 1 [4, 8) has cost: 49.635 ms
Partition 2 [8, 12) has cost: 49.635 ms
Partition 3 [12, 16) has cost: 49.635 ms
Partition 4 [16, 20) has cost: 49.635 ms
Partition 5 [20, 24) has cost: 49.635 ms
Partition 6 [24, 28) has cost: 49.635 ms
Partition 7 [28, 32) has cost: 48.652 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 28.816 ms
GPU 0, Compute+Comm Time: 24.210 ms, Bubble Time: 4.607 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 21.146 ms, Bubble Time: 4.693 ms, Imbalance Overhead: 2.978 ms
GPU 2, Compute+Comm Time: 21.146 ms, Bubble Time: 4.784 ms, Imbalance Overhead: 2.887 ms
GPU 3, Compute+Comm Time: 21.146 ms, Bubble Time: 4.872 ms, Imbalance Overhead: 2.798 ms
GPU 4, Compute+Comm Time: 21.146 ms, Bubble Time: 4.966 ms, Imbalance Overhead: 2.704 ms
GPU 5, Compute+Comm Time: 21.146 ms, Bubble Time: 5.062 ms, Imbalance Overhead: 2.609 ms
GPU 6, Compute+Comm Time: 21.146 ms, Bubble Time: 5.162 ms, Imbalance Overhead: 2.508 ms
GPU 7, Compute+Comm Time: 19.773 ms, Bubble Time: 5.283 ms, Imbalance Overhead: 3.761 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 52.667 ms
GPU 0, Compute+Comm Time: 38.258 ms, Bubble Time: 9.672 ms, Imbalance Overhead: 4.737 ms
GPU 1, Compute+Comm Time: 37.868 ms, Bubble Time: 9.515 ms, Imbalance Overhead: 5.284 ms
GPU 2, Compute+Comm Time: 37.868 ms, Bubble Time: 9.327 ms, Imbalance Overhead: 5.471 ms
GPU 3, Compute+Comm Time: 37.868 ms, Bubble Time: 9.134 ms, Imbalance Overhead: 5.665 ms
GPU 4, Compute+Comm Time: 37.868 ms, Bubble Time: 8.951 ms, Imbalance Overhead: 5.848 ms
GPU 5, Compute+Comm Time: 37.868 ms, Bubble Time: 8.770 ms, Imbalance Overhead: 6.028 ms
GPU 6, Compute+Comm Time: 37.868 ms, Bubble Time: 8.580 ms, Imbalance Overhead: 6.219 ms
GPU 7, Compute+Comm Time: 44.274 ms, Bubble Time: 8.393 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 85.557 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 108.739 ms
Partition 0 [0, 8) has cost: 108.739 ms
Partition 1 [8, 16) has cost: 99.270 ms
Partition 2 [16, 24) has cost: 99.270 ms
Partition 3 [24, 32) has cost: 98.286 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.020 ms
GPU 0, Compute+Comm Time: 26.409 ms, Bubble Time: 4.612 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 24.876 ms, Bubble Time: 4.696 ms, Imbalance Overhead: 1.448 ms
GPU 2, Compute+Comm Time: 24.876 ms, Bubble Time: 4.802 ms, Imbalance Overhead: 1.343 ms
GPU 3, Compute+Comm Time: 24.188 ms, Bubble Time: 4.949 ms, Imbalance Overhead: 1.884 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 52.717 ms
GPU 0, Compute+Comm Time: 41.827 ms, Bubble Time: 8.415 ms, Imbalance Overhead: 2.474 ms
GPU 1, Compute+Comm Time: 41.633 ms, Bubble Time: 8.221 ms, Imbalance Overhead: 2.863 ms
GPU 2, Compute+Comm Time: 41.633 ms, Bubble Time: 8.032 ms, Imbalance Overhead: 3.052 ms
GPU 3, Compute+Comm Time: 44.853 ms, Bubble Time: 7.863 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 87.924 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 208.009 ms
Partition 0 [0, 16) has cost: 208.009 ms
Partition 1 [16, 32) has cost: 197.556 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 44.665 ms
GPU 0, Compute+Comm Time: 39.842 ms, Bubble Time: 4.823 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 38.732 ms, Bubble Time: 4.986 ms, Imbalance Overhead: 0.947 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 64.498 ms
GPU 0, Compute+Comm Time: 55.979 ms, Bubble Time: 7.149 ms, Imbalance Overhead: 1.371 ms
GPU 1, Compute+Comm Time: 57.499 ms, Bubble Time: 6.999 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 114.621 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 405.564 ms
Partition 0 [0, 32) has cost: 405.564 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 138.230 ms
GPU 0, Compute+Comm Time: 138.230 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 155.711 ms
GPU 0, Compute+Comm Time: 155.711 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 308.637 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 1 initializing the weights for op[0, 66)...
+++++++++ Node 5 initializing the weights for op[130, 194)...
+++++++++ Node 2 initializing the weights for op[66, 130)...
+++++++++ Node 6 initializing the weights for op[194, 257)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
+++++++++ Node 7 initializing the weights for op[194, 257)...
+++++++++ Node 0 initializing the weights for op[0, 66)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 5.3375	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 50:	Loss 1.6267	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 100:	Loss 1.6135	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 150:	Loss 1.5904	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 200:	Loss 1.5913	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 250:	Loss 1.5911	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 300:	Loss 1.5872	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 350:	Loss 1.5876	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 400:	Loss 1.5836	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 450:	Loss 1.5854	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 500:	Loss 1.5837	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 550:	Loss 1.5806	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 600:	Loss 1.5782	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 650:	Loss 1.5776	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 700:	Loss 1.5769	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 750:	Loss 1.5768	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 800:	Loss 1.5780	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 850:	Loss 1.5736	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 900:	Loss 1.5689	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 950:	Loss 1.5615	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1000:	Loss 1.5599	TrainAcc 0.4216	ValidAcc 0.4237	TestAcc 0.4233	BestValid 0.4239
	Epoch 1050:	Loss 1.5564	TrainAcc 0.4214	ValidAcc 0.4238	TestAcc 0.4235	BestValid 0.4239
	Epoch 1100:	Loss 1.5542	TrainAcc 0.4166	ValidAcc 0.4177	TestAcc 0.4190	BestValid 0.4239
	Epoch 1150:	Loss 1.5537	TrainAcc 0.3451	ValidAcc 0.3483	TestAcc 0.3489	BestValid 0.4239
	Epoch 1200:	Loss 1.5495	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1250:	Loss 1.5474	TrainAcc 0.4178	ValidAcc 0.4193	TestAcc 0.4190	BestValid 0.4239
	Epoch 1300:	Loss 1.5458	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1350:	Loss 1.5424	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1400:	Loss 1.5409	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1450:	Loss 1.5396	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1500:	Loss 1.5404	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1550:	Loss 1.5393	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4239
	Epoch 1600:	Loss 1.5367	TrainAcc 0.4212	ValidAcc 0.4239	TestAcc 0.4229	BestValid 0.4239
	Epoch 1650:	Loss 1.5362	TrainAcc 0.4244	ValidAcc 0.4270	TestAcc 0.4261	BestValid 0.4270
	Epoch 1700:	Loss 1.5378	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 1750:	Loss 1.5369	TrainAcc 0.4149	ValidAcc 0.4175	TestAcc 0.4183	BestValid 0.4270
	Epoch 1800:	Loss 1.5375	TrainAcc 0.3617	ValidAcc 0.3621	TestAcc 0.3604	BestValid 0.4270
	Epoch 1850:	Loss 1.5345	TrainAcc 0.4225	ValidAcc 0.4222	TestAcc 0.4216	BestValid 0.4270
	Epoch 1900:	Loss 1.5361	TrainAcc 0.3894	ValidAcc 0.3939	TestAcc 0.3927	BestValid 0.4270
	Epoch 1950:	Loss 1.5338	TrainAcc 0.4215	ValidAcc 0.4238	TestAcc 0.4230	BestValid 0.4270
	Epoch 2000:	Loss 1.5339	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2050:	Loss 1.5347	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2100:	Loss 1.5331	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2150:	Loss 1.5321	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2200:	Loss 1.5324	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2250:	Loss 1.5320	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2300:	Loss 1.5303	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2350:	Loss 1.5334	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2400:	Loss 1.5323	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2450:	Loss 1.5334	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2500:	Loss 1.5303	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2550:	Loss 1.5282	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2600:	Loss 1.5276	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2650:	Loss 1.5261	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2700:	Loss 1.5279	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2750:	Loss 1.5250	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2800:	Loss 1.5249	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2850:	Loss 1.5234	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2900:	Loss 1.5223	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 2950:	Loss 1.5204	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3000:	Loss 1.5219	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3050:	Loss 1.5230	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3100:	Loss 1.5205	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3150:	Loss 1.5205	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3200:	Loss 1.5206	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3250:	Loss 1.5174	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3300:	Loss 1.5167	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3350:	Loss 1.5159	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3400:	Loss 1.5189	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3450:	Loss 1.5216	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3500:	Loss 1.5191	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3550:	Loss 1.5164	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3600:	Loss 1.5176	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3650:	Loss 1.5156	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3700:	Loss 1.5139	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3750:	Loss 1.5088	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3800:	Loss 1.5057	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3850:	Loss 1.5012	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3900:	Loss 1.5065	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 3950:	Loss 1.4963	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4000:	Loss 1.4963	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4050:	Loss 1.5017	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4100:	Loss 1.4966	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4150:	Loss 1.4954	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4200:	Loss 1.4922	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4250:	Loss 1.4934	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4300:	Loss 1.4963	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4350:	Loss 1.4921	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4400:	Loss 1.4999	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4450:	Loss 1.4973	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4500:	Loss 1.4913	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4550:	Loss 1.4925	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4600:	Loss 1.4876	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4650:	Loss 1.4922	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4700:	Loss 1.4913	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4750:	Loss 1.4879	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4800:	Loss 1.4895	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4850:	Loss 1.4867	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4900:	Loss 1.4830	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 4950:	Loss 1.4827	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
	Epoch 5000:	Loss 1.4848	TrainAcc 0.4216	ValidAcc 0.4239	TestAcc 0.4234	BestValid 0.4270
****** Epoch Time (Excluding Evaluation Cost): 0.125 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 18.700 ms (Max: 19.210, Min: 17.722, Sum: 149.603)
Cluster-Wide Average, Compute: 64.577 ms (Max: 67.676, Min: 61.928, Sum: 516.619)
Cluster-Wide Average, Communication-Layer: 6.510 ms (Max: 7.578, Min: 5.599, Sum: 52.077)
Cluster-Wide Average, Bubble-Imbalance: 3.276 ms (Max: 5.078, Min: 1.763, Sum: 26.210)
Cluster-Wide Average, Communication-Graph: 27.048 ms (Max: 28.100, Min: 26.438, Sum: 216.380)
Cluster-Wide Average, Optimization: 2.412 ms (Max: 2.925, Min: 2.060, Sum: 19.296)
Cluster-Wide Average, Others: 2.273 ms (Max: 3.726, Min: 1.586, Sum: 18.186)
****** Breakdown Sum: 124.796 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.137 GB (Max: 4.034, Min: 2.854, Sum: 25.099)
Cluster-Wide Average, Graph-Level Communication Throughput: 98.323 Gbps (Max: 103.947, Min: 94.859, Sum: 786.582)
Cluster-Wide Average, Layer-Level Communication Throughput: 32.096 Gbps (Max: 40.754, Min: 23.553, Sum: 256.771)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 1.998 GB
****** Accuracy Results ******
Highest valid_acc: 0.4270
Target test_acc: 0.4261
Epoch to reach the target acc: 1649
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
