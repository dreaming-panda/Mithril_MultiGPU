Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INITInitialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4

Initialized node 2 on machine gnerv4
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.022 seconds.
        It takes 0.016 seconds.
        It takes 0.018 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.019 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.099 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.111 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.459 Gbps (per GPU), 483.673 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.154 Gbps (per GPU), 481.236 Gbps (aggregated)
The layer-level communication performance: 60.155 Gbps (per GPU), 481.237 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.899 Gbps (per GPU), 479.189 Gbps (aggregated)
The layer-level communication performance: 59.864 Gbps (per GPU), 478.913 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.662 Gbps (per GPU), 477.300 Gbps (aggregated)
The layer-level communication performance: 59.618 Gbps (per GPU), 476.948 Gbps (aggregated)
The layer-level communication performance: 59.583 Gbps (per GPU), 476.667 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.624 Gbps (per GPU), 1284.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.621 Gbps (per GPU), 1284.970 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.600 Gbps (per GPU), 1284.798 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.618 Gbps (per GPU), 1284.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.603 Gbps (per GPU), 1284.823 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.591 Gbps (per GPU), 1284.724 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.588 Gbps (per GPU), 1284.702 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.594 Gbps (per GPU), 1284.749 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.579 Gbps (per GPU), 836.635 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.576 Gbps (per GPU), 836.609 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.574 Gbps (per GPU), 836.588 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.583 Gbps (per GPU), 836.665 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.574 Gbps (per GPU), 836.588 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.582 Gbps (per GPU), 836.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.578 Gbps (per GPU), 836.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.578 Gbps (per GPU), 836.623 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.483 Gbps (per GPU), 307.863 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.483 Gbps (per GPU), 307.863 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.481 Gbps (per GPU), 307.849 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.482 Gbps (per GPU), 307.857 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.481 Gbps (per GPU), 307.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.476 Gbps (per GPU), 307.809 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.482 Gbps (per GPU), 307.857 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.476 Gbps (per GPU), 307.811 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.31ms  0.64ms  1.23ms  3.94  2.81K  0.03M
 chk_1  0.31ms  0.65ms  1.23ms  3.92  2.82K  0.03M
 chk_2  0.33ms  0.64ms  1.23ms  3.75  2.80K  0.03M
 chk_3  0.31ms  0.64ms  1.23ms  3.96  2.80K  0.03M
 chk_4  0.32ms  0.64ms  1.24ms  3.90  2.82K  0.03M
 chk_5  0.32ms  0.65ms  1.24ms  3.84  2.75K  0.03M
 chk_6  0.31ms  0.63ms  1.22ms  3.96  2.71K  0.03M
 chk_7  0.31ms  0.64ms  1.23ms  3.91  2.76K  0.03M
 chk_8  0.32ms  0.64ms  1.23ms  3.91  2.79K  0.03M
 chk_9  0.31ms  0.64ms  1.23ms  3.93  2.81K  0.03M
chk_10  0.31ms  0.64ms  1.23ms  3.96  2.81K  0.03M
chk_11  0.32ms  0.64ms  1.24ms  3.90  2.74K  0.03M
chk_12  0.32ms  0.64ms  1.24ms  3.92  2.76K  0.03M
chk_13  0.31ms  0.64ms  1.23ms  3.92  2.75K  0.03M
chk_14  0.31ms  0.64ms  1.24ms  3.97  2.81K  0.03M
chk_15  0.31ms  0.64ms  1.24ms  3.97  2.77K  0.03M
chk_16  0.31ms  0.64ms  1.23ms  3.96  2.78K  0.03M
chk_17  0.31ms  0.64ms  1.24ms  3.93  2.79K  0.03M
chk_18  0.32ms  0.64ms  1.24ms  3.92  2.82K  0.03M
chk_19  0.31ms  0.63ms  1.23ms  3.99  2.81K  0.03M
chk_20  0.32ms  0.64ms  1.24ms  3.91  2.77K  0.03M
chk_21  0.31ms  0.64ms  1.23ms  3.92  2.84K  0.02M
chk_22  0.32ms  0.64ms  1.24ms  3.91  2.78K  0.03M
chk_23  0.32ms  0.64ms  1.24ms  3.91  2.80K  0.03M
chk_24  0.31ms  0.64ms  1.23ms  3.94  2.80K  0.03M
chk_25  0.31ms  0.64ms  1.23ms  3.99  2.81K  0.03M
chk_26  0.31ms  0.64ms  1.24ms  3.98  2.81K  0.03M
chk_27  0.31ms  0.64ms  1.24ms  3.95  2.79K  0.03M
chk_28  0.31ms  0.64ms  1.23ms  3.92  2.77K  0.03M
chk_29  0.31ms  0.64ms  1.23ms  3.94  2.77K  0.03M
chk_30  0.32ms  0.64ms  1.24ms  3.92  2.80K  0.03M
chk_31  0.31ms  0.64ms  1.24ms  3.96  2.78K  0.03M
   Avg  0.31  0.64  1.23
   Max  0.33  0.65  1.24
   Min  0.31  0.63  1.22
 Ratio  1.07  1.02  1.01
   Var  0.00  0.00  0.00
Profiling takes 0.905 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 55)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [55, 111)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [111, 167)
*** Node 4, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [55, 111)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [111, 167)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [167, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [167, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 55)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 55)...
+++++++++ Node 1 initializing the weights for op[0, 55)...
+++++++++ Node 4 initializing the weights for op[111, 167)...
+++++++++ Node 2 initializing the weights for op[55, 111)...
+++++++++ Node 5 initializing the weights for op[111, 167)...
+++++++++ Node 3 initializing the weights for op[55, 111)...
+++++++++ Node 6 initializing the weights for op[167, 229)...
+++++++++ Node 7 initializing the weights for op[167, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 2, starting task scheduling...



*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.4602	TrainAcc 0.4214	ValidAcc 0.4238	TestAcc 0.4233	BestValid 0.4238
	Epoch 50:	Loss 1.5602	TrainAcc 0.4537	ValidAcc 0.4573	TestAcc 0.4578	BestValid 0.4573
	Epoch 100:	Loss 1.5037	TrainAcc 0.4686	ValidAcc 0.4703	TestAcc 0.4674	BestValid 0.4703
	Epoch 150:	Loss 1.4680	TrainAcc 0.4864	ValidAcc 0.4868	TestAcc 0.4859	BestValid 0.4868
	Epoch 200:	Loss 1.4515	TrainAcc 0.4923	ValidAcc 0.4918	TestAcc 0.4887	BestValid 0.4918
	Epoch 250:	Loss 1.4444	TrainAcc 0.4951	ValidAcc 0.4947	TestAcc 0.4923	BestValid 0.4947
	Epoch 300:	Loss 1.4378	TrainAcc 0.4856	ValidAcc 0.4831	TestAcc 0.4808	BestValid 0.4947
	Epoch 350:	Loss 1.4290	TrainAcc 0.4865	ValidAcc 0.4853	TestAcc 0.4816	BestValid 0.4947
	Epoch 400:	Loss 1.4225	TrainAcc 0.4856	ValidAcc 0.4809	TestAcc 0.4806	BestValid 0.4947
	Epoch 450:	Loss 1.4174	TrainAcc 0.5054	ValidAcc 0.4980	TestAcc 0.4986	BestValid 0.4980
	Epoch 500:	Loss 1.4126	TrainAcc 0.5076	ValidAcc 0.5000	TestAcc 0.5015	BestValid 0.5000
	Epoch 550:	Loss 1.4064	TrainAcc 0.4903	ValidAcc 0.4823	TestAcc 0.4828	BestValid 0.5000
	Epoch 600:	Loss 1.4066	TrainAcc 0.5106	ValidAcc 0.5024	TestAcc 0.5018	BestValid 0.5024
	Epoch 650:	Loss 1.4004	TrainAcc 0.5017	ValidAcc 0.4925	TestAcc 0.4937	BestValid 0.5024
	Epoch 700:	Loss 1.3909	TrainAcc 0.5157	ValidAcc 0.5063	TestAcc 0.5061	BestValid 0.5063
	Epoch 750:	Loss 1.3885	TrainAcc 0.5049	ValidAcc 0.4944	TestAcc 0.4963	BestValid 0.5063
	Epoch 800:	Loss 1.3875	TrainAcc 0.5111	ValidAcc 0.5022	TestAcc 0.5024	BestValid 0.5063
	Epoch 850:	Loss 1.3848	TrainAcc 0.5144	ValidAcc 0.5053	TestAcc 0.5042	BestValid 0.5063
	Epoch 900:	Loss 1.3815	TrainAcc 0.5188	ValidAcc 0.5105	TestAcc 0.5084	BestValid 0.5105
	Epoch 950:	Loss 1.3746	TrainAcc 0.5233	ValidAcc 0.5120	TestAcc 0.5129	BestValid 0.5120
	Epoch 1000:	Loss 1.3721	TrainAcc 0.5077	ValidAcc 0.4950	TestAcc 0.4985	BestValid 0.5120
	Epoch 1050:	Loss 1.3679	TrainAcc 0.5119	ValidAcc 0.4984	TestAcc 0.5021	BestValid 0.5120
	Epoch 1100:	Loss 1.3653	TrainAcc 0.5235	ValidAcc 0.5114	TestAcc 0.5113	BestValid 0.5120
	Epoch 1150:	Loss 1.3648	TrainAcc 0.5279	ValidAcc 0.5133	TestAcc 0.5129	BestValid 0.5133
	Epoch 1200:	Loss 1.3590	TrainAcc 0.5133	ValidAcc 0.4978	TestAcc 0.5014	BestValid 0.5133
	Epoch 1250:	Loss 1.3604	TrainAcc 0.5208	ValidAcc 0.5059	TestAcc 0.5079	BestValid 0.5133
	Epoch 1300:	Loss 1.3582	TrainAcc 0.4977	ValidAcc 0.4859	TestAcc 0.4873	BestValid 0.5133
	Epoch 1350:	Loss 1.3570	TrainAcc 0.5245	ValidAcc 0.5104	TestAcc 0.5080	BestValid 0.5133
	Epoch 1400:	Loss 1.3522	TrainAcc 0.5315	ValidAcc 0.5137	TestAcc 0.5149	BestValid 0.5137
	Epoch 1450:	Loss 1.3482	TrainAcc 0.5092	ValidAcc 0.4918	TestAcc 0.4900	BestValid 0.5137
	Epoch 1500:	Loss 1.3478	TrainAcc 0.5358	ValidAcc 0.5156	TestAcc 0.5163	BestValid 0.5156
	Epoch 1550:	Loss 1.3454	TrainAcc 0.5164	ValidAcc 0.5025	TestAcc 0.5027	BestValid 0.5156
	Epoch 1600:	Loss 1.3381	TrainAcc 0.5374	ValidAcc 0.5160	TestAcc 0.5155	BestValid 0.5160
	Epoch 1650:	Loss 1.3404	TrainAcc 0.5379	ValidAcc 0.5139	TestAcc 0.5148	BestValid 0.5160
	Epoch 1700:	Loss 1.3351	TrainAcc 0.5424	ValidAcc 0.5178	TestAcc 0.5161	BestValid 0.5178
	Epoch 1750:	Loss 1.3388	TrainAcc 0.5267	ValidAcc 0.5074	TestAcc 0.5063	BestValid 0.5178
	Epoch 1800:	Loss 1.3349	TrainAcc 0.5447	ValidAcc 0.5186	TestAcc 0.5176	BestValid 0.5186
	Epoch 1850:	Loss 1.3305	TrainAcc 0.5125	ValidAcc 0.4931	TestAcc 0.4956	BestValid 0.5186
	Epoch 1900:	Loss 1.3301	TrainAcc 0.5413	ValidAcc 0.5150	TestAcc 0.5134	BestValid 0.5186
	Epoch 1950:	Loss 1.3295	TrainAcc 0.5200	ValidAcc 0.4988	TestAcc 0.5006	BestValid 0.5186
	Epoch 2000:	Loss 1.3247	TrainAcc 0.5472	ValidAcc 0.5183	TestAcc 0.5169	BestValid 0.5186
	Epoch 2050:	Loss 1.3253	TrainAcc 0.5481	ValidAcc 0.5219	TestAcc 0.5183	BestValid 0.5219
	Epoch 2100:	Loss 1.3276	TrainAcc 0.5306	ValidAcc 0.5040	TestAcc 0.5025	BestValid 0.5219
	Epoch 2150:	Loss 1.3185	TrainAcc 0.5244	ValidAcc 0.4963	TestAcc 0.5015	BestValid 0.5219
	Epoch 2200:	Loss 1.3159	TrainAcc 0.5451	ValidAcc 0.5162	TestAcc 0.5192	BestValid 0.5219
	Epoch 2250:	Loss 1.3144	TrainAcc 0.5522	ValidAcc 0.5203	TestAcc 0.5203	BestValid 0.5219
	Epoch 2300:	Loss 1.3175	TrainAcc 0.5507	ValidAcc 0.5209	TestAcc 0.5200	BestValid 0.5219
	Epoch 2350:	Loss 1.3114	TrainAcc 0.5416	ValidAcc 0.5091	TestAcc 0.5082	BestValid 0.5219
	Epoch 2400:	Loss 1.3124	TrainAcc 0.5549	ValidAcc 0.5225	TestAcc 0.5212	BestValid 0.5225
	Epoch 2450:	Loss 1.3067	TrainAcc 0.5314	ValidAcc 0.5080	TestAcc 0.5089	BestValid 0.5225
	Epoch 2500:	Loss 1.3048	TrainAcc 0.5419	ValidAcc 0.5120	TestAcc 0.5146	BestValid 0.5225
	Epoch 2550:	Loss 1.3039	TrainAcc 0.5540	ValidAcc 0.5236	TestAcc 0.5225	BestValid 0.5236
	Epoch 2600:	Loss 1.3044	TrainAcc 0.5446	ValidAcc 0.5125	TestAcc 0.5143	BestValid 0.5236
	Epoch 2650:	Loss 1.3036	TrainAcc 0.5278	ValidAcc 0.5033	TestAcc 0.5030	BestValid 0.5236
	Epoch 2700:	Loss 1.2970	TrainAcc 0.5585	ValidAcc 0.5215	TestAcc 0.5219	BestValid 0.5236
	Epoch 2750:	Loss 1.2952	TrainAcc 0.5605	ValidAcc 0.5220	TestAcc 0.5221	BestValid 0.5236
	Epoch 2800:	Loss 1.2949	TrainAcc 0.5600	ValidAcc 0.5196	TestAcc 0.5218	BestValid 0.5236
	Epoch 2850:	Loss 1.2909	TrainAcc 0.5252	ValidAcc 0.5013	TestAcc 0.5038	BestValid 0.5236
	Epoch 2900:	Loss 1.3057	TrainAcc 0.5043	ValidAcc 0.4825	TestAcc 0.4840	BestValid 0.5236
	Epoch 2950:	Loss 1.2939	TrainAcc 0.5587	ValidAcc 0.5155	TestAcc 0.5160	BestValid 0.5236
	Epoch 3000:	Loss 1.2901	TrainAcc 0.5501	ValidAcc 0.5040	TestAcc 0.5049	BestValid 0.5236
	Epoch 3050:	Loss 1.2864	TrainAcc 0.5631	ValidAcc 0.5199	TestAcc 0.5223	BestValid 0.5236
	Epoch 3100:	Loss 1.2864	TrainAcc 0.5504	ValidAcc 0.5127	TestAcc 0.5142	BestValid 0.5236
	Epoch 3150:	Loss 1.2816	TrainAcc 0.5607	ValidAcc 0.5164	TestAcc 0.5182	BestValid 0.5236
	Epoch 3200:	Loss 1.2813	TrainAcc 0.5436	ValidAcc 0.5056	TestAcc 0.5100	BestValid 0.5236
	Epoch 3250:	Loss 1.2797	TrainAcc 0.5440	ValidAcc 0.5065	TestAcc 0.5117	BestValid 0.5236
	Epoch 3300:	Loss 1.2800	TrainAcc 0.5647	ValidAcc 0.5187	TestAcc 0.5213	BestValid 0.5236
	Epoch 3350:	Loss 1.2760	TrainAcc 0.5497	ValidAcc 0.5135	TestAcc 0.5179	BestValid 0.5236
	Epoch 3400:	Loss 1.2792	TrainAcc 0.5708	ValidAcc 0.5260	TestAcc 0.5252	BestValid 0.5260
	Epoch 3450:	Loss 1.2757	TrainAcc 0.5390	ValidAcc 0.5069	TestAcc 0.5102	BestValid 0.5260
	Epoch 3500:	Loss 1.2809	TrainAcc 0.4857	ValidAcc 0.4545	TestAcc 0.4572	BestValid 0.5260
	Epoch 3550:	Loss 1.2720	TrainAcc 0.5395	ValidAcc 0.5067	TestAcc 0.5106	BestValid 0.5260
	Epoch 3600:	Loss 1.2690	TrainAcc 0.5607	ValidAcc 0.5238	TestAcc 0.5216	BestValid 0.5260
	Epoch 3650:	Loss 1.2747	TrainAcc 0.5474	ValidAcc 0.5004	TestAcc 0.5019	BestValid 0.5260
	Epoch 3700:	Loss 1.2673	TrainAcc 0.5698	ValidAcc 0.5260	TestAcc 0.5242	BestValid 0.5260
	Epoch 3750:	Loss 1.2665	TrainAcc 0.5610	ValidAcc 0.5115	TestAcc 0.5145	BestValid 0.5260
	Epoch 3800:	Loss 1.2632	TrainAcc 0.5609	ValidAcc 0.5102	TestAcc 0.5108	BestValid 0.5260
	Epoch 3850:	Loss 1.2692	TrainAcc 0.5367	ValidAcc 0.5023	TestAcc 0.5062	BestValid 0.5260
	Epoch 3900:	Loss 1.2618	TrainAcc 0.5641	ValidAcc 0.5228	TestAcc 0.5236	BestValid 0.5260
	Epoch 3950:	Loss 1.2594	TrainAcc 0.5576	ValidAcc 0.5158	TestAcc 0.5166	BestValid 0.5260
	Epoch 4000:	Loss 1.2590	TrainAcc 0.5776	ValidAcc 0.5265	TestAcc 0.5260	BestValid 0.5265
	Epoch 4050:	Loss 1.2687	TrainAcc 0.5499	ValidAcc 0.4988	TestAcc 0.5006	BestValid 0.5265
	Epoch 4100:	Loss 1.2546	TrainAcc 0.5691	ValidAcc 0.5256	TestAcc 0.5251	BestValid 0.5265
	Epoch 4150:	Loss 1.2566	TrainAcc 0.5415	ValidAcc 0.4996	TestAcc 0.5050	BestValid 0.5265
	Epoch 4200:	Loss 1.2539	TrainAcc 0.5798	ValidAcc 0.5259	TestAcc 0.5254	BestValid 0.5265
	Epoch 4250:	Loss 1.2517	TrainAcc 0.5755	ValidAcc 0.5265	TestAcc 0.5287	BestValid 0.5265
	Epoch 4300:	Loss 1.2548	TrainAcc 0.5544	ValidAcc 0.5151	TestAcc 0.5184	BestValid 0.5265
	Epoch 4350:	Loss 1.2535	TrainAcc 0.5624	ValidAcc 0.5127	TestAcc 0.5112	BestValid 0.5265
	Epoch 4400:	Loss 1.2612	TrainAcc 0.5249	ValidAcc 0.4956	TestAcc 0.4974	BestValid 0.5265
	Epoch 4450:	Loss 1.2510	TrainAcc 0.5721	ValidAcc 0.5218	TestAcc 0.5237	BestValid 0.5265
	Epoch 4500:	Loss 1.2466	TrainAcc 0.5796	ValidAcc 0.5224	TestAcc 0.5242	BestValid 0.5265
	Epoch 4550:	Loss 1.2484	TrainAcc 0.5768	ValidAcc 0.5211	TestAcc 0.5219	BestValid 0.5265
	Epoch 4600:	Loss 1.2449	TrainAcc 0.5360	ValidAcc 0.4998	TestAcc 0.5023	BestValid 0.5265
	Epoch 4650:	Loss 1.2440	TrainAcc 0.5794	ValidAcc 0.5190	TestAcc 0.5213	BestValid 0.5265
	Epoch 4700:	Loss 1.2406	TrainAcc 0.5379	ValidAcc 0.4969	TestAcc 0.5013	BestValid 0.5265
	Epoch 4750:	Loss 1.2407	TrainAcc 0.5689	ValidAcc 0.5190	TestAcc 0.5206	BestValid 0.5265
	Epoch 4800:	Loss 1.2447	TrainAcc 0.5732	ValidAcc 0.5231	TestAcc 0.5229	BestValid 0.5265
	Epoch 4850:	Loss 1.2417	TrainAcc 0.5635	ValidAcc 0.5082	TestAcc 0.5124	BestValid 0.5265
	Epoch 4900:	Loss 1.2381	TrainAcc 0.5811	ValidAcc 0.5187	TestAcc 0.5193	BestValid 0.5265
	Epoch 4950:	Loss 1.2380	TrainAcc 0.5830	ValidAcc 0.5218	TestAcc 0.5230	BestValid 0.5265
	Epoch 5000:	Loss 1.2356	TrainAcc 0.5679	ValidAcc 0.5091	TestAcc 0.5141	BestValid 0.5265
****** Epoch Time (Excluding Evaluation Cost): 0.153 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 22.624 ms (Max: 23.487, Min: 21.262, Sum: 180.993)
Cluster-Wide Average, Compute: 80.716 ms (Max: 90.974, Min: 72.461, Sum: 645.730)
Cluster-Wide Average, Communication-Layer: 6.526 ms (Max: 7.570, Min: 5.712, Sum: 52.208)
Cluster-Wide Average, Bubble-Imbalance: 8.566 ms (Max: 17.579, Min: 0.445, Sum: 68.525)
Cluster-Wide Average, Communication-Graph: 28.250 ms (Max: 31.900, Min: 26.988, Sum: 226.000)
Cluster-Wide Average, Optimization: 2.578 ms (Max: 5.482, Min: 1.469, Sum: 20.622)
Cluster-Wide Average, Others: 4.190 ms (Max: 6.200, Min: 1.425, Sum: 33.518)
****** Breakdown Sum: 153.450 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.569 GB (Max: 4.434, Min: 3.383, Sum: 28.550)
Cluster-Wide Average, Graph-Level Communication Throughput: 94.045 Gbps (Max: 101.238, Min: 76.992, Sum: 752.362)
Cluster-Wide Average, Layer-Level Communication Throughput: 32.025 Gbps (Max: 40.705, Min: 23.718, Sum: 256.196)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 1.995 GB
****** Accuracy Results ******
Highest valid_acc: 0.5265
Target test_acc: 0.5287
Epoch to reach the target acc: 4249
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
