Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv2
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INITDONE MPI INIT
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3

Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
        It takes 0.017 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
        It takes 0.019 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.119 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.120 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.298 Gbps (per GPU), 482.384 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.965 Gbps (per GPU), 479.721 Gbps (aggregated)
The layer-level communication performance: 59.957 Gbps (per GPU), 479.654 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.716 Gbps (per GPU), 477.724 Gbps (aggregated)
The layer-level communication performance: 59.681 Gbps (per GPU), 477.444 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.435 Gbps (per GPU), 475.478 Gbps (aggregated)
The layer-level communication performance: 59.389 Gbps (per GPU), 475.109 Gbps (aggregated)
The layer-level communication performance: 59.355 Gbps (per GPU), 474.844 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.161 Gbps (per GPU), 1257.285 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.146 Gbps (per GPU), 1257.168 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.105 Gbps (per GPU), 1256.838 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.149 Gbps (per GPU), 1257.191 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.146 Gbps (per GPU), 1257.168 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.146 Gbps (per GPU), 1257.169 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.108 Gbps (per GPU), 1256.863 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.149 Gbps (per GPU), 1257.191 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.901 Gbps (per GPU), 807.205 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.894 Gbps (per GPU), 807.153 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.900 Gbps (per GPU), 807.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.897 Gbps (per GPU), 807.173 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.902 Gbps (per GPU), 807.218 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.896 Gbps (per GPU), 807.166 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.901 Gbps (per GPU), 807.205 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.846 Gbps (per GPU), 806.765 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.266 Gbps (per GPU), 282.127 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.265 Gbps (per GPU), 282.118 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.265 Gbps (per GPU), 282.121 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.266 Gbps (per GPU), 282.126 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.260 Gbps (per GPU), 282.081 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.264 Gbps (per GPU), 282.110 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.262 Gbps (per GPU), 282.099 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.258 Gbps (per GPU), 282.062 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.45ms  0.85ms  1.52ms  3.42  2.81K  0.03M
 chk_1  0.47ms  0.85ms  1.53ms  3.25  2.82K  0.03M
 chk_2  0.45ms  0.84ms  1.53ms  3.43  2.80K  0.03M
 chk_3  0.45ms  0.84ms  1.53ms  3.38  2.80K  0.03M
 chk_4  0.45ms  0.84ms  1.53ms  3.36  2.82K  0.03M
 chk_5  0.46ms  0.85ms  1.53ms  3.34  2.75K  0.03M
 chk_6  0.45ms  0.84ms  1.51ms  3.40  2.71K  0.03M
 chk_7  0.45ms  0.84ms  1.52ms  3.36  2.76K  0.03M
 chk_8  0.45ms  0.84ms  1.52ms  3.38  2.79K  0.03M
 chk_9  0.45ms  0.84ms  1.53ms  3.39  2.81K  0.03M
chk_10  0.45ms  0.84ms  1.52ms  3.40  2.81K  0.03M
chk_11  0.45ms  0.85ms  1.53ms  3.37  2.74K  0.03M
chk_12  0.45ms  0.84ms  1.53ms  3.38  2.76K  0.03M
chk_13  0.45ms  0.84ms  1.53ms  3.36  2.75K  0.03M
chk_14  0.45ms  0.84ms  1.53ms  3.40  2.81K  0.03M
chk_15  0.45ms  0.84ms  1.52ms  3.38  2.77K  0.03M
chk_16  0.45ms  0.84ms  1.52ms  3.40  2.78K  0.03M
chk_17  0.46ms  0.85ms  1.53ms  3.37  2.79K  0.03M
chk_18  0.45ms  0.85ms  1.53ms  3.37  2.82K  0.03M
chk_19  0.45ms  0.83ms  1.52ms  3.40  2.81K  0.03M
chk_20  0.45ms  0.84ms  1.53ms  3.37  2.77K  0.03M
chk_21  0.45ms  0.85ms  1.53ms  3.37  2.84K  0.02M
chk_22  0.45ms  0.84ms  1.53ms  3.38  2.78K  0.03M
chk_23  0.45ms  0.84ms  1.53ms  3.38  2.80K  0.03M
chk_24  0.45ms  0.84ms  1.52ms  3.39  2.80K  0.03M
chk_25  0.45ms  0.84ms  1.53ms  3.40  2.81K  0.03M
chk_26  0.45ms  0.84ms  1.52ms  3.40  2.81K  0.03M
chk_27  0.45ms  0.85ms  1.53ms  3.37  2.79K  0.03M
chk_28  0.45ms  0.84ms  1.53ms  3.38  2.77K  0.03M
chk_29  0.45ms  0.84ms  1.53ms  3.40  2.77K  0.03M
chk_30  0.45ms  0.85ms  1.53ms  3.38  2.80K  0.03M
chk_31  0.45ms  0.85ms  1.53ms  3.40  2.78K  0.03M
   Avg  0.45  0.84  1.53
   Max  0.47  0.85  1.53
   Min  0.45  0.83  1.51
 Ratio  1.06  1.02  1.01
   Var  0.00  0.00  0.00
Profiling takes 1.134 s
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 204)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 5, starting model training...
Num Stages: 4 / 4
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [100, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [308, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 100)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 308)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [204, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 7 owns the model-level partition [308, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 100)...
+++++++++ Node 1 initializing the weights for op[0, 100)...
+++++++++ Node 2 initializing the weights for op[100, 204)...
+++++++++ Node 3 initializing the weights for op[100, 204)...
+++++++++ Node 4 initializing the weights for op[204, 308)...
+++++++++ Node 5 initializing the weights for op[204, 308)...
+++++++++ Node 6 initializing the weights for op[308, 421)...
+++++++++ Node 7 initializing the weights for op[308, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.4760	TrainAcc 0.4207	ValidAcc 0.4232	TestAcc 0.4229	BestValid 0.4232
	Epoch 50:	Loss 1.5253	TrainAcc 0.4794	ValidAcc 0.4773	TestAcc 0.4783	BestValid 0.4773
	Epoch 100:	Loss 1.4247	TrainAcc 0.5109	ValidAcc 0.5055	TestAcc 0.5071	BestValid 0.5055
	Epoch 150:	Loss 1.3919	TrainAcc 0.5234	ValidAcc 0.5185	TestAcc 0.5194	BestValid 0.5185
	Epoch 200:	Loss 1.3776	TrainAcc 0.5305	ValidAcc 0.5203	TestAcc 0.5227	BestValid 0.5203
	Epoch 250:	Loss 1.3605	TrainAcc 0.5347	ValidAcc 0.5237	TestAcc 0.5249	BestValid 0.5237
	Epoch 300:	Loss 1.3576	TrainAcc 0.5039	ValidAcc 0.4972	TestAcc 0.4924	BestValid 0.5237
	Epoch 350:	Loss 1.3443	TrainAcc 0.5422	ValidAcc 0.5289	TestAcc 0.5292	BestValid 0.5289
	Epoch 400:	Loss 1.3353	TrainAcc 0.5444	ValidAcc 0.5290	TestAcc 0.5296	BestValid 0.5290
	Epoch 450:	Loss 1.3267	TrainAcc 0.5462	ValidAcc 0.5271	TestAcc 0.5294	BestValid 0.5290
	Epoch 500:	Loss 1.3183	TrainAcc 0.5422	ValidAcc 0.5234	TestAcc 0.5254	BestValid 0.5290
	Epoch 550:	Loss 1.3145	TrainAcc 0.5511	ValidAcc 0.5306	TestAcc 0.5318	BestValid 0.5306
	Epoch 600:	Loss 1.3076	TrainAcc 0.5471	ValidAcc 0.5243	TestAcc 0.5269	BestValid 0.5306
	Epoch 650:	Loss 1.3016	TrainAcc 0.5547	ValidAcc 0.5292	TestAcc 0.5303	BestValid 0.5306
	Epoch 700:	Loss 1.3041	TrainAcc 0.5571	ValidAcc 0.5328	TestAcc 0.5327	BestValid 0.5328
	Epoch 750:	Loss 1.2891	TrainAcc 0.5610	ValidAcc 0.5349	TestAcc 0.5352	BestValid 0.5349
	Epoch 800:	Loss 1.2876	TrainAcc 0.5647	ValidAcc 0.5345	TestAcc 0.5331	BestValid 0.5349
	Epoch 850:	Loss 1.2820	TrainAcc 0.5634	ValidAcc 0.5335	TestAcc 0.5335	BestValid 0.5349
	Epoch 900:	Loss 1.2723	TrainAcc 0.5682	ValidAcc 0.5326	TestAcc 0.5333	BestValid 0.5349
	Epoch 950:	Loss 1.2656	TrainAcc 0.5540	ValidAcc 0.5181	TestAcc 0.5171	BestValid 0.5349
	Epoch 1000:	Loss 1.2599	TrainAcc 0.5680	ValidAcc 0.5306	TestAcc 0.5326	BestValid 0.5349
	Epoch 1050:	Loss 1.2625	TrainAcc 0.5609	ValidAcc 0.5273	TestAcc 0.5284	BestValid 0.5349
	Epoch 1100:	Loss 1.2523	TrainAcc 0.5780	ValidAcc 0.5365	TestAcc 0.5366	BestValid 0.5365
	Epoch 1150:	Loss 1.2482	TrainAcc 0.5778	ValidAcc 0.5348	TestAcc 0.5345	BestValid 0.5365
	Epoch 1200:	Loss 1.2436	TrainAcc 0.5729	ValidAcc 0.5243	TestAcc 0.5249	BestValid 0.5365
	Epoch 1250:	Loss 1.2369	TrainAcc 0.5691	ValidAcc 0.5240	TestAcc 0.5290	BestValid 0.5365
	Epoch 1300:	Loss 1.2308	TrainAcc 0.5832	ValidAcc 0.5287	TestAcc 0.5322	BestValid 0.5365
	Epoch 1350:	Loss 1.2268	TrainAcc 0.5916	ValidAcc 0.5344	TestAcc 0.5368	BestValid 0.5365
	Epoch 1400:	Loss 1.2227	TrainAcc 0.5922	ValidAcc 0.5336	TestAcc 0.5374	BestValid 0.5365
	Epoch 1450:	Loss 1.2157	TrainAcc 0.5786	ValidAcc 0.5213	TestAcc 0.5248	BestValid 0.5365
	Epoch 1500:	Loss 1.2139	TrainAcc 0.5876	ValidAcc 0.5303	TestAcc 0.5334	BestValid 0.5365
	Epoch 1550:	Loss 1.2042	TrainAcc 0.5962	ValidAcc 0.5323	TestAcc 0.5376	BestValid 0.5365
	Epoch 1600:	Loss 1.2034	TrainAcc 0.5928	ValidAcc 0.5320	TestAcc 0.5368	BestValid 0.5365
	Epoch 1650:	Loss 1.2013	TrainAcc 0.5926	ValidAcc 0.5332	TestAcc 0.5331	BestValid 0.5365
	Epoch 1700:	Loss 1.1904	TrainAcc 0.6013	ValidAcc 0.5296	TestAcc 0.5344	BestValid 0.5365
	Epoch 1750:	Loss 1.1914	TrainAcc 0.5923	ValidAcc 0.5151	TestAcc 0.5218	BestValid 0.5365
	Epoch 1800:	Loss 1.1843	TrainAcc 0.6022	ValidAcc 0.5307	TestAcc 0.5334	BestValid 0.5365
	Epoch 1850:	Loss 1.1773	TrainAcc 0.6001	ValidAcc 0.5281	TestAcc 0.5331	BestValid 0.5365
	Epoch 1900:	Loss 1.1749	TrainAcc 0.6002	ValidAcc 0.5229	TestAcc 0.5250	BestValid 0.5365
	Epoch 1950:	Loss 1.1704	TrainAcc 0.5873	ValidAcc 0.5247	TestAcc 0.5292	BestValid 0.5365
	Epoch 2000:	Loss 1.1704	TrainAcc 0.6094	ValidAcc 0.5319	TestAcc 0.5369	BestValid 0.5365
	Epoch 2050:	Loss 1.1622	TrainAcc 0.6163	ValidAcc 0.5318	TestAcc 0.5385	BestValid 0.5365
	Epoch 2100:	Loss 1.1589	TrainAcc 0.6217	ValidAcc 0.5307	TestAcc 0.5361	BestValid 0.5365
	Epoch 2150:	Loss 1.1489	TrainAcc 0.6204	ValidAcc 0.5295	TestAcc 0.5337	BestValid 0.5365
	Epoch 2200:	Loss 1.1556	TrainAcc 0.6006	ValidAcc 0.5268	TestAcc 0.5314	BestValid 0.5365
	Epoch 2250:	Loss 1.1510	TrainAcc 0.5875	ValidAcc 0.5187	TestAcc 0.5212	BestValid 0.5365
	Epoch 2300:	Loss 1.1428	TrainAcc 0.6244	ValidAcc 0.5261	TestAcc 0.5312	BestValid 0.5365
	Epoch 2350:	Loss 1.1354	TrainAcc 0.6251	ValidAcc 0.5242	TestAcc 0.5300	BestValid 0.5365
	Epoch 2400:	Loss 1.1301	TrainAcc 0.6176	ValidAcc 0.5239	TestAcc 0.5291	BestValid 0.5365
	Epoch 2450:	Loss 1.1311	TrainAcc 0.6354	ValidAcc 0.5277	TestAcc 0.5336	BestValid 0.5365
	Epoch 2500:	Loss 1.1200	TrainAcc 0.6282	ValidAcc 0.5314	TestAcc 0.5365	BestValid 0.5365
	Epoch 2550:	Loss 1.1198	TrainAcc 0.6253	ValidAcc 0.5267	TestAcc 0.5296	BestValid 0.5365
	Epoch 2600:	Loss 1.1157	TrainAcc 0.5931	ValidAcc 0.5207	TestAcc 0.5192	BestValid 0.5365
	Epoch 2650:	Loss 1.1083	TrainAcc 0.6422	ValidAcc 0.5271	TestAcc 0.5335	BestValid 0.5365
	Epoch 2700:	Loss 1.1104	TrainAcc 0.6399	ValidAcc 0.5260	TestAcc 0.5313	BestValid 0.5365
	Epoch 2750:	Loss 1.0978	TrainAcc 0.6375	ValidAcc 0.5267	TestAcc 0.5305	BestValid 0.5365
	Epoch 2800:	Loss 1.0962	TrainAcc 0.6446	ValidAcc 0.5232	TestAcc 0.5297	BestValid 0.5365
	Epoch 2850:	Loss 1.0909	TrainAcc 0.6503	ValidAcc 0.5247	TestAcc 0.5303	BestValid 0.5365
	Epoch 2900:	Loss 1.0877	TrainAcc 0.6401	ValidAcc 0.5210	TestAcc 0.5261	BestValid 0.5365
	Epoch 2950:	Loss 1.0866	TrainAcc 0.6408	ValidAcc 0.5304	TestAcc 0.5346	BestValid 0.5365
	Epoch 3000:	Loss 1.0831	TrainAcc 0.6516	ValidAcc 0.5280	TestAcc 0.5342	BestValid 0.5365
	Epoch 3050:	Loss 1.0745	TrainAcc 0.6440	ValidAcc 0.5150	TestAcc 0.5206	BestValid 0.5365
	Epoch 3100:	Loss 1.0754	TrainAcc 0.6346	ValidAcc 0.5246	TestAcc 0.5279	BestValid 0.5365
	Epoch 3150:	Loss 1.0678	TrainAcc 0.6549	ValidAcc 0.5250	TestAcc 0.5306	BestValid 0.5365
	Epoch 3200:	Loss 1.0612	TrainAcc 0.6556	ValidAcc 0.5199	TestAcc 0.5235	BestValid 0.5365
	Epoch 3250:	Loss 1.0534	TrainAcc 0.6654	ValidAcc 0.5205	TestAcc 0.5252	BestValid 0.5365
	Epoch 3300:	Loss 1.0526	TrainAcc 0.6600	ValidAcc 0.5239	TestAcc 0.5277	BestValid 0.5365
	Epoch 3350:	Loss 1.0472	TrainAcc 0.6677	ValidAcc 0.5219	TestAcc 0.5254	BestValid 0.5365
	Epoch 3400:	Loss 1.0478	TrainAcc 0.6702	ValidAcc 0.5214	TestAcc 0.5268	BestValid 0.5365
	Epoch 3450:	Loss 1.0418	TrainAcc 0.6702	ValidAcc 0.5147	TestAcc 0.5196	BestValid 0.5365
	Epoch 3500:	Loss 1.0452	TrainAcc 0.6535	ValidAcc 0.5161	TestAcc 0.5206	BestValid 0.5365
	Epoch 3550:	Loss 1.0381	TrainAcc 0.6646	ValidAcc 0.5086	TestAcc 0.5135	BestValid 0.5365
	Epoch 3600:	Loss 1.0270	TrainAcc 0.6733	ValidAcc 0.5190	TestAcc 0.5239	BestValid 0.5365
	Epoch 3650:	Loss 1.0230	TrainAcc 0.6705	ValidAcc 0.5162	TestAcc 0.5226	BestValid 0.5365
	Epoch 3700:	Loss 1.0206	TrainAcc 0.6761	ValidAcc 0.5160	TestAcc 0.5224	BestValid 0.5365
	Epoch 3750:	Loss 1.0152	TrainAcc 0.6636	ValidAcc 0.5095	TestAcc 0.5127	BestValid 0.5365
	Epoch 3800:	Loss 1.0219	TrainAcc 0.6474	ValidAcc 0.5193	TestAcc 0.5218	BestValid 0.5365
	Epoch 3850:	Loss 1.0144	TrainAcc 0.6761	ValidAcc 0.5204	TestAcc 0.5221	BestValid 0.5365
	Epoch 3900:	Loss 1.0041	TrainAcc 0.6741	ValidAcc 0.4997	TestAcc 0.5042	BestValid 0.5365
	Epoch 3950:	Loss 0.9953	TrainAcc 0.6750	ValidAcc 0.5208	TestAcc 0.5247	BestValid 0.5365
	Epoch 4000:	Loss 0.9936	TrainAcc 0.6333	ValidAcc 0.5052	TestAcc 0.5087	BestValid 0.5365
	Epoch 4050:	Loss 0.9962	TrainAcc 0.6703	ValidAcc 0.5153	TestAcc 0.5199	BestValid 0.5365
	Epoch 4100:	Loss 0.9847	TrainAcc 0.6839	ValidAcc 0.5135	TestAcc 0.5201	BestValid 0.5365
	Epoch 4150:	Loss 0.9772	TrainAcc 0.6923	ValidAcc 0.5108	TestAcc 0.5158	BestValid 0.5365
	Epoch 4200:	Loss 0.9739	TrainAcc 0.6997	ValidAcc 0.5112	TestAcc 0.5158	BestValid 0.5365
	Epoch 4250:	Loss 0.9736	TrainAcc 0.6710	ValidAcc 0.5087	TestAcc 0.5128	BestValid 0.5365
	Epoch 4300:	Loss 0.9711	TrainAcc 0.6965	ValidAcc 0.5148	TestAcc 0.5205	BestValid 0.5365
	Epoch 4350:	Loss 0.9691	TrainAcc 0.6905	ValidAcc 0.5155	TestAcc 0.5196	BestValid 0.5365
	Epoch 4400:	Loss 0.9685	TrainAcc 0.6799	ValidAcc 0.5036	TestAcc 0.5049	BestValid 0.5365
	Epoch 4450:	Loss 0.9646	TrainAcc 0.6662	ValidAcc 0.5166	TestAcc 0.5197	BestValid 0.5365
	Epoch 4500:	Loss 0.9507	TrainAcc 0.7110	ValidAcc 0.5134	TestAcc 0.5149	BestValid 0.5365
	Epoch 4550:	Loss 0.9507	TrainAcc 0.6835	ValidAcc 0.5090	TestAcc 0.5115	BestValid 0.5365
	Epoch 4600:	Loss 0.9466	TrainAcc 0.7080	ValidAcc 0.5064	TestAcc 0.5078	BestValid 0.5365
	Epoch 4650:	Loss 0.9423	TrainAcc 0.6917	ValidAcc 0.5164	TestAcc 0.5190	BestValid 0.5365
	Epoch 4700:	Loss 0.9417	TrainAcc 0.7057	ValidAcc 0.5117	TestAcc 0.5147	BestValid 0.5365
	Epoch 4750:	Loss 0.9347	TrainAcc 0.6946	ValidAcc 0.5074	TestAcc 0.5076	BestValid 0.5365
	Epoch 4800:	Loss 0.9274	TrainAcc 0.6988	ValidAcc 0.5091	TestAcc 0.5105	BestValid 0.5365
	Epoch 4850:	Loss 0.9251	TrainAcc 0.7228	ValidAcc 0.5125	TestAcc 0.5138	BestValid 0.5365
	Epoch 4900:	Loss 0.9295	TrainAcc 0.7043	ValidAcc 0.4872	TestAcc 0.4894	BestValid 0.5365
	Epoch 4950:	Loss 0.9202	TrainAcc 0.7216	ValidAcc 0.5112	TestAcc 0.5154	BestValid 0.5365
	Epoch 5000:	Loss 0.9119	TrainAcc 0.7256	ValidAcc 0.5054	TestAcc 0.5076	BestValid 0.5365
****** Epoch Time (Excluding Evaluation Cost): 0.190 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 27.774 ms (Max: 28.515, Min: 26.245, Sum: 222.194)
Cluster-Wide Average, Compute: 105.102 ms (Max: 117.045, Min: 98.736, Sum: 840.814)
Cluster-Wide Average, Communication-Layer: 6.649 ms (Max: 8.044, Min: 5.656, Sum: 53.191)
Cluster-Wide Average, Bubble-Imbalance: 10.006 ms (Max: 15.768, Min: 0.592, Sum: 80.044)
Cluster-Wide Average, Communication-Graph: 29.138 ms (Max: 33.991, Min: 27.521, Sum: 233.106)
Cluster-Wide Average, Optimization: 4.679 ms (Max: 9.776, Min: 2.734, Sum: 37.428)
Cluster-Wide Average, Others: 6.783 ms (Max: 9.723, Min: 1.792, Sum: 54.267)
****** Breakdown Sum: 190.131 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.913 GB (Max: 4.760, Min: 3.727, Sum: 31.302)
Cluster-Wide Average, Graph-Level Communication Throughput: 90.739 Gbps (Max: 97.337, Min: 73.175, Sum: 725.909)
Cluster-Wide Average, Layer-Level Communication Throughput: 31.418 Gbps (Max: 40.316, Min: 23.586, Sum: 251.347)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 1.997 GB
****** Accuracy Results ******
Highest valid_acc: 0.5365
Target test_acc: 0.5366
Epoch to reach the target acc: 1099
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
