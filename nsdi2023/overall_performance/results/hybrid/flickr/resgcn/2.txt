Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
        It takes 0.016 seconds.
        It takes 0.017 seconds.
        It takes 0.022 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.109 seconds.
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.113 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.485 Gbps (per GPU), 483.882 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.173 Gbps (per GPU), 481.386 Gbps (aggregated)
The layer-level communication performance: 60.164 Gbps (per GPU), 481.312 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.902 Gbps (per GPU), 479.216 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.652 Gbps (per GPU), 477.213 Gbps (aggregated)
The layer-level communication performance: 59.611 Gbps (per GPU), 476.887 Gbps (aggregated)
The layer-level communication performance: 59.576 Gbps (per GPU), 476.606 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.862 Gbps (per GPU), 478.893 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.356 Gbps (per GPU), 1266.850 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.365 Gbps (per GPU), 1266.922 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.368 Gbps (per GPU), 1266.943 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.348 Gbps (per GPU), 1266.782 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.365 Gbps (per GPU), 1266.924 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.365 Gbps (per GPU), 1266.922 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.368 Gbps (per GPU), 1266.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.350 Gbps (per GPU), 1266.803 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.843 Gbps (per GPU), 838.742 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.835 Gbps (per GPU), 838.679 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.842 Gbps (per GPU), 838.735 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.839 Gbps (per GPU), 838.714 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.832 Gbps (per GPU), 838.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.839 Gbps (per GPU), 838.714 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.793 Gbps (per GPU), 838.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.840 Gbps (per GPU), 838.721 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.049 Gbps (per GPU), 296.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.049 Gbps (per GPU), 296.394 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.050 Gbps (per GPU), 296.398 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.050 Gbps (per GPU), 296.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.050 Gbps (per GPU), 296.402 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.044 Gbps (per GPU), 296.354 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.049 Gbps (per GPU), 296.393 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.044 Gbps (per GPU), 296.353 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.69ms  1.31ms  4.11  2.81K  0.03M
 chk_1  0.32ms  0.69ms  1.31ms  4.09  2.82K  0.03M
 chk_2  0.34ms  0.69ms  1.31ms  3.83  2.80K  0.03M
 chk_3  0.32ms  0.68ms  1.31ms  4.10  2.80K  0.03M
 chk_4  0.33ms  0.68ms  1.32ms  4.04  2.82K  0.03M
 chk_5  0.33ms  0.69ms  1.32ms  3.99  2.75K  0.03M
 chk_6  0.32ms  0.68ms  1.31ms  4.11  2.71K  0.03M
 chk_7  0.32ms  0.68ms  1.31ms  4.06  2.76K  0.03M
 chk_8  0.32ms  0.68ms  1.32ms  4.06  2.79K  0.03M
 chk_9  0.32ms  0.69ms  1.32ms  4.09  2.81K  0.03M
chk_10  0.32ms  0.68ms  1.32ms  4.11  2.81K  0.03M
chk_11  0.33ms  0.69ms  1.32ms  4.03  2.74K  0.03M
chk_12  0.33ms  0.69ms  1.32ms  4.05  2.76K  0.03M
chk_13  0.32ms  0.69ms  1.32ms  4.09  2.75K  0.03M
chk_14  0.32ms  0.68ms  1.32ms  4.08  2.81K  0.03M
chk_15  0.32ms  0.68ms  1.32ms  4.08  2.77K  0.03M
chk_16  0.32ms  0.68ms  1.32ms  4.09  2.78K  0.03M
chk_17  0.32ms  0.68ms  1.32ms  4.07  2.79K  0.03M
chk_18  0.32ms  0.68ms  1.32ms  4.08  2.82K  0.03M
chk_19  0.32ms  0.68ms  1.31ms  4.12  2.81K  0.03M
chk_20  0.33ms  0.69ms  1.32ms  4.05  2.77K  0.03M
chk_21  0.32ms  0.68ms  1.32ms  4.09  2.84K  0.02M
chk_22  0.33ms  0.68ms  1.32ms  4.05  2.78K  0.03M
chk_23  0.32ms  0.68ms  1.32ms  4.06  2.80K  0.03M
chk_24  0.32ms  0.68ms  1.32ms  4.08  2.80K  0.03M
chk_25  0.32ms  0.68ms  1.31ms  4.11  2.81K  0.03M
chk_26  0.32ms  0.68ms  1.32ms  4.14  2.81K  0.03M
chk_27  0.32ms  0.68ms  1.32ms  4.09  2.79K  0.03M
chk_28  0.32ms  0.68ms  1.32ms  4.09  2.77K  0.03M
chk_29  0.32ms  0.68ms  1.32ms  4.08  2.77K  0.03M
chk_30  0.32ms  0.68ms  1.33ms  4.09  2.80K  0.03M
chk_31  0.32ms  0.68ms  1.32ms  4.09  2.78K  0.03M
   Avg  0.32  0.68  1.32
   Max  0.34  0.69  1.33
   Min  0.32  0.68  1.31
 Ratio  1.08  1.02  1.02
   Var  0.00  0.00  0.00
Profiling takes 0.957 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 55)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 55)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [111, 167)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [55, 111)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [167, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [55, 111)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [167, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [111, 167)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 55)...
+++++++++ Node 2 initializing the weights for op[55, 111)...
+++++++++ Node 3 initializing the weights for op[55, 111)...
+++++++++ Node 0 initializing the weights for op[0, 55)...
+++++++++ Node 4 initializing the weights for op[111, 167)...
+++++++++ Node 5 initializing the weights for op[111, 167)...
+++++++++ Node 7 initializing the weights for op[167, 229)...
+++++++++ Node 6 initializing the weights for op[167, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 7, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2452	TrainAcc 0.3294	ValidAcc 0.3336	TestAcc 0.3351	BestValid 0.3336
	Epoch 50:	Loss 1.5637	TrainAcc 0.4573	ValidAcc 0.4594	TestAcc 0.4597	BestValid 0.4594
	Epoch 100:	Loss 1.5036	TrainAcc 0.4743	ValidAcc 0.4752	TestAcc 0.4729	BestValid 0.4752
	Epoch 150:	Loss 1.4628	TrainAcc 0.4884	ValidAcc 0.4882	TestAcc 0.4876	BestValid 0.4882
	Epoch 200:	Loss 1.4447	TrainAcc 0.4806	ValidAcc 0.4783	TestAcc 0.4786	BestValid 0.4882
	Epoch 250:	Loss 1.4321	TrainAcc 0.4942	ValidAcc 0.4881	TestAcc 0.4890	BestValid 0.4882
	Epoch 300:	Loss 1.4236	TrainAcc 0.4643	ValidAcc 0.4617	TestAcc 0.4600	BestValid 0.4882
	Epoch 350:	Loss 1.4112	TrainAcc 0.5047	ValidAcc 0.4963	TestAcc 0.4969	BestValid 0.4963
	Epoch 400:	Loss 1.4047	TrainAcc 0.5049	ValidAcc 0.4952	TestAcc 0.4950	BestValid 0.4963
	Epoch 450:	Loss 1.3983	TrainAcc 0.4589	ValidAcc 0.4534	TestAcc 0.4559	BestValid 0.4963
	Epoch 500:	Loss 1.3944	TrainAcc 0.5140	ValidAcc 0.5035	TestAcc 0.5058	BestValid 0.5035
	Epoch 550:	Loss 1.3891	TrainAcc 0.5158	ValidAcc 0.5061	TestAcc 0.5055	BestValid 0.5061
	Epoch 600:	Loss 1.3872	TrainAcc 0.5184	ValidAcc 0.5080	TestAcc 0.5074	BestValid 0.5080
	Epoch 650:	Loss 1.3826	TrainAcc 0.5166	ValidAcc 0.5043	TestAcc 0.5055	BestValid 0.5080
	Epoch 700:	Loss 1.3759	TrainAcc 0.5185	ValidAcc 0.5061	TestAcc 0.5064	BestValid 0.5080
	Epoch 750:	Loss 1.3713	TrainAcc 0.5226	ValidAcc 0.5108	TestAcc 0.5086	BestValid 0.5108
	Epoch 800:	Loss 1.3667	TrainAcc 0.5229	ValidAcc 0.5107	TestAcc 0.5078	BestValid 0.5108
	Epoch 850:	Loss 1.3627	TrainAcc 0.5092	ValidAcc 0.4930	TestAcc 0.4982	BestValid 0.5108
	Epoch 900:	Loss 1.3636	TrainAcc 0.4995	ValidAcc 0.4870	TestAcc 0.4888	BestValid 0.5108
	Epoch 950:	Loss 1.3598	TrainAcc 0.5267	ValidAcc 0.5113	TestAcc 0.5125	BestValid 0.5113
	Epoch 1000:	Loss 1.3542	TrainAcc 0.5314	ValidAcc 0.5157	TestAcc 0.5141	BestValid 0.5157
	Epoch 1050:	Loss 1.3541	TrainAcc 0.5262	ValidAcc 0.5113	TestAcc 0.5121	BestValid 0.5157
	Epoch 1100:	Loss 1.3508	TrainAcc 0.5277	ValidAcc 0.5094	TestAcc 0.5097	BestValid 0.5157
	Epoch 1150:	Loss 1.3437	TrainAcc 0.5358	ValidAcc 0.5177	TestAcc 0.5183	BestValid 0.5177
	Epoch 1200:	Loss 1.3425	TrainAcc 0.5256	ValidAcc 0.5083	TestAcc 0.5131	BestValid 0.5177
	Epoch 1250:	Loss 1.3446	TrainAcc 0.5325	ValidAcc 0.5173	TestAcc 0.5153	BestValid 0.5177
	Epoch 1300:	Loss 1.3443	TrainAcc 0.5413	ValidAcc 0.5206	TestAcc 0.5197	BestValid 0.5206
	Epoch 1350:	Loss 1.3343	TrainAcc 0.5395	ValidAcc 0.5192	TestAcc 0.5184	BestValid 0.5206
	Epoch 1400:	Loss 1.3328	TrainAcc 0.5402	ValidAcc 0.5187	TestAcc 0.5197	BestValid 0.5206
	Epoch 1450:	Loss 1.3324	TrainAcc 0.5096	ValidAcc 0.4909	TestAcc 0.4884	BestValid 0.5206
	Epoch 1500:	Loss 1.3269	TrainAcc 0.5467	ValidAcc 0.5208	TestAcc 0.5226	BestValid 0.5208
	Epoch 1550:	Loss 1.3275	TrainAcc 0.5319	ValidAcc 0.5121	TestAcc 0.5137	BestValid 0.5208
	Epoch 1600:	Loss 1.3232	TrainAcc 0.5485	ValidAcc 0.5213	TestAcc 0.5232	BestValid 0.5213
	Epoch 1650:	Loss 1.3229	TrainAcc 0.5425	ValidAcc 0.5212	TestAcc 0.5203	BestValid 0.5213
	Epoch 1700:	Loss 1.3213	TrainAcc 0.5354	ValidAcc 0.5127	TestAcc 0.5135	BestValid 0.5213
	Epoch 1750:	Loss 1.3279	TrainAcc 0.5230	ValidAcc 0.5000	TestAcc 0.5023	BestValid 0.5213
	Epoch 1800:	Loss 1.3212	TrainAcc 0.5288	ValidAcc 0.5093	TestAcc 0.5113	BestValid 0.5213
	Epoch 1850:	Loss 1.3128	TrainAcc 0.5425	ValidAcc 0.5173	TestAcc 0.5174	BestValid 0.5213
	Epoch 1900:	Loss 1.3111	TrainAcc 0.5511	ValidAcc 0.5184	TestAcc 0.5223	BestValid 0.5213
	Epoch 1950:	Loss 1.3084	TrainAcc 0.5519	ValidAcc 0.5218	TestAcc 0.5242	BestValid 0.5218
	Epoch 2000:	Loss 1.3077	TrainAcc 0.5530	ValidAcc 0.5180	TestAcc 0.5205	BestValid 0.5218
	Epoch 2050:	Loss 1.3040	TrainAcc 0.5509	ValidAcc 0.5181	TestAcc 0.5231	BestValid 0.5218
	Epoch 2100:	Loss 1.3051	TrainAcc 0.5558	ValidAcc 0.5223	TestAcc 0.5246	BestValid 0.5223
	Epoch 2150:	Loss 1.3059	TrainAcc 0.5569	ValidAcc 0.5227	TestAcc 0.5269	BestValid 0.5227
	Epoch 2200:	Loss 1.3010	TrainAcc 0.5097	ValidAcc 0.4901	TestAcc 0.4942	BestValid 0.5227
	Epoch 2250:	Loss 1.2957	TrainAcc 0.5535	ValidAcc 0.5187	TestAcc 0.5220	BestValid 0.5227
	Epoch 2300:	Loss 1.2941	TrainAcc 0.5523	ValidAcc 0.5177	TestAcc 0.5204	BestValid 0.5227
	Epoch 2350:	Loss 1.2932	TrainAcc 0.5537	ValidAcc 0.5167	TestAcc 0.5198	BestValid 0.5227
	Epoch 2400:	Loss 1.2877	TrainAcc 0.5409	ValidAcc 0.5129	TestAcc 0.5162	BestValid 0.5227
	Epoch 2450:	Loss 1.2897	TrainAcc 0.5524	ValidAcc 0.5182	TestAcc 0.5201	BestValid 0.5227
	Epoch 2500:	Loss 1.2923	TrainAcc 0.5374	ValidAcc 0.5007	TestAcc 0.5045	BestValid 0.5227
	Epoch 2550:	Loss 1.2886	TrainAcc 0.5482	ValidAcc 0.5160	TestAcc 0.5180	BestValid 0.5227
	Epoch 2600:	Loss 1.2869	TrainAcc 0.5475	ValidAcc 0.5122	TestAcc 0.5122	BestValid 0.5227
	Epoch 2650:	Loss 1.2893	TrainAcc 0.5577	ValidAcc 0.5203	TestAcc 0.5245	BestValid 0.5227
	Epoch 2700:	Loss 1.2848	TrainAcc 0.5627	ValidAcc 0.5180	TestAcc 0.5229	BestValid 0.5227
	Epoch 2750:	Loss 1.2852	TrainAcc 0.5568	ValidAcc 0.5178	TestAcc 0.5211	BestValid 0.5227
	Epoch 2800:	Loss 1.2797	TrainAcc 0.5711	ValidAcc 0.5267	TestAcc 0.5300	BestValid 0.5267
	Epoch 2850:	Loss 1.2762	TrainAcc 0.5094	ValidAcc 0.4877	TestAcc 0.4926	BestValid 0.5267
	Epoch 2900:	Loss 1.2848	TrainAcc 0.4840	ValidAcc 0.4658	TestAcc 0.4694	BestValid 0.5267
	Epoch 2950:	Loss 1.2778	TrainAcc 0.5389	ValidAcc 0.5063	TestAcc 0.5127	BestValid 0.5267
	Epoch 3000:	Loss 1.2712	TrainAcc 0.5348	ValidAcc 0.4995	TestAcc 0.5064	BestValid 0.5267
	Epoch 3050:	Loss 1.2727	TrainAcc 0.5534	ValidAcc 0.5138	TestAcc 0.5184	BestValid 0.5267
	Epoch 3100:	Loss 1.2717	TrainAcc 0.5506	ValidAcc 0.5125	TestAcc 0.5172	BestValid 0.5267
	Epoch 3150:	Loss 1.2674	TrainAcc 0.5723	ValidAcc 0.5228	TestAcc 0.5264	BestValid 0.5267
	Epoch 3200:	Loss 1.2782	TrainAcc 0.5609	ValidAcc 0.5163	TestAcc 0.5194	BestValid 0.5267
	Epoch 3250:	Loss 1.2664	TrainAcc 0.5230	ValidAcc 0.4936	TestAcc 0.4980	BestValid 0.5267
	Epoch 3300:	Loss 1.2650	TrainAcc 0.5728	ValidAcc 0.5193	TestAcc 0.5233	BestValid 0.5267
	Epoch 3350:	Loss 1.2648	TrainAcc 0.5715	ValidAcc 0.5209	TestAcc 0.5250	BestValid 0.5267
	Epoch 3400:	Loss 1.2672	TrainAcc 0.5679	ValidAcc 0.5204	TestAcc 0.5246	BestValid 0.5267
	Epoch 3450:	Loss 1.2621	TrainAcc 0.5562	ValidAcc 0.5185	TestAcc 0.5201	BestValid 0.5267
	Epoch 3500:	Loss 1.2591	TrainAcc 0.5565	ValidAcc 0.5069	TestAcc 0.5090	BestValid 0.5267
	Epoch 3550:	Loss 1.2606	TrainAcc 0.5419	ValidAcc 0.4900	TestAcc 0.4934	BestValid 0.5267
	Epoch 3600:	Loss 1.2587	TrainAcc 0.5665	ValidAcc 0.5112	TestAcc 0.5185	BestValid 0.5267
	Epoch 3650:	Loss 1.2569	TrainAcc 0.5411	ValidAcc 0.5047	TestAcc 0.5102	BestValid 0.5267
	Epoch 3700:	Loss 1.2552	TrainAcc 0.5799	ValidAcc 0.5273	TestAcc 0.5313	BestValid 0.5273
	Epoch 3750:	Loss 1.2529	TrainAcc 0.5697	ValidAcc 0.5147	TestAcc 0.5173	BestValid 0.5273
	Epoch 3800:	Loss 1.2530	TrainAcc 0.5703	ValidAcc 0.5172	TestAcc 0.5232	BestValid 0.5273
	Epoch 3850:	Loss 1.2483	TrainAcc 0.5574	ValidAcc 0.5100	TestAcc 0.5146	BestValid 0.5273
	Epoch 3900:	Loss 1.2473	TrainAcc 0.5743	ValidAcc 0.5186	TestAcc 0.5211	BestValid 0.5273
	Epoch 3950:	Loss 1.2473	TrainAcc 0.5726	ValidAcc 0.5221	TestAcc 0.5258	BestValid 0.5273
	Epoch 4000:	Loss 1.2447	TrainAcc 0.5808	ValidAcc 0.5255	TestAcc 0.5308	BestValid 0.5273
	Epoch 4050:	Loss 1.2557	TrainAcc 0.5154	ValidAcc 0.4830	TestAcc 0.4886	BestValid 0.5273
	Epoch 4100:	Loss 1.2454	TrainAcc 0.5758	ValidAcc 0.5171	TestAcc 0.5240	BestValid 0.5273
	Epoch 4150:	Loss 1.2520	TrainAcc 0.5557	ValidAcc 0.5044	TestAcc 0.5092	BestValid 0.5273
	Epoch 4200:	Loss 1.2414	TrainAcc 0.5837	ValidAcc 0.5245	TestAcc 0.5289	BestValid 0.5273
	Epoch 4250:	Loss 1.2445	TrainAcc 0.5681	ValidAcc 0.5121	TestAcc 0.5177	BestValid 0.5273
	Epoch 4300:	Loss 1.2397	TrainAcc 0.5513	ValidAcc 0.5069	TestAcc 0.5146	BestValid 0.5273
	Epoch 4350:	Loss 1.2453	TrainAcc 0.5595	ValidAcc 0.5146	TestAcc 0.5163	BestValid 0.5273
	Epoch 4400:	Loss 1.2382	TrainAcc 0.5776	ValidAcc 0.5181	TestAcc 0.5226	BestValid 0.5273
	Epoch 4450:	Loss 1.2609	TrainAcc 0.5809	ValidAcc 0.5241	TestAcc 0.5286	BestValid 0.5273
	Epoch 4500:	Loss 1.2388	TrainAcc 0.5520	ValidAcc 0.5089	TestAcc 0.5157	BestValid 0.5273
	Epoch 4550:	Loss 1.2358	TrainAcc 0.5508	ValidAcc 0.5022	TestAcc 0.5108	BestValid 0.5273
	Epoch 4600:	Loss 1.2325	TrainAcc 0.5881	ValidAcc 0.5245	TestAcc 0.5270	BestValid 0.5273
	Epoch 4650:	Loss 1.2336	TrainAcc 0.5771	ValidAcc 0.5146	TestAcc 0.5213	BestValid 0.5273
	Epoch 4700:	Loss 1.2352	TrainAcc 0.5820	ValidAcc 0.5153	TestAcc 0.5187	BestValid 0.5273
	Epoch 4750:	Loss 1.2315	TrainAcc 0.5883	ValidAcc 0.5216	TestAcc 0.5262	BestValid 0.5273
	Epoch 4800:	Loss 1.2278	TrainAcc 0.5666	ValidAcc 0.5112	TestAcc 0.5171	BestValid 0.5273
	Epoch 4850:	Loss 1.2296	TrainAcc 0.5663	ValidAcc 0.5222	TestAcc 0.5233	BestValid 0.5273
	Epoch 4900:	Loss 1.2297	TrainAcc 0.5780	ValidAcc 0.5158	TestAcc 0.5211	BestValid 0.5273
	Epoch 4950:	Loss 1.2276	TrainAcc 0.5820	ValidAcc 0.5138	TestAcc 0.5219	BestValid 0.5273
	Epoch 5000:	Loss 1.2242	TrainAcc 0.5472	ValidAcc 0.5027	TestAcc 0.5110	BestValid 0.5273
****** Epoch Time (Excluding Evaluation Cost): 0.154 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 22.723 ms (Max: 23.597, Min: 21.153, Sum: 181.783)
Cluster-Wide Average, Compute: 81.561 ms (Max: 91.359, Min: 76.519, Sum: 652.485)
Cluster-Wide Average, Communication-Layer: 6.592 ms (Max: 7.630, Min: 5.737, Sum: 52.735)
Cluster-Wide Average, Bubble-Imbalance: 9.136 ms (Max: 13.816, Min: 0.465, Sum: 73.091)
Cluster-Wide Average, Communication-Graph: 27.812 ms (Max: 28.430, Min: 27.006, Sum: 222.495)
Cluster-Wide Average, Optimization: 2.490 ms (Max: 5.138, Min: 1.399, Sum: 19.921)
Cluster-Wide Average, Others: 3.943 ms (Max: 5.844, Min: 1.442, Sum: 31.547)
****** Breakdown Sum: 154.257 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.568 GB (Max: 4.432, Min: 3.383, Sum: 28.548)
Cluster-Wide Average, Graph-Level Communication Throughput: 96.153 Gbps (Max: 98.742, Min: 94.922, Sum: 769.223)
Cluster-Wide Average, Layer-Level Communication Throughput: 31.670 Gbps (Max: 39.647, Min: 23.697, Sum: 253.364)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 1.995 GB
****** Accuracy Results ******
Highest valid_acc: 0.5273
Target test_acc: 0.5313
Epoch to reach the target acc: 3699
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
