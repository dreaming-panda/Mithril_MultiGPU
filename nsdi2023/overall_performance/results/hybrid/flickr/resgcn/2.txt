Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.030 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
        It takes 0.017 seconds.
        It takes 0.019 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.006 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.116 seconds.
Building the Label Vector...
        It takes 0.121 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.255 Gbps (per GPU), 482.044 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.944 Gbps (per GPU), 479.549 Gbps (aggregated)
The layer-level communication performance: 59.930 Gbps (per GPU), 479.441 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.707 Gbps (per GPU), 477.654 Gbps (aggregated)
The layer-level communication performance: 59.678 Gbps (per GPU), 477.421 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.474 Gbps (per GPU), 475.792 Gbps (aggregated)
The layer-level communication performance: 59.429 Gbps (per GPU), 475.431 Gbps (aggregated)
The layer-level communication performance: 59.396 Gbps (per GPU), 475.167 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.908 Gbps (per GPU), 1247.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.919 Gbps (per GPU), 1247.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.870 Gbps (per GPU), 1246.959 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.913 Gbps (per GPU), 1247.307 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.925 Gbps (per GPU), 1247.400 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.919 Gbps (per GPU), 1247.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.870 Gbps (per GPU), 1246.962 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.919 Gbps (per GPU), 1247.353 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.267 Gbps (per GPU), 810.135 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.267 Gbps (per GPU), 810.135 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.267 Gbps (per GPU), 810.135 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.263 Gbps (per GPU), 810.102 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.267 Gbps (per GPU), 810.135 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.263 Gbps (per GPU), 810.102 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.264 Gbps (per GPU), 810.114 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.264 Gbps (per GPU), 810.109 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.645 Gbps (per GPU), 285.163 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.646 Gbps (per GPU), 285.165 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.645 Gbps (per GPU), 285.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.642 Gbps (per GPU), 285.137 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.643 Gbps (per GPU), 285.146 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.642 Gbps (per GPU), 285.139 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.640 Gbps (per GPU), 285.121 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.640 Gbps (per GPU), 285.118 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.45ms  0.86ms  1.52ms  3.42  2.81K  0.03M
 chk_1  0.47ms  0.86ms  1.54ms  3.27  2.82K  0.03M
 chk_2  0.45ms  0.85ms  1.54ms  3.43  2.80K  0.03M
 chk_3  0.46ms  0.85ms  1.54ms  3.38  2.80K  0.03M
 chk_4  0.46ms  0.85ms  1.54ms  3.36  2.82K  0.03M
 chk_5  0.46ms  0.86ms  1.54ms  3.34  2.75K  0.03M
 chk_6  0.45ms  0.84ms  1.53ms  3.40  2.71K  0.03M
 chk_7  0.46ms  0.85ms  1.53ms  3.36  2.76K  0.03M
 chk_8  0.45ms  0.85ms  1.53ms  3.38  2.79K  0.03M
 chk_9  0.45ms  0.85ms  1.54ms  3.38  2.81K  0.03M
chk_10  0.45ms  0.84ms  1.53ms  3.38  2.81K  0.03M
chk_11  0.46ms  0.85ms  1.54ms  3.37  2.74K  0.03M
chk_12  0.46ms  0.85ms  1.54ms  3.38  2.76K  0.03M
chk_13  0.46ms  0.85ms  1.54ms  3.37  2.75K  0.03M
chk_14  0.45ms  0.84ms  1.54ms  3.40  2.81K  0.03M
chk_15  0.45ms  0.85ms  1.53ms  3.39  2.77K  0.03M
chk_16  0.45ms  0.84ms  1.53ms  3.40  2.78K  0.03M
chk_17  0.46ms  0.85ms  1.54ms  3.36  2.79K  0.03M
chk_18  0.46ms  0.85ms  1.54ms  3.36  2.82K  0.03M
chk_19  0.45ms  0.84ms  1.53ms  3.39  2.81K  0.03M
chk_20  0.46ms  0.85ms  1.54ms  3.37  2.77K  0.03M
chk_21  0.46ms  0.85ms  1.54ms  3.35  2.84K  0.02M
chk_22  0.45ms  0.85ms  1.53ms  3.37  2.78K  0.03M
chk_23  0.45ms  0.85ms  1.54ms  3.38  2.80K  0.03M
chk_24  0.45ms  0.85ms  1.53ms  3.38  2.80K  0.03M
chk_25  0.45ms  0.85ms  1.55ms  3.42  2.81K  0.03M
chk_26  0.45ms  0.85ms  1.53ms  3.39  2.81K  0.03M
chk_27  0.46ms  0.85ms  1.54ms  3.36  2.79K  0.03M
chk_28  0.46ms  0.85ms  1.54ms  3.37  2.77K  0.03M
chk_29  0.45ms  0.85ms  1.53ms  3.38  2.77K  0.03M
chk_30  0.46ms  0.85ms  1.54ms  3.37  2.80K  0.03M
chk_31  0.46ms  0.85ms  1.54ms  3.37  2.78K  0.03M
   Avg  0.45  0.85  1.54
   Max  0.47  0.86  1.55
   Min  0.45  0.84  1.52
 Ratio  1.05  1.02  1.02
   Var  0.00  0.00  0.00
Profiling takes 1.141 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 100)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 308)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 204)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [204, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [100, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [308, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [308, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 100)...
+++++++++ Node 1 initializing the weights for op[0, 100)...
+++++++++ Node 4 initializing the weights for op[204, 308)...
+++++++++ Node 5 initializing the weights for op[204, 308)...
+++++++++ Node 2 initializing the weights for op[100, 204)...
+++++++++ Node 3 initializing the weights for op[100, 204)...
+++++++++ Node 7 initializing the weights for op[308, 421)...
+++++++++ Node 6 initializing the weights for op[308, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5517	TrainAcc 0.4211	ValidAcc 0.4232	TestAcc 0.4229	BestValid 0.4232
	Epoch 50:	Loss 1.5680	TrainAcc 0.4520	ValidAcc 0.4510	TestAcc 0.4524	BestValid 0.4510
	Epoch 100:	Loss 1.4744	TrainAcc 0.5019	ValidAcc 0.5000	TestAcc 0.5016	BestValid 0.5000
	Epoch 150:	Loss 1.4179	TrainAcc 0.5211	ValidAcc 0.5152	TestAcc 0.5156	BestValid 0.5152
	Epoch 200:	Loss 1.3919	TrainAcc 0.5283	ValidAcc 0.5173	TestAcc 0.5216	BestValid 0.5173
	Epoch 250:	Loss 1.3719	TrainAcc 0.5235	ValidAcc 0.5123	TestAcc 0.5157	BestValid 0.5173
	Epoch 300:	Loss 1.3660	TrainAcc 0.5279	ValidAcc 0.5170	TestAcc 0.5177	BestValid 0.5173
	Epoch 350:	Loss 1.3591	TrainAcc 0.5299	ValidAcc 0.5191	TestAcc 0.5192	BestValid 0.5191
	Epoch 400:	Loss 1.3465	TrainAcc 0.5370	ValidAcc 0.5220	TestAcc 0.5250	BestValid 0.5220
	Epoch 450:	Loss 1.3384	TrainAcc 0.5380	ValidAcc 0.5213	TestAcc 0.5236	BestValid 0.5220
	Epoch 500:	Loss 1.3327	TrainAcc 0.5473	ValidAcc 0.5291	TestAcc 0.5308	BestValid 0.5291
	Epoch 550:	Loss 1.3236	TrainAcc 0.5313	ValidAcc 0.5140	TestAcc 0.5179	BestValid 0.5291
	Epoch 600:	Loss 1.3186	TrainAcc 0.5508	ValidAcc 0.5304	TestAcc 0.5328	BestValid 0.5304
	Epoch 650:	Loss 1.3093	TrainAcc 0.5496	ValidAcc 0.5255	TestAcc 0.5300	BestValid 0.5304
	Epoch 700:	Loss 1.3058	TrainAcc 0.5547	ValidAcc 0.5288	TestAcc 0.5331	BestValid 0.5304
	Epoch 750:	Loss 1.2986	TrainAcc 0.5543	ValidAcc 0.5253	TestAcc 0.5324	BestValid 0.5304
	Epoch 800:	Loss 1.2955	TrainAcc 0.5609	ValidAcc 0.5311	TestAcc 0.5351	BestValid 0.5311
	Epoch 850:	Loss 1.2867	TrainAcc 0.5639	ValidAcc 0.5310	TestAcc 0.5357	BestValid 0.5311
	Epoch 900:	Loss 1.2884	TrainAcc 0.5460	ValidAcc 0.5165	TestAcc 0.5214	BestValid 0.5311
	Epoch 950:	Loss 1.2802	TrainAcc 0.5583	ValidAcc 0.5250	TestAcc 0.5317	BestValid 0.5311
	Epoch 1000:	Loss 1.2736	TrainAcc 0.5594	ValidAcc 0.5247	TestAcc 0.5318	BestValid 0.5311
	Epoch 1050:	Loss 1.2698	TrainAcc 0.5657	ValidAcc 0.5295	TestAcc 0.5337	BestValid 0.5311
	Epoch 1100:	Loss 1.2624	TrainAcc 0.5727	ValidAcc 0.5336	TestAcc 0.5356	BestValid 0.5336
	Epoch 1150:	Loss 1.2553	TrainAcc 0.5745	ValidAcc 0.5277	TestAcc 0.5350	BestValid 0.5336
	Epoch 1200:	Loss 1.2508	TrainAcc 0.5657	ValidAcc 0.5221	TestAcc 0.5254	BestValid 0.5336
	Epoch 1250:	Loss 1.2492	TrainAcc 0.5781	ValidAcc 0.5296	TestAcc 0.5359	BestValid 0.5336
	Epoch 1300:	Loss 1.2406	TrainAcc 0.5753	ValidAcc 0.5299	TestAcc 0.5313	BestValid 0.5336
	Epoch 1350:	Loss 1.2427	TrainAcc 0.5617	ValidAcc 0.5205	TestAcc 0.5261	BestValid 0.5336
	Epoch 1400:	Loss 1.2364	TrainAcc 0.5853	ValidAcc 0.5308	TestAcc 0.5394	BestValid 0.5336
	Epoch 1450:	Loss 1.2270	TrainAcc 0.5422	ValidAcc 0.5016	TestAcc 0.5075	BestValid 0.5336
	Epoch 1500:	Loss 1.2235	TrainAcc 0.5827	ValidAcc 0.5263	TestAcc 0.5347	BestValid 0.5336
	Epoch 1550:	Loss 1.2220	TrainAcc 0.5796	ValidAcc 0.5248	TestAcc 0.5333	BestValid 0.5336
	Epoch 1600:	Loss 1.2119	TrainAcc 0.5843	ValidAcc 0.5240	TestAcc 0.5331	BestValid 0.5336
	Epoch 1650:	Loss 1.2104	TrainAcc 0.5947	ValidAcc 0.5293	TestAcc 0.5395	BestValid 0.5336
	Epoch 1700:	Loss 1.2043	TrainAcc 0.5888	ValidAcc 0.5286	TestAcc 0.5353	BestValid 0.5336
	Epoch 1750:	Loss 1.2209	TrainAcc 0.5316	ValidAcc 0.5040	TestAcc 0.5051	BestValid 0.5336
	Epoch 1800:	Loss 1.1991	TrainAcc 0.5920	ValidAcc 0.5254	TestAcc 0.5336	BestValid 0.5336
	Epoch 1850:	Loss 1.1902	TrainAcc 0.5940	ValidAcc 0.5247	TestAcc 0.5326	BestValid 0.5336
	Epoch 1900:	Loss 1.1871	TrainAcc 0.5609	ValidAcc 0.5158	TestAcc 0.5198	BestValid 0.5336
	Epoch 1950:	Loss 1.1820	TrainAcc 0.5894	ValidAcc 0.5237	TestAcc 0.5298	BestValid 0.5336
	Epoch 2000:	Loss 1.1794	TrainAcc 0.6096	ValidAcc 0.5310	TestAcc 0.5398	BestValid 0.5336
	Epoch 2050:	Loss 1.1749	TrainAcc 0.6107	ValidAcc 0.5311	TestAcc 0.5389	BestValid 0.5336
	Epoch 2100:	Loss 1.1731	TrainAcc 0.6026	ValidAcc 0.5285	TestAcc 0.5339	BestValid 0.5336
	Epoch 2150:	Loss 1.1672	TrainAcc 0.6115	ValidAcc 0.5320	TestAcc 0.5372	BestValid 0.5336
	Epoch 2200:	Loss 1.1619	TrainAcc 0.6047	ValidAcc 0.5251	TestAcc 0.5325	BestValid 0.5336
	Epoch 2250:	Loss 1.1546	TrainAcc 0.5905	ValidAcc 0.5180	TestAcc 0.5262	BestValid 0.5336
	Epoch 2300:	Loss 1.1497	TrainAcc 0.6173	ValidAcc 0.5262	TestAcc 0.5331	BestValid 0.5336
	Epoch 2350:	Loss 1.1685	TrainAcc 0.5776	ValidAcc 0.5105	TestAcc 0.5145	BestValid 0.5336
	Epoch 2400:	Loss 1.1508	TrainAcc 0.6143	ValidAcc 0.5225	TestAcc 0.5312	BestValid 0.5336
	Epoch 2450:	Loss 1.1407	TrainAcc 0.6163	ValidAcc 0.5201	TestAcc 0.5286	BestValid 0.5336
	Epoch 2500:	Loss 1.1383	TrainAcc 0.6157	ValidAcc 0.5239	TestAcc 0.5304	BestValid 0.5336
	Epoch 2550:	Loss 1.1290	TrainAcc 0.6242	ValidAcc 0.5176	TestAcc 0.5237	BestValid 0.5336
	Epoch 2600:	Loss 1.1411	TrainAcc 0.6198	ValidAcc 0.5268	TestAcc 0.5335	BestValid 0.5336
	Epoch 2650:	Loss 1.1212	TrainAcc 0.6178	ValidAcc 0.5182	TestAcc 0.5251	BestValid 0.5336
	Epoch 2700:	Loss 1.1278	TrainAcc 0.5739	ValidAcc 0.4966	TestAcc 0.4998	BestValid 0.5336
	Epoch 2750:	Loss 1.1160	TrainAcc 0.5780	ValidAcc 0.5090	TestAcc 0.5146	BestValid 0.5336
	Epoch 2800:	Loss 1.1102	TrainAcc 0.6303	ValidAcc 0.5219	TestAcc 0.5263	BestValid 0.5336
	Epoch 2850:	Loss 1.1045	TrainAcc 0.6352	ValidAcc 0.5129	TestAcc 0.5194	BestValid 0.5336
	Epoch 2900:	Loss 1.1227	TrainAcc 0.6294	ValidAcc 0.5280	TestAcc 0.5314	BestValid 0.5336
	Epoch 2950:	Loss 1.1009	TrainAcc 0.6400	ValidAcc 0.5169	TestAcc 0.5240	BestValid 0.5336
	Epoch 3000:	Loss 1.1047	TrainAcc 0.6309	ValidAcc 0.5208	TestAcc 0.5254	BestValid 0.5336
	Epoch 3050:	Loss 1.0932	TrainAcc 0.6398	ValidAcc 0.5259	TestAcc 0.5314	BestValid 0.5336
	Epoch 3100:	Loss 1.0875	TrainAcc 0.6313	ValidAcc 0.5221	TestAcc 0.5281	BestValid 0.5336
	Epoch 3150:	Loss 1.0824	TrainAcc 0.6507	ValidAcc 0.5255	TestAcc 0.5304	BestValid 0.5336
	Epoch 3200:	Loss 1.0870	TrainAcc 0.6408	ValidAcc 0.5212	TestAcc 0.5270	BestValid 0.5336
	Epoch 3250:	Loss 1.0806	TrainAcc 0.6500	ValidAcc 0.5250	TestAcc 0.5292	BestValid 0.5336
	Epoch 3300:	Loss 1.0725	TrainAcc 0.6553	ValidAcc 0.5245	TestAcc 0.5296	BestValid 0.5336
	Epoch 3350:	Loss 1.0717	TrainAcc 0.6570	ValidAcc 0.5141	TestAcc 0.5188	BestValid 0.5336
	Epoch 3400:	Loss 1.0681	TrainAcc 0.6485	ValidAcc 0.5205	TestAcc 0.5249	BestValid 0.5336
	Epoch 3450:	Loss 1.0574	TrainAcc 0.6487	ValidAcc 0.5209	TestAcc 0.5276	BestValid 0.5336
	Epoch 3500:	Loss 1.0611	TrainAcc 0.6592	ValidAcc 0.5125	TestAcc 0.5188	BestValid 0.5336
	Epoch 3550:	Loss 1.0512	TrainAcc 0.6667	ValidAcc 0.5180	TestAcc 0.5227	BestValid 0.5336
	Epoch 3600:	Loss 1.0551	TrainAcc 0.6578	ValidAcc 0.5221	TestAcc 0.5259	BestValid 0.5336
	Epoch 3650:	Loss 1.0414	TrainAcc 0.6598	ValidAcc 0.5159	TestAcc 0.5196	BestValid 0.5336
	Epoch 3700:	Loss 1.0399	TrainAcc 0.6433	ValidAcc 0.5130	TestAcc 0.5185	BestValid 0.5336
	Epoch 3750:	Loss 1.0390	TrainAcc 0.6190	ValidAcc 0.4972	TestAcc 0.5036	BestValid 0.5336
	Epoch 3800:	Loss 1.0402	TrainAcc 0.6179	ValidAcc 0.5003	TestAcc 0.5033	BestValid 0.5336
	Epoch 3850:	Loss 1.0347	TrainAcc 0.6663	ValidAcc 0.5210	TestAcc 0.5251	BestValid 0.5336
	Epoch 3900:	Loss 1.0227	TrainAcc 0.6785	ValidAcc 0.5146	TestAcc 0.5180	BestValid 0.5336
	Epoch 3950:	Loss 1.0188	TrainAcc 0.6769	ValidAcc 0.5096	TestAcc 0.5141	BestValid 0.5336
	Epoch 4000:	Loss 1.0076	TrainAcc 0.6766	ValidAcc 0.5171	TestAcc 0.5212	BestValid 0.5336
	Epoch 4050:	Loss 1.0120	TrainAcc 0.6771	ValidAcc 0.5112	TestAcc 0.5154	BestValid 0.5336
	Epoch 4100:	Loss 1.0065	TrainAcc 0.6835	ValidAcc 0.5195	TestAcc 0.5216	BestValid 0.5336
	Epoch 4150:	Loss 1.0073	TrainAcc 0.6811	ValidAcc 0.5063	TestAcc 0.5091	BestValid 0.5336
	Epoch 4200:	Loss 1.0045	TrainAcc 0.6789	ValidAcc 0.5123	TestAcc 0.5142	BestValid 0.5336
	Epoch 4250:	Loss 0.9976	TrainAcc 0.6783	ValidAcc 0.5106	TestAcc 0.5128	BestValid 0.5336
	Epoch 4300:	Loss 0.9866	TrainAcc 0.6895	ValidAcc 0.5126	TestAcc 0.5174	BestValid 0.5336
	Epoch 4350:	Loss 0.9861	TrainAcc 0.6729	ValidAcc 0.5190	TestAcc 0.5248	BestValid 0.5336
	Epoch 4400:	Loss 0.9984	TrainAcc 0.7000	ValidAcc 0.5050	TestAcc 0.5100	BestValid 0.5336
	Epoch 4450:	Loss 0.9786	TrainAcc 0.6830	ValidAcc 0.5065	TestAcc 0.5088	BestValid 0.5336
	Epoch 4500:	Loss 0.9760	TrainAcc 0.6881	ValidAcc 0.5115	TestAcc 0.5154	BestValid 0.5336
	Epoch 4550:	Loss 0.9675	TrainAcc 0.7041	ValidAcc 0.5021	TestAcc 0.5080	BestValid 0.5336
	Epoch 4600:	Loss 0.9695	TrainAcc 0.7033	ValidAcc 0.5151	TestAcc 0.5190	BestValid 0.5336
	Epoch 4650:	Loss 0.9651	TrainAcc 0.6761	ValidAcc 0.5098	TestAcc 0.5115	BestValid 0.5336
	Epoch 4700:	Loss 0.9626	TrainAcc 0.6672	ValidAcc 0.5055	TestAcc 0.5070	BestValid 0.5336
	Epoch 4750:	Loss 0.9535	TrainAcc 0.7120	ValidAcc 0.5098	TestAcc 0.5146	BestValid 0.5336
	Epoch 4800:	Loss 0.9500	TrainAcc 0.7054	ValidAcc 0.5131	TestAcc 0.5163	BestValid 0.5336
	Epoch 4850:	Loss 0.9488	TrainAcc 0.7065	ValidAcc 0.5032	TestAcc 0.5063	BestValid 0.5336
	Epoch 4900:	Loss 0.9459	TrainAcc 0.7004	ValidAcc 0.5087	TestAcc 0.5133	BestValid 0.5336
	Epoch 4950:	Loss 0.9481	TrainAcc 0.7067	ValidAcc 0.5100	TestAcc 0.5142	BestValid 0.5336
	Epoch 5000:	Loss 0.9405	TrainAcc 0.6698	ValidAcc 0.4851	TestAcc 0.4891	BestValid 0.5336
****** Epoch Time (Excluding Evaluation Cost): 0.187 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 27.472 ms (Max: 28.181, Min: 26.154, Sum: 219.776)
Cluster-Wide Average, Compute: 104.207 ms (Max: 115.006, Min: 95.205, Sum: 833.654)
Cluster-Wide Average, Communication-Layer: 6.651 ms (Max: 8.082, Min: 5.627, Sum: 53.206)
Cluster-Wide Average, Bubble-Imbalance: 8.725 ms (Max: 13.781, Min: 0.526, Sum: 69.802)
Cluster-Wide Average, Communication-Graph: 28.911 ms (Max: 33.139, Min: 27.064, Sum: 231.285)
Cluster-Wide Average, Optimization: 4.296 ms (Max: 9.342, Min: 2.447, Sum: 34.372)
Cluster-Wide Average, Others: 6.672 ms (Max: 9.613, Min: 1.751, Sum: 53.373)
****** Breakdown Sum: 186.933 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.913 GB (Max: 4.760, Min: 3.727, Sum: 31.302)
Cluster-Wide Average, Graph-Level Communication Throughput: 91.277 Gbps (Max: 99.644, Min: 75.825, Sum: 730.213)
Cluster-Wide Average, Layer-Level Communication Throughput: 31.407 Gbps (Max: 40.401, Min: 23.868, Sum: 251.255)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 1.997 GB
****** Accuracy Results ******
Highest valid_acc: 0.5336
Target test_acc: 0.5356
Epoch to reach the target acc: 1099
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
