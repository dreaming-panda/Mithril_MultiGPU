Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2

Initialized node 0 on machine gnerv2
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
        It takes 0.016 seconds.
        It takes 0.017 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.025 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.116 seconds.
Building the Label Vector...
        It takes 0.120 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.137 Gbps (per GPU), 481.096 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.830 Gbps (per GPU), 478.640 Gbps (aggregated)
The layer-level communication performance: 59.826 Gbps (per GPU), 478.610 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.578 Gbps (per GPU), 476.627 Gbps (aggregated)
The layer-level communication performance: 59.546 Gbps (per GPU), 476.368 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.334 Gbps (per GPU), 474.670 Gbps (aggregated)
The layer-level communication performance: 59.286 Gbps (per GPU), 474.292 Gbps (aggregated)
The layer-level communication performance: 59.253 Gbps (per GPU), 474.027 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.231 Gbps (per GPU), 1257.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.228 Gbps (per GPU), 1257.827 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.172 Gbps (per GPU), 1257.380 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.225 Gbps (per GPU), 1257.804 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.223 Gbps (per GPU), 1257.780 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.234 Gbps (per GPU), 1257.875 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.175 Gbps (per GPU), 1257.403 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.228 Gbps (per GPU), 1257.827 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.154 Gbps (per GPU), 809.229 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.155 Gbps (per GPU), 809.243 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.156 Gbps (per GPU), 809.249 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.155 Gbps (per GPU), 809.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.156 Gbps (per GPU), 809.249 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.151 Gbps (per GPU), 809.210 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.154 Gbps (per GPU), 809.229 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.151 Gbps (per GPU), 809.210 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.784 Gbps (per GPU), 294.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.782 Gbps (per GPU), 294.257 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.783 Gbps (per GPU), 294.260 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.778 Gbps (per GPU), 294.221 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.782 Gbps (per GPU), 294.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.779 Gbps (per GPU), 294.231 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.779 Gbps (per GPU), 294.230 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.779 Gbps (per GPU), 294.232 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.57ms  0.84ms  1.50ms  2.66  2.81K  0.03M
 chk_1  0.56ms  0.84ms  1.51ms  2.70  2.82K  0.03M
 chk_2  0.57ms  0.84ms  1.50ms  2.64  2.80K  0.03M
 chk_3  0.45ms  0.84ms  1.51ms  3.32  2.80K  0.03M
 chk_4  0.45ms  0.83ms  1.51ms  3.33  2.82K  0.03M
 chk_5  0.46ms  0.84ms  1.51ms  3.31  2.75K  0.03M
 chk_6  0.45ms  0.83ms  1.50ms  3.36  2.71K  0.03M
 chk_7  0.45ms  0.84ms  1.51ms  3.34  2.76K  0.03M
 chk_8  0.45ms  0.83ms  1.51ms  3.35  2.79K  0.03M
 chk_9  0.45ms  0.83ms  1.51ms  3.36  2.81K  0.03M
chk_10  0.45ms  0.83ms  1.51ms  3.36  2.81K  0.03M
chk_11  0.45ms  0.84ms  1.51ms  3.33  2.74K  0.03M
chk_12  0.45ms  0.84ms  1.51ms  3.34  2.76K  0.03M
chk_13  0.45ms  0.84ms  1.51ms  3.34  2.75K  0.03M
chk_14  0.45ms  0.83ms  1.51ms  3.36  2.81K  0.03M
chk_15  0.45ms  0.84ms  1.51ms  3.35  2.77K  0.03M
chk_16  0.45ms  0.83ms  1.51ms  3.36  2.78K  0.03M
chk_17  0.46ms  0.84ms  1.51ms  3.32  2.79K  0.03M
chk_18  0.46ms  0.84ms  1.51ms  3.32  2.82K  0.03M
chk_19  0.45ms  0.83ms  1.50ms  3.38  2.81K  0.03M
chk_20  0.45ms  0.84ms  1.51ms  3.34  2.77K  0.03M
chk_21  0.45ms  0.84ms  1.51ms  3.32  2.84K  0.02M
chk_22  0.45ms  0.83ms  1.51ms  3.34  2.78K  0.03M
chk_23  0.45ms  0.83ms  1.51ms  3.34  2.80K  0.03M
chk_24  0.45ms  0.83ms  1.51ms  3.36  2.80K  0.03M
chk_25  0.45ms  0.84ms  1.51ms  3.37  2.81K  0.03M
chk_26  0.45ms  0.83ms  1.50ms  3.38  2.81K  0.03M
chk_27  0.45ms  0.84ms  1.51ms  3.33  2.79K  0.03M
chk_28  0.47ms  0.84ms  1.51ms  3.20  2.77K  0.03M
chk_29  0.48ms  0.84ms  1.50ms  3.14  2.77K  0.03M
chk_30  0.47ms  0.84ms  1.51ms  3.20  2.80K  0.03M
chk_31  0.45ms  0.84ms  1.51ms  3.37  2.78K  0.03M
   Avg  0.46  0.84  1.51
   Max  0.57  0.84  1.51
   Min  0.45  0.83  1.50
 Ratio  1.28  1.02  1.01
   Var  0.00  0.00  0.00
Profiling takes 1.133 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 100)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [100, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 204)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [204, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [308, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [308, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 4 owns the model-level partition [204, 308)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 100)...
+++++++++ Node 1 initializing the weights for op[0, 100)...
+++++++++ Node 4 initializing the weights for op[204, 308)...
+++++++++ Node 5 initializing the weights for op[204, 308)...
+++++++++ Node 6 initializing the weights for op[308, 421)...
+++++++++ Node 3 initializing the weights for op[100, 204)...
+++++++++ Node 2 initializing the weights for op[100, 204)...
+++++++++ Node 7 initializing the weights for op[308, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2917	TrainAcc 0.3958	ValidAcc 0.3982	TestAcc 0.3939	BestValid 0.3982
	Epoch 50:	Loss 1.5482	TrainAcc 0.4664	ValidAcc 0.4628	TestAcc 0.4657	BestValid 0.4628
	Epoch 100:	Loss 1.4466	TrainAcc 0.4929	ValidAcc 0.4880	TestAcc 0.4924	BestValid 0.4880
	Epoch 150:	Loss 1.4078	TrainAcc 0.5203	ValidAcc 0.5133	TestAcc 0.5159	BestValid 0.5133
	Epoch 200:	Loss 1.3841	TrainAcc 0.5266	ValidAcc 0.5170	TestAcc 0.5223	BestValid 0.5170
	Epoch 250:	Loss 1.3712	TrainAcc 0.5267	ValidAcc 0.5185	TestAcc 0.5201	BestValid 0.5185
	Epoch 300:	Loss 1.3566	TrainAcc 0.5141	ValidAcc 0.5022	TestAcc 0.5021	BestValid 0.5185
	Epoch 350:	Loss 1.3476	TrainAcc 0.5384	ValidAcc 0.5248	TestAcc 0.5276	BestValid 0.5248
	Epoch 400:	Loss 1.3390	TrainAcc 0.5349	ValidAcc 0.5202	TestAcc 0.5239	BestValid 0.5248
	Epoch 450:	Loss 1.3284	TrainAcc 0.5366	ValidAcc 0.5212	TestAcc 0.5243	BestValid 0.5248
	Epoch 500:	Loss 1.3203	TrainAcc 0.5433	ValidAcc 0.5251	TestAcc 0.5291	BestValid 0.5251
	Epoch 550:	Loss 1.3137	TrainAcc 0.5477	ValidAcc 0.5259	TestAcc 0.5284	BestValid 0.5259
	Epoch 600:	Loss 1.3059	TrainAcc 0.5532	ValidAcc 0.5324	TestAcc 0.5333	BestValid 0.5324
	Epoch 650:	Loss 1.3005	TrainAcc 0.5446	ValidAcc 0.5224	TestAcc 0.5268	BestValid 0.5324
	Epoch 700:	Loss 1.2922	TrainAcc 0.5439	ValidAcc 0.5197	TestAcc 0.5246	BestValid 0.5324
	Epoch 750:	Loss 1.2883	TrainAcc 0.5304	ValidAcc 0.5090	TestAcc 0.5153	BestValid 0.5324
	Epoch 800:	Loss 1.2824	TrainAcc 0.5547	ValidAcc 0.5289	TestAcc 0.5343	BestValid 0.5324
	Epoch 850:	Loss 1.2735	TrainAcc 0.5602	ValidAcc 0.5310	TestAcc 0.5300	BestValid 0.5324
	Epoch 900:	Loss 1.2726	TrainAcc 0.5682	ValidAcc 0.5322	TestAcc 0.5348	BestValid 0.5324
	Epoch 950:	Loss 1.2693	TrainAcc 0.5641	ValidAcc 0.5314	TestAcc 0.5362	BestValid 0.5324
	Epoch 1000:	Loss 1.2643	TrainAcc 0.5710	ValidAcc 0.5330	TestAcc 0.5358	BestValid 0.5330
	Epoch 1050:	Loss 1.2533	TrainAcc 0.5722	ValidAcc 0.5329	TestAcc 0.5365	BestValid 0.5330
	Epoch 1100:	Loss 1.2499	TrainAcc 0.5741	ValidAcc 0.5350	TestAcc 0.5361	BestValid 0.5350
	Epoch 1150:	Loss 1.2448	TrainAcc 0.5789	ValidAcc 0.5350	TestAcc 0.5372	BestValid 0.5350
	Epoch 1200:	Loss 1.2409	TrainAcc 0.5769	ValidAcc 0.5294	TestAcc 0.5359	BestValid 0.5350
	Epoch 1250:	Loss 1.2403	TrainAcc 0.5800	ValidAcc 0.5291	TestAcc 0.5328	BestValid 0.5350
	Epoch 1300:	Loss 1.2369	TrainAcc 0.5504	ValidAcc 0.5182	TestAcc 0.5226	BestValid 0.5350
	Epoch 1350:	Loss 1.2334	TrainAcc 0.5823	ValidAcc 0.5369	TestAcc 0.5369	BestValid 0.5369
	Epoch 1400:	Loss 1.2272	TrainAcc 0.5874	ValidAcc 0.5332	TestAcc 0.5376	BestValid 0.5369
	Epoch 1450:	Loss 1.2180	TrainAcc 0.5796	ValidAcc 0.5225	TestAcc 0.5258	BestValid 0.5369
	Epoch 1500:	Loss 1.2181	TrainAcc 0.5857	ValidAcc 0.5336	TestAcc 0.5341	BestValid 0.5369
	Epoch 1550:	Loss 1.2148	TrainAcc 0.5839	ValidAcc 0.5288	TestAcc 0.5355	BestValid 0.5369
	Epoch 1600:	Loss 1.2071	TrainAcc 0.5715	ValidAcc 0.5278	TestAcc 0.5307	BestValid 0.5369
	Epoch 1650:	Loss 1.2048	TrainAcc 0.5644	ValidAcc 0.5139	TestAcc 0.5110	BestValid 0.5369
	Epoch 1700:	Loss 1.2010	TrainAcc 0.5968	ValidAcc 0.5299	TestAcc 0.5357	BestValid 0.5369
	Epoch 1750:	Loss 1.1939	TrainAcc 0.5961	ValidAcc 0.5231	TestAcc 0.5295	BestValid 0.5369
	Epoch 1800:	Loss 1.1950	TrainAcc 0.5943	ValidAcc 0.5362	TestAcc 0.5340	BestValid 0.5369
	Epoch 1850:	Loss 1.1915	TrainAcc 0.6046	ValidAcc 0.5338	TestAcc 0.5367	BestValid 0.5369
	Epoch 1900:	Loss 1.1826	TrainAcc 0.5986	ValidAcc 0.5243	TestAcc 0.5284	BestValid 0.5369
	Epoch 1950:	Loss 1.1917	TrainAcc 0.5663	ValidAcc 0.5240	TestAcc 0.5262	BestValid 0.5369
	Epoch 2000:	Loss 1.1770	TrainAcc 0.5982	ValidAcc 0.5327	TestAcc 0.5363	BestValid 0.5369
	Epoch 2050:	Loss 1.1703	TrainAcc 0.6125	ValidAcc 0.5295	TestAcc 0.5335	BestValid 0.5369
	Epoch 2100:	Loss 1.1697	TrainAcc 0.5936	ValidAcc 0.5249	TestAcc 0.5317	BestValid 0.5369
	Epoch 2150:	Loss 1.1685	TrainAcc 0.6141	ValidAcc 0.5295	TestAcc 0.5339	BestValid 0.5369
	Epoch 2200:	Loss 1.1583	TrainAcc 0.6158	ValidAcc 0.5342	TestAcc 0.5366	BestValid 0.5369
	Epoch 2250:	Loss 1.1570	TrainAcc 0.6150	ValidAcc 0.5239	TestAcc 0.5321	BestValid 0.5369
	Epoch 2300:	Loss 1.1508	TrainAcc 0.5826	ValidAcc 0.5247	TestAcc 0.5287	BestValid 0.5369
	Epoch 2350:	Loss 1.1548	TrainAcc 0.6185	ValidAcc 0.5311	TestAcc 0.5346	BestValid 0.5369
	Epoch 2400:	Loss 1.1461	TrainAcc 0.6172	ValidAcc 0.5276	TestAcc 0.5287	BestValid 0.5369
	Epoch 2450:	Loss 1.1426	TrainAcc 0.6182	ValidAcc 0.5311	TestAcc 0.5337	BestValid 0.5369
	Epoch 2500:	Loss 1.1403	TrainAcc 0.6165	ValidAcc 0.5293	TestAcc 0.5295	BestValid 0.5369
	Epoch 2550:	Loss 1.1373	TrainAcc 0.6252	ValidAcc 0.5322	TestAcc 0.5361	BestValid 0.5369
	Epoch 2600:	Loss 1.1371	TrainAcc 0.5838	ValidAcc 0.5069	TestAcc 0.5087	BestValid 0.5369
	Epoch 2650:	Loss 1.1336	TrainAcc 0.5808	ValidAcc 0.5136	TestAcc 0.5193	BestValid 0.5369
	Epoch 2700:	Loss 1.1211	TrainAcc 0.6311	ValidAcc 0.5258	TestAcc 0.5342	BestValid 0.5369
	Epoch 2750:	Loss 1.1187	TrainAcc 0.6305	ValidAcc 0.5283	TestAcc 0.5348	BestValid 0.5369
	Epoch 2800:	Loss 1.1201	TrainAcc 0.6333	ValidAcc 0.5284	TestAcc 0.5325	BestValid 0.5369
	Epoch 2850:	Loss 1.1176	TrainAcc 0.6100	ValidAcc 0.5269	TestAcc 0.5321	BestValid 0.5369
	Epoch 2900:	Loss 1.1252	TrainAcc 0.5869	ValidAcc 0.5147	TestAcc 0.5187	BestValid 0.5369
	Epoch 2950:	Loss 1.1066	TrainAcc 0.6347	ValidAcc 0.5228	TestAcc 0.5298	BestValid 0.5369
	Epoch 3000:	Loss 1.1020	TrainAcc 0.6287	ValidAcc 0.5265	TestAcc 0.5331	BestValid 0.5369
	Epoch 3050:	Loss 1.0948	TrainAcc 0.6393	ValidAcc 0.5177	TestAcc 0.5259	BestValid 0.5369
	Epoch 3100:	Loss 1.0942	TrainAcc 0.6452	ValidAcc 0.5247	TestAcc 0.5327	BestValid 0.5369
	Epoch 3150:	Loss 1.0933	TrainAcc 0.6405	ValidAcc 0.5311	TestAcc 0.5338	BestValid 0.5369
	Epoch 3200:	Loss 1.0861	TrainAcc 0.6428	ValidAcc 0.5125	TestAcc 0.5168	BestValid 0.5369
	Epoch 3250:	Loss 1.0838	TrainAcc 0.6398	ValidAcc 0.5145	TestAcc 0.5162	BestValid 0.5369
	Epoch 3300:	Loss 1.0783	TrainAcc 0.6402	ValidAcc 0.5255	TestAcc 0.5330	BestValid 0.5369
	Epoch 3350:	Loss 1.0777	TrainAcc 0.6221	ValidAcc 0.5056	TestAcc 0.5137	BestValid 0.5369
	Epoch 3400:	Loss 1.0720	TrainAcc 0.6383	ValidAcc 0.5168	TestAcc 0.5232	BestValid 0.5369
	Epoch 3450:	Loss 1.0731	TrainAcc 0.6448	ValidAcc 0.5203	TestAcc 0.5256	BestValid 0.5369
	Epoch 3500:	Loss 1.0746	TrainAcc 0.6591	ValidAcc 0.5228	TestAcc 0.5275	BestValid 0.5369
	Epoch 3550:	Loss 1.0740	TrainAcc 0.6459	ValidAcc 0.5212	TestAcc 0.5304	BestValid 0.5369
	Epoch 3600:	Loss 1.0613	TrainAcc 0.6519	ValidAcc 0.5244	TestAcc 0.5288	BestValid 0.5369
	Epoch 3650:	Loss 1.0584	TrainAcc 0.6638	ValidAcc 0.5239	TestAcc 0.5253	BestValid 0.5369
	Epoch 3700:	Loss 1.0510	TrainAcc 0.6620	ValidAcc 0.5220	TestAcc 0.5279	BestValid 0.5369
	Epoch 3750:	Loss 1.0503	TrainAcc 0.6487	ValidAcc 0.5231	TestAcc 0.5236	BestValid 0.5369
	Epoch 3800:	Loss 1.0472	TrainAcc 0.6433	ValidAcc 0.5173	TestAcc 0.5211	BestValid 0.5369
	Epoch 3850:	Loss 1.0607	TrainAcc 0.6519	ValidAcc 0.5234	TestAcc 0.5275	BestValid 0.5369
	Epoch 3900:	Loss 1.0445	TrainAcc 0.6627	ValidAcc 0.5256	TestAcc 0.5276	BestValid 0.5369
	Epoch 3950:	Loss 1.0335	TrainAcc 0.6651	ValidAcc 0.5099	TestAcc 0.5119	BestValid 0.5369
	Epoch 4000:	Loss 1.0311	TrainAcc 0.6666	ValidAcc 0.5228	TestAcc 0.5283	BestValid 0.5369
	Epoch 4050:	Loss 1.0310	TrainAcc 0.6398	ValidAcc 0.4826	TestAcc 0.4877	BestValid 0.5369
	Epoch 4100:	Loss 1.0252	TrainAcc 0.6656	ValidAcc 0.5087	TestAcc 0.5112	BestValid 0.5369
	Epoch 4150:	Loss 1.0202	TrainAcc 0.6698	ValidAcc 0.5067	TestAcc 0.5077	BestValid 0.5369
	Epoch 4200:	Loss 1.0150	TrainAcc 0.6639	ValidAcc 0.5118	TestAcc 0.5194	BestValid 0.5369
	Epoch 4250:	Loss 1.0131	TrainAcc 0.6716	ValidAcc 0.5116	TestAcc 0.5121	BestValid 0.5369
	Epoch 4300:	Loss 1.0072	TrainAcc 0.6760	ValidAcc 0.5221	TestAcc 0.5265	BestValid 0.5369
	Epoch 4350:	Loss 1.0043	TrainAcc 0.6893	ValidAcc 0.5157	TestAcc 0.5199	BestValid 0.5369
	Epoch 4400:	Loss 1.0000	TrainAcc 0.6870	ValidAcc 0.5160	TestAcc 0.5199	BestValid 0.5369
	Epoch 4450:	Loss 0.9929	TrainAcc 0.6852	ValidAcc 0.5109	TestAcc 0.5168	BestValid 0.5369
	Epoch 4500:	Loss 0.9924	TrainAcc 0.6904	ValidAcc 0.5122	TestAcc 0.5127	BestValid 0.5369
	Epoch 4550:	Loss 0.9893	TrainAcc 0.6864	ValidAcc 0.5013	TestAcc 0.5052	BestValid 0.5369
	Epoch 4600:	Loss 0.9850	TrainAcc 0.6767	ValidAcc 0.5096	TestAcc 0.5185	BestValid 0.5369
	Epoch 4650:	Loss 0.9842	TrainAcc 0.6652	ValidAcc 0.5187	TestAcc 0.5237	BestValid 0.5369
	Epoch 4700:	Loss 0.9825	TrainAcc 0.6977	ValidAcc 0.5153	TestAcc 0.5177	BestValid 0.5369
	Epoch 4750:	Loss 0.9764	TrainAcc 0.6982	ValidAcc 0.5185	TestAcc 0.5200	BestValid 0.5369
	Epoch 4800:	Loss 0.9719	TrainAcc 0.6770	ValidAcc 0.5171	TestAcc 0.5224	BestValid 0.5369
	Epoch 4850:	Loss 0.9657	TrainAcc 0.6874	ValidAcc 0.5140	TestAcc 0.5199	BestValid 0.5369
	Epoch 4900:	Loss 0.9625	TrainAcc 0.6933	ValidAcc 0.5191	TestAcc 0.5229	BestValid 0.5369
	Epoch 4950:	Loss 0.9875	TrainAcc 0.6877	ValidAcc 0.4911	TestAcc 0.4970	BestValid 0.5369
	Epoch 5000:	Loss 0.9606	TrainAcc 0.7000	ValidAcc 0.4996	TestAcc 0.5023	BestValid 0.5369
****** Epoch Time (Excluding Evaluation Cost): 0.183 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 27.208 ms (Max: 27.838, Min: 26.077, Sum: 217.667)
Cluster-Wide Average, Compute: 103.255 ms (Max: 112.771, Min: 95.305, Sum: 826.038)
Cluster-Wide Average, Communication-Layer: 6.605 ms (Max: 7.987, Min: 5.680, Sum: 52.841)
Cluster-Wide Average, Bubble-Imbalance: 7.532 ms (Max: 13.181, Min: 0.753, Sum: 60.252)
Cluster-Wide Average, Communication-Graph: 29.246 ms (Max: 35.048, Min: 26.989, Sum: 233.965)
Cluster-Wide Average, Optimization: 3.896 ms (Max: 7.960, Min: 2.348, Sum: 31.168)
Cluster-Wide Average, Others: 5.714 ms (Max: 8.324, Min: 1.738, Sum: 45.715)
****** Breakdown Sum: 183.456 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.913 GB (Max: 4.760, Min: 3.727, Sum: 31.302)
Cluster-Wide Average, Graph-Level Communication Throughput: 89.944 Gbps (Max: 101.079, Min: 70.221, Sum: 719.554)
Cluster-Wide Average, Layer-Level Communication Throughput: 31.640 Gbps (Max: 40.843, Min: 23.748, Sum: 253.117)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 1.997 GB
****** Accuracy Results ******
Highest valid_acc: 0.5369
Target test_acc: 0.5369
Epoch to reach the target acc: 1349
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
