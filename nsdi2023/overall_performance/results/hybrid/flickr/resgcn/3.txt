Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 3 on machine gnerv4
DONE MPI INIT
Initialized node 0 on machine gnerv4
Initialized node 1 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
Initialized node 5 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.029 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
        It takes 0.018 seconds.
        It takes 0.016 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.024 seconds.
Building the Feature Vector...
        It takes 0.019 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.117 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.603 Gbps (per GPU), 484.828 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.293 Gbps (per GPU), 482.348 Gbps (aggregated)
The layer-level communication performance: 60.298 Gbps (per GPU), 482.381 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.020 Gbps (per GPU), 480.160 Gbps (aggregated)
The layer-level communication performance: 59.993 Gbps (per GPU), 479.947 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.693 Gbps (per GPU), 477.542 Gbps (aggregated)
The layer-level communication performance: 59.657 Gbps (per GPU), 477.253 Gbps (aggregated)
The layer-level communication performance: 59.742 Gbps (per GPU), 477.933 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.395 Gbps (per GPU), 1267.163 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.413 Gbps (per GPU), 1267.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.386 Gbps (per GPU), 1267.090 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.386 Gbps (per GPU), 1267.090 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.395 Gbps (per GPU), 1267.161 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.476 Gbps (per GPU), 1267.808 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.392 Gbps (per GPU), 1267.137 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.464 Gbps (per GPU), 1267.712 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.000 Gbps (per GPU), 831.997 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.003 Gbps (per GPU), 832.024 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.996 Gbps (per GPU), 831.969 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.979 Gbps (per GPU), 831.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.000 Gbps (per GPU), 832.004 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.999 Gbps (per GPU), 831.990 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.001 Gbps (per GPU), 832.005 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.910 Gbps (per GPU), 831.282 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.478 Gbps (per GPU), 291.821 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.473 Gbps (per GPU), 291.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.469 Gbps (per GPU), 291.750 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.474 Gbps (per GPU), 291.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.468 Gbps (per GPU), 291.747 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.474 Gbps (per GPU), 291.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.475 Gbps (per GPU), 291.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.475 Gbps (per GPU), 291.800 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.68ms  1.31ms  4.14  2.81K  0.03M
 chk_1  0.32ms  0.69ms  1.31ms  4.13  2.82K  0.03M
 chk_2  0.34ms  0.68ms  1.31ms  3.89  2.80K  0.03M
 chk_3  0.32ms  0.68ms  1.31ms  4.16  2.80K  0.03M
 chk_4  0.32ms  0.68ms  1.32ms  4.06  2.82K  0.03M
 chk_5  0.33ms  0.69ms  1.32ms  3.99  2.75K  0.03M
 chk_6  0.32ms  0.68ms  1.31ms  4.13  2.71K  0.03M
 chk_7  0.32ms  0.68ms  1.31ms  4.10  2.76K  0.03M
 chk_8  0.32ms  0.68ms  1.32ms  4.09  2.79K  0.03M
 chk_9  0.32ms  0.69ms  1.31ms  4.10  2.81K  0.03M
chk_10  0.32ms  0.68ms  1.31ms  4.13  2.81K  0.03M
chk_11  0.32ms  0.69ms  1.32ms  4.08  2.74K  0.03M
chk_12  0.32ms  0.68ms  1.32ms  4.09  2.76K  0.03M
chk_13  0.32ms  0.69ms  1.31ms  4.11  2.75K  0.03M
chk_14  0.32ms  0.68ms  1.31ms  4.10  2.81K  0.03M
chk_15  0.32ms  0.68ms  1.31ms  4.11  2.77K  0.03M
chk_16  0.32ms  0.68ms  1.32ms  4.12  2.78K  0.03M
chk_17  0.32ms  0.68ms  1.32ms  4.09  2.79K  0.03M
chk_18  0.32ms  0.68ms  1.32ms  4.09  2.82K  0.03M
chk_19  0.33ms  0.67ms  1.31ms  3.99  2.81K  0.03M
chk_20  0.32ms  0.68ms  1.32ms  4.07  2.77K  0.03M
chk_21  0.32ms  0.68ms  1.32ms  4.09  2.84K  0.02M
chk_22  0.32ms  0.68ms  1.31ms  4.06  2.78K  0.03M
chk_23  0.32ms  0.68ms  1.32ms  4.06  2.80K  0.03M
chk_24  0.32ms  0.68ms  1.31ms  4.08  2.80K  0.03M
chk_25  0.32ms  0.68ms  1.31ms  4.11  2.81K  0.03M
chk_26  0.32ms  0.68ms  1.32ms  4.12  2.81K  0.03M
chk_27  0.32ms  0.68ms  1.32ms  4.09  2.79K  0.03M
chk_28  0.32ms  0.68ms  1.32ms  4.09  2.77K  0.03M
chk_29  0.32ms  0.68ms  1.31ms  4.09  2.77K  0.03M
chk_30  0.32ms  0.68ms  1.32ms  4.10  2.80K  0.03M
chk_31  0.32ms  0.68ms  1.32ms  4.11  2.78K  0.03M
   Avg  0.32  0.68  1.32
   Max  0.34  0.69  1.32
   Min  0.32  0.67  1.31
 Ratio  1.07  1.02  1.01
   Var  0.00  0.00  0.00
Profiling takes 0.955 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 55)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 55)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2, starting model training...
Num Stages: 4 / 4
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [111, 167)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [55, 111)
*** Node 2, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [167, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [167, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [55, 111)
*** Node 3, constructing the helper classes...
*** Node 4 owns the model-level partition [111, 167)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 55)...
+++++++++ Node 1 initializing the weights for op[0, 55)...
+++++++++ Node 2 initializing the weights for op[55, 111)...
+++++++++ Node 3 initializing the weights for op[55, 111)...
+++++++++ Node 4 initializing the weights for op[111, 167)...
+++++++++ Node 5 initializing the weights for op[111, 167)...
+++++++++ Node 6 initializing the weights for op[167, 229)...
+++++++++ Node 7 initializing the weights for op[167, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.5608	TrainAcc 0.4205	ValidAcc 0.4227	TestAcc 0.4224	BestValid 0.4227
	Epoch 50:	Loss 1.5742	TrainAcc 0.4571	ValidAcc 0.4597	TestAcc 0.4596	BestValid 0.4597
	Epoch 100:	Loss 1.5091	TrainAcc 0.4857	ValidAcc 0.4877	TestAcc 0.4849	BestValid 0.4877
	Epoch 150:	Loss 1.4608	TrainAcc 0.4884	ValidAcc 0.4881	TestAcc 0.4847	BestValid 0.4881
	Epoch 200:	Loss 1.4410	TrainAcc 0.4936	ValidAcc 0.4899	TestAcc 0.4881	BestValid 0.4899
	Epoch 250:	Loss 1.4315	TrainAcc 0.4960	ValidAcc 0.4926	TestAcc 0.4897	BestValid 0.4926
	Epoch 300:	Loss 1.4237	TrainAcc 0.4809	ValidAcc 0.4767	TestAcc 0.4764	BestValid 0.4926
	Epoch 350:	Loss 1.4179	TrainAcc 0.5012	ValidAcc 0.4951	TestAcc 0.4944	BestValid 0.4951
	Epoch 400:	Loss 1.4092	TrainAcc 0.5013	ValidAcc 0.4938	TestAcc 0.4924	BestValid 0.4951
	Epoch 450:	Loss 1.4026	TrainAcc 0.5126	ValidAcc 0.5047	TestAcc 0.5027	BestValid 0.5047
	Epoch 500:	Loss 1.3982	TrainAcc 0.4933	ValidAcc 0.4869	TestAcc 0.4866	BestValid 0.5047
	Epoch 550:	Loss 1.3925	TrainAcc 0.5109	ValidAcc 0.4987	TestAcc 0.5012	BestValid 0.5047
	Epoch 600:	Loss 1.3960	TrainAcc 0.5073	ValidAcc 0.5001	TestAcc 0.4970	BestValid 0.5047
	Epoch 650:	Loss 1.3858	TrainAcc 0.4881	ValidAcc 0.4794	TestAcc 0.4795	BestValid 0.5047
	Epoch 700:	Loss 1.3811	TrainAcc 0.5157	ValidAcc 0.5030	TestAcc 0.5054	BestValid 0.5047
	Epoch 750:	Loss 1.3769	TrainAcc 0.5153	ValidAcc 0.5020	TestAcc 0.5032	BestValid 0.5047
	Epoch 800:	Loss 1.3726	TrainAcc 0.5167	ValidAcc 0.5055	TestAcc 0.5029	BestValid 0.5055
	Epoch 850:	Loss 1.3692	TrainAcc 0.5226	ValidAcc 0.5091	TestAcc 0.5052	BestValid 0.5091
	Epoch 900:	Loss 1.3667	TrainAcc 0.5246	ValidAcc 0.5100	TestAcc 0.5075	BestValid 0.5100
	Epoch 950:	Loss 1.3674	TrainAcc 0.5002	ValidAcc 0.4870	TestAcc 0.4889	BestValid 0.5100
	Epoch 1000:	Loss 1.3623	TrainAcc 0.5295	ValidAcc 0.5134	TestAcc 0.5119	BestValid 0.5134
	Epoch 1050:	Loss 1.3574	TrainAcc 0.5274	ValidAcc 0.5084	TestAcc 0.5076	BestValid 0.5134
	Epoch 1100:	Loss 1.3563	TrainAcc 0.5323	ValidAcc 0.5119	TestAcc 0.5107	BestValid 0.5134
	Epoch 1150:	Loss 1.3506	TrainAcc 0.5312	ValidAcc 0.5113	TestAcc 0.5141	BestValid 0.5134
	Epoch 1200:	Loss 1.3510	TrainAcc 0.5087	ValidAcc 0.4925	TestAcc 0.4882	BestValid 0.5134
	Epoch 1250:	Loss 1.3474	TrainAcc 0.5342	ValidAcc 0.5125	TestAcc 0.5172	BestValid 0.5134
	Epoch 1300:	Loss 1.3486	TrainAcc 0.5283	ValidAcc 0.5078	TestAcc 0.5097	BestValid 0.5134
	Epoch 1350:	Loss 1.3433	TrainAcc 0.5368	ValidAcc 0.5145	TestAcc 0.5175	BestValid 0.5145
	Epoch 1400:	Loss 1.3394	TrainAcc 0.5316	ValidAcc 0.5132	TestAcc 0.5127	BestValid 0.5145
	Epoch 1450:	Loss 1.3414	TrainAcc 0.4572	ValidAcc 0.4417	TestAcc 0.4445	BestValid 0.5145
	Epoch 1500:	Loss 1.3356	TrainAcc 0.5309	ValidAcc 0.5072	TestAcc 0.5101	BestValid 0.5145
	Epoch 1550:	Loss 1.3320	TrainAcc 0.5377	ValidAcc 0.5125	TestAcc 0.5139	BestValid 0.5145
	Epoch 1600:	Loss 1.3379	TrainAcc 0.5274	ValidAcc 0.5013	TestAcc 0.5065	BestValid 0.5145
	Epoch 1650:	Loss 1.3251	TrainAcc 0.5467	ValidAcc 0.5179	TestAcc 0.5198	BestValid 0.5179
	Epoch 1700:	Loss 1.3241	TrainAcc 0.5338	ValidAcc 0.5065	TestAcc 0.5088	BestValid 0.5179
	Epoch 1750:	Loss 1.3254	TrainAcc 0.5363	ValidAcc 0.5076	TestAcc 0.5124	BestValid 0.5179
	Epoch 1800:	Loss 1.3221	TrainAcc 0.5501	ValidAcc 0.5221	TestAcc 0.5240	BestValid 0.5221
	Epoch 1850:	Loss 1.3260	TrainAcc 0.4813	ValidAcc 0.4639	TestAcc 0.4625	BestValid 0.5221
	Epoch 1900:	Loss 1.3152	TrainAcc 0.5507	ValidAcc 0.5206	TestAcc 0.5238	BestValid 0.5221
	Epoch 1950:	Loss 1.3137	TrainAcc 0.5436	ValidAcc 0.5113	TestAcc 0.5140	BestValid 0.5221
	Epoch 2000:	Loss 1.3136	TrainAcc 0.5523	ValidAcc 0.5190	TestAcc 0.5223	BestValid 0.5221
	Epoch 2050:	Loss 1.3142	TrainAcc 0.5334	ValidAcc 0.4997	TestAcc 0.5015	BestValid 0.5221
	Epoch 2100:	Loss 1.3080	TrainAcc 0.5528	ValidAcc 0.5250	TestAcc 0.5250	BestValid 0.5250
	Epoch 2150:	Loss 1.3061	TrainAcc 0.5575	ValidAcc 0.5238	TestAcc 0.5255	BestValid 0.5250
	Epoch 2200:	Loss 1.3086	TrainAcc 0.5475	ValidAcc 0.5144	TestAcc 0.5209	BestValid 0.5250
	Epoch 2250:	Loss 1.3035	TrainAcc 0.5527	ValidAcc 0.5116	TestAcc 0.5208	BestValid 0.5250
	Epoch 2300:	Loss 1.3017	TrainAcc 0.5461	ValidAcc 0.5169	TestAcc 0.5149	BestValid 0.5250
	Epoch 2350:	Loss 1.2990	TrainAcc 0.5272	ValidAcc 0.4978	TestAcc 0.5028	BestValid 0.5250
	Epoch 2400:	Loss 1.3023	TrainAcc 0.5017	ValidAcc 0.4773	TestAcc 0.4838	BestValid 0.5250
	Epoch 2450:	Loss 1.2986	TrainAcc 0.5400	ValidAcc 0.5058	TestAcc 0.5123	BestValid 0.5250
	Epoch 2500:	Loss 1.2953	TrainAcc 0.5091	ValidAcc 0.4838	TestAcc 0.4889	BestValid 0.5250
	Epoch 2550:	Loss 1.2916	TrainAcc 0.5619	ValidAcc 0.5194	TestAcc 0.5244	BestValid 0.5250
	Epoch 2600:	Loss 1.2904	TrainAcc 0.5291	ValidAcc 0.4921	TestAcc 0.5007	BestValid 0.5250
	Epoch 2650:	Loss 1.2965	TrainAcc 0.5323	ValidAcc 0.5043	TestAcc 0.5011	BestValid 0.5250
	Epoch 2700:	Loss 1.2889	TrainAcc 0.5504	ValidAcc 0.5084	TestAcc 0.5100	BestValid 0.5250
	Epoch 2750:	Loss 1.2886	TrainAcc 0.5479	ValidAcc 0.5109	TestAcc 0.5153	BestValid 0.5250
	Epoch 2800:	Loss 1.2899	TrainAcc 0.5582	ValidAcc 0.5252	TestAcc 0.5275	BestValid 0.5252
	Epoch 2850:	Loss 1.2834	TrainAcc 0.5661	ValidAcc 0.5212	TestAcc 0.5266	BestValid 0.5252
	Epoch 2900:	Loss 1.2848	TrainAcc 0.5654	ValidAcc 0.5242	TestAcc 0.5255	BestValid 0.5252
	Epoch 2950:	Loss 1.2876	TrainAcc 0.5479	ValidAcc 0.5114	TestAcc 0.5158	BestValid 0.5252
	Epoch 3000:	Loss 1.2806	TrainAcc 0.5569	ValidAcc 0.5096	TestAcc 0.5136	BestValid 0.5252
	Epoch 3050:	Loss 1.2797	TrainAcc 0.5632	ValidAcc 0.5204	TestAcc 0.5223	BestValid 0.5252
	Epoch 3100:	Loss 1.2783	TrainAcc 0.5166	ValidAcc 0.4846	TestAcc 0.4896	BestValid 0.5252
	Epoch 3150:	Loss 1.2755	TrainAcc 0.5623	ValidAcc 0.5219	TestAcc 0.5260	BestValid 0.5252
	Epoch 3200:	Loss 1.2841	TrainAcc 0.5657	ValidAcc 0.5169	TestAcc 0.5236	BestValid 0.5252
	Epoch 3250:	Loss 1.2739	TrainAcc 0.5308	ValidAcc 0.4934	TestAcc 0.4948	BestValid 0.5252
	Epoch 3300:	Loss 1.2736	TrainAcc 0.5701	ValidAcc 0.5215	TestAcc 0.5266	BestValid 0.5252
	Epoch 3350:	Loss 1.2732	TrainAcc 0.5532	ValidAcc 0.5109	TestAcc 0.5174	BestValid 0.5252
	Epoch 3400:	Loss 1.2669	TrainAcc 0.5699	ValidAcc 0.5206	TestAcc 0.5278	BestValid 0.5252
	Epoch 3450:	Loss 1.2672	TrainAcc 0.5749	ValidAcc 0.5251	TestAcc 0.5292	BestValid 0.5252
	Epoch 3500:	Loss 1.2695	TrainAcc 0.5656	ValidAcc 0.5194	TestAcc 0.5236	BestValid 0.5252
	Epoch 3550:	Loss 1.2757	TrainAcc 0.5425	ValidAcc 0.5053	TestAcc 0.5041	BestValid 0.5252
	Epoch 3600:	Loss 1.2633	TrainAcc 0.5490	ValidAcc 0.5073	TestAcc 0.5133	BestValid 0.5252
	Epoch 3650:	Loss 1.2602	TrainAcc 0.5657	ValidAcc 0.5121	TestAcc 0.5205	BestValid 0.5252
	Epoch 3700:	Loss 1.2613	TrainAcc 0.5783	ValidAcc 0.5245	TestAcc 0.5311	BestValid 0.5252
	Epoch 3750:	Loss 1.2656	TrainAcc 0.5063	ValidAcc 0.4771	TestAcc 0.4829	BestValid 0.5252
	Epoch 3800:	Loss 1.2653	TrainAcc 0.5667	ValidAcc 0.5216	TestAcc 0.5256	BestValid 0.5252
	Epoch 3850:	Loss 1.2579	TrainAcc 0.5702	ValidAcc 0.5201	TestAcc 0.5270	BestValid 0.5252
	Epoch 3900:	Loss 1.2572	TrainAcc 0.5592	ValidAcc 0.5150	TestAcc 0.5206	BestValid 0.5252
	Epoch 3950:	Loss 1.2573	TrainAcc 0.5784	ValidAcc 0.5211	TestAcc 0.5303	BestValid 0.5252
	Epoch 4000:	Loss 1.2546	TrainAcc 0.5559	ValidAcc 0.5085	TestAcc 0.5145	BestValid 0.5252
	Epoch 4050:	Loss 1.2581	TrainAcc 0.5399	ValidAcc 0.5023	TestAcc 0.5060	BestValid 0.5252
	Epoch 4100:	Loss 1.2537	TrainAcc 0.5718	ValidAcc 0.5216	TestAcc 0.5237	BestValid 0.5252
	Epoch 4150:	Loss 1.2517	TrainAcc 0.5622	ValidAcc 0.5084	TestAcc 0.5162	BestValid 0.5252
	Epoch 4200:	Loss 1.2508	TrainAcc 0.5470	ValidAcc 0.5052	TestAcc 0.5037	BestValid 0.5252
	Epoch 4250:	Loss 1.2503	TrainAcc 0.5785	ValidAcc 0.5227	TestAcc 0.5288	BestValid 0.5252
	Epoch 4300:	Loss 1.2502	TrainAcc 0.5841	ValidAcc 0.5213	TestAcc 0.5294	BestValid 0.5252
	Epoch 4350:	Loss 1.2536	TrainAcc 0.5019	ValidAcc 0.4681	TestAcc 0.4732	BestValid 0.5252
	Epoch 4400:	Loss 1.2441	TrainAcc 0.5825	ValidAcc 0.5247	TestAcc 0.5310	BestValid 0.5252
	Epoch 4450:	Loss 1.2583	TrainAcc 0.5183	ValidAcc 0.4893	TestAcc 0.4907	BestValid 0.5252
	Epoch 4500:	Loss 1.2449	TrainAcc 0.5638	ValidAcc 0.5140	TestAcc 0.5206	BestValid 0.5252
	Epoch 4550:	Loss 1.2405	TrainAcc 0.5806	ValidAcc 0.5202	TestAcc 0.5241	BestValid 0.5252
	Epoch 4600:	Loss 1.2411	TrainAcc 0.5505	ValidAcc 0.5069	TestAcc 0.5122	BestValid 0.5252
	Epoch 4650:	Loss 1.2441	TrainAcc 0.5190	ValidAcc 0.4826	TestAcc 0.4866	BestValid 0.5252
	Epoch 4700:	Loss 1.2387	TrainAcc 0.5762	ValidAcc 0.5200	TestAcc 0.5266	BestValid 0.5252
	Epoch 4750:	Loss 1.2358	TrainAcc 0.5737	ValidAcc 0.5192	TestAcc 0.5256	BestValid 0.5252
	Epoch 4800:	Loss 1.2373	TrainAcc 0.5509	ValidAcc 0.5044	TestAcc 0.5108	BestValid 0.5252
	Epoch 4850:	Loss 1.2388	TrainAcc 0.5553	ValidAcc 0.5034	TestAcc 0.5104	BestValid 0.5252
	Epoch 4900:	Loss 1.2362	TrainAcc 0.5851	ValidAcc 0.5148	TestAcc 0.5245	BestValid 0.5252
	Epoch 4950:	Loss 1.2315	TrainAcc 0.5572	ValidAcc 0.5013	TestAcc 0.5095	BestValid 0.5252
	Epoch 5000:	Loss 1.2307	TrainAcc 0.5865	ValidAcc 0.5244	TestAcc 0.5295	BestValid 0.5252
****** Epoch Time (Excluding Evaluation Cost): 0.154 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 22.759 ms (Max: 23.369, Min: 21.491, Sum: 182.071)
Cluster-Wide Average, Compute: 81.525 ms (Max: 90.653, Min: 76.573, Sum: 652.204)
Cluster-Wide Average, Communication-Layer: 6.539 ms (Max: 7.526, Min: 5.764, Sum: 52.312)
Cluster-Wide Average, Bubble-Imbalance: 7.491 ms (Max: 12.969, Min: 0.519, Sum: 59.926)
Cluster-Wide Average, Communication-Graph: 28.620 ms (Max: 32.948, Min: 27.239, Sum: 228.962)
Cluster-Wide Average, Optimization: 2.609 ms (Max: 5.609, Min: 1.472, Sum: 20.873)
Cluster-Wide Average, Others: 4.288 ms (Max: 6.478, Min: 1.442, Sum: 34.302)
****** Breakdown Sum: 153.831 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.569 GB (Max: 4.434, Min: 3.383, Sum: 28.550)
Cluster-Wide Average, Graph-Level Communication Throughput: 93.101 Gbps (Max: 98.351, Min: 76.002, Sum: 744.806)
Cluster-Wide Average, Layer-Level Communication Throughput: 31.972 Gbps (Max: 40.290, Min: 23.590, Sum: 255.779)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 1.793 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 1.995 GB
****** Accuracy Results ******
Highest valid_acc: 0.5252
Target test_acc: 0.5275
Epoch to reach the target acc: 2799
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 97472 RUNNING AT gnerv8
=   EXIT CODE: 9
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Killed (signal 9)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
