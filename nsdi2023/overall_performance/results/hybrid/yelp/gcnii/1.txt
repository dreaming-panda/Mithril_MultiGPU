Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 7 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2

Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.238 seconds.
Building the CSC structure...
        It takes 0.263 seconds.
Building the CSC structure...
        It takes 0.285 seconds.
Building the CSC structure...
        It takes 0.297 seconds.
Building the CSC structure...
        It takes 0.311 seconds.
Building the CSC structure...
        It takes 0.309 seconds.
Building the CSC structure...
        It takes 0.318 seconds.
Building the CSC structure...
        It takes 0.320 seconds.
Building the CSC structure...
        It takes 0.229 seconds.
        It takes 0.267 seconds.
        It takes 0.275 seconds.
        It takes 0.281 seconds.
Building the Feature Vector...
        It takes 0.295 seconds.
        It takes 0.320 seconds.
        It takes 0.319 seconds.
        It takes 0.323 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.458 seconds.
Building the Label Vector...
        It takes 0.477 seconds.
Building the Label Vector...
        It takes 0.531 seconds.
Building the Label Vector...
        It takes 0.486 seconds.
Building the Label Vector...
        It takes 0.178 seconds.
        It takes 0.482 seconds.
Building the Label Vector...
        It takes 0.484 seconds.
Building the Label Vector...
        It takes 0.547 seconds.
Building the Label Vector...
        It takes 0.495 seconds.
Building the Label Vector...
        It takes 0.188 seconds.
        It takes 0.200 seconds.
        It takes 0.189 seconds.
        It takes 0.187 seconds.
        It takes 0.183 seconds.
        It takes 0.200 seconds.
        It takes 0.196 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/yelp/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 100
Number of feature dimensions: 300
Number of vertices: 716847
Number of GPUs: 8
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 22402
train nodes 537635, valid nodes 107527, test nodes 71685
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 24122) 1-[24122, 44991) 2-[44991, 66905) 3-[66905, 90565) 4-[90565, 109350) 5-[109350, 132203) 6-[132203, 154486) 7-[154486, 177346) 8-[177346, 198991) ... 31-[695934, 716847)
716847, 13954819, 13954819
Number of vertices per chunk: 22402
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.564 Gbps (per GPU), 476.513 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.268 Gbps (per GPU), 474.142 Gbps (aggregated)
The layer-level communication performance: 59.266 Gbps (per GPU), 474.129 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.010 Gbps (per GPU), 472.082 Gbps (aggregated)
The layer-level communication performance: 58.984 Gbps (per GPU), 471.876 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.769 Gbps (per GPU), 470.155 Gbps (aggregated)
The layer-level communication performance: 58.718 Gbps (per GPU), 469.747 Gbps (aggregated)
The layer-level communication performance: 58.694 Gbps (per GPU), 469.550 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.422 Gbps (per GPU), 1267.377 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.422 Gbps (per GPU), 1267.377 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.371 Gbps (per GPU), 1266.970 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.404 Gbps (per GPU), 1267.233 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.371 Gbps (per GPU), 1266.970 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.407 Gbps (per GPU), 1267.257 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.404 Gbps (per GPU), 1267.233 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.404 Gbps (per GPU), 1267.233 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.376 Gbps (per GPU), 803.007 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.374 Gbps (per GPU), 802.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.375 Gbps (per GPU), 803.000 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.371 Gbps (per GPU), 802.967 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.372 Gbps (per GPU), 802.975 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.361 Gbps (per GPU), 802.891 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.373 Gbps (per GPU), 802.981 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.371 Gbps (per GPU), 802.968 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.333 Gbps (per GPU), 282.668 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.333 Gbps (per GPU), 282.666 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.333 Gbps (per GPU), 282.664 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.333 Gbps (per GPU), 282.665 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.333 Gbps (per GPU), 282.664 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.333 Gbps (per GPU), 282.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.332 Gbps (per GPU), 282.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.332 Gbps (per GPU), 282.660 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.39ms  1.73ms  2.05ms  1.47 24.12K  0.35M
 chk_1  1.29ms  1.74ms  2.03ms  1.57 20.87K  0.49M
 chk_2  1.33ms  1.65ms  1.93ms  1.45 21.91K  0.46M
 chk_3  1.37ms  1.75ms  2.05ms  1.49 23.66K  0.35M
 chk_4  1.18ms  1.62ms  1.88ms  1.59 18.79K  0.62M
 chk_5  1.35ms  1.70ms  2.00ms  1.48 22.85K  0.37M
 chk_6  1.34ms  1.73ms  2.02ms  1.51 22.28K  0.40M
 chk_7  1.35ms  1.70ms  1.99ms  1.48 22.86K  0.39M
 chk_8  1.32ms  1.70ms  1.99ms  1.51 21.64K  0.45M
 chk_9  1.35ms  1.68ms  1.97ms  1.46 22.92K  0.37M
chk_10  1.24ms  1.72ms  2.00ms  1.62 20.30K  0.56M
chk_11  1.37ms  1.73ms  2.03ms  1.48 23.32K  0.33M
chk_12  1.30ms  1.71ms  2.00ms  1.54 21.10K  0.49M
chk_13  1.29ms  1.65ms  1.94ms  1.51 20.79K  0.49M
chk_14  1.37ms  1.68ms  1.98ms  1.44 23.53K  0.36M
chk_15  1.36ms  1.64ms  1.94ms  1.42 23.21K  0.39M
chk_16  1.40ms  1.68ms  1.99ms  1.42 24.39K  0.32M
chk_17  1.39ms  1.72ms  2.02ms  1.46 23.94K  0.34M
chk_18  1.32ms  1.69ms  1.98ms  1.50 21.61K  0.45M
chk_19  1.38ms  1.69ms  1.99ms  1.44 23.89K  0.34M
chk_20  1.32ms  1.72ms  2.02ms  1.53 21.64K  0.47M
chk_21  1.37ms  1.72ms  2.02ms  1.47 23.43K  0.34M
chk_22  1.35ms  1.69ms  1.99ms  1.48 22.84K  0.37M
chk_23  1.37ms  1.72ms  2.02ms  1.48 23.35K  0.37M
chk_24  1.34ms  1.72ms  2.02ms  1.50 22.72K  0.40M
chk_25  1.33ms  1.75ms  2.04ms  1.53 21.95K  0.46M
chk_26  1.34ms  1.74ms  2.02ms  1.52 22.06K  0.41M
chk_27  1.35ms  1.70ms  1.99ms  1.47 23.02K  0.35M
chk_28  1.35ms  1.71ms  2.00ms  1.48 22.97K  0.36M
chk_29  1.34ms  1.65ms  1.94ms  1.45 22.14K  0.45M
chk_30  1.33ms  1.72ms  2.01ms  1.51 21.84K  0.44M
chk_31  1.29ms  1.71ms  2.00ms  1.55 20.91K  0.49M
   Avg  1.34  1.70  2.00
   Max  1.40  1.75  2.05
   Min  1.18  1.62  1.88
 Ratio  1.18  1.08  1.09
   Var  0.00  0.00  0.00
Profiling takes 1.993 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 232)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 232)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 354156, Num Local Vertices: 362691
Node 6, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 6 initializing the weights for op[174, 232)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 7 initializing the weights for op[174, 232)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 1731824
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 15.3805	TrainAcc 0.1697	ValidAcc 0.1686	TestAcc 0.1689	BestValid 0.1686
	Epoch 50:	Loss 0.7124	TrainAcc 0.2201	ValidAcc 0.2194	TestAcc 0.2199	BestValid 0.2194
	Epoch 100:	Loss 0.7176	TrainAcc 0.2191	ValidAcc 0.2184	TestAcc 0.2189	BestValid 0.2194
	Epoch 150:	Loss 0.7050	TrainAcc 0.2221	ValidAcc 0.2214	TestAcc 0.2221	BestValid 0.2214
	Epoch 200:	Loss 0.7012	TrainAcc 0.2005	ValidAcc 0.2001	TestAcc 0.2004	BestValid 0.2214
	Epoch 250:	Loss 0.7573	TrainAcc 0.1744	ValidAcc 0.1741	TestAcc 0.1745	BestValid 0.2214
	Epoch 300:	Loss 0.6986	TrainAcc 0.2021	ValidAcc 0.2018	TestAcc 0.2023	BestValid 0.2214
	Epoch 350:	Loss 0.6977	TrainAcc 0.1837	ValidAcc 0.1832	TestAcc 0.1839	BestValid 0.2214
	Epoch 400:	Loss 0.6982	TrainAcc 0.1851	ValidAcc 0.1844	TestAcc 0.1854	BestValid 0.2214
	Epoch 450:	Loss 0.6995	TrainAcc 0.1861	ValidAcc 0.1858	TestAcc 0.1865	BestValid 0.2214
	Epoch 500:	Loss 0.7151	TrainAcc 0.1878	ValidAcc 0.1876	TestAcc 0.1882	BestValid 0.2214
	Epoch 550:	Loss 0.7423	TrainAcc 0.1737	ValidAcc 0.1734	TestAcc 0.1741	BestValid 0.2214
	Epoch 600:	Loss 0.6953	TrainAcc 0.1617	ValidAcc 0.1616	TestAcc 0.1622	BestValid 0.2214
	Epoch 650:	Loss 0.7037	TrainAcc 0.1812	ValidAcc 0.1809	TestAcc 0.1818	BestValid 0.2214
	Epoch 700:	Loss 0.6949	TrainAcc 0.1818	ValidAcc 0.1816	TestAcc 0.1824	BestValid 0.2214
	Epoch 750:	Loss 0.6938	TrainAcc 0.2012	ValidAcc 0.2006	TestAcc 0.2013	BestValid 0.2214
	Epoch 800:	Loss 0.6964	TrainAcc 0.1956	ValidAcc 0.1950	TestAcc 0.1957	BestValid 0.2214
	Epoch 850:	Loss 0.6967	TrainAcc 0.2094	ValidAcc 0.2088	TestAcc 0.2100	BestValid 0.2214
	Epoch 900:	Loss 0.6941	TrainAcc 0.2137	ValidAcc 0.2131	TestAcc 0.2140	BestValid 0.2214
	Epoch 950:	Loss 0.7008	TrainAcc 0.2099	ValidAcc 0.2094	TestAcc 0.2103	BestValid 0.2214
	Epoch 1000:	Loss 0.6938	TrainAcc 0.2105	ValidAcc 0.2101	TestAcc 0.2109	BestValid 0.2214
	Epoch 1050:	Loss 0.6972	TrainAcc 0.2127	ValidAcc 0.2124	TestAcc 0.2132	BestValid 0.2214
	Epoch 1100:	Loss 0.6955	TrainAcc 0.2150	ValidAcc 0.2148	TestAcc 0.2155	BestValid 0.2214
	Epoch 1150:	Loss 0.6981	TrainAcc 0.2132	ValidAcc 0.2130	TestAcc 0.2136	BestValid 0.2214
	Epoch 1200:	Loss 0.6934	TrainAcc 0.2044	ValidAcc 0.2045	TestAcc 0.2051	BestValid 0.2214
	Epoch 1250:	Loss 0.6940	TrainAcc 0.2006	ValidAcc 0.2008	TestAcc 0.2013	BestValid 0.2214
	Epoch 1300:	Loss 0.6945	TrainAcc 0.1983	ValidAcc 0.1982	TestAcc 0.1988	BestValid 0.2214
	Epoch 1350:	Loss 0.6975	TrainAcc 0.1958	ValidAcc 0.1958	TestAcc 0.1962	BestValid 0.2214
	Epoch 1400:	Loss 0.6940	TrainAcc 0.1932	ValidAcc 0.1931	TestAcc 0.1937	BestValid 0.2214
	Epoch 1450:	Loss 0.6976	TrainAcc 0.1901	ValidAcc 0.1900	TestAcc 0.1905	BestValid 0.2214
	Epoch 1500:	Loss 0.6979	TrainAcc 0.1935	ValidAcc 0.1937	TestAcc 0.1941	BestValid 0.2214
	Epoch 1550:	Loss 0.6935	TrainAcc 0.1870	ValidAcc 0.1871	TestAcc 0.1877	BestValid 0.2214
	Epoch 1600:	Loss 0.6953	TrainAcc 0.1872	ValidAcc 0.1873	TestAcc 0.1879	BestValid 0.2214
	Epoch 1650:	Loss 0.6985	TrainAcc 0.1926	ValidAcc 0.1927	TestAcc 0.1934	BestValid 0.2214
	Epoch 1700:	Loss 0.6935	TrainAcc 0.1786	ValidAcc 0.1787	TestAcc 0.1793	BestValid 0.2214
	Epoch 1750:	Loss 0.6940	TrainAcc 0.1767	ValidAcc 0.1766	TestAcc 0.1773	BestValid 0.2214
	Epoch 1800:	Loss 0.6931	TrainAcc 0.1744	ValidAcc 0.1743	TestAcc 0.1750	BestValid 0.2214
	Epoch 1850:	Loss 0.6950	TrainAcc 0.1722	ValidAcc 0.1721	TestAcc 0.1728	BestValid 0.2214
	Epoch 1900:	Loss 0.6932	TrainAcc 0.1708	ValidAcc 0.1707	TestAcc 0.1713	BestValid 0.2214
	Epoch 1950:	Loss 0.6933	TrainAcc 0.1701	ValidAcc 0.1700	TestAcc 0.1707	BestValid 0.2214
	Epoch 2000:	Loss 0.6934	TrainAcc 0.1701	ValidAcc 0.1700	TestAcc 0.1707	BestValid 0.2214
	Epoch 2050:	Loss 0.6931	TrainAcc 0.1699	ValidAcc 0.1697	TestAcc 0.1704	BestValid 0.2214
	Epoch 2100:	Loss 0.6940	TrainAcc 0.1696	ValidAcc 0.1695	TestAcc 0.1702	BestValid 0.2214
	Epoch 2150:	Loss 0.6937	TrainAcc 0.1693	ValidAcc 0.1691	TestAcc 0.1698	BestValid 0.2214
	Epoch 2200:	Loss 0.6934	TrainAcc 0.1692	ValidAcc 0.1690	TestAcc 0.1697	BestValid 0.2214
	Epoch 2250:	Loss 0.6938	TrainAcc 0.1693	ValidAcc 0.1691	TestAcc 0.1699	BestValid 0.2214
	Epoch 2300:	Loss 0.6931	TrainAcc 0.1691	ValidAcc 0.1690	TestAcc 0.1697	BestValid 0.2214
	Epoch 2350:	Loss 0.6944	TrainAcc 0.1688	ValidAcc 0.1687	TestAcc 0.1695	BestValid 0.2214
	Epoch 2400:	Loss 0.6937	TrainAcc 0.1687	ValidAcc 0.1686	TestAcc 0.1694	BestValid 0.2214
	Epoch 2450:	Loss 0.6934	TrainAcc 0.1684	ValidAcc 0.1683	TestAcc 0.1692	BestValid 0.2214
	Epoch 2500:	Loss 0.6931	TrainAcc 0.1682	ValidAcc 0.1680	TestAcc 0.1689	BestValid 0.2214
	Epoch 2550:	Loss 0.6931	TrainAcc 0.1678	ValidAcc 0.1678	TestAcc 0.1686	BestValid 0.2214
	Epoch 2600:	Loss 0.6931	TrainAcc 0.1674	ValidAcc 0.1674	TestAcc 0.1682	BestValid 0.2214
	Epoch 2650:	Loss 0.6934	TrainAcc 0.1665	ValidAcc 0.1665	TestAcc 0.1673	BestValid 0.2214
	Epoch 2700:	Loss 0.6948	TrainAcc 0.1668	ValidAcc 0.1667	TestAcc 0.1675	BestValid 0.2214
	Epoch 2750:	Loss 0.6931	TrainAcc 0.1643	ValidAcc 0.1644	TestAcc 0.1653	BestValid 0.2214
	Epoch 2800:	Loss 0.6933	TrainAcc 0.1607	ValidAcc 0.1608	TestAcc 0.1616	BestValid 0.2214
	Epoch 2850:	Loss 0.6932	TrainAcc 0.1599	ValidAcc 0.1600	TestAcc 0.1608	BestValid 0.2214
	Epoch 2900:	Loss 0.6931	TrainAcc 0.1595	ValidAcc 0.1597	TestAcc 0.1604	BestValid 0.2214
	Epoch 2950:	Loss 0.6932	TrainAcc 0.1588	ValidAcc 0.1591	TestAcc 0.1597	BestValid 0.2214
	Epoch 3000:	Loss 0.6931	TrainAcc 0.1570	ValidAcc 0.1573	TestAcc 0.1578	BestValid 0.2214
	Epoch 3050:	Loss 0.6943	TrainAcc 0.1551	ValidAcc 0.1554	TestAcc 0.1559	BestValid 0.2214
	Epoch 3100:	Loss 0.6932	TrainAcc 0.1541	ValidAcc 0.1544	TestAcc 0.1549	BestValid 0.2214
	Epoch 3150:	Loss 0.6932	TrainAcc 0.1526	ValidAcc 0.1529	TestAcc 0.1533	BestValid 0.2214
	Epoch 3200:	Loss 0.6939	TrainAcc 0.1530	ValidAcc 0.1533	TestAcc 0.1538	BestValid 0.2214
	Epoch 3250:	Loss 0.6934	TrainAcc 0.1535	ValidAcc 0.1539	TestAcc 0.1543	BestValid 0.2214
	Epoch 3300:	Loss 0.6932	TrainAcc 0.1516	ValidAcc 0.1520	TestAcc 0.1523	BestValid 0.2214
	Epoch 3350:	Loss 0.6932	TrainAcc 0.1498	ValidAcc 0.1500	TestAcc 0.1505	BestValid 0.2214
	Epoch 3400:	Loss 0.6933	TrainAcc 0.1489	ValidAcc 0.1491	TestAcc 0.1497	BestValid 0.2214
	Epoch 3450:	Loss 0.6953	TrainAcc 0.1479	ValidAcc 0.1481	TestAcc 0.1487	BestValid 0.2214
	Epoch 3500:	Loss 0.6931	TrainAcc 0.1473	ValidAcc 0.1476	TestAcc 0.1481	BestValid 0.2214
	Epoch 3550:	Loss 0.6933	TrainAcc 0.1471	ValidAcc 0.1473	TestAcc 0.1478	BestValid 0.2214
	Epoch 3600:	Loss 0.6932	TrainAcc 0.1450	ValidAcc 0.1452	TestAcc 0.1457	BestValid 0.2214
	Epoch 3650:	Loss 0.6933	TrainAcc 0.1444	ValidAcc 0.1445	TestAcc 0.1450	BestValid 0.2214
	Epoch 3700:	Loss 0.6931	TrainAcc 0.1443	ValidAcc 0.1444	TestAcc 0.1450	BestValid 0.2214
	Epoch 3750:	Loss 0.6932	TrainAcc 0.1441	ValidAcc 0.1443	TestAcc 0.1448	BestValid 0.2214
	Epoch 3800:	Loss 0.6933	TrainAcc 0.1438	ValidAcc 0.1440	TestAcc 0.1445	BestValid 0.2214
	Epoch 3850:	Loss 0.6930	TrainAcc 0.1435	ValidAcc 0.1437	TestAcc 0.1442	BestValid 0.2214
	Epoch 3900:	Loss 0.6930	TrainAcc 0.1435	ValidAcc 0.1437	TestAcc 0.1442	BestValid 0.2214
	Epoch 3950:	Loss 0.6931	TrainAcc 0.1435	ValidAcc 0.1436	TestAcc 0.1441	BestValid 0.2214
	Epoch 4000:	Loss 0.6931	TrainAcc 0.1431	ValidAcc 0.1433	TestAcc 0.1438	BestValid 0.2214
	Epoch 4050:	Loss 0.6932	TrainAcc 0.1430	ValidAcc 0.1432	TestAcc 0.1437	BestValid 0.2214
	Epoch 4100:	Loss 0.6931	TrainAcc 0.1429	ValidAcc 0.1431	TestAcc 0.1437	BestValid 0.2214
	Epoch 4150:	Loss 0.6931	TrainAcc 0.1428	ValidAcc 0.1430	TestAcc 0.1435	BestValid 0.2214
	Epoch 4200:	Loss 0.6939	TrainAcc 0.1427	ValidAcc 0.1429	TestAcc 0.1434	BestValid 0.2214
	Epoch 4250:	Loss 0.6930	TrainAcc 0.1426	ValidAcc 0.1428	TestAcc 0.1434	BestValid 0.2214
	Epoch 4300:	Loss 0.6931	TrainAcc 0.1427	ValidAcc 0.1428	TestAcc 0.1434	BestValid 0.2214
	Epoch 4350:	Loss 0.6931	TrainAcc 0.1426	ValidAcc 0.1428	TestAcc 0.1434	BestValid 0.2214
	Epoch 4400:	Loss 0.6931	TrainAcc 0.1425	ValidAcc 0.1427	TestAcc 0.1433	BestValid 0.2214
	Epoch 4450:	Loss 0.6936	TrainAcc 0.1424	ValidAcc 0.1426	TestAcc 0.1432	BestValid 0.2214
	Epoch 4500:	Loss 0.6931	TrainAcc 0.1424	ValidAcc 0.1426	TestAcc 0.1431	BestValid 0.2214
	Epoch 4550:	Loss 0.6930	TrainAcc 0.1423	ValidAcc 0.1425	TestAcc 0.1430	BestValid 0.2214
	Epoch 4600:	Loss 0.6933	TrainAcc 0.1425	ValidAcc 0.1426	TestAcc 0.1432	BestValid 0.2214
	Epoch 4650:	Loss 0.6930	TrainAcc 0.1424	ValidAcc 0.1426	TestAcc 0.1432	BestValid 0.2214
	Epoch 4700:	Loss 0.6935	TrainAcc 0.1423	ValidAcc 0.1425	TestAcc 0.1431	BestValid 0.2214
	Epoch 4750:	Loss 0.6937	TrainAcc 0.1425	ValidAcc 0.1426	TestAcc 0.1431	BestValid 0.2214
	Epoch 4800:	Loss 0.6932	TrainAcc 0.1424	ValidAcc 0.1425	TestAcc 0.1431	BestValid 0.2214
	Epoch 4850:	Loss 0.6932	TrainAcc 0.1424	ValidAcc 0.1425	TestAcc 0.1431	BestValid 0.2214
	Epoch 4900:	Loss 0.6931	TrainAcc 0.1424	ValidAcc 0.1425	TestAcc 0.1430	BestValid 0.2214
	Epoch 4950:	Loss 0.6930	TrainAcc 0.1423	ValidAcc 0.1425	TestAcc 0.1430	BestValid 0.2214
	Epoch 5000:	Loss 0.6931	TrainAcc 0.1423	ValidAcc 0.1425	TestAcc 0.1430	BestValid 0.2214
****** Epoch Time (Excluding Evaluation Cost): 0.501 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 72.236 ms (Max: 76.421, Min: 67.952, Sum: 577.888)
Cluster-Wide Average, Compute: 219.856 ms (Max: 233.531, Min: 210.674, Sum: 1758.846)
Cluster-Wide Average, Communication-Layer: 88.065 ms (Max: 101.008, Min: 74.823, Sum: 704.522)
Cluster-Wide Average, Bubble-Imbalance: 12.873 ms (Max: 19.321, Min: 8.874, Sum: 102.981)
Cluster-Wide Average, Communication-Graph: 96.385 ms (Max: 99.204, Min: 93.925, Sum: 771.082)
Cluster-Wide Average, Optimization: 2.192 ms (Max: 2.875, Min: 1.668, Sum: 17.536)
Cluster-Wide Average, Others: 10.971 ms (Max: 12.800, Min: 10.207, Sum: 87.766)
****** Breakdown Sum: 502.578 ms ******
Cluster-Wide Average, GPU Memory Consumption: 12.874 GB (Max: 15.337, Min: 12.008, Sum: 102.993)
Cluster-Wide Average, Graph-Level Communication Throughput: 140.821 Gbps (Max: 148.050, Min: 133.500, Sum: 1126.566)
Cluster-Wide Average, Layer-Level Communication Throughput: 38.005 Gbps (Max: 46.275, Min: 29.729, Sum: 304.041)
Layer-level communication (cluster-wide, per-epoch): 3.205 GB
Graph-level communication (cluster-wide, per-epoch): 10.322 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 13.530 GB
****** Accuracy Results ******
Highest valid_acc: 0.2214
Target test_acc: 0.2221
Epoch to reach the target acc: 149
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
