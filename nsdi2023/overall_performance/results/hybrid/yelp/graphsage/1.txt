Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INITDONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3

DONE MPI INIT
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.232 seconds.
Building the CSC structure...
        It takes 0.255 seconds.
Building the CSC structure...
        It takes 0.274 seconds.
Building the CSC structure...
        It takes 0.276 seconds.
Building the CSC structure...
        It takes 0.292 seconds.
Building the CSC structure...
        It takes 0.312 seconds.
Building the CSC structure...
        It takes 0.319 seconds.
Building the CSC structure...
        It takes 0.323 seconds.
Building the CSC structure...
        It takes 0.225 seconds.
        It takes 0.244 seconds.
        It takes 0.279 seconds.
        It takes 0.281 seconds.
        It takes 0.281 seconds.
Building the Feature Vector...
        It takes 0.302 seconds.
        It takes 0.316 seconds.
        It takes 0.314 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.465 seconds.
Building the Label Vector...
        It takes 0.478 seconds.
Building the Label Vector...
        It takes 0.478 seconds.
Building the Label Vector...
        It takes 0.489 seconds.
Building the Label Vector...
        It takes 0.534 seconds.
Building the Label Vector...
        It takes 0.185 seconds.
        It takes 0.483 seconds.
Building the Label Vector...
        It takes 0.508 seconds.
Building the Label Vector...
        It takes 0.537 seconds.
Building the Label Vector...
        It takes 0.191 seconds.
        It takes 0.183 seconds.
        It takes 0.183 seconds.
        It takes 0.199 seconds.
        It takes 0.177 seconds.
        It takes 0.190 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/yelp/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 100
Number of feature dimensions: 300
Number of vertices: 716847
Number of GPUs: 8
        It takes 0.198 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
716847, 13954819, 13954819
Number of vertices per chunk: 22402
Number of vertices per chunk: 22402
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
train nodes 537635, valid nodes 107527, test nodes 71685
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 24122) 1-[24122, 44991) 2-[44991, 66905) 3-[66905, 90565) 4-[90565, 109350) 5-[109350, 132203) 6-[132203, 154486) 7-[154486, 177346) 8-[177346, 198991) ... 31-[695934, 716847)
716847, 13954819, 13954819
Number of vertices per chunk: 22402
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.336 Gbps (per GPU), 474.687 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.050 Gbps (per GPU), 472.399 Gbps (aggregated)
The layer-level communication performance: 59.039 Gbps (per GPU), 472.316 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.805 Gbps (per GPU), 470.437 Gbps (aggregated)
The layer-level communication performance: 58.764 Gbps (per GPU), 470.116 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.566 Gbps (per GPU), 468.528 Gbps (aggregated)
The layer-level communication performance: 58.516 Gbps (per GPU), 468.130 Gbps (aggregated)
The layer-level communication performance: 58.484 Gbps (per GPU), 467.874 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.399 Gbps (per GPU), 1251.191 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.390 Gbps (per GPU), 1251.121 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.361 Gbps (per GPU), 1250.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.388 Gbps (per GPU), 1251.100 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.405 Gbps (per GPU), 1251.237 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.390 Gbps (per GPU), 1251.121 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.274 Gbps (per GPU), 1250.191 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.393 Gbps (per GPU), 1251.144 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.637 Gbps (per GPU), 797.099 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.638 Gbps (per GPU), 797.106 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.637 Gbps (per GPU), 797.099 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.639 Gbps (per GPU), 797.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.639 Gbps (per GPU), 797.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.634 Gbps (per GPU), 797.074 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.637 Gbps (per GPU), 797.099 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.635 Gbps (per GPU), 797.081 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.728 Gbps (per GPU), 269.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.727 Gbps (per GPU), 269.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.728 Gbps (per GPU), 269.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.727 Gbps (per GPU), 269.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.727 Gbps (per GPU), 269.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.727 Gbps (per GPU), 269.815 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.727 Gbps (per GPU), 269.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.727 Gbps (per GPU), 269.817 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.87ms  1.76ms  1.47ms  1.96 24.12K  0.35M
 chk_1  2.84ms  1.77ms  1.52ms  1.87 20.87K  0.49M
 chk_2  2.76ms  1.67ms  1.40ms  1.98 21.91K  0.46M
 chk_3  2.87ms  1.76ms  1.48ms  1.95 23.66K  0.35M
 chk_4  2.63ms  1.65ms  1.41ms  1.87 18.79K  0.62M
 chk_5  2.81ms  1.72ms  1.45ms  1.95 22.85K  0.37M
 chk_6  2.86ms  1.75ms  1.47ms  1.94 22.28K  0.40M
 chk_7  2.81ms  1.71ms  1.43ms  1.96 22.86K  0.39M
 chk_8  2.82ms  1.72ms  1.45ms  1.94 21.64K  0.45M
 chk_9  2.80ms  1.69ms  1.41ms  1.98 22.92K  0.37M
chk_10  2.76ms  1.75ms  1.49ms  1.85 20.30K  0.56M
chk_11  2.86ms  1.74ms  1.46ms  1.96 23.32K  0.33M
chk_12  2.82ms  1.74ms  1.48ms  1.90 21.10K  0.49M
chk_13  2.77ms  1.69ms  1.43ms  1.94 20.79K  0.49M
chk_14  2.82ms  1.69ms  1.43ms  1.97 23.53K  0.36M
chk_15  2.76ms  1.65ms  1.37ms  2.02 23.21K  0.39M
chk_16  2.83ms  1.69ms  1.40ms  2.02 24.39K  0.32M
chk_17  2.86ms  1.73ms  1.44ms  1.98 23.94K  0.34M
chk_18  2.80ms  1.71ms  1.44ms  1.95 21.61K  0.45M
chk_19  2.84ms  1.70ms  1.42ms  2.00 23.89K  0.34M
chk_20  2.84ms  1.75ms  1.48ms  1.92 21.64K  0.47M
chk_21  2.85ms  1.73ms  1.88ms  1.65 23.43K  0.34M
chk_22  2.81ms  1.72ms  1.44ms  1.95 22.84K  0.37M
chk_23  2.86ms  1.74ms  1.45ms  1.96 23.35K  0.37M
chk_24  2.84ms  1.74ms  1.46ms  1.94 22.72K  0.40M
chk_25  2.88ms  1.76ms  1.49ms  1.92 21.95K  0.46M
chk_26  2.86ms  1.75ms  1.48ms  1.93 22.06K  0.41M
chk_27  2.82ms  1.71ms  1.43ms  1.97 23.02K  0.35M
chk_28  2.83ms  1.72ms  1.44ms  1.97 22.97K  0.36M
chk_29  2.78ms  1.67ms  1.40ms  1.99 22.14K  0.45M
chk_30  2.84ms  1.74ms  1.47ms  1.94 21.84K  0.44M
chk_31  2.81ms  1.74ms  1.48ms  1.90 20.91K  0.49M
   Avg  2.82  1.72  1.46
   Max  2.88  1.77  1.88
   Min  2.63  1.65  1.37
 Ratio  1.09  1.08  1.38
   Var  0.00  0.00  0.01
Profiling takes 2.331 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 256)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 256)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 66)...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 2 initializing the weights for op[66, 130)...
+++++++++ Node 6 initializing the weights for op[194, 256)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
+++++++++ Node 0 initializing the weights for op[0, 66)...
+++++++++ Node 7 initializing the weights for op[194, 256)...
+++++++++ Node 5 initializing the weights for op[130, 194)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 1731824
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 46.0753	TrainAcc 0.1744	ValidAcc 0.1734	TestAcc 0.1733	BestValid 0.1734
	Epoch 50:	Loss 0.6931	TrainAcc 0.1691	ValidAcc 0.1685	TestAcc 0.1684	BestValid 0.1734
	Epoch 100:	Loss 0.6931	TrainAcc 0.1702	ValidAcc 0.1698	TestAcc 0.1695	BestValid 0.1734
	Epoch 150:	Loss 0.6931	TrainAcc 0.1692	ValidAcc 0.1687	TestAcc 0.1686	BestValid 0.1734
	Epoch 200:	Loss 0.6931	TrainAcc 0.1689	ValidAcc 0.1684	TestAcc 0.1683	BestValid 0.1734
	Epoch 250:	Loss 0.6931	TrainAcc 0.1680	ValidAcc 0.1676	TestAcc 0.1674	BestValid 0.1734
	Epoch 300:	Loss 0.6931	TrainAcc 0.1675	ValidAcc 0.1671	TestAcc 0.1669	BestValid 0.1734
	Epoch 350:	Loss 0.6931	TrainAcc 0.1669	ValidAcc 0.1664	TestAcc 0.1663	BestValid 0.1734
	Epoch 400:	Loss 0.6931	TrainAcc 0.1685	ValidAcc 0.1680	TestAcc 0.1680	BestValid 0.1734
	Epoch 450:	Loss 0.6931	TrainAcc 0.1692	ValidAcc 0.1686	TestAcc 0.1686	BestValid 0.1734
	Epoch 500:	Loss 0.6931	TrainAcc 0.1668	ValidAcc 0.1664	TestAcc 0.1663	BestValid 0.1734
	Epoch 550:	Loss 0.6931	TrainAcc 0.1746	ValidAcc 0.1743	TestAcc 0.1742	BestValid 0.1743
	Epoch 600:	Loss 0.6931	TrainAcc 0.1825	ValidAcc 0.1819	TestAcc 0.1818	BestValid 0.1819
	Epoch 650:	Loss 0.6931	TrainAcc 0.1818	ValidAcc 0.1812	TestAcc 0.1812	BestValid 0.1819
	Epoch 700:	Loss 0.6931	TrainAcc 0.1811	ValidAcc 0.1806	TestAcc 0.1806	BestValid 0.1819
	Epoch 750:	Loss 0.6931	TrainAcc 0.1802	ValidAcc 0.1797	TestAcc 0.1796	BestValid 0.1819
	Epoch 800:	Loss 0.6931	TrainAcc 0.1768	ValidAcc 0.1761	TestAcc 0.1762	BestValid 0.1819
	Epoch 850:	Loss 0.6931	TrainAcc 0.1748	ValidAcc 0.1742	TestAcc 0.1743	BestValid 0.1819
	Epoch 900:	Loss 0.6931	TrainAcc 0.1752	ValidAcc 0.1746	TestAcc 0.1747	BestValid 0.1819
	Epoch 950:	Loss 0.6931	TrainAcc 0.1795	ValidAcc 0.1789	TestAcc 0.1790	BestValid 0.1819
	Epoch 1000:	Loss 0.6931	TrainAcc 0.1805	ValidAcc 0.1800	TestAcc 0.1801	BestValid 0.1819
	Epoch 1050:	Loss 0.6931	TrainAcc 0.1788	ValidAcc 0.1783	TestAcc 0.1784	BestValid 0.1819
	Epoch 1100:	Loss 0.6931	TrainAcc 0.1782	ValidAcc 0.1777	TestAcc 0.1779	BestValid 0.1819
	Epoch 1150:	Loss 0.6931	TrainAcc 0.1788	ValidAcc 0.1782	TestAcc 0.1784	BestValid 0.1819
	Epoch 1200:	Loss 0.6931	TrainAcc 0.1742	ValidAcc 0.1736	TestAcc 0.1739	BestValid 0.1819
	Epoch 1250:	Loss 0.6931	TrainAcc 0.1757	ValidAcc 0.1752	TestAcc 0.1753	BestValid 0.1819
	Epoch 1300:	Loss 0.6931	TrainAcc 0.1737	ValidAcc 0.1732	TestAcc 0.1733	BestValid 0.1819
	Epoch 1350:	Loss 0.6931	TrainAcc 0.1752	ValidAcc 0.1746	TestAcc 0.1747	BestValid 0.1819
	Epoch 1400:	Loss 0.6931	TrainAcc 0.1860	ValidAcc 0.1857	TestAcc 0.1858	BestValid 0.1857
	Epoch 1450:	Loss 0.6931	TrainAcc 0.1791	ValidAcc 0.1785	TestAcc 0.1786	BestValid 0.1857
	Epoch 1500:	Loss 0.6931	TrainAcc 0.1788	ValidAcc 0.1781	TestAcc 0.1783	BestValid 0.1857
	Epoch 1550:	Loss 0.6931	TrainAcc 0.1741	ValidAcc 0.1736	TestAcc 0.1737	BestValid 0.1857
	Epoch 1600:	Loss 0.6931	TrainAcc 0.1913	ValidAcc 0.1907	TestAcc 0.1910	BestValid 0.1907
	Epoch 1650:	Loss 0.6931	TrainAcc 0.2009	ValidAcc 0.2002	TestAcc 0.2004	BestValid 0.2002
	Epoch 1700:	Loss 0.6931	TrainAcc 0.1992	ValidAcc 0.1986	TestAcc 0.1988	BestValid 0.2002
	Epoch 1750:	Loss 0.6931	TrainAcc 0.1966	ValidAcc 0.1958	TestAcc 0.1961	BestValid 0.2002
	Epoch 1800:	Loss 0.6930	TrainAcc 0.2036	ValidAcc 0.2030	TestAcc 0.2032	BestValid 0.2030
	Epoch 1850:	Loss 0.6930	TrainAcc 0.2169	ValidAcc 0.2164	TestAcc 0.2165	BestValid 0.2164
	Epoch 1900:	Loss 0.6930	TrainAcc 0.2278	ValidAcc 0.2272	TestAcc 0.2274	BestValid 0.2272
	Epoch 1950:	Loss 0.6930	TrainAcc 0.2041	ValidAcc 0.2036	TestAcc 0.2036	BestValid 0.2272
	Epoch 2000:	Loss 0.6930	TrainAcc 0.2054	ValidAcc 0.2049	TestAcc 0.2048	BestValid 0.2272
	Epoch 2050:	Loss 0.6929	TrainAcc 0.2068	ValidAcc 0.2063	TestAcc 0.2063	BestValid 0.2272
	Epoch 2100:	Loss 0.6928	TrainAcc 0.2038	ValidAcc 0.2032	TestAcc 0.2035	BestValid 0.2272
	Epoch 2150:	Loss 0.6923	TrainAcc 0.2160	ValidAcc 0.2154	TestAcc 0.2158	BestValid 0.2272
	Epoch 2200:	Loss 0.6831	TrainAcc 0.2326	ValidAcc 0.2322	TestAcc 0.2324	BestValid 0.2322
	Epoch 2250:	Loss 0.3258	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2300:	Loss 0.3149	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2350:	Loss 0.3150	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2400:	Loss 0.2933	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2450:	Loss 0.2919	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2500:	Loss 0.2894	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2550:	Loss 0.2876	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2600:	Loss 0.2863	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2650:	Loss 0.2852	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2700:	Loss 0.2844	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2750:	Loss 0.2834	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2800:	Loss 0.2830	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2850:	Loss 0.2822	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2900:	Loss 0.2821	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 2950:	Loss 0.2811	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3000:	Loss 0.2806	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3050:	Loss 0.2801	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3100:	Loss 0.2794	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3150:	Loss 0.2789	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3200:	Loss 0.2786	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3250:	Loss 0.2780	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3300:	Loss 0.2779	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3350:	Loss 0.2776	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3400:	Loss 0.2767	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3450:	Loss 0.2762	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3500:	Loss 0.2762	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3550:	Loss 0.2761	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3600:	Loss 0.2758	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3650:	Loss 0.2752	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3700:	Loss 0.2752	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3750:	Loss 0.2746	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3800:	Loss 0.2746	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3850:	Loss 0.2741	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3900:	Loss 0.2740	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 3950:	Loss 0.2736	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4000:	Loss 0.2734	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4050:	Loss 0.2736	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4100:	Loss 0.2730	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4150:	Loss 0.2727	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4200:	Loss 0.2727	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4250:	Loss 0.2724	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4300:	Loss 0.2723	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4350:	Loss 0.2722	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4400:	Loss 0.2720	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4450:	Loss 0.2718	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4500:	Loss 0.2718	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4550:	Loss 0.2716	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4600:	Loss 0.2717	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4650:	Loss 0.2712	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4700:	Loss 0.2714	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4750:	Loss 0.2713	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4800:	Loss 0.2712	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4850:	Loss 0.2711	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4900:	Loss 0.2709	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 4950:	Loss 0.2710	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
	Epoch 5000:	Loss 0.2708	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2322
****** Epoch Time (Excluding Evaluation Cost): 0.473 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 71.766 ms (Max: 77.302, Min: 66.317, Sum: 574.126)
Cluster-Wide Average, Compute: 228.385 ms (Max: 245.041, Min: 219.989, Sum: 1827.082)
Cluster-Wide Average, Communication-Layer: 45.597 ms (Max: 55.977, Min: 33.309, Sum: 364.778)
Cluster-Wide Average, Bubble-Imbalance: 17.070 ms (Max: 32.247, Min: 8.314, Sum: 136.563)
Cluster-Wide Average, Communication-Graph: 95.090 ms (Max: 96.828, Min: 93.470, Sum: 760.722)
Cluster-Wide Average, Optimization: 3.992 ms (Max: 8.535, Min: 2.116, Sum: 31.933)
Cluster-Wide Average, Others: 12.390 ms (Max: 14.786, Min: 10.404, Sum: 99.118)
****** Breakdown Sum: 474.290 ms ******
Cluster-Wide Average, GPU Memory Consumption: 11.318 GB (Max: 13.368, Min: 10.628, Sum: 90.540)
Cluster-Wide Average, Graph-Level Communication Throughput: 142.866 Gbps (Max: 148.590, Min: 137.265, Sum: 1142.928)
Cluster-Wide Average, Layer-Level Communication Throughput: 36.771 Gbps (Max: 42.380, Min: 29.677, Sum: 294.170)
Layer-level communication (cluster-wide, per-epoch): 1.602 GB
Graph-level communication (cluster-wide, per-epoch): 10.322 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 11.930 GB
****** Accuracy Results ******
Highest valid_acc: 0.2322
Target test_acc: 0.2324
Epoch to reach the target acc: 2199
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
