Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3
DONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.266 seconds.
Building the CSC structure...
        It takes 0.276 seconds.
Building the CSC structure...
        It takes 0.294 seconds.
Building the CSC structure...
        It takes 0.294 seconds.
Building the CSC structure...
        It takes 0.298 seconds.
Building the CSC structure...
        It takes 0.306 seconds.
Building the CSC structure...
        It takes 0.318 seconds.
Building the CSC structure...
        It takes 0.327 seconds.
Building the CSC structure...
        It takes 0.273 seconds.
        It takes 0.297 seconds.
        It takes 0.280 seconds.
        It takes 0.278 seconds.
        It takes 0.288 seconds.
        It takes 0.305 seconds.
        It takes 0.320 seconds.
        It takes 0.316 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.470 seconds.
Building the Label Vector...
        It takes 0.476 seconds.
Building the Label Vector...
        It takes 0.485 seconds.
Building the Label Vector...
        It takes 0.526 seconds.
        It takes 0.503 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.503 seconds.
Building the Label Vector...
        It takes 0.487 seconds.
Building the Label Vector...
        It takes 0.186 seconds.
        It takes 0.529 seconds.
Building the Label Vector...
        It takes 0.184 seconds.
        It takes 0.190 seconds.
        It takes 0.197 seconds.
        It takes 0.197 seconds.
        It takes 0.177 seconds.
        It takes 0.190 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/yelp/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 100
Number of feature dimensions: 300
Number of vertices: 716847
Number of GPUs: 8
        It takes 0.194 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
train nodes 537635, valid nodes 107527, test nodes 71685
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 24122) 1-[24122, 44991) 2-[44991, 66905) 3-[66905, 90565) 4-[90565, 109350) 5-[109350, 132203) 6-[132203, 154486) 7-[154486, 177346) 8-[177346, 198991) ... 31-[695934, 716847)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
716847, 13954819, 13954819
Number of vertices per chunk: 22402
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 22402
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.355 Gbps (per GPU), 474.840 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.047 Gbps (per GPU), 472.374 Gbps (aggregated)
The layer-level communication performance: 59.030 Gbps (per GPU), 472.241 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.782 Gbps (per GPU), 470.258 Gbps (aggregated)
The layer-level communication performance: 58.749 Gbps (per GPU), 469.991 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.542 Gbps (per GPU), 468.336 Gbps (aggregated)
The layer-level communication performance: 58.496 Gbps (per GPU), 467.964 Gbps (aggregated)
The layer-level communication performance: 58.464 Gbps (per GPU), 467.709 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.704 Gbps (per GPU), 1261.634 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.698 Gbps (per GPU), 1261.587 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.701 Gbps (per GPU), 1261.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.696 Gbps (per GPU), 1261.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.663 Gbps (per GPU), 1261.303 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.695 Gbps (per GPU), 1261.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.698 Gbps (per GPU), 1261.587 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.698 Gbps (per GPU), 1261.587 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 94.046 Gbps (per GPU), 752.370 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.048 Gbps (per GPU), 752.381 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.038 Gbps (per GPU), 752.308 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.033 Gbps (per GPU), 752.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.049 Gbps (per GPU), 752.392 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.048 Gbps (per GPU), 752.381 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.046 Gbps (per GPU), 752.365 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 94.035 Gbps (per GPU), 752.280 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.281 Gbps (per GPU), 274.251 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.281 Gbps (per GPU), 274.250 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.281 Gbps (per GPU), 274.244 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.282 Gbps (per GPU), 274.255 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.281 Gbps (per GPU), 274.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.281 Gbps (per GPU), 274.249 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.280 Gbps (per GPU), 274.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.280 Gbps (per GPU), 274.237 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.49ms  1.21ms  0.93ms  1.59 24.12K  0.35M
 chk_1  1.56ms  1.28ms  1.03ms  1.51 20.87K  0.49M
 chk_2  1.45ms  1.16ms  0.90ms  1.60 21.91K  0.46M
 chk_3  1.51ms  1.24ms  0.96ms  1.58 23.66K  0.35M
 chk_4  1.45ms  1.20ms  0.97ms  1.50 18.79K  0.62M
 chk_5  1.48ms  1.21ms  0.94ms  1.58 22.85K  0.37M
 chk_6  1.52ms  1.25ms  0.98ms  1.55 22.28K  0.40M
 chk_7  1.47ms  1.19ms  0.92ms  1.59 22.86K  0.39M
 chk_8  1.50ms  1.22ms  0.97ms  1.55 21.64K  0.45M
 chk_9  1.45ms  1.17ms  0.90ms  1.60 22.92K  0.37M
chk_10  1.53ms  1.27ms  1.03ms  1.49 20.30K  0.56M
chk_11  1.50ms  1.22ms  0.95ms  1.57 23.32K  0.33M
chk_12  1.53ms  1.25ms  1.00ms  1.52 21.10K  0.49M
chk_13  1.48ms  1.20ms  0.95ms  1.55 20.79K  0.49M
chk_14  1.45ms  1.18ms  0.90ms  1.61 23.53K  0.36M
chk_15  1.42ms  1.14ms  0.86ms  1.64 23.21K  0.39M
chk_16  1.43ms  1.16ms  0.87ms  1.64 24.39K  0.32M
chk_17  1.48ms  1.20ms  0.93ms  1.60 23.94K  0.34M
chk_18  1.49ms  1.21ms  0.96ms  1.56 21.61K  0.45M
chk_19  1.46ms  1.18ms  0.90ms  1.61 23.89K  0.34M
chk_20  1.53ms  1.25ms  0.99ms  1.55 21.64K  0.47M
chk_21  1.48ms  1.20ms  0.93ms  1.59 23.43K  0.34M
chk_22  1.48ms  1.20ms  0.93ms  1.59 22.84K  0.37M
chk_23  1.49ms  1.21ms  0.94ms  1.59 23.35K  0.37M
chk_24  1.50ms  1.23ms  0.96ms  1.57 22.72K  0.40M
chk_25  1.55ms  1.27ms  1.01ms  1.54 21.95K  0.46M
chk_26  1.53ms  1.25ms  0.99ms  1.55 22.06K  0.41M
chk_27  1.48ms  1.19ms  0.92ms  1.60 23.02K  0.35M
chk_28  1.48ms  1.20ms  0.93ms  1.59 22.97K  0.36M
chk_29  1.45ms  1.17ms  0.91ms  1.60 22.14K  0.45M
chk_30  1.52ms  1.24ms  0.98ms  1.55 21.84K  0.44M
chk_31  1.57ms  1.25ms  1.00ms  1.56 20.91K  0.49M
   Avg  1.49  1.21  0.95
   Max  1.57  1.28  1.03
   Min  1.42  1.14  0.86
 Ratio  1.11  1.13  1.20
   Var  0.00  0.00  0.00
Profiling takes 1.474 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 41)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 121)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 81)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [81, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [41, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [121, 159)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 354156
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [121, 159)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 354156, Num Local Vertices: 362691
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 41)...
+++++++++ Node 3 initializing the weights for op[41, 81)...
+++++++++ Node 2 initializing the weights for op[41, 81)...
+++++++++ Node 0 initializing the weights for op[0, 41)...
+++++++++ Node 4 initializing the weights for op[81, 121)...
+++++++++ Node 5 initializing the weights for op[81, 121)...
+++++++++ Node 6 initializing the weights for op[121, 159)...
+++++++++ Node 7 initializing the weights for op[121, 159)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 1731824
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 3, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 0.6931	TrainAcc 0.1921	ValidAcc 0.1912	TestAcc 0.1913	BestValid 0.1912
	Epoch 50:	Loss 0.6929	TrainAcc 0.1116	ValidAcc 0.1114	TestAcc 0.1116	BestValid 0.1912
	Epoch 100:	Loss 0.4877	TrainAcc 0.2227	ValidAcc 0.2230	TestAcc 0.2224	BestValid 0.2230
	Epoch 150:	Loss 0.3644	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2230
	Epoch 200:	Loss 0.3192	TrainAcc 0.2584	ValidAcc 0.2586	TestAcc 0.2581	BestValid 0.2586
	Epoch 250:	Loss 0.3088	TrainAcc 0.2575	ValidAcc 0.2578	TestAcc 0.2574	BestValid 0.2586
	Epoch 300:	Loss 0.3122	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.2586
	Epoch 350:	Loss 0.3114	TrainAcc 0.3159	ValidAcc 0.3153	TestAcc 0.3153	BestValid 0.3153
	Epoch 400:	Loss 0.3064	TrainAcc 0.3110	ValidAcc 0.3105	TestAcc 0.3101	BestValid 0.3153
	Epoch 450:	Loss 0.3086	TrainAcc 0.3201	ValidAcc 0.3201	TestAcc 0.3198	BestValid 0.3201
	Epoch 500:	Loss 0.3107	TrainAcc 0.3320	ValidAcc 0.3317	TestAcc 0.3311	BestValid 0.3317
	Epoch 550:	Loss 0.3073	TrainAcc 0.2913	ValidAcc 0.2905	TestAcc 0.2902	BestValid 0.3317
	Epoch 600:	Loss 0.3097	TrainAcc 0.2954	ValidAcc 0.2945	TestAcc 0.2943	BestValid 0.3317
	Epoch 650:	Loss 0.3141	TrainAcc 0.2730	ValidAcc 0.2732	TestAcc 0.2735	BestValid 0.3317
	Epoch 700:	Loss 0.3133	TrainAcc 0.2852	ValidAcc 0.2842	TestAcc 0.2842	BestValid 0.3317
	Epoch 750:	Loss 0.3064	TrainAcc 0.2787	ValidAcc 0.2777	TestAcc 0.2776	BestValid 0.3317
	Epoch 800:	Loss 0.3100	TrainAcc 0.3230	ValidAcc 0.3224	TestAcc 0.3221	BestValid 0.3317
	Epoch 850:	Loss 0.3137	TrainAcc 0.2735	ValidAcc 0.2737	TestAcc 0.2738	BestValid 0.3317
	Epoch 900:	Loss 0.3100	TrainAcc 0.2739	ValidAcc 0.2728	TestAcc 0.2727	BestValid 0.3317
	Epoch 950:	Loss 0.3097	TrainAcc 0.2704	ValidAcc 0.2694	TestAcc 0.2693	BestValid 0.3317
	Epoch 1000:	Loss 0.3221	TrainAcc 0.2853	ValidAcc 0.2855	TestAcc 0.2853	BestValid 0.3317
	Epoch 1050:	Loss 0.3219	TrainAcc 0.2759	ValidAcc 0.2748	TestAcc 0.2748	BestValid 0.3317
	Epoch 1100:	Loss 0.3100	TrainAcc 0.2677	ValidAcc 0.2667	TestAcc 0.2666	BestValid 0.3317
	Epoch 1150:	Loss 0.3098	TrainAcc 0.2831	ValidAcc 0.2822	TestAcc 0.2820	BestValid 0.3317
	Epoch 1200:	Loss 0.3340	TrainAcc 0.2759	ValidAcc 0.2748	TestAcc 0.2748	BestValid 0.3317
	Epoch 1250:	Loss 0.3215	TrainAcc 0.2677	ValidAcc 0.2667	TestAcc 0.2666	BestValid 0.3317
	Epoch 1300:	Loss 0.3101	TrainAcc 0.2677	ValidAcc 0.2667	TestAcc 0.2666	BestValid 0.3317
	Epoch 1350:	Loss 0.3235	TrainAcc 0.2874	ValidAcc 0.2875	TestAcc 0.2871	BestValid 0.3317
	Epoch 1400:	Loss 0.3256	TrainAcc 0.3139	ValidAcc 0.3132	TestAcc 0.3133	BestValid 0.3317
	Epoch 1450:	Loss 0.3108	TrainAcc 0.2739	ValidAcc 0.2728	TestAcc 0.2727	BestValid 0.3317
	Epoch 1500:	Loss 0.3195	TrainAcc 0.3174	ValidAcc 0.3171	TestAcc 0.3167	BestValid 0.3317
	Epoch 1550:	Loss 0.3319	TrainAcc 0.2734	ValidAcc 0.2736	TestAcc 0.2737	BestValid 0.3317
	Epoch 1600:	Loss 0.3222	TrainAcc 0.2777	ValidAcc 0.2766	TestAcc 0.2766	BestValid 0.3317
	Epoch 1650:	Loss 0.3106	TrainAcc 0.2738	ValidAcc 0.2728	TestAcc 0.2727	BestValid 0.3317
	Epoch 1700:	Loss 0.3210	TrainAcc 0.3143	ValidAcc 0.3136	TestAcc 0.3137	BestValid 0.3317
	Epoch 1750:	Loss 0.3661	TrainAcc 0.1862	ValidAcc 0.1862	TestAcc 0.1871	BestValid 0.3317
	Epoch 1800:	Loss 0.3253	TrainAcc 0.2950	ValidAcc 0.2940	TestAcc 0.2939	BestValid 0.3317
	Epoch 1850:	Loss 0.3114	TrainAcc 0.2798	ValidAcc 0.2787	TestAcc 0.2787	BestValid 0.3317
	Epoch 1900:	Loss 0.3260	TrainAcc 0.2798	ValidAcc 0.2787	TestAcc 0.2787	BestValid 0.3317
	Epoch 1950:	Loss 0.3437	TrainAcc 0.2991	ValidAcc 0.2984	TestAcc 0.2989	BestValid 0.3317
	Epoch 2000:	Loss 0.3292	TrainAcc 0.2896	ValidAcc 0.2886	TestAcc 0.2885	BestValid 0.3317
	Epoch 2050:	Loss 0.3224	TrainAcc 0.2896	ValidAcc 0.2886	TestAcc 0.2885	BestValid 0.3317
	Epoch 2100:	Loss 0.3587	TrainAcc 0.1414	ValidAcc 0.1421	TestAcc 0.1417	BestValid 0.3317
	Epoch 2150:	Loss 0.3500	TrainAcc 0.3041	ValidAcc 0.3040	TestAcc 0.3034	BestValid 0.3317
	Epoch 2200:	Loss 0.3236	TrainAcc 0.3056	ValidAcc 0.3047	TestAcc 0.3046	BestValid 0.3317
	Epoch 2250:	Loss 0.3142	TrainAcc 0.2897	ValidAcc 0.2887	TestAcc 0.2886	BestValid 0.3317
	Epoch 2300:	Loss 0.3282	TrainAcc 0.3326	ValidAcc 0.3324	TestAcc 0.3317	BestValid 0.3324
	Epoch 2350:	Loss 0.3469	TrainAcc 0.3114	ValidAcc 0.3106	TestAcc 0.3112	BestValid 0.3324
	Epoch 2400:	Loss 0.3216	TrainAcc 0.2831	ValidAcc 0.2821	TestAcc 0.2820	BestValid 0.3324
	Epoch 2450:	Loss 0.3180	TrainAcc 0.2896	ValidAcc 0.2886	TestAcc 0.2885	BestValid 0.3324
	Epoch 2500:	Loss 0.3559	TrainAcc 0.2734	ValidAcc 0.2736	TestAcc 0.2736	BestValid 0.3324
	Epoch 2550:	Loss 0.3785	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
	Epoch 2600:	Loss 0.3315	TrainAcc 0.3182	ValidAcc 0.3175	TestAcc 0.3177	BestValid 0.3324
	Epoch 2650:	Loss 0.3254	TrainAcc 0.2842	ValidAcc 0.2831	TestAcc 0.2832	BestValid 0.3324
	Epoch 2700:	Loss 0.3617	TrainAcc 0.1417	ValidAcc 0.1419	TestAcc 0.1424	BestValid 0.3324
	Epoch 2750:	Loss 0.4254	TrainAcc 0.1415	ValidAcc 0.1421	TestAcc 0.1419	BestValid 0.3324
	Epoch 2800:	Loss 0.4291	TrainAcc 0.1417	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
	Epoch 2850:	Loss 0.3797	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1425	BestValid 0.3324
	Epoch 2900:	Loss 0.3772	TrainAcc 0.1412	ValidAcc 0.1417	TestAcc 0.1421	BestValid 0.3324
	Epoch 2950:	Loss 0.4013	TrainAcc 0.1415	ValidAcc 0.1421	TestAcc 0.1419	BestValid 0.3324
	Epoch 3000:	Loss 0.3858	TrainAcc 0.1416	ValidAcc 0.1419	TestAcc 0.1424	BestValid 0.3324
	Epoch 3050:	Loss 0.3756	TrainAcc 0.1426	ValidAcc 0.1430	TestAcc 0.1434	BestValid 0.3324
	Epoch 3100:	Loss 0.3591	TrainAcc 0.1414	ValidAcc 0.1421	TestAcc 0.1418	BestValid 0.3324
	Epoch 3150:	Loss 0.3538	TrainAcc 0.1413	ValidAcc 0.1422	TestAcc 0.1418	BestValid 0.3324
	Epoch 3200:	Loss 0.3442	TrainAcc 0.2628	ValidAcc 0.2629	TestAcc 0.2636	BestValid 0.3324
	Epoch 3250:	Loss 0.3204	TrainAcc 0.3303	ValidAcc 0.3299	TestAcc 0.3294	BestValid 0.3324
	Epoch 3300:	Loss 0.3159	TrainAcc 0.2915	ValidAcc 0.2905	TestAcc 0.2904	BestValid 0.3324
	Epoch 3350:	Loss 0.3295	TrainAcc 0.2950	ValidAcc 0.2940	TestAcc 0.2939	BestValid 0.3324
	Epoch 3400:	Loss 0.3775	TrainAcc 0.1430	ValidAcc 0.1429	TestAcc 0.1438	BestValid 0.3324
	Epoch 3450:	Loss 0.3490	TrainAcc 0.2564	ValidAcc 0.2560	TestAcc 0.2572	BestValid 0.3324
	Epoch 3500:	Loss 0.3210	TrainAcc 0.2891	ValidAcc 0.2879	TestAcc 0.2881	BestValid 0.3324
	Epoch 3550:	Loss 0.3348	TrainAcc 0.3281	ValidAcc 0.3278	TestAcc 0.3276	BestValid 0.3324
	Epoch 3600:	Loss 0.4048	TrainAcc 0.1413	ValidAcc 0.1420	TestAcc 0.1419	BestValid 0.3324
	Epoch 3650:	Loss 0.5037	TrainAcc 0.1413	ValidAcc 0.1420	TestAcc 0.1419	BestValid 0.3324
	Epoch 3700:	Loss 0.6389	TrainAcc 0.1413	ValidAcc 0.1420	TestAcc 0.1419	BestValid 0.3324
	Epoch 3750:	Loss 0.6626	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3324
	Epoch 3800:	Loss 0.3722	TrainAcc 0.1415	ValidAcc 0.1417	TestAcc 0.1423	BestValid 0.3324
	Epoch 3850:	Loss 0.3920	TrainAcc 0.1414	ValidAcc 0.1421	TestAcc 0.1419	BestValid 0.3324
	Epoch 3900:	Loss 0.4194	TrainAcc 0.1413	ValidAcc 0.1419	TestAcc 0.1418	BestValid 0.3324
	Epoch 3950:	Loss 0.3921	TrainAcc 0.1421	ValidAcc 0.1422	TestAcc 0.1429	BestValid 0.3324
	Epoch 4000:	Loss 0.3721	TrainAcc 0.1414	ValidAcc 0.1421	TestAcc 0.1419	BestValid 0.3324
	Epoch 4050:	Loss 0.3784	TrainAcc 0.1414	ValidAcc 0.1420	TestAcc 0.1419	BestValid 0.3324
	Epoch 4100:	Loss 0.3739	TrainAcc 0.1418	ValidAcc 0.1419	TestAcc 0.1426	BestValid 0.3324
	Epoch 4150:	Loss 0.3634	TrainAcc 0.1416	ValidAcc 0.1417	TestAcc 0.1424	BestValid 0.3324
	Epoch 4200:	Loss 0.3457	TrainAcc 0.1416	ValidAcc 0.1417	TestAcc 0.1424	BestValid 0.3324
	Epoch 4250:	Loss 0.3350	TrainAcc 0.2565	ValidAcc 0.2561	TestAcc 0.2573	BestValid 0.3324
	Epoch 4300:	Loss 0.3354	TrainAcc 0.2856	ValidAcc 0.2850	TestAcc 0.2856	BestValid 0.3324
	Epoch 4350:	Loss 0.3837	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
	Epoch 4400:	Loss 0.4310	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
	Epoch 4450:	Loss 0.4961	TrainAcc 0.1426	ValidAcc 0.1433	TestAcc 0.1437	BestValid 0.3324
	Epoch 4500:	Loss 0.5967	TrainAcc 0.1412	ValidAcc 0.1414	TestAcc 0.1418	BestValid 0.3324
	Epoch 4550:	Loss 0.4428	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
	Epoch 4600:	Loss 0.3835	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1423	BestValid 0.3324
	Epoch 4650:	Loss 0.3889	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
	Epoch 4700:	Loss 0.3963	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
	Epoch 4750:	Loss 0.4003	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
	Epoch 4800:	Loss 0.4201	TrainAcc 0.1417	ValidAcc 0.1419	TestAcc 0.1424	BestValid 0.3324
	Epoch 4850:	Loss 0.4676	TrainAcc 0.1415	ValidAcc 0.1421	TestAcc 0.1419	BestValid 0.3324
	Epoch 4900:	Loss 0.4255	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
	Epoch 4950:	Loss 0.3955	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
	Epoch 5000:	Loss 0.4348	TrainAcc 0.1416	ValidAcc 0.1418	TestAcc 0.1424	BestValid 0.3324
****** Epoch Time (Excluding Evaluation Cost): 0.358 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 52.059 ms (Max: 55.050, Min: 49.224, Sum: 416.470)
Cluster-Wide Average, Compute: 147.497 ms (Max: 152.251, Min: 144.900, Sum: 1179.979)
Cluster-Wide Average, Communication-Layer: 40.999 ms (Max: 50.856, Min: 30.584, Sum: 327.995)
Cluster-Wide Average, Bubble-Imbalance: 11.118 ms (Max: 20.627, Min: 4.326, Sum: 88.944)
Cluster-Wide Average, Communication-Graph: 94.622 ms (Max: 95.576, Min: 93.635, Sum: 756.975)
Cluster-Wide Average, Optimization: 1.735 ms (Max: 1.976, Min: 1.473, Sum: 13.880)
Cluster-Wide Average, Others: 10.839 ms (Max: 12.499, Min: 10.152, Sum: 86.709)
****** Breakdown Sum: 358.869 ms ******
Cluster-Wide Average, GPU Memory Consumption: 11.862 GB (Max: 13.522, Min: 11.294, Sum: 94.896)
Cluster-Wide Average, Graph-Level Communication Throughput: 143.687 Gbps (Max: 147.631, Min: 139.162, Sum: 1149.492)
Cluster-Wide Average, Layer-Level Communication Throughput: 40.882 Gbps (Max: 46.549, Min: 34.759, Sum: 327.053)
Layer-level communication (cluster-wide, per-epoch): 1.602 GB
Graph-level communication (cluster-wide, per-epoch): 10.322 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 11.927 GB
****** Accuracy Results ******
Highest valid_acc: 0.3324
Target test_acc: 0.3317
Epoch to reach the target acc: 2299
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
