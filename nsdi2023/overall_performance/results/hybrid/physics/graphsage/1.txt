Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INITDONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7

Initialized node 0 on machine gnerv7
DONE MPI INITDONE MPI INIT
Initialized node 4 on machine gnerv8

Initialized node 5 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
        It takes 0.011 seconds.
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.011 seconds.
        It takes 0.017 seconds.
        It takes 0.013 seconds.
Building the Feature Vector...
        It takes 0.013 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.455 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.457 seconds.
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.517 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.518 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.579 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.620 seconds.
Building the Label Vector...
        It takes 0.625 seconds.
Building the Label Vector...
        It takes 0.004 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/physics/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 300
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 8
        It takes 0.618 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.003 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 1025) 1-[1025, 2154) 2-[2154, 3262) 3-[3262, 4284) 4-[4284, 5334) 5-[5334, 6511) 6-[6511, 7597) 7-[7597, 8680) 8-[8680, 9764) ... 31-[33337, 34493)
34493, 530417, 530417
Number of vertices per chunk: 1078
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 59.963 Gbps (per GPU), 479.703 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.676 Gbps (per GPU), 477.410 Gbps (aggregated)
The layer-level communication performance: 59.918 Gbps (per GPU), 479.345 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.395 Gbps (per GPU), 475.157 Gbps (aggregated)
The layer-level communication performance: 59.374 Gbps (per GPU), 474.996 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.153 Gbps (per GPU), 473.220 Gbps (aggregated)
The layer-level communication performance: 59.101 Gbps (per GPU), 472.809 Gbps (aggregated)
The layer-level communication performance: 59.073 Gbps (per GPU), 472.582 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 149.933 Gbps (per GPU), 1199.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 149.933 Gbps (per GPU), 1199.463 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 149.928 Gbps (per GPU), 1199.422 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 149.941 Gbps (per GPU), 1199.531 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 149.925 Gbps (per GPU), 1199.401 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 149.503 Gbps (per GPU), 1196.023 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 2): 139.794 Gbps (per GPU), 1118.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 139.801 Gbps (per GPU), 1118.407 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 105.127 Gbps (per GPU), 841.012 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.131 Gbps (per GPU), 841.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.131 Gbps (per GPU), 841.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.132 Gbps (per GPU), 841.054 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.117 Gbps (per GPU), 840.935 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.132 Gbps (per GPU), 841.055 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.130 Gbps (per GPU), 841.041 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.033 Gbps (per GPU), 840.261 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.621 Gbps (per GPU), 308.965 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.619 Gbps (per GPU), 308.952 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.618 Gbps (per GPU), 308.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.619 Gbps (per GPU), 308.956 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.619 Gbps (per GPU), 308.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.620 Gbps (per GPU), 308.959 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.616 Gbps (per GPU), 308.927 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.618 Gbps (per GPU), 308.943 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.89ms  0.34ms  0.27ms  7.10  1.02K  0.02M
 chk_1  2.00ms  0.34ms  0.25ms  8.15  1.13K  0.01M
 chk_2  1.99ms  0.34ms  0.25ms  8.05  1.11K  0.01M
 chk_3  1.89ms  0.33ms  0.25ms  7.70  1.02K  0.02M
 chk_4  1.93ms  0.34ms  0.25ms  7.87  1.05K  0.02M
 chk_5  2.06ms  0.34ms  0.25ms  8.34  1.18K  0.01M
 chk_6  1.98ms  0.34ms  0.25ms  8.08  1.09K  0.02M
 chk_7  1.99ms  0.34ms  0.28ms  7.22  1.08K  0.02M
 chk_8  1.99ms  0.34ms  0.25ms  7.99  1.08K  0.01M
 chk_9  1.94ms  0.33ms  0.25ms  7.88  1.06K  0.02M
chk_10  1.93ms  0.34ms  0.27ms  7.06  1.04K  0.02M
chk_11  2.00ms  0.34ms  0.25ms  8.06  1.11K  0.01M
chk_12  2.01ms  0.34ms  0.25ms  8.03  1.11K  0.01M
chk_13  1.89ms  0.32ms  0.25ms  7.58  1.01K  0.02M
chk_14  1.99ms  0.34ms  0.25ms  8.02  1.09K  0.01M
chk_15  1.92ms  0.33ms  0.25ms  7.78  1.03K  0.02M
chk_16  1.84ms  0.33ms  0.25ms  7.50  0.98K  0.02M
chk_17  1.99ms  0.34ms  0.27ms  7.31  1.09K  0.01M
chk_18  1.93ms  0.33ms  0.25ms  7.58  1.04K  0.02M
chk_19  2.01ms  0.34ms  0.25ms  8.13  1.12K  0.01M
chk_20  2.00ms  0.34ms  0.25ms  8.12  1.13K  0.01M
chk_21  1.93ms  0.33ms  0.25ms  7.80  1.05K  0.02M
chk_22  2.00ms  0.34ms  0.24ms  8.16  1.11K  0.01M
chk_23  1.99ms  0.34ms  0.24ms  8.18  1.10K  0.01M
chk_24  1.90ms  0.33ms  0.25ms  7.73  1.02K  0.02M
chk_25  1.89ms  0.33ms  0.25ms  7.60  1.01K  0.02M
chk_26  2.01ms  0.34ms  0.25ms  8.17  1.12K  0.01M
chk_27  1.96ms  0.34ms  0.25ms  7.83  1.08K  0.02M
chk_28  1.96ms  0.34ms  0.28ms  7.09  1.07K  0.02M
chk_29  2.06ms  0.34ms  0.25ms  8.23  1.11K  0.01M
chk_30  1.99ms  0.33ms  0.27ms  7.25  1.10K  0.01M
chk_31  2.05ms  0.34ms  0.25ms  8.20  1.16K  0.01M
   Avg  1.97  0.34  0.25
   Max  2.06  0.34  0.28
   Min  1.84  0.32  0.24
 Ratio  1.12  1.06  1.13
   Var  0.00  0.00  0.00
Profiling takes 1.049 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 17213
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 17213
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 17213
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 17213, Num Local Vertices: 17280
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 17213, Num Local Vertices: 17280
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 17213
Node 5, Local Vertex Begin: 17213, Num Local Vertices: 17280
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 17213, Num Local Vertices: 17280
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 66)...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 6 initializing the weights for op[194, 257)...
+++++++++ Node 5 initializing the weights for op[130, 194)...
+++++++++ Node 7 initializing the weights for op[194, 257)...
+++++++++ Node 2 initializing the weights for op[66, 130)...
+++++++++ Node 1 initializing the weights for op[0, 66)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 89728
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6848	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 10:	Loss 1.5271	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 20:	Loss 1.4277	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 30:	Loss 1.4784	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 40:	Loss 1.4266	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 50:	Loss 1.3726	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 60:	Loss 1.3788	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 70:	Loss 1.3676	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 80:	Loss 1.3529	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 90:	Loss 1.3222	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 100:	Loss 1.3291	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 110:	Loss 1.3302	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 120:	Loss 1.3170	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 130:	Loss 1.3069	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 140:	Loss 1.3070	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 150:	Loss 1.3002	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 160:	Loss 1.2946	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 170:	Loss 1.2942	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 180:	Loss 1.2875	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 190:	Loss 1.2806	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 200:	Loss 1.2861	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 210:	Loss 1.2712	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 220:	Loss 1.2483	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 230:	Loss 1.2575	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 240:	Loss 1.2750	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 250:	Loss 1.2461	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 260:	Loss 1.2521	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 270:	Loss 1.2805	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 280:	Loss 1.2461	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.0840
	Epoch 290:	Loss 1.2611	TrainAcc 0.2100	ValidAcc 0.1020	TestAcc 0.1040	BestValid 0.1020
	Epoch 300:	Loss 1.2340	TrainAcc 0.2200	ValidAcc 0.1020	TestAcc 0.1110	BestValid 0.1020
****** Epoch Time (Excluding Evaluation Cost): 0.126 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 18.349 ms (Max: 21.072, Min: 14.674, Sum: 146.792)
Cluster-Wide Average, Compute: 60.922 ms (Max: 82.723, Min: 51.899, Sum: 487.379)
Cluster-Wide Average, Communication-Layer: 2.975 ms (Max: 3.774, Min: 2.019, Sum: 23.800)
Cluster-Wide Average, Bubble-Imbalance: 17.253 ms (Max: 25.796, Min: 0.538, Sum: 138.027)
Cluster-Wide Average, Communication-Graph: 20.533 ms (Max: 21.794, Min: 19.715, Sum: 164.264)
Cluster-Wide Average, Optimization: 2.787 ms (Max: 4.965, Min: 1.912, Sum: 22.298)
Cluster-Wide Average, Others: 2.742 ms (Max: 3.815, Min: 0.837, Sum: 21.937)
****** Breakdown Sum: 125.562 ms ******
Cluster-Wide Average, GPU Memory Consumption: 2.858 GB (Max: 4.546, Min: 2.311, Sum: 22.860)
Cluster-Wide Average, Graph-Level Communication Throughput: 41.755 Gbps (Max: 44.387, Min: 38.947, Sum: 334.042)
Cluster-Wide Average, Layer-Level Communication Throughput: 27.355 Gbps (Max: 32.860, Min: 20.052, Sum: 218.843)
Layer-level communication (cluster-wide, per-epoch): 0.077 GB
Graph-level communication (cluster-wide, per-epoch): 0.535 GB
Weight-sync communication (cluster-wide, per-epoch): 0.017 GB
Total communication (cluster-wide, per-epoch): 0.629 GB
****** Accuracy Results ******
Highest valid_acc: 0.1020
Target test_acc: 0.1040
Epoch to reach the target acc: 289
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
