Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv8

Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7

Initialized node 1 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.068 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
        It takes 0.048 seconds.
        It takes 0.051 seconds.
        It takes 0.054 seconds.
        It takes 0.051 seconds.
        It takes 0.055 seconds.
        It takes 0.056 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
        It takes 0.058 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.027 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.028 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.865 Gbps (per GPU), 486.922 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.563 Gbps (per GPU), 484.506 Gbps (aggregated)
The layer-level communication performance: 60.559 Gbps (per GPU), 484.471 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.311 Gbps (per GPU), 482.484 Gbps (aggregated)
The layer-level communication performance: 60.277 Gbps (per GPU), 482.214 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.067 Gbps (per GPU), 480.535 Gbps (aggregated)
The layer-level communication performance: 60.016 Gbps (per GPU), 480.131 Gbps (aggregated)
The layer-level communication performance: 59.985 Gbps (per GPU), 479.879 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.832 Gbps (per GPU), 1278.654 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.829 Gbps (per GPU), 1278.629 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.816 Gbps (per GPU), 1278.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.826 Gbps (per GPU), 1278.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.832 Gbps (per GPU), 1278.654 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.829 Gbps (per GPU), 1278.629 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.829 Gbps (per GPU), 1278.629 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.829 Gbps (per GPU), 1278.636 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.820 Gbps (per GPU), 838.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.817 Gbps (per GPU), 838.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.820 Gbps (per GPU), 838.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.817 Gbps (per GPU), 838.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.821 Gbps (per GPU), 838.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.812 Gbps (per GPU), 838.497 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.820 Gbps (per GPU), 838.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.813 Gbps (per GPU), 838.504 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.607 Gbps (per GPU), 276.858 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.608 Gbps (per GPU), 276.865 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.608 Gbps (per GPU), 276.865 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.609 Gbps (per GPU), 276.870 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.604 Gbps (per GPU), 276.833 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.603 Gbps (per GPU), 276.828 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.605 Gbps (per GPU), 276.837 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.603 Gbps (per GPU), 276.822 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.57ms  0.50ms  0.35ms  1.65  5.55K  0.06M
 chk_1  0.58ms  0.50ms  0.35ms  1.65  5.75K  0.05M
 chk_2  0.58ms  0.50ms  0.36ms  1.62  5.20K  0.07M
 chk_3  0.58ms  0.50ms  0.35ms  1.65  5.57K  0.06M
 chk_4  0.57ms  0.50ms  0.35ms  1.63  5.05K  0.08M
 chk_5  0.59ms  0.52ms  0.38ms  1.55  4.73K  0.11M
 chk_6  0.57ms  0.49ms  0.35ms  1.62  4.98K  0.08M
 chk_7  0.57ms  0.50ms  0.36ms  1.60  5.01K  0.09M
 chk_8  0.58ms  0.50ms  0.35ms  1.66  5.73K  0.05M
 chk_9  0.56ms  0.49ms  0.36ms  1.58  4.54K  0.11M
chk_10  0.57ms  0.50ms  0.35ms  1.63  5.36K  0.07M
chk_11  0.58ms  0.51ms  0.36ms  1.62  5.39K  0.08M
chk_12  0.58ms  0.50ms  0.35ms  1.66  5.77K  0.05M
chk_13  0.58ms  0.50ms  0.35ms  1.63  5.43K  0.06M
chk_14  0.57ms  0.50ms  0.35ms  1.64  5.46K  0.06M
chk_15  0.57ms  0.50ms  0.34ms  1.66  5.88K  0.04M
chk_16  0.58ms  0.50ms  0.35ms  1.63  5.50K  0.06M
chk_17  0.59ms  0.52ms  0.37ms  1.56  4.86K  0.09M
chk_18  0.60ms  0.52ms  0.38ms  1.59  5.39K  0.07M
chk_19  0.58ms  0.51ms  0.36ms  1.61  5.20K  0.07M
chk_20  0.57ms  0.50ms  0.35ms  1.65  5.51K  0.06M
chk_21  0.57ms  0.49ms  0.34ms  1.68  5.81K  0.05M
chk_22  0.57ms  0.50ms  0.35ms  1.63  5.32K  0.07M
chk_23  0.59ms  0.52ms  0.37ms  1.60  5.39K  0.07M
chk_24  0.57ms  0.50ms  0.55ms  1.14  4.62K  0.11M
chk_25  0.58ms  0.50ms  0.36ms  1.60  5.04K  0.08M
chk_26  0.56ms  0.50ms  0.36ms  1.58  4.55K  0.11M
chk_27  0.57ms  0.49ms  0.35ms  1.63  5.30K  0.06M
chk_28  0.57ms  0.50ms  0.35ms  1.65  5.58K  0.06M
chk_29  0.58ms  0.51ms  0.37ms  1.57  4.98K  0.09M
chk_30  0.58ms  0.50ms  0.35ms  1.66  5.50K  0.07M
chk_31  0.58ms  0.50ms  0.35ms  1.64  5.38K  0.07M
   Avg  0.58  0.50  0.36
   Max  0.60  0.52  0.55
   Min  0.56  0.49  0.34
 Ratio  1.06  1.07  1.62
   Var  0.00  0.00  0.00
Profiling takes 0.666 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 85420, Num Local Vertices: 83923
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[66, 130)...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
+++++++++ Node 5 initializing the weights for op[130, 194)...
+++++++++ Node 0 initializing the weights for op[0, 66)...
+++++++++ Node 6 initializing the weights for op[194, 257)...
+++++++++ Node 1 initializing the weights for op[0, 66)...
+++++++++ Node 7 initializing the weights for op[194, 257)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 327544
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 9.3435	TrainAcc 0.0128	ValidAcc 0.0279	TestAcc 0.0342	BestValid 0.0279
	Epoch 50:	Loss 3.6887	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 100:	Loss 3.6883	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 150:	Loss 3.2594	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 200:	Loss 2.8824	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 250:	Loss 2.8573	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 300:	Loss 2.8532	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 350:	Loss 2.8517	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 400:	Loss 2.8258	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 450:	Loss 2.8159	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 500:	Loss 2.8044	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 550:	Loss 2.7978	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 600:	Loss 2.8006	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 650:	Loss 2.7929	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 700:	Loss 2.7935	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 750:	Loss 2.7906	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 800:	Loss 2.7910	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 850:	Loss 2.7850	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 900:	Loss 2.7811	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 950:	Loss 2.7753	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1000:	Loss 2.7755	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1050:	Loss 2.7694	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 1100:	Loss 2.7704	TrainAcc 0.1801	ValidAcc 0.0775	TestAcc 0.0589	BestValid 0.0775
	Epoch 1150:	Loss 2.7706	TrainAcc 0.1810	ValidAcc 0.0783	TestAcc 0.0593	BestValid 0.0783
	Epoch 1200:	Loss 2.7698	TrainAcc 0.1807	ValidAcc 0.0781	TestAcc 0.0592	BestValid 0.0783
	Epoch 1250:	Loss 2.7675	TrainAcc 0.1841	ValidAcc 0.0816	TestAcc 0.0605	BestValid 0.0816
	Epoch 1300:	Loss 2.7259	TrainAcc 0.2172	ValidAcc 0.0941	TestAcc 0.0743	BestValid 0.0941
	Epoch 1350:	Loss 2.6795	TrainAcc 0.2188	ValidAcc 0.0932	TestAcc 0.0737	BestValid 0.0941
	Epoch 1400:	Loss 2.6564	TrainAcc 0.2207	ValidAcc 0.0941	TestAcc 0.0745	BestValid 0.0941
	Epoch 1450:	Loss 2.6617	TrainAcc 0.2216	ValidAcc 0.0945	TestAcc 0.0743	BestValid 0.0945
	Epoch 1500:	Loss 2.6404	TrainAcc 0.2234	ValidAcc 0.0952	TestAcc 0.0753	BestValid 0.0952
	Epoch 1550:	Loss 2.6013	TrainAcc 0.2168	ValidAcc 0.0938	TestAcc 0.0729	BestValid 0.0952
	Epoch 1600:	Loss 2.5347	TrainAcc 0.2251	ValidAcc 0.0980	TestAcc 0.0768	BestValid 0.0980
	Epoch 1650:	Loss 2.4876	TrainAcc 0.0397	ValidAcc 0.0168	TestAcc 0.0151	BestValid 0.0980
	Epoch 1700:	Loss 2.4500	TrainAcc 0.1843	ValidAcc 0.0805	TestAcc 0.0623	BestValid 0.0980
	Epoch 1750:	Loss 2.4326	TrainAcc 0.1792	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0980
	Epoch 1800:	Loss 2.3988	TrainAcc 0.1828	ValidAcc 0.0776	TestAcc 0.0598	BestValid 0.0980
	Epoch 1850:	Loss 2.3938	TrainAcc 0.1793	ValidAcc 0.0766	TestAcc 0.0589	BestValid 0.0980
	Epoch 1900:	Loss 2.3866	TrainAcc 0.1791	ValidAcc 0.0764	TestAcc 0.0588	BestValid 0.0980
	Epoch 1950:	Loss 2.3748	TrainAcc 0.1793	ValidAcc 0.0764	TestAcc 0.0587	BestValid 0.0980
	Epoch 2000:	Loss 2.3589	TrainAcc 0.1829	ValidAcc 0.0783	TestAcc 0.0601	BestValid 0.0980
	Epoch 2050:	Loss 2.3650	TrainAcc 0.1791	ValidAcc 0.0765	TestAcc 0.0588	BestValid 0.0980
	Epoch 2100:	Loss 2.3548	TrainAcc 0.1800	ValidAcc 0.0769	TestAcc 0.0592	BestValid 0.0980
	Epoch 2150:	Loss 2.3417	TrainAcc 0.1793	ValidAcc 0.0766	TestAcc 0.0588	BestValid 0.0980
	Epoch 2200:	Loss 2.3352	TrainAcc 0.1797	ValidAcc 0.0767	TestAcc 0.0591	BestValid 0.0980
	Epoch 2250:	Loss 2.3347	TrainAcc 0.1802	ValidAcc 0.0768	TestAcc 0.0590	BestValid 0.0980
	Epoch 2300:	Loss 2.3402	TrainAcc 0.1901	ValidAcc 0.0824	TestAcc 0.0636	BestValid 0.0980
	Epoch 2350:	Loss 2.3535	TrainAcc 0.1849	ValidAcc 0.0793	TestAcc 0.0607	BestValid 0.0980
	Epoch 2400:	Loss 2.3184	TrainAcc 0.1795	ValidAcc 0.0764	TestAcc 0.0587	BestValid 0.0980
	Epoch 2450:	Loss 2.3113	TrainAcc 0.1794	ValidAcc 0.0765	TestAcc 0.0587	BestValid 0.0980
	Epoch 2500:	Loss 2.3238	TrainAcc 0.1802	ValidAcc 0.0773	TestAcc 0.0593	BestValid 0.0980
	Epoch 2550:	Loss 2.3029	TrainAcc 0.1793	ValidAcc 0.0765	TestAcc 0.0588	BestValid 0.0980
	Epoch 2600:	Loss 2.3208	TrainAcc 0.1883	ValidAcc 0.0815	TestAcc 0.0628	BestValid 0.0980
	Epoch 2650:	Loss 2.3096	TrainAcc 0.1844	ValidAcc 0.0791	TestAcc 0.0610	BestValid 0.0980
	Epoch 2700:	Loss 2.3043	TrainAcc 0.2063	ValidAcc 0.0876	TestAcc 0.0684	BestValid 0.0980
	Epoch 2750:	Loss 2.3033	TrainAcc 0.1811	ValidAcc 0.0776	TestAcc 0.0595	BestValid 0.0980
	Epoch 2800:	Loss 2.2969	TrainAcc 0.2309	ValidAcc 0.1030	TestAcc 0.0802	BestValid 0.1030
	Epoch 2850:	Loss 2.2909	TrainAcc 0.2245	ValidAcc 0.0981	TestAcc 0.0775	BestValid 0.1030
	Epoch 2900:	Loss 2.2996	TrainAcc 0.1815	ValidAcc 0.0774	TestAcc 0.0594	BestValid 0.1030
	Epoch 2950:	Loss 2.2943	TrainAcc 0.1844	ValidAcc 0.0795	TestAcc 0.0611	BestValid 0.1030
	Epoch 3000:	Loss 2.2947	TrainAcc 0.1980	ValidAcc 0.0861	TestAcc 0.0667	BestValid 0.1030
	Epoch 3050:	Loss 2.2929	TrainAcc 0.1984	ValidAcc 0.0886	TestAcc 0.0686	BestValid 0.1030
	Epoch 3100:	Loss 2.3006	TrainAcc 0.1979	ValidAcc 0.0842	TestAcc 0.0657	BestValid 0.1030
	Epoch 3150:	Loss 2.3012	TrainAcc 0.2002	ValidAcc 0.0881	TestAcc 0.0688	BestValid 0.1030
	Epoch 3200:	Loss 2.3006	TrainAcc 0.1986	ValidAcc 0.0857	TestAcc 0.0667	BestValid 0.1030
	Epoch 3250:	Loss 2.2997	TrainAcc 0.1794	ValidAcc 0.0767	TestAcc 0.0588	BestValid 0.1030
	Epoch 3300:	Loss 2.3070	TrainAcc 0.1792	ValidAcc 0.0763	TestAcc 0.0587	BestValid 0.1030
	Epoch 3350:	Loss 2.3071	TrainAcc 0.1802	ValidAcc 0.0766	TestAcc 0.0590	BestValid 0.1030
	Epoch 3400:	Loss 2.2854	TrainAcc 0.0628	ValidAcc 0.0332	TestAcc 0.0269	BestValid 0.1030
	Epoch 3450:	Loss 2.2890	TrainAcc 0.1784	ValidAcc 0.0756	TestAcc 0.0577	BestValid 0.1030
	Epoch 3500:	Loss 2.2935	TrainAcc 0.1795	ValidAcc 0.0770	TestAcc 0.0588	BestValid 0.1030
	Epoch 3550:	Loss 2.2950	TrainAcc 0.1813	ValidAcc 0.0786	TestAcc 0.0602	BestValid 0.1030
	Epoch 3600:	Loss 2.2918	TrainAcc 0.1420	ValidAcc 0.0553	TestAcc 0.0445	BestValid 0.1030
	Epoch 3650:	Loss 2.2892	TrainAcc 0.1811	ValidAcc 0.0769	TestAcc 0.0595	BestValid 0.1030
	Epoch 3700:	Loss 2.3022	TrainAcc 0.1776	ValidAcc 0.0755	TestAcc 0.0574	BestValid 0.1030
	Epoch 3750:	Loss 2.2915	TrainAcc 0.1797	ValidAcc 0.0769	TestAcc 0.0590	BestValid 0.1030
	Epoch 3800:	Loss 2.2949	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0587	BestValid 0.1030
	Epoch 3850:	Loss 2.2811	TrainAcc 0.1818	ValidAcc 0.0776	TestAcc 0.0595	BestValid 0.1030
	Epoch 3900:	Loss 2.2819	TrainAcc 0.1804	ValidAcc 0.0769	TestAcc 0.0593	BestValid 0.1030
	Epoch 3950:	Loss 2.2840	TrainAcc 0.1629	ValidAcc 0.0657	TestAcc 0.0508	BestValid 0.1030
	Epoch 4000:	Loss 2.2938	TrainAcc 0.1009	ValidAcc 0.0302	TestAcc 0.0235	BestValid 0.1030
	Epoch 4050:	Loss 2.3574	TrainAcc 0.0994	ValidAcc 0.0378	TestAcc 0.0327	BestValid 0.1030
	Epoch 4100:	Loss 2.3053	TrainAcc 0.1798	ValidAcc 0.0764	TestAcc 0.0589	BestValid 0.1030
	Epoch 4150:	Loss 2.2855	TrainAcc 0.1837	ValidAcc 0.0783	TestAcc 0.0603	BestValid 0.1030
	Epoch 4200:	Loss 2.2914	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0587	BestValid 0.1030
	Epoch 4250:	Loss 2.2718	TrainAcc 0.1784	ValidAcc 0.0763	TestAcc 0.0584	BestValid 0.1030
	Epoch 4300:	Loss 2.2726	TrainAcc 0.0643	ValidAcc 0.0390	TestAcc 0.0294	BestValid 0.1030
	Epoch 4350:	Loss 2.2799	TrainAcc 0.2017	ValidAcc 0.0885	TestAcc 0.0681	BestValid 0.1030
	Epoch 4400:	Loss 2.2705	TrainAcc 0.1958	ValidAcc 0.0843	TestAcc 0.0648	BestValid 0.1030
	Epoch 4450:	Loss 2.2769	TrainAcc 0.1189	ValidAcc 0.0696	TestAcc 0.0573	BestValid 0.1030
	Epoch 4500:	Loss 2.2867	TrainAcc 0.0869	ValidAcc 0.0517	TestAcc 0.0418	BestValid 0.1030
	Epoch 4550:	Loss 2.2753	TrainAcc 0.1943	ValidAcc 0.0851	TestAcc 0.0656	BestValid 0.1030
	Epoch 4600:	Loss 2.2762	TrainAcc 0.0344	ValidAcc 0.0170	TestAcc 0.0151	BestValid 0.1030
	Epoch 4650:	Loss 2.2736	TrainAcc 0.0218	ValidAcc 0.0084	TestAcc 0.0081	BestValid 0.1030
	Epoch 4700:	Loss 2.2721	TrainAcc 0.0774	ValidAcc 0.0417	TestAcc 0.0350	BestValid 0.1030
	Epoch 4750:	Loss 2.2675	TrainAcc 0.1712	ValidAcc 0.0711	TestAcc 0.0557	BestValid 0.1030
	Epoch 4800:	Loss 2.2768	TrainAcc 0.1824	ValidAcc 0.0781	TestAcc 0.0598	BestValid 0.1030
	Epoch 4850:	Loss 2.2677	TrainAcc 0.0929	ValidAcc 0.0471	TestAcc 0.0387	BestValid 0.1030
	Epoch 4900:	Loss 2.2769	TrainAcc 0.1734	ValidAcc 0.0756	TestAcc 0.0579	BestValid 0.1030
	Epoch 4950:	Loss 2.2679	TrainAcc 0.0846	ValidAcc 0.0453	TestAcc 0.0360	BestValid 0.1030
	Epoch 5000:	Loss 2.2885	TrainAcc 0.1785	ValidAcc 0.0761	TestAcc 0.0585	BestValid 0.1030
****** Epoch Time (Excluding Evaluation Cost): 0.154 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 24.013 ms (Max: 25.726, Min: 19.496, Sum: 192.106)
Cluster-Wide Average, Compute: 77.334 ms (Max: 79.908, Min: 75.113, Sum: 618.672)
Cluster-Wide Average, Communication-Layer: 11.476 ms (Max: 12.713, Min: 9.937, Sum: 91.810)
Cluster-Wide Average, Bubble-Imbalance: 4.599 ms (Max: 6.442, Min: 3.496, Sum: 36.793)
Cluster-Wide Average, Communication-Graph: 29.632 ms (Max: 30.839, Min: 28.791, Sum: 237.057)
Cluster-Wide Average, Optimization: 2.270 ms (Max: 2.794, Min: 1.808, Sum: 18.161)
Cluster-Wide Average, Others: 4.510 ms (Max: 10.012, Min: 2.648, Sum: 36.078)
****** Breakdown Sum: 153.835 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.320 GB (Max: 5.229, Min: 4.071, Sum: 34.560)
Cluster-Wide Average, Graph-Level Communication Throughput: 95.586 Gbps (Max: 98.849, Min: 90.614, Sum: 764.688)
Cluster-Wide Average, Layer-Level Communication Throughput: 34.698 Gbps (Max: 45.311, Min: 25.261, Sum: 277.583)
Layer-level communication (cluster-wide, per-epoch): 0.379 GB
Graph-level communication (cluster-wide, per-epoch): 1.952 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 2.336 GB
****** Accuracy Results ******
Highest valid_acc: 0.1030
Target test_acc: 0.0802
Epoch to reach the target acc: 2799
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
