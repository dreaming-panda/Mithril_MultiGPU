Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 3 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INITDONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv8
Initialized node 5 on machine gnerv8

Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.068 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.054 seconds.
        It takes 0.050 seconds.
        It takes 0.055 seconds.
        It takes 0.052 seconds.
        It takes 0.051 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.058 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.051 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.022 seconds.
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.074 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
        It takes 0.028 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 61.415 Gbps (per GPU), 491.316 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 61.113 Gbps (per GPU), 488.902 Gbps (aggregated)
The layer-level communication performance: 61.103 Gbps (per GPU), 488.821 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.850 Gbps (per GPU), 486.803 Gbps (aggregated)
The layer-level communication performance: 60.825 Gbps (per GPU), 486.599 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.587 Gbps (per GPU), 484.698 Gbps (aggregated)
The layer-level communication performance: 60.535 Gbps (per GPU), 484.281 Gbps (aggregated)
The layer-level communication performance: 60.505 Gbps (per GPU), 484.036 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.841 Gbps (per GPU), 1278.727 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.841 Gbps (per GPU), 1278.727 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.838 Gbps (per GPU), 1278.703 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.838 Gbps (per GPU), 1278.702 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.835 Gbps (per GPU), 1278.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.841 Gbps (per GPU), 1278.727 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.844 Gbps (per GPU), 1278.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.841 Gbps (per GPU), 1278.727 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 105.258 Gbps (per GPU), 842.068 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.254 Gbps (per GPU), 842.032 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.244 Gbps (per GPU), 841.955 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.255 Gbps (per GPU), 842.039 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.260 Gbps (per GPU), 842.082 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.226 Gbps (per GPU), 841.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.259 Gbps (per GPU), 842.075 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.187 Gbps (per GPU), 841.498 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.031 Gbps (per GPU), 312.248 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.030 Gbps (per GPU), 312.244 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.030 Gbps (per GPU), 312.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.031 Gbps (per GPU), 312.250 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.032 Gbps (per GPU), 312.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.030 Gbps (per GPU), 312.242 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.028 Gbps (per GPU), 312.221 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.025 Gbps (per GPU), 312.203 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.59ms  0.51ms  0.36ms  1.66  5.55K  0.06M
 chk_1  0.59ms  0.51ms  0.36ms  1.66  5.75K  0.05M
 chk_2  0.59ms  0.51ms  0.37ms  1.63  5.20K  0.07M
 chk_3  0.60ms  0.52ms  0.36ms  1.65  5.57K  0.06M
 chk_4  0.59ms  0.51ms  0.36ms  1.63  5.05K  0.08M
 chk_5  0.61ms  0.53ms  0.39ms  1.55  4.73K  0.11M
 chk_6  0.58ms  0.51ms  0.36ms  1.62  4.98K  0.08M
 chk_7  0.59ms  0.52ms  0.37ms  1.60  5.01K  0.09M
 chk_8  0.59ms  0.51ms  0.36ms  1.66  5.73K  0.05M
 chk_9  0.58ms  0.67ms  0.37ms  1.84  4.54K  0.11M
chk_10  0.59ms  0.51ms  0.36ms  1.63  5.36K  0.07M
chk_11  0.60ms  0.52ms  0.37ms  1.63  5.39K  0.08M
chk_12  0.59ms  0.51ms  0.36ms  1.66  5.77K  0.05M
chk_13  0.59ms  0.51ms  0.36ms  1.63  5.43K  0.06M
chk_14  0.59ms  0.51ms  0.36ms  1.62  5.46K  0.06M
chk_15  0.59ms  0.51ms  0.36ms  1.65  5.88K  0.04M
chk_16  0.60ms  0.51ms  0.37ms  1.62  5.50K  0.06M
chk_17  0.60ms  0.53ms  0.39ms  1.56  4.86K  0.09M
chk_18  0.62ms  0.54ms  0.39ms  1.59  5.39K  0.07M
chk_19  0.60ms  0.52ms  0.37ms  1.61  5.20K  0.07M
chk_20  0.59ms  0.51ms  0.36ms  1.66  5.51K  0.06M
chk_21  0.58ms  0.50ms  0.35ms  1.68  5.81K  0.05M
chk_22  0.59ms  0.51ms  0.36ms  1.62  5.32K  0.07M
chk_23  0.60ms  0.53ms  0.38ms  1.59  5.39K  0.07M
chk_24  0.59ms  0.51ms  0.38ms  1.54  4.62K  0.11M
chk_25  0.59ms  0.52ms  0.37ms  1.60  5.04K  0.08M
chk_26  0.58ms  0.51ms  0.37ms  1.58  4.55K  0.11M
chk_27  0.58ms  0.50ms  0.36ms  1.62  5.30K  0.06M
chk_28  0.58ms  0.51ms  0.36ms  1.62  5.58K  0.06M
chk_29  0.60ms  0.52ms  0.38ms  1.56  4.98K  0.09M
chk_30  0.59ms  0.51ms  0.36ms  1.63  5.50K  0.07M
chk_31  0.59ms  0.51ms  0.36ms  1.62  5.38K  0.07M
   Avg  0.59  0.52  0.37
   Max  0.62  0.67  0.39
   Min  0.58  0.50  0.35
 Ratio  1.06  1.34  1.12
   Var  0.00  0.00  0.00
Profiling takes 0.689 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 85420, Num Local Vertices: 83923
Node 0, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[194, 257)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
+++++++++ Node 2 initializing the weights for op[66, 130)...
+++++++++ Node 0 initializing the weights for op[0, 66)...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 5 initializing the weights for op[130, 194)...
+++++++++ Node 7 initializing the weights for op[194, 257)...
+++++++++ Node 1 initializing the weights for op[0, 66)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 327544
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 8.5440	TrainAcc 0.0181	ValidAcc 0.0078	TestAcc 0.0074	BestValid 0.0078
	Epoch 50:	Loss 3.6885	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 100:	Loss 3.3243	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 150:	Loss 2.8815	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 200:	Loss 2.8553	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 250:	Loss 2.8373	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 300:	Loss 2.8532	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 350:	Loss 2.8263	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 400:	Loss 2.8117	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 450:	Loss 2.8046	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 500:	Loss 2.7982	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 550:	Loss 2.7987	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 600:	Loss 2.7938	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 650:	Loss 2.7892	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 700:	Loss 2.7828	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 750:	Loss 2.7814	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 800:	Loss 2.7763	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 850:	Loss 2.7762	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 900:	Loss 2.7707	TrainAcc 0.1792	ValidAcc 0.0764	TestAcc 0.0586	BestValid 0.0764
	Epoch 950:	Loss 2.7702	TrainAcc 0.1800	ValidAcc 0.0772	TestAcc 0.0588	BestValid 0.0772
	Epoch 1000:	Loss 2.7598	TrainAcc 0.1924	ValidAcc 0.0860	TestAcc 0.0661	BestValid 0.0860
	Epoch 1050:	Loss 2.6961	TrainAcc 0.2171	ValidAcc 0.0935	TestAcc 0.0737	BestValid 0.0935
	Epoch 1100:	Loss 2.6497	TrainAcc 0.2221	ValidAcc 0.0941	TestAcc 0.0748	BestValid 0.0941
	Epoch 1150:	Loss 2.6217	TrainAcc 0.2224	ValidAcc 0.0963	TestAcc 0.0761	BestValid 0.0963
	Epoch 1200:	Loss 2.6125	TrainAcc 0.1064	ValidAcc 0.0556	TestAcc 0.0458	BestValid 0.0963
	Epoch 1250:	Loss 2.5799	TrainAcc 0.1501	ValidAcc 0.2458	TestAcc 0.2294	BestValid 0.2458
	Epoch 1300:	Loss 2.5323	TrainAcc 0.1506	ValidAcc 0.2468	TestAcc 0.2301	BestValid 0.2468
	Epoch 1350:	Loss 2.5129	TrainAcc 0.3164	ValidAcc 0.3181	TestAcc 0.2848	BestValid 0.3181
	Epoch 1400:	Loss 2.4267	TrainAcc 0.3178	ValidAcc 0.3171	TestAcc 0.2846	BestValid 0.3181
	Epoch 1450:	Loss 2.4214	TrainAcc 0.3234	ValidAcc 0.3210	TestAcc 0.2880	BestValid 0.3210
	Epoch 1500:	Loss 2.3845	TrainAcc 0.3183	ValidAcc 0.3181	TestAcc 0.2877	BestValid 0.3210
	Epoch 1550:	Loss 2.3654	TrainAcc 0.2217	ValidAcc 0.0952	TestAcc 0.0740	BestValid 0.3210
	Epoch 1600:	Loss 2.3705	TrainAcc 0.2126	ValidAcc 0.0895	TestAcc 0.0707	BestValid 0.3210
	Epoch 1650:	Loss 2.3523	TrainAcc 0.1795	ValidAcc 0.0764	TestAcc 0.0587	BestValid 0.3210
	Epoch 1700:	Loss 2.3383	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.3210
	Epoch 1750:	Loss 2.3473	TrainAcc 0.1812	ValidAcc 0.0776	TestAcc 0.0592	BestValid 0.3210
	Epoch 1800:	Loss 2.3340	TrainAcc 0.2370	ValidAcc 0.1083	TestAcc 0.0848	BestValid 0.3210
	Epoch 1850:	Loss 2.3425	TrainAcc 0.2338	ValidAcc 0.1048	TestAcc 0.0824	BestValid 0.3210
	Epoch 1900:	Loss 2.3282	TrainAcc 0.1316	ValidAcc 0.0536	TestAcc 0.0418	BestValid 0.3210
	Epoch 1950:	Loss 2.3319	TrainAcc 0.1423	ValidAcc 0.0573	TestAcc 0.0429	BestValid 0.3210
	Epoch 2000:	Loss 2.3157	TrainAcc 0.0332	ValidAcc 0.0142	TestAcc 0.0130	BestValid 0.3210
	Epoch 2050:	Loss 2.3158	TrainAcc 0.0398	ValidAcc 0.0169	TestAcc 0.0151	BestValid 0.3210
	Epoch 2100:	Loss 2.3164	TrainAcc 0.0330	ValidAcc 0.0149	TestAcc 0.0135	BestValid 0.3210
	Epoch 2150:	Loss 2.3124	TrainAcc 0.0330	ValidAcc 0.0143	TestAcc 0.0132	BestValid 0.3210
	Epoch 2200:	Loss 2.3072	TrainAcc 0.1816	ValidAcc 0.0775	TestAcc 0.0596	BestValid 0.3210
	Epoch 2250:	Loss 2.3034	TrainAcc 0.0638	ValidAcc 0.0406	TestAcc 0.0344	BestValid 0.3210
	Epoch 2300:	Loss 2.2946	TrainAcc 0.0328	ValidAcc 0.0145	TestAcc 0.0130	BestValid 0.3210
	Epoch 2350:	Loss 2.3006	TrainAcc 0.0768	ValidAcc 0.0488	TestAcc 0.0403	BestValid 0.3210
	Epoch 2400:	Loss 2.2900	TrainAcc 0.1007	ValidAcc 0.0450	TestAcc 0.0369	BestValid 0.3210
	Epoch 2450:	Loss 2.2859	TrainAcc 0.2205	ValidAcc 0.0983	TestAcc 0.0768	BestValid 0.3210
	Epoch 2500:	Loss 2.3033	TrainAcc 0.1132	ValidAcc 0.0609	TestAcc 0.0500	BestValid 0.3210
	Epoch 2550:	Loss 2.2923	TrainAcc 0.1915	ValidAcc 0.0839	TestAcc 0.0645	BestValid 0.3210
	Epoch 2600:	Loss 2.2955	TrainAcc 0.2175	ValidAcc 0.0977	TestAcc 0.0762	BestValid 0.3210
	Epoch 2650:	Loss 2.2913	TrainAcc 0.2189	ValidAcc 0.1017	TestAcc 0.0800	BestValid 0.3210
	Epoch 2700:	Loss 2.2886	TrainAcc 0.0331	ValidAcc 0.0143	TestAcc 0.0131	BestValid 0.3210
	Epoch 2750:	Loss 2.2848	TrainAcc 0.1798	ValidAcc 0.0767	TestAcc 0.0587	BestValid 0.3210
	Epoch 2800:	Loss 2.2871	TrainAcc 0.0611	ValidAcc 0.0271	TestAcc 0.0241	BestValid 0.3210
	Epoch 2850:	Loss 2.2745	TrainAcc 0.0982	ValidAcc 0.0493	TestAcc 0.0429	BestValid 0.3210
	Epoch 2900:	Loss 2.2820	TrainAcc 0.1005	ValidAcc 0.0491	TestAcc 0.0384	BestValid 0.3210
	Epoch 2950:	Loss 2.2854	TrainAcc 0.0397	ValidAcc 0.0169	TestAcc 0.0151	BestValid 0.3210
	Epoch 3000:	Loss 2.2849	TrainAcc 0.1137	ValidAcc 0.0429	TestAcc 0.0353	BestValid 0.3210
	Epoch 3050:	Loss 2.2786	TrainAcc 0.0565	ValidAcc 0.0241	TestAcc 0.0209	BestValid 0.3210
	Epoch 3100:	Loss 2.2702	TrainAcc 0.2138	ValidAcc 0.0928	TestAcc 0.0725	BestValid 0.3210
	Epoch 3150:	Loss 2.2699	TrainAcc 0.2173	ValidAcc 0.0929	TestAcc 0.0728	BestValid 0.3210
	Epoch 3200:	Loss 2.2896	TrainAcc 0.2374	ValidAcc 0.1056	TestAcc 0.0832	BestValid 0.3210
	Epoch 3250:	Loss 2.2641	TrainAcc 0.1899	ValidAcc 0.0756	TestAcc 0.0590	BestValid 0.3210
	Epoch 3300:	Loss 2.2549	TrainAcc 0.2203	ValidAcc 0.0950	TestAcc 0.0753	BestValid 0.3210
	Epoch 3350:	Loss 2.2553	TrainAcc 0.0396	ValidAcc 0.0168	TestAcc 0.0151	BestValid 0.3210
	Epoch 3400:	Loss 2.2486	TrainAcc 0.2188	ValidAcc 0.0948	TestAcc 0.0749	BestValid 0.3210
	Epoch 3450:	Loss 2.2449	TrainAcc 0.2225	ValidAcc 0.0994	TestAcc 0.0780	BestValid 0.3210
	Epoch 3500:	Loss 2.2510	TrainAcc 0.2195	ValidAcc 0.0930	TestAcc 0.0731	BestValid 0.3210
	Epoch 3550:	Loss 2.2463	TrainAcc 0.2330	ValidAcc 0.1027	TestAcc 0.0817	BestValid 0.3210
	Epoch 3600:	Loss 2.2424	TrainAcc 0.2275	ValidAcc 0.1002	TestAcc 0.0792	BestValid 0.3210
	Epoch 3650:	Loss 2.2429	TrainAcc 0.2323	ValidAcc 0.1015	TestAcc 0.0804	BestValid 0.3210
	Epoch 3700:	Loss 2.2416	TrainAcc 0.2099	ValidAcc 0.0882	TestAcc 0.0690	BestValid 0.3210
	Epoch 3750:	Loss 2.2533	TrainAcc 0.2314	ValidAcc 0.1002	TestAcc 0.0793	BestValid 0.3210
	Epoch 3800:	Loss 2.2394	TrainAcc 0.1802	ValidAcc 0.0774	TestAcc 0.0593	BestValid 0.3210
	Epoch 3850:	Loss 2.2269	TrainAcc 0.2220	ValidAcc 0.0944	TestAcc 0.0745	BestValid 0.3210
	Epoch 3900:	Loss 2.2328	TrainAcc 0.1377	ValidAcc 0.0579	TestAcc 0.0455	BestValid 0.3210
	Epoch 3950:	Loss 2.2328	TrainAcc 0.0864	ValidAcc 0.0277	TestAcc 0.0244	BestValid 0.3210
	Epoch 4000:	Loss 2.2641	TrainAcc 0.2041	ValidAcc 0.0937	TestAcc 0.0728	BestValid 0.3210
	Epoch 4050:	Loss 2.2971	TrainAcc 0.2428	ValidAcc 0.1109	TestAcc 0.0881	BestValid 0.3210
	Epoch 4100:	Loss 2.2365	TrainAcc 0.1989	ValidAcc 0.0858	TestAcc 0.0667	BestValid 0.3210
	Epoch 4150:	Loss 2.2745	TrainAcc 0.0784	ValidAcc 0.0257	TestAcc 0.0222	BestValid 0.3210
	Epoch 4200:	Loss 2.2348	TrainAcc 0.1951	ValidAcc 0.0860	TestAcc 0.0658	BestValid 0.3210
	Epoch 4250:	Loss 2.2287	TrainAcc 0.1982	ValidAcc 0.0884	TestAcc 0.0692	BestValid 0.3210
	Epoch 4300:	Loss 2.2435	TrainAcc 0.2079	ValidAcc 0.0945	TestAcc 0.0729	BestValid 0.3210
	Epoch 4350:	Loss 2.2702	TrainAcc 0.1983	ValidAcc 0.0870	TestAcc 0.0677	BestValid 0.3210
	Epoch 4400:	Loss 2.2620	TrainAcc 0.2031	ValidAcc 0.0914	TestAcc 0.0706	BestValid 0.3210
	Epoch 4450:	Loss 2.2312	TrainAcc 0.2101	ValidAcc 0.0956	TestAcc 0.0744	BestValid 0.3210
	Epoch 4500:	Loss 2.2442	TrainAcc 0.0337	ValidAcc 0.0146	TestAcc 0.0139	BestValid 0.3210
	Epoch 4550:	Loss 2.2286	TrainAcc 0.1808	ValidAcc 0.0778	TestAcc 0.0606	BestValid 0.3210
	Epoch 4600:	Loss 2.2216	TrainAcc 0.2152	ValidAcc 0.0935	TestAcc 0.0733	BestValid 0.3210
	Epoch 4650:	Loss 2.2374	TrainAcc 0.2037	ValidAcc 0.0938	TestAcc 0.0722	BestValid 0.3210
	Epoch 4700:	Loss 2.2689	TrainAcc 0.1754	ValidAcc 0.0731	TestAcc 0.0551	BestValid 0.3210
	Epoch 4750:	Loss 2.2409	TrainAcc 0.2054	ValidAcc 0.0913	TestAcc 0.0714	BestValid 0.3210
	Epoch 4800:	Loss 2.2573	TrainAcc 0.1861	ValidAcc 0.0795	TestAcc 0.0607	BestValid 0.3210
	Epoch 4850:	Loss 2.2746	TrainAcc 0.1520	ValidAcc 0.0663	TestAcc 0.0516	BestValid 0.3210
	Epoch 4900:	Loss 2.2459	TrainAcc 0.1559	ValidAcc 0.0644	TestAcc 0.0481	BestValid 0.3210
	Epoch 4950:	Loss 2.2261	TrainAcc 0.2208	ValidAcc 0.0989	TestAcc 0.0779	BestValid 0.3210
	Epoch 5000:	Loss 2.2338	TrainAcc 0.1911	ValidAcc 0.0827	TestAcc 0.0635	BestValid 0.3210
****** Epoch Time (Excluding Evaluation Cost): 0.155 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 24.198 ms (Max: 26.296, Min: 19.024, Sum: 193.586)
Cluster-Wide Average, Compute: 77.896 ms (Max: 82.183, Min: 74.817, Sum: 623.170)
Cluster-Wide Average, Communication-Layer: 11.522 ms (Max: 12.827, Min: 9.865, Sum: 92.172)
Cluster-Wide Average, Bubble-Imbalance: 4.983 ms (Max: 7.942, Min: 1.460, Sum: 39.863)
Cluster-Wide Average, Communication-Graph: 29.889 ms (Max: 30.866, Min: 28.946, Sum: 239.113)
Cluster-Wide Average, Optimization: 2.305 ms (Max: 2.703, Min: 2.075, Sum: 18.438)
Cluster-Wide Average, Others: 4.520 ms (Max: 10.039, Min: 2.656, Sum: 36.163)
****** Breakdown Sum: 155.313 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.320 GB (Max: 5.229, Min: 4.071, Sum: 34.560)
Cluster-Wide Average, Graph-Level Communication Throughput: 95.134 Gbps (Max: 98.470, Min: 90.722, Sum: 761.074)
Cluster-Wide Average, Layer-Level Communication Throughput: 34.561 Gbps (Max: 44.845, Min: 25.141, Sum: 276.486)
Layer-level communication (cluster-wide, per-epoch): 0.379 GB
Graph-level communication (cluster-wide, per-epoch): 1.952 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 2.336 GB
****** Accuracy Results ******
Highest valid_acc: 0.3210
Target test_acc: 0.2880
Epoch to reach the target acc: 1449
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
