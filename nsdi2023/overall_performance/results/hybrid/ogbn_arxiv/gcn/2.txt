Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 7 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8

Initialized node 6 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv7
Initialized node 2 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.048 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.046 seconds.
        It takes 0.051 seconds.
        It takes 0.050 seconds.
        It takes 0.054 seconds.
        It takes 0.053 seconds.
        It takes 0.048 seconds.
        It takes 0.056 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.065 seconds.
        It takes 0.057 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.067 seconds.
        It takes 0.069 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.069 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 61.053 Gbps (per GPU), 488.422 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.753 Gbps (per GPU), 486.027 Gbps (aggregated)
The layer-level communication performance: 60.743 Gbps (per GPU), 485.940 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.502 Gbps (per GPU), 484.019 Gbps (aggregated)
The layer-level communication performance: 60.473 Gbps (per GPU), 483.786 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.248 Gbps (per GPU), 481.985 Gbps (aggregated)
The layer-level communication performance: 60.197 Gbps (per GPU), 481.573 Gbps (aggregated)
The layer-level communication performance: 60.165 Gbps (per GPU), 481.320 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 161.272 Gbps (per GPU), 1290.180 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.245 Gbps (per GPU), 1289.960 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.264 Gbps (per GPU), 1290.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.239 Gbps (per GPU), 1289.910 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.273 Gbps (per GPU), 1290.183 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.270 Gbps (per GPU), 1290.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.273 Gbps (per GPU), 1290.183 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.257 Gbps (per GPU), 1290.059 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 105.289 Gbps (per GPU), 842.314 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.290 Gbps (per GPU), 842.321 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.290 Gbps (per GPU), 842.321 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.287 Gbps (per GPU), 842.299 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.288 Gbps (per GPU), 842.300 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.287 Gbps (per GPU), 842.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.284 Gbps (per GPU), 842.272 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.275 Gbps (per GPU), 842.202 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.593 Gbps (per GPU), 300.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.595 Gbps (per GPU), 300.759 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.590 Gbps (per GPU), 300.723 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.585 Gbps (per GPU), 300.681 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.592 Gbps (per GPU), 300.739 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.590 Gbps (per GPU), 300.721 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.589 Gbps (per GPU), 300.712 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.589 Gbps (per GPU), 300.709 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.30ms  0.33ms  0.23ms  1.48  5.55K  0.06M
 chk_1  0.30ms  0.33ms  0.23ms  1.47  5.75K  0.05M
 chk_2  0.31ms  0.34ms  0.24ms  1.45  5.20K  0.07M
 chk_3  0.31ms  0.34ms  0.24ms  1.45  5.57K  0.06M
 chk_4  0.31ms  0.34ms  0.23ms  1.45  5.05K  0.08M
 chk_5  0.33ms  0.36ms  0.26ms  1.39  4.73K  0.11M
 chk_6  0.31ms  0.34ms  0.23ms  1.45  4.98K  0.08M
 chk_7  0.31ms  0.44ms  0.24ms  1.85  5.01K  0.09M
 chk_8  0.30ms  0.33ms  0.23ms  1.45  5.73K  0.05M
 chk_9  0.31ms  0.33ms  0.24ms  1.40  4.54K  0.11M
chk_10  0.31ms  0.33ms  0.23ms  1.43  5.36K  0.07M
chk_11  0.32ms  0.34ms  0.24ms  1.41  5.39K  0.08M
chk_12  0.30ms  0.33ms  0.22ms  1.45  5.77K  0.05M
chk_13  0.31ms  0.33ms  0.24ms  1.42  5.43K  0.06M
chk_14  0.38ms  0.33ms  0.23ms  1.68  5.46K  0.06M
chk_15  0.30ms  0.32ms  0.22ms  1.46  5.88K  0.04M
chk_16  0.31ms  0.33ms  0.23ms  1.42  5.50K  0.06M
chk_17  0.33ms  0.35ms  0.26ms  1.37  4.86K  0.09M
chk_18  0.33ms  0.35ms  0.26ms  1.37  5.39K  0.07M
chk_19  0.31ms  0.34ms  0.24ms  1.41  5.20K  0.07M
chk_20  0.30ms  0.33ms  0.22ms  1.45  5.51K  0.06M
chk_21  0.29ms  0.32ms  0.22ms  1.47  5.81K  0.05M
chk_22  0.31ms  0.33ms  0.23ms  1.44  5.32K  0.07M
chk_23  0.32ms  0.35ms  0.25ms  1.39  5.39K  0.07M
chk_24  0.32ms  0.34ms  0.25ms  1.37  4.62K  0.11M
chk_25  0.32ms  0.34ms  0.24ms  1.41  5.04K  0.08M
chk_26  0.31ms  0.33ms  0.24ms  1.38  4.55K  0.11M
chk_27  0.30ms  0.33ms  0.23ms  1.44  5.30K  0.06M
chk_28  0.31ms  0.33ms  0.23ms  1.43  5.58K  0.06M
chk_29  0.32ms  0.34ms  0.25ms  1.38  4.98K  0.09M
chk_30  0.31ms  0.33ms  0.23ms  1.44  5.50K  0.07M
chk_31  0.31ms  0.33ms  0.23ms  1.44  5.38K  0.07M
   Avg  0.31  0.34  0.24
   Max  0.38  0.44  0.26
   Min  0.29  0.32  0.22
 Ratio  1.30  1.39  1.21
   Var  0.00  0.00  0.00
Profiling takes 0.444 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 41)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 81)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [41, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 121)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [81, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [121, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [121, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 41)...
+++++++++ Node 3 initializing the weights for op[41, 81)...
+++++++++ Node 0 initializing the weights for op[0, 41)...
+++++++++ Node 2 initializing the weights for op[41, 81)...
+++++++++ Node 5 initializing the weights for op[81, 121)...
+++++++++ Node 6 initializing the weights for op[121, 160)...
+++++++++ Node 4 initializing the weights for op[81, 121)...
+++++++++ Node 7 initializing the weights for op[121, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 327544
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.6889	TrainAcc 0.0060	ValidAcc 0.0029	TestAcc 0.0013	BestValid 0.0029
	Epoch 50:	Loss 3.1218	TrainAcc 0.1790	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 100:	Loss 2.8937	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 150:	Loss 2.7343	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 200:	Loss 2.6010	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 250:	Loss 2.5205	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 300:	Loss 2.5139	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 350:	Loss 2.4826	TrainAcc 0.1790	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 400:	Loss 2.4487	TrainAcc 0.2802	ValidAcc 0.3157	TestAcc 0.3029	BestValid 0.3157
	Epoch 450:	Loss 2.3687	TrainAcc 0.2812	ValidAcc 0.2320	TestAcc 0.2647	BestValid 0.3157
	Epoch 500:	Loss 2.3230	TrainAcc 0.3446	ValidAcc 0.3415	TestAcc 0.3166	BestValid 0.3415
	Epoch 550:	Loss 2.3063	TrainAcc 0.3479	ValidAcc 0.3500	TestAcc 0.3253	BestValid 0.3500
	Epoch 600:	Loss 2.2822	TrainAcc 0.3453	ValidAcc 0.3501	TestAcc 0.3254	BestValid 0.3501
	Epoch 650:	Loss 2.2704	TrainAcc 0.3442	ValidAcc 0.3464	TestAcc 0.3196	BestValid 0.3501
	Epoch 700:	Loss 2.2530	TrainAcc 0.3468	ValidAcc 0.3519	TestAcc 0.3265	BestValid 0.3519
	Epoch 750:	Loss 2.2250	TrainAcc 0.3711	ValidAcc 0.3718	TestAcc 0.3617	BestValid 0.3718
	Epoch 800:	Loss 2.1588	TrainAcc 0.3530	ValidAcc 0.2994	TestAcc 0.2802	BestValid 0.3718
	Epoch 850:	Loss 2.1240	TrainAcc 0.3474	ValidAcc 0.2432	TestAcc 0.2226	BestValid 0.3718
	Epoch 900:	Loss 2.1148	TrainAcc 0.3902	ValidAcc 0.3937	TestAcc 0.3998	BestValid 0.3937
	Epoch 950:	Loss 2.0751	TrainAcc 0.3398	ValidAcc 0.2430	TestAcc 0.2272	BestValid 0.3937
	Epoch 1000:	Loss 2.0517	TrainAcc 0.3491	ValidAcc 0.2500	TestAcc 0.2365	BestValid 0.3937
	Epoch 1050:	Loss 2.0353	TrainAcc 0.2553	ValidAcc 0.1136	TestAcc 0.0925	BestValid 0.3937
	Epoch 1100:	Loss 2.0219	TrainAcc 0.2898	ValidAcc 0.1503	TestAcc 0.1200	BestValid 0.3937
	Epoch 1150:	Loss 1.9693	TrainAcc 0.4204	ValidAcc 0.4445	TestAcc 0.4339	BestValid 0.4445
	Epoch 1200:	Loss 1.9086	TrainAcc 0.4364	ValidAcc 0.4210	TestAcc 0.3924	BestValid 0.4445
	Epoch 1250:	Loss 1.8893	TrainAcc 0.3810	ValidAcc 0.2588	TestAcc 0.2331	BestValid 0.4445
	Epoch 1300:	Loss 1.8809	TrainAcc 0.3165	ValidAcc 0.1476	TestAcc 0.1152	BestValid 0.4445
	Epoch 1350:	Loss 1.8633	TrainAcc 0.4525	ValidAcc 0.4165	TestAcc 0.3948	BestValid 0.4445
	Epoch 1400:	Loss 1.8358	TrainAcc 0.4323	ValidAcc 0.3900	TestAcc 0.3655	BestValid 0.4445
	Epoch 1450:	Loss 1.8360	TrainAcc 0.3586	ValidAcc 0.2083	TestAcc 0.2029	BestValid 0.4445
	Epoch 1500:	Loss 1.8135	TrainAcc 0.4638	ValidAcc 0.4098	TestAcc 0.3779	BestValid 0.4445
	Epoch 1550:	Loss 1.7889	TrainAcc 0.3152	ValidAcc 0.1530	TestAcc 0.1163	BestValid 0.4445
	Epoch 1600:	Loss 1.7901	TrainAcc 0.4071	ValidAcc 0.3630	TestAcc 0.3344	BestValid 0.4445
	Epoch 1650:	Loss 1.7773	TrainAcc 0.3445	ValidAcc 0.1554	TestAcc 0.1272	BestValid 0.4445
	Epoch 1700:	Loss 1.7709	TrainAcc 0.3662	ValidAcc 0.1733	TestAcc 0.1392	BestValid 0.4445
	Epoch 1750:	Loss 1.7653	TrainAcc 0.2982	ValidAcc 0.1396	TestAcc 0.1073	BestValid 0.4445
	Epoch 1800:	Loss 1.7515	TrainAcc 0.4268	ValidAcc 0.3757	TestAcc 0.3407	BestValid 0.4445
	Epoch 1850:	Loss 1.7500	TrainAcc 0.3064	ValidAcc 0.1351	TestAcc 0.1105	BestValid 0.4445
	Epoch 1900:	Loss 1.7421	TrainAcc 0.5038	ValidAcc 0.5098	TestAcc 0.5124	BestValid 0.5098
	Epoch 1950:	Loss 1.7399	TrainAcc 0.3155	ValidAcc 0.1402	TestAcc 0.1121	BestValid 0.5098
	Epoch 2000:	Loss 1.7337	TrainAcc 0.3145	ValidAcc 0.1501	TestAcc 0.1146	BestValid 0.5098
	Epoch 2050:	Loss 1.7230	TrainAcc 0.4221	ValidAcc 0.2912	TestAcc 0.3113	BestValid 0.5098
	Epoch 2100:	Loss 1.7172	TrainAcc 0.4022	ValidAcc 0.3581	TestAcc 0.3204	BestValid 0.5098
	Epoch 2150:	Loss 1.7153	TrainAcc 0.3476	ValidAcc 0.1748	TestAcc 0.1472	BestValid 0.5098
	Epoch 2200:	Loss 1.7147	TrainAcc 0.4390	ValidAcc 0.3406	TestAcc 0.3305	BestValid 0.5098
	Epoch 2250:	Loss 1.7045	TrainAcc 0.3423	ValidAcc 0.1577	TestAcc 0.1293	BestValid 0.5098
	Epoch 2300:	Loss 1.7011	TrainAcc 0.5244	ValidAcc 0.5112	TestAcc 0.4923	BestValid 0.5112
	Epoch 2350:	Loss 1.7086	TrainAcc 0.4616	ValidAcc 0.3942	TestAcc 0.3545	BestValid 0.5112
	Epoch 2400:	Loss 1.6947	TrainAcc 0.3286	ValidAcc 0.1462	TestAcc 0.1153	BestValid 0.5112
	Epoch 2450:	Loss 1.6931	TrainAcc 0.3865	ValidAcc 0.1887	TestAcc 0.1582	BestValid 0.5112
	Epoch 2500:	Loss 1.6949	TrainAcc 0.5079	ValidAcc 0.4675	TestAcc 0.4465	BestValid 0.5112
	Epoch 2550:	Loss 1.6936	TrainAcc 0.3823	ValidAcc 0.1942	TestAcc 0.1540	BestValid 0.5112
	Epoch 2600:	Loss 1.6925	TrainAcc 0.4835	ValidAcc 0.4324	TestAcc 0.4052	BestValid 0.5112
	Epoch 2650:	Loss 1.6768	TrainAcc 0.4112	ValidAcc 0.2438	TestAcc 0.2138	BestValid 0.5112
	Epoch 2700:	Loss 1.6878	TrainAcc 0.3587	ValidAcc 0.1684	TestAcc 0.1370	BestValid 0.5112
	Epoch 2750:	Loss 1.6771	TrainAcc 0.3577	ValidAcc 0.1853	TestAcc 0.1568	BestValid 0.5112
	Epoch 2800:	Loss 1.6717	TrainAcc 0.3778	ValidAcc 0.1809	TestAcc 0.1474	BestValid 0.5112
	Epoch 2850:	Loss 1.6703	TrainAcc 0.3709	ValidAcc 0.1793	TestAcc 0.1412	BestValid 0.5112
	Epoch 2900:	Loss 1.6681	TrainAcc 0.5258	ValidAcc 0.4982	TestAcc 0.4755	BestValid 0.5112
	Epoch 2950:	Loss 1.6682	TrainAcc 0.5159	ValidAcc 0.4579	TestAcc 0.4415	BestValid 0.5112
	Epoch 3000:	Loss 1.6585	TrainAcc 0.3548	ValidAcc 0.1758	TestAcc 0.1376	BestValid 0.5112
	Epoch 3050:	Loss 1.6622	TrainAcc 0.4668	ValidAcc 0.3983	TestAcc 0.3551	BestValid 0.5112
	Epoch 3100:	Loss 1.6571	TrainAcc 0.5081	ValidAcc 0.4346	TestAcc 0.3913	BestValid 0.5112
	Epoch 3150:	Loss 1.6608	TrainAcc 0.4692	ValidAcc 0.4075	TestAcc 0.3734	BestValid 0.5112
	Epoch 3200:	Loss 1.6622	TrainAcc 0.4277	ValidAcc 0.2866	TestAcc 0.2725	BestValid 0.5112
	Epoch 3250:	Loss 1.6571	TrainAcc 0.4765	ValidAcc 0.4190	TestAcc 0.3941	BestValid 0.5112
	Epoch 3300:	Loss 1.6539	TrainAcc 0.3589	ValidAcc 0.1636	TestAcc 0.1318	BestValid 0.5112
	Epoch 3350:	Loss 1.6536	TrainAcc 0.5258	ValidAcc 0.4887	TestAcc 0.4851	BestValid 0.5112
	Epoch 3400:	Loss 1.6468	TrainAcc 0.5354	ValidAcc 0.4994	TestAcc 0.4714	BestValid 0.5112
	Epoch 3450:	Loss 1.6575	TrainAcc 0.5013	ValidAcc 0.4605	TestAcc 0.4687	BestValid 0.5112
	Epoch 3500:	Loss 1.6520	TrainAcc 0.5105	ValidAcc 0.4421	TestAcc 0.4031	BestValid 0.5112
	Epoch 3550:	Loss 1.6434	TrainAcc 0.4668	ValidAcc 0.3845	TestAcc 0.4061	BestValid 0.5112
	Epoch 3600:	Loss 1.6455	TrainAcc 0.4463	ValidAcc 0.3481	TestAcc 0.3674	BestValid 0.5112
	Epoch 3650:	Loss 1.6534	TrainAcc 0.4249	ValidAcc 0.3009	TestAcc 0.2929	BestValid 0.5112
	Epoch 3700:	Loss 1.6434	TrainAcc 0.4232	ValidAcc 0.2774	TestAcc 0.2772	BestValid 0.5112
	Epoch 3750:	Loss 1.6483	TrainAcc 0.3641	ValidAcc 0.1773	TestAcc 0.1394	BestValid 0.5112
	Epoch 3800:	Loss 1.6423	TrainAcc 0.4937	ValidAcc 0.4537	TestAcc 0.4363	BestValid 0.5112
	Epoch 3850:	Loss 1.6444	TrainAcc 0.3378	ValidAcc 0.1649	TestAcc 0.1360	BestValid 0.5112
	Epoch 3900:	Loss 1.6404	TrainAcc 0.4815	ValidAcc 0.4099	TestAcc 0.3679	BestValid 0.5112
	Epoch 3950:	Loss 1.6355	TrainAcc 0.3356	ValidAcc 0.1578	TestAcc 0.1227	BestValid 0.5112
	Epoch 4000:	Loss 1.6378	TrainAcc 0.4990	ValidAcc 0.4509	TestAcc 0.4380	BestValid 0.5112
	Epoch 4050:	Loss 1.6381	TrainAcc 0.3693	ValidAcc 0.1858	TestAcc 0.1525	BestValid 0.5112
	Epoch 4100:	Loss 1.6390	TrainAcc 0.5063	ValidAcc 0.4899	TestAcc 0.4604	BestValid 0.5112
	Epoch 4150:	Loss 1.6333	TrainAcc 0.4033	ValidAcc 0.2038	TestAcc 0.1625	BestValid 0.5112
	Epoch 4200:	Loss 1.6316	TrainAcc 0.4619	ValidAcc 0.3747	TestAcc 0.4074	BestValid 0.5112
	Epoch 4250:	Loss 1.6261	TrainAcc 0.4325	ValidAcc 0.2478	TestAcc 0.2350	BestValid 0.5112
	Epoch 4300:	Loss 1.6388	TrainAcc 0.4546	ValidAcc 0.3890	TestAcc 0.3478	BestValid 0.5112
	Epoch 4350:	Loss 1.6389	TrainAcc 0.3512	ValidAcc 0.1641	TestAcc 0.1347	BestValid 0.5112
	Epoch 4400:	Loss 1.6255	TrainAcc 0.3927	ValidAcc 0.2012	TestAcc 0.1607	BestValid 0.5112
	Epoch 4450:	Loss 1.6222	TrainAcc 0.3600	ValidAcc 0.1681	TestAcc 0.1338	BestValid 0.5112
	Epoch 4500:	Loss 1.6236	TrainAcc 0.5125	ValidAcc 0.4420	TestAcc 0.4114	BestValid 0.5112
	Epoch 4550:	Loss 1.6337	TrainAcc 0.4320	ValidAcc 0.3753	TestAcc 0.3342	BestValid 0.5112
	Epoch 4600:	Loss 1.6282	TrainAcc 0.3270	ValidAcc 0.1487	TestAcc 0.1215	BestValid 0.5112
	Epoch 4650:	Loss 1.6342	TrainAcc 0.3375	ValidAcc 0.2572	TestAcc 0.2828	BestValid 0.5112
	Epoch 4700:	Loss 1.6321	TrainAcc 0.4960	ValidAcc 0.4482	TestAcc 0.4302	BestValid 0.5112
	Epoch 4750:	Loss 1.6306	TrainAcc 0.4140	ValidAcc 0.3282	TestAcc 0.3183	BestValid 0.5112
	Epoch 4800:	Loss 1.6282	TrainAcc 0.5180	ValidAcc 0.4560	TestAcc 0.4211	BestValid 0.5112
	Epoch 4850:	Loss 1.6224	TrainAcc 0.4857	ValidAcc 0.4241	TestAcc 0.3984	BestValid 0.5112
	Epoch 4900:	Loss 1.6304	TrainAcc 0.4878	ValidAcc 0.4209	TestAcc 0.3853	BestValid 0.5112
	Epoch 4950:	Loss 1.6190	TrainAcc 0.3664	ValidAcc 0.1738	TestAcc 0.1376	BestValid 0.5112
	Epoch 5000:	Loss 1.6176	TrainAcc 0.5671	ValidAcc 0.5720	TestAcc 0.5703	BestValid 0.5720
****** Epoch Time (Excluding Evaluation Cost): 0.116 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 19.041 ms (Max: 20.874, Min: 14.065, Sum: 152.330)
Cluster-Wide Average, Compute: 47.833 ms (Max: 51.790, Min: 45.901, Sum: 382.666)
Cluster-Wide Average, Communication-Layer: 10.042 ms (Max: 12.435, Min: 7.289, Sum: 80.333)
Cluster-Wide Average, Bubble-Imbalance: 4.489 ms (Max: 5.485, Min: 2.924, Sum: 35.915)
Cluster-Wide Average, Communication-Graph: 28.982 ms (Max: 29.916, Min: 28.580, Sum: 231.858)
Cluster-Wide Average, Optimization: 1.542 ms (Max: 1.664, Min: 1.321, Sum: 12.333)
Cluster-Wide Average, Others: 4.359 ms (Max: 9.810, Min: 2.532, Sum: 34.876)
****** Breakdown Sum: 116.289 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.364 GB (Max: 5.024, Min: 4.172, Sum: 34.913)
Cluster-Wide Average, Graph-Level Communication Throughput: 98.034 Gbps (Max: 100.162, Min: 94.820, Sum: 784.271)
Cluster-Wide Average, Layer-Level Communication Throughput: 40.494 Gbps (Max: 57.912, Min: 25.088, Sum: 323.952)
Layer-level communication (cluster-wide, per-epoch): 0.379 GB
Graph-level communication (cluster-wide, per-epoch): 1.952 GB
Weight-sync communication (cluster-wide, per-epoch): 0.002 GB
Total communication (cluster-wide, per-epoch): 2.333 GB
****** Accuracy Results ******
Highest valid_acc: 0.5720
Target test_acc: 0.5703
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
