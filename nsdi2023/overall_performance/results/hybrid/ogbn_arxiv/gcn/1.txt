Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.048 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.051 seconds.
        It takes 0.050 seconds.
        It takes 0.056 seconds.
        It takes 0.051 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
        It takes 0.054 seconds.
        It takes 0.054 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.050 seconds.
Building the Label Vector...
        It takes 0.021 seconds.
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
        It takes 0.058 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.070 seconds.
Building the Label Vector...
        It takes 0.070 seconds.
Building the Label Vector...
        It takes 0.071 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.727 Gbps (per GPU), 485.817 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.425 Gbps (per GPU), 483.402 Gbps (aggregated)
The layer-level communication performance: 60.430 Gbps (per GPU), 483.439 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.169 Gbps (per GPU), 481.353 Gbps (aggregated)
The layer-level communication performance: 60.142 Gbps (per GPU), 481.132 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.910 Gbps (per GPU), 479.280 Gbps (aggregated)
The layer-level communication performance: 59.855 Gbps (per GPU), 478.839 Gbps (aggregated)
The layer-level communication performance: 59.824 Gbps (per GPU), 478.593 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.668 Gbps (per GPU), 1269.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.668 Gbps (per GPU), 1269.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.656 Gbps (per GPU), 1269.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.650 Gbps (per GPU), 1269.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.671 Gbps (per GPU), 1269.366 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.674 Gbps (per GPU), 1269.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.656 Gbps (per GPU), 1269.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.650 Gbps (per GPU), 1269.201 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.319 Gbps (per GPU), 834.549 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.310 Gbps (per GPU), 834.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.318 Gbps (per GPU), 834.542 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.314 Gbps (per GPU), 834.514 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.319 Gbps (per GPU), 834.556 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.315 Gbps (per GPU), 834.521 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.318 Gbps (per GPU), 834.542 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.314 Gbps (per GPU), 834.514 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.814 Gbps (per GPU), 294.512 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.816 Gbps (per GPU), 294.526 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.809 Gbps (per GPU), 294.473 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.809 Gbps (per GPU), 294.475 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.814 Gbps (per GPU), 294.508 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.808 Gbps (per GPU), 294.466 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.809 Gbps (per GPU), 294.468 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.808 Gbps (per GPU), 294.461 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.30ms  0.33ms  0.23ms  1.45  5.55K  0.06M
 chk_1  0.30ms  0.33ms  0.23ms  1.43  5.75K  0.05M
 chk_2  0.31ms  0.34ms  0.24ms  1.42  5.20K  0.07M
 chk_3  0.31ms  0.34ms  0.24ms  1.42  5.57K  0.06M
 chk_4  0.30ms  0.33ms  0.24ms  1.42  5.05K  0.08M
 chk_5  0.33ms  0.36ms  0.26ms  1.37  4.73K  0.11M
 chk_6  0.30ms  0.33ms  0.23ms  1.41  4.98K  0.08M
 chk_7  0.31ms  0.34ms  0.24ms  1.40  5.01K  0.09M
 chk_8  0.30ms  0.33ms  0.23ms  1.44  5.73K  0.05M
 chk_9  0.31ms  0.33ms  0.24ms  1.39  4.54K  0.11M
chk_10  0.30ms  0.33ms  0.40ms  1.31  5.36K  0.07M
chk_11  0.31ms  0.34ms  0.24ms  1.41  5.39K  0.08M
chk_12  0.30ms  0.33ms  0.23ms  1.45  5.77K  0.05M
chk_13  0.33ms  0.34ms  0.24ms  1.42  5.43K  0.06M
chk_14  0.30ms  0.33ms  0.23ms  1.44  5.46K  0.06M
chk_15  0.29ms  0.33ms  0.22ms  1.46  5.88K  0.04M
chk_16  0.31ms  0.34ms  0.24ms  1.42  5.50K  0.06M
chk_17  0.32ms  0.35ms  0.26ms  1.37  4.86K  0.09M
chk_18  0.33ms  0.36ms  0.26ms  1.38  5.39K  0.07M
chk_19  0.31ms  0.34ms  0.24ms  1.41  5.20K  0.07M
chk_20  0.30ms  0.33ms  0.23ms  1.46  5.51K  0.06M
chk_21  0.29ms  0.32ms  0.22ms  1.48  5.81K  0.05M
chk_22  0.30ms  0.33ms  0.23ms  1.43  5.32K  0.07M
chk_23  0.32ms  0.35ms  0.25ms  1.40  5.39K  0.07M
chk_24  0.31ms  0.34ms  0.25ms  1.37  4.62K  0.11M
chk_25  0.31ms  0.34ms  0.24ms  1.40  5.04K  0.08M
chk_26  0.31ms  0.33ms  0.24ms  1.39  4.55K  0.11M
chk_27  0.30ms  0.33ms  0.23ms  1.43  5.30K  0.06M
chk_28  0.30ms  0.33ms  0.23ms  1.43  5.58K  0.06M
chk_29  0.32ms  0.35ms  0.25ms  1.37  4.98K  0.09M
chk_30  0.30ms  0.33ms  0.23ms  1.45  5.50K  0.07M
chk_31  0.30ms  0.33ms  0.23ms  1.43  5.38K  0.07M
   Avg  0.31  0.34  0.24
   Max  0.33  0.36  0.40
   Min  0.29  0.32  0.22
 Ratio  1.14  1.12  1.85
   Var  0.00  0.00  0.00
Profiling takes 0.445 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 41)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 81)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [41, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 121)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [81, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [121, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [121, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 41)...
+++++++++ Node 1 initializing the weights for op[0, 41)...
+++++++++ Node 3 initializing the weights for op[41, 81)...
+++++++++ Node 2 initializing the weights for op[41, 81)...
+++++++++ Node 5 initializing the weights for op[81, 121)...
+++++++++ Node 7 initializing the weights for op[121, 160)...
+++++++++ Node 4 initializing the weights for op[81, 121)...
+++++++++ Node 6 initializing the weights for op[121, 160)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 327544
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.6889	TrainAcc 0.0222	ValidAcc 0.0110	TestAcc 0.0099	BestValid 0.0110
	Epoch 50:	Loss 3.6888	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299	BestValid 0.0413
	Epoch 100:	Loss 3.6889	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299	BestValid 0.0413
	Epoch 150:	Loss 3.6885	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299	BestValid 0.0413
	Epoch 200:	Loss 3.1747	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 250:	Loss 2.9037	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 300:	Loss 2.7976	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 350:	Loss 2.6786	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 400:	Loss 2.6780	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 450:	Loss 2.5897	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 500:	Loss 2.5140	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 550:	Loss 2.4601	TrainAcc 0.1810	ValidAcc 0.0767	TestAcc 0.0591	BestValid 0.0767
	Epoch 600:	Loss 2.4557	TrainAcc 0.1318	ValidAcc 0.2482	TestAcc 0.2418	BestValid 0.2482
	Epoch 650:	Loss 2.4665	TrainAcc 0.2962	ValidAcc 0.3171	TestAcc 0.2944	BestValid 0.3171
	Epoch 700:	Loss 2.4454	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.3171
	Epoch 750:	Loss 2.4321	TrainAcc 0.3031	ValidAcc 0.3338	TestAcc 0.3268	BestValid 0.3338
	Epoch 800:	Loss 2.3775	TrainAcc 0.2689	ValidAcc 0.2937	TestAcc 0.2651	BestValid 0.3338
	Epoch 850:	Loss 2.3330	TrainAcc 0.3472	ValidAcc 0.3554	TestAcc 0.3397	BestValid 0.3554
	Epoch 900:	Loss 2.3120	TrainAcc 0.3500	ValidAcc 0.3833	TestAcc 0.3897	BestValid 0.3833
	Epoch 950:	Loss 2.2896	TrainAcc 0.3472	ValidAcc 0.3534	TestAcc 0.3348	BestValid 0.3833
	Epoch 1000:	Loss 2.2739	TrainAcc 0.2860	ValidAcc 0.2404	TestAcc 0.2843	BestValid 0.3833
	Epoch 1050:	Loss 2.2585	TrainAcc 0.3513	ValidAcc 0.3577	TestAcc 0.3389	BestValid 0.3833
	Epoch 1100:	Loss 2.2465	TrainAcc 0.2770	ValidAcc 0.2326	TestAcc 0.2734	BestValid 0.3833
	Epoch 1150:	Loss 2.2318	TrainAcc 0.2792	ValidAcc 0.2353	TestAcc 0.2780	BestValid 0.3833
	Epoch 1200:	Loss 2.2287	TrainAcc 0.2759	ValidAcc 0.2318	TestAcc 0.2728	BestValid 0.3833
	Epoch 1250:	Loss 2.2147	TrainAcc 0.2768	ValidAcc 0.2315	TestAcc 0.2715	BestValid 0.3833
	Epoch 1300:	Loss 2.2097	TrainAcc 0.2329	ValidAcc 0.1078	TestAcc 0.0853	BestValid 0.3833
	Epoch 1350:	Loss 2.2012	TrainAcc 0.3172	ValidAcc 0.2674	TestAcc 0.2453	BestValid 0.3833
	Epoch 1400:	Loss 2.2042	TrainAcc 0.3030	ValidAcc 0.2330	TestAcc 0.2143	BestValid 0.3833
	Epoch 1450:	Loss 2.2006	TrainAcc 0.3278	ValidAcc 0.3097	TestAcc 0.3059	BestValid 0.3833
	Epoch 1500:	Loss 2.1961	TrainAcc 0.2942	ValidAcc 0.2476	TestAcc 0.2908	BestValid 0.3833
	Epoch 1550:	Loss 2.1876	TrainAcc 0.2826	ValidAcc 0.2374	TestAcc 0.2795	BestValid 0.3833
	Epoch 1600:	Loss 2.1817	TrainAcc 0.3208	ValidAcc 0.3028	TestAcc 0.3048	BestValid 0.3833
	Epoch 1650:	Loss 2.1402	TrainAcc 0.2610	ValidAcc 0.1189	TestAcc 0.0952	BestValid 0.3833
	Epoch 1700:	Loss 2.1149	TrainAcc 0.3156	ValidAcc 0.2227	TestAcc 0.2008	BestValid 0.3833
	Epoch 1750:	Loss 2.1162	TrainAcc 0.3622	ValidAcc 0.3146	TestAcc 0.3068	BestValid 0.3833
	Epoch 1800:	Loss 2.0934	TrainAcc 0.3126	ValidAcc 0.2225	TestAcc 0.1972	BestValid 0.3833
	Epoch 1850:	Loss 2.0712	TrainAcc 0.3452	ValidAcc 0.2515	TestAcc 0.2353	BestValid 0.3833
	Epoch 1900:	Loss 2.0461	TrainAcc 0.3103	ValidAcc 0.2352	TestAcc 0.2659	BestValid 0.3833
	Epoch 1950:	Loss 2.0266	TrainAcc 0.3372	ValidAcc 0.2295	TestAcc 0.2068	BestValid 0.3833
	Epoch 2000:	Loss 1.9955	TrainAcc 0.3390	ValidAcc 0.2597	TestAcc 0.3035	BestValid 0.3833
	Epoch 2050:	Loss 1.9829	TrainAcc 0.4059	ValidAcc 0.3823	TestAcc 0.4200	BestValid 0.3833
	Epoch 2100:	Loss 1.9709	TrainAcc 0.3323	ValidAcc 0.2327	TestAcc 0.2190	BestValid 0.3833
	Epoch 2150:	Loss 1.9161	TrainAcc 0.4435	ValidAcc 0.4091	TestAcc 0.3923	BestValid 0.4091
	Epoch 2200:	Loss 1.8900	TrainAcc 0.3920	ValidAcc 0.2779	TestAcc 0.2580	BestValid 0.4091
	Epoch 2250:	Loss 1.8801	TrainAcc 0.4842	ValidAcc 0.5249	TestAcc 0.5437	BestValid 0.5249
	Epoch 2300:	Loss 1.8663	TrainAcc 0.4292	ValidAcc 0.3854	TestAcc 0.3599	BestValid 0.5249
	Epoch 2350:	Loss 1.8543	TrainAcc 0.4568	ValidAcc 0.4426	TestAcc 0.4493	BestValid 0.5249
	Epoch 2400:	Loss 1.8371	TrainAcc 0.3611	ValidAcc 0.1957	TestAcc 0.1678	BestValid 0.5249
	Epoch 2450:	Loss 1.8326	TrainAcc 0.4467	ValidAcc 0.3916	TestAcc 0.3558	BestValid 0.5249
	Epoch 2500:	Loss 1.8264	TrainAcc 0.4813	ValidAcc 0.5251	TestAcc 0.5408	BestValid 0.5251
	Epoch 2550:	Loss 1.8187	TrainAcc 0.4829	ValidAcc 0.4898	TestAcc 0.4692	BestValid 0.5251
	Epoch 2600:	Loss 1.8132	TrainAcc 0.4634	ValidAcc 0.4174	TestAcc 0.3803	BestValid 0.5251
	Epoch 2650:	Loss 1.8068	TrainAcc 0.3125	ValidAcc 0.1458	TestAcc 0.1128	BestValid 0.5251
	Epoch 2700:	Loss 1.8069	TrainAcc 0.3906	ValidAcc 0.3522	TestAcc 0.3172	BestValid 0.5251
	Epoch 2750:	Loss 1.7982	TrainAcc 0.3857	ValidAcc 0.2730	TestAcc 0.2870	BestValid 0.5251
	Epoch 2800:	Loss 1.8019	TrainAcc 0.3279	ValidAcc 0.1600	TestAcc 0.1284	BestValid 0.5251
	Epoch 2850:	Loss 1.7891	TrainAcc 0.3565	ValidAcc 0.1855	TestAcc 0.1520	BestValid 0.5251
	Epoch 2900:	Loss 1.7824	TrainAcc 0.2500	ValidAcc 0.1225	TestAcc 0.0987	BestValid 0.5251
	Epoch 2950:	Loss 1.7822	TrainAcc 0.4874	ValidAcc 0.4731	TestAcc 0.4424	BestValid 0.5251
	Epoch 3000:	Loss 1.7737	TrainAcc 0.3434	ValidAcc 0.1703	TestAcc 0.1351	BestValid 0.5251
	Epoch 3050:	Loss 1.7725	TrainAcc 0.3747	ValidAcc 0.2420	TestAcc 0.2133	BestValid 0.5251
	Epoch 3100:	Loss 1.7706	TrainAcc 0.3212	ValidAcc 0.1448	TestAcc 0.1148	BestValid 0.5251
	Epoch 3150:	Loss 1.7720	TrainAcc 0.3675	ValidAcc 0.1932	TestAcc 0.1580	BestValid 0.5251
	Epoch 3200:	Loss 1.7679	TrainAcc 0.4366	ValidAcc 0.4286	TestAcc 0.4337	BestValid 0.5251
	Epoch 3250:	Loss 1.7587	TrainAcc 0.4803	ValidAcc 0.4637	TestAcc 0.4404	BestValid 0.5251
	Epoch 3300:	Loss 1.7565	TrainAcc 0.3160	ValidAcc 0.1559	TestAcc 0.1300	BestValid 0.5251
	Epoch 3350:	Loss 1.7517	TrainAcc 0.4876	ValidAcc 0.4774	TestAcc 0.4523	BestValid 0.5251
	Epoch 3400:	Loss 1.7507	TrainAcc 0.3796	ValidAcc 0.2722	TestAcc 0.3140	BestValid 0.5251
	Epoch 3450:	Loss 1.7458	TrainAcc 0.3464	ValidAcc 0.1670	TestAcc 0.1292	BestValid 0.5251
	Epoch 3500:	Loss 1.7426	TrainAcc 0.3979	ValidAcc 0.2706	TestAcc 0.2807	BestValid 0.5251
	Epoch 3550:	Loss 1.7401	TrainAcc 0.4870	ValidAcc 0.4285	TestAcc 0.3901	BestValid 0.5251
	Epoch 3600:	Loss 1.7349	TrainAcc 0.3750	ValidAcc 0.1877	TestAcc 0.1506	BestValid 0.5251
	Epoch 3650:	Loss 1.7257	TrainAcc 0.4214	ValidAcc 0.2813	TestAcc 0.2512	BestValid 0.5251
	Epoch 3700:	Loss 1.7225	TrainAcc 0.4523	ValidAcc 0.3871	TestAcc 0.3455	BestValid 0.5251
	Epoch 3750:	Loss 1.7370	TrainAcc 0.3334	ValidAcc 0.1649	TestAcc 0.1309	BestValid 0.5251
	Epoch 3800:	Loss 1.7193	TrainAcc 0.3838	ValidAcc 0.2100	TestAcc 0.1942	BestValid 0.5251
	Epoch 3850:	Loss 1.7262	TrainAcc 0.3737	ValidAcc 0.2164	TestAcc 0.1838	BestValid 0.5251
	Epoch 3900:	Loss 1.7175	TrainAcc 0.3457	ValidAcc 0.1687	TestAcc 0.1344	BestValid 0.5251
	Epoch 3950:	Loss 1.7099	TrainAcc 0.3701	ValidAcc 0.1886	TestAcc 0.1521	BestValid 0.5251
	Epoch 4000:	Loss 1.7110	TrainAcc 0.4057	ValidAcc 0.2449	TestAcc 0.2390	BestValid 0.5251
	Epoch 4050:	Loss 1.7055	TrainAcc 0.4047	ValidAcc 0.2570	TestAcc 0.2269	BestValid 0.5251
	Epoch 4100:	Loss 1.6990	TrainAcc 0.5190	ValidAcc 0.4837	TestAcc 0.4466	BestValid 0.5251
	Epoch 4150:	Loss 1.7034	TrainAcc 0.3935	ValidAcc 0.2654	TestAcc 0.2817	BestValid 0.5251
	Epoch 4200:	Loss 1.7036	TrainAcc 0.5234	ValidAcc 0.5004	TestAcc 0.4722	BestValid 0.5251
	Epoch 4250:	Loss 1.6962	TrainAcc 0.2992	ValidAcc 0.1395	TestAcc 0.1096	BestValid 0.5251
	Epoch 4300:	Loss 1.6940	TrainAcc 0.4686	ValidAcc 0.3901	TestAcc 0.3621	BestValid 0.5251
	Epoch 4350:	Loss 1.6939	TrainAcc 0.4253	ValidAcc 0.2782	TestAcc 0.2485	BestValid 0.5251
	Epoch 4400:	Loss 1.6927	TrainAcc 0.3651	ValidAcc 0.1819	TestAcc 0.1456	BestValid 0.5251
	Epoch 4450:	Loss 1.6870	TrainAcc 0.3635	ValidAcc 0.1813	TestAcc 0.1472	BestValid 0.5251
	Epoch 4500:	Loss 1.6876	TrainAcc 0.3696	ValidAcc 0.1979	TestAcc 0.1734	BestValid 0.5251
	Epoch 4550:	Loss 1.6890	TrainAcc 0.3344	ValidAcc 0.2157	TestAcc 0.1942	BestValid 0.5251
	Epoch 4600:	Loss 1.6871	TrainAcc 0.3725	ValidAcc 0.2597	TestAcc 0.2798	BestValid 0.5251
	Epoch 4650:	Loss 1.6787	TrainAcc 0.3788	ValidAcc 0.1951	TestAcc 0.1557	BestValid 0.5251
	Epoch 4700:	Loss 1.6821	TrainAcc 0.5165	ValidAcc 0.5099	TestAcc 0.4867	BestValid 0.5251
	Epoch 4750:	Loss 1.6777	TrainAcc 0.3360	ValidAcc 0.1660	TestAcc 0.1322	BestValid 0.5251
	Epoch 4800:	Loss 1.6774	TrainAcc 0.4527	ValidAcc 0.3873	TestAcc 0.3478	BestValid 0.5251
	Epoch 4850:	Loss 1.6663	TrainAcc 0.3505	ValidAcc 0.1564	TestAcc 0.1276	BestValid 0.5251
	Epoch 4900:	Loss 1.6788	TrainAcc 0.4098	ValidAcc 0.2211	TestAcc 0.1909	BestValid 0.5251
	Epoch 4950:	Loss 1.6663	TrainAcc 0.4342	ValidAcc 0.2789	TestAcc 0.2481	BestValid 0.5251
	Epoch 5000:	Loss 1.6707	TrainAcc 0.5248	ValidAcc 0.4662	TestAcc 0.4267	BestValid 0.5251
****** Epoch Time (Excluding Evaluation Cost): 0.118 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 19.427 ms (Max: 21.191, Min: 14.486, Sum: 155.414)
Cluster-Wide Average, Compute: 48.250 ms (Max: 53.204, Min: 45.879, Sum: 385.997)
Cluster-Wide Average, Communication-Layer: 10.043 ms (Max: 12.408, Min: 7.203, Sum: 80.342)
Cluster-Wide Average, Bubble-Imbalance: 4.470 ms (Max: 5.887, Min: 2.216, Sum: 35.759)
Cluster-Wide Average, Communication-Graph: 29.477 ms (Max: 30.268, Min: 28.684, Sum: 235.815)
Cluster-Wide Average, Optimization: 1.583 ms (Max: 1.688, Min: 1.373, Sum: 12.664)
Cluster-Wide Average, Others: 4.368 ms (Max: 9.862, Min: 2.531, Sum: 34.945)
****** Breakdown Sum: 117.617 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.364 GB (Max: 5.024, Min: 4.172, Sum: 34.913)
Cluster-Wide Average, Graph-Level Communication Throughput: 96.641 Gbps (Max: 100.886, Min: 92.251, Sum: 773.126)
Cluster-Wide Average, Layer-Level Communication Throughput: 40.476 Gbps (Max: 57.064, Min: 25.066, Sum: 323.809)
Layer-level communication (cluster-wide, per-epoch): 0.379 GB
Graph-level communication (cluster-wide, per-epoch): 1.952 GB
Weight-sync communication (cluster-wide, per-epoch): 0.002 GB
Total communication (cluster-wide, per-epoch): 2.333 GB
****** Accuracy Results ******
Highest valid_acc: 0.5251
Target test_acc: 0.5408
Epoch to reach the target acc: 2499
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
