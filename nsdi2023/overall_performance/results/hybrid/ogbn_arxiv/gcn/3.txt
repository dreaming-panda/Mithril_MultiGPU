Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.067 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
        It takes 0.049 seconds.
        It takes 0.055 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.056 seconds.
        It takes 0.051 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
        It takes 0.023 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.667 Gbps (per GPU), 485.335 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.378 Gbps (per GPU), 483.027 Gbps (aggregated)
The layer-level communication performance: 60.372 Gbps (per GPU), 482.976 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.130 Gbps (per GPU), 481.042 Gbps (aggregated)
The layer-level communication performance: 60.100 Gbps (per GPU), 480.803 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.882 Gbps (per GPU), 479.055 Gbps (aggregated)
The layer-level communication performance: 59.830 Gbps (per GPU), 478.641 Gbps (aggregated)
The layer-level communication performance: 59.794 Gbps (per GPU), 478.354 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.213 Gbps (per GPU), 1273.703 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.192 Gbps (per GPU), 1273.534 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.219 Gbps (per GPU), 1273.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.204 Gbps (per GPU), 1273.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.213 Gbps (per GPU), 1273.702 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.201 Gbps (per GPU), 1273.606 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.213 Gbps (per GPU), 1273.703 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.201 Gbps (per GPU), 1273.606 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.787 Gbps (per GPU), 838.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.783 Gbps (per GPU), 838.267 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.787 Gbps (per GPU), 838.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.784 Gbps (per GPU), 838.274 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.784 Gbps (per GPU), 838.274 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.781 Gbps (per GPU), 838.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.763 Gbps (per GPU), 838.107 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.712 Gbps (per GPU), 837.695 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.035 Gbps (per GPU), 288.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.034 Gbps (per GPU), 288.273 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.033 Gbps (per GPU), 288.265 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.034 Gbps (per GPU), 288.272 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.029 Gbps (per GPU), 288.234 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.035 Gbps (per GPU), 288.280 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.026 Gbps (per GPU), 288.212 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.027 Gbps (per GPU), 288.214 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.30ms  0.32ms  0.22ms  1.45  5.55K  0.06M
 chk_1  0.30ms  0.33ms  0.23ms  1.45  5.75K  0.05M
 chk_2  0.31ms  0.33ms  0.23ms  1.43  5.20K  0.07M
 chk_3  0.31ms  0.33ms  0.23ms  1.44  5.57K  0.06M
 chk_4  0.30ms  0.33ms  0.23ms  1.43  5.05K  0.08M
 chk_5  0.33ms  0.36ms  0.26ms  1.37  4.73K  0.11M
 chk_6  0.30ms  0.33ms  0.23ms  1.43  4.98K  0.08M
 chk_7  0.31ms  0.33ms  0.24ms  1.41  5.01K  0.09M
 chk_8  0.30ms  0.33ms  0.22ms  1.45  5.73K  0.05M
 chk_9  0.31ms  0.33ms  0.24ms  1.41  4.54K  0.11M
chk_10  0.30ms  0.33ms  0.23ms  1.44  5.36K  0.07M
chk_11  0.31ms  0.34ms  0.24ms  1.42  5.39K  0.08M
chk_12  0.30ms  0.34ms  0.22ms  1.54  5.77K  0.05M
chk_13  0.30ms  0.33ms  0.23ms  1.43  5.43K  0.06M
chk_14  0.30ms  0.33ms  0.23ms  1.45  5.46K  0.06M
chk_15  0.29ms  0.32ms  0.22ms  1.47  5.88K  0.04M
chk_16  0.30ms  0.33ms  0.23ms  1.43  5.50K  0.06M
chk_17  0.32ms  0.35ms  0.25ms  1.38  4.86K  0.09M
chk_18  0.33ms  0.35ms  0.26ms  1.38  5.39K  0.07M
chk_19  0.31ms  0.34ms  0.24ms  1.42  5.20K  0.07M
chk_20  0.30ms  0.32ms  0.22ms  1.47  5.51K  0.06M
chk_21  0.29ms  0.32ms  0.21ms  1.49  5.81K  0.05M
chk_22  0.30ms  0.33ms  0.23ms  1.44  5.32K  0.07M
chk_23  0.32ms  0.35ms  0.24ms  1.42  5.39K  0.07M
chk_24  0.31ms  0.34ms  0.25ms  1.38  4.62K  0.11M
chk_25  0.31ms  0.34ms  0.24ms  1.41  5.04K  0.08M
chk_26  0.30ms  0.33ms  0.24ms  1.41  4.55K  0.11M
chk_27  0.30ms  0.32ms  0.22ms  1.45  5.30K  0.06M
chk_28  0.30ms  0.33ms  0.23ms  1.45  5.58K  0.06M
chk_29  0.32ms  0.34ms  0.25ms  1.39  4.98K  0.09M
chk_30  0.30ms  0.33ms  0.23ms  1.45  5.50K  0.07M
chk_31  0.30ms  0.33ms  0.23ms  1.44  5.38K  0.07M
   Avg  0.30  0.33  0.23
   Max  0.33  0.36  0.26
   Min  0.29  0.32  0.21
 Ratio  1.15  1.13  1.22
   Var  0.00  0.00  0.00
Profiling takes 0.432 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 41)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 2, starting model training...
Num Stages: 4 / 4
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 81)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 85420
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [41, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [81, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [121, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 4 owns the model-level partition [81, 121)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [121, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 41)...
+++++++++ Node 4 initializing the weights for op[81, 121)...
+++++++++ Node 1 initializing the weights for op[0, 41)...
+++++++++ Node 5 initializing the weights for op[81, 121)...
+++++++++ Node 2 initializing the weights for op[41, 81)...
+++++++++ Node 6 initializing the weights for op[121, 160)...
+++++++++ Node 3 initializing the weights for op[41, 81)...
+++++++++ Node 7 initializing the weights for op[121, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 327544
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...


*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000

The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.6889	TrainAcc 0.0465	ValidAcc 0.0638	TestAcc 0.0645	BestValid 0.0638
	Epoch 50:	Loss 3.6888	TrainAcc 0.0396	ValidAcc 0.0168	TestAcc 0.0151	BestValid 0.0638
	Epoch 100:	Loss 3.6886	TrainAcc 0.0396	ValidAcc 0.0168	TestAcc 0.0151	BestValid 0.0638
	Epoch 150:	Loss 3.4271	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 200:	Loss 3.0135	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 250:	Loss 3.0521	TrainAcc 0.1099	ValidAcc 0.2297	TestAcc 0.2156	BestValid 0.2297
	Epoch 300:	Loss 2.9305	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.2297
	Epoch 350:	Loss 2.8506	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.2297
	Epoch 400:	Loss 2.7156	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.2297
	Epoch 450:	Loss 2.6130	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.2297
	Epoch 500:	Loss 2.5734	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.2297
	Epoch 550:	Loss 2.5298	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.2297
	Epoch 600:	Loss 2.5590	TrainAcc 0.0267	ValidAcc 0.0420	TestAcc 0.0581	BestValid 0.2297
	Epoch 650:	Loss 2.6213	TrainAcc 0.2076	ValidAcc 0.0926	TestAcc 0.0721	BestValid 0.2297
	Epoch 700:	Loss 2.6319	TrainAcc 0.2233	ValidAcc 0.1052	TestAcc 0.0820	BestValid 0.2297
	Epoch 750:	Loss 2.4176	TrainAcc 0.2314	ValidAcc 0.1075	TestAcc 0.0848	BestValid 0.2297
	Epoch 800:	Loss 2.3914	TrainAcc 0.2332	ValidAcc 0.1084	TestAcc 0.0857	BestValid 0.2297
	Epoch 850:	Loss 2.3813	TrainAcc 0.3298	ValidAcc 0.3301	TestAcc 0.2938	BestValid 0.3301
	Epoch 900:	Loss 2.3440	TrainAcc 0.2315	ValidAcc 0.1075	TestAcc 0.0847	BestValid 0.3301
	Epoch 950:	Loss 2.3397	TrainAcc 0.2313	ValidAcc 0.1072	TestAcc 0.0849	BestValid 0.3301
	Epoch 1000:	Loss 2.3851	TrainAcc 0.2319	ValidAcc 0.1074	TestAcc 0.0850	BestValid 0.3301
	Epoch 1050:	Loss 2.3716	TrainAcc 0.2987	ValidAcc 0.3173	TestAcc 0.2884	BestValid 0.3301
	Epoch 1100:	Loss 2.3119	TrainAcc 0.3363	ValidAcc 0.3362	TestAcc 0.3035	BestValid 0.3362
	Epoch 1150:	Loss 2.2887	TrainAcc 0.3390	ValidAcc 0.3405	TestAcc 0.3127	BestValid 0.3405
	Epoch 1200:	Loss 2.2737	TrainAcc 0.3448	ValidAcc 0.3530	TestAcc 0.3327	BestValid 0.3530
	Epoch 1250:	Loss 2.2625	TrainAcc 0.3492	ValidAcc 0.3616	TestAcc 0.3502	BestValid 0.3616
	Epoch 1300:	Loss 2.2529	TrainAcc 0.3490	ValidAcc 0.3650	TestAcc 0.3577	BestValid 0.3650
	Epoch 1350:	Loss 2.2460	TrainAcc 0.3459	ValidAcc 0.3494	TestAcc 0.3243	BestValid 0.3650
	Epoch 1400:	Loss 2.2272	TrainAcc 0.2947	ValidAcc 0.2174	TestAcc 0.1969	BestValid 0.3650
	Epoch 1450:	Loss 2.2055	TrainAcc 0.2932	ValidAcc 0.2233	TestAcc 0.2049	BestValid 0.3650
	Epoch 1500:	Loss 2.1918	TrainAcc 0.2869	ValidAcc 0.2416	TestAcc 0.2901	BestValid 0.3650
	Epoch 1550:	Loss 2.1806	TrainAcc 0.3029	ValidAcc 0.2878	TestAcc 0.2810	BestValid 0.3650
	Epoch 1600:	Loss 2.1709	TrainAcc 0.3301	ValidAcc 0.3321	TestAcc 0.3606	BestValid 0.3650
	Epoch 1650:	Loss 2.1615	TrainAcc 0.3582	ValidAcc 0.3645	TestAcc 0.3533	BestValid 0.3650
	Epoch 1700:	Loss 2.1497	TrainAcc 0.3701	ValidAcc 0.3956	TestAcc 0.3773	BestValid 0.3956
	Epoch 1750:	Loss 2.1430	TrainAcc 0.3063	ValidAcc 0.2355	TestAcc 0.2161	BestValid 0.3956
	Epoch 1800:	Loss 2.1362	TrainAcc 0.3070	ValidAcc 0.2313	TestAcc 0.2080	BestValid 0.3956
	Epoch 1850:	Loss 2.1307	TrainAcc 0.3708	ValidAcc 0.3939	TestAcc 0.3692	BestValid 0.3956
	Epoch 1900:	Loss 2.1241	TrainAcc 0.3487	ValidAcc 0.3442	TestAcc 0.3118	BestValid 0.3956
	Epoch 1950:	Loss 2.1129	TrainAcc 0.2953	ValidAcc 0.2314	TestAcc 0.2168	BestValid 0.3956
	Epoch 2000:	Loss 2.0857	TrainAcc 0.3449	ValidAcc 0.3356	TestAcc 0.3058	BestValid 0.3956
	Epoch 2050:	Loss 2.0610	TrainAcc 0.3330	ValidAcc 0.2540	TestAcc 0.2418	BestValid 0.3956
	Epoch 2100:	Loss 2.0408	TrainAcc 0.3392	ValidAcc 0.2527	TestAcc 0.2404	BestValid 0.3956
	Epoch 2150:	Loss 2.0226	TrainAcc 0.3366	ValidAcc 0.2513	TestAcc 0.2384	BestValid 0.3956
	Epoch 2200:	Loss 2.0100	TrainAcc 0.3437	ValidAcc 0.2357	TestAcc 0.2157	BestValid 0.3956
	Epoch 2250:	Loss 1.9996	TrainAcc 0.3476	ValidAcc 0.2590	TestAcc 0.2498	BestValid 0.3956
	Epoch 2300:	Loss 1.9866	TrainAcc 0.3326	ValidAcc 0.2275	TestAcc 0.2067	BestValid 0.3956
	Epoch 2350:	Loss 1.9759	TrainAcc 0.4055	ValidAcc 0.3782	TestAcc 0.3462	BestValid 0.3956
	Epoch 2400:	Loss 1.9627	TrainAcc 0.3835	ValidAcc 0.3521	TestAcc 0.3179	BestValid 0.3956
	Epoch 2450:	Loss 1.9596	TrainAcc 0.3721	ValidAcc 0.3461	TestAcc 0.3128	BestValid 0.3956
slurmstepd-gnerv7: error: *** STEP 3763.28 ON gnerv7 CANCELLED AT 2023-09-20T12:25:25 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
