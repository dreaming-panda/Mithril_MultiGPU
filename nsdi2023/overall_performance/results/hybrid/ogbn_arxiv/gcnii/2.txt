Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 7 on machine gnerv8
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.053 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
        It takes 0.055 seconds.
        It takes 0.056 seconds.
        It takes 0.058 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.050 seconds.
Building the Label Vector...
        It takes 0.051 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.021 seconds.
        It takes 0.022 seconds.
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.069 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.814 Gbps (per GPU), 486.512 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.506 Gbps (per GPU), 484.046 Gbps (aggregated)
The layer-level communication performance: 60.503 Gbps (per GPU), 484.025 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.247 Gbps (per GPU), 481.976 Gbps (aggregated)
The layer-level communication performance: 60.216 Gbps (per GPU), 481.730 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.001 Gbps (per GPU), 480.008 Gbps (aggregated)
The layer-level communication performance: 59.952 Gbps (per GPU), 479.616 Gbps (aggregated)
The layer-level communication performance: 59.916 Gbps (per GPU), 479.332 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.238 Gbps (per GPU), 1281.902 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.235 Gbps (per GPU), 1281.878 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.238 Gbps (per GPU), 1281.902 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.232 Gbps (per GPU), 1281.853 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.241 Gbps (per GPU), 1281.927 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.238 Gbps (per GPU), 1281.902 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.232 Gbps (per GPU), 1281.853 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.216 Gbps (per GPU), 1281.731 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.297 Gbps (per GPU), 834.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.291 Gbps (per GPU), 834.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.297 Gbps (per GPU), 834.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.284 Gbps (per GPU), 834.272 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.298 Gbps (per GPU), 834.383 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.291 Gbps (per GPU), 834.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.297 Gbps (per GPU), 834.375 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.296 Gbps (per GPU), 834.369 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.035 Gbps (per GPU), 312.279 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.035 Gbps (per GPU), 312.280 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.034 Gbps (per GPU), 312.272 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.034 Gbps (per GPU), 312.270 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.034 Gbps (per GPU), 312.270 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.026 Gbps (per GPU), 312.208 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.031 Gbps (per GPU), 312.251 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.023 Gbps (per GPU), 312.183 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.33ms  0.46ms  0.61ms  1.87  5.55K  0.06M
 chk_1  0.33ms  0.47ms  0.62ms  1.88  5.75K  0.05M
 chk_2  0.97ms  0.47ms  0.62ms  2.07  5.20K  0.07M
 chk_3  0.33ms  0.47ms  0.62ms  1.91  5.57K  0.06M
 chk_4  0.32ms  0.46ms  0.61ms  1.90  5.05K  0.08M
 chk_5  0.31ms  0.48ms  0.63ms  2.02  4.73K  0.11M
 chk_6  0.32ms  0.46ms  0.61ms  1.90  4.98K  0.08M
 chk_7  0.32ms  0.46ms  0.62ms  1.93  5.01K  0.09M
 chk_8  0.33ms  0.46ms  0.62ms  1.89  5.73K  0.05M
 chk_9  0.31ms  0.46ms  0.61ms  1.97  4.54K  0.11M
chk_10  0.32ms  0.47ms  0.62ms  1.92  5.36K  0.07M
chk_11  0.33ms  0.47ms  0.63ms  1.91  5.39K  0.08M
chk_12  0.33ms  0.46ms  0.62ms  1.87  5.77K  0.05M
chk_13  0.32ms  0.46ms  0.62ms  1.90  5.43K  0.06M
chk_14  0.32ms  0.46ms  0.61ms  1.88  5.46K  0.06M
chk_15  0.33ms  0.46ms  0.62ms  1.86  5.88K  0.04M
chk_16  0.33ms  0.46ms  0.62ms  1.90  5.50K  0.06M
chk_17  0.32ms  0.48ms  0.63ms  1.99  4.86K  0.09M
chk_18  0.33ms  0.49ms  0.65ms  1.98  5.39K  0.07M
chk_19  0.32ms  0.47ms  0.62ms  1.93  5.20K  0.07M
chk_20  0.33ms  0.46ms  0.61ms  1.87  5.51K  0.06M
chk_21  0.33ms  0.46ms  0.61ms  1.85  5.81K  0.05M
chk_22  0.32ms  0.46ms  0.62ms  1.90  5.32K  0.07M
chk_23  0.33ms  0.48ms  0.63ms  1.94  5.39K  0.07M
chk_24  0.31ms  0.46ms  0.61ms  1.99  4.62K  0.11M
chk_25  0.32ms  0.47ms  0.62ms  1.94  5.04K  0.08M
chk_26  0.31ms  0.46ms  0.61ms  1.96  4.55K  0.11M
chk_27  0.32ms  0.45ms  0.60ms  1.87  5.30K  0.06M
chk_28  0.33ms  0.46ms  0.61ms  1.87  5.58K  0.06M
chk_29  0.32ms  0.65ms  0.62ms  2.04  4.98K  0.09M
chk_30  0.33ms  0.46ms  0.62ms  1.88  5.50K  0.07M
chk_31  0.33ms  0.46ms  0.62ms  1.89  5.38K  0.07M
   Avg  0.34  0.47  0.62
   Max  0.97  0.65  0.65
   Min  0.31  0.45  0.60
 Ratio  3.13  1.43  1.07
   Var  0.01  0.00  0.00
Profiling takes 0.682 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 327544
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.8748	TrainAcc 0.0193	ValidAcc 0.0097	TestAcc 0.0101	BestValid 0.0097
	Epoch 50:	Loss 3.1493	TrainAcc 0.1798	ValidAcc 0.0773	TestAcc 0.0590	BestValid 0.0773
	Epoch 100:	Loss 2.8273	TrainAcc 0.3236	ValidAcc 0.3205	TestAcc 0.2885	BestValid 0.3205
	Epoch 150:	Loss 2.5744	TrainAcc 0.3974	ValidAcc 0.3766	TestAcc 0.3458	BestValid 0.3766
	Epoch 200:	Loss 2.3713	TrainAcc 0.4302	ValidAcc 0.4036	TestAcc 0.3768	BestValid 0.4036
	Epoch 250:	Loss 2.2591	TrainAcc 0.4601	ValidAcc 0.4427	TestAcc 0.4181	BestValid 0.4427
	Epoch 300:	Loss 2.1580	TrainAcc 0.4893	ValidAcc 0.4833	TestAcc 0.4651	BestValid 0.4833
	Epoch 350:	Loss 2.0849	TrainAcc 0.5089	ValidAcc 0.5071	TestAcc 0.4902	BestValid 0.5071
	Epoch 400:	Loss 2.0313	TrainAcc 0.5246	ValidAcc 0.5272	TestAcc 0.5103	BestValid 0.5272
	Epoch 450:	Loss 1.9735	TrainAcc 0.5346	ValidAcc 0.5397	TestAcc 0.5266	BestValid 0.5397
	Epoch 500:	Loss 1.9266	TrainAcc 0.5442	ValidAcc 0.5499	TestAcc 0.5398	BestValid 0.5499
	Epoch 550:	Loss 1.8926	TrainAcc 0.5517	ValidAcc 0.5561	TestAcc 0.5469	BestValid 0.5561
	Epoch 600:	Loss 1.8611	TrainAcc 0.5601	ValidAcc 0.5653	TestAcc 0.5554	BestValid 0.5653
	Epoch 650:	Loss 1.8283	TrainAcc 0.5677	ValidAcc 0.5726	TestAcc 0.5634	BestValid 0.5726
	Epoch 700:	Loss 1.8026	TrainAcc 0.5748	ValidAcc 0.5817	TestAcc 0.5733	BestValid 0.5817
	Epoch 750:	Loss 1.7774	TrainAcc 0.5823	ValidAcc 0.5917	TestAcc 0.5826	BestValid 0.5917
	Epoch 800:	Loss 1.7499	TrainAcc 0.5874	ValidAcc 0.5950	TestAcc 0.5850	BestValid 0.5950
	Epoch 850:	Loss 1.7319	TrainAcc 0.5940	ValidAcc 0.6034	TestAcc 0.5929	BestValid 0.6034
	Epoch 900:	Loss 1.7091	TrainAcc 0.5981	ValidAcc 0.6085	TestAcc 0.5981	BestValid 0.6085
	Epoch 950:	Loss 1.7017	TrainAcc 0.6021	ValidAcc 0.6129	TestAcc 0.6016	BestValid 0.6129
	Epoch 1000:	Loss 1.6826	TrainAcc 0.6062	ValidAcc 0.6176	TestAcc 0.6066	BestValid 0.6176
	Epoch 1050:	Loss 1.6689	TrainAcc 0.6091	ValidAcc 0.6191	TestAcc 0.6082	BestValid 0.6191
	Epoch 1100:	Loss 1.6501	TrainAcc 0.6127	ValidAcc 0.6226	TestAcc 0.6119	BestValid 0.6226
	Epoch 1150:	Loss 1.6384	TrainAcc 0.6156	ValidAcc 0.6258	TestAcc 0.6155	BestValid 0.6258
	Epoch 1200:	Loss 1.6248	TrainAcc 0.6180	ValidAcc 0.6281	TestAcc 0.6182	BestValid 0.6281
	Epoch 1250:	Loss 1.6140	TrainAcc 0.6200	ValidAcc 0.6296	TestAcc 0.6187	BestValid 0.6296
	Epoch 1300:	Loss 1.6037	TrainAcc 0.6221	ValidAcc 0.6320	TestAcc 0.6209	BestValid 0.6320
	Epoch 1350:	Loss 1.5995	TrainAcc 0.6226	ValidAcc 0.6302	TestAcc 0.6201	BestValid 0.6320
	Epoch 1400:	Loss 1.5940	TrainAcc 0.6254	ValidAcc 0.6335	TestAcc 0.6232	BestValid 0.6335
	Epoch 1450:	Loss 1.5746	TrainAcc 0.6274	ValidAcc 0.6359	TestAcc 0.6262	BestValid 0.6359
	Epoch 1500:	Loss 1.5710	TrainAcc 0.6297	ValidAcc 0.6380	TestAcc 0.6282	BestValid 0.6380
	Epoch 1550:	Loss 1.5731	TrainAcc 0.6311	ValidAcc 0.6393	TestAcc 0.6306	BestValid 0.6393
	Epoch 1600:	Loss 1.5613	TrainAcc 0.6325	ValidAcc 0.6415	TestAcc 0.6321	BestValid 0.6415
	Epoch 1650:	Loss 1.5500	TrainAcc 0.6347	ValidAcc 0.6421	TestAcc 0.6330	BestValid 0.6421
	Epoch 1700:	Loss 1.5456	TrainAcc 0.6345	ValidAcc 0.6410	TestAcc 0.6318	BestValid 0.6421
	Epoch 1750:	Loss 1.5352	TrainAcc 0.6347	ValidAcc 0.6403	TestAcc 0.6319	BestValid 0.6421
	Epoch 1800:	Loss 1.5340	TrainAcc 0.6366	ValidAcc 0.6430	TestAcc 0.6347	BestValid 0.6430
	Epoch 1850:	Loss 1.5257	TrainAcc 0.6389	ValidAcc 0.6449	TestAcc 0.6357	BestValid 0.6449
	Epoch 1900:	Loss 1.5201	TrainAcc 0.6397	ValidAcc 0.6460	TestAcc 0.6374	BestValid 0.6460
	Epoch 1950:	Loss 1.5202	TrainAcc 0.6418	ValidAcc 0.6471	TestAcc 0.6381	BestValid 0.6471
	Epoch 2000:	Loss 1.5106	TrainAcc 0.6424	ValidAcc 0.6479	TestAcc 0.6390	BestValid 0.6479
	Epoch 2050:	Loss 1.5144	TrainAcc 0.6446	ValidAcc 0.6504	TestAcc 0.6411	BestValid 0.6504
	Epoch 2100:	Loss 1.5057	TrainAcc 0.6460	ValidAcc 0.6521	TestAcc 0.6436	BestValid 0.6521
	Epoch 2150:	Loss 1.4968	TrainAcc 0.6463	ValidAcc 0.6512	TestAcc 0.6423	BestValid 0.6521
	Epoch 2200:	Loss 1.4950	TrainAcc 0.6469	ValidAcc 0.6505	TestAcc 0.6429	BestValid 0.6521
	Epoch 2250:	Loss 1.4846	TrainAcc 0.6493	ValidAcc 0.6534	TestAcc 0.6450	BestValid 0.6534
	Epoch 2300:	Loss 1.4859	TrainAcc 0.6497	ValidAcc 0.6540	TestAcc 0.6461	BestValid 0.6540
	Epoch 2350:	Loss 1.4894	TrainAcc 0.6511	ValidAcc 0.6560	TestAcc 0.6482	BestValid 0.6560
	Epoch 2400:	Loss 1.4792	TrainAcc 0.6504	ValidAcc 0.6535	TestAcc 0.6465	BestValid 0.6560
	Epoch 2450:	Loss 1.4731	TrainAcc 0.6525	ValidAcc 0.6563	TestAcc 0.6475	BestValid 0.6563
	Epoch 2500:	Loss 1.4696	TrainAcc 0.6532	ValidAcc 0.6562	TestAcc 0.6482	BestValid 0.6563
	Epoch 2550:	Loss 1.4661	TrainAcc 0.6530	ValidAcc 0.6550	TestAcc 0.6475	BestValid 0.6563
	Epoch 2600:	Loss 1.4659	TrainAcc 0.6548	ValidAcc 0.6573	TestAcc 0.6510	BestValid 0.6573
	Epoch 2650:	Loss 1.4550	TrainAcc 0.6566	ValidAcc 0.6607	TestAcc 0.6529	BestValid 0.6607
	Epoch 2700:	Loss 1.4606	TrainAcc 0.6569	ValidAcc 0.6607	TestAcc 0.6545	BestValid 0.6607
	Epoch 2750:	Loss 1.4553	TrainAcc 0.6571	ValidAcc 0.6596	TestAcc 0.6526	BestValid 0.6607
	Epoch 2800:	Loss 1.4508	TrainAcc 0.6593	ValidAcc 0.6620	TestAcc 0.6542	BestValid 0.6620
	Epoch 2850:	Loss 1.4461	TrainAcc 0.6594	ValidAcc 0.6617	TestAcc 0.6538	BestValid 0.6620
	Epoch 2900:	Loss 1.4561	TrainAcc 0.6605	ValidAcc 0.6639	TestAcc 0.6573	BestValid 0.6639
	Epoch 2950:	Loss 1.4413	TrainAcc 0.6606	ValidAcc 0.6632	TestAcc 0.6566	BestValid 0.6639
	Epoch 3000:	Loss 1.4409	TrainAcc 0.6617	ValidAcc 0.6645	TestAcc 0.6572	BestValid 0.6645
	Epoch 3050:	Loss 1.4327	TrainAcc 0.6621	ValidAcc 0.6633	TestAcc 0.6560	BestValid 0.6645
	Epoch 3100:	Loss 1.4409	TrainAcc 0.6636	ValidAcc 0.6662	TestAcc 0.6601	BestValid 0.6662
	Epoch 3150:	Loss 1.4323	TrainAcc 0.6642	ValidAcc 0.6661	TestAcc 0.6595	BestValid 0.6662
	Epoch 3200:	Loss 1.4298	TrainAcc 0.6651	ValidAcc 0.6671	TestAcc 0.6608	BestValid 0.6671
	Epoch 3250:	Loss 1.4246	TrainAcc 0.6668	ValidAcc 0.6679	TestAcc 0.6610	BestValid 0.6679
	Epoch 3300:	Loss 1.4220	TrainAcc 0.6668	ValidAcc 0.6675	TestAcc 0.6604	BestValid 0.6679
	Epoch 3350:	Loss 1.4199	TrainAcc 0.6666	ValidAcc 0.6676	TestAcc 0.6616	BestValid 0.6679
	Epoch 3400:	Loss 1.4214	TrainAcc 0.6670	ValidAcc 0.6684	TestAcc 0.6636	BestValid 0.6684
	Epoch 3450:	Loss 1.4170	TrainAcc 0.6682	ValidAcc 0.6694	TestAcc 0.6635	BestValid 0.6694
	Epoch 3500:	Loss 1.4135	TrainAcc 0.6690	ValidAcc 0.6693	TestAcc 0.6629	BestValid 0.6694
	Epoch 3550:	Loss 1.4102	TrainAcc 0.6690	ValidAcc 0.6697	TestAcc 0.6637	BestValid 0.6697
	Epoch 3600:	Loss 1.4062	TrainAcc 0.6699	ValidAcc 0.6715	TestAcc 0.6664	BestValid 0.6715
	Epoch 3650:	Loss 1.4050	TrainAcc 0.6695	ValidAcc 0.6706	TestAcc 0.6657	BestValid 0.6715
	Epoch 3700:	Loss 1.4049	TrainAcc 0.6706	ValidAcc 0.6723	TestAcc 0.6673	BestValid 0.6723
	Epoch 3750:	Loss 1.4033	TrainAcc 0.6711	ValidAcc 0.6716	TestAcc 0.6665	BestValid 0.6723
	Epoch 3800:	Loss 1.4055	TrainAcc 0.6709	ValidAcc 0.6720	TestAcc 0.6673	BestValid 0.6723
	Epoch 3850:	Loss 1.3997	TrainAcc 0.6720	ValidAcc 0.6739	TestAcc 0.6694	BestValid 0.6739
	Epoch 3900:	Loss 1.3939	TrainAcc 0.6727	ValidAcc 0.6746	TestAcc 0.6713	BestValid 0.6746
	Epoch 3950:	Loss 1.3959	TrainAcc 0.6722	ValidAcc 0.6735	TestAcc 0.6686	BestValid 0.6746
	Epoch 4000:	Loss 1.3964	TrainAcc 0.6736	ValidAcc 0.6767	TestAcc 0.6723	BestValid 0.6767
	Epoch 4050:	Loss 1.3880	TrainAcc 0.6737	ValidAcc 0.6750	TestAcc 0.6710	BestValid 0.6767
	Epoch 4100:	Loss 1.3889	TrainAcc 0.6743	ValidAcc 0.6761	TestAcc 0.6716	BestValid 0.6767
	Epoch 4150:	Loss 1.3914	TrainAcc 0.6741	ValidAcc 0.6760	TestAcc 0.6717	BestValid 0.6767
	Epoch 4200:	Loss 1.3880	TrainAcc 0.6750	ValidAcc 0.6760	TestAcc 0.6721	BestValid 0.6767
	Epoch 4250:	Loss 1.3872	TrainAcc 0.6753	ValidAcc 0.6759	TestAcc 0.6726	BestValid 0.6767
	Epoch 4300:	Loss 1.3866	TrainAcc 0.6757	ValidAcc 0.6772	TestAcc 0.6730	BestValid 0.6772
	Epoch 4350:	Loss 1.3785	TrainAcc 0.6759	ValidAcc 0.6780	TestAcc 0.6739	BestValid 0.6780
	Epoch 4400:	Loss 1.3777	TrainAcc 0.6769	ValidAcc 0.6768	TestAcc 0.6726	BestValid 0.6780
	Epoch 4450:	Loss 1.3783	TrainAcc 0.6767	ValidAcc 0.6776	TestAcc 0.6745	BestValid 0.6780
	Epoch 4500:	Loss 1.3772	TrainAcc 0.6774	ValidAcc 0.6792	TestAcc 0.6755	BestValid 0.6792
	Epoch 4550:	Loss 1.3792	TrainAcc 0.6779	ValidAcc 0.6798	TestAcc 0.6759	BestValid 0.6798
	Epoch 4600:	Loss 1.3723	TrainAcc 0.6782	ValidAcc 0.6798	TestAcc 0.6767	BestValid 0.6798
	Epoch 4650:	Loss 1.3782	TrainAcc 0.6785	ValidAcc 0.6808	TestAcc 0.6775	BestValid 0.6808
	Epoch 4700:	Loss 1.3734	TrainAcc 0.6792	ValidAcc 0.6805	TestAcc 0.6769	BestValid 0.6808
	Epoch 4750:	Loss 1.3656	TrainAcc 0.6792	ValidAcc 0.6802	TestAcc 0.6776	BestValid 0.6808
	Epoch 4800:	Loss 1.3748	TrainAcc 0.6796	ValidAcc 0.6805	TestAcc 0.6768	BestValid 0.6808
	Epoch 4850:	Loss 1.3675	TrainAcc 0.6800	ValidAcc 0.6816	TestAcc 0.6759	BestValid 0.6816
	Epoch 4900:	Loss 1.3638	TrainAcc 0.6803	ValidAcc 0.6817	TestAcc 0.6786	BestValid 0.6817
	Epoch 4950:	Loss 1.3643	TrainAcc 0.6802	ValidAcc 0.6820	TestAcc 0.6776	BestValid 0.6820
	Epoch 5000:	Loss 1.3657	TrainAcc 0.6809	ValidAcc 0.6823	TestAcc 0.6778	BestValid 0.6823
****** Epoch Time (Excluding Evaluation Cost): 0.152 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 24.308 ms (Max: 26.317, Min: 19.830, Sum: 194.461)
Cluster-Wide Average, Compute: 67.351 ms (Max: 74.706, Min: 62.860, Sum: 538.812)
Cluster-Wide Average, Communication-Layer: 18.426 ms (Max: 21.246, Min: 14.915, Sum: 147.412)
Cluster-Wide Average, Bubble-Imbalance: 6.298 ms (Max: 10.162, Min: 1.321, Sum: 50.383)
Cluster-Wide Average, Communication-Graph: 29.893 ms (Max: 31.726, Min: 28.878, Sum: 239.141)
Cluster-Wide Average, Optimization: 1.734 ms (Max: 2.370, Min: 1.278, Sum: 13.870)
Cluster-Wide Average, Others: 4.396 ms (Max: 9.906, Min: 2.547, Sum: 35.165)
****** Breakdown Sum: 152.405 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.768 GB (Max: 6.005, Min: 4.493, Sum: 38.144)
Cluster-Wide Average, Graph-Level Communication Throughput: 94.261 Gbps (Max: 100.634, Min: 85.700, Sum: 754.089)
Cluster-Wide Average, Layer-Level Communication Throughput: 43.098 Gbps (Max: 55.282, Min: 31.282, Sum: 344.780)
Layer-level communication (cluster-wide, per-epoch): 0.757 GB
Graph-level communication (cluster-wide, per-epoch): 1.952 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 2.712 GB
****** Accuracy Results ******
Highest valid_acc: 0.6823
Target test_acc: 0.6778
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
