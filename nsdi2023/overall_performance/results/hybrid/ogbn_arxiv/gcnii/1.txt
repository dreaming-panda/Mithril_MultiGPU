Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8

Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
DONE MPI INITInitialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7

Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.069 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.050 seconds.
        It takes 0.053 seconds.
        It takes 0.049 seconds.
        It takes 0.056 seconds.
        It takes 0.051 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.027 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 5292
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.923 Gbps (per GPU), 487.383 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.630 Gbps (per GPU), 485.038 Gbps (aggregated)
The layer-level communication performance: 60.633 Gbps (per GPU), 485.061 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.330 Gbps (per GPU), 482.643 Gbps (aggregated)
The layer-level communication performance: 60.090 Gbps (per GPU), 480.719 Gbps (aggregated)
The layer-level communication performance: 60.041 Gbps (per GPU), 480.329 Gbps (aggregated)
The layer-level communication performance: 60.004 Gbps (per GPU), 480.029 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.360 Gbps (per GPU), 482.878 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.322 Gbps (per GPU), 1274.574 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.325 Gbps (per GPU), 1274.598 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.331 Gbps (per GPU), 1274.647 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.316 Gbps (per GPU), 1274.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.334 Gbps (per GPU), 1274.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.325 Gbps (per GPU), 1274.598 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.334 Gbps (per GPU), 1274.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.331 Gbps (per GPU), 1274.647 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.607 Gbps (per GPU), 836.859 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.604 Gbps (per GPU), 836.831 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.607 Gbps (per GPU), 836.859 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.605 Gbps (per GPU), 836.838 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.608 Gbps (per GPU), 836.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.605 Gbps (per GPU), 836.838 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.604 Gbps (per GPU), 836.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.602 Gbps (per GPU), 836.818 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.122 Gbps (per GPU), 280.976 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.117 Gbps (per GPU), 280.934 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.122 Gbps (per GPU), 280.972 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.116 Gbps (per GPU), 280.931 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.112 Gbps (per GPU), 280.898 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.117 Gbps (per GPU), 280.935 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.45ms  0.60ms  1.86  5.55K  0.06M
 chk_1  0.33ms  0.46ms  0.61ms  1.88  5.75K  0.05M
 chk_2  0.32ms  0.46ms  0.61ms  1.91  5.20K  0.07M
 chk_3  0.32ms  0.46ms  0.61ms  1.89  5.57K  0.06M
 chk_4  0.31ms  0.45ms  0.60ms  1.90  5.05K  0.08M
 chk_5  0.31ms  0.48ms  0.62ms  2.02  4.73K  0.11M
 chk_6  0.32ms  0.45ms  0.60ms  1.89  4.98K  0.08M
 chk_7  0.31ms  0.46ms  0.60ms  1.92  5.01K  0.09M
 chk_8  0.32ms  0.45ms  0.61ms  1.88  5.73K  0.05M
 chk_9  0.30ms  0.45ms  0.59ms  1.96  4.54K  0.11M
chk_10  0.32ms  0.46ms  0.61ms  1.90  5.36K  0.07M
chk_11  0.32ms  0.46ms  0.61ms  1.90  5.39K  0.08M
chk_12  0.33ms  0.46ms  0.61ms  1.87  5.77K  0.05M
chk_13  0.32ms  0.45ms  0.60ms  1.89  5.43K  0.06M
chk_14  0.32ms  0.45ms  0.60ms  1.87  5.46K  0.06M
chk_15  0.33ms  0.45ms  0.61ms  1.86  5.88K  0.04M
chk_16  0.32ms  0.46ms  0.61ms  1.89  5.50K  0.06M
chk_17  0.31ms  0.47ms  0.62ms  1.99  4.86K  0.09M
chk_18  0.32ms  0.48ms  0.63ms  1.97  5.39K  0.07M
chk_19  0.32ms  0.46ms  0.61ms  1.92  5.20K  0.07M
chk_20  0.32ms  0.45ms  0.73ms  2.27  5.51K  0.06M
chk_21  0.33ms  0.45ms  0.60ms  1.85  5.81K  0.05M
chk_22  0.32ms  0.45ms  0.60ms  1.89  5.32K  0.07M
chk_23  0.32ms  0.47ms  0.62ms  1.94  5.39K  0.07M
chk_24  0.30ms  0.46ms  0.60ms  1.98  4.62K  0.11M
chk_25  0.32ms  0.46ms  0.60ms  1.92  5.04K  0.08M
chk_26  0.30ms  0.45ms  0.59ms  1.96  4.55K  0.11M
chk_27  0.32ms  0.44ms  0.59ms  1.87  5.30K  0.06M
chk_28  0.32ms  0.45ms  0.60ms  1.87  5.58K  0.06M
chk_29  0.31ms  0.47ms  0.61ms  1.97  4.98K  0.09M
chk_30  0.32ms  0.45ms  0.61ms  1.89  5.50K  0.07M
chk_31  0.32ms  0.46ms  0.60ms  1.89  5.38K  0.07M
   Avg  0.32  0.46  0.61
   Max  0.33  0.48  0.73
   Min  0.30  0.44  0.59
 Ratio  1.08  1.09  1.23
   Var  0.00  0.00  0.00
Profiling takes 0.675 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 85420
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 7, starting model training...
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 85420
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 327544
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.8794	TrainAcc 0.0203	ValidAcc 0.0188	TestAcc 0.0165	BestValid 0.0188
	Epoch 50:	Loss 3.1204	TrainAcc 0.1813	ValidAcc 0.0794	TestAcc 0.0603	BestValid 0.0794
	Epoch 100:	Loss 2.7403	TrainAcc 0.3395	ValidAcc 0.3330	TestAcc 0.2984	BestValid 0.3330
	Epoch 150:	Loss 2.4727	TrainAcc 0.4077	ValidAcc 0.3815	TestAcc 0.3502	BestValid 0.3815
	Epoch 200:	Loss 2.3002	TrainAcc 0.4441	ValidAcc 0.4199	TestAcc 0.3935	BestValid 0.4199
	Epoch 250:	Loss 2.1915	TrainAcc 0.4778	ValidAcc 0.4659	TestAcc 0.4454	BestValid 0.4659
	Epoch 300:	Loss 2.0959	TrainAcc 0.5041	ValidAcc 0.4985	TestAcc 0.4818	BestValid 0.4985
	Epoch 350:	Loss 2.0436	TrainAcc 0.5240	ValidAcc 0.5240	TestAcc 0.5075	BestValid 0.5240
	Epoch 400:	Loss 1.9771	TrainAcc 0.5361	ValidAcc 0.5408	TestAcc 0.5283	BestValid 0.5408
	Epoch 450:	Loss 1.9302	TrainAcc 0.5462	ValidAcc 0.5519	TestAcc 0.5401	BestValid 0.5519
	Epoch 500:	Loss 1.8969	TrainAcc 0.5541	ValidAcc 0.5631	TestAcc 0.5539	BestValid 0.5631
	Epoch 550:	Loss 1.8611	TrainAcc 0.5639	ValidAcc 0.5747	TestAcc 0.5663	BestValid 0.5747
	Epoch 600:	Loss 1.8334	TrainAcc 0.5704	ValidAcc 0.5799	TestAcc 0.5699	BestValid 0.5799
	Epoch 650:	Loss 1.8045	TrainAcc 0.5795	ValidAcc 0.5919	TestAcc 0.5837	BestValid 0.5919
	Epoch 700:	Loss 1.7768	TrainAcc 0.5853	ValidAcc 0.5965	TestAcc 0.5861	BestValid 0.5965
	Epoch 750:	Loss 1.7459	TrainAcc 0.5917	ValidAcc 0.6032	TestAcc 0.5932	BestValid 0.6032
	Epoch 800:	Loss 1.7228	TrainAcc 0.5970	ValidAcc 0.6102	TestAcc 0.5981	BestValid 0.6102
	Epoch 850:	Loss 1.7111	TrainAcc 0.6013	ValidAcc 0.6155	TestAcc 0.6034	BestValid 0.6155
	Epoch 900:	Loss 1.6991	TrainAcc 0.6043	ValidAcc 0.6172	TestAcc 0.6056	BestValid 0.6172
	Epoch 950:	Loss 1.6820	TrainAcc 0.6073	ValidAcc 0.6201	TestAcc 0.6079	BestValid 0.6201
	Epoch 1000:	Loss 1.6654	TrainAcc 0.6108	ValidAcc 0.6255	TestAcc 0.6138	BestValid 0.6255
	Epoch 1050:	Loss 1.6521	TrainAcc 0.6136	ValidAcc 0.6273	TestAcc 0.6165	BestValid 0.6273
	Epoch 1100:	Loss 1.6412	TrainAcc 0.6160	ValidAcc 0.6286	TestAcc 0.6177	BestValid 0.6286
	Epoch 1150:	Loss 1.6282	TrainAcc 0.6184	ValidAcc 0.6304	TestAcc 0.6202	BestValid 0.6304
	Epoch 1200:	Loss 1.6180	TrainAcc 0.6200	ValidAcc 0.6319	TestAcc 0.6218	BestValid 0.6319
	Epoch 1250:	Loss 1.6079	TrainAcc 0.6226	ValidAcc 0.6342	TestAcc 0.6235	BestValid 0.6342
	Epoch 1300:	Loss 1.6043	TrainAcc 0.6242	ValidAcc 0.6351	TestAcc 0.6247	BestValid 0.6351
	Epoch 1350:	Loss 1.5883	TrainAcc 0.6256	ValidAcc 0.6362	TestAcc 0.6265	BestValid 0.6362
	Epoch 1400:	Loss 1.5808	TrainAcc 0.6280	ValidAcc 0.6379	TestAcc 0.6286	BestValid 0.6379
	Epoch 1450:	Loss 1.5807	TrainAcc 0.6294	ValidAcc 0.6396	TestAcc 0.6296	BestValid 0.6396
	Epoch 1500:	Loss 1.5661	TrainAcc 0.6312	ValidAcc 0.6408	TestAcc 0.6321	BestValid 0.6408
	Epoch 1550:	Loss 1.5636	TrainAcc 0.6332	ValidAcc 0.6415	TestAcc 0.6320	BestValid 0.6415
	Epoch 1600:	Loss 1.5562	TrainAcc 0.6347	ValidAcc 0.6428	TestAcc 0.6344	BestValid 0.6428
	Epoch 1650:	Loss 1.5474	TrainAcc 0.6358	ValidAcc 0.6436	TestAcc 0.6351	BestValid 0.6436
	Epoch 1700:	Loss 1.5363	TrainAcc 0.6370	ValidAcc 0.6433	TestAcc 0.6350	BestValid 0.6436
	Epoch 1750:	Loss 1.5378	TrainAcc 0.6391	ValidAcc 0.6455	TestAcc 0.6371	BestValid 0.6455
	Epoch 1800:	Loss 1.5284	TrainAcc 0.6395	ValidAcc 0.6454	TestAcc 0.6359	BestValid 0.6455
	Epoch 1850:	Loss 1.5215	TrainAcc 0.6419	ValidAcc 0.6473	TestAcc 0.6392	BestValid 0.6473
	Epoch 1900:	Loss 1.5151	TrainAcc 0.6428	ValidAcc 0.6481	TestAcc 0.6400	BestValid 0.6481
	Epoch 1950:	Loss 1.5113	TrainAcc 0.6440	ValidAcc 0.6490	TestAcc 0.6412	BestValid 0.6490
	Epoch 2000:	Loss 1.5107	TrainAcc 0.6457	ValidAcc 0.6518	TestAcc 0.6425	BestValid 0.6518
	Epoch 2050:	Loss 1.5014	TrainAcc 0.6462	ValidAcc 0.6510	TestAcc 0.6431	BestValid 0.6518
	Epoch 2100:	Loss 1.5003	TrainAcc 0.6474	ValidAcc 0.6514	TestAcc 0.6433	BestValid 0.6518
	Epoch 2150:	Loss 1.4981	TrainAcc 0.6483	ValidAcc 0.6530	TestAcc 0.6448	BestValid 0.6530
	Epoch 2200:	Loss 1.4975	TrainAcc 0.6499	ValidAcc 0.6544	TestAcc 0.6454	BestValid 0.6544
	Epoch 2250:	Loss 1.4901	TrainAcc 0.6506	ValidAcc 0.6543	TestAcc 0.6458	BestValid 0.6544
	Epoch 2300:	Loss 1.4814	TrainAcc 0.6512	ValidAcc 0.6554	TestAcc 0.6481	BestValid 0.6554
	Epoch 2350:	Loss 1.4835	TrainAcc 0.6520	ValidAcc 0.6555	TestAcc 0.6462	BestValid 0.6555
	Epoch 2400:	Loss 1.4761	TrainAcc 0.6527	ValidAcc 0.6566	TestAcc 0.6486	BestValid 0.6566
	Epoch 2450:	Loss 1.4692	TrainAcc 0.6532	ValidAcc 0.6570	TestAcc 0.6486	BestValid 0.6570
	Epoch 2500:	Loss 1.4643	TrainAcc 0.6545	ValidAcc 0.6578	TestAcc 0.6493	BestValid 0.6578
	Epoch 2550:	Loss 1.4666	TrainAcc 0.6557	ValidAcc 0.6588	TestAcc 0.6508	BestValid 0.6588
	Epoch 2600:	Loss 1.4608	TrainAcc 0.6560	ValidAcc 0.6597	TestAcc 0.6516	BestValid 0.6597
	Epoch 2650:	Loss 1.4606	TrainAcc 0.6570	ValidAcc 0.6602	TestAcc 0.6528	BestValid 0.6602
	Epoch 2700:	Loss 1.4617	TrainAcc 0.6584	ValidAcc 0.6618	TestAcc 0.6540	BestValid 0.6618
	Epoch 2750:	Loss 1.4525	TrainAcc 0.6589	ValidAcc 0.6625	TestAcc 0.6549	BestValid 0.6625
	Epoch 2800:	Loss 1.4509	TrainAcc 0.6597	ValidAcc 0.6618	TestAcc 0.6540	BestValid 0.6625
	Epoch 2850:	Loss 1.4488	TrainAcc 0.6605	ValidAcc 0.6639	TestAcc 0.6558	BestValid 0.6639
	Epoch 2900:	Loss 1.4413	TrainAcc 0.6610	ValidAcc 0.6635	TestAcc 0.6558	BestValid 0.6639
	Epoch 2950:	Loss 1.4484	TrainAcc 0.6624	ValidAcc 0.6641	TestAcc 0.6554	BestValid 0.6641
	Epoch 3000:	Loss 1.4360	TrainAcc 0.6618	ValidAcc 0.6644	TestAcc 0.6567	BestValid 0.6644
	Epoch 3050:	Loss 1.4328	TrainAcc 0.6628	ValidAcc 0.6649	TestAcc 0.6562	BestValid 0.6649
	Epoch 3100:	Loss 1.4413	TrainAcc 0.6636	ValidAcc 0.6669	TestAcc 0.6595	BestValid 0.6669
	Epoch 3150:	Loss 1.4342	TrainAcc 0.6643	ValidAcc 0.6659	TestAcc 0.6581	BestValid 0.6669
	Epoch 3200:	Loss 1.4266	TrainAcc 0.6646	ValidAcc 0.6674	TestAcc 0.6602	BestValid 0.6674
	Epoch 3250:	Loss 1.4231	TrainAcc 0.6654	ValidAcc 0.6669	TestAcc 0.6583	BestValid 0.6674
	Epoch 3300:	Loss 1.4257	TrainAcc 0.6661	ValidAcc 0.6690	TestAcc 0.6615	BestValid 0.6690
	Epoch 3350:	Loss 1.4221	TrainAcc 0.6669	ValidAcc 0.6694	TestAcc 0.6622	BestValid 0.6694
	Epoch 3400:	Loss 1.4181	TrainAcc 0.6667	ValidAcc 0.6697	TestAcc 0.6613	BestValid 0.6697
	Epoch 3450:	Loss 1.4245	TrainAcc 0.6678	ValidAcc 0.6708	TestAcc 0.6636	BestValid 0.6708
	Epoch 3500:	Loss 1.4159	TrainAcc 0.6684	ValidAcc 0.6725	TestAcc 0.6655	BestValid 0.6725
	Epoch 3550:	Loss 1.4107	TrainAcc 0.6694	ValidAcc 0.6716	TestAcc 0.6647	BestValid 0.6725
	Epoch 3600:	Loss 1.4129	TrainAcc 0.6696	ValidAcc 0.6716	TestAcc 0.6644	BestValid 0.6725
	Epoch 3650:	Loss 1.4149	TrainAcc 0.6707	ValidAcc 0.6727	TestAcc 0.6656	BestValid 0.6727
	Epoch 3700:	Loss 1.4065	TrainAcc 0.6692	ValidAcc 0.6719	TestAcc 0.6643	BestValid 0.6727
	Epoch 3750:	Loss 1.4064	TrainAcc 0.6700	ValidAcc 0.6740	TestAcc 0.6662	BestValid 0.6740
	Epoch 3800:	Loss 1.4016	TrainAcc 0.6689	ValidAcc 0.6714	TestAcc 0.6643	BestValid 0.6740
	Epoch 3850:	Loss 1.4018	TrainAcc 0.6712	ValidAcc 0.6735	TestAcc 0.6656	BestValid 0.6740
	Epoch 3900:	Loss 1.4016	TrainAcc 0.6727	ValidAcc 0.6756	TestAcc 0.6685	BestValid 0.6756
	Epoch 3950:	Loss 1.3905	TrainAcc 0.6715	ValidAcc 0.6739	TestAcc 0.6668	BestValid 0.6756
	Epoch 4000:	Loss 1.3974	TrainAcc 0.6729	ValidAcc 0.6748	TestAcc 0.6675	BestValid 0.6756
	Epoch 4050:	Loss 1.3960	TrainAcc 0.6732	ValidAcc 0.6751	TestAcc 0.6680	BestValid 0.6756
	Epoch 4100:	Loss 1.3930	TrainAcc 0.6746	ValidAcc 0.6765	TestAcc 0.6680	BestValid 0.6765
	Epoch 4150:	Loss 1.3926	TrainAcc 0.6739	ValidAcc 0.6767	TestAcc 0.6699	BestValid 0.6767
	Epoch 4200:	Loss 1.3951	TrainAcc 0.6745	ValidAcc 0.6763	TestAcc 0.6690	BestValid 0.6767
	Epoch 4250:	Loss 1.3873	TrainAcc 0.6749	ValidAcc 0.6775	TestAcc 0.6704	BestValid 0.6775
	Epoch 4300:	Loss 1.3836	TrainAcc 0.6746	ValidAcc 0.6772	TestAcc 0.6700	BestValid 0.6775
	Epoch 4350:	Loss 1.3877	TrainAcc 0.6758	ValidAcc 0.6784	TestAcc 0.6726	BestValid 0.6784
	Epoch 4400:	Loss 1.3947	TrainAcc 0.6763	ValidAcc 0.6784	TestAcc 0.6709	BestValid 0.6784
	Epoch 4450:	Loss 1.3841	TrainAcc 0.6764	ValidAcc 0.6792	TestAcc 0.6731	BestValid 0.6792
	Epoch 4500:	Loss 1.3786	TrainAcc 0.6769	ValidAcc 0.6793	TestAcc 0.6723	BestValid 0.6793
	Epoch 4550:	Loss 1.3762	TrainAcc 0.6780	ValidAcc 0.6792	TestAcc 0.6715	BestValid 0.6793
	Epoch 4600:	Loss 1.3686	TrainAcc 0.6785	ValidAcc 0.6800	TestAcc 0.6729	BestValid 0.6800
	Epoch 4650:	Loss 1.3746	TrainAcc 0.6765	ValidAcc 0.6778	TestAcc 0.6704	BestValid 0.6800
	Epoch 4700:	Loss 1.3718	TrainAcc 0.6777	ValidAcc 0.6803	TestAcc 0.6737	BestValid 0.6803
	Epoch 4750:	Loss 1.3768	TrainAcc 0.6785	ValidAcc 0.6821	TestAcc 0.6772	BestValid 0.6821
	Epoch 4800:	Loss 1.3705	TrainAcc 0.6794	ValidAcc 0.6817	TestAcc 0.6756	BestValid 0.6821
	Epoch 4850:	Loss 1.3615	TrainAcc 0.6781	ValidAcc 0.6817	TestAcc 0.6765	BestValid 0.6821
	Epoch 4900:	Loss 1.3663	TrainAcc 0.6793	ValidAcc 0.6811	TestAcc 0.6740	BestValid 0.6821
	Epoch 4950:	Loss 1.3616	TrainAcc 0.6788	ValidAcc 0.6810	TestAcc 0.6747	BestValid 0.6821
	Epoch 5000:	Loss 1.3634	TrainAcc 0.6801	ValidAcc 0.6832	TestAcc 0.6767	BestValid 0.6832
****** Epoch Time (Excluding Evaluation Cost): 0.151 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 24.055 ms (Max: 25.727, Min: 20.072, Sum: 192.441)
Cluster-Wide Average, Compute: 67.258 ms (Max: 72.457, Min: 62.484, Sum: 538.067)
Cluster-Wide Average, Communication-Layer: 18.364 ms (Max: 21.381, Min: 14.818, Sum: 146.912)
Cluster-Wide Average, Bubble-Imbalance: 5.601 ms (Max: 8.644, Min: 2.904, Sum: 44.811)
Cluster-Wide Average, Communication-Graph: 29.818 ms (Max: 30.999, Min: 28.590, Sum: 238.545)
Cluster-Wide Average, Optimization: 1.715 ms (Max: 2.357, Min: 1.284, Sum: 13.719)
Cluster-Wide Average, Others: 4.387 ms (Max: 9.856, Min: 2.538, Sum: 35.097)
****** Breakdown Sum: 151.199 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.768 GB (Max: 6.005, Min: 4.493, Sum: 38.147)
Cluster-Wide Average, Graph-Level Communication Throughput: 94.446 Gbps (Max: 99.678, Min: 90.100, Sum: 755.568)
Cluster-Wide Average, Layer-Level Communication Throughput: 43.284 Gbps (Max: 56.135, Min: 31.132, Sum: 346.273)
Layer-level communication (cluster-wide, per-epoch): 0.757 GB
Graph-level communication (cluster-wide, per-epoch): 1.952 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 2.712 GB
****** Accuracy Results ******
Highest valid_acc: 0.6832
Target test_acc: 0.6767
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
