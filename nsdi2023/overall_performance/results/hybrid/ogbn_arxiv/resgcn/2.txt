Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.070 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.051 seconds.
        It takes 0.049 seconds.
        It takes 0.049 seconds.
        It takes 0.051 seconds.
        It takes 0.052 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.050 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 61.317 Gbps (per GPU), 490.538 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 61.026 Gbps (per GPU), 488.209 Gbps (aggregated)
The layer-level communication performance: 61.017 Gbps (per GPU), 488.134 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.774 Gbps (per GPU), 486.194 Gbps (aggregated)
The layer-level communication performance: 60.741 Gbps (per GPU), 485.925 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.531 Gbps (per GPU), 484.248 Gbps (aggregated)
The layer-level communication performance: 60.481 Gbps (per GPU), 483.844 Gbps (aggregated)
The layer-level communication performance: 60.450 Gbps (per GPU), 483.598 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.300 Gbps (per GPU), 1266.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.291 Gbps (per GPU), 1266.324 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.294 Gbps (per GPU), 1266.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.297 Gbps (per GPU), 1266.372 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.300 Gbps (per GPU), 1266.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.284 Gbps (per GPU), 1266.274 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.303 Gbps (per GPU), 1266.420 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.294 Gbps (per GPU), 1266.348 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.139 Gbps (per GPU), 833.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.143 Gbps (per GPU), 833.147 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.112 Gbps (per GPU), 832.899 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.144 Gbps (per GPU), 833.154 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.078 Gbps (per GPU), 832.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.141 Gbps (per GPU), 833.126 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.087 Gbps (per GPU), 832.699 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.083 Gbps (per GPU), 832.664 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.610 Gbps (per GPU), 316.879 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.610 Gbps (per GPU), 316.884 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.608 Gbps (per GPU), 316.867 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.607 Gbps (per GPU), 316.854 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.607 Gbps (per GPU), 316.859 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.604 Gbps (per GPU), 316.830 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.606 Gbps (per GPU), 316.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.602 Gbps (per GPU), 316.815 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.49ms  1.06ms  1.83ms  3.74  5.55K  0.06M
 chk_1  0.51ms  1.07ms  2.16ms  4.20  5.75K  0.05M
 chk_2  0.50ms  1.05ms  1.81ms  3.63  5.20K  0.07M
 chk_3  0.50ms  1.07ms  1.84ms  3.65  5.57K  0.06M
 chk_4  0.50ms  1.03ms  1.77ms  3.57  5.05K  0.08M
 chk_5  0.52ms  1.05ms  1.77ms  3.41  4.73K  0.11M
 chk_6  0.49ms  1.03ms  1.76ms  3.57  4.98K  0.08M
 chk_7  0.50ms  1.03ms  1.77ms  3.55  5.01K  0.09M
 chk_8  0.50ms  1.18ms  1.85ms  3.73  5.73K  0.05M
 chk_9  0.50ms  1.01ms  1.72ms  3.47  4.54K  0.11M
chk_10  0.50ms  1.06ms  1.82ms  3.63  5.36K  0.07M
chk_11  0.50ms  1.06ms  1.82ms  3.62  5.39K  0.08M
chk_12  0.49ms  1.07ms  1.85ms  3.74  5.77K  0.05M
chk_13  0.50ms  1.05ms  1.81ms  3.64  5.43K  0.06M
chk_14  0.49ms  1.05ms  1.81ms  3.67  5.46K  0.06M
chk_15  0.50ms  1.07ms  1.86ms  3.74  5.88K  0.04M
chk_16  0.50ms  1.05ms  1.82ms  3.63  5.50K  0.06M
chk_17  0.51ms  1.04ms  1.77ms  3.46  4.86K  0.09M
chk_18  0.52ms  1.08ms  1.83ms  3.52  5.39K  0.07M
chk_19  0.50ms  1.04ms  1.79ms  3.57  5.20K  0.07M
chk_20  0.49ms  1.05ms  1.82ms  3.69  5.51K  0.06M
chk_21  0.49ms  1.06ms  1.84ms  3.78  5.81K  0.05M
chk_22  0.50ms  1.05ms  1.81ms  3.65  5.32K  0.07M
chk_23  0.51ms  1.07ms  1.83ms  3.57  5.39K  0.07M
chk_24  0.50ms  1.02ms  1.74ms  3.47  4.62K  0.11M
chk_25  0.50ms  1.03ms  1.77ms  3.53  5.04K  0.08M
chk_26  0.49ms  1.01ms  1.72ms  3.48  4.55K  0.11M
chk_27  0.49ms  1.04ms  1.79ms  3.67  5.30K  0.06M
chk_28  0.49ms  1.06ms  1.82ms  3.70  5.58K  0.06M
chk_29  0.51ms  1.04ms  1.77ms  3.50  4.98K  0.09M
chk_30  0.76ms  1.06ms  1.82ms  2.40  5.50K  0.07M
chk_31  0.50ms  1.05ms  1.82ms  3.64  5.38K  0.07M
   Avg  0.51  1.05  1.81
   Max  0.76  1.18  2.16
   Min  0.49  1.01  1.72
 Ratio  1.56  1.17  1.26
   Var  0.00  0.00  0.01
Profiling takes 1.350 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 100)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 308)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [204, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [100, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 7, starting model training...
Num Stages: 4 / 4
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 204)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [308, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 85420
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [308, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 100)...
+++++++++ Node 0 initializing the weights for op[0, 100)...
+++++++++ Node 4 initializing the weights for op[204, 308)...
+++++++++ Node 2 initializing the weights for op[100, 204)...
+++++++++ Node 5 initializing the weights for op[204, 308)...
+++++++++ Node 3 initializing the weights for op[100, 204)...
+++++++++ Node 7 initializing the weights for op[308, 421)...
+++++++++ Node 6 initializing the weights for op[308, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 327544
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.1702	TrainAcc 0.1788	ValidAcc 0.0765	TestAcc 0.0589	BestValid 0.0765
	Epoch 50:	Loss 2.0290	TrainAcc 0.5108	ValidAcc 0.5470	TestAcc 0.5454	BestValid 0.5470
	Epoch 100:	Loss 1.6437	TrainAcc 0.6042	ValidAcc 0.6189	TestAcc 0.6149	BestValid 0.6189
	Epoch 150:	Loss 1.4506	TrainAcc 0.6482	ValidAcc 0.6546	TestAcc 0.6462	BestValid 0.6546
	Epoch 200:	Loss 1.3439	TrainAcc 0.6673	ValidAcc 0.6747	TestAcc 0.6676	BestValid 0.6747
	Epoch 250:	Loss 1.2828	TrainAcc 0.6863	ValidAcc 0.6864	TestAcc 0.6755	BestValid 0.6864
	Epoch 300:	Loss 1.2327	TrainAcc 0.6939	ValidAcc 0.6930	TestAcc 0.6823	BestValid 0.6930
	Epoch 350:	Loss 1.2019	TrainAcc 0.7019	ValidAcc 0.7019	TestAcc 0.6916	BestValid 0.7019
	Epoch 400:	Loss 1.1803	TrainAcc 0.7057	ValidAcc 0.7025	TestAcc 0.6952	BestValid 0.7025
	Epoch 450:	Loss 1.1596	TrainAcc 0.7115	ValidAcc 0.7069	TestAcc 0.6970	BestValid 0.7069
	Epoch 500:	Loss 1.1434	TrainAcc 0.7140	ValidAcc 0.7100	TestAcc 0.7005	BestValid 0.7100
	Epoch 550:	Loss 1.1329	TrainAcc 0.7180	ValidAcc 0.7125	TestAcc 0.6994	BestValid 0.7125
	Epoch 600:	Loss 1.1197	TrainAcc 0.7188	ValidAcc 0.7133	TestAcc 0.7076	BestValid 0.7133
	Epoch 650:	Loss 1.1100	TrainAcc 0.7203	ValidAcc 0.7125	TestAcc 0.7011	BestValid 0.7133
	Epoch 700:	Loss 1.0978	TrainAcc 0.7237	ValidAcc 0.7152	TestAcc 0.7020	BestValid 0.7152
	Epoch 750:	Loss 1.0926	TrainAcc 0.7255	ValidAcc 0.7164	TestAcc 0.7062	BestValid 0.7164
	Epoch 800:	Loss 1.0858	TrainAcc 0.7267	ValidAcc 0.7170	TestAcc 0.7046	BestValid 0.7170
	Epoch 850:	Loss 1.0766	TrainAcc 0.7282	ValidAcc 0.7170	TestAcc 0.7066	BestValid 0.7170
	Epoch 900:	Loss 1.0682	TrainAcc 0.7290	ValidAcc 0.7173	TestAcc 0.7064	BestValid 0.7173
	Epoch 950:	Loss 1.0605	TrainAcc 0.7306	ValidAcc 0.7164	TestAcc 0.7025	BestValid 0.7173
	Epoch 1000:	Loss 1.0619	TrainAcc 0.7308	ValidAcc 0.7181	TestAcc 0.7082	BestValid 0.7181
	Epoch 1050:	Loss 1.0540	TrainAcc 0.7328	ValidAcc 0.7196	TestAcc 0.7090	BestValid 0.7196
	Epoch 1100:	Loss 1.0466	TrainAcc 0.7348	ValidAcc 0.7212	TestAcc 0.7101	BestValid 0.7212
	Epoch 1150:	Loss 1.0417	TrainAcc 0.7350	ValidAcc 0.7200	TestAcc 0.7074	BestValid 0.7212
	Epoch 1200:	Loss 1.0423	TrainAcc 0.7365	ValidAcc 0.7195	TestAcc 0.7067	BestValid 0.7212
	Epoch 1250:	Loss 1.0328	TrainAcc 0.7366	ValidAcc 0.7204	TestAcc 0.7074	BestValid 0.7212
	Epoch 1300:	Loss 1.0311	TrainAcc 0.7370	ValidAcc 0.7201	TestAcc 0.7094	BestValid 0.7212
	Epoch 1350:	Loss 1.0234	TrainAcc 0.7388	ValidAcc 0.7216	TestAcc 0.7085	BestValid 0.7216
	Epoch 1400:	Loss 1.0201	TrainAcc 0.7391	ValidAcc 0.7221	TestAcc 0.7122	BestValid 0.7221
	Epoch 1450:	Loss 1.0198	TrainAcc 0.7399	ValidAcc 0.7224	TestAcc 0.7147	BestValid 0.7224
	Epoch 1500:	Loss 1.0173	TrainAcc 0.7408	ValidAcc 0.7216	TestAcc 0.7086	BestValid 0.7224
	Epoch 1550:	Loss 1.0100	TrainAcc 0.7408	ValidAcc 0.7235	TestAcc 0.7145	BestValid 0.7235
	Epoch 1600:	Loss 1.0082	TrainAcc 0.7412	ValidAcc 0.7230	TestAcc 0.7117	BestValid 0.7235
	Epoch 1650:	Loss 1.0019	TrainAcc 0.7435	ValidAcc 0.7237	TestAcc 0.7163	BestValid 0.7237
	Epoch 1700:	Loss 1.0008	TrainAcc 0.7432	ValidAcc 0.7230	TestAcc 0.7137	BestValid 0.7237
	Epoch 1750:	Loss 0.9976	TrainAcc 0.7432	ValidAcc 0.7238	TestAcc 0.7155	BestValid 0.7238
	Epoch 1800:	Loss 0.9912	TrainAcc 0.7431	ValidAcc 0.7187	TestAcc 0.7049	BestValid 0.7238
	Epoch 1850:	Loss 0.9933	TrainAcc 0.7453	ValidAcc 0.7230	TestAcc 0.7110	BestValid 0.7238
	Epoch 1900:	Loss 0.9861	TrainAcc 0.7452	ValidAcc 0.7246	TestAcc 0.7148	BestValid 0.7246
	Epoch 1950:	Loss 0.9851	TrainAcc 0.7453	ValidAcc 0.7218	TestAcc 0.7096	BestValid 0.7246
	Epoch 2000:	Loss 0.9827	TrainAcc 0.7472	ValidAcc 0.7266	TestAcc 0.7138	BestValid 0.7266
	Epoch 2050:	Loss 0.9743	TrainAcc 0.7471	ValidAcc 0.7254	TestAcc 0.7150	BestValid 0.7266
	Epoch 2100:	Loss 0.9735	TrainAcc 0.7473	ValidAcc 0.7213	TestAcc 0.7074	BestValid 0.7266
	Epoch 2150:	Loss 0.9751	TrainAcc 0.7474	ValidAcc 0.7251	TestAcc 0.7156	BestValid 0.7266
	Epoch 2200:	Loss 0.9729	TrainAcc 0.7491	ValidAcc 0.7260	TestAcc 0.7163	BestValid 0.7266
	Epoch 2250:	Loss 0.9684	TrainAcc 0.7489	ValidAcc 0.7252	TestAcc 0.7145	BestValid 0.7266
	Epoch 2300:	Loss 0.9703	TrainAcc 0.7491	ValidAcc 0.7266	TestAcc 0.7200	BestValid 0.7266
	Epoch 2350:	Loss 0.9624	TrainAcc 0.7499	ValidAcc 0.7263	TestAcc 0.7180	BestValid 0.7266
	Epoch 2400:	Loss 0.9633	TrainAcc 0.7498	ValidAcc 0.7267	TestAcc 0.7165	BestValid 0.7267
	Epoch 2450:	Loss 0.9622	TrainAcc 0.7517	ValidAcc 0.7281	TestAcc 0.7182	BestValid 0.7281
	Epoch 2500:	Loss 0.9535	TrainAcc 0.7512	ValidAcc 0.7264	TestAcc 0.7153	BestValid 0.7281
	Epoch 2550:	Loss 0.9570	TrainAcc 0.7513	ValidAcc 0.7271	TestAcc 0.7177	BestValid 0.7281
	Epoch 2600:	Loss 0.9544	TrainAcc 0.7516	ValidAcc 0.7275	TestAcc 0.7195	BestValid 0.7281
	Epoch 2650:	Loss 0.9508	TrainAcc 0.7528	ValidAcc 0.7298	TestAcc 0.7203	BestValid 0.7298
	Epoch 2700:	Loss 0.9486	TrainAcc 0.7521	ValidAcc 0.7212	TestAcc 0.7063	BestValid 0.7298
	Epoch 2750:	Loss 0.9454	TrainAcc 0.7536	ValidAcc 0.7265	TestAcc 0.7156	BestValid 0.7298
	Epoch 2800:	Loss 0.9424	TrainAcc 0.7530	ValidAcc 0.7241	TestAcc 0.7110	BestValid 0.7298
	Epoch 2850:	Loss 0.9450	TrainAcc 0.7529	ValidAcc 0.7272	TestAcc 0.7171	BestValid 0.7298
	Epoch 2900:	Loss 0.9391	TrainAcc 0.7531	ValidAcc 0.7263	TestAcc 0.7163	BestValid 0.7298
	Epoch 2950:	Loss 0.9374	TrainAcc 0.7556	ValidAcc 0.7292	TestAcc 0.7207	BestValid 0.7298
	Epoch 3000:	Loss 0.9373	TrainAcc 0.7546	ValidAcc 0.7284	TestAcc 0.7216	BestValid 0.7298
	Epoch 3050:	Loss 0.9352	TrainAcc 0.7557	ValidAcc 0.7279	TestAcc 0.7179	BestValid 0.7298
	Epoch 3100:	Loss 0.9315	TrainAcc 0.7545	ValidAcc 0.7283	TestAcc 0.7172	BestValid 0.7298
	Epoch 3150:	Loss 0.9293	TrainAcc 0.7565	ValidAcc 0.7274	TestAcc 0.7179	BestValid 0.7298
	Epoch 3200:	Loss 0.9269	TrainAcc 0.7562	ValidAcc 0.7281	TestAcc 0.7161	BestValid 0.7298
	Epoch 3250:	Loss 0.9276	TrainAcc 0.7553	ValidAcc 0.7277	TestAcc 0.7167	BestValid 0.7298
	Epoch 3300:	Loss 0.9278	TrainAcc 0.7573	ValidAcc 0.7296	TestAcc 0.7193	BestValid 0.7298
	Epoch 3350:	Loss 0.9213	TrainAcc 0.7574	ValidAcc 0.7266	TestAcc 0.7155	BestValid 0.7298
	Epoch 3400:	Loss 0.9211	TrainAcc 0.7578	ValidAcc 0.7289	TestAcc 0.7209	BestValid 0.7298
	Epoch 3450:	Loss 0.9175	TrainAcc 0.7580	ValidAcc 0.7299	TestAcc 0.7202	BestValid 0.7299
	Epoch 3500:	Loss 0.9182	TrainAcc 0.7582	ValidAcc 0.7275	TestAcc 0.7135	BestValid 0.7299
	Epoch 3550:	Loss 0.9181	TrainAcc 0.7588	ValidAcc 0.7288	TestAcc 0.7197	BestValid 0.7299
	Epoch 3600:	Loss 0.9154	TrainAcc 0.7592	ValidAcc 0.7277	TestAcc 0.7174	BestValid 0.7299
	Epoch 3650:	Loss 0.9142	TrainAcc 0.7603	ValidAcc 0.7276	TestAcc 0.7189	BestValid 0.7299
	Epoch 3700:	Loss 0.9134	TrainAcc 0.7603	ValidAcc 0.7283	TestAcc 0.7172	BestValid 0.7299
	Epoch 3750:	Loss 0.9122	TrainAcc 0.7585	ValidAcc 0.7266	TestAcc 0.7172	BestValid 0.7299
	Epoch 3800:	Loss 0.9112	TrainAcc 0.7620	ValidAcc 0.7288	TestAcc 0.7172	BestValid 0.7299
	Epoch 3850:	Loss 0.9067	TrainAcc 0.7607	ValidAcc 0.7291	TestAcc 0.7197	BestValid 0.7299
	Epoch 3900:	Loss 0.9026	TrainAcc 0.7610	ValidAcc 0.7295	TestAcc 0.7206	BestValid 0.7299
	Epoch 3950:	Loss 0.9056	TrainAcc 0.7605	ValidAcc 0.7300	TestAcc 0.7202	BestValid 0.7300
	Epoch 4000:	Loss 0.9017	TrainAcc 0.7622	ValidAcc 0.7296	TestAcc 0.7187	BestValid 0.7300
	Epoch 4050:	Loss 0.9014	TrainAcc 0.7607	ValidAcc 0.7302	TestAcc 0.7234	BestValid 0.7302
	Epoch 4100:	Loss 0.8990	TrainAcc 0.7630	ValidAcc 0.7302	TestAcc 0.7238	BestValid 0.7302
	Epoch 4150:	Loss 0.9000	TrainAcc 0.7634	ValidAcc 0.7305	TestAcc 0.7209	BestValid 0.7305
	Epoch 4200:	Loss 0.8949	TrainAcc 0.7635	ValidAcc 0.7279	TestAcc 0.7179	BestValid 0.7305
	Epoch 4250:	Loss 0.8969	TrainAcc 0.7634	ValidAcc 0.7303	TestAcc 0.7202	BestValid 0.7305
	Epoch 4300:	Loss 0.8922	TrainAcc 0.7623	ValidAcc 0.7310	TestAcc 0.7234	BestValid 0.7310
	Epoch 4350:	Loss 0.8933	TrainAcc 0.7637	ValidAcc 0.7303	TestAcc 0.7210	BestValid 0.7310
	Epoch 4400:	Loss 0.8929	TrainAcc 0.7640	ValidAcc 0.7306	TestAcc 0.7215	BestValid 0.7310
	Epoch 4450:	Loss 0.8918	TrainAcc 0.7645	ValidAcc 0.7277	TestAcc 0.7127	BestValid 0.7310
	Epoch 4500:	Loss 0.8892	TrainAcc 0.7650	ValidAcc 0.7295	TestAcc 0.7181	BestValid 0.7310
	Epoch 4550:	Loss 0.8894	TrainAcc 0.7651	ValidAcc 0.7291	TestAcc 0.7192	BestValid 0.7310
	Epoch 4600:	Loss 0.8885	TrainAcc 0.7660	ValidAcc 0.7306	TestAcc 0.7216	BestValid 0.7310
	Epoch 4650:	Loss 0.8874	TrainAcc 0.7645	ValidAcc 0.7286	TestAcc 0.7189	BestValid 0.7310
	Epoch 4700:	Loss 0.8802	TrainAcc 0.7668	ValidAcc 0.7303	TestAcc 0.7192	BestValid 0.7310
	Epoch 4750:	Loss 0.8815	TrainAcc 0.7658	ValidAcc 0.7295	TestAcc 0.7239	BestValid 0.7310
	Epoch 4800:	Loss 0.8807	TrainAcc 0.7669	ValidAcc 0.7293	TestAcc 0.7196	BestValid 0.7310
	Epoch 4850:	Loss 0.8792	TrainAcc 0.7669	ValidAcc 0.7288	TestAcc 0.7208	BestValid 0.7310
	Epoch 4900:	Loss 0.8792	TrainAcc 0.7679	ValidAcc 0.7296	TestAcc 0.7208	BestValid 0.7310
	Epoch 4950:	Loss 0.8773	TrainAcc 0.7684	ValidAcc 0.7314	TestAcc 0.7207	BestValid 0.7314
	Epoch 5000:	Loss 0.8726	TrainAcc 0.7675	ValidAcc 0.7255	TestAcc 0.7104	BestValid 0.7314
****** Epoch Time (Excluding Evaluation Cost): 0.227 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 32.860 ms (Max: 35.450, Min: 27.351, Sum: 262.879)
Cluster-Wide Average, Compute: 131.087 ms (Max: 144.579, Min: 121.510, Sum: 1048.699)
Cluster-Wide Average, Communication-Layer: 12.046 ms (Max: 13.895, Min: 10.523, Sum: 96.365)
Cluster-Wide Average, Bubble-Imbalance: 11.734 ms (Max: 20.109, Min: 0.378, Sum: 93.874)
Cluster-Wide Average, Communication-Graph: 31.522 ms (Max: 32.668, Min: 30.587, Sum: 252.175)
Cluster-Wide Average, Optimization: 2.794 ms (Max: 3.217, Min: 2.340, Sum: 22.353)
Cluster-Wide Average, Others: 4.728 ms (Max: 10.197, Min: 2.869, Sum: 37.823)
****** Breakdown Sum: 226.771 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.534 GB (Max: 6.399, Min: 5.237, Sum: 44.269)
Cluster-Wide Average, Graph-Level Communication Throughput: 88.612 Gbps (Max: 92.781, Min: 84.555, Sum: 708.894)
Cluster-Wide Average, Layer-Level Communication Throughput: 33.041 Gbps (Max: 42.652, Min: 23.686, Sum: 264.327)
Layer-level communication (cluster-wide, per-epoch): 0.379 GB
Graph-level communication (cluster-wide, per-epoch): 1.952 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 2.336 GB
****** Accuracy Results ******
Highest valid_acc: 0.7314
Target test_acc: 0.7207
Epoch to reach the target acc: 4949
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
