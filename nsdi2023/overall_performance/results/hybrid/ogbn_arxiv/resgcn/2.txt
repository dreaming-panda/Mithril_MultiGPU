Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 6 on machine gnerv8
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.070 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.051 seconds.
        It takes 0.049 seconds.
        It takes 0.049 seconds.
        It takes 0.051 seconds.
        It takes 0.052 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.050 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.028 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 61.317 Gbps (per GPU), 490.538 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 61.026 Gbps (per GPU), 488.209 Gbps (aggregated)
The layer-level communication performance: 61.017 Gbps (per GPU), 488.134 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.774 Gbps (per GPU), 486.194 Gbps (aggregated)
The layer-level communication performance: 60.741 Gbps (per GPU), 485.925 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.531 Gbps (per GPU), 484.248 Gbps (aggregated)
The layer-level communication performance: 60.481 Gbps (per GPU), 483.844 Gbps (aggregated)
The layer-level communication performance: 60.450 Gbps (per GPU), 483.598 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.300 Gbps (per GPU), 1266.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.291 Gbps (per GPU), 1266.324 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.294 Gbps (per GPU), 1266.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.297 Gbps (per GPU), 1266.372 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.300 Gbps (per GPU), 1266.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.284 Gbps (per GPU), 1266.274 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.303 Gbps (per GPU), 1266.420 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.294 Gbps (per GPU), 1266.348 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.139 Gbps (per GPU), 833.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.143 Gbps (per GPU), 833.147 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.112 Gbps (per GPU), 832.899 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.144 Gbps (per GPU), 833.154 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.078 Gbps (per GPU), 832.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.141 Gbps (per GPU), 833.126 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.087 Gbps (per GPU), 832.699 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.083 Gbps (per GPU), 832.664 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.610 Gbps (per GPU), 316.879 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.610 Gbps (per GPU), 316.884 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.608 Gbps (per GPU), 316.867 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.607 Gbps (per GPU), 316.854 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.607 Gbps (per GPU), 316.859 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.604 Gbps (per GPU), 316.830 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.606 Gbps (per GPU), 316.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.602 Gbps (per GPU), 316.815 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.49ms  1.06ms  1.83ms  3.74  5.55K  0.06M
 chk_1  0.51ms  1.07ms  2.16ms  4.20  5.75K  0.05M
 chk_2  0.50ms  1.05ms  1.81ms  3.63  5.20K  0.07M
 chk_3  0.50ms  1.07ms  1.84ms  3.65  5.57K  0.06M
 chk_4  0.50ms  1.03ms  1.77ms  3.57  5.05K  0.08M
 chk_5  0.52ms  1.05ms  1.77ms  3.41  4.73K  0.11M
 chk_6  0.49ms  1.03ms  1.76ms  3.57  4.98K  0.08M
 chk_7  0.50ms  1.03ms  1.77ms  3.55  5.01K  0.09M
 chk_8  0.50ms  1.18ms  1.85ms  3.73  5.73K  0.05M
 chk_9  0.50ms  1.01ms  1.72ms  3.47  4.54K  0.11M
chk_10  0.50ms  1.06ms  1.82ms  3.63  5.36K  0.07M
chk_11  0.50ms  1.06ms  1.82ms  3.62  5.39K  0.08M
chk_12  0.49ms  1.07ms  1.85ms  3.74  5.77K  0.05M
chk_13  0.50ms  1.05ms  1.81ms  3.64  5.43K  0.06M
chk_14  0.49ms  1.05ms  1.81ms  3.67  5.46K  0.06M
chk_15  0.50ms  1.07ms  1.86ms  3.74  5.88K  0.04M
chk_16  0.50ms  1.05ms  1.82ms  3.63  5.50K  0.06M
chk_17  0.51ms  1.04ms  1.77ms  3.46  4.86K  0.09M
chk_18  0.52ms  1.08ms  1.83ms  3.52  5.39K  0.07M
chk_19  0.50ms  1.04ms  1.79ms  3.57  5.20K  0.07M
chk_20  0.49ms  1.05ms  1.82ms  3.69  5.51K  0.06M
chk_21  0.49ms  1.06ms  1.84ms  3.78  5.81K  0.05M
chk_22  0.50ms  1.05ms  1.81ms  3.65  5.32K  0.07M
chk_23  0.51ms  1.07ms  1.83ms  3.57  5.39K  0.07M
chk_24  0.50ms  1.02ms  1.74ms  3.47  4.62K  0.11M
chk_25  0.50ms  1.03ms  1.77ms  3.53  5.04K  0.08M
chk_26  0.49ms  1.01ms  1.72ms  3.48  4.55K  0.11M
chk_27  0.49ms  1.04ms  1.79ms  3.67  5.30K  0.06M
chk_28  0.49ms  1.06ms  1.82ms  3.70  5.58K  0.06M
chk_29  0.51ms  1.04ms  1.77ms  3.50  4.98K  0.09M
chk_30  0.76ms  1.06ms  1.82ms  2.40  5.50K  0.07M
chk_31  0.50ms  1.05ms  1.82ms  3.64  5.38K  0.07M
   Avg  0.51  1.05  1.81
   Max  0.76  1.18  2.16
   Min  0.49  1.01  1.72
 Ratio  1.56  1.17  1.26
   Var  0.00  0.00  0.01
Profiling takes 1.350 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 100)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 308)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [204, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [100, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 7, starting model training...
Num Stages: 4 / 4
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 204)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [308, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 85420
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [308, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 100)...
+++++++++ Node 0 initializing the weights for op[0, 100)...
+++++++++ Node 4 initializing the weights for op[204, 308)...
+++++++++ Node 2 initializing the weights for op[100, 204)...
+++++++++ Node 5 initializing the weights for op[204, 308)...
+++++++++ Node 3 initializing the weights for op[100, 204)...
+++++++++ Node 7 initializing the weights for op[308, 421)...
+++++++++ Node 6 initializing the weights for op[308, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 327544
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.1702	TrainAcc 0.1788	ValidAcc 0.0765	TestAcc 0.0589	BestValid 0.0765
	Epoch 50:	Loss 2.0290	TrainAcc 0.5108	ValidAcc 0.5470	TestAcc 0.5454	BestValid 0.5470
	Epoch 100:	Loss 1.6437	TrainAcc 0.6042	ValidAcc 0.6189	TestAcc 0.6149	BestValid 0.6189
	Epoch 150:	Loss 1.4506	TrainAcc 0.6482	ValidAcc 0.6546	TestAcc 0.6462	BestValid 0.6546
	Epoch 200:	Loss 1.3439	TrainAcc 0.6673	ValidAcc 0.6747	TestAcc 0.6676	BestValid 0.6747
	Epoch 250:	Loss 1.2828	TrainAcc 0.6863	ValidAcc 0.6864	TestAcc 0.6755	BestValid 0.6864
	Epoch 300:	Loss 1.2327	TrainAcc 0.6939	ValidAcc 0.6930	TestAcc 0.6823	BestValid 0.6930
	Epoch 350:	Loss 1.2019	TrainAcc 0.7019	ValidAcc 0.7019	TestAcc 0.6916	BestValid 0.7019
	Epoch 400:	Loss 1.1803	TrainAcc 0.7057	ValidAcc 0.7025	TestAcc 0.6952	BestValid 0.7025
	Epoch 450:	Loss 1.1596	TrainAcc 0.7115	ValidAcc 0.7069	TestAcc 0.6970	BestValid 0.7069
	Epoch 500:	Loss 1.1434	TrainAcc 0.7140	ValidAcc 0.7100	TestAcc 0.7005	BestValid 0.7100
	Epoch 550:	Loss 1.1329	TrainAcc 0.7180	ValidAcc 0.7125	TestAcc 0.6994	BestValid 0.7125
	Epoch 600:	Loss 1.1197	TrainAcc 0.7188	ValidAcc 0.7133	TestAcc 0.7076	BestValid 0.7133
	Epoch 650:	Loss 1.1100	TrainAcc 0.7203	ValidAcc 0.7125	TestAcc 0.7011	BestValid 0.7133
	Epoch 700:	Loss 1.0978	TrainAcc 0.7237	ValidAcc 0.7152	TestAcc 0.7020	BestValid 0.7152
	Epoch 750:	Loss 1.0926	TrainAcc 0.7255	ValidAcc 0.7164	TestAcc 0.7062	BestValid 0.7164
	Epoch 800:	Loss 1.0858	TrainAcc 0.7267	ValidAcc 0.7170	TestAcc 0.7046	BestValid 0.7170
	Epoch 850:	Loss 1.0766	TrainAcc 0.7282	ValidAcc 0.7170	TestAcc 0.7066	BestValid 0.7170
	Epoch 900:	Loss 1.0682	TrainAcc 0.7290	ValidAcc 0.7173	TestAcc 0.7064	BestValid 0.7173
	Epoch 950:	Loss 1.0605	TrainAcc 0.7306	ValidAcc 0.7164	TestAcc 0.7025	BestValid 0.7173
	Epoch 1000:	Loss 1.0619	TrainAcc 0.7308	ValidAcc 0.7181	TestAcc 0.7082	BestValid 0.7181
	Epoch 1050:	Loss 1.0540	TrainAcc 0.7328	ValidAcc 0.7196	TestAcc 0.7090	BestValid 0.7196
	Epoch 1100:	Loss 1.0466	TrainAcc 0.7348	ValidAcc 0.7212	TestAcc 0.7101	BestValid 0.7212
	Epoch 1150:	Loss 1.0417	TrainAcc 0.7350	ValidAcc 0.7200	TestAcc 0.7074	BestValid 0.7212
	Epoch 1200:	Loss 1.0423	TrainAcc 0.7365	ValidAcc 0.7195	TestAcc 0.7067	BestValid 0.7212
	Epoch 1250:	Loss 1.0328	TrainAcc 0.7366	ValidAcc 0.7204	TestAcc 0.7074	BestValid 0.7212
	Epoch 1300:	Loss 1.0311	TrainAcc 0.7370	ValidAcc 0.7201	TestAcc 0.7094	BestValid 0.7212
	Epoch 1350:	Loss 1.0234	TrainAcc 0.7388	ValidAcc 0.7216	TestAcc 0.7085	BestValid 0.7216
	Epoch 1400:	Loss 1.0201	TrainAcc 0.7391	ValidAcc 0.7221	TestAcc 0.7122	BestValid 0.7221
	Epoch 1450:	Loss 1.0198	TrainAcc 0.7399	ValidAcc 0.7224	TestAcc 0.7147	BestValid 0.7224
	Epoch 1500:	Loss 1.0173	TrainAcc 0.7408	ValidAcc 0.7216	TestAcc 0.7086	BestValid 0.7224
	Epoch 1550:	Loss 1.0100	TrainAcc 0.7408	ValidAcc 0.7235	TestAcc 0.7145	BestValid 0.7235
	Epoch 1600:	Loss 1.0082	TrainAcc 0.7412	ValidAcc 0.7230	TestAcc 0.7117	BestValid 0.7235
	Epoch 1650:	Loss 1.0019	TrainAcc 0.7435	ValidAcc 0.7237	TestAcc 0.7163	BestValid 0.7237
	Epoch 1700:	Loss 1.0008	TrainAcc 0.7432	ValidAcc 0.7230	TestAcc 0.7137	BestValid 0.7237
	Epoch 1750:	Loss 0.9976	TrainAcc 0.7432	ValidAcc 0.7238	TestAcc 0.7155	BestValid 0.7238
	Epoch 1800:	Loss 0.9912	TrainAcc 0.7431	ValidAcc 0.7187	TestAcc 0.7049	BestValid 0.7238
	Epoch 1850:	Loss 0.9933	TrainAcc 0.7453	ValidAcc 0.7230	TestAcc 0.7110	BestValid 0.7238
	Epoch 1900:	Loss 0.9861	TrainAcc 0.7452	ValidAcc 0.7246	TestAcc 0.7148	BestValid 0.7246
	Epoch 1950:	Loss 0.9851	TrainAcc 0.7453	ValidAcc 0.7218	TestAcc 0.7096	BestValid 0.7246
	Epoch 2000:	Loss 0.9827	TrainAcc 0.7472	ValidAcc 0.7266	TestAcc 0.7138	BestValid 0.7266
	Epoch 2050:	Loss 0.9743	TrainAcc 0.7471	ValidAcc 0.7254	TestAcc 0.7150	BestValid 0.7266
	Epoch 2100:	Loss 0.9735	TrainAcc 0.7473	ValidAcc 0.7213	TestAcc 0.7074	BestValid 0.7266
	Epoch 2150:	Loss 0.9751	TrainAcc 0.7474	ValidAcc 0.7251	TestAcc 0.7156	BestValid 0.7266
	Epoch 2200:	Loss 0.9729	TrainAcc 0.7491	ValidAcc 0.7260	TestAcc 0.7163	BestValid 0.7266
	Epoch 2250:	Loss 0.9684	TrainAcc 0.7489	ValidAcc 0.7252	TestAcc 0.7145	BestValid 0.7266
	Epoch 2300:	Loss 0.9703	TrainAcc 0.7491	ValidAcc 0.7266	TestAcc 0.7200	BestValid 0.7266
	Epoch 2350:	Loss 0.9624	TrainAcc 0.7499	ValidAcc 0.7263	TestAcc 0.7180	BestValid 0.7266
	Epoch 2400:	Loss 0.9633	TrainAcc 0.7498	ValidAcc 0.7267	TestAcc 0.7165	BestValid 0.7267
	Epoch 2450:	Loss 0.9622	TrainAcc 0.7517	ValidAcc 0.7281	TestAcc 0.7182	BestValid 0.7281
	Epoch 2500:	Loss 0.9535	TrainAcc 0.7512	ValidAcc 0.7264	TestAcc 0.7153	BestValid 0.7281
