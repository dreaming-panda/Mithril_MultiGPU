Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8
Initialized node 4 on machine gnerv8

Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.049 seconds.
        It takes 0.050 seconds.
        It takes 0.054 seconds.
        It takes 0.055 seconds.
        It takes 0.051 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.051 seconds.
Building the Label Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.023 seconds.
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.069 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.028 seconds.
        It takes 0.028 seconds.
        It takes 0.028 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.815 Gbps (per GPU), 486.524 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.519 Gbps (per GPU), 484.155 Gbps (aggregated)
The layer-level communication performance: 60.504 Gbps (per GPU), 484.031 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.240 Gbps (per GPU), 481.918 Gbps (aggregated)
The layer-level communication performance: 60.204 Gbps (per GPU), 481.634 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.972 Gbps (per GPU), 479.774 Gbps (aggregated)
The layer-level communication performance: 59.923 Gbps (per GPU), 479.381 Gbps (aggregated)
The layer-level communication performance: 59.891 Gbps (per GPU), 479.125 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.506 Gbps (per GPU), 1268.047 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.497 Gbps (per GPU), 1267.975 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.497 Gbps (per GPU), 1267.972 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.494 Gbps (per GPU), 1267.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.509 Gbps (per GPU), 1268.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.482 Gbps (per GPU), 1267.856 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.509 Gbps (per GPU), 1268.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.485 Gbps (per GPU), 1267.880 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.205 Gbps (per GPU), 833.643 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.207 Gbps (per GPU), 833.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.205 Gbps (per GPU), 833.644 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.202 Gbps (per GPU), 833.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.196 Gbps (per GPU), 833.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.202 Gbps (per GPU), 833.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.200 Gbps (per GPU), 833.602 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.202 Gbps (per GPU), 833.616 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.703 Gbps (per GPU), 301.622 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.699 Gbps (per GPU), 301.589 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.702 Gbps (per GPU), 301.615 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.699 Gbps (per GPU), 301.590 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.697 Gbps (per GPU), 301.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.698 Gbps (per GPU), 301.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.695 Gbps (per GPU), 301.563 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.690 Gbps (per GPU), 301.524 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.49ms  1.06ms  1.83ms  3.75  5.55K  0.06M
 chk_1  0.51ms  1.19ms  1.86ms  3.62  5.75K  0.05M
 chk_2  0.50ms  1.05ms  1.80ms  3.62  5.20K  0.07M
 chk_3  0.50ms  1.07ms  1.84ms  3.68  5.57K  0.06M
 chk_4  0.50ms  1.03ms  1.76ms  3.56  5.05K  0.08M
 chk_5  0.52ms  1.05ms  1.78ms  3.43  4.73K  0.11M
 chk_6  0.49ms  1.02ms  1.76ms  3.58  4.98K  0.08M
 chk_7  0.50ms  1.03ms  1.77ms  3.56  5.01K  0.09M
 chk_8  0.50ms  1.07ms  1.85ms  3.73  5.73K  0.05M
 chk_9  0.49ms  1.01ms  1.73ms  3.50  4.54K  0.11M
chk_10  0.50ms  1.06ms  1.82ms  3.63  5.36K  0.07M
chk_11  0.51ms  1.06ms  1.82ms  3.61  5.39K  0.08M
chk_12  0.50ms  1.07ms  1.85ms  3.72  5.77K  0.05M
chk_13  0.50ms  1.05ms  1.82ms  3.65  5.43K  0.06M
chk_14  0.49ms  1.04ms  1.82ms  3.68  5.46K  0.06M
chk_15  0.49ms  1.07ms  1.86ms  3.76  5.88K  0.04M
chk_16  0.50ms  1.05ms  1.82ms  3.64  5.50K  0.06M
chk_17  0.51ms  1.04ms  1.78ms  3.47  4.86K  0.09M
chk_18  0.52ms  1.08ms  1.84ms  3.54  5.39K  0.07M
chk_19  0.50ms  1.04ms  1.80ms  3.58  5.20K  0.07M
chk_20  0.49ms  1.05ms  1.82ms  3.69  5.51K  0.06M
chk_21  0.49ms  1.07ms  1.85ms  3.80  5.81K  0.05M
chk_22  0.50ms  1.05ms  1.81ms  3.65  5.32K  0.07M
chk_23  0.51ms  1.07ms  1.83ms  3.58  5.39K  0.07M
chk_24  0.50ms  1.02ms  1.75ms  3.49  4.62K  0.11M
chk_25  0.50ms  1.03ms  1.78ms  3.56  5.04K  0.08M
chk_26  0.76ms  1.01ms  1.73ms  2.28  4.55K  0.11M
chk_27  0.49ms  1.03ms  1.80ms  3.66  5.30K  0.06M
chk_28  0.50ms  1.06ms  1.83ms  3.69  5.58K  0.06M
chk_29  0.51ms  1.04ms  1.78ms  3.53  4.98K  0.09M
chk_30  0.50ms  1.06ms  1.83ms  3.68  5.50K  0.07M
chk_31  0.50ms  1.05ms  1.82ms  3.64  5.38K  0.07M
   Avg  0.51  1.05  1.81
   Max  0.76  1.19  1.86
   Min  0.49  1.01  1.73
 Ratio  1.56  1.19  1.08
   Var  0.00  0.00  0.00
Profiling takes 1.353 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 100)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 204)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [100, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_ADD
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [204, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 85420, Num Local Vertices: 83923
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 308)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 85420
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [308, 421)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [308, 421)
*** Node 7, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 85420
Node 7, Local Vertex Begin: 85420, Num Local Vertices: 83923
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 100)...
+++++++++ Node 5 initializing the weights for op[204, 308)...
+++++++++ Node 0 initializing the weights for op[0, 100)...
+++++++++ Node 2 initializing the weights for op[100, 204)...
+++++++++ Node 4 initializing the weights for op[204, 308)...
+++++++++ Node 7 initializing the weights for op[308, 421)...
+++++++++ Node 3 initializing the weights for op[100, 204)...
+++++++++ Node 6 initializing the weights for op[308, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 327544
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...


*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000

The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.0959	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 50:	Loss 2.1543	TrainAcc 0.4730	ValidAcc 0.4962	TestAcc 0.4786	BestValid 0.4962
	Epoch 100:	Loss 1.6864	TrainAcc 0.5907	ValidAcc 0.6088	TestAcc 0.6107	BestValid 0.6088
	Epoch 150:	Loss 1.4622	TrainAcc 0.6451	ValidAcc 0.6572	TestAcc 0.6527	BestValid 0.6572
	Epoch 200:	Loss 1.3496	TrainAcc 0.6675	ValidAcc 0.6703	TestAcc 0.6624	BestValid 0.6703
	Epoch 250:	Loss 1.2871	TrainAcc 0.6839	ValidAcc 0.6829	TestAcc 0.6751	BestValid 0.6829
	Epoch 300:	Loss 1.2417	TrainAcc 0.6933	ValidAcc 0.6915	TestAcc 0.6849	BestValid 0.6915
	Epoch 350:	Loss 1.2094	TrainAcc 0.7007	ValidAcc 0.6985	TestAcc 0.6901	BestValid 0.6985
	Epoch 400:	Loss 1.1853	TrainAcc 0.7062	ValidAcc 0.7022	TestAcc 0.6927	BestValid 0.7022
	Epoch 450:	Loss 1.1666	TrainAcc 0.7098	ValidAcc 0.7059	TestAcc 0.6982	BestValid 0.7059
	Epoch 500:	Loss 1.1514	TrainAcc 0.7140	ValidAcc 0.7077	TestAcc 0.6965	BestValid 0.7077
	Epoch 550:	Loss 1.1386	TrainAcc 0.7171	ValidAcc 0.7101	TestAcc 0.6951	BestValid 0.7101
	Epoch 600:	Loss 1.1236	TrainAcc 0.7184	ValidAcc 0.7106	TestAcc 0.6979	BestValid 0.7106
	Epoch 650:	Loss 1.1105	TrainAcc 0.7209	ValidAcc 0.7127	TestAcc 0.7002	BestValid 0.7127
	Epoch 700:	Loss 1.1001	TrainAcc 0.7232	ValidAcc 0.7139	TestAcc 0.7005	BestValid 0.7139
	Epoch 750:	Loss 1.0960	TrainAcc 0.7245	ValidAcc 0.7147	TestAcc 0.7013	BestValid 0.7147
	Epoch 800:	Loss 1.0866	TrainAcc 0.7256	ValidAcc 0.7160	TestAcc 0.7039	BestValid 0.7160
	Epoch 850:	Loss 1.0823	TrainAcc 0.7271	ValidAcc 0.7164	TestAcc 0.7029	BestValid 0.7164
	Epoch 900:	Loss 1.0735	TrainAcc 0.7293	ValidAcc 0.7164	TestAcc 0.7030	BestValid 0.7164
	Epoch 950:	Loss 1.0664	TrainAcc 0.7307	ValidAcc 0.7181	TestAcc 0.7071	BestValid 0.7181
	Epoch 1000:	Loss 1.0612	TrainAcc 0.7301	ValidAcc 0.7196	TestAcc 0.7102	BestValid 0.7196
	Epoch 1050:	Loss 1.0526	TrainAcc 0.7311	ValidAcc 0.7170	TestAcc 0.7029	BestValid 0.7196
	Epoch 1100:	Loss 1.0519	TrainAcc 0.7322	ValidAcc 0.7168	TestAcc 0.7031	BestValid 0.7196
	Epoch 1150:	Loss 1.0425	TrainAcc 0.7333	ValidAcc 0.7171	TestAcc 0.7025	BestValid 0.7196
	Epoch 1200:	Loss 1.0427	TrainAcc 0.7337	ValidAcc 0.7193	TestAcc 0.7066	BestValid 0.7196
	Epoch 1250:	Loss 1.0344	TrainAcc 0.7357	ValidAcc 0.7192	TestAcc 0.7064	BestValid 0.7196
	Epoch 1300:	Loss 1.0298	TrainAcc 0.7352	ValidAcc 0.7190	TestAcc 0.7075	BestValid 0.7196
	Epoch 1350:	Loss 1.0272	TrainAcc 0.7360	ValidAcc 0.7194	TestAcc 0.7079	BestValid 0.7196
	Epoch 1400:	Loss 1.0204	TrainAcc 0.7381	ValidAcc 0.7193	TestAcc 0.7099	BestValid 0.7196
	Epoch 1450:	Loss 1.0186	TrainAcc 0.7372	ValidAcc 0.7188	TestAcc 0.7046	BestValid 0.7196
	Epoch 1500:	Loss 1.0150	TrainAcc 0.7384	ValidAcc 0.7184	TestAcc 0.7038	BestValid 0.7196
	Epoch 1550:	Loss 1.0115	TrainAcc 0.7387	ValidAcc 0.7175	TestAcc 0.7026	BestValid 0.7196
	Epoch 1600:	Loss 1.0066	TrainAcc 0.7401	ValidAcc 0.7203	TestAcc 0.7079	BestValid 0.7203
	Epoch 1650:	Loss 1.0022	TrainAcc 0.7422	ValidAcc 0.7224	TestAcc 0.7093	BestValid 0.7224
	Epoch 1700:	Loss 0.9993	TrainAcc 0.7412	ValidAcc 0.7222	TestAcc 0.7085	BestValid 0.7224
	Epoch 1750:	Loss 0.9944	TrainAcc 0.7408	ValidAcc 0.7226	TestAcc 0.7123	BestValid 0.7226
	Epoch 1800:	Loss 0.9954	TrainAcc 0.7435	ValidAcc 0.7221	TestAcc 0.7106	BestValid 0.7226
	Epoch 1850:	Loss 0.9932	TrainAcc 0.7437	ValidAcc 0.7231	TestAcc 0.7119	BestValid 0.7231
	Epoch 1900:	Loss 0.9877	TrainAcc 0.7427	ValidAcc 0.7206	TestAcc 0.7047	BestValid 0.7231
	Epoch 1950:	Loss 0.9834	TrainAcc 0.7437	ValidAcc 0.7202	TestAcc 0.7032	BestValid 0.7231
	Epoch 2000:	Loss 0.9809	TrainAcc 0.7452	ValidAcc 0.7209	TestAcc 0.7055	BestValid 0.7231
	Epoch 2050:	Loss 0.9820	TrainAcc 0.7458	ValidAcc 0.7214	TestAcc 0.7066	BestValid 0.7231
	Epoch 2100:	Loss 0.9756	TrainAcc 0.7458	ValidAcc 0.7229	TestAcc 0.7092	BestValid 0.7231
	Epoch 2150:	Loss 0.9736	TrainAcc 0.7448	ValidAcc 0.7188	TestAcc 0.7035	BestValid 0.7231
	Epoch 2200:	Loss 0.9732	TrainAcc 0.7468	ValidAcc 0.7244	TestAcc 0.7127	BestValid 0.7244
	Epoch 2250:	Loss 0.9711	TrainAcc 0.7472	ValidAcc 0.7233	TestAcc 0.7096	BestValid 0.7244
	Epoch 2300:	Loss 0.9692	TrainAcc 0.7489	ValidAcc 0.7248	TestAcc 0.7142	BestValid 0.7248
	Epoch 2350:	Loss 0.9608	TrainAcc 0.7470	ValidAcc 0.7191	TestAcc 0.7017	BestValid 0.7248
	Epoch 2400:	Loss 0.9630	TrainAcc 0.7490	ValidAcc 0.7242	TestAcc 0.7124	BestValid 0.7248
	Epoch 2450:	Loss 0.9600	TrainAcc 0.7491	ValidAcc 0.7223	TestAcc 0.7069	BestValid 0.7248
	Epoch 2500:	Loss 0.9564	TrainAcc 0.7508	ValidAcc 0.7260	TestAcc 0.7135	BestValid 0.7260
	Epoch 2550:	Loss 0.9534	TrainAcc 0.7496	ValidAcc 0.7214	TestAcc 0.7063	BestValid 0.7260
	Epoch 2600:	Loss 0.9544	TrainAcc 0.7493	ValidAcc 0.7201	TestAcc 0.7038	BestValid 0.7260
	Epoch 2650:	Loss 0.9562	TrainAcc 0.7516	ValidAcc 0.7263	TestAcc 0.7146	BestValid 0.7263
	Epoch 2700:	Loss 0.9508	TrainAcc 0.7515	ValidAcc 0.7239	TestAcc 0.7097	BestValid 0.7263
	Epoch 2750:	Loss 0.9476	TrainAcc 0.7520	ValidAcc 0.7255	TestAcc 0.7118	BestValid 0.7263
	Epoch 2800:	Loss 0.9450	TrainAcc 0.7536	ValidAcc 0.7238	TestAcc 0.7095	BestValid 0.7263
	Epoch 2850:	Loss 0.9428	TrainAcc 0.7518	ValidAcc 0.7244	TestAcc 0.7086	BestValid 0.7263
	Epoch 2900:	Loss 0.9362	TrainAcc 0.7532	ValidAcc 0.7239	TestAcc 0.7079	BestValid 0.7263
	Epoch 2950:	Loss 0.9358	TrainAcc 0.7518	ValidAcc 0.7186	TestAcc 0.7005	BestValid 0.7263
	Epoch 3000:	Loss 0.9338	TrainAcc 0.7539	ValidAcc 0.7233	TestAcc 0.7058	BestValid 0.7263
	Epoch 3050:	Loss 0.9310	TrainAcc 0.7551	ValidAcc 0.7272	TestAcc 0.7141	BestValid 0.7272
	Epoch 3100:	Loss 0.9342	TrainAcc 0.7556	ValidAcc 0.7266	TestAcc 0.7111	BestValid 0.7272
	Epoch 3150:	Loss 0.9309	TrainAcc 0.7548	ValidAcc 0.7233	TestAcc 0.7067	BestValid 0.7272
	Epoch 3200:	Loss 0.9311	TrainAcc 0.7553	ValidAcc 0.7262	TestAcc 0.7125	BestValid 0.7272
	Epoch 3250:	Loss 0.9237	TrainAcc 0.7560	ValidAcc 0.7277	TestAcc 0.7169	BestValid 0.7277
	Epoch 3300:	Loss 0.9238	TrainAcc 0.7566	ValidAcc 0.7248	TestAcc 0.7079	BestValid 0.7277
	Epoch 3350:	Loss 0.9214	TrainAcc 0.7568	ValidAcc 0.7229	TestAcc 0.7046	BestValid 0.7277
	Epoch 3400:	Loss 0.9216	TrainAcc 0.7567	ValidAcc 0.7222	TestAcc 0.7039	BestValid 0.7277
	Epoch 3450:	Loss 0.9219	TrainAcc 0.7572	ValidAcc 0.7267	TestAcc 0.7138	BestValid 0.7277
	Epoch 3500:	Loss 0.9168	TrainAcc 0.7570	ValidAcc 0.7221	TestAcc 0.7006	BestValid 0.7277
	Epoch 3550:	Loss 0.9142	TrainAcc 0.7574	ValidAcc 0.7247	TestAcc 0.7087	BestValid 0.7277
	Epoch 3600:	Loss 0.9133	TrainAcc 0.7565	ValidAcc 0.7210	TestAcc 0.7022	BestValid 0.7277
	Epoch 3650:	Loss 0.9135	TrainAcc 0.7593	ValidAcc 0.7276	TestAcc 0.7131	BestValid 0.7277
	Epoch 3700:	Loss 0.9117	TrainAcc 0.7589	ValidAcc 0.7216	TestAcc 0.7021	BestValid 0.7277
	Epoch 3750:	Loss 0.9141	TrainAcc 0.7596	ValidAcc 0.7250	TestAcc 0.7084	BestValid 0.7277
	Epoch 3800:	Loss 0.9111	TrainAcc 0.7599	ValidAcc 0.7242	TestAcc 0.7077	BestValid 0.7277
	Epoch 3850:	Loss 0.9093	TrainAcc 0.7611	ValidAcc 0.7253	TestAcc 0.7134	BestValid 0.7277
	Epoch 3900:	Loss 0.9088	TrainAcc 0.7601	ValidAcc 0.7249	TestAcc 0.7086	BestValid 0.7277
	Epoch 3950:	Loss 0.9047	TrainAcc 0.7616	ValidAcc 0.7278	TestAcc 0.7112	BestValid 0.7278
	Epoch 4000:	Loss 0.9035	TrainAcc 0.7607	ValidAcc 0.7253	TestAcc 0.7083	BestValid 0.7278
	Epoch 4050:	Loss 0.9009	TrainAcc 0.7609	ValidAcc 0.7234	TestAcc 0.7062	BestValid 0.7278
	Epoch 4100:	Loss 0.8997	TrainAcc 0.7617	ValidAcc 0.7292	TestAcc 0.7168	BestValid 0.7292
	Epoch 4150:	Loss 0.8985	TrainAcc 0.7639	ValidAcc 0.7288	TestAcc 0.7148	BestValid 0.7292
	Epoch 4200:	Loss 0.8945	TrainAcc 0.7616	ValidAcc 0.7271	TestAcc 0.7143	BestValid 0.7292
	Epoch 4250:	Loss 0.8908	TrainAcc 0.7636	ValidAcc 0.7266	TestAcc 0.7106	BestValid 0.7292
	Epoch 4300:	Loss 0.8964	TrainAcc 0.7644	ValidAcc 0.7260	TestAcc 0.7125	BestValid 0.7292
	Epoch 4350:	Loss 0.8915	TrainAcc 0.7637	ValidAcc 0.7286	TestAcc 0.7139	BestValid 0.7292
	Epoch 4400:	Loss 0.8923	TrainAcc 0.7644	ValidAcc 0.7255	TestAcc 0.7109	BestValid 0.7292
	Epoch 4450:	Loss 0.8887	TrainAcc 0.7636	ValidAcc 0.7197	TestAcc 0.7011	BestValid 0.7292
	Epoch 4500:	Loss 0.8890	TrainAcc 0.7632	ValidAcc 0.7284	TestAcc 0.7151	BestValid 0.7292
	Epoch 4550:	Loss 0.8874	TrainAcc 0.7645	ValidAcc 0.7255	TestAcc 0.7092	BestValid 0.7292
	Epoch 4600:	Loss 0.8876	TrainAcc 0.7662	ValidAcc 0.7291	TestAcc 0.7142	BestValid 0.7292
	Epoch 4650:	Loss 0.8865	TrainAcc 0.7649	ValidAcc 0.7249	TestAcc 0.7108	BestValid 0.7292
	Epoch 4700:	Loss 0.8825	TrainAcc 0.7650	ValidAcc 0.7288	TestAcc 0.7144	BestValid 0.7292
	Epoch 4750:	Loss 0.8803	TrainAcc 0.7657	ValidAcc 0.7272	TestAcc 0.7114	BestValid 0.7292
	Epoch 4800:	Loss 0.8824	TrainAcc 0.7675	ValidAcc 0.7266	TestAcc 0.7079	BestValid 0.7292
	Epoch 4850:	Loss 0.8787	TrainAcc 0.7666	ValidAcc 0.7252	TestAcc 0.7065	BestValid 0.7292
	Epoch 4900:	Loss 0.8794	TrainAcc 0.7625	ValidAcc 0.7142	TestAcc 0.6937	BestValid 0.7292
	Epoch 4950:	Loss 0.8753	TrainAcc 0.7668	ValidAcc 0.7250	TestAcc 0.7097	BestValid 0.7292
	Epoch 5000:	Loss 0.8739	TrainAcc 0.7661	ValidAcc 0.7239	TestAcc 0.7067	BestValid 0.7292
****** Epoch Time (Excluding Evaluation Cost): 0.233 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 33.148 ms (Max: 36.636, Min: 26.377, Sum: 265.187)
Cluster-Wide Average, Compute: 131.901 ms (Max: 149.939, Min: 122.096, Sum: 1055.212)
Cluster-Wide Average, Communication-Layer: 12.091 ms (Max: 13.695, Min: 10.587, Sum: 96.729)
Cluster-Wide Average, Bubble-Imbalance: 16.296 ms (Max: 24.222, Min: 0.312, Sum: 130.368)
Cluster-Wide Average, Communication-Graph: 31.819 ms (Max: 33.366, Min: 30.370, Sum: 254.552)
Cluster-Wide Average, Optimization: 2.887 ms (Max: 3.239, Min: 2.632, Sum: 23.099)
Cluster-Wide Average, Others: 4.743 ms (Max: 10.261, Min: 2.877, Sum: 37.943)
****** Breakdown Sum: 232.886 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.534 GB (Max: 6.399, Min: 5.237, Sum: 44.269)
Cluster-Wide Average, Graph-Level Communication Throughput: 87.856 Gbps (Max: 93.484, Min: 82.461, Sum: 702.847)
Cluster-Wide Average, Layer-Level Communication Throughput: 32.909 Gbps (Max: 42.120, Min: 23.718, Sum: 263.275)
Layer-level communication (cluster-wide, per-epoch): 0.379 GB
Graph-level communication (cluster-wide, per-epoch): 1.952 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 2.336 GB
****** Accuracy Results ******
Highest valid_acc: 0.7292
Target test_acc: 0.7168
Epoch to reach the target acc: 4099
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
