Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INITInitialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4

Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv8
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.006 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.015 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.014 seconds.
        It takes 0.013 seconds.
        It takes 0.009 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.943 Gbps (per GPU), 487.545 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.643 Gbps (per GPU), 485.140 Gbps (aggregated)
The layer-level communication performance: 60.628 Gbps (per GPU), 485.028 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.398 Gbps (per GPU), 483.181 Gbps (aggregated)
The layer-level communication performance: 60.360 Gbps (per GPU), 482.883 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.124 Gbps (per GPU), 480.992 Gbps (aggregated)
The layer-level communication performance: 60.079 Gbps (per GPU), 480.629 Gbps (aggregated)
The layer-level communication performance: 60.040 Gbps (per GPU), 480.321 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.990 Gbps (per GPU), 1271.917 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.996 Gbps (per GPU), 1271.965 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.962 Gbps (per GPU), 1271.700 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.921 Gbps (per GPU), 1271.365 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.972 Gbps (per GPU), 1271.772 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.917 Gbps (per GPU), 1271.338 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.993 Gbps (per GPU), 1271.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.972 Gbps (per GPU), 1271.776 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.342 Gbps (per GPU), 834.736 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.340 Gbps (per GPU), 834.722 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.339 Gbps (per GPU), 834.715 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.342 Gbps (per GPU), 834.736 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.340 Gbps (per GPU), 834.722 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.339 Gbps (per GPU), 834.715 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.335 Gbps (per GPU), 834.680 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.339 Gbps (per GPU), 834.715 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.257 Gbps (per GPU), 298.059 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.256 Gbps (per GPU), 298.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.255 Gbps (per GPU), 298.041 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.249 Gbps (per GPU), 297.991 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.254 Gbps (per GPU), 298.030 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.254 Gbps (per GPU), 298.032 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.260 Gbps (per GPU), 298.081 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.261 Gbps (per GPU), 298.091 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.44ms  0.72ms  1.26ms  2.86  0.08K  0.02M
 chk_1  0.54ms  0.76ms  1.34ms  2.45  0.16K  0.01M
 chk_2  0.48ms  0.72ms  1.28ms  2.69  0.13K  0.01M
 chk_3  0.57ms  0.79ms  1.37ms  2.40  0.18K  0.01M
 chk_4  0.53ms  0.76ms  1.33ms  2.52  0.15K  0.01M
 chk_5  0.61ms  0.82ms  1.40ms  2.29  0.25K  0.01M
 chk_6  0.45ms  0.72ms  1.27ms  2.84  0.10K  0.02M
 chk_7  0.47ms  0.73ms  1.28ms  2.71  0.11K  0.02M
 chk_8  0.59ms  0.81ms  1.39ms  2.36  0.23K  0.01M
 chk_9  0.51ms  0.76ms  1.33ms  2.62  0.14K  0.01M
chk_10  0.58ms  0.80ms  1.37ms  2.35  0.20K  0.01M
chk_11  0.45ms  0.72ms  1.27ms  2.83  0.09K  0.02M
chk_12  0.52ms  0.76ms  1.33ms  2.58  0.16K  0.01M
chk_13  0.52ms  0.76ms  1.33ms  2.58  0.16K  0.01M
chk_14  0.51ms  0.76ms  1.32ms  2.60  0.14K  0.01M
chk_15  0.59ms  0.81ms  1.39ms  2.36  0.21K  0.01M
chk_16  0.56ms  0.80ms  1.37ms  2.44  0.18K  0.01M
chk_17  0.62ms  0.82ms  1.42ms  2.29  0.29K  0.01M
chk_18  0.63ms  0.82ms  1.43ms  2.28  0.31K  0.00M
chk_19  0.47ms  0.72ms  1.28ms  2.73  0.13K  0.01M
chk_20  0.47ms  0.72ms  1.28ms  2.71  0.13K  0.01M
chk_21  0.56ms  0.79ms  1.37ms  2.44  0.18K  0.01M
chk_22  0.47ms  0.72ms  1.28ms  2.72  0.13K  0.01M
chk_23  0.56ms  0.79ms  1.36ms  2.45  0.16K  0.01M
chk_24  0.45ms  0.72ms  1.26ms  2.83  0.09K  0.02M
chk_25  0.45ms  0.72ms  1.27ms  2.80  0.09K  0.02M
chk_26  0.56ms  0.79ms  1.37ms  2.45  0.18K  0.01M
chk_27  0.47ms  0.72ms  1.28ms  2.73  0.13K  0.01M
chk_28  0.55ms  0.79ms  1.36ms  2.46  0.17K  0.01M
chk_29  0.52ms  0.76ms  1.34ms  2.59  0.15K  0.01M
chk_30  0.59ms  0.81ms  1.40ms  2.36  0.24K  0.01M
chk_31  0.58ms  0.79ms  1.37ms  2.37  0.20K  0.01M
   Avg  0.53  0.77  1.33
   Max  0.63  0.82  1.43
   Min  0.44  0.72  1.26
 Ratio  1.42  1.15  1.14
   Var  0.00  0.00  0.00
Profiling takes 1.063 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 55)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [55, 111)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [55, 111)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 55)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [111, 167)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [111, 167)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [167, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [167, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 55)...
+++++++++ Node 2 initializing the weights for op[55, 111)...
+++++++++ Node 5 initializing the weights for op[111, 167)...
+++++++++ Node 0 initializing the weights for op[0, 55)...
+++++++++ Node 3 initializing the weights for op[55, 111)...
+++++++++ Node 6 initializing the weights for op[167, 229)...
+++++++++ Node 4 initializing the weights for op[111, 167)...
+++++++++ Node 7 initializing the weights for op[167, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.0100	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2055
	Epoch 50:	Loss 1.7313	TrainAcc 0.2268	ValidAcc 0.2169	TestAcc 0.2277	BestValid 0.2169
	Epoch 100:	Loss 1.6733	TrainAcc 0.2484	ValidAcc 0.2314	TestAcc 0.2450	BestValid 0.2314
	Epoch 150:	Loss 1.6039	TrainAcc 0.2556	ValidAcc 0.2608	TestAcc 0.2603	BestValid 0.2608
	Epoch 200:	Loss 1.5772	TrainAcc 0.2564	ValidAcc 0.2506	TestAcc 0.2709	BestValid 0.2608
	Epoch 250:	Loss 1.5058	TrainAcc 0.2792	ValidAcc 0.2626	TestAcc 0.2959	BestValid 0.2626
	Epoch 300:	Loss 1.4684	TrainAcc 0.3073	ValidAcc 0.2746	TestAcc 0.2863	BestValid 0.2746
	Epoch 350:	Loss 1.3529	TrainAcc 0.3918	ValidAcc 0.3083	TestAcc 0.3208	BestValid 0.3083
	Epoch 400:	Loss 1.2682	TrainAcc 0.4615	ValidAcc 0.3534	TestAcc 0.3679	BestValid 0.3534
	Epoch 450:	Loss 1.2181	TrainAcc 0.4211	ValidAcc 0.3329	TestAcc 0.3449	BestValid 0.3534
	Epoch 500:	Loss 1.1615	TrainAcc 0.5172	ValidAcc 0.3774	TestAcc 0.4035	BestValid 0.3774
	Epoch 550:	Loss 1.0912	TrainAcc 0.5521	ValidAcc 0.3954	TestAcc 0.4217	BestValid 0.3954
	Epoch 600:	Loss 1.0568	TrainAcc 0.5212	ValidAcc 0.3726	TestAcc 0.3910	BestValid 0.3954
	Epoch 650:	Loss 0.9981	TrainAcc 0.5805	ValidAcc 0.3852	TestAcc 0.4169	BestValid 0.3954
	Epoch 700:	Loss 0.9348	TrainAcc 0.5545	ValidAcc 0.3708	TestAcc 0.3890	BestValid 0.3954
	Epoch 750:	Loss 0.8320	TrainAcc 0.6639	ValidAcc 0.4387	TestAcc 0.4621	BestValid 0.4387
	Epoch 800:	Loss 0.7713	TrainAcc 0.6951	ValidAcc 0.4483	TestAcc 0.4909	BestValid 0.4483
	Epoch 850:	Loss 0.7283	TrainAcc 0.7143	ValidAcc 0.4657	TestAcc 0.4966	BestValid 0.4657
	Epoch 900:	Loss 0.7060	TrainAcc 0.7228	ValidAcc 0.4633	TestAcc 0.4774	BestValid 0.4657
	Epoch 950:	Loss 0.6952	TrainAcc 0.7103	ValidAcc 0.4447	TestAcc 0.4582	BestValid 0.4657
	Epoch 1000:	Loss 0.6378	TrainAcc 0.7175	ValidAcc 0.4519	TestAcc 0.4822	BestValid 0.4657
	Epoch 1050:	Loss 0.6030	TrainAcc 0.7676	ValidAcc 0.4784	TestAcc 0.4976	BestValid 0.4784
	Epoch 1100:	Loss 0.5833	TrainAcc 0.7937	ValidAcc 0.4832	TestAcc 0.4861	BestValid 0.4832
	Epoch 1150:	Loss 0.5759	TrainAcc 0.7680	ValidAcc 0.4748	TestAcc 0.4918	BestValid 0.4832
	Epoch 1200:	Loss 0.5037	TrainAcc 0.8085	ValidAcc 0.5012	TestAcc 0.5139	BestValid 0.5012
	Epoch 1250:	Loss 0.4992	TrainAcc 0.7885	ValidAcc 0.4808	TestAcc 0.4995	BestValid 0.5012
	Epoch 1300:	Loss 0.4622	TrainAcc 0.7969	ValidAcc 0.4784	TestAcc 0.5005	BestValid 0.5012
	Epoch 1350:	Loss 0.4510	TrainAcc 0.8438	ValidAcc 0.5096	TestAcc 0.5226	BestValid 0.5096
	Epoch 1400:	Loss 0.4442	TrainAcc 0.8165	ValidAcc 0.4742	TestAcc 0.4995	BestValid 0.5096
	Epoch 1450:	Loss 0.4225	TrainAcc 0.7408	ValidAcc 0.4519	TestAcc 0.4669	BestValid 0.5096
	Epoch 1500:	Loss 0.4104	TrainAcc 0.8385	ValidAcc 0.4904	TestAcc 0.5226	BestValid 0.5096
	Epoch 1550:	Loss 0.3809	TrainAcc 0.8417	ValidAcc 0.5060	TestAcc 0.5207	BestValid 0.5096
	Epoch 1600:	Loss 0.3742	TrainAcc 0.8538	ValidAcc 0.5054	TestAcc 0.5351	BestValid 0.5096
	Epoch 1650:	Loss 0.3576	TrainAcc 0.8225	ValidAcc 0.4675	TestAcc 0.5062	BestValid 0.5096
	Epoch 1700:	Loss 0.3240	TrainAcc 0.8854	ValidAcc 0.5210	TestAcc 0.5437	BestValid 0.5210
	Epoch 1750:	Loss 0.3385	TrainAcc 0.8446	ValidAcc 0.4688	TestAcc 0.5091	BestValid 0.5210
	Epoch 1800:	Loss 0.3447	TrainAcc 0.8277	ValidAcc 0.4718	TestAcc 0.5072	BestValid 0.5210
	Epoch 1850:	Loss 0.2749	TrainAcc 0.9135	ValidAcc 0.5331	TestAcc 0.5629	BestValid 0.5331
	Epoch 1900:	Loss 0.2632	TrainAcc 0.9147	ValidAcc 0.5288	TestAcc 0.5476	BestValid 0.5331
	Epoch 1950:	Loss 0.2818	TrainAcc 0.8978	ValidAcc 0.5120	TestAcc 0.5476	BestValid 0.5331
	Epoch 2000:	Loss 0.2562	TrainAcc 0.9099	ValidAcc 0.5138	TestAcc 0.5572	BestValid 0.5331
	Epoch 2050:	Loss 0.2585	TrainAcc 0.8157	ValidAcc 0.4706	TestAcc 0.4928	BestValid 0.5331
	Epoch 2100:	Loss 0.2458	TrainAcc 0.9307	ValidAcc 0.5270	TestAcc 0.5418	BestValid 0.5331
	Epoch 2150:	Loss 0.2427	TrainAcc 0.8898	ValidAcc 0.5012	TestAcc 0.5255	BestValid 0.5331
	Epoch 2200:	Loss 0.2649	TrainAcc 0.8858	ValidAcc 0.5168	TestAcc 0.5264	BestValid 0.5331
	Epoch 2250:	Loss 0.2318	TrainAcc 0.9119	ValidAcc 0.5120	TestAcc 0.5255	BestValid 0.5331
	Epoch 2300:	Loss 0.2370	TrainAcc 0.9091	ValidAcc 0.5174	TestAcc 0.5408	BestValid 0.5331
	Epoch 2350:	Loss 0.2266	TrainAcc 0.9299	ValidAcc 0.5349	TestAcc 0.5629	BestValid 0.5349
	Epoch 2400:	Loss 0.2295	TrainAcc 0.9275	ValidAcc 0.5246	TestAcc 0.5427	BestValid 0.5349
	Epoch 2450:	Loss 0.2085	TrainAcc 0.9079	ValidAcc 0.5210	TestAcc 0.5447	BestValid 0.5349
	Epoch 2500:	Loss 0.1981	TrainAcc 0.9111	ValidAcc 0.5210	TestAcc 0.5591	BestValid 0.5349
	Epoch 2550:	Loss 0.1764	TrainAcc 0.9479	ValidAcc 0.5415	TestAcc 0.5735	BestValid 0.5415
	Epoch 2600:	Loss 0.1952	TrainAcc 0.9107	ValidAcc 0.5192	TestAcc 0.5466	BestValid 0.5415
	Epoch 2650:	Loss 0.1924	TrainAcc 0.9467	ValidAcc 0.5439	TestAcc 0.5773	BestValid 0.5439
	Epoch 2700:	Loss 0.1826	TrainAcc 0.9459	ValidAcc 0.5391	TestAcc 0.5610	BestValid 0.5439
	Epoch 2750:	Loss 0.1677	TrainAcc 0.9343	ValidAcc 0.5343	TestAcc 0.5581	BestValid 0.5439
	Epoch 2800:	Loss 0.1747	TrainAcc 0.9419	ValidAcc 0.5415	TestAcc 0.5706	BestValid 0.5439
	Epoch 2850:	Loss 0.1663	TrainAcc 0.9119	ValidAcc 0.5114	TestAcc 0.5226	BestValid 0.5439
	Epoch 2900:	Loss 0.1532	TrainAcc 0.9375	ValidAcc 0.5264	TestAcc 0.5562	BestValid 0.5439
	Epoch 2950:	Loss 0.1531	TrainAcc 0.9275	ValidAcc 0.5120	TestAcc 0.5312	BestValid 0.5439
	Epoch 3000:	Loss 0.1425	TrainAcc 0.9487	ValidAcc 0.5294	TestAcc 0.5495	BestValid 0.5439
	Epoch 3050:	Loss 0.1325	TrainAcc 0.9443	ValidAcc 0.5300	TestAcc 0.5524	BestValid 0.5439
	Epoch 3100:	Loss 0.1562	TrainAcc 0.9095	ValidAcc 0.5090	TestAcc 0.5197	BestValid 0.5439
	Epoch 3150:	Loss 0.1377	TrainAcc 0.9191	ValidAcc 0.5138	TestAcc 0.5101	BestValid 0.5439
	Epoch 3200:	Loss 0.1267	TrainAcc 0.9571	ValidAcc 0.5421	TestAcc 0.5658	BestValid 0.5439
	Epoch 3250:	Loss 0.1465	TrainAcc 0.9583	ValidAcc 0.5475	TestAcc 0.5562	BestValid 0.5475
	Epoch 3300:	Loss 0.1474	TrainAcc 0.9375	ValidAcc 0.5252	TestAcc 0.5418	BestValid 0.5475
	Epoch 3350:	Loss 0.1378	TrainAcc 0.9483	ValidAcc 0.5355	TestAcc 0.5610	BestValid 0.5475
	Epoch 3400:	Loss 0.1254	TrainAcc 0.9323	ValidAcc 0.5361	TestAcc 0.5543	BestValid 0.5475
	Epoch 3450:	Loss 0.1169	TrainAcc 0.9712	ValidAcc 0.5523	TestAcc 0.5600	BestValid 0.5523
	Epoch 3500:	Loss 0.1160	TrainAcc 0.9639	ValidAcc 0.5457	TestAcc 0.5668	BestValid 0.5523
	Epoch 3550:	Loss 0.1058	TrainAcc 0.9539	ValidAcc 0.5325	TestAcc 0.5514	BestValid 0.5523
	Epoch 3600:	Loss 0.1062	TrainAcc 0.9575	ValidAcc 0.5493	TestAcc 0.5600	BestValid 0.5523
	Epoch 3650:	Loss 0.1507	TrainAcc 0.9655	ValidAcc 0.5445	TestAcc 0.5610	BestValid 0.5523
	Epoch 3700:	Loss 0.1008	TrainAcc 0.9671	ValidAcc 0.5493	TestAcc 0.5610	BestValid 0.5523
	Epoch 3750:	Loss 0.1206	TrainAcc 0.9639	ValidAcc 0.5427	TestAcc 0.5572	BestValid 0.5523
	Epoch 3800:	Loss 0.1280	TrainAcc 0.9211	ValidAcc 0.5331	TestAcc 0.5485	BestValid 0.5523
	Epoch 3850:	Loss 0.1348	TrainAcc 0.9471	ValidAcc 0.5319	TestAcc 0.5504	BestValid 0.5523
	Epoch 3900:	Loss 0.1010	TrainAcc 0.9607	ValidAcc 0.5415	TestAcc 0.5504	BestValid 0.5523
	Epoch 3950:	Loss 0.1033	TrainAcc 0.9387	ValidAcc 0.5210	TestAcc 0.5351	BestValid 0.5523
	Epoch 4000:	Loss 0.0899	TrainAcc 0.9700	ValidAcc 0.5421	TestAcc 0.5552	BestValid 0.5523
	Epoch 4050:	Loss 0.1165	TrainAcc 0.9515	ValidAcc 0.5373	TestAcc 0.5476	BestValid 0.5523
	Epoch 4100:	Loss 0.0804	TrainAcc 0.9700	ValidAcc 0.5409	TestAcc 0.5533	BestValid 0.5523
	Epoch 4150:	Loss 0.1390	TrainAcc 0.9459	ValidAcc 0.5288	TestAcc 0.5552	BestValid 0.5523
	Epoch 4200:	Loss 0.0974	TrainAcc 0.9712	ValidAcc 0.5463	TestAcc 0.5485	BestValid 0.5523
	Epoch 4250:	Loss 0.0769	TrainAcc 0.9740	ValidAcc 0.5517	TestAcc 0.5600	BestValid 0.5523
	Epoch 4300:	Loss 0.0740	TrainAcc 0.9511	ValidAcc 0.5306	TestAcc 0.5552	BestValid 0.5523
	Epoch 4350:	Loss 0.0916	TrainAcc 0.9627	ValidAcc 0.5439	TestAcc 0.5543	BestValid 0.5523
	Epoch 4400:	Loss 0.0749	TrainAcc 0.9720	ValidAcc 0.5475	TestAcc 0.5629	BestValid 0.5523
	Epoch 4450:	Loss 0.0752	TrainAcc 0.9712	ValidAcc 0.5517	TestAcc 0.5620	BestValid 0.5523
	Epoch 4500:	Loss 0.1072	TrainAcc 0.9688	ValidAcc 0.5415	TestAcc 0.5514	BestValid 0.5523
	Epoch 4550:	Loss 0.0706	TrainAcc 0.9780	ValidAcc 0.5487	TestAcc 0.5437	BestValid 0.5523
	Epoch 4600:	Loss 0.0693	TrainAcc 0.9776	ValidAcc 0.5499	TestAcc 0.5552	BestValid 0.5523
	Epoch 4650:	Loss 0.0684	TrainAcc 0.9780	ValidAcc 0.5517	TestAcc 0.5725	BestValid 0.5523
	Epoch 4700:	Loss 0.0663	TrainAcc 0.9792	ValidAcc 0.5517	TestAcc 0.5610	BestValid 0.5523
	Epoch 4750:	Loss 0.0978	TrainAcc 0.9239	ValidAcc 0.5096	TestAcc 0.5379	BestValid 0.5523
	Epoch 4800:	Loss 0.0673	TrainAcc 0.9796	ValidAcc 0.5463	TestAcc 0.5591	BestValid 0.5523
	Epoch 4850:	Loss 0.0672	TrainAcc 0.9752	ValidAcc 0.5427	TestAcc 0.5600	BestValid 0.5523
	Epoch 4900:	Loss 0.1065	TrainAcc 0.9523	ValidAcc 0.5294	TestAcc 0.5427	BestValid 0.5523
	Epoch 4950:	Loss 0.1111	TrainAcc 0.9263	ValidAcc 0.5144	TestAcc 0.5207	BestValid 0.5523
	Epoch 5000:	Loss 0.0657	TrainAcc 0.9820	ValidAcc 0.5445	TestAcc 0.5639	BestValid 0.5523
****** Epoch Time (Excluding Evaluation Cost): 0.175 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 23.324 ms (Max: 23.390, Min: 23.238, Sum: 186.591)
Cluster-Wide Average, Compute: 89.490 ms (Max: 94.150, Min: 85.730, Sum: 715.922)
Cluster-Wide Average, Communication-Layer: 3.427 ms (Max: 4.287, Min: 2.639, Sum: 27.418)
Cluster-Wide Average, Bubble-Imbalance: 7.241 ms (Max: 9.989, Min: 4.436, Sum: 57.924)
Cluster-Wide Average, Communication-Graph: 25.513 ms (Max: 27.727, Min: 23.531, Sum: 204.103)
Cluster-Wide Average, Optimization: 22.263 ms (Max: 24.848, Min: 20.633, Sum: 178.100)
Cluster-Wide Average, Others: 3.386 ms (Max: 4.743, Min: 1.134, Sum: 27.091)
****** Breakdown Sum: 174.644 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.493 GB (Max: 4.169, Min: 3.315, Sum: 27.946)
Cluster-Wide Average, Graph-Level Communication Throughput: 59.160 Gbps (Max: 69.154, Min: 50.093, Sum: 473.276)
Cluster-Wide Average, Layer-Level Communication Throughput: 35.391 Gbps (Max: 42.712, Min: 29.216, Sum: 283.130)
Layer-level communication (cluster-wide, per-epoch): 0.116 GB
Graph-level communication (cluster-wide, per-epoch): 1.021 GB
Weight-sync communication (cluster-wide, per-epoch): 0.254 GB
Total communication (cluster-wide, per-epoch): 1.391 GB
****** Accuracy Results ******
Highest valid_acc: 0.5523
Target test_acc: 0.5600
Epoch to reach the target acc: 3449
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
