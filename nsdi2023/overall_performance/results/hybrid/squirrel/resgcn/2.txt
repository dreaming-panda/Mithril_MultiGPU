Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITDONE MPI INIT
Initialized node 6 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8

Initialized node 5 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.006 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.012 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.009 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.013 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.497 Gbps (per GPU), 483.974 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.185 Gbps (per GPU), 481.484 Gbps (aggregated)
The layer-level communication performance: 60.178 Gbps (per GPU), 481.427 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.936 Gbps (per GPU), 479.489 Gbps (aggregated)
The layer-level communication performance: 59.898 Gbps (per GPU), 479.186 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.679 Gbps (per GPU), 477.435 Gbps (aggregated)
The layer-level communication performance: 59.631 Gbps (per GPU), 477.051 Gbps (aggregated)
The layer-level communication performance: 59.592 Gbps (per GPU), 476.739 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.576 Gbps (per GPU), 1276.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.579 Gbps (per GPU), 1276.635 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.567 Gbps (per GPU), 1276.538 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.558 Gbps (per GPU), 1276.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.591 Gbps (per GPU), 1276.732 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.579 Gbps (per GPU), 1276.635 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.564 Gbps (per GPU), 1276.513 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.561 Gbps (per GPU), 1276.489 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.977 Gbps (per GPU), 839.813 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.975 Gbps (per GPU), 839.799 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.970 Gbps (per GPU), 839.764 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.977 Gbps (per GPU), 839.813 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.973 Gbps (per GPU), 839.785 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.974 Gbps (per GPU), 839.792 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.969 Gbps (per GPU), 839.750 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.973 Gbps (per GPU), 839.785 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.947 Gbps (per GPU), 279.574 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.947 Gbps (per GPU), 279.579 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.945 Gbps (per GPU), 279.558 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.945 Gbps (per GPU), 279.564 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.942 Gbps (per GPU), 279.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.944 Gbps (per GPU), 279.553 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.942 Gbps (per GPU), 279.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.943 Gbps (per GPU), 279.546 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.44ms  0.70ms  1.21ms  2.78  0.08K  0.02M
 chk_1  0.53ms  0.73ms  1.28ms  2.41  0.16K  0.01M
 chk_2  0.47ms  0.69ms  1.23ms  2.64  0.13K  0.01M
 chk_3  0.56ms  0.77ms  1.31ms  2.35  0.18K  0.01M
 chk_4  0.51ms  0.73ms  1.28ms  2.51  0.15K  0.01M
 chk_5  0.59ms  0.78ms  1.34ms  2.26  0.25K  0.01M
 chk_6  0.44ms  0.69ms  1.22ms  2.76  0.10K  0.02M
 chk_7  0.47ms  0.70ms  1.23ms  2.62  0.11K  0.02M
 chk_8  0.59ms  0.78ms  1.34ms  2.29  0.23K  0.01M
 chk_9  0.50ms  0.72ms  1.27ms  2.51  0.14K  0.01M
chk_10  0.58ms  0.77ms  1.32ms  2.28  0.20K  0.01M
chk_11  0.44ms  0.69ms  1.21ms  2.75  0.09K  0.02M
chk_12  0.51ms  0.73ms  1.28ms  2.51  0.16K  0.01M
chk_13  0.51ms  0.73ms  1.28ms  2.51  0.16K  0.01M
chk_14  0.50ms  0.73ms  1.27ms  2.52  0.14K  0.01M
chk_15  0.58ms  0.78ms  1.33ms  2.30  0.21K  0.01M
chk_16  0.56ms  0.77ms  1.32ms  2.37  0.18K  0.01M
chk_17  0.62ms  0.79ms  1.37ms  2.22  0.29K  0.01M
chk_18  0.62ms  0.79ms  1.38ms  2.21  0.31K  0.00M
chk_19  0.46ms  0.69ms  1.23ms  2.66  0.13K  0.01M
chk_20  0.46ms  0.69ms  1.23ms  2.65  0.13K  0.01M
chk_21  0.56ms  0.76ms  1.31ms  2.36  0.18K  0.01M
chk_22  0.46ms  0.69ms  1.23ms  2.65  0.13K  0.01M
chk_23  0.55ms  0.76ms  1.31ms  2.38  0.16K  0.01M
chk_24  0.44ms  0.69ms  1.22ms  2.76  0.09K  0.02M
chk_25  0.45ms  0.69ms  1.22ms  2.73  0.09K  0.02M
chk_26  0.55ms  0.76ms  1.31ms  2.37  0.18K  0.01M
chk_27  0.46ms  0.69ms  1.23ms  2.65  0.13K  0.01M
chk_28  0.55ms  0.76ms  1.31ms  2.38  0.17K  0.01M
chk_29  0.51ms  0.73ms  1.28ms  2.52  0.15K  0.01M
chk_30  0.59ms  0.78ms  1.35ms  2.28  0.24K  0.01M
chk_31  0.57ms  0.76ms  1.32ms  2.29  0.20K  0.01M
   Avg  0.52  0.74  1.28
   Max  0.62  0.79  1.38
   Min  0.44  0.69  1.21
 Ratio  1.43  1.15  1.14
   Var  0.00  0.00  0.00
Profiling takes 1.025 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 55)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 55)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [55, 111)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [55, 111)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [111, 167)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [111, 167)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [167, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [167, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 55)...
+++++++++ Node 3 initializing the weights for op[55, 111)...
+++++++++ Node 7 initializing the weights for op[167, 229)...
+++++++++ Node 2 initializing the weights for op[55, 111)...
+++++++++ Node 6 initializing the weights for op[167, 229)...
+++++++++ Node 5 initializing the weights for op[111, 167)...
+++++++++ Node 4 initializing the weights for op[111, 167)...
+++++++++ Node 1 initializing the weights for op[0, 55)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.0490	TrainAcc 0.2059	ValidAcc 0.1953	TestAcc 0.1931	BestValid 0.1953
	Epoch 50:	Loss 1.7497	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 100:	Loss 1.6914	TrainAcc 0.2416	ValidAcc 0.2284	TestAcc 0.2354	BestValid 0.2284
	Epoch 150:	Loss 1.6453	TrainAcc 0.2688	ValidAcc 0.2566	TestAcc 0.2757	BestValid 0.2566
	Epoch 200:	Loss 1.5867	TrainAcc 0.2624	ValidAcc 0.2632	TestAcc 0.2671	BestValid 0.2632
	Epoch 250:	Loss 1.5527	TrainAcc 0.2752	ValidAcc 0.2788	TestAcc 0.2767	BestValid 0.2788
	Epoch 300:	Loss 1.5165	TrainAcc 0.2929	ValidAcc 0.2861	TestAcc 0.2863	BestValid 0.2861
	Epoch 350:	Loss 1.4579	TrainAcc 0.2873	ValidAcc 0.2662	TestAcc 0.3055	BestValid 0.2861
	Epoch 400:	Loss 1.3690	TrainAcc 0.4030	ValidAcc 0.3365	TestAcc 0.3410	BestValid 0.3365
	Epoch 450:	Loss 1.2902	TrainAcc 0.4571	ValidAcc 0.3612	TestAcc 0.3631	BestValid 0.3612
	Epoch 500:	Loss 1.1962	TrainAcc 0.5048	ValidAcc 0.3792	TestAcc 0.3910	BestValid 0.3792
	Epoch 550:	Loss 1.1142	TrainAcc 0.5112	ValidAcc 0.3516	TestAcc 0.3670	BestValid 0.3792
	Epoch 600:	Loss 1.0647	TrainAcc 0.5757	ValidAcc 0.4002	TestAcc 0.4121	BestValid 0.4002
	Epoch 650:	Loss 0.9836	TrainAcc 0.6262	ValidAcc 0.4261	TestAcc 0.4361	BestValid 0.4261
	Epoch 700:	Loss 0.9247	TrainAcc 0.6270	ValidAcc 0.4279	TestAcc 0.4496	BestValid 0.4279
	Epoch 750:	Loss 0.8667	TrainAcc 0.6454	ValidAcc 0.4483	TestAcc 0.4669	BestValid 0.4483
	Epoch 800:	Loss 0.8379	TrainAcc 0.6390	ValidAcc 0.4243	TestAcc 0.4563	BestValid 0.4483
	Epoch 850:	Loss 0.7854	TrainAcc 0.7031	ValidAcc 0.4663	TestAcc 0.4890	BestValid 0.4663
	Epoch 900:	Loss 0.8104	TrainAcc 0.6823	ValidAcc 0.4507	TestAcc 0.4582	BestValid 0.4663
	Epoch 950:	Loss 0.7094	TrainAcc 0.7059	ValidAcc 0.4675	TestAcc 0.4793	BestValid 0.4675
	Epoch 1000:	Loss 0.6974	TrainAcc 0.6683	ValidAcc 0.4273	TestAcc 0.4400	BestValid 0.4675
	Epoch 1050:	Loss 0.6419	TrainAcc 0.7536	ValidAcc 0.4910	TestAcc 0.5312	BestValid 0.4910
	Epoch 1100:	Loss 0.6135	TrainAcc 0.7688	ValidAcc 0.4802	TestAcc 0.5005	BestValid 0.4910
	Epoch 1150:	Loss 0.5508	TrainAcc 0.7973	ValidAcc 0.4862	TestAcc 0.5370	BestValid 0.4910
	Epoch 1200:	Loss 0.5495	TrainAcc 0.7897	ValidAcc 0.4946	TestAcc 0.5139	BestValid 0.4946
	Epoch 1250:	Loss 0.5806	TrainAcc 0.7716	ValidAcc 0.4790	TestAcc 0.5005	BestValid 0.4946
	Epoch 1300:	Loss 0.4679	TrainAcc 0.8197	ValidAcc 0.4994	TestAcc 0.5149	BestValid 0.4994
	Epoch 1350:	Loss 0.4647	TrainAcc 0.8253	ValidAcc 0.5030	TestAcc 0.5226	BestValid 0.5030
	Epoch 1400:	Loss 0.4657	TrainAcc 0.7973	ValidAcc 0.4976	TestAcc 0.5216	BestValid 0.5030
	Epoch 1450:	Loss 0.4321	TrainAcc 0.8305	ValidAcc 0.5012	TestAcc 0.5322	BestValid 0.5030
	Epoch 1500:	Loss 0.5225	TrainAcc 0.8157	ValidAcc 0.4754	TestAcc 0.5082	BestValid 0.5030
	Epoch 1550:	Loss 0.4259	TrainAcc 0.8277	ValidAcc 0.5042	TestAcc 0.5110	BestValid 0.5042
	Epoch 1600:	Loss 0.3955	TrainAcc 0.8746	ValidAcc 0.5138	TestAcc 0.5312	BestValid 0.5138
	Epoch 1650:	Loss 0.3673	TrainAcc 0.8409	ValidAcc 0.5042	TestAcc 0.5168	BestValid 0.5138
	Epoch 1700:	Loss 0.3723	TrainAcc 0.8794	ValidAcc 0.5325	TestAcc 0.5427	BestValid 0.5325
	Epoch 1750:	Loss 0.3494	TrainAcc 0.8806	ValidAcc 0.5204	TestAcc 0.5370	BestValid 0.5325
	Epoch 1800:	Loss 0.3516	TrainAcc 0.8666	ValidAcc 0.5048	TestAcc 0.5187	BestValid 0.5325
	Epoch 1850:	Loss 0.2933	TrainAcc 0.9071	ValidAcc 0.5312	TestAcc 0.5408	BestValid 0.5325
	Epoch 1900:	Loss 0.3138	TrainAcc 0.8730	ValidAcc 0.5204	TestAcc 0.5351	BestValid 0.5325
	Epoch 1950:	Loss 0.2846	TrainAcc 0.8846	ValidAcc 0.5270	TestAcc 0.5216	BestValid 0.5325
	Epoch 2000:	Loss 0.2911	TrainAcc 0.9103	ValidAcc 0.5409	TestAcc 0.5639	BestValid 0.5409
	Epoch 2050:	Loss 0.3228	TrainAcc 0.8245	ValidAcc 0.4700	TestAcc 0.4726	BestValid 0.5409
	Epoch 2100:	Loss 0.2746	TrainAcc 0.8582	ValidAcc 0.4982	TestAcc 0.5024	BestValid 0.5409
	Epoch 2150:	Loss 0.2398	TrainAcc 0.9275	ValidAcc 0.5481	TestAcc 0.5504	BestValid 0.5481
	Epoch 2200:	Loss 0.2323	TrainAcc 0.9079	ValidAcc 0.5439	TestAcc 0.5389	BestValid 0.5481
	Epoch 2250:	Loss 0.2446	TrainAcc 0.9155	ValidAcc 0.5306	TestAcc 0.5466	BestValid 0.5481
	Epoch 2300:	Loss 0.2274	TrainAcc 0.8726	ValidAcc 0.5066	TestAcc 0.5197	BestValid 0.5481
	Epoch 2350:	Loss 0.2411	TrainAcc 0.8878	ValidAcc 0.5198	TestAcc 0.5197	BestValid 0.5481
	Epoch 2400:	Loss 0.2179	TrainAcc 0.9267	ValidAcc 0.5451	TestAcc 0.5495	BestValid 0.5481
	Epoch 2450:	Loss 0.1965	TrainAcc 0.9187	ValidAcc 0.5270	TestAcc 0.5399	BestValid 0.5481
	Epoch 2500:	Loss 0.2015	TrainAcc 0.8990	ValidAcc 0.5186	TestAcc 0.5149	BestValid 0.5481
	Epoch 2550:	Loss 0.1772	TrainAcc 0.9299	ValidAcc 0.5469	TestAcc 0.5437	BestValid 0.5481
	Epoch 2600:	Loss 0.1856	TrainAcc 0.9195	ValidAcc 0.5367	TestAcc 0.5351	BestValid 0.5481
	Epoch 2650:	Loss 0.2122	TrainAcc 0.9175	ValidAcc 0.5337	TestAcc 0.5331	BestValid 0.5481
	Epoch 2700:	Loss 0.1846	TrainAcc 0.9427	ValidAcc 0.5493	TestAcc 0.5591	BestValid 0.5493
	Epoch 2750:	Loss 0.1758	TrainAcc 0.9203	ValidAcc 0.5306	TestAcc 0.5293	BestValid 0.5493
	Epoch 2800:	Loss 0.1783	TrainAcc 0.9175	ValidAcc 0.5258	TestAcc 0.5447	BestValid 0.5493
	Epoch 2850:	Loss 0.1997	TrainAcc 0.9403	ValidAcc 0.5481	TestAcc 0.5485	BestValid 0.5493
	Epoch 2900:	Loss 0.1857	TrainAcc 0.9483	ValidAcc 0.5517	TestAcc 0.5610	BestValid 0.5517
	Epoch 2950:	Loss 0.1590	TrainAcc 0.9335	ValidAcc 0.5319	TestAcc 0.5389	BestValid 0.5517
	Epoch 3000:	Loss 0.1444	TrainAcc 0.9038	ValidAcc 0.5090	TestAcc 0.5235	BestValid 0.5517
	Epoch 3050:	Loss 0.1602	TrainAcc 0.9115	ValidAcc 0.5138	TestAcc 0.5168	BestValid 0.5517
	Epoch 3100:	Loss 0.1523	TrainAcc 0.9639	ValidAcc 0.5541	TestAcc 0.5648	BestValid 0.5541
	Epoch 3150:	Loss 0.1277	TrainAcc 0.9447	ValidAcc 0.5451	TestAcc 0.5552	BestValid 0.5541
	Epoch 3200:	Loss 0.1494	TrainAcc 0.9547	ValidAcc 0.5535	TestAcc 0.5591	BestValid 0.5541
	Epoch 3250:	Loss 0.1321	TrainAcc 0.9515	ValidAcc 0.5493	TestAcc 0.5600	BestValid 0.5541
	Epoch 3300:	Loss 0.1313	TrainAcc 0.8986	ValidAcc 0.5066	TestAcc 0.5245	BestValid 0.5541
	Epoch 3350:	Loss 0.1550	TrainAcc 0.9323	ValidAcc 0.5288	TestAcc 0.5389	BestValid 0.5541
	Epoch 3400:	Loss 0.1106	TrainAcc 0.9619	ValidAcc 0.5493	TestAcc 0.5591	BestValid 0.5541
	Epoch 3450:	Loss 0.1252	TrainAcc 0.9647	ValidAcc 0.5475	TestAcc 0.5533	BestValid 0.5541
	Epoch 3500:	Loss 0.1323	TrainAcc 0.9199	ValidAcc 0.5144	TestAcc 0.5274	BestValid 0.5541
	Epoch 3550:	Loss 0.1056	TrainAcc 0.9503	ValidAcc 0.5409	TestAcc 0.5427	BestValid 0.5541
	Epoch 3600:	Loss 0.1404	TrainAcc 0.9531	ValidAcc 0.5397	TestAcc 0.5581	BestValid 0.5541
	Epoch 3650:	Loss 0.1073	TrainAcc 0.9667	ValidAcc 0.5487	TestAcc 0.5572	BestValid 0.5541
	Epoch 3700:	Loss 0.1116	TrainAcc 0.9643	ValidAcc 0.5433	TestAcc 0.5620	BestValid 0.5541
	Epoch 3750:	Loss 0.1431	TrainAcc 0.9371	ValidAcc 0.5204	TestAcc 0.5274	BestValid 0.5541
	Epoch 3800:	Loss 0.1402	TrainAcc 0.9175	ValidAcc 0.5234	TestAcc 0.5351	BestValid 0.5541
	Epoch 3850:	Loss 0.1101	TrainAcc 0.9515	ValidAcc 0.5421	TestAcc 0.5341	BestValid 0.5541
	Epoch 3900:	Loss 0.1037	TrainAcc 0.9435	ValidAcc 0.5300	TestAcc 0.5303	BestValid 0.5541
	Epoch 3950:	Loss 0.1123	TrainAcc 0.9487	ValidAcc 0.5325	TestAcc 0.5264	BestValid 0.5541
	Epoch 4000:	Loss 0.0966	TrainAcc 0.9415	ValidAcc 0.5264	TestAcc 0.5312	BestValid 0.5541
	Epoch 4050:	Loss 0.0986	TrainAcc 0.9772	ValidAcc 0.5541	TestAcc 0.5552	BestValid 0.5541
	Epoch 4100:	Loss 0.0927	TrainAcc 0.9704	ValidAcc 0.5517	TestAcc 0.5639	BestValid 0.5541
	Epoch 4150:	Loss 0.1337	TrainAcc 0.9551	ValidAcc 0.5493	TestAcc 0.5504	BestValid 0.5541
	Epoch 4200:	Loss 0.1033	TrainAcc 0.9615	ValidAcc 0.5445	TestAcc 0.5456	BestValid 0.5541
	Epoch 4250:	Loss 0.0837	TrainAcc 0.9704	ValidAcc 0.5445	TestAcc 0.5591	BestValid 0.5541
	Epoch 4300:	Loss 0.0927	TrainAcc 0.9760	ValidAcc 0.5571	TestAcc 0.5725	BestValid 0.5571
	Epoch 4350:	Loss 0.0978	TrainAcc 0.9451	ValidAcc 0.5240	TestAcc 0.5312	BestValid 0.5571
	Epoch 4400:	Loss 0.1159	TrainAcc 0.9042	ValidAcc 0.4988	TestAcc 0.5072	BestValid 0.5571
	Epoch 4450:	Loss 0.0763	TrainAcc 0.9651	ValidAcc 0.5493	TestAcc 0.5639	BestValid 0.5571
	Epoch 4500:	Loss 0.0674	TrainAcc 0.9776	ValidAcc 0.5541	TestAcc 0.5735	BestValid 0.5571
	Epoch 4550:	Loss 0.0919	TrainAcc 0.9756	ValidAcc 0.5481	TestAcc 0.5668	BestValid 0.5571
	Epoch 4600:	Loss 0.0684	TrainAcc 0.9736	ValidAcc 0.5475	TestAcc 0.5514	BestValid 0.5571
	Epoch 4650:	Loss 0.0909	TrainAcc 0.9623	ValidAcc 0.5367	TestAcc 0.5533	BestValid 0.5571
	Epoch 4700:	Loss 0.0937	TrainAcc 0.9623	ValidAcc 0.5385	TestAcc 0.5418	BestValid 0.5571
	Epoch 4750:	Loss 0.0676	TrainAcc 0.9720	ValidAcc 0.5415	TestAcc 0.5610	BestValid 0.5571
	Epoch 4800:	Loss 0.0708	TrainAcc 0.9832	ValidAcc 0.5493	TestAcc 0.5620	BestValid 0.5571
	Epoch 4850:	Loss 0.0632	TrainAcc 0.9679	ValidAcc 0.5397	TestAcc 0.5456	BestValid 0.5571
	Epoch 4900:	Loss 0.0746	TrainAcc 0.9840	ValidAcc 0.5529	TestAcc 0.5581	BestValid 0.5571
	Epoch 4950:	Loss 0.0877	TrainAcc 0.9651	ValidAcc 0.5439	TestAcc 0.5572	BestValid 0.5571
	Epoch 5000:	Loss 0.0793	TrainAcc 0.9756	ValidAcc 0.5469	TestAcc 0.5735	BestValid 0.5571
****** Epoch Time (Excluding Evaluation Cost): 0.175 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 23.415 ms (Max: 24.049, Min: 22.631, Sum: 187.319)
Cluster-Wide Average, Compute: 90.078 ms (Max: 98.800, Min: 84.894, Sum: 720.627)
Cluster-Wide Average, Communication-Layer: 3.444 ms (Max: 4.326, Min: 2.674, Sum: 27.554)
Cluster-Wide Average, Bubble-Imbalance: 9.230 ms (Max: 13.792, Min: 1.988, Sum: 73.842)
Cluster-Wide Average, Communication-Graph: 25.420 ms (Max: 26.796, Min: 23.931, Sum: 203.359)
Cluster-Wide Average, Optimization: 20.866 ms (Max: 22.494, Min: 19.764, Sum: 166.924)
Cluster-Wide Average, Others: 2.606 ms (Max: 3.501, Min: 1.197, Sum: 20.850)
****** Breakdown Sum: 175.059 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.493 GB (Max: 4.169, Min: 3.315, Sum: 27.946)
Cluster-Wide Average, Graph-Level Communication Throughput: 59.278 Gbps (Max: 65.574, Min: 53.497, Sum: 474.222)
Cluster-Wide Average, Layer-Level Communication Throughput: 35.216 Gbps (Max: 42.479, Min: 29.375, Sum: 281.727)
Layer-level communication (cluster-wide, per-epoch): 0.116 GB
Graph-level communication (cluster-wide, per-epoch): 1.021 GB
Weight-sync communication (cluster-wide, per-epoch): 0.254 GB
Total communication (cluster-wide, per-epoch): 1.391 GB
****** Accuracy Results ******
Highest valid_acc: 0.5571
Target test_acc: 0.5725
Epoch to reach the target acc: 4299
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
