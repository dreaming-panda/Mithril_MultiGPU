Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
Initialized node 0 on machine gnerv4
DONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv8
Initialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INIT
Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.010 seconds.
        It takes 0.010 seconds.
Building the Feature Vector...
        It takes 0.012 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.031 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.026 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.390 Gbps (per GPU), 483.120 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.076 Gbps (per GPU), 480.606 Gbps (aggregated)
The layer-level communication performance: 60.067 Gbps (per GPU), 480.533 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.822 Gbps (per GPU), 478.577 Gbps (aggregated)
The layer-level communication performance: 59.791 Gbps (per GPU), 478.327 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.587 Gbps (per GPU), 476.699 Gbps (aggregated)
The layer-level communication performance: 59.539 Gbps (per GPU), 476.314 Gbps (aggregated)
The layer-level communication performance: 59.500 Gbps (per GPU), 476.003 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.213 Gbps (per GPU), 1265.704 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.213 Gbps (per GPU), 1265.704 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.180 Gbps (per GPU), 1265.443 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.204 Gbps (per GPU), 1265.632 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.183 Gbps (per GPU), 1265.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.216 Gbps (per GPU), 1265.727 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.210 Gbps (per GPU), 1265.680 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.207 Gbps (per GPU), 1265.656 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.438 Gbps (per GPU), 827.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.438 Gbps (per GPU), 827.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.440 Gbps (per GPU), 827.524 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.436 Gbps (per GPU), 827.490 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.438 Gbps (per GPU), 827.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.438 Gbps (per GPU), 827.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.438 Gbps (per GPU), 827.504 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.436 Gbps (per GPU), 827.490 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.358 Gbps (per GPU), 306.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.364 Gbps (per GPU), 306.912 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.359 Gbps (per GPU), 306.874 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.358 Gbps (per GPU), 306.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.358 Gbps (per GPU), 306.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.358 Gbps (per GPU), 306.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.355 Gbps (per GPU), 306.841 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.351 Gbps (per GPU), 306.811 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.94ms  0.73ms  0.85ms  1.30  0.08K  0.02M
 chk_1  1.06ms  0.75ms  0.89ms  1.41  0.16K  0.01M
 chk_2  0.99ms  0.73ms  0.85ms  1.36  0.13K  0.01M
 chk_3  1.09ms  0.79ms  0.91ms  1.39  0.18K  0.01M
 chk_4  1.04ms  0.75ms  0.88ms  1.38  0.15K  0.01M
 chk_5  1.16ms  0.81ms  0.94ms  1.44  0.25K  0.01M
 chk_6  0.95ms  0.73ms  0.85ms  1.31  0.10K  0.02M
 chk_7  0.99ms  0.73ms  0.86ms  1.35  0.11K  0.02M
 chk_8  1.15ms  0.80ms  0.93ms  1.43  0.23K  0.01M
 chk_9  1.03ms  0.75ms  0.88ms  1.38  0.14K  0.01M
chk_10  1.13ms  0.79ms  0.92ms  1.44  0.20K  0.01M
chk_11  0.95ms  0.72ms  0.85ms  1.31  0.09K  0.02M
chk_12  1.05ms  0.75ms  0.88ms  1.39  0.16K  0.01M
chk_13  1.05ms  0.75ms  0.88ms  1.39  0.16K  0.01M
chk_14  1.04ms  0.75ms  0.88ms  1.38  0.14K  0.01M
chk_15  1.15ms  0.80ms  0.93ms  1.44  0.21K  0.01M
chk_16  1.09ms  0.78ms  0.91ms  1.40  0.18K  0.01M
chk_17  1.21ms  0.81ms  0.94ms  1.50  0.29K  0.01M
chk_18  1.23ms  0.81ms  0.94ms  1.51  0.31K  0.00M
chk_19  0.98ms  0.73ms  0.85ms  1.34  0.13K  0.01M
chk_20  0.98ms  0.73ms  0.86ms  1.34  0.13K  0.01M
chk_21  1.09ms  0.78ms  0.91ms  1.40  0.18K  0.01M
chk_22  0.98ms  0.73ms  0.85ms  1.34  0.13K  0.01M
chk_23  1.09ms  0.78ms  0.90ms  1.40  0.16K  0.01M
chk_24  0.95ms  0.73ms  0.85ms  1.30  0.09K  0.02M
chk_25  0.95ms  0.73ms  0.85ms  1.30  0.09K  0.02M
chk_26  1.09ms  0.78ms  0.91ms  1.39  0.18K  0.01M
chk_27  0.98ms  0.73ms  0.85ms  1.34  0.13K  0.01M
chk_28  1.09ms  0.78ms  0.91ms  1.39  0.17K  0.01M
chk_29  1.05ms  0.76ms  0.88ms  1.39  0.15K  0.01M
chk_30  1.16ms  0.81ms  0.93ms  1.44  0.24K  0.01M
chk_31  1.14ms  0.79ms  0.92ms  1.45  0.20K  0.01M
   Avg  1.06  0.76  0.89
   Max  1.23  0.81  0.94
   Min  0.94  0.72  0.85
 Ratio  1.30  1.12  1.11
   Var  0.01  0.00  0.00
Profiling takes 1.088 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 0 owns the model-level partition [0, 67)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 1 owns the model-level partition [0, 67)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 2, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 2 owns the model-level partition [67, 131)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 3, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 3 owns the model-level partition [67, 131)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 5, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 5 owns the model-level partition [131, 195)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [195, 262)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [195, 262)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
Node 4, Pipeline Input Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
Node 4, Pipeline Output Tensor: OPERATOR_LAYER_NORM_NO_AFFINE
*** Node 4 owns the model-level partition [131, 195)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[67, 131)...
+++++++++ Node 1 initializing the weights for op[0, 67)...
+++++++++ Node 0 initializing the weights for op[0, 67)...
+++++++++ Node 3 initializing the weights for op[67, 131)...
+++++++++ Node 5 initializing the weights for op[131, 195)...
+++++++++ Node 6 initializing the weights for op[195, 262)...
+++++++++ Node 7 initializing the weights for op[195, 262)...
+++++++++ Node 4 initializing the weights for op[131, 195)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.1867	TrainAcc 0.2524	ValidAcc 0.2458	TestAcc 0.2430	BestValid 0.2458
	Epoch 50:	Loss 1.5305	TrainAcc 0.3037	ValidAcc 0.3053	TestAcc 0.2882	BestValid 0.3053
	Epoch 100:	Loss 1.4845	TrainAcc 0.3229	ValidAcc 0.3089	TestAcc 0.3064	BestValid 0.3089
	Epoch 150:	Loss 1.2910	TrainAcc 0.3726	ValidAcc 0.3359	TestAcc 0.3429	BestValid 0.3359
	Epoch 200:	Loss 1.1811	TrainAcc 0.4667	ValidAcc 0.3948	TestAcc 0.4025	BestValid 0.3948
	Epoch 250:	Loss 1.0876	TrainAcc 0.5349	ValidAcc 0.4171	TestAcc 0.4131	BestValid 0.4171
	Epoch 300:	Loss 0.9969	TrainAcc 0.5333	ValidAcc 0.4069	TestAcc 0.4409	BestValid 0.4171
	Epoch 350:	Loss 0.8996	TrainAcc 0.4948	ValidAcc 0.3870	TestAcc 0.3833	BestValid 0.4171
	Epoch 400:	Loss 0.8449	TrainAcc 0.5577	ValidAcc 0.4075	TestAcc 0.4265	BestValid 0.4171
	Epoch 450:	Loss 0.7633	TrainAcc 0.6106	ValidAcc 0.4525	TestAcc 0.4409	BestValid 0.4525
	Epoch 500:	Loss 0.7307	TrainAcc 0.5533	ValidAcc 0.3930	TestAcc 0.4188	BestValid 0.4525
	Epoch 550:	Loss 0.6857	TrainAcc 0.5865	ValidAcc 0.4285	TestAcc 0.4409	BestValid 0.4525
	Epoch 600:	Loss 0.6891	TrainAcc 0.5653	ValidAcc 0.4333	TestAcc 0.4063	BestValid 0.4525
	Epoch 650:	Loss 0.6303	TrainAcc 0.5982	ValidAcc 0.4189	TestAcc 0.4313	BestValid 0.4525
	Epoch 700:	Loss 0.6152	TrainAcc 0.6046	ValidAcc 0.4369	TestAcc 0.4294	BestValid 0.4525
	Epoch 750:	Loss 0.5940	TrainAcc 0.5493	ValidAcc 0.3714	TestAcc 0.3823	BestValid 0.4525
	Epoch 800:	Loss 0.5480	TrainAcc 0.5709	ValidAcc 0.3786	TestAcc 0.4025	BestValid 0.4525
	Epoch 850:	Loss 0.5278	TrainAcc 0.6230	ValidAcc 0.4411	TestAcc 0.4438	BestValid 0.4525
	Epoch 900:	Loss 0.5706	TrainAcc 0.6490	ValidAcc 0.4207	TestAcc 0.4544	BestValid 0.4525
	Epoch 950:	Loss 0.4997	TrainAcc 0.7035	ValidAcc 0.4561	TestAcc 0.4649	BestValid 0.4561
	Epoch 1000:	Loss 0.5090	TrainAcc 0.6615	ValidAcc 0.4465	TestAcc 0.4659	BestValid 0.4561
	Epoch 1050:	Loss 0.4705	TrainAcc 0.7143	ValidAcc 0.4736	TestAcc 0.4938	BestValid 0.4736
	Epoch 1100:	Loss 0.4556	TrainAcc 0.7135	ValidAcc 0.4730	TestAcc 0.4861	BestValid 0.4736
	Epoch 1150:	Loss 0.4452	TrainAcc 0.6490	ValidAcc 0.3948	TestAcc 0.4246	BestValid 0.4736
	Epoch 1200:	Loss 0.4747	TrainAcc 0.6811	ValidAcc 0.4495	TestAcc 0.4524	BestValid 0.4736
	Epoch 1250:	Loss 0.4464	TrainAcc 0.7568	ValidAcc 0.4772	TestAcc 0.5034	BestValid 0.4772
	Epoch 1300:	Loss 0.4388	TrainAcc 0.6967	ValidAcc 0.4219	TestAcc 0.4438	BestValid 0.4772
	Epoch 1350:	Loss 0.4072	TrainAcc 0.7444	ValidAcc 0.4675	TestAcc 0.4649	BestValid 0.4772
	Epoch 1400:	Loss 0.4027	TrainAcc 0.7099	ValidAcc 0.4603	TestAcc 0.4573	BestValid 0.4772
	Epoch 1450:	Loss 0.4214	TrainAcc 0.7432	ValidAcc 0.4808	TestAcc 0.5120	BestValid 0.4808
	Epoch 1500:	Loss 0.4201	TrainAcc 0.6270	ValidAcc 0.3750	TestAcc 0.3814	BestValid 0.4808
	Epoch 1550:	Loss 0.3596	TrainAcc 0.7416	ValidAcc 0.4802	TestAcc 0.5014	BestValid 0.4808
	Epoch 1600:	Loss 0.3587	TrainAcc 0.6426	ValidAcc 0.3942	TestAcc 0.4131	BestValid 0.4808
	Epoch 1650:	Loss 0.3553	TrainAcc 0.7568	ValidAcc 0.4772	TestAcc 0.4986	BestValid 0.4808
	Epoch 1700:	Loss 0.3285	TrainAcc 0.6414	ValidAcc 0.3828	TestAcc 0.4073	BestValid 0.4808
	Epoch 1750:	Loss 0.3552	TrainAcc 0.7256	ValidAcc 0.4489	TestAcc 0.4496	BestValid 0.4808
	Epoch 1800:	Loss 0.3590	TrainAcc 0.6955	ValidAcc 0.4537	TestAcc 0.4505	BestValid 0.4808
	Epoch 1850:	Loss 0.3506	TrainAcc 0.7055	ValidAcc 0.4093	TestAcc 0.4284	BestValid 0.4808
	Epoch 1900:	Loss 0.3191	TrainAcc 0.7740	ValidAcc 0.4784	TestAcc 0.4966	BestValid 0.4808
	Epoch 1950:	Loss 0.2952	TrainAcc 0.7680	ValidAcc 0.4651	TestAcc 0.4707	BestValid 0.4808
	Epoch 2000:	Loss 0.3126	TrainAcc 0.7512	ValidAcc 0.4772	TestAcc 0.4688	BestValid 0.4808
	Epoch 2050:	Loss 0.3104	TrainAcc 0.7151	ValidAcc 0.4471	TestAcc 0.4486	BestValid 0.4808
	Epoch 2100:	Loss 0.3109	TrainAcc 0.7596	ValidAcc 0.4681	TestAcc 0.4774	BestValid 0.4808
	Epoch 2150:	Loss 0.2892	TrainAcc 0.7576	ValidAcc 0.4838	TestAcc 0.4765	BestValid 0.4838
	Epoch 2200:	Loss 0.2884	TrainAcc 0.7568	ValidAcc 0.4669	TestAcc 0.4745	BestValid 0.4838
	Epoch 2250:	Loss 0.2761	TrainAcc 0.7792	ValidAcc 0.4754	TestAcc 0.4841	BestValid 0.4838
	Epoch 2300:	Loss 0.2627	TrainAcc 0.7292	ValidAcc 0.4465	TestAcc 0.4544	BestValid 0.4838
	Epoch 2350:	Loss 0.2837	TrainAcc 0.7019	ValidAcc 0.4255	TestAcc 0.4409	BestValid 0.4838
	Epoch 2400:	Loss 0.2613	TrainAcc 0.7680	ValidAcc 0.4868	TestAcc 0.4841	BestValid 0.4868
	Epoch 2450:	Loss 0.2582	TrainAcc 0.6839	ValidAcc 0.3924	TestAcc 0.4102	BestValid 0.4868
	Epoch 2500:	Loss 0.2548	TrainAcc 0.7913	ValidAcc 0.4742	TestAcc 0.4717	BestValid 0.4868
	Epoch 2550:	Loss 0.2607	TrainAcc 0.7324	ValidAcc 0.4387	TestAcc 0.4659	BestValid 0.4868
	Epoch 2600:	Loss 0.2516	TrainAcc 0.7608	ValidAcc 0.4820	TestAcc 0.4861	BestValid 0.4868
	Epoch 2650:	Loss 0.2392	TrainAcc 0.6955	ValidAcc 0.4345	TestAcc 0.4284	BestValid 0.4868
	Epoch 2700:	Loss 0.2297	TrainAcc 0.7424	ValidAcc 0.4681	TestAcc 0.4803	BestValid 0.4868
	Epoch 2750:	Loss 0.2281	TrainAcc 0.7268	ValidAcc 0.4213	TestAcc 0.4304	BestValid 0.4868
	Epoch 2800:	Loss 0.2282	TrainAcc 0.7997	ValidAcc 0.4904	TestAcc 0.5139	BestValid 0.4904
	Epoch 2850:	Loss 0.2175	TrainAcc 0.7424	ValidAcc 0.4597	TestAcc 0.4736	BestValid 0.4904
	Epoch 2900:	Loss 0.2324	TrainAcc 0.7348	ValidAcc 0.4405	TestAcc 0.4553	BestValid 0.4904
	Epoch 2950:	Loss 0.2378	TrainAcc 0.6891	ValidAcc 0.4081	TestAcc 0.4207	BestValid 0.4904
	Epoch 3000:	Loss 0.2212	TrainAcc 0.7516	ValidAcc 0.4585	TestAcc 0.4765	BestValid 0.4904
	Epoch 3050:	Loss 0.2203	TrainAcc 0.7584	ValidAcc 0.4633	TestAcc 0.4678	BestValid 0.4904
	Epoch 3100:	Loss 0.2105	TrainAcc 0.7837	ValidAcc 0.4772	TestAcc 0.4784	BestValid 0.4904
	Epoch 3150:	Loss 0.1887	TrainAcc 0.7444	ValidAcc 0.4447	TestAcc 0.4524	BestValid 0.4904
	Epoch 3200:	Loss 0.2182	TrainAcc 0.7861	ValidAcc 0.4772	TestAcc 0.4841	BestValid 0.4904
	Epoch 3250:	Loss 0.1864	TrainAcc 0.6947	ValidAcc 0.4153	TestAcc 0.4159	BestValid 0.4904
	Epoch 3300:	Loss 0.1795	TrainAcc 0.7324	ValidAcc 0.4345	TestAcc 0.4515	BestValid 0.4904
	Epoch 3350:	Loss 0.1840	TrainAcc 0.8265	ValidAcc 0.4994	TestAcc 0.4947	BestValid 0.4994
	Epoch 3400:	Loss 0.1792	TrainAcc 0.7612	ValidAcc 0.4706	TestAcc 0.4966	BestValid 0.4994
	Epoch 3450:	Loss 0.2050	TrainAcc 0.8321	ValidAcc 0.5222	TestAcc 0.5149	BestValid 0.5222
	Epoch 3500:	Loss 0.1979	TrainAcc 0.6895	ValidAcc 0.4026	TestAcc 0.4332	BestValid 0.5222
	Epoch 3550:	Loss 0.1858	TrainAcc 0.7676	ValidAcc 0.4700	TestAcc 0.4457	BestValid 0.5222
	Epoch 3600:	Loss 0.1765	TrainAcc 0.7997	ValidAcc 0.4718	TestAcc 0.4765	BestValid 0.5222
	Epoch 3650:	Loss 0.1713	TrainAcc 0.8365	ValidAcc 0.5222	TestAcc 0.5082	BestValid 0.5222
	Epoch 3700:	Loss 0.1791	TrainAcc 0.7937	ValidAcc 0.4718	TestAcc 0.4765	BestValid 0.5222
	Epoch 3750:	Loss 0.1950	TrainAcc 0.7949	ValidAcc 0.4555	TestAcc 0.4659	BestValid 0.5222
	Epoch 3800:	Loss 0.1741	TrainAcc 0.8237	ValidAcc 0.4958	TestAcc 0.5072	BestValid 0.5222
	Epoch 3850:	Loss 0.1572	TrainAcc 0.7576	ValidAcc 0.4549	TestAcc 0.4438	BestValid 0.5222
	Epoch 3900:	Loss 0.1646	TrainAcc 0.7728	ValidAcc 0.4718	TestAcc 0.4774	BestValid 0.5222
	Epoch 3950:	Loss 0.1512	TrainAcc 0.8205	ValidAcc 0.5114	TestAcc 0.5159	BestValid 0.5222
	Epoch 4000:	Loss 0.1678	TrainAcc 0.8005	ValidAcc 0.4603	TestAcc 0.4688	BestValid 0.5222
	Epoch 4050:	Loss 0.1824	TrainAcc 0.7732	ValidAcc 0.4579	TestAcc 0.4544	BestValid 0.5222
	Epoch 4100:	Loss 0.1591	TrainAcc 0.8518	ValidAcc 0.5325	TestAcc 0.5389	BestValid 0.5325
	Epoch 4150:	Loss 0.1689	TrainAcc 0.7175	ValidAcc 0.4261	TestAcc 0.4352	BestValid 0.5325
	Epoch 4200:	Loss 0.1488	TrainAcc 0.7849	ValidAcc 0.4706	TestAcc 0.4621	BestValid 0.5325
	Epoch 4250:	Loss 0.1477	TrainAcc 0.8061	ValidAcc 0.4898	TestAcc 0.4899	BestValid 0.5325
	Epoch 4300:	Loss 0.1408	TrainAcc 0.8069	ValidAcc 0.4760	TestAcc 0.4870	BestValid 0.5325
	Epoch 4350:	Loss 0.1493	TrainAcc 0.8482	ValidAcc 0.5162	TestAcc 0.5149	BestValid 0.5325
	Epoch 4400:	Loss 0.1720	TrainAcc 0.7023	ValidAcc 0.4032	TestAcc 0.4044	BestValid 0.5325
	Epoch 4450:	Loss 0.1635	TrainAcc 0.7981	ValidAcc 0.4922	TestAcc 0.5005	BestValid 0.5325
	Epoch 4500:	Loss 0.1549	TrainAcc 0.8021	ValidAcc 0.4627	TestAcc 0.4621	BestValid 0.5325
	Epoch 4550:	Loss 0.1409	TrainAcc 0.7945	ValidAcc 0.4856	TestAcc 0.4899	BestValid 0.5325
	Epoch 4600:	Loss 0.1266	TrainAcc 0.7220	ValidAcc 0.3924	TestAcc 0.4140	BestValid 0.5325
	Epoch 4650:	Loss 0.1594	TrainAcc 0.8454	ValidAcc 0.5343	TestAcc 0.5351	BestValid 0.5343
	Epoch 4700:	Loss 0.1586	TrainAcc 0.7388	ValidAcc 0.4345	TestAcc 0.4496	BestValid 0.5343
	Epoch 4750:	Loss 0.1379	TrainAcc 0.8057	ValidAcc 0.5012	TestAcc 0.5053	BestValid 0.5343
	Epoch 4800:	Loss 0.1359	TrainAcc 0.8265	ValidAcc 0.4808	TestAcc 0.4976	BestValid 0.5343
	Epoch 4850:	Loss 0.1259	TrainAcc 0.8317	ValidAcc 0.5066	TestAcc 0.4841	BestValid 0.5343
	Epoch 4900:	Loss 0.1327	TrainAcc 0.8401	ValidAcc 0.4922	TestAcc 0.4909	BestValid 0.5343
	Epoch 4950:	Loss 0.1273	TrainAcc 0.8029	ValidAcc 0.4988	TestAcc 0.4986	BestValid 0.5343
	Epoch 5000:	Loss 0.1427	TrainAcc 0.7901	ValidAcc 0.4525	TestAcc 0.4553	BestValid 0.5343
****** Epoch Time (Excluding Evaluation Cost): 0.187 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 25.164 ms (Max: 26.007, Min: 24.468, Sum: 201.310)
Cluster-Wide Average, Compute: 99.588 ms (Max: 104.773, Min: 96.917, Sum: 796.701)
Cluster-Wide Average, Communication-Layer: 3.475 ms (Max: 4.398, Min: 2.663, Sum: 27.802)
Cluster-Wide Average, Bubble-Imbalance: 7.649 ms (Max: 10.549, Min: 4.898, Sum: 61.191)
Cluster-Wide Average, Communication-Graph: 25.842 ms (Max: 28.479, Min: 24.545, Sum: 206.734)
Cluster-Wide Average, Optimization: 23.040 ms (Max: 24.082, Min: 20.791, Sum: 184.322)
Cluster-Wide Average, Others: 2.437 ms (Max: 4.379, Min: 1.338, Sum: 19.500)
****** Breakdown Sum: 187.195 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.442 GB (Max: 4.169, Min: 3.294, Sum: 27.534)
Cluster-Wide Average, Graph-Level Communication Throughput: 57.916 Gbps (Max: 64.539, Min: 48.445, Sum: 463.330)
Cluster-Wide Average, Layer-Level Communication Throughput: 34.911 Gbps (Max: 42.604, Min: 28.684, Sum: 279.291)
Layer-level communication (cluster-wide, per-epoch): 0.116 GB
Graph-level communication (cluster-wide, per-epoch): 1.021 GB
Weight-sync communication (cluster-wide, per-epoch): 0.254 GB
Total communication (cluster-wide, per-epoch): 1.391 GB
****** Accuracy Results ******
Highest valid_acc: 0.5343
Target test_acc: 0.5351
Epoch to reach the target acc: 4649
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
