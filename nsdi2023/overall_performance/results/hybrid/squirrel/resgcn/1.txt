Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITInitialized node 2 on machine gnerv4

Initialized node 3 on machine gnerv4
DONE MPI INIT
Initialized node 1 on machine gnerv4
DONE MPI INIT
Initialized node 0 on machine gnerv4
DONE MPI INIT
DONE MPI INIT
Initialized node 5 on machine gnerv8
DONE MPI INITInitialized node 4 on machine gnerv8
DONE MPI INIT
Initialized node 6 on machine gnerv8

Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.007 seconds.
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.017 seconds.
Building the Feature Vector...
        It takes 0.016 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.729 Gbps (per GPU), 485.833 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.420 Gbps (per GPU), 483.357 Gbps (aggregated)
The layer-level communication performance: 60.417 Gbps (per GPU), 483.332 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.146 Gbps (per GPU), 481.167 Gbps (aggregated)
The layer-level communication performance: 60.116 Gbps (per GPU), 480.928 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.889 Gbps (per GPU), 479.110 Gbps (aggregated)
The layer-level communication performance: 59.841 Gbps (per GPU), 478.724 Gbps (aggregated)
The layer-level communication performance: 59.806 Gbps (per GPU), 478.445 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.395 Gbps (per GPU), 1267.161 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.372 Gbps (per GPU), 1266.973 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.342 Gbps (per GPU), 1266.734 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.374 Gbps (per GPU), 1266.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.380 Gbps (per GPU), 1267.042 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.369 Gbps (per GPU), 1266.948 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.347 Gbps (per GPU), 1266.779 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.362 Gbps (per GPU), 1266.898 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.116 Gbps (per GPU), 832.926 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.112 Gbps (per GPU), 832.899 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.117 Gbps (per GPU), 832.940 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.114 Gbps (per GPU), 832.912 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.119 Gbps (per GPU), 832.954 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.114 Gbps (per GPU), 832.912 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.117 Gbps (per GPU), 832.933 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.117 Gbps (per GPU), 832.940 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.159 Gbps (per GPU), 305.272 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.155 Gbps (per GPU), 305.242 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.156 Gbps (per GPU), 305.247 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.157 Gbps (per GPU), 305.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.154 Gbps (per GPU), 305.230 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.152 Gbps (per GPU), 305.212 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.152 Gbps (per GPU), 305.215 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.147 Gbps (per GPU), 305.177 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.45ms  0.73ms  1.27ms  2.84  0.08K  0.02M
 chk_1  0.55ms  0.77ms  1.35ms  2.45  0.16K  0.01M
 chk_2  0.48ms  0.73ms  1.30ms  2.71  0.13K  0.01M
 chk_3  0.57ms  0.80ms  1.38ms  2.41  0.18K  0.01M
 chk_4  0.53ms  0.76ms  1.34ms  2.55  0.15K  0.01M
 chk_5  0.61ms  0.82ms  1.41ms  2.33  0.25K  0.01M
 chk_6  0.45ms  0.73ms  1.28ms  2.82  0.10K  0.02M
 chk_7  0.48ms  0.74ms  1.29ms  2.69  0.11K  0.02M
 chk_8  0.60ms  0.82ms  1.41ms  2.37  0.23K  0.01M
 chk_9  0.51ms  0.76ms  1.33ms  2.60  0.14K  0.01M
chk_10  0.59ms  0.81ms  1.39ms  2.36  0.20K  0.01M
chk_11  0.45ms  0.73ms  1.27ms  2.83  0.09K  0.02M
chk_12  0.52ms  0.77ms  1.35ms  2.60  0.16K  0.01M
chk_13  0.52ms  0.77ms  1.34ms  2.58  0.16K  0.01M
chk_14  0.52ms  0.76ms  1.34ms  2.59  0.14K  0.01M
chk_15  0.60ms  0.82ms  1.40ms  2.33  0.21K  0.01M
chk_16  0.57ms  0.80ms  1.38ms  2.44  0.18K  0.01M
chk_17  0.63ms  0.83ms  1.44ms  2.29  0.29K  0.01M
chk_18  0.63ms  0.83ms  1.45ms  2.30  0.31K  0.00M
chk_19  0.47ms  0.73ms  1.30ms  2.75  0.13K  0.01M
chk_20  0.47ms  0.73ms  1.30ms  2.74  0.13K  0.01M
chk_21  0.56ms  0.80ms  1.38ms  2.45  0.18K  0.01M
chk_22  0.47ms  0.73ms  1.30ms  2.74  0.13K  0.01M
chk_23  0.56ms  0.80ms  1.38ms  2.47  0.16K  0.01M
chk_24  0.45ms  0.73ms  1.28ms  2.84  0.09K  0.02M
chk_25  0.45ms  0.73ms  1.28ms  2.83  0.09K  0.02M
chk_26  0.56ms  0.80ms  1.38ms  2.45  0.18K  0.01M
chk_27  0.47ms  0.73ms  1.29ms  2.74  0.13K  0.01M
chk_28  0.56ms  0.80ms  1.38ms  2.46  0.17K  0.01M
chk_29  0.52ms  0.76ms  1.35ms  2.60  0.15K  0.01M
chk_30  0.60ms  0.82ms  1.41ms  2.36  0.24K  0.01M
chk_31  0.58ms  0.80ms  1.38ms  2.38  0.20K  0.01M
   Avg  0.53  0.77  1.35
   Max  0.63  0.83  1.45
   Min  0.45  0.73  1.27
 Ratio  1.41  1.15  1.14
   Var  0.00  0.00  0.00
Profiling takes 1.072 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 55)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 55)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [55, 111)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [111, 167)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [55, 111)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [111, 167)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [167, 229)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [167, 229)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 55)...
+++++++++ Node 0 initializing the weights for op[0, 55)...
+++++++++ Node 3 initializing the weights for op[55, 111)...
+++++++++ Node 4 initializing the weights for op[111, 167)...
+++++++++ Node 5 initializing the weights for op[111, 167)...
+++++++++ Node 2 initializing the weights for op[55, 111)...
+++++++++ Node 7 initializing the weights for op[167, 229)...
+++++++++ Node 6 initializing the weights for op[167, 229)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.0429	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 1.6985	TrainAcc 0.2175	ValidAcc 0.2151	TestAcc 0.2065	BestValid 0.2151
	Epoch 100:	Loss 1.6794	TrainAcc 0.2704	ValidAcc 0.2572	TestAcc 0.2891	BestValid 0.2572
	Epoch 150:	Loss 1.6252	TrainAcc 0.2628	ValidAcc 0.2488	TestAcc 0.2651	BestValid 0.2572
	Epoch 200:	Loss 1.5827	TrainAcc 0.2580	ValidAcc 0.2674	TestAcc 0.2719	BestValid 0.2674
	Epoch 250:	Loss 1.5490	TrainAcc 0.2600	ValidAcc 0.2710	TestAcc 0.2757	BestValid 0.2710
	Epoch 300:	Loss 1.4707	TrainAcc 0.2937	ValidAcc 0.2734	TestAcc 0.2939	BestValid 0.2734
	Epoch 350:	Loss 1.4174	TrainAcc 0.2953	ValidAcc 0.2776	TestAcc 0.2988	BestValid 0.2776
	Epoch 400:	Loss 1.3534	TrainAcc 0.4127	ValidAcc 0.3335	TestAcc 0.3410	BestValid 0.3335
	Epoch 450:	Loss 1.2720	TrainAcc 0.4591	ValidAcc 0.3666	TestAcc 0.3689	BestValid 0.3666
	Epoch 500:	Loss 1.1959	TrainAcc 0.4992	ValidAcc 0.3618	TestAcc 0.3890	BestValid 0.3666
	Epoch 550:	Loss 1.1663	TrainAcc 0.5296	ValidAcc 0.3726	TestAcc 0.4006	BestValid 0.3726
	Epoch 600:	Loss 1.1597	TrainAcc 0.5385	ValidAcc 0.4020	TestAcc 0.3890	BestValid 0.4020
	Epoch 650:	Loss 1.0595	TrainAcc 0.5817	ValidAcc 0.4159	TestAcc 0.4198	BestValid 0.4159
	Epoch 700:	Loss 0.9301	TrainAcc 0.6154	ValidAcc 0.4165	TestAcc 0.4428	BestValid 0.4165
	Epoch 750:	Loss 0.9164	TrainAcc 0.6282	ValidAcc 0.4219	TestAcc 0.4476	BestValid 0.4219
	Epoch 800:	Loss 0.8100	TrainAcc 0.6639	ValidAcc 0.4423	TestAcc 0.4649	BestValid 0.4423
	Epoch 850:	Loss 0.7346	TrainAcc 0.6951	ValidAcc 0.4417	TestAcc 0.4832	BestValid 0.4423
	Epoch 900:	Loss 0.6974	TrainAcc 0.7248	ValidAcc 0.4681	TestAcc 0.4841	BestValid 0.4681
	Epoch 950:	Loss 0.6235	TrainAcc 0.7628	ValidAcc 0.4748	TestAcc 0.4947	BestValid 0.4748
	Epoch 1000:	Loss 0.5936	TrainAcc 0.7484	ValidAcc 0.4453	TestAcc 0.4861	BestValid 0.4748
	Epoch 1050:	Loss 0.6070	TrainAcc 0.7356	ValidAcc 0.4471	TestAcc 0.4813	BestValid 0.4748
	Epoch 1100:	Loss 0.5106	TrainAcc 0.8217	ValidAcc 0.5018	TestAcc 0.5331	BestValid 0.5018
	Epoch 1150:	Loss 0.5041	TrainAcc 0.8185	ValidAcc 0.5036	TestAcc 0.5226	BestValid 0.5036
	Epoch 1200:	Loss 0.4971	TrainAcc 0.8201	ValidAcc 0.4976	TestAcc 0.5360	BestValid 0.5036
	Epoch 1250:	Loss 0.4615	TrainAcc 0.8101	ValidAcc 0.4910	TestAcc 0.5187	BestValid 0.5036
	Epoch 1300:	Loss 0.4706	TrainAcc 0.8486	ValidAcc 0.5042	TestAcc 0.5418	BestValid 0.5042
	Epoch 1350:	Loss 0.4050	TrainAcc 0.8662	ValidAcc 0.5150	TestAcc 0.5524	BestValid 0.5150
	Epoch 1400:	Loss 0.3753	TrainAcc 0.8814	ValidAcc 0.5168	TestAcc 0.5543	BestValid 0.5168
	Epoch 1450:	Loss 0.4184	TrainAcc 0.8413	ValidAcc 0.5162	TestAcc 0.5283	BestValid 0.5168
	Epoch 1500:	Loss 0.4574	TrainAcc 0.8586	ValidAcc 0.5144	TestAcc 0.5370	BestValid 0.5168
	Epoch 1550:	Loss 0.3763	TrainAcc 0.8578	ValidAcc 0.5120	TestAcc 0.5322	BestValid 0.5168
	Epoch 1600:	Loss 0.3483	TrainAcc 0.8926	ValidAcc 0.5270	TestAcc 0.5543	BestValid 0.5270
	Epoch 1650:	Loss 0.2966	TrainAcc 0.8710	ValidAcc 0.5144	TestAcc 0.5341	BestValid 0.5270
	Epoch 1700:	Loss 0.3205	TrainAcc 0.8714	ValidAcc 0.5246	TestAcc 0.5341	BestValid 0.5270
	Epoch 1750:	Loss 0.3060	TrainAcc 0.8922	ValidAcc 0.5234	TestAcc 0.5447	BestValid 0.5270
	Epoch 1800:	Loss 0.3023	TrainAcc 0.9075	ValidAcc 0.5343	TestAcc 0.5360	BestValid 0.5343
	Epoch 1850:	Loss 0.2537	TrainAcc 0.9147	ValidAcc 0.5282	TestAcc 0.5418	BestValid 0.5343
	Epoch 1900:	Loss 0.2805	TrainAcc 0.9087	ValidAcc 0.5216	TestAcc 0.5283	BestValid 0.5343
	Epoch 1950:	Loss 0.2600	TrainAcc 0.8890	ValidAcc 0.5234	TestAcc 0.5379	BestValid 0.5343
	Epoch 2000:	Loss 0.2594	TrainAcc 0.9159	ValidAcc 0.5198	TestAcc 0.5331	BestValid 0.5343
	Epoch 2050:	Loss 0.2522	TrainAcc 0.8954	ValidAcc 0.5168	TestAcc 0.5235	BestValid 0.5343
	Epoch 2100:	Loss 0.2307	TrainAcc 0.9103	ValidAcc 0.5150	TestAcc 0.5370	BestValid 0.5343
	Epoch 2150:	Loss 0.2501	TrainAcc 0.9127	ValidAcc 0.5337	TestAcc 0.5303	BestValid 0.5343
	Epoch 2200:	Loss 0.2076	TrainAcc 0.9355	ValidAcc 0.5397	TestAcc 0.5447	BestValid 0.5397
	Epoch 2250:	Loss 0.2000	TrainAcc 0.9207	ValidAcc 0.5337	TestAcc 0.5456	BestValid 0.5397
	Epoch 2300:	Loss 0.2100	TrainAcc 0.9151	ValidAcc 0.5198	TestAcc 0.5399	BestValid 0.5397
	Epoch 2350:	Loss 0.2096	TrainAcc 0.8986	ValidAcc 0.4964	TestAcc 0.5014	BestValid 0.5397
	Epoch 2400:	Loss 0.1973	TrainAcc 0.8922	ValidAcc 0.5228	TestAcc 0.5331	BestValid 0.5397
	Epoch 2450:	Loss 0.1902	TrainAcc 0.9311	ValidAcc 0.5439	TestAcc 0.5495	BestValid 0.5439
	Epoch 2500:	Loss 0.2194	TrainAcc 0.9307	ValidAcc 0.5337	TestAcc 0.5303	BestValid 0.5439
	Epoch 2550:	Loss 0.1797	TrainAcc 0.9026	ValidAcc 0.5084	TestAcc 0.5264	BestValid 0.5439
	Epoch 2600:	Loss 0.1855	TrainAcc 0.9331	ValidAcc 0.5451	TestAcc 0.5495	BestValid 0.5451
	Epoch 2650:	Loss 0.1802	TrainAcc 0.9327	ValidAcc 0.5403	TestAcc 0.5408	BestValid 0.5451
	Epoch 2700:	Loss 0.1451	TrainAcc 0.9563	ValidAcc 0.5457	TestAcc 0.5562	BestValid 0.5457
	Epoch 2750:	Loss 0.1454	TrainAcc 0.9463	ValidAcc 0.5337	TestAcc 0.5562	BestValid 0.5457
	Epoch 2800:	Loss 0.1596	TrainAcc 0.9419	ValidAcc 0.5421	TestAcc 0.5696	BestValid 0.5457
	Epoch 2850:	Loss 0.1503	TrainAcc 0.9583	ValidAcc 0.5433	TestAcc 0.5543	BestValid 0.5457
	Epoch 2900:	Loss 0.1471	TrainAcc 0.9107	ValidAcc 0.5210	TestAcc 0.5427	BestValid 0.5457
	Epoch 2950:	Loss 0.2005	TrainAcc 0.8770	ValidAcc 0.4904	TestAcc 0.5024	BestValid 0.5457
	Epoch 3000:	Loss 0.1366	TrainAcc 0.9511	ValidAcc 0.5469	TestAcc 0.5514	BestValid 0.5469
	Epoch 3050:	Loss 0.1235	TrainAcc 0.9555	ValidAcc 0.5475	TestAcc 0.5514	BestValid 0.5475
	Epoch 3100:	Loss 0.1343	TrainAcc 0.9531	ValidAcc 0.5421	TestAcc 0.5504	BestValid 0.5475
	Epoch 3150:	Loss 0.1206	TrainAcc 0.9451	ValidAcc 0.5475	TestAcc 0.5687	BestValid 0.5475
	Epoch 3200:	Loss 0.1142	TrainAcc 0.9383	ValidAcc 0.5234	TestAcc 0.5312	BestValid 0.5475
	Epoch 3250:	Loss 0.1116	TrainAcc 0.9511	ValidAcc 0.5409	TestAcc 0.5648	BestValid 0.5475
	Epoch 3300:	Loss 0.1370	TrainAcc 0.9651	ValidAcc 0.5547	TestAcc 0.5658	BestValid 0.5547
	Epoch 3350:	Loss 0.1008	TrainAcc 0.9663	ValidAcc 0.5559	TestAcc 0.5658	BestValid 0.5559
	Epoch 3400:	Loss 0.1060	TrainAcc 0.9371	ValidAcc 0.5469	TestAcc 0.5620	BestValid 0.5559
	Epoch 3450:	Loss 0.0954	TrainAcc 0.9671	ValidAcc 0.5529	TestAcc 0.5591	BestValid 0.5559
	Epoch 3500:	Loss 0.1095	TrainAcc 0.9351	ValidAcc 0.5270	TestAcc 0.5303	BestValid 0.5559
	Epoch 3550:	Loss 0.1240	TrainAcc 0.9275	ValidAcc 0.5150	TestAcc 0.5360	BestValid 0.5559
	Epoch 3600:	Loss 0.1004	TrainAcc 0.9651	ValidAcc 0.5559	TestAcc 0.5639	BestValid 0.5559
	Epoch 3650:	Loss 0.0959	TrainAcc 0.9732	ValidAcc 0.5517	TestAcc 0.5495	BestValid 0.5559
	Epoch 3700:	Loss 0.0879	TrainAcc 0.9688	ValidAcc 0.5493	TestAcc 0.5600	BestValid 0.5559
	Epoch 3750:	Loss 0.1318	TrainAcc 0.9046	ValidAcc 0.4934	TestAcc 0.5091	BestValid 0.5559
	Epoch 3800:	Loss 0.1228	TrainAcc 0.9235	ValidAcc 0.5168	TestAcc 0.5245	BestValid 0.5559
	Epoch 3850:	Loss 0.0842	TrainAcc 0.9659	ValidAcc 0.5547	TestAcc 0.5495	BestValid 0.5559
	Epoch 3900:	Loss 0.0845	TrainAcc 0.9712	ValidAcc 0.5535	TestAcc 0.5620	BestValid 0.5559
	Epoch 3950:	Loss 0.0962	TrainAcc 0.9611	ValidAcc 0.5517	TestAcc 0.5533	BestValid 0.5559
	Epoch 4000:	Loss 0.0843	TrainAcc 0.9655	ValidAcc 0.5409	TestAcc 0.5629	BestValid 0.5559
	Epoch 4050:	Loss 0.0863	TrainAcc 0.9635	ValidAcc 0.5439	TestAcc 0.5600	BestValid 0.5559
	Epoch 4100:	Loss 0.0898	TrainAcc 0.9732	ValidAcc 0.5613	TestAcc 0.5696	BestValid 0.5613
	Epoch 4150:	Loss 0.0756	TrainAcc 0.9527	ValidAcc 0.5337	TestAcc 0.5447	BestValid 0.5613
	Epoch 4200:	Loss 0.0686	TrainAcc 0.9800	ValidAcc 0.5553	TestAcc 0.5543	BestValid 0.5613
	Epoch 4250:	Loss 0.0676	TrainAcc 0.9720	ValidAcc 0.5511	TestAcc 0.5629	BestValid 0.5613
	Epoch 4300:	Loss 0.0608	TrainAcc 0.9760	ValidAcc 0.5463	TestAcc 0.5648	BestValid 0.5613
	Epoch 4350:	Loss 0.1483	TrainAcc 0.9503	ValidAcc 0.5355	TestAcc 0.5514	BestValid 0.5613
	Epoch 4400:	Loss 0.0719	TrainAcc 0.9748	ValidAcc 0.5565	TestAcc 0.5620	BestValid 0.5613
	Epoch 4450:	Loss 0.0615	TrainAcc 0.9688	ValidAcc 0.5577	TestAcc 0.5639	BestValid 0.5613
	Epoch 4500:	Loss 0.0660	TrainAcc 0.9716	ValidAcc 0.5523	TestAcc 0.5735	BestValid 0.5613
	Epoch 4550:	Loss 0.0631	TrainAcc 0.9740	ValidAcc 0.5463	TestAcc 0.5600	BestValid 0.5613
	Epoch 4600:	Loss 0.0672	TrainAcc 0.9828	ValidAcc 0.5523	TestAcc 0.5591	BestValid 0.5613
	Epoch 4650:	Loss 0.0742	TrainAcc 0.9728	ValidAcc 0.5505	TestAcc 0.5610	BestValid 0.5613
	Epoch 4700:	Loss 0.0678	TrainAcc 0.9820	ValidAcc 0.5511	TestAcc 0.5658	BestValid 0.5613
	Epoch 4750:	Loss 0.0701	TrainAcc 0.9615	ValidAcc 0.5373	TestAcc 0.5466	BestValid 0.5613
	Epoch 4800:	Loss 0.0707	TrainAcc 0.9671	ValidAcc 0.5481	TestAcc 0.5600	BestValid 0.5613
	Epoch 4850:	Loss 0.0607	TrainAcc 0.9792	ValidAcc 0.5529	TestAcc 0.5600	BestValid 0.5613
	Epoch 4900:	Loss 0.0745	TrainAcc 0.9848	ValidAcc 0.5541	TestAcc 0.5677	BestValid 0.5613
	Epoch 4950:	Loss 0.0475	TrainAcc 0.9864	ValidAcc 0.5607	TestAcc 0.5658	BestValid 0.5613
	Epoch 5000:	Loss 0.0623	TrainAcc 0.9784	ValidAcc 0.5535	TestAcc 0.5610	BestValid 0.5613
****** Epoch Time (Excluding Evaluation Cost): 0.176 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 23.552 ms (Max: 23.947, Min: 22.949, Sum: 188.417)
Cluster-Wide Average, Compute: 90.418 ms (Max: 97.723, Min: 85.421, Sum: 723.344)
Cluster-Wide Average, Communication-Layer: 3.450 ms (Max: 4.325, Min: 2.679, Sum: 27.597)
Cluster-Wide Average, Bubble-Imbalance: 8.092 ms (Max: 12.253, Min: 2.647, Sum: 64.738)
Cluster-Wide Average, Communication-Graph: 25.610 ms (Max: 27.554, Min: 24.041, Sum: 204.882)
Cluster-Wide Average, Optimization: 21.637 ms (Max: 23.386, Min: 18.877, Sum: 173.099)
Cluster-Wide Average, Others: 2.811 ms (Max: 5.234, Min: 1.209, Sum: 22.489)
****** Breakdown Sum: 175.571 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.493 GB (Max: 4.167, Min: 3.315, Sum: 27.944)
Cluster-Wide Average, Graph-Level Communication Throughput: 58.851 Gbps (Max: 65.539, Min: 54.318, Sum: 470.804)
Cluster-Wide Average, Layer-Level Communication Throughput: 35.161 Gbps (Max: 42.820, Min: 29.035, Sum: 281.286)
Layer-level communication (cluster-wide, per-epoch): 0.116 GB
Graph-level communication (cluster-wide, per-epoch): 1.021 GB
Weight-sync communication (cluster-wide, per-epoch): 0.254 GB
Total communication (cluster-wide, per-epoch): 1.391 GB
****** Accuracy Results ******
Highest valid_acc: 0.5613
Target test_acc: 0.5696
Epoch to reach the target acc: 4099
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
