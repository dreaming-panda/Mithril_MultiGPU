Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.009 seconds.
        It takes 0.008 seconds.
        It takes 0.011 seconds.
Building the Feature Vector...
        It takes 0.010 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCN layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.025 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.028 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.029 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.178 Gbps (per GPU), 457.428 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.893 Gbps (per GPU), 455.143 Gbps (aggregated)
The layer-level communication performance: 56.900 Gbps (per GPU), 455.204 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.618 Gbps (per GPU), 452.945 Gbps (aggregated)
The layer-level communication performance: 56.650 Gbps (per GPU), 453.202 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.411 Gbps (per GPU), 451.287 Gbps (aggregated)
The layer-level communication performance: 56.369 Gbps (per GPU), 450.950 Gbps (aggregated)
The layer-level communication performance: 56.339 Gbps (per GPU), 450.711 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.592 Gbps (per GPU), 1252.732 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.603 Gbps (per GPU), 1252.826 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.594 Gbps (per GPU), 1252.756 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.568 Gbps (per GPU), 1252.545 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.594 Gbps (per GPU), 1252.756 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.621 Gbps (per GPU), 1252.970 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.597 Gbps (per GPU), 1252.779 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.574 Gbps (per GPU), 1252.593 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.095 Gbps (per GPU), 808.761 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.097 Gbps (per GPU), 808.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.094 Gbps (per GPU), 808.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.098 Gbps (per GPU), 808.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.094 Gbps (per GPU), 808.749 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.099 Gbps (per GPU), 808.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.096 Gbps (per GPU), 808.768 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.080 Gbps (per GPU), 808.638 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.046 Gbps (per GPU), 256.368 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.047 Gbps (per GPU), 256.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.043 Gbps (per GPU), 256.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.047 Gbps (per GPU), 256.373 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.042 Gbps (per GPU), 256.335 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.045 Gbps (per GPU), 256.360 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.043 Gbps (per GPU), 256.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.044 Gbps (per GPU), 256.348 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.39ms  0.44ms  0.27ms  1.61  0.08K  0.02M
 chk_1  0.43ms  0.40ms  0.29ms  1.47  0.16K  0.01M
 chk_2  0.40ms  0.38ms  0.29ms  1.40  0.13K  0.01M
 chk_3  0.43ms  0.45ms  0.28ms  1.61  0.18K  0.01M
 chk_4  0.42ms  0.41ms  0.24ms  1.78  0.15K  0.01M
 chk_5  0.47ms  0.43ms  0.23ms  2.04  0.25K  0.01M
 chk_6  0.51ms  0.36ms  0.25ms  2.02  0.10K  0.02M
 chk_7  0.48ms  0.37ms  0.25ms  1.95  0.11K  0.02M
 chk_8  0.53ms  0.42ms  0.23ms  2.30  0.23K  0.01M
 chk_9  0.42ms  0.38ms  0.23ms  1.81  0.14K  0.01M
chk_10  0.47ms  0.42ms  0.24ms  2.01  0.20K  0.01M
chk_11  0.39ms  0.36ms  0.25ms  1.54  0.09K  0.02M
chk_12  0.43ms  0.38ms  0.24ms  1.81  0.16K  0.01M
chk_13  0.45ms  0.38ms  0.23ms  1.91  0.16K  0.01M
chk_14  0.45ms  0.38ms  0.23ms  1.92  0.14K  0.01M
chk_15  0.49ms  0.42ms  0.24ms  2.06  0.21K  0.01M
chk_16  0.43ms  0.43ms  0.23ms  1.85  0.18K  0.01M
chk_17  0.49ms  0.42ms  0.20ms  2.52  0.29K  0.01M
chk_18  0.50ms  0.42ms  0.19ms  2.61  0.31K  0.00M
chk_19  0.42ms  0.36ms  0.24ms  1.78  0.13K  0.01M
chk_20  0.43ms  0.36ms  0.24ms  1.80  0.13K  0.01M
chk_21  0.44ms  0.42ms  0.24ms  1.88  0.18K  0.01M
chk_22  0.40ms  0.36ms  0.23ms  1.71  0.13K  0.01M
chk_23  0.45ms  0.42ms  0.24ms  1.90  0.16K  0.01M
chk_24  0.41ms  0.36ms  0.25ms  1.64  0.09K  0.02M
chk_25  0.46ms  0.36ms  0.26ms  1.81  0.09K  0.02M
chk_26  0.47ms  0.42ms  0.23ms  2.00  0.18K  0.01M
chk_27  0.40ms  0.36ms  0.24ms  1.70  0.13K  0.01M
chk_28  0.43ms  0.42ms  0.23ms  1.83  0.17K  0.01M
chk_29  0.42ms  0.39ms  0.24ms  1.76  0.15K  0.01M
chk_30  0.47ms  0.42ms  0.23ms  2.01  0.24K  0.01M
chk_31  0.47ms  0.41ms  0.23ms  2.01  0.20K  0.01M
   Avg  0.44  0.40  0.24
   Max  0.53  0.45  0.29
   Min  0.39  0.36  0.19
 Ratio  1.37  1.26  1.51
   Var  0.00  0.00  0.00
Profiling takes 0.503 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 52.344 ms
Partition 0 [0, 4) has cost: 52.344 ms
Partition 1 [4, 8) has cost: 50.815 ms
Partition 2 [8, 12) has cost: 50.815 ms
Partition 3 [12, 16) has cost: 50.815 ms
Partition 4 [16, 20) has cost: 50.815 ms
Partition 5 [20, 24) has cost: 50.815 ms
Partition 6 [24, 28) has cost: 50.815 ms
Partition 7 [28, 32) has cost: 45.806 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 26.431 ms
GPU 0, Compute+Comm Time: 20.596 ms, Bubble Time: 4.555 ms, Imbalance Overhead: 1.280 ms
GPU 1, Compute+Comm Time: 19.735 ms, Bubble Time: 4.532 ms, Imbalance Overhead: 2.163 ms
GPU 2, Compute+Comm Time: 19.735 ms, Bubble Time: 4.522 ms, Imbalance Overhead: 2.174 ms
GPU 3, Compute+Comm Time: 19.735 ms, Bubble Time: 4.511 ms, Imbalance Overhead: 2.184 ms
GPU 4, Compute+Comm Time: 19.735 ms, Bubble Time: 4.544 ms, Imbalance Overhead: 2.151 ms
GPU 5, Compute+Comm Time: 19.735 ms, Bubble Time: 4.567 ms, Imbalance Overhead: 2.128 ms
GPU 6, Compute+Comm Time: 19.735 ms, Bubble Time: 4.634 ms, Imbalance Overhead: 2.062 ms
GPU 7, Compute+Comm Time: 18.177 ms, Bubble Time: 4.735 ms, Imbalance Overhead: 3.519 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 49.917 ms
GPU 0, Compute+Comm Time: 33.451 ms, Bubble Time: 8.728 ms, Imbalance Overhead: 7.738 ms
GPU 1, Compute+Comm Time: 36.902 ms, Bubble Time: 8.530 ms, Imbalance Overhead: 4.486 ms
GPU 2, Compute+Comm Time: 36.902 ms, Bubble Time: 8.441 ms, Imbalance Overhead: 4.574 ms
GPU 3, Compute+Comm Time: 36.902 ms, Bubble Time: 8.434 ms, Imbalance Overhead: 4.581 ms
GPU 4, Compute+Comm Time: 36.902 ms, Bubble Time: 8.455 ms, Imbalance Overhead: 4.560 ms
GPU 5, Compute+Comm Time: 36.902 ms, Bubble Time: 8.565 ms, Imbalance Overhead: 4.450 ms
GPU 6, Compute+Comm Time: 36.902 ms, Bubble Time: 8.675 ms, Imbalance Overhead: 4.340 ms
GPU 7, Compute+Comm Time: 37.569 ms, Bubble Time: 8.824 ms, Imbalance Overhead: 3.524 ms
The estimated cost of the whole pipeline: 80.165 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 103.159 ms
Partition 0 [0, 8) has cost: 103.159 ms
Partition 1 [8, 16) has cost: 101.631 ms
Partition 2 [16, 24) has cost: 101.631 ms
Partition 3 [24, 32) has cost: 96.622 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 28.658 ms
GPU 0, Compute+Comm Time: 23.196 ms, Bubble Time: 4.353 ms, Imbalance Overhead: 1.109 ms
GPU 1, Compute+Comm Time: 22.718 ms, Bubble Time: 4.398 ms, Imbalance Overhead: 1.542 ms
GPU 2, Compute+Comm Time: 22.718 ms, Bubble Time: 4.514 ms, Imbalance Overhead: 1.426 ms
GPU 3, Compute+Comm Time: 21.953 ms, Bubble Time: 4.690 ms, Imbalance Overhead: 2.015 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 51.547 ms
GPU 0, Compute+Comm Time: 39.010 ms, Bubble Time: 8.185 ms, Imbalance Overhead: 4.352 ms
GPU 1, Compute+Comm Time: 40.779 ms, Bubble Time: 7.906 ms, Imbalance Overhead: 2.862 ms
GPU 2, Compute+Comm Time: 40.779 ms, Bubble Time: 7.794 ms, Imbalance Overhead: 2.974 ms
GPU 3, Compute+Comm Time: 41.210 ms, Bubble Time: 7.965 ms, Imbalance Overhead: 2.371 ms
    The estimated cost with 2 DP ways is 84.215 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 204.790 ms
Partition 0 [0, 16) has cost: 204.790 ms
Partition 1 [16, 32) has cost: 198.253 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 40.677 ms
GPU 0, Compute+Comm Time: 35.351 ms, Bubble Time: 4.130 ms, Imbalance Overhead: 1.196 ms
GPU 1, Compute+Comm Time: 34.705 ms, Bubble Time: 4.528 ms, Imbalance Overhead: 1.444 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.965 ms
GPU 0, Compute+Comm Time: 53.247 ms, Bubble Time: 6.733 ms, Imbalance Overhead: 1.985 ms
GPU 1, Compute+Comm Time: 54.407 ms, Bubble Time: 6.392 ms, Imbalance Overhead: 1.165 ms
    The estimated cost with 4 DP ways is 107.773 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 403.043 ms
Partition 0 [0, 32) has cost: 403.043 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 133.810 ms
GPU 0, Compute+Comm Time: 133.810 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 152.637 ms
GPU 0, Compute+Comm Time: 152.637 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 300.769 ms

*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [81, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 41)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [121, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [121, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 81)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 121)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [41, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 41)...
+++++++++ Node 3 initializing the weights for op[41, 81)...
+++++++++ Node 5 initializing the weights for op[81, 121)...
+++++++++ Node 1 initializing the weights for op[0, 41)...
+++++++++ Node 7 initializing the weights for op[121, 160)...
+++++++++ Node 2 initializing the weights for op[41, 81)...
+++++++++ Node 4 initializing the weights for op[81, 121)...
+++++++++ Node 6 initializing the weights for op[121, 160)...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2055
	Epoch 50:	Loss 1.6013	TrainAcc 0.1959	ValidAcc 0.2109	TestAcc 0.2075	BestValid 0.2109
	Epoch 100:	Loss 1.5989	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2109
	Epoch 150:	Loss 1.5974	TrainAcc 0.2200	ValidAcc 0.2374	TestAcc 0.2161	BestValid 0.2374
	Epoch 200:	Loss 1.6021	TrainAcc 0.2320	ValidAcc 0.2212	TestAcc 0.2238	BestValid 0.2374
	Epoch 250:	Loss 1.5997	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 300:	Loss 1.5980	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 350:	Loss 1.6312	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 400:	Loss 1.6051	TrainAcc 0.2135	ValidAcc 0.2127	TestAcc 0.2075	BestValid 0.2374
	Epoch 450:	Loss 1.6199	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 500:	Loss 1.5977	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 550:	Loss 1.5927	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 600:	Loss 1.5853	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 650:	Loss 1.5835	TrainAcc 0.2007	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 700:	Loss 1.5748	TrainAcc 0.1987	ValidAcc 0.1917	TestAcc 0.2046	BestValid 0.2374
	Epoch 750:	Loss 1.5806	TrainAcc 0.2031	ValidAcc 0.2037	TestAcc 0.1988	BestValid 0.2374
	Epoch 800:	Loss 1.5746	TrainAcc 0.2031	ValidAcc 0.2025	TestAcc 0.1988	BestValid 0.2374
	Epoch 850:	Loss 1.5790	TrainAcc 0.2099	ValidAcc 0.2001	TestAcc 0.1998	BestValid 0.2374
	Epoch 900:	Loss 1.5827	TrainAcc 0.2023	ValidAcc 0.1941	TestAcc 0.2065	BestValid 0.2374
	Epoch 950:	Loss 1.5788	TrainAcc 0.2115	ValidAcc 0.2037	TestAcc 0.2209	BestValid 0.2374
	Epoch 1000:	Loss 1.5737	TrainAcc 0.2083	ValidAcc 0.2085	TestAcc 0.2008	BestValid 0.2374
	Epoch 1050:	Loss 1.5715	TrainAcc 0.2091	ValidAcc 0.2085	TestAcc 0.2017	BestValid 0.2374
	Epoch 1100:	Loss 1.5787	TrainAcc 0.2023	ValidAcc 0.2037	TestAcc 0.1988	BestValid 0.2374
	Epoch 1150:	Loss 1.5732	TrainAcc 0.2007	ValidAcc 0.1941	TestAcc 0.2065	BestValid 0.2374
	Epoch 1200:	Loss 1.5850	TrainAcc 0.2031	ValidAcc 0.2019	TestAcc 0.1988	BestValid 0.2374
	Epoch 1250:	Loss 1.5861	TrainAcc 0.2011	ValidAcc 0.2013	TestAcc 0.1988	BestValid 0.2374
	Epoch 1300:	Loss 1.5868	TrainAcc 0.2015	ValidAcc 0.2019	TestAcc 0.1988	BestValid 0.2374
	Epoch 1350:	Loss 1.5666	TrainAcc 0.2051	ValidAcc 0.2055	TestAcc 0.1998	BestValid 0.2374
	Epoch 1400:	Loss 1.5683	TrainAcc 0.2344	ValidAcc 0.2224	TestAcc 0.2478	BestValid 0.2374
	Epoch 1450:	Loss 1.5763	TrainAcc 0.2015	ValidAcc 0.1959	TestAcc 0.2085	BestValid 0.2374
	Epoch 1500:	Loss 1.5613	TrainAcc 0.2027	ValidAcc 0.1947	TestAcc 0.2065	BestValid 0.2374
	Epoch 1550:	Loss 1.5682	TrainAcc 0.2123	ValidAcc 0.2079	TestAcc 0.2056	BestValid 0.2374
	Epoch 1600:	Loss 1.5629	TrainAcc 0.2324	ValidAcc 0.2236	TestAcc 0.2373	BestValid 0.2374
	Epoch 1650:	Loss 1.5679	TrainAcc 0.2055	ValidAcc 0.2079	TestAcc 0.1988	BestValid 0.2374
	Epoch 1700:	Loss 1.5654	TrainAcc 0.2015	ValidAcc 0.2019	TestAcc 0.1988	BestValid 0.2374
	Epoch 1750:	Loss 1.5747	TrainAcc 0.2115	ValidAcc 0.2121	TestAcc 0.2027	BestValid 0.2374
	Epoch 1800:	Loss 1.5739	TrainAcc 0.2139	ValidAcc 0.2049	TestAcc 0.2085	BestValid 0.2374
	Epoch 1850:	Loss 1.5811	TrainAcc 0.2043	ValidAcc 0.2049	TestAcc 0.1979	BestValid 0.2374
	Epoch 1900:	Loss 1.5771	TrainAcc 0.2071	ValidAcc 0.2073	TestAcc 0.2027	BestValid 0.2374
	Epoch 1950:	Loss 1.5744	TrainAcc 0.2039	ValidAcc 0.2055	TestAcc 0.1998	BestValid 0.2374
	Epoch 2000:	Loss 1.5738	TrainAcc 0.2115	ValidAcc 0.2133	TestAcc 0.2037	BestValid 0.2374
	Epoch 2050:	Loss 1.5847	TrainAcc 0.2019	ValidAcc 0.1941	TestAcc 0.2065	BestValid 0.2374
	Epoch 2100:	Loss 1.5772	TrainAcc 0.2063	ValidAcc 0.2007	TestAcc 0.2104	BestValid 0.2374
	Epoch 2150:	Loss 1.5675	TrainAcc 0.2332	ValidAcc 0.2194	TestAcc 0.2488	BestValid 0.2374
	Epoch 2200:	Loss 1.5620	TrainAcc 0.2007	ValidAcc 0.1947	TestAcc 0.2065	BestValid 0.2374
	Epoch 2250:	Loss 1.5708	TrainAcc 0.1991	ValidAcc 0.1917	TestAcc 0.2046	BestValid 0.2374
	Epoch 2300:	Loss 1.5804	TrainAcc 0.2204	ValidAcc 0.2175	TestAcc 0.2305	BestValid 0.2374
	Epoch 2350:	Loss 1.5737	TrainAcc 0.2420	ValidAcc 0.2218	TestAcc 0.2498	BestValid 0.2374
	Epoch 2400:	Loss 1.5678	TrainAcc 0.2039	ValidAcc 0.1983	TestAcc 0.2075	BestValid 0.2374
	Epoch 2450:	Loss 1.5684	TrainAcc 0.2063	ValidAcc 0.2001	TestAcc 0.2094	BestValid 0.2374
	Epoch 2500:	Loss 1.5744	TrainAcc 0.2131	ValidAcc 0.2139	TestAcc 0.2075	BestValid 0.2374
	Epoch 2550:	Loss 1.5724	TrainAcc 0.2067	ValidAcc 0.2055	TestAcc 0.2017	BestValid 0.2374
	Epoch 2600:	Loss 1.5744	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 2650:	Loss 1.5754	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 2700:	Loss 1.5759	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 2750:	Loss 1.5741	TrainAcc 0.2248	ValidAcc 0.2206	TestAcc 0.2257	BestValid 0.2374
	Epoch 2800:	Loss 1.5706	TrainAcc 0.2123	ValidAcc 0.2055	TestAcc 0.2152	BestValid 0.2374
	Epoch 2850:	Loss 1.5732	TrainAcc 0.2071	ValidAcc 0.2073	TestAcc 0.2075	BestValid 0.2374
	Epoch 2900:	Loss 1.5678	TrainAcc 0.2376	ValidAcc 0.2206	TestAcc 0.2440	BestValid 0.2374
	Epoch 2950:	Loss 1.5683	TrainAcc 0.2059	ValidAcc 0.2079	TestAcc 0.2027	BestValid 0.2374
	Epoch 3000:	Loss 1.5726	TrainAcc 0.2007	ValidAcc 0.1917	TestAcc 0.2075	BestValid 0.2374
	Epoch 3050:	Loss 1.5641	TrainAcc 0.2071	ValidAcc 0.2001	TestAcc 0.2113	BestValid 0.2374
	Epoch 3100:	Loss 1.5655	TrainAcc 0.2059	ValidAcc 0.1971	TestAcc 0.2017	BestValid 0.2374
	Epoch 3150:	Loss 1.5685	TrainAcc 0.2015	ValidAcc 0.2019	TestAcc 0.1988	BestValid 0.2374
	Epoch 3200:	Loss 1.5786	TrainAcc 0.2043	ValidAcc 0.2061	TestAcc 0.2008	BestValid 0.2374
	Epoch 3250:	Loss 1.5625	TrainAcc 0.2396	ValidAcc 0.2230	TestAcc 0.2430	BestValid 0.2374
	Epoch 3300:	Loss 1.6215	TrainAcc 0.2007	ValidAcc 0.1941	TestAcc 0.2065	BestValid 0.2374
	Epoch 3350:	Loss 1.5747	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 3400:	Loss 1.5763	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 3450:	Loss 1.5734	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 3500:	Loss 1.5724	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 3550:	Loss 1.5675	TrainAcc 0.2039	ValidAcc 0.2067	TestAcc 0.2008	BestValid 0.2374
	Epoch 3600:	Loss 1.5684	TrainAcc 0.2015	ValidAcc 0.1941	TestAcc 0.2065	BestValid 0.2374
	Epoch 3650:	Loss 1.5640	TrainAcc 0.2292	ValidAcc 0.2194	TestAcc 0.2123	BestValid 0.2374
	Epoch 3700:	Loss 1.5617	TrainAcc 0.2380	ValidAcc 0.2187	TestAcc 0.2421	BestValid 0.2374
	Epoch 3750:	Loss 1.5662	TrainAcc 0.2208	ValidAcc 0.2145	TestAcc 0.2094	BestValid 0.2374
	Epoch 3800:	Loss 1.5692	TrainAcc 0.2011	ValidAcc 0.1953	TestAcc 0.2065	BestValid 0.2374
	Epoch 3850:	Loss 1.5573	TrainAcc 0.2067	ValidAcc 0.2073	TestAcc 0.2017	BestValid 0.2374
	Epoch 3900:	Loss 1.5568	TrainAcc 0.2011	ValidAcc 0.1929	TestAcc 0.2027	BestValid 0.2374
	Epoch 3950:	Loss 1.5571	TrainAcc 0.1983	ValidAcc 0.1923	TestAcc 0.2056	BestValid 0.2374
	Epoch 4000:	Loss 1.5583	TrainAcc 0.2372	ValidAcc 0.2175	TestAcc 0.2459	BestValid 0.2374
	Epoch 4050:	Loss 1.5599	TrainAcc 0.2204	ValidAcc 0.2103	TestAcc 0.2209	BestValid 0.2374
	Epoch 4100:	Loss 1.5605	TrainAcc 0.2095	ValidAcc 0.2085	TestAcc 0.2065	BestValid 0.2374
	Epoch 4150:	Loss 1.5519	TrainAcc 0.2388	ValidAcc 0.2187	TestAcc 0.2421	BestValid 0.2374
	Epoch 4200:	Loss 1.5582	TrainAcc 0.2135	ValidAcc 0.2031	TestAcc 0.2152	BestValid 0.2374
	Epoch 4250:	Loss 1.5509	TrainAcc 0.2115	ValidAcc 0.1965	TestAcc 0.2113	BestValid 0.2374
	Epoch 4300:	Loss 1.5613	TrainAcc 0.2488	ValidAcc 0.2260	TestAcc 0.2478	BestValid 0.2374
	Epoch 4350:	Loss 1.5626	TrainAcc 0.2167	ValidAcc 0.2079	TestAcc 0.2075	BestValid 0.2374
	Epoch 4400:	Loss 1.5556	TrainAcc 0.2015	ValidAcc 0.1965	TestAcc 0.2094	BestValid 0.2374
	Epoch 4450:	Loss 1.5546	TrainAcc 0.2011	ValidAcc 0.1929	TestAcc 0.2075	BestValid 0.2374
	Epoch 4500:	Loss 1.5580	TrainAcc 0.2216	ValidAcc 0.2139	TestAcc 0.2219	BestValid 0.2374
	Epoch 4550:	Loss 1.5546	TrainAcc 0.2047	ValidAcc 0.1953	TestAcc 0.2142	BestValid 0.2374
	Epoch 4600:	Loss 1.5565	TrainAcc 0.2043	ValidAcc 0.1947	TestAcc 0.2075	BestValid 0.2374
	Epoch 4650:	Loss 1.5602	TrainAcc 0.2123	ValidAcc 0.2061	TestAcc 0.2037	BestValid 0.2374
	Epoch 4700:	Loss 1.5533	TrainAcc 0.1983	ValidAcc 0.1917	TestAcc 0.2075	BestValid 0.2374
	Epoch 4750:	Loss 1.5545	TrainAcc 0.2512	ValidAcc 0.2302	TestAcc 0.2555	BestValid 0.2374
	Epoch 4800:	Loss 1.6309	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2374
	Epoch 4850:	Loss 1.5563	TrainAcc 0.2240	ValidAcc 0.2139	TestAcc 0.2133	BestValid 0.2374
	Epoch 4900:	Loss 1.5523	TrainAcc 0.2027	ValidAcc 0.1947	TestAcc 0.2094	BestValid 0.2374
	Epoch 4950:	Loss 1.5472	TrainAcc 0.2208	ValidAcc 0.2127	TestAcc 0.2046	BestValid 0.2374
	Epoch 5000:	Loss 1.5658	TrainAcc 0.2196	ValidAcc 0.2121	TestAcc 0.1998	BestValid 0.2374
****** Epoch Time (Excluding Evaluation Cost): 0.112 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 15.088 ms (Max: 15.680, Min: 14.513, Sum: 120.702)
Cluster-Wide Average, Compute: 49.263 ms (Max: 51.665, Min: 45.773, Sum: 394.102)
Cluster-Wide Average, Communication-Layer: 2.994 ms (Max: 3.946, Min: 2.093, Sum: 23.953)
Cluster-Wide Average, Bubble-Imbalance: 5.268 ms (Max: 7.948, Min: 3.851, Sum: 42.142)
Cluster-Wide Average, Communication-Graph: 23.057 ms (Max: 23.976, Min: 22.229, Sum: 184.452)
Cluster-Wide Average, Optimization: 13.772 ms (Max: 14.852, Min: 12.977, Sum: 110.179)
Cluster-Wide Average, Others: 1.992 ms (Max: 2.764, Min: 1.183, Sum: 15.937)
****** Breakdown Sum: 111.433 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.228 GB (Max: 3.712, Min: 3.030, Sum: 25.821)
Cluster-Wide Average, Graph-Level Communication Throughput: 67.002 Gbps (Max: 73.957, Min: 60.649, Sum: 536.017)
Cluster-Wide Average, Layer-Level Communication Throughput: 40.874 Gbps (Max: 46.235, Min: 36.154, Sum: 326.991)
Layer-level communication (cluster-wide, per-epoch): 0.116 GB
Graph-level communication (cluster-wide, per-epoch): 1.021 GB
Weight-sync communication (cluster-wide, per-epoch): 0.239 GB
Total communication (cluster-wide, per-epoch): 1.376 GB
****** Accuracy Results ******
Highest valid_acc: 0.2374
Target test_acc: 0.2161
Epoch to reach the target acc: 149
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
