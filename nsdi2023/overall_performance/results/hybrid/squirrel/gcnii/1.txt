Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.006 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.006 seconds.
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
Building the Feature Vector...
        It takes 0.010 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.013 seconds.
Building the Feature Vector...
        It takes 0.013 seconds.
        It takes 0.018 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.032 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.032 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.696 Gbps (per GPU), 453.566 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.432 Gbps (per GPU), 451.453 Gbps (aggregated)
The layer-level communication performance: 56.418 Gbps (per GPU), 451.348 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.212 Gbps (per GPU), 449.696 Gbps (aggregated)
The layer-level communication performance: 56.180 Gbps (per GPU), 449.440 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.997 Gbps (per GPU), 447.972 Gbps (aggregated)
The layer-level communication performance: 55.953 Gbps (per GPU), 447.627 Gbps (aggregated)
The layer-level communication performance: 55.928 Gbps (per GPU), 447.420 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.915 Gbps (per GPU), 1263.322 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.915 Gbps (per GPU), 1263.321 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.853 Gbps (per GPU), 1262.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.912 Gbps (per GPU), 1263.297 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.918 Gbps (per GPU), 1263.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.921 Gbps (per GPU), 1263.368 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.859 Gbps (per GPU), 1262.869 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.918 Gbps (per GPU), 1263.345 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 95.530 Gbps (per GPU), 764.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.526 Gbps (per GPU), 764.210 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.531 Gbps (per GPU), 764.245 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.524 Gbps (per GPU), 764.193 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.528 Gbps (per GPU), 764.228 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.525 Gbps (per GPU), 764.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.531 Gbps (per GPU), 764.251 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 95.524 Gbps (per GPU), 764.193 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.831 Gbps (per GPU), 254.648 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.828 Gbps (per GPU), 254.626 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.832 Gbps (per GPU), 254.656 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.830 Gbps (per GPU), 254.642 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.828 Gbps (per GPU), 254.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.828 Gbps (per GPU), 254.621 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.826 Gbps (per GPU), 254.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.821 Gbps (per GPU), 254.565 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.35ms  0.41ms  0.54ms  1.55  0.08K  0.02M
 chk_1  0.44ms  0.44ms  0.58ms  1.33  0.16K  0.01M
 chk_2  0.39ms  0.42ms  0.55ms  1.41  0.13K  0.01M
 chk_3  0.45ms  0.48ms  0.62ms  1.36  0.18K  0.01M
 chk_4  0.43ms  0.44ms  0.57ms  1.33  0.15K  0.01M
 chk_5  0.51ms  0.49ms  0.62ms  1.27  0.25K  0.01M
 chk_6  0.35ms  0.42ms  0.55ms  1.56  0.10K  0.02M
 chk_7  0.39ms  0.43ms  0.56ms  1.45  0.11K  0.02M
 chk_8  0.50ms  0.49ms  0.62ms  1.28  0.23K  0.01M
 chk_9  0.42ms  0.43ms  0.57ms  1.35  0.14K  0.01M
chk_10  0.49ms  0.48ms  0.62ms  1.28  0.20K  0.01M
chk_11  0.35ms  0.41ms  0.54ms  1.56  0.09K  0.02M
chk_12  0.44ms  0.44ms  0.58ms  1.33  0.16K  0.01M
chk_13  0.44ms  0.44ms  0.58ms  1.32  0.16K  0.01M
chk_14  0.43ms  0.44ms  0.57ms  1.34  0.14K  0.01M
chk_15  0.50ms  0.48ms  0.62ms  1.28  0.21K  0.01M
chk_16  0.45ms  0.48ms  0.62ms  1.36  0.18K  0.01M
chk_17  0.56ms  0.49ms  0.63ms  1.29  0.29K  0.01M
chk_18  0.58ms  0.49ms  0.63ms  1.28  0.31K  0.00M
chk_19  0.39ms  0.42ms  0.55ms  1.41  0.13K  0.01M
chk_20  0.39ms  0.42ms  0.55ms  1.42  0.13K  0.01M
chk_21  0.45ms  0.48ms  0.62ms  1.36  0.18K  0.01M
chk_22  0.39ms  0.42ms  0.55ms  1.42  0.13K  0.01M
chk_23  0.45ms  0.48ms  0.61ms  1.37  0.16K  0.01M
chk_24  0.35ms  0.41ms  0.54ms  1.56  0.09K  0.02M
chk_25  0.35ms  0.42ms  0.55ms  1.57  0.09K  0.02M
chk_26  0.46ms  0.48ms  0.61ms  1.35  0.18K  0.01M
chk_27  0.39ms  0.42ms  0.55ms  1.42  0.13K  0.01M
chk_28  0.45ms  0.47ms  0.61ms  1.35  0.17K  0.01M
chk_29  0.43ms  0.44ms  0.58ms  1.34  0.15K  0.01M
chk_30  0.51ms  0.49ms  0.62ms  1.28  0.24K  0.01M
chk_31  0.50ms  0.48ms  0.61ms  1.29  0.20K  0.01M
   Avg  0.44  0.45  0.59
   Max  0.58  0.49  0.63
   Min  0.35  0.41  0.54
 Ratio  1.65  1.19  1.17
   Var  0.00  0.00  0.00
Profiling takes 0.670 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 71.588 ms
Partition 0 [0, 5) has cost: 71.588 ms
Partition 1 [5, 9) has cost: 57.634 ms
Partition 2 [9, 13) has cost: 57.634 ms
Partition 3 [13, 17) has cost: 57.634 ms
Partition 4 [17, 21) has cost: 57.634 ms
Partition 5 [21, 25) has cost: 57.634 ms
Partition 6 [25, 29) has cost: 57.634 ms
Partition 7 [29, 33) has cost: 61.952 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.764 ms
GPU 0, Compute+Comm Time: 26.116 ms, Bubble Time: 5.286 ms, Imbalance Overhead: 0.362 ms
GPU 1, Compute+Comm Time: 21.874 ms, Bubble Time: 5.264 ms, Imbalance Overhead: 4.627 ms
GPU 2, Compute+Comm Time: 21.874 ms, Bubble Time: 5.319 ms, Imbalance Overhead: 4.571 ms
GPU 3, Compute+Comm Time: 21.874 ms, Bubble Time: 5.320 ms, Imbalance Overhead: 4.570 ms
GPU 4, Compute+Comm Time: 21.874 ms, Bubble Time: 5.428 ms, Imbalance Overhead: 4.462 ms
GPU 5, Compute+Comm Time: 21.874 ms, Bubble Time: 5.486 ms, Imbalance Overhead: 4.405 ms
GPU 6, Compute+Comm Time: 21.874 ms, Bubble Time: 5.606 ms, Imbalance Overhead: 4.284 ms
GPU 7, Compute+Comm Time: 23.101 ms, Bubble Time: 5.607 ms, Imbalance Overhead: 3.056 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 62.742 ms
GPU 0, Compute+Comm Time: 44.721 ms, Bubble Time: 10.986 ms, Imbalance Overhead: 7.035 ms
GPU 1, Compute+Comm Time: 41.631 ms, Bubble Time: 10.998 ms, Imbalance Overhead: 10.112 ms
GPU 2, Compute+Comm Time: 41.631 ms, Bubble Time: 10.718 ms, Imbalance Overhead: 10.392 ms
GPU 3, Compute+Comm Time: 41.631 ms, Bubble Time: 10.650 ms, Imbalance Overhead: 10.460 ms
GPU 4, Compute+Comm Time: 41.631 ms, Bubble Time: 10.422 ms, Imbalance Overhead: 10.688 ms
GPU 5, Compute+Comm Time: 41.631 ms, Bubble Time: 10.448 ms, Imbalance Overhead: 10.662 ms
GPU 6, Compute+Comm Time: 41.631 ms, Bubble Time: 10.364 ms, Imbalance Overhead: 10.746 ms
GPU 7, Compute+Comm Time: 51.343 ms, Bubble Time: 10.478 ms, Imbalance Overhead: 0.920 ms
The estimated cost of the whole pipeline: 99.231 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 129.223 ms
Partition 0 [0, 9) has cost: 129.223 ms
Partition 1 [9, 17) has cost: 115.269 ms
Partition 2 [17, 25) has cost: 115.269 ms
Partition 3 [25, 33) has cost: 119.586 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 32.696 ms
GPU 0, Compute+Comm Time: 27.058 ms, Bubble Time: 4.958 ms, Imbalance Overhead: 0.680 ms
GPU 1, Compute+Comm Time: 24.809 ms, Bubble Time: 5.012 ms, Imbalance Overhead: 2.875 ms
GPU 2, Compute+Comm Time: 24.809 ms, Bubble Time: 5.223 ms, Imbalance Overhead: 2.664 ms
GPU 3, Compute+Comm Time: 25.427 ms, Bubble Time: 5.448 ms, Imbalance Overhead: 1.821 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 62.031 ms
GPU 0, Compute+Comm Time: 47.387 ms, Bubble Time: 10.354 ms, Imbalance Overhead: 4.291 ms
GPU 1, Compute+Comm Time: 45.824 ms, Bubble Time: 9.874 ms, Imbalance Overhead: 6.333 ms
GPU 2, Compute+Comm Time: 45.824 ms, Bubble Time: 9.510 ms, Imbalance Overhead: 6.697 ms
GPU 3, Compute+Comm Time: 51.092 ms, Bubble Time: 9.413 ms, Imbalance Overhead: 1.526 ms
    The estimated cost with 2 DP ways is 99.464 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 244.491 ms
Partition 0 [0, 17) has cost: 244.491 ms
Partition 1 [17, 33) has cost: 234.855 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 44.877 ms
GPU 0, Compute+Comm Time: 39.048 ms, Bubble Time: 4.565 ms, Imbalance Overhead: 1.264 ms
GPU 1, Compute+Comm Time: 38.166 ms, Bubble Time: 4.803 ms, Imbalance Overhead: 1.908 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 72.008 ms
GPU 0, Compute+Comm Time: 61.226 ms, Bubble Time: 7.825 ms, Imbalance Overhead: 2.957 ms
GPU 1, Compute+Comm Time: 63.313 ms, Bubble Time: 7.405 ms, Imbalance Overhead: 1.290 ms
    The estimated cost with 4 DP ways is 122.730 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 479.346 ms
Partition 0 [0, 33) has cost: 479.346 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 136.899 ms
GPU 0, Compute+Comm Time: 136.899 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 160.867 ms
GPU 0, Compute+Comm Time: 160.867 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 312.655 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6114	TrainAcc 0.2007	ValidAcc 0.2007	TestAcc 0.1988	BestValid 0.2007
	Epoch 50:	Loss 1.1641	TrainAcc 0.5893	ValidAcc 0.3323	TestAcc 0.3112	BestValid 0.3323
	Epoch 100:	Loss 0.9632	TrainAcc 0.7500	ValidAcc 0.3269	TestAcc 0.3295	BestValid 0.3323
	Epoch 150:	Loss 0.9013	TrainAcc 0.8045	ValidAcc 0.3227	TestAcc 0.3372	BestValid 0.3323
	Epoch 200:	Loss 0.8076	TrainAcc 0.8329	ValidAcc 0.3239	TestAcc 0.3372	BestValid 0.3323
	Epoch 250:	Loss 0.7824	TrainAcc 0.8630	ValidAcc 0.3245	TestAcc 0.3439	BestValid 0.3323
	Epoch 300:	Loss 0.7306	TrainAcc 0.8846	ValidAcc 0.3293	TestAcc 0.3324	BestValid 0.3323
	Epoch 350:	Loss 0.6907	TrainAcc 0.8918	ValidAcc 0.3383	TestAcc 0.3372	BestValid 0.3383
	Epoch 400:	Loss 0.6778	TrainAcc 0.9079	ValidAcc 0.3456	TestAcc 0.3353	BestValid 0.3456
	Epoch 450:	Loss 0.6516	TrainAcc 0.9255	ValidAcc 0.3444	TestAcc 0.3449	BestValid 0.3456
	Epoch 500:	Loss 0.6066	TrainAcc 0.9335	ValidAcc 0.3335	TestAcc 0.3525	BestValid 0.3456
	Epoch 550:	Loss 0.5572	TrainAcc 0.9439	ValidAcc 0.3456	TestAcc 0.3497	BestValid 0.3456
	Epoch 600:	Loss 0.5596	TrainAcc 0.9507	ValidAcc 0.3462	TestAcc 0.3477	BestValid 0.3462
	Epoch 650:	Loss 0.5341	TrainAcc 0.9511	ValidAcc 0.3492	TestAcc 0.3497	BestValid 0.3492
	Epoch 700:	Loss 0.4801	TrainAcc 0.9583	ValidAcc 0.3450	TestAcc 0.3650	BestValid 0.3492
	Epoch 750:	Loss 0.4719	TrainAcc 0.9619	ValidAcc 0.3528	TestAcc 0.3612	BestValid 0.3528
	Epoch 800:	Loss 0.4570	TrainAcc 0.9587	ValidAcc 0.3498	TestAcc 0.3641	BestValid 0.3528
	Epoch 850:	Loss 0.4309	TrainAcc 0.9623	ValidAcc 0.3540	TestAcc 0.3602	BestValid 0.3540
	Epoch 900:	Loss 0.4198	TrainAcc 0.9647	ValidAcc 0.3510	TestAcc 0.3612	BestValid 0.3540
	Epoch 950:	Loss 0.4028	TrainAcc 0.9671	ValidAcc 0.3588	TestAcc 0.3583	BestValid 0.3588
	Epoch 1000:	Loss 0.4015	TrainAcc 0.9700	ValidAcc 0.3570	TestAcc 0.3650	BestValid 0.3588
	Epoch 1050:	Loss 0.3751	TrainAcc 0.9696	ValidAcc 0.3588	TestAcc 0.3583	BestValid 0.3588
	Epoch 1100:	Loss 0.3645	TrainAcc 0.9696	ValidAcc 0.3510	TestAcc 0.3602	BestValid 0.3588
	Epoch 1150:	Loss 0.3414	TrainAcc 0.9756	ValidAcc 0.3666	TestAcc 0.3650	BestValid 0.3666
	Epoch 1200:	Loss 0.3471	TrainAcc 0.9728	ValidAcc 0.3570	TestAcc 0.3622	BestValid 0.3666
	Epoch 1250:	Loss 0.3314	TrainAcc 0.9760	ValidAcc 0.3576	TestAcc 0.3670	BestValid 0.3666
	Epoch 1300:	Loss 0.3417	TrainAcc 0.9756	ValidAcc 0.3576	TestAcc 0.3660	BestValid 0.3666
	Epoch 1350:	Loss 0.3181	TrainAcc 0.9784	ValidAcc 0.3612	TestAcc 0.3650	BestValid 0.3666
	Epoch 1400:	Loss 0.3055	TrainAcc 0.9792	ValidAcc 0.3612	TestAcc 0.3670	BestValid 0.3666
	Epoch 1450:	Loss 0.3061	TrainAcc 0.9776	ValidAcc 0.3582	TestAcc 0.3641	BestValid 0.3666
	Epoch 1500:	Loss 0.2877	TrainAcc 0.9764	ValidAcc 0.3582	TestAcc 0.3631	BestValid 0.3666
	Epoch 1550:	Loss 0.2880	TrainAcc 0.9796	ValidAcc 0.3624	TestAcc 0.3650	BestValid 0.3666
	Epoch 1600:	Loss 0.2518	TrainAcc 0.9812	ValidAcc 0.3630	TestAcc 0.3631	BestValid 0.3666
	Epoch 1650:	Loss 0.2628	TrainAcc 0.9844	ValidAcc 0.3720	TestAcc 0.3622	BestValid 0.3720
	Epoch 1700:	Loss 0.2693	TrainAcc 0.9832	ValidAcc 0.3648	TestAcc 0.3622	BestValid 0.3720
	Epoch 1750:	Loss 0.2528	TrainAcc 0.9808	ValidAcc 0.3600	TestAcc 0.3650	BestValid 0.3720
	Epoch 1800:	Loss 0.2559	TrainAcc 0.9864	ValidAcc 0.3720	TestAcc 0.3593	BestValid 0.3720
	Epoch 1850:	Loss 0.2663	TrainAcc 0.9832	ValidAcc 0.3636	TestAcc 0.3612	BestValid 0.3720
	Epoch 1900:	Loss 0.2294	TrainAcc 0.9800	ValidAcc 0.3612	TestAcc 0.3622	BestValid 0.3720
	Epoch 1950:	Loss 0.2290	TrainAcc 0.9848	ValidAcc 0.3630	TestAcc 0.3660	BestValid 0.3720
	Epoch 2000:	Loss 0.2328	TrainAcc 0.9868	ValidAcc 0.3624	TestAcc 0.3641	BestValid 0.3720
	Epoch 2050:	Loss 0.2419	TrainAcc 0.9824	ValidAcc 0.3492	TestAcc 0.3679	BestValid 0.3720
	Epoch 2100:	Loss 0.2433	TrainAcc 0.9800	ValidAcc 0.3606	TestAcc 0.3593	BestValid 0.3720
	Epoch 2150:	Loss 0.2417	TrainAcc 0.9808	ValidAcc 0.3564	TestAcc 0.3718	BestValid 0.3720
	Epoch 2200:	Loss 0.2261	TrainAcc 0.9804	ValidAcc 0.3624	TestAcc 0.3698	BestValid 0.3720
	Epoch 2250:	Loss 0.2238	TrainAcc 0.9768	ValidAcc 0.3594	TestAcc 0.3650	BestValid 0.3720
	Epoch 2300:	Loss 0.2050	TrainAcc 0.9832	ValidAcc 0.3618	TestAcc 0.3670	BestValid 0.3720
	Epoch 2350:	Loss 0.1992	TrainAcc 0.9860	ValidAcc 0.3750	TestAcc 0.3660	BestValid 0.3750
	Epoch 2400:	Loss 0.2222	TrainAcc 0.9880	ValidAcc 0.3780	TestAcc 0.3631	BestValid 0.3780
	Epoch 2450:	Loss 0.2011	TrainAcc 0.9868	ValidAcc 0.3576	TestAcc 0.3689	BestValid 0.3780
	Epoch 2500:	Loss 0.2165	TrainAcc 0.9868	ValidAcc 0.3612	TestAcc 0.3679	BestValid 0.3780
	Epoch 2550:	Loss 0.2234	TrainAcc 0.9876	ValidAcc 0.3612	TestAcc 0.3679	BestValid 0.3780
	Epoch 2600:	Loss 0.2004	TrainAcc 0.9860	ValidAcc 0.3702	TestAcc 0.3708	BestValid 0.3780
	Epoch 2650:	Loss 0.2064	TrainAcc 0.9896	ValidAcc 0.3666	TestAcc 0.3689	BestValid 0.3780
	Epoch 2700:	Loss 0.2100	TrainAcc 0.9864	ValidAcc 0.3666	TestAcc 0.3766	BestValid 0.3780
	Epoch 2750:	Loss 0.1930	TrainAcc 0.9872	ValidAcc 0.3654	TestAcc 0.3641	BestValid 0.3780
	Epoch 2800:	Loss 0.1968	TrainAcc 0.9852	ValidAcc 0.3684	TestAcc 0.3679	BestValid 0.3780
	Epoch 2850:	Loss 0.1889	TrainAcc 0.9840	ValidAcc 0.3624	TestAcc 0.3698	BestValid 0.3780
	Epoch 2900:	Loss 0.1870	TrainAcc 0.9884	ValidAcc 0.3678	TestAcc 0.3670	BestValid 0.3780
	Epoch 2950:	Loss 0.1849	TrainAcc 0.9880	ValidAcc 0.3642	TestAcc 0.3679	BestValid 0.3780
	Epoch 3000:	Loss 0.1711	TrainAcc 0.9888	ValidAcc 0.3678	TestAcc 0.3689	BestValid 0.3780
	Epoch 3050:	Loss 0.1724	TrainAcc 0.9848	ValidAcc 0.3660	TestAcc 0.3641	BestValid 0.3780
	Epoch 3100:	Loss 0.1916	TrainAcc 0.9836	ValidAcc 0.3702	TestAcc 0.3650	BestValid 0.3780
	Epoch 3150:	Loss 0.1626	TrainAcc 0.9892	ValidAcc 0.3702	TestAcc 0.3660	BestValid 0.3780
	Epoch 3200:	Loss 0.1703	TrainAcc 0.9912	ValidAcc 0.3744	TestAcc 0.3708	BestValid 0.3780
	Epoch 3250:	Loss 0.1724	TrainAcc 0.9848	ValidAcc 0.3750	TestAcc 0.3622	BestValid 0.3780
	Epoch 3300:	Loss 0.1670	TrainAcc 0.9884	ValidAcc 0.3762	TestAcc 0.3727	BestValid 0.3780
	Epoch 3350:	Loss 0.1610	TrainAcc 0.9908	ValidAcc 0.3726	TestAcc 0.3766	BestValid 0.3780
	Epoch 3400:	Loss 0.1619	TrainAcc 0.9864	ValidAcc 0.3666	TestAcc 0.3727	BestValid 0.3780
	Epoch 3450:	Loss 0.1591	TrainAcc 0.9880	ValidAcc 0.3666	TestAcc 0.3727	BestValid 0.3780
	Epoch 3500:	Loss 0.1560	TrainAcc 0.9864	ValidAcc 0.3630	TestAcc 0.3737	BestValid 0.3780
	Epoch 3550:	Loss 0.1683	TrainAcc 0.9828	ValidAcc 0.3660	TestAcc 0.3814	BestValid 0.3780
	Epoch 3600:	Loss 0.1650	TrainAcc 0.9876	ValidAcc 0.3654	TestAcc 0.3746	BestValid 0.3780
	Epoch 3650:	Loss 0.1517	TrainAcc 0.9868	ValidAcc 0.3636	TestAcc 0.3718	BestValid 0.3780
	Epoch 3700:	Loss 0.1615	TrainAcc 0.9876	ValidAcc 0.3672	TestAcc 0.3737	BestValid 0.3780
	Epoch 3750:	Loss 0.1582	TrainAcc 0.9880	ValidAcc 0.3684	TestAcc 0.3814	BestValid 0.3780
	Epoch 3800:	Loss 0.1377	TrainAcc 0.9896	ValidAcc 0.3642	TestAcc 0.3814	BestValid 0.3780
	Epoch 3850:	Loss 0.1495	TrainAcc 0.9908	ValidAcc 0.3750	TestAcc 0.3746	BestValid 0.3780
	Epoch 3900:	Loss 0.1731	TrainAcc 0.9912	ValidAcc 0.3744	TestAcc 0.3746	BestValid 0.3780
	Epoch 3950:	Loss 0.1468	TrainAcc 0.9932	ValidAcc 0.3732	TestAcc 0.3727	BestValid 0.3780
	Epoch 4000:	Loss 0.1493	TrainAcc 0.9908	ValidAcc 0.3774	TestAcc 0.3833	BestValid 0.3780
	Epoch 4050:	Loss 0.1606	TrainAcc 0.9836	ValidAcc 0.3654	TestAcc 0.3833	BestValid 0.3780
	Epoch 4100:	Loss 0.1579	TrainAcc 0.9872	ValidAcc 0.3654	TestAcc 0.3785	BestValid 0.3780
	Epoch 4150:	Loss 0.1631	TrainAcc 0.9896	ValidAcc 0.3768	TestAcc 0.3785	BestValid 0.3780
	Epoch 4200:	Loss 0.1449	TrainAcc 0.9880	ValidAcc 0.3744	TestAcc 0.3737	BestValid 0.3780
	Epoch 4250:	Loss 0.1497	TrainAcc 0.9888	ValidAcc 0.3732	TestAcc 0.3833	BestValid 0.3780
	Epoch 4300:	Loss 0.1446	TrainAcc 0.9904	ValidAcc 0.3684	TestAcc 0.3727	BestValid 0.3780
	Epoch 4350:	Loss 0.1408	TrainAcc 0.9888	ValidAcc 0.3678	TestAcc 0.3823	BestValid 0.3780
	Epoch 4400:	Loss 0.1262	TrainAcc 0.9936	ValidAcc 0.3750	TestAcc 0.3766	BestValid 0.3780
	Epoch 4450:	Loss 0.1424	TrainAcc 0.9936	ValidAcc 0.3732	TestAcc 0.3746	BestValid 0.3780
	Epoch 4500:	Loss 0.1564	TrainAcc 0.9908	ValidAcc 0.3726	TestAcc 0.3804	BestValid 0.3780
	Epoch 4550:	Loss 0.1240	TrainAcc 0.9912	ValidAcc 0.3750	TestAcc 0.3794	BestValid 0.3780
	Epoch 4600:	Loss 0.1313	TrainAcc 0.9896	ValidAcc 0.3720	TestAcc 0.3775	BestValid 0.3780
	Epoch 4650:	Loss 0.1250	TrainAcc 0.9868	ValidAcc 0.3714	TestAcc 0.3679	BestValid 0.3780
	Epoch 4700:	Loss 0.1377	TrainAcc 0.9912	ValidAcc 0.3702	TestAcc 0.3718	BestValid 0.3780
	Epoch 4750:	Loss 0.1311	TrainAcc 0.9940	ValidAcc 0.3768	TestAcc 0.3756	BestValid 0.3780
	Epoch 4800:	Loss 0.1270	TrainAcc 0.9928	ValidAcc 0.3720	TestAcc 0.3804	BestValid 0.3780
	Epoch 4850:	Loss 0.1353	TrainAcc 0.9876	ValidAcc 0.3666	TestAcc 0.3746	BestValid 0.3780
	Epoch 4900:	Loss 0.1318	TrainAcc 0.9960	ValidAcc 0.3708	TestAcc 0.3794	BestValid 0.3780
	Epoch 4950:	Loss 0.1424	TrainAcc 0.9896	ValidAcc 0.3684	TestAcc 0.3766	BestValid 0.3780
	Epoch 5000:	Loss 0.1217	TrainAcc 0.9904	ValidAcc 0.3702	TestAcc 0.3804	BestValid 0.3780
****** Epoch Time (Excluding Evaluation Cost): 0.140 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.991 ms (Max: 18.472, Min: 17.418, Sum: 143.927)
Cluster-Wide Average, Compute: 61.455 ms (Max: 67.905, Min: 57.720, Sum: 491.636)
Cluster-Wide Average, Communication-Layer: 6.856 ms (Max: 8.083, Min: 5.712, Sum: 54.849)
Cluster-Wide Average, Bubble-Imbalance: 7.034 ms (Max: 8.639, Min: 2.878, Sum: 56.274)
Cluster-Wide Average, Communication-Graph: 23.899 ms (Max: 25.973, Min: 22.348, Sum: 191.191)
Cluster-Wide Average, Optimization: 15.423 ms (Max: 22.184, Min: 12.338, Sum: 123.383)
Cluster-Wide Average, Others: 7.684 ms (Max: 10.447, Min: 1.734, Sum: 61.469)
****** Breakdown Sum: 140.341 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.510 GB (Max: 4.530, Min: 3.282, Sum: 28.081)
Cluster-Wide Average, Graph-Level Communication Throughput: 63.568 Gbps (Max: 72.804, Min: 54.034, Sum: 508.542)
Cluster-Wide Average, Layer-Level Communication Throughput: 35.386 Gbps (Max: 43.879, Min: 27.366, Sum: 283.085)
Layer-level communication (cluster-wide, per-epoch): 0.233 GB
Graph-level communication (cluster-wide, per-epoch): 1.021 GB
Weight-sync communication (cluster-wide, per-epoch): 0.254 GB
Total communication (cluster-wide, per-epoch): 1.507 GB
****** Accuracy Results ******
Highest valid_acc: 0.3780
Target test_acc: 0.3631
Epoch to reach the target acc: 2399
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
