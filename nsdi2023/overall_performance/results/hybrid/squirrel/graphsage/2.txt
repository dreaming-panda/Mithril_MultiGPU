Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
        It takes 0.009 seconds.
        It takes 0.009 seconds.
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
        It takes 0.010 seconds.
Building the CSC structure...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
Building the Feature Vector...
        It takes 0.011 seconds.
        It takes 0.012 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 32
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.672 Gbps (per GPU), 477.373 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.367 Gbps (per GPU), 474.938 Gbps (aggregated)
The layer-level communication performance: 59.368 Gbps (per GPU), 474.945 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.107 Gbps (per GPU), 472.858 Gbps (aggregated)
The layer-level communication performance: 59.076 Gbps (per GPU), 472.610 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.881 Gbps (per GPU), 471.047 Gbps (aggregated)
The layer-level communication performance: 58.839 Gbps (per GPU), 470.715 Gbps (aggregated)
The layer-level communication performance: 58.804 Gbps (per GPU), 470.431 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.960 Gbps (per GPU), 1247.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.963 Gbps (per GPU), 1247.701 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.968 Gbps (per GPU), 1247.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.968 Gbps (per GPU), 1247.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.971 Gbps (per GPU), 1247.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.971 Gbps (per GPU), 1247.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.971 Gbps (per GPU), 1247.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.974 Gbps (per GPU), 1247.792 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 97.820 Gbps (per GPU), 782.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.816 Gbps (per GPU), 782.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.817 Gbps (per GPU), 782.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.816 Gbps (per GPU), 782.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.817 Gbps (per GPU), 782.538 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.814 Gbps (per GPU), 782.513 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.818 Gbps (per GPU), 782.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.814 Gbps (per GPU), 782.513 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.035 Gbps (per GPU), 256.279 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.037 Gbps (per GPU), 256.292 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.034 Gbps (per GPU), 256.273 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.034 Gbps (per GPU), 256.270 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.030 Gbps (per GPU), 256.238 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.033 Gbps (per GPU), 256.264 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.024 Gbps (per GPU), 256.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.032 Gbps (per GPU), 256.260 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.71ms  0.52ms  0.37ms  1.93  0.08K  0.02M
 chk_1  0.84ms  0.61ms  0.36ms  2.36  0.16K  0.01M
 chk_2  0.77ms  0.56ms  0.35ms  2.20  0.13K  0.01M
 chk_3  0.86ms  0.68ms  0.35ms  2.42  0.18K  0.01M
 chk_4  0.84ms  0.61ms  0.35ms  2.36  0.15K  0.01M
 chk_5  0.96ms  0.69ms  0.35ms  2.75  0.25K  0.01M
 chk_6  0.72ms  0.53ms  0.36ms  1.97  0.10K  0.02M
 chk_7  0.76ms  0.56ms  0.36ms  2.14  0.11K  0.02M
 chk_8  0.93ms  0.68ms  0.35ms  2.67  0.23K  0.01M
 chk_9  0.82ms  0.60ms  0.34ms  2.38  0.14K  0.01M
chk_10  0.93ms  0.67ms  0.36ms  2.61  0.20K  0.01M
chk_11  0.71ms  0.53ms  0.36ms  1.95  0.09K  0.02M
chk_12  0.83ms  0.61ms  0.35ms  2.35  0.16K  0.01M
chk_13  0.83ms  0.61ms  0.35ms  2.36  0.16K  0.01M
chk_14  0.82ms  0.61ms  0.34ms  2.38  0.14K  0.01M
chk_15  0.94ms  0.68ms  0.35ms  2.64  0.21K  0.01M
chk_16  0.86ms  0.80ms  0.35ms  2.43  0.18K  0.01M
chk_17  1.01ms  0.70ms  0.31ms  3.28  0.29K  0.01M
chk_18  1.03ms  0.70ms  0.31ms  3.30  0.31K  0.00M
chk_19  0.76ms  0.56ms  0.35ms  2.16  0.13K  0.01M
chk_20  0.76ms  0.56ms  0.35ms  2.17  0.13K  0.01M
chk_21  0.86ms  0.68ms  0.35ms  2.42  0.18K  0.01M
chk_22  0.75ms  0.55ms  0.35ms  2.17  0.13K  0.01M
chk_23  0.85ms  0.67ms  0.35ms  2.40  0.16K  0.01M
chk_24  0.71ms  0.53ms  0.36ms  1.96  0.09K  0.02M
chk_25  0.72ms  0.54ms  0.37ms  1.95  0.09K  0.02M
chk_26  0.86ms  0.68ms  0.35ms  2.43  0.18K  0.01M
chk_27  0.77ms  0.56ms  0.35ms  2.21  0.13K  0.01M
chk_28  0.85ms  0.68ms  0.35ms  2.41  0.17K  0.01M
chk_29  0.83ms  0.61ms  0.36ms  2.30  0.15K  0.01M
chk_30  0.95ms  0.69ms  0.35ms  2.70  0.24K  0.01M
chk_31  0.92ms  0.67ms  0.35ms  2.61  0.20K  0.01M
   Avg  0.84  0.62  0.35
   Max  1.03  0.80  0.37
   Min  0.71  0.52  0.31
 Ratio  1.45  1.53  1.20
   Var  0.01  0.00  0.00
Profiling takes 0.776 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 86.595 ms
Partition 0 [0, 4) has cost: 86.595 ms
Partition 1 [4, 8) has cost: 79.804 ms
Partition 2 [8, 12) has cost: 79.804 ms
Partition 3 [12, 16) has cost: 79.804 ms
Partition 4 [16, 20) has cost: 79.804 ms
Partition 5 [20, 24) has cost: 79.804 ms
Partition 6 [24, 28) has cost: 79.804 ms
Partition 7 [28, 32) has cost: 71.099 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 37.916 ms
GPU 0, Compute+Comm Time: 29.810 ms, Bubble Time: 6.361 ms, Imbalance Overhead: 1.746 ms
GPU 1, Compute+Comm Time: 27.515 ms, Bubble Time: 6.221 ms, Imbalance Overhead: 4.180 ms
GPU 2, Compute+Comm Time: 27.515 ms, Bubble Time: 6.226 ms, Imbalance Overhead: 4.176 ms
GPU 3, Compute+Comm Time: 27.515 ms, Bubble Time: 6.152 ms, Imbalance Overhead: 4.249 ms
GPU 4, Compute+Comm Time: 27.515 ms, Bubble Time: 6.228 ms, Imbalance Overhead: 4.174 ms
GPU 5, Compute+Comm Time: 27.515 ms, Bubble Time: 6.238 ms, Imbalance Overhead: 4.163 ms
GPU 6, Compute+Comm Time: 27.515 ms, Bubble Time: 6.363 ms, Imbalance Overhead: 4.038 ms
GPU 7, Compute+Comm Time: 25.250 ms, Bubble Time: 6.483 ms, Imbalance Overhead: 6.184 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 81.953 ms
GPU 0, Compute+Comm Time: 51.427 ms, Bubble Time: 13.737 ms, Imbalance Overhead: 16.789 ms
GPU 1, Compute+Comm Time: 57.867 ms, Bubble Time: 13.379 ms, Imbalance Overhead: 10.707 ms
GPU 2, Compute+Comm Time: 57.867 ms, Bubble Time: 13.072 ms, Imbalance Overhead: 11.014 ms
GPU 3, Compute+Comm Time: 57.867 ms, Bubble Time: 13.168 ms, Imbalance Overhead: 10.917 ms
GPU 4, Compute+Comm Time: 57.867 ms, Bubble Time: 13.154 ms, Imbalance Overhead: 10.932 ms
GPU 5, Compute+Comm Time: 57.867 ms, Bubble Time: 13.460 ms, Imbalance Overhead: 10.626 ms
GPU 6, Compute+Comm Time: 57.867 ms, Bubble Time: 13.614 ms, Imbalance Overhead: 10.472 ms
GPU 7, Compute+Comm Time: 62.364 ms, Bubble Time: 14.107 ms, Imbalance Overhead: 5.482 ms
The estimated cost of the whole pipeline: 125.862 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 166.399 ms
Partition 0 [0, 8) has cost: 166.399 ms
Partition 1 [8, 16) has cost: 159.608 ms
Partition 2 [16, 24) has cost: 159.608 ms
Partition 3 [24, 32) has cost: 150.903 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 39.811 ms
GPU 0, Compute+Comm Time: 32.449 ms, Bubble Time: 5.912 ms, Imbalance Overhead: 1.450 ms
GPU 1, Compute+Comm Time: 31.253 ms, Bubble Time: 6.222 ms, Imbalance Overhead: 2.336 ms
GPU 2, Compute+Comm Time: 31.253 ms, Bubble Time: 6.508 ms, Imbalance Overhead: 2.050 ms
GPU 3, Compute+Comm Time: 30.020 ms, Bubble Time: 6.874 ms, Imbalance Overhead: 2.917 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 82.924 ms
GPU 0, Compute+Comm Time: 60.614 ms, Bubble Time: 14.139 ms, Imbalance Overhead: 8.171 ms
GPU 1, Compute+Comm Time: 64.246 ms, Bubble Time: 13.382 ms, Imbalance Overhead: 5.297 ms
GPU 2, Compute+Comm Time: 64.246 ms, Bubble Time: 12.867 ms, Imbalance Overhead: 5.812 ms
GPU 3, Compute+Comm Time: 66.611 ms, Bubble Time: 12.439 ms, Imbalance Overhead: 3.874 ms
    The estimated cost with 2 DP ways is 128.872 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 326.007 ms
Partition 0 [0, 16) has cost: 326.007 ms
Partition 1 [16, 32) has cost: 310.510 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 51.757 ms
GPU 0, Compute+Comm Time: 45.446 ms, Bubble Time: 5.237 ms, Imbalance Overhead: 1.073 ms
GPU 1, Compute+Comm Time: 44.151 ms, Bubble Time: 6.125 ms, Imbalance Overhead: 1.482 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.342 ms
GPU 0, Compute+Comm Time: 78.478 ms, Bubble Time: 11.033 ms, Imbalance Overhead: 2.831 ms
GPU 1, Compute+Comm Time: 81.808 ms, Bubble Time: 9.348 ms, Imbalance Overhead: 1.186 ms
    The estimated cost with 4 DP ways is 151.303 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 636.517 ms
Partition 0 [0, 32) has cost: 636.517 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 143.559 ms
GPU 0, Compute+Comm Time: 143.559 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 179.601 ms
GPU 0, Compute+Comm Time: 179.601 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 339.318 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 66)...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 0 initializing the weights for op[0, 66)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
+++++++++ Node 7 initializing the weights for op[194, 257)...
+++++++++ Node 5 initializing the weights for op[130, 194)...
+++++++++ Node 6 initializing the weights for op[194, 257)...
+++++++++ Node 2 initializing the weights for op[66, 130)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 3, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.7104	TrainAcc 0.1979	ValidAcc 0.2043	TestAcc 0.1979	BestValid 0.2043
	Epoch 50:	Loss 1.6074	TrainAcc 0.2276	ValidAcc 0.2181	TestAcc 0.2459	BestValid 0.2181
	Epoch 100:	Loss 1.6092	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 150:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 200:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 250:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 300:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 350:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 400:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 450:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 500:	Loss 1.6094	TrainAcc 0.1947	ValidAcc 0.2055	TestAcc 0.2017	BestValid 0.2181
	Epoch 550:	Loss 1.6094	TrainAcc 0.2015	ValidAcc 0.2115	TestAcc 0.2094	BestValid 0.2181
	Epoch 600:	Loss 1.6094	TrainAcc 0.2047	ValidAcc 0.2073	TestAcc 0.2104	BestValid 0.2181
	Epoch 650:	Loss 1.6094	TrainAcc 0.1975	ValidAcc 0.2049	TestAcc 0.1902	BestValid 0.2181
	Epoch 700:	Loss 1.6094	TrainAcc 0.1983	ValidAcc 0.1995	TestAcc 0.1940	BestValid 0.2181
	Epoch 750:	Loss 1.6094	TrainAcc 0.2079	ValidAcc 0.1947	TestAcc 0.2171	BestValid 0.2181
	Epoch 800:	Loss 1.6094	TrainAcc 0.2059	ValidAcc 0.1953	TestAcc 0.1931	BestValid 0.2181
	Epoch 850:	Loss 1.6094	TrainAcc 0.2059	ValidAcc 0.1953	TestAcc 0.1931	BestValid 0.2181
	Epoch 900:	Loss 1.6093	TrainAcc 0.2059	ValidAcc 0.1953	TestAcc 0.1931	BestValid 0.2181
	Epoch 950:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1000:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1050:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1100:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1150:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1200:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1250:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1300:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1350:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1400:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1450:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1500:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1550:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1600:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1650:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1700:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1750:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1800:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1850:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1900:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 1950:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2000:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2050:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2100:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2150:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2200:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2250:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2300:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2350:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2400:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2450:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2500:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2550:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2600:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2650:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2700:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2750:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2800:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2850:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2900:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 2950:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3000:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3050:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3100:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3150:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3200:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3250:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3300:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3350:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3400:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3450:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3500:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3550:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3600:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3650:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3700:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3750:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3800:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3850:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3900:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 3950:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4000:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4050:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4100:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4150:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4200:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4250:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4300:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4350:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4400:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4450:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4500:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4550:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4600:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4650:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4700:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4750:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4800:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4850:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4900:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 4950:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
	Epoch 5000:	Loss 1.6094	TrainAcc 0.1951	ValidAcc 0.2055	TestAcc 0.2046	BestValid 0.2181
****** Epoch Time (Excluding Evaluation Cost): 0.189 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 25.007 ms (Max: 26.163, Min: 23.940, Sum: 200.054)
Cluster-Wide Average, Compute: 95.795 ms (Max: 101.409, Min: 90.677, Sum: 766.362)
Cluster-Wide Average, Communication-Layer: 3.506 ms (Max: 4.362, Min: 2.693, Sum: 28.044)
Cluster-Wide Average, Bubble-Imbalance: 8.900 ms (Max: 12.377, Min: 4.754, Sum: 71.200)
Cluster-Wide Average, Communication-Graph: 27.547 ms (Max: 28.739, Min: 26.285, Sum: 220.380)
Cluster-Wide Average, Optimization: 25.275 ms (Max: 26.763, Min: 23.614, Sum: 202.203)
Cluster-Wide Average, Others: 3.017 ms (Max: 4.864, Min: 1.728, Sum: 24.139)
****** Breakdown Sum: 189.048 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.703 GB (Max: 4.446, Min: 3.428, Sum: 29.624)
Cluster-Wide Average, Graph-Level Communication Throughput: 53.351 Gbps (Max: 59.357, Min: 48.577, Sum: 426.807)
Cluster-Wide Average, Layer-Level Communication Throughput: 34.622 Gbps (Max: 42.390, Min: 28.717, Sum: 276.979)
Layer-level communication (cluster-wide, per-epoch): 0.116 GB
Graph-level communication (cluster-wide, per-epoch): 1.021 GB
Weight-sync communication (cluster-wide, per-epoch): 0.478 GB
Total communication (cluster-wide, per-epoch): 1.615 GB
****** Accuracy Results ******
Highest valid_acc: 0.2181
Target test_acc: 0.2459
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
