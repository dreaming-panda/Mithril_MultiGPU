Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 4 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv8
Initialized node 7 on machine gnerv8
DONE MPI INIT
Initialized node 5 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.432 seconds.
Building the CSC structure...
        It takes 2.449 seconds.
Building the CSC structure...
        It takes 2.480 seconds.
Building the CSC structure...
        It takes 2.581 seconds.
Building the CSC structure...
        It takes 2.614 seconds.
Building the CSC structure...
        It takes 2.698 seconds.
Building the CSC structure...
        It takes 2.750 seconds.
Building the CSC structure...
        It takes 2.760 seconds.
Building the CSC structure...
        It takes 2.346 seconds.
        It takes 2.395 seconds.
        It takes 2.451 seconds.
        It takes 2.302 seconds.
        It takes 2.381 seconds.
        It takes 2.322 seconds.
        It takes 2.330 seconds.
        It takes 2.412 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.344 seconds.
Building the Label Vector...
        It takes 0.344 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 0.045 seconds.
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.322 seconds.
Building the Label Vector...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.035 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.973 Gbps (per GPU), 479.783 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.671 Gbps (per GPU), 477.364 Gbps (aggregated)
The layer-level communication performance: 59.661 Gbps (per GPU), 477.289 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.405 Gbps (per GPU), 475.237 Gbps (aggregated)
The layer-level communication performance: 59.372 Gbps (per GPU), 474.975 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.166 Gbps (per GPU), 473.326 Gbps (aggregated)
The layer-level communication performance: 59.114 Gbps (per GPU), 472.910 Gbps (aggregated)
The layer-level communication performance: 59.086 Gbps (per GPU), 472.687 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.344 Gbps (per GPU), 1266.755 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.332 Gbps (per GPU), 1266.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.344 Gbps (per GPU), 1266.755 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.329 Gbps (per GPU), 1266.635 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.341 Gbps (per GPU), 1266.731 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.332 Gbps (per GPU), 1266.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.344 Gbps (per GPU), 1266.755 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.335 Gbps (per GPU), 1266.683 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.517 Gbps (per GPU), 836.136 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.516 Gbps (per GPU), 836.128 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.519 Gbps (per GPU), 836.150 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.517 Gbps (per GPU), 836.136 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.518 Gbps (per GPU), 836.143 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.517 Gbps (per GPU), 836.136 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.519 Gbps (per GPU), 836.150 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.517 Gbps (per GPU), 836.136 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.480 Gbps (per GPU), 315.843 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.480 Gbps (per GPU), 315.843 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.480 Gbps (per GPU), 315.840 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.479 Gbps (per GPU), 315.833 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.480 Gbps (per GPU), 315.841 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.477 Gbps (per GPU), 315.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.472 Gbps (per GPU), 315.778 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.472 Gbps (per GPU), 315.779 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.72ms  3.25ms  4.29ms  1.57  8.38K  3.53M
 chk_1  2.98ms  3.41ms  4.25ms  1.42  6.74K  3.60M
 chk_2  2.88ms  3.32ms  4.17ms  1.45  7.27K  3.53M
 chk_3  2.91ms  3.39ms  4.28ms  1.47  7.92K  3.61M
 chk_4  2.79ms  3.13ms  3.84ms  1.37  5.33K  3.68M
 chk_5  2.89ms  3.44ms  4.91ms  1.70 10.07K  3.45M
 chk_6  3.04ms  3.58ms  4.58ms  1.51  9.41K  3.48M
 chk_7  2.83ms  3.31ms  4.23ms  1.50  8.12K  3.60M
 chk_8  2.91ms  3.29ms  4.08ms  1.40  6.09K  3.64M
 chk_9  2.83ms  3.41ms  4.56ms  1.61 11.10K  3.38M
chk_10  2.97ms  3.31ms  4.05ms  1.36  5.67K  3.63M
chk_11  2.87ms  3.34ms  4.27ms  1.49  8.16K  3.54M
chk_12  3.07ms  3.49ms  4.33ms  1.41  7.24K  3.55M
chk_13  2.86ms  3.19ms  3.92ms  1.37  5.41K  3.68M
chk_14  3.12ms  3.55ms  4.41ms  1.41  7.14K  3.53M
chk_15  3.03ms  3.56ms  4.54ms  1.50  9.25K  3.49M
chk_16  2.77ms  3.07ms  3.75ms  1.35  4.78K  3.77M
chk_17  2.94ms  3.35ms  4.16ms  1.41  6.85K  3.60M
chk_18  2.75ms  3.17ms  4.04ms  1.47  7.47K  3.57M
chk_19  2.76ms  3.10ms  3.79ms  1.37  4.88K  3.75M
chk_20  2.80ms  3.22ms  4.04ms  1.44  7.00K  3.63M
chk_21  2.76ms  3.11ms  3.84ms  1.39  5.41K  3.68M
chk_22  3.07ms  3.70ms  4.83ms  1.58 11.07K  3.39M
chk_23  2.90ms  3.38ms  4.17ms  1.44  7.23K  3.64M
chk_24  3.01ms  3.62ms  4.63ms  1.54 10.13K  3.43M
chk_25  2.77ms  3.15ms  3.93ms  1.42  6.40K  3.57M
chk_26  2.97ms  3.33ms  4.05ms  1.36  5.78K  3.55M
chk_27  2.95ms  3.45ms  4.42ms  1.50  9.34K  3.48M
chk_28  3.15ms  3.56ms  4.33ms  1.38  6.37K  3.57M
chk_29  2.93ms  3.27ms  3.96ms  1.35  5.16K  3.78M
chk_30  2.82ms  3.18ms  3.89ms  1.38  5.44K  3.67M
chk_31  3.00ms  3.38ms  4.16ms  1.39  6.33K  3.63M
   Avg  2.91  3.34  4.21
   Max  3.15  3.70  4.91
   Min  2.72  3.07  3.75
 Ratio  1.15  1.21  1.31
   Var  0.01  0.03  0.08
Profiling takes 3.863 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 100)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [204, 308)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [0, 100)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [204, 308)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [100, 204)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [308, 421)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [100, 204)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [308, 421)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[100, 204)...
+++++++++ Node 5 initializing the weights for op[204, 308)...
+++++++++ Node 3 initializing the weights for op[100, 204)...
+++++++++ Node 6 initializing the weights for op[308, 421)...
+++++++++ Node 4 initializing the weights for op[204, 308)...
+++++++++ Node 7 initializing the weights for op[308, 421)...
+++++++++ Node 0 initializing the weights for op[0, 100)...
+++++++++ Node 1 initializing the weights for op[0, 100)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.3195	TrainAcc 0.1676	ValidAcc 0.1966	TestAcc 0.1974	BestValid 0.1966
	Epoch 50:	Loss 0.9423	TrainAcc 0.8423	ValidAcc 0.8545	TestAcc 0.8505	BestValid 0.8545
	Epoch 100:	Loss 0.4375	TrainAcc 0.9356	ValidAcc 0.9416	TestAcc 0.9399	BestValid 0.9416
	Epoch 150:	Loss 0.3373	TrainAcc 0.9472	ValidAcc 0.9502	TestAcc 0.9494	BestValid 0.9502
	Epoch 200:	Loss 0.2874	TrainAcc 0.9535	ValidAcc 0.9556	TestAcc 0.9544	BestValid 0.9556
	Epoch 250:	Loss 0.2584	TrainAcc 0.9579	ValidAcc 0.9579	TestAcc 0.9572	BestValid 0.9579
	Epoch 300:	Loss 0.2392	TrainAcc 0.9611	ValidAcc 0.9593	TestAcc 0.9593	BestValid 0.9593
	Epoch 350:	Loss 0.2206	TrainAcc 0.9635	ValidAcc 0.9601	TestAcc 0.9596	BestValid 0.9601
	Epoch 400:	Loss 0.2083	TrainAcc 0.9659	ValidAcc 0.9612	TestAcc 0.9604	BestValid 0.9612
	Epoch 450:	Loss 0.1957	TrainAcc 0.9680	ValidAcc 0.9621	TestAcc 0.9611	BestValid 0.9621
	Epoch 500:	Loss 0.1859	TrainAcc 0.9698	ValidAcc 0.9631	TestAcc 0.9611	BestValid 0.9631
	Epoch 550:	Loss 0.1793	TrainAcc 0.9712	ValidAcc 0.9640	TestAcc 0.9620	BestValid 0.9640
	Epoch 600:	Loss 0.1710	TrainAcc 0.9728	ValidAcc 0.9640	TestAcc 0.9620	BestValid 0.9640
	Epoch 650:	Loss 0.1635	TrainAcc 0.9738	ValidAcc 0.9638	TestAcc 0.9629	BestValid 0.9640
	Epoch 700:	Loss 0.1566	TrainAcc 0.9750	ValidAcc 0.9641	TestAcc 0.9632	BestValid 0.9641
	Epoch 750:	Loss 0.1503	TrainAcc 0.9762	ValidAcc 0.9647	TestAcc 0.9632	BestValid 0.9647
	Epoch 800:	Loss 0.1455	TrainAcc 0.9772	ValidAcc 0.9647	TestAcc 0.9632	BestValid 0.9647
	Epoch 850:	Loss 0.1407	TrainAcc 0.9783	ValidAcc 0.9647	TestAcc 0.9636	BestValid 0.9647
	Epoch 900:	Loss 0.1355	TrainAcc 0.9795	ValidAcc 0.9654	TestAcc 0.9634	BestValid 0.9654
	Epoch 950:	Loss 0.1314	TrainAcc 0.9804	ValidAcc 0.9650	TestAcc 0.9639	BestValid 0.9654
	Epoch 1000:	Loss 0.1270	TrainAcc 0.9813	ValidAcc 0.9650	TestAcc 0.9637	BestValid 0.9654
	Epoch 1050:	Loss 0.1216	TrainAcc 0.9821	ValidAcc 0.9650	TestAcc 0.9641	BestValid 0.9654
	Epoch 1100:	Loss 0.1188	TrainAcc 0.9831	ValidAcc 0.9652	TestAcc 0.9639	BestValid 0.9654
	Epoch 1150:	Loss 0.1126	TrainAcc 0.9839	ValidAcc 0.9650	TestAcc 0.9640	BestValid 0.9654
	Epoch 1200:	Loss 0.1117	TrainAcc 0.9845	ValidAcc 0.9647	TestAcc 0.9639	BestValid 0.9654
	Epoch 1250:	Loss 0.1047	TrainAcc 0.9854	ValidAcc 0.9648	TestAcc 0.9640	BestValid 0.9654
	Epoch 1300:	Loss 0.1024	TrainAcc 0.9864	ValidAcc 0.9650	TestAcc 0.9638	BestValid 0.9654
	Epoch 1350:	Loss 0.0993	TrainAcc 0.9869	ValidAcc 0.9647	TestAcc 0.9638	BestValid 0.9654
	Epoch 1400:	Loss 0.0960	TrainAcc 0.9877	ValidAcc 0.9648	TestAcc 0.9629	BestValid 0.9654
	Epoch 1450:	Loss 0.0937	TrainAcc 0.9884	ValidAcc 0.9649	TestAcc 0.9633	BestValid 0.9654
	Epoch 1500:	Loss 0.0910	TrainAcc 0.9889	ValidAcc 0.9651	TestAcc 0.9631	BestValid 0.9654
	Epoch 1550:	Loss 0.0870	TrainAcc 0.9894	ValidAcc 0.9645	TestAcc 0.9631	BestValid 0.9654
	Epoch 1600:	Loss 0.0832	TrainAcc 0.9901	ValidAcc 0.9644	TestAcc 0.9629	BestValid 0.9654
	Epoch 1650:	Loss 0.0805	TrainAcc 0.9907	ValidAcc 0.9644	TestAcc 0.9627	BestValid 0.9654
	Epoch 1700:	Loss 0.0774	TrainAcc 0.9914	ValidAcc 0.9642	TestAcc 0.9630	BestValid 0.9654
	Epoch 1750:	Loss 0.0745	TrainAcc 0.9916	ValidAcc 0.9643	TestAcc 0.9631	BestValid 0.9654
	Epoch 1800:	Loss 0.0732	TrainAcc 0.9922	ValidAcc 0.9644	TestAcc 0.9628	BestValid 0.9654
	Epoch 1850:	Loss 0.0699	TrainAcc 0.9925	ValidAcc 0.9644	TestAcc 0.9630	BestValid 0.9654
	Epoch 1900:	Loss 0.0667	TrainAcc 0.9931	ValidAcc 0.9648	TestAcc 0.9625	BestValid 0.9654
	Epoch 1950:	Loss 0.0653	TrainAcc 0.9935	ValidAcc 0.9647	TestAcc 0.9622	BestValid 0.9654
	Epoch 2000:	Loss 0.0634	TrainAcc 0.9938	ValidAcc 0.9646	TestAcc 0.9625	BestValid 0.9654
	Epoch 2050:	Loss 0.0619	TrainAcc 0.9943	ValidAcc 0.9643	TestAcc 0.9626	BestValid 0.9654
	Epoch 2100:	Loss 0.0592	TrainAcc 0.9945	ValidAcc 0.9639	TestAcc 0.9621	BestValid 0.9654
	Epoch 2150:	Loss 0.0558	TrainAcc 0.9950	ValidAcc 0.9644	TestAcc 0.9624	BestValid 0.9654
	Epoch 2200:	Loss 0.0536	TrainAcc 0.9953	ValidAcc 0.9641	TestAcc 0.9624	BestValid 0.9654
	Epoch 2250:	Loss 0.0525	TrainAcc 0.9958	ValidAcc 0.9641	TestAcc 0.9624	BestValid 0.9654
	Epoch 2300:	Loss 0.0503	TrainAcc 0.9959	ValidAcc 0.9639	TestAcc 0.9626	BestValid 0.9654
	Epoch 2350:	Loss 0.0477	TrainAcc 0.9964	ValidAcc 0.9637	TestAcc 0.9616	BestValid 0.9654
	Epoch 2400:	Loss 0.0471	TrainAcc 0.9966	ValidAcc 0.9637	TestAcc 0.9619	BestValid 0.9654
	Epoch 2450:	Loss 0.0454	TrainAcc 0.9968	ValidAcc 0.9637	TestAcc 0.9619	BestValid 0.9654
	Epoch 2500:	Loss 0.0438	TrainAcc 0.9971	ValidAcc 0.9631	TestAcc 0.9619	BestValid 0.9654
	Epoch 2550:	Loss 0.0424	TrainAcc 0.9973	ValidAcc 0.9633	TestAcc 0.9618	BestValid 0.9654
