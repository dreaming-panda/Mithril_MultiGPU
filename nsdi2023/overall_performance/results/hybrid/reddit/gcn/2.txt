Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.803 seconds.
Building the CSC structure...
        It takes 1.875 seconds.
Building the CSC structure...
        It takes 1.909 seconds.
Building the CSC structure...
        It takes 1.922 seconds.
Building the CSC structure...
        It takes 2.301 seconds.
Building the CSC structure...
        It takes 2.504 seconds.
Building the CSC structure...
        It takes 2.533 seconds.
Building the CSC structure...
        It takes 2.630 seconds.
Building the CSC structure...
        It takes 1.849 seconds.
        It takes 1.779 seconds.
        It takes 1.850 seconds.
        It takes 1.829 seconds.
        It takes 2.173 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.283 seconds.
        It takes 2.354 seconds.
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 0.045 seconds.
        It takes 2.363 seconds.
        It takes 0.295 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.039 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.266 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.029 seconds.
Building the Feature Vector...
        It takes 0.249 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.991 Gbps (per GPU), 463.928 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.020 Gbps (per GPU), 464.158 Gbps (aggregated)
The layer-level communication performance: 57.703 Gbps (per GPU), 461.628 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.463 Gbps (per GPU), 459.702 Gbps (aggregated)
The layer-level communication performance: 57.486 Gbps (per GPU), 459.889 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.267 Gbps (per GPU), 458.135 Gbps (aggregated)
The layer-level communication performance: 57.522 Gbps (per GPU), 460.177 Gbps (aggregated)
The layer-level communication performance: 57.181 Gbps (per GPU), 457.448 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.890 Gbps (per GPU), 1279.117 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.902 Gbps (per GPU), 1279.214 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.887 Gbps (per GPU), 1279.092 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.881 Gbps (per GPU), 1279.047 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.887 Gbps (per GPU), 1279.092 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.905 Gbps (per GPU), 1279.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.877 Gbps (per GPU), 1279.019 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.905 Gbps (per GPU), 1279.239 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.445 Gbps (per GPU), 811.559 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.445 Gbps (per GPU), 811.559 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.444 Gbps (per GPU), 811.552 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.446 Gbps (per GPU), 811.565 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.444 Gbps (per GPU), 811.552 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.445 Gbps (per GPU), 811.559 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.445 Gbps (per GPU), 811.559 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.446 Gbps (per GPU), 811.565 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.696 Gbps (per GPU), 317.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.697 Gbps (per GPU), 317.578 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.697 Gbps (per GPU), 317.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.695 Gbps (per GPU), 317.561 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.697 Gbps (per GPU), 317.576 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.694 Gbps (per GPU), 317.554 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.697 Gbps (per GPU), 317.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.695 Gbps (per GPU), 317.560 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.43ms  2.34ms  2.28ms  1.07  8.38K  3.53M
 chk_1  2.79ms  2.63ms  2.51ms  1.11  6.74K  3.60M
 chk_2  2.67ms  2.49ms  2.42ms  1.10  7.27K  3.53M
 chk_3  2.68ms  2.53ms  2.44ms  1.10  7.92K  3.61M
 chk_4  2.62ms  2.49ms  2.44ms  1.07  5.33K  3.68M
 chk_5  2.60ms  2.39ms  2.26ms  1.15 10.07K  3.45M
 chk_6  2.77ms  2.57ms  2.45ms  1.13  9.41K  3.48M
 chk_7  2.62ms  2.44ms  2.36ms  1.11  8.12K  3.60M
 chk_8  2.72ms  2.59ms  2.66ms  1.05  6.09K  3.64M
 chk_9  2.51ms  2.28ms  2.16ms  1.17 11.10K  3.38M
chk_10  2.76ms  2.64ms  2.57ms  1.07  5.67K  3.63M
chk_11  2.62ms  2.45ms  2.36ms  1.11  8.16K  3.54M
chk_12  2.83ms  2.67ms  2.59ms  1.09  7.24K  3.55M
chk_13  2.64ms  2.54ms  2.47ms  1.07  5.41K  3.68M
chk_14  2.90ms  2.74ms  2.65ms  1.09  7.14K  3.53M
chk_15  2.74ms  2.55ms  2.44ms  1.12  9.25K  3.49M
chk_16  2.55ms  2.47ms  2.54ms  1.03  4.78K  3.77M
chk_17  2.70ms  2.56ms  2.47ms  1.09  6.85K  3.60M
chk_18  2.51ms  2.51ms  2.28ms  1.10  7.47K  3.57M
chk_19  2.59ms  2.48ms  2.41ms  1.08  4.88K  3.75M
chk_20  2.59ms  2.44ms  2.37ms  1.09  7.00K  3.63M
chk_21  2.57ms  2.45ms  2.41ms  1.07  5.41K  3.68M
chk_22  2.78ms  2.54ms  2.42ms  1.15 11.07K  3.39M
chk_23  2.69ms  2.53ms  2.46ms  1.09  7.23K  3.64M
chk_24  2.71ms  2.54ms  2.50ms  1.08 10.13K  3.43M
chk_25  2.54ms  2.41ms  2.30ms  1.10  6.40K  3.57M
chk_26  2.74ms  2.61ms  2.56ms  1.07  5.78K  3.55M
chk_27  2.63ms  2.41ms  2.34ms  1.12  9.34K  3.48M
chk_28  2.96ms  2.76ms  2.69ms  1.10  6.37K  3.57M
chk_29  2.75ms  2.63ms  2.55ms  1.08  5.16K  3.78M
chk_30  2.63ms  2.53ms  2.43ms  1.08  5.44K  3.67M
chk_31  2.78ms  2.65ms  2.62ms  1.06  6.33K  3.63M
   Avg  2.68  2.53  2.45
   Max  2.96  2.76  2.69
   Min  2.43  2.28  2.16
 Ratio  1.22  1.21  1.25
   Var  0.01  0.01  0.02
Profiling takes 2.845 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 328.247 ms
Partition 0 [0, 4) has cost: 328.247 ms
Partition 1 [4, 8) has cost: 323.498 ms
Partition 2 [8, 12) has cost: 323.498 ms
Partition 3 [12, 16) has cost: 323.498 ms
Partition 4 [16, 20) has cost: 323.498 ms
Partition 5 [20, 24) has cost: 323.498 ms
Partition 6 [24, 28) has cost: 323.498 ms
Partition 7 [28, 32) has cost: 321.046 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 155.349 ms
GPU 0, Compute+Comm Time: 122.829 ms, Bubble Time: 27.805 ms, Imbalance Overhead: 4.715 ms
GPU 1, Compute+Comm Time: 120.708 ms, Bubble Time: 27.399 ms, Imbalance Overhead: 7.243 ms
GPU 2, Compute+Comm Time: 120.708 ms, Bubble Time: 27.296 ms, Imbalance Overhead: 7.346 ms
GPU 3, Compute+Comm Time: 120.708 ms, Bubble Time: 27.138 ms, Imbalance Overhead: 7.504 ms
GPU 4, Compute+Comm Time: 120.708 ms, Bubble Time: 27.011 ms, Imbalance Overhead: 7.631 ms
GPU 5, Compute+Comm Time: 120.708 ms, Bubble Time: 26.990 ms, Imbalance Overhead: 7.652 ms
GPU 6, Compute+Comm Time: 120.708 ms, Bubble Time: 26.969 ms, Imbalance Overhead: 7.673 ms
GPU 7, Compute+Comm Time: 119.807 ms, Bubble Time: 27.124 ms, Imbalance Overhead: 8.418 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 293.513 ms
GPU 0, Compute+Comm Time: 226.949 ms, Bubble Time: 51.308 ms, Imbalance Overhead: 15.256 ms
GPU 1, Compute+Comm Time: 228.501 ms, Bubble Time: 51.232 ms, Imbalance Overhead: 13.780 ms
GPU 2, Compute+Comm Time: 228.501 ms, Bubble Time: 51.252 ms, Imbalance Overhead: 13.760 ms
GPU 3, Compute+Comm Time: 228.501 ms, Bubble Time: 51.273 ms, Imbalance Overhead: 13.739 ms
GPU 4, Compute+Comm Time: 228.501 ms, Bubble Time: 51.551 ms, Imbalance Overhead: 13.461 ms
GPU 5, Compute+Comm Time: 228.501 ms, Bubble Time: 51.868 ms, Imbalance Overhead: 13.144 ms
GPU 6, Compute+Comm Time: 228.501 ms, Bubble Time: 52.083 ms, Imbalance Overhead: 12.929 ms
GPU 7, Compute+Comm Time: 231.128 ms, Bubble Time: 53.039 ms, Imbalance Overhead: 9.345 ms
The estimated cost of the whole pipeline: 471.305 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 651.745 ms
Partition 0 [0, 8) has cost: 651.745 ms
Partition 1 [8, 16) has cost: 646.996 ms
Partition 2 [16, 24) has cost: 646.996 ms
Partition 3 [24, 32) has cost: 644.544 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 163.461 ms
GPU 0, Compute+Comm Time: 134.224 ms, Bubble Time: 26.085 ms, Imbalance Overhead: 3.153 ms
GPU 1, Compute+Comm Time: 133.195 ms, Bubble Time: 25.427 ms, Imbalance Overhead: 4.839 ms
GPU 2, Compute+Comm Time: 133.195 ms, Bubble Time: 25.095 ms, Imbalance Overhead: 5.171 ms
GPU 3, Compute+Comm Time: 132.820 ms, Bubble Time: 24.752 ms, Imbalance Overhead: 5.890 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 296.731 ms
GPU 0, Compute+Comm Time: 241.856 ms, Bubble Time: 45.351 ms, Imbalance Overhead: 9.524 ms
GPU 1, Compute+Comm Time: 242.428 ms, Bubble Time: 45.745 ms, Imbalance Overhead: 8.558 ms
GPU 2, Compute+Comm Time: 242.428 ms, Bubble Time: 46.125 ms, Imbalance Overhead: 8.177 ms
GPU 3, Compute+Comm Time: 243.723 ms, Bubble Time: 47.427 ms, Imbalance Overhead: 5.582 ms
    The estimated cost with 2 DP ways is 483.202 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1298.741 ms
Partition 0 [0, 16) has cost: 1298.741 ms
Partition 1 [16, 32) has cost: 1291.540 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 211.860 ms
GPU 0, Compute+Comm Time: 183.926 ms, Bubble Time: 22.882 ms, Imbalance Overhead: 5.052 ms
GPU 1, Compute+Comm Time: 183.208 ms, Bubble Time: 23.235 ms, Imbalance Overhead: 5.417 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 338.568 ms
GPU 0, Compute+Comm Time: 295.505 ms, Bubble Time: 36.955 ms, Imbalance Overhead: 6.108 ms
GPU 1, Compute+Comm Time: 296.358 ms, Bubble Time: 36.964 ms, Imbalance Overhead: 5.245 ms
    The estimated cost with 4 DP ways is 577.949 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2590.281 ms
Partition 0 [0, 32) has cost: 2590.281 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 491.596 ms
GPU 0, Compute+Comm Time: 491.596 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 606.262 ms
GPU 0, Compute+Comm Time: 606.262 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1152.750 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 121)
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 41)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [81, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [121, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 81)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [121, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [41, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[81, 121)...
+++++++++ Node 4 initializing the weights for op[81, 121)...
+++++++++ Node 1 initializing the weights for op[0, 41)...
+++++++++ Node 6 initializing the weights for op[121, 160)...
+++++++++ Node 3 initializing the weights for op[41, 81)...
+++++++++ Node 7 initializing the weights for op[121, 160)...
+++++++++ Node 0 initializing the weights for op[0, 41)...
+++++++++ Node 2 initializing the weights for op[41, 81)...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7136	TrainAcc 0.0391	ValidAcc 0.0383	TestAcc 0.0369	BestValid 0.0383
	Epoch 50:	Loss 3.4936	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.0383
	Epoch 100:	Loss 3.3516	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.3027	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 3.2366	TrainAcc 0.1236	ValidAcc 0.1183	TestAcc 0.1146	BestValid 0.1183
	Epoch 250:	Loss 2.9770	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1183
	Epoch 300:	Loss 2.5470	TrainAcc 0.0692	ValidAcc 0.0586	TestAcc 0.0576	BestValid 0.1183
	Epoch 350:	Loss 2.1560	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1183
	Epoch 400:	Loss 2.1214	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1183
	Epoch 450:	Loss 2.2515	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1183
	Epoch 500:	Loss 2.2771	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1183
	Epoch 550:	Loss 2.5112	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1183
	Epoch 600:	Loss 2.6358	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1183
	Epoch 650:	Loss 2.9521	TrainAcc 0.1253	ValidAcc 0.1203	TestAcc 0.1162	BestValid 0.1203
	Epoch 700:	Loss 3.1649	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1203
	Epoch 750:	Loss 3.2416	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1203
	Epoch 800:	Loss 3.0947	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1203
	Epoch 850:	Loss 3.0265	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1203
	Epoch 900:	Loss 3.0553	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1203
	Epoch 950:	Loss 3.0735	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1203
	Epoch 1000:	Loss 3.3354	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1203
	Epoch 1050:	Loss 3.6109	TrainAcc 0.1229	ValidAcc 0.1182	TestAcc 0.1143	BestValid 0.1203
	Epoch 1100:	Loss 3.0883	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1203
	Epoch 1150:	Loss 2.9965	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1203
	Epoch 1200:	Loss 2.7860	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1203
	Epoch 1250:	Loss 2.8950	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1203
	Epoch 1300:	Loss 3.1516	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1203
	Epoch 1350:	Loss 3.2050	TrainAcc 0.1226	ValidAcc 0.1180	TestAcc 0.1140	BestValid 0.1203
	Epoch 1400:	Loss 3.0608	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1203
	Epoch 1450:	Loss 3.0293	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1203
	Epoch 1500:	Loss 3.0100	TrainAcc 0.1244	ValidAcc 0.1195	TestAcc 0.1158	BestValid 0.1203
	Epoch 1550:	Loss 2.9832	TrainAcc 0.1229	ValidAcc 0.1183	TestAcc 0.1146	BestValid 0.1203
	Epoch 1600:	Loss 2.9652	TrainAcc 0.1342	ValidAcc 0.1271	TestAcc 0.1242	BestValid 0.1271
	Epoch 1650:	Loss 2.9228	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1271
	Epoch 1700:	Loss 2.8277	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1271
	Epoch 1750:	Loss 2.8087	TrainAcc 0.1054	ValidAcc 0.1167	TestAcc 0.1123	BestValid 0.1271
	Epoch 1800:	Loss 2.8947	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1271
	Epoch 1850:	Loss 2.9193	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1271
	Epoch 1900:	Loss 3.2463	TrainAcc 0.1244	ValidAcc 0.1195	TestAcc 0.1160	BestValid 0.1271
	Epoch 1950:	Loss 2.9986	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1271
	Epoch 2000:	Loss 3.0568	TrainAcc 0.0708	ValidAcc 0.0600	TestAcc 0.0594	BestValid 0.1271
	Epoch 2050:	Loss 3.3747	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.1271
	Epoch 2100:	Loss 3.2307	TrainAcc 0.1373	ValidAcc 0.1286	TestAcc 0.1276	BestValid 0.1286
	Epoch 2150:	Loss 2.9959	TrainAcc 0.1247	ValidAcc 0.1200	TestAcc 0.1158	BestValid 0.1286
	Epoch 2200:	Loss 3.0066	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1286
	Epoch 2250:	Loss 3.0979	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1286
	Epoch 2300:	Loss 3.0889	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1286
	Epoch 2350:	Loss 3.0716	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1286
	Epoch 2400:	Loss 3.0634	TrainAcc 0.2235	ValidAcc 0.2410	TestAcc 0.2412	BestValid 0.2410
	Epoch 2450:	Loss 3.0200	TrainAcc 0.1360	ValidAcc 0.1286	TestAcc 0.1257	BestValid 0.2410
	Epoch 2500:	Loss 3.0117	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 2550:	Loss 3.0196	TrainAcc 0.1868	ValidAcc 0.2148	TestAcc 0.2119	BestValid 0.2410
	Epoch 2600:	Loss 2.9833	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 2650:	Loss 2.9804	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 2700:	Loss 3.1780	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 2750:	Loss 3.0323	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 2800:	Loss 3.0135	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 2850:	Loss 3.1630	TrainAcc 0.0699	ValidAcc 0.0595	TestAcc 0.0586	BestValid 0.2410
	Epoch 2900:	Loss 3.1528	TrainAcc 0.0552	ValidAcc 0.0554	TestAcc 0.0545	BestValid 0.2410
	Epoch 2950:	Loss 3.1592	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2410
	Epoch 3000:	Loss 3.0626	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3050:	Loss 3.0281	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3100:	Loss 3.3130	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3150:	Loss 3.4136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3200:	Loss 3.4101	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3250:	Loss 3.4031	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3300:	Loss 3.4007	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3350:	Loss 3.4010	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3400:	Loss 3.3993	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3450:	Loss 3.3959	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3500:	Loss 3.1924	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3550:	Loss 3.0539	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2410
	Epoch 3600:	Loss 3.0206	TrainAcc 0.0552	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3650:	Loss 3.0077	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3700:	Loss 3.0090	TrainAcc 0.1318	ValidAcc 0.1263	TestAcc 0.1229	BestValid 0.2410
	Epoch 3750:	Loss 2.9823	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2410
	Epoch 3800:	Loss 2.9817	TrainAcc 0.1349	ValidAcc 0.1283	TestAcc 0.1252	BestValid 0.2410
	Epoch 3850:	Loss 2.9795	TrainAcc 0.2439	ValidAcc 0.2768	TestAcc 0.2741	BestValid 0.2768
	Epoch 3900:	Loss 2.9612	TrainAcc 0.1203	ValidAcc 0.1184	TestAcc 0.1144	BestValid 0.2768
	Epoch 3950:	Loss 2.9511	TrainAcc 0.1603	ValidAcc 0.1624	TestAcc 0.1593	BestValid 0.2768
	Epoch 4000:	Loss 2.9301	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4050:	Loss 2.9666	TrainAcc 0.2056	ValidAcc 0.2056	TestAcc 0.2006	BestValid 0.2768
	Epoch 4100:	Loss 2.9155	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4150:	Loss 2.9145	TrainAcc 0.2246	ValidAcc 0.2495	TestAcc 0.2470	BestValid 0.2768
	Epoch 4200:	Loss 2.8867	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4250:	Loss 2.8884	TrainAcc 0.1989	ValidAcc 0.1934	TestAcc 0.1874	BestValid 0.2768
	Epoch 4300:	Loss 2.8557	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4350:	Loss 2.8738	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4400:	Loss 2.8892	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4450:	Loss 2.9606	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4500:	Loss 3.4060	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4550:	Loss 3.4260	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4600:	Loss 3.4144	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4650:	Loss 3.4088	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4700:	Loss 3.4103	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4750:	Loss 3.4103	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4800:	Loss 3.4100	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4850:	Loss 3.4083	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4900:	Loss 3.4142	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 4950:	Loss 3.4101	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
	Epoch 5000:	Loss 3.4026	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2768
****** Epoch Time (Excluding Evaluation Cost): 0.392 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 58.205 ms (Max: 60.207, Min: 52.828, Sum: 465.641)
Cluster-Wide Average, Compute: 235.914 ms (Max: 241.822, Min: 228.868, Sum: 1887.309)
Cluster-Wide Average, Communication-Layer: 16.534 ms (Max: 19.774, Min: 13.753, Sum: 132.270)
Cluster-Wide Average, Bubble-Imbalance: 11.971 ms (Max: 15.287, Min: 8.149, Sum: 95.767)
Cluster-Wide Average, Communication-Graph: 62.190 ms (Max: 65.518, Min: 59.738, Sum: 497.520)
Cluster-Wide Average, Optimization: 1.737 ms (Max: 1.829, Min: 1.637, Sum: 13.900)
Cluster-Wide Average, Others: 5.538 ms (Max: 11.713, Min: 3.447, Sum: 44.303)
****** Breakdown Sum: 392.089 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.054 GB (Max: 8.198, Min: 6.696, Sum: 56.433)
Cluster-Wide Average, Graph-Level Communication Throughput: 114.525 Gbps (Max: 126.862, Min: 101.423, Sum: 916.199)
Cluster-Wide Average, Layer-Level Communication Throughput: 33.082 Gbps (Max: 42.547, Min: 23.714, Sum: 264.660)
Layer-level communication (cluster-wide, per-epoch): 0.521 GB
Graph-level communication (cluster-wide, per-epoch): 5.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 5.868 GB
****** Accuracy Results ******
Highest valid_acc: 0.2768
Target test_acc: 0.2741
Epoch to reach the target acc: 3849
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
