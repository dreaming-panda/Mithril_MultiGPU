Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.846 seconds.
Building the CSC structure...
        It takes 2.049 seconds.
Building the CSC structure...
        It takes 2.056 seconds.
Building the CSC structure...
        It takes 2.188 seconds.
Building the CSC structure...
        It takes 2.425 seconds.
Building the CSC structure...
        It takes 2.427 seconds.
Building the CSC structure...
        It takes 2.594 seconds.
Building the CSC structure...
        It takes 2.651 seconds.
Building the CSC structure...
        It takes 1.899 seconds.
        It takes 1.858 seconds.
        It takes 1.899 seconds.
        It takes 2.383 seconds.
        It takes 2.270 seconds.
        It takes 2.343 seconds.
Building the Feature Vector...
        It takes 2.276 seconds.
        It takes 2.380 seconds.
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.298 seconds.
Building the Label Vector...
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.265 seconds.
        It takes 0.033 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.278 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.297 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.042 seconds.
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.084 Gbps (per GPU), 464.674 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.797 Gbps (per GPU), 462.374 Gbps (aggregated)
The layer-level communication performance: 57.789 Gbps (per GPU), 462.310 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.575 Gbps (per GPU), 460.600 Gbps (aggregated)
The layer-level communication performance: 57.539 Gbps (per GPU), 460.311 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.341 Gbps (per GPU), 458.730 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.268 Gbps (per GPU), 458.142 Gbps (aggregated)
The layer-level communication performance: 57.536 Gbps (per GPU), 460.292 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.022 Gbps (per GPU), 1256.179 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.043 Gbps (per GPU), 1256.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.025 Gbps (per GPU), 1256.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.037 Gbps (per GPU), 1256.297 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.025 Gbps (per GPU), 1256.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.025 Gbps (per GPU), 1256.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.028 Gbps (per GPU), 1256.226 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.920 Gbps (per GPU), 1255.357 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.001 Gbps (per GPU), 816.006 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.002 Gbps (per GPU), 816.012 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.999 Gbps (per GPU), 815.993 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.999 Gbps (per GPU), 815.992 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.000 Gbps (per GPU), 815.999 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.999 Gbps (per GPU), 815.993 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.002 Gbps (per GPU), 816.012 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.000 Gbps (per GPU), 815.999 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.724 Gbps (per GPU), 317.793 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.724 Gbps (per GPU), 317.791 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.722 Gbps (per GPU), 317.773 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.722 Gbps (per GPU), 317.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.723 Gbps (per GPU), 317.782 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.724 Gbps (per GPU), 317.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.721 Gbps (per GPU), 317.768 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.724 Gbps (per GPU), 317.792 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.43ms  2.37ms  2.28ms  1.07  8.38K  3.53M
 chk_1  2.76ms  2.62ms  2.56ms  1.08  6.74K  3.60M
 chk_2  2.70ms  2.52ms  2.42ms  1.12  7.27K  3.53M
 chk_3  2.70ms  2.54ms  2.44ms  1.11  7.92K  3.61M
 chk_4  2.62ms  2.50ms  2.44ms  1.07  5.33K  3.68M
 chk_5  2.60ms  2.39ms  2.28ms  1.14 10.07K  3.45M
 chk_6  2.79ms  2.57ms  2.46ms  1.13  9.41K  3.48M
 chk_7  2.62ms  2.47ms  2.37ms  1.11  8.12K  3.60M
 chk_8  2.73ms  2.61ms  2.53ms  1.08  6.09K  3.64M
 chk_9  2.54ms  2.29ms  2.17ms  1.17 11.10K  3.38M
chk_10  2.76ms  2.65ms  2.58ms  1.07  5.67K  3.63M
chk_11  2.62ms  2.46ms  2.38ms  1.10  8.16K  3.54M
chk_12  2.83ms  2.68ms  2.59ms  1.09  7.24K  3.55M
chk_13  2.66ms  2.53ms  2.46ms  1.08  5.41K  3.68M
chk_14  2.90ms  2.75ms  2.65ms  1.09  7.14K  3.53M
chk_15  2.77ms  2.55ms  2.43ms  1.14  9.25K  3.49M
chk_16  2.58ms  2.49ms  2.40ms  1.07  4.78K  3.77M
chk_17  2.72ms  2.57ms  2.48ms  1.10  6.85K  3.60M
chk_18  2.54ms  2.36ms  2.28ms  1.11  7.47K  3.57M
chk_19  2.61ms  2.51ms  2.41ms  1.08  4.88K  3.75M
chk_20  2.62ms  2.47ms  2.36ms  1.11  7.00K  3.63M
chk_21  2.59ms  2.47ms  2.39ms  1.08  5.41K  3.68M
chk_22  2.78ms  2.54ms  2.40ms  1.16 11.07K  3.39M
chk_23  2.72ms  2.54ms  2.45ms  1.11  7.23K  3.64M
chk_24  2.75ms  2.53ms  2.38ms  1.16 10.13K  3.43M
chk_25  2.55ms  2.41ms  2.32ms  1.10  6.40K  3.57M
chk_26  2.76ms  2.63ms  2.55ms  1.08  5.78K  3.55M
chk_27  2.63ms  2.43ms  2.32ms  1.13  9.34K  3.48M
chk_28  2.97ms  2.99ms  2.69ms  1.11  6.37K  3.57M
chk_29  2.75ms  2.61ms  2.55ms  1.08  5.16K  3.78M
chk_30  2.62ms  2.57ms  2.43ms  1.08  5.44K  3.67M
chk_31  2.77ms  2.67ms  2.55ms  1.09  6.33K  3.63M
   Avg  2.69  2.54  2.44
   Max  2.97  2.99  2.69
   Min  2.43  2.29  2.17
 Ratio  1.22  1.31  1.24
   Var  0.01  0.02  0.01
Profiling takes 2.842 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 329.785 ms
Partition 0 [0, 4) has cost: 329.785 ms
Partition 1 [4, 8) has cost: 325.096 ms
Partition 2 [8, 12) has cost: 325.096 ms
Partition 3 [12, 16) has cost: 325.096 ms
Partition 4 [16, 20) has cost: 325.096 ms
Partition 5 [20, 24) has cost: 325.096 ms
Partition 6 [24, 28) has cost: 325.096 ms
Partition 7 [28, 32) has cost: 321.843 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 155.865 ms
GPU 0, Compute+Comm Time: 123.304 ms, Bubble Time: 28.120 ms, Imbalance Overhead: 4.441 ms
GPU 1, Compute+Comm Time: 121.170 ms, Bubble Time: 27.690 ms, Imbalance Overhead: 7.006 ms
GPU 2, Compute+Comm Time: 121.170 ms, Bubble Time: 27.471 ms, Imbalance Overhead: 7.224 ms
GPU 3, Compute+Comm Time: 121.170 ms, Bubble Time: 27.211 ms, Imbalance Overhead: 7.485 ms
GPU 4, Compute+Comm Time: 121.170 ms, Bubble Time: 27.024 ms, Imbalance Overhead: 7.671 ms
GPU 5, Compute+Comm Time: 121.170 ms, Bubble Time: 26.908 ms, Imbalance Overhead: 7.788 ms
GPU 6, Compute+Comm Time: 121.170 ms, Bubble Time: 26.792 ms, Imbalance Overhead: 7.904 ms
GPU 7, Compute+Comm Time: 119.925 ms, Bubble Time: 26.934 ms, Imbalance Overhead: 9.007 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 300.159 ms
GPU 0, Compute+Comm Time: 227.587 ms, Bubble Time: 51.359 ms, Imbalance Overhead: 21.213 ms
GPU 1, Compute+Comm Time: 229.595 ms, Bubble Time: 51.244 ms, Imbalance Overhead: 19.319 ms
GPU 2, Compute+Comm Time: 229.595 ms, Bubble Time: 51.303 ms, Imbalance Overhead: 19.260 ms
GPU 3, Compute+Comm Time: 229.595 ms, Bubble Time: 51.363 ms, Imbalance Overhead: 19.200 ms
GPU 4, Compute+Comm Time: 229.595 ms, Bubble Time: 52.276 ms, Imbalance Overhead: 18.287 ms
GPU 5, Compute+Comm Time: 229.595 ms, Bubble Time: 53.440 ms, Imbalance Overhead: 17.123 ms
GPU 6, Compute+Comm Time: 229.595 ms, Bubble Time: 54.532 ms, Imbalance Overhead: 16.031 ms
GPU 7, Compute+Comm Time: 232.149 ms, Bubble Time: 56.290 ms, Imbalance Overhead: 11.719 ms
The estimated cost of the whole pipeline: 478.825 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 654.881 ms
Partition 0 [0, 8) has cost: 654.881 ms
Partition 1 [8, 16) has cost: 650.192 ms
Partition 2 [16, 24) has cost: 650.192 ms
Partition 3 [24, 32) has cost: 646.939 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.030 ms
GPU 0, Compute+Comm Time: 134.787 ms, Bubble Time: 26.292 ms, Imbalance Overhead: 2.951 ms
GPU 1, Compute+Comm Time: 133.740 ms, Bubble Time: 25.641 ms, Imbalance Overhead: 4.649 ms
GPU 2, Compute+Comm Time: 133.740 ms, Bubble Time: 25.128 ms, Imbalance Overhead: 5.162 ms
GPU 3, Compute+Comm Time: 133.149 ms, Bubble Time: 24.626 ms, Imbalance Overhead: 6.255 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 303.455 ms
GPU 0, Compute+Comm Time: 243.636 ms, Bubble Time: 45.472 ms, Imbalance Overhead: 14.347 ms
GPU 1, Compute+Comm Time: 244.616 ms, Bubble Time: 45.873 ms, Imbalance Overhead: 12.967 ms
GPU 2, Compute+Comm Time: 244.616 ms, Bubble Time: 46.285 ms, Imbalance Overhead: 12.554 ms
GPU 3, Compute+Comm Time: 245.747 ms, Bubble Time: 47.496 ms, Imbalance Overhead: 10.212 ms
    The estimated cost with 2 DP ways is 490.859 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1305.073 ms
Partition 0 [0, 16) has cost: 1305.073 ms
Partition 1 [16, 32) has cost: 1297.131 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 211.210 ms
GPU 0, Compute+Comm Time: 183.862 ms, Bubble Time: 22.952 ms, Imbalance Overhead: 4.396 ms
GPU 1, Compute+Comm Time: 183.076 ms, Bubble Time: 23.325 ms, Imbalance Overhead: 4.809 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 344.358 ms
GPU 0, Compute+Comm Time: 298.670 ms, Bubble Time: 37.134 ms, Imbalance Overhead: 8.555 ms
GPU 1, Compute+Comm Time: 299.691 ms, Bubble Time: 37.026 ms, Imbalance Overhead: 7.642 ms
    The estimated cost with 4 DP ways is 583.346 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2602.203 ms
Partition 0 [0, 32) has cost: 2602.203 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 491.865 ms
GPU 0, Compute+Comm Time: 491.865 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 612.713 ms
GPU 0, Compute+Comm Time: 612.713 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1159.807 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 121)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 41)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [81, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [121, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 81)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [121, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [41, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[81, 121)...
+++++++++ Node 1 initializing the weights for op[0, 41)...
+++++++++ Node 5 initializing the weights for op[81, 121)...
+++++++++ Node 2 initializing the weights for op[41, 81)...
+++++++++ Node 6 initializing the weights for op[121, 160)...
+++++++++ Node 0 initializing the weights for op[0, 41)...
+++++++++ Node 7 initializing the weights for op[121, 160)...
+++++++++ Node 3 initializing the weights for op[41, 81)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7136	TrainAcc 0.0214	ValidAcc 0.0168	TestAcc 0.0171	BestValid 0.0168
	Epoch 50:	Loss 3.4398	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.3022	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.0584
	Epoch 150:	Loss 3.1263	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 3.0544	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 250:	Loss 2.9918	TrainAcc 0.1790	ValidAcc 0.1760	TestAcc 0.1722	BestValid 0.1760
	Epoch 300:	Loss 2.7597	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1760
	Epoch 350:	Loss 2.5887	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1760
	Epoch 400:	Loss 2.4522	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.1760
	Epoch 450:	Loss 2.3553	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1760
	Epoch 500:	Loss 2.5613	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1760
	Epoch 550:	Loss 2.9252	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1760
	Epoch 600:	Loss 3.0389	TrainAcc 0.2738	ValidAcc 0.2903	TestAcc 0.2863	BestValid 0.2903
	Epoch 650:	Loss 2.9276	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 700:	Loss 2.9017	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 750:	Loss 3.0954	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 800:	Loss 2.9058	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 850:	Loss 2.8653	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 900:	Loss 3.1427	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2903
	Epoch 950:	Loss 3.3465	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2903
	Epoch 1000:	Loss 3.3882	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 1050:	Loss 3.3264	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 1100:	Loss 3.3049	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 1150:	Loss 3.2354	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 1200:	Loss 3.3194	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2903
	Epoch 1250:	Loss 3.3174	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 1300:	Loss 3.2385	TrainAcc 0.0551	ValidAcc 0.0604	TestAcc 0.0577	BestValid 0.2903
	Epoch 1350:	Loss 3.2790	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 1400:	Loss 3.2787	TrainAcc 0.0579	ValidAcc 0.0650	TestAcc 0.0622	BestValid 0.2903
	Epoch 1450:	Loss 3.0371	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 1500:	Loss 3.0181	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 1550:	Loss 3.0067	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 1600:	Loss 3.3117	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 1650:	Loss 3.4068	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 1700:	Loss 3.1462	TrainAcc 0.1021	ValidAcc 0.1189	TestAcc 0.1172	BestValid 0.2903
	Epoch 1750:	Loss 3.0878	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 1800:	Loss 3.2361	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 1850:	Loss 3.1583	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 1900:	Loss 3.0901	TrainAcc 0.1620	ValidAcc 0.1496	TestAcc 0.1510	BestValid 0.2903
	Epoch 1950:	Loss 3.1277	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2903
	Epoch 2000:	Loss 3.1766	TrainAcc 0.0606	ValidAcc 0.0671	TestAcc 0.0674	BestValid 0.2903
	Epoch 2050:	Loss 3.1346	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2100:	Loss 3.0421	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 2150:	Loss 3.0566	TrainAcc 0.1925	ValidAcc 0.1799	TestAcc 0.1798	BestValid 0.2903
	Epoch 2200:	Loss 3.0665	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2250:	Loss 3.0382	TrainAcc 0.1345	ValidAcc 0.1284	TestAcc 0.1238	BestValid 0.2903
	Epoch 2300:	Loss 3.0962	TrainAcc 0.0867	ValidAcc 0.1161	TestAcc 0.1116	BestValid 0.2903
	Epoch 2350:	Loss 3.3958	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2400:	Loss 3.3999	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2450:	Loss 3.3962	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2500:	Loss 3.3956	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2550:	Loss 3.3908	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2600:	Loss 3.4024	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2650:	Loss 3.3819	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2700:	Loss 3.3586	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2750:	Loss 3.2423	TrainAcc 0.1063	ValidAcc 0.1021	TestAcc 0.0979	BestValid 0.2903
	Epoch 2800:	Loss 2.8691	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2903
	Epoch 2850:	Loss 3.1484	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 2900:	Loss 3.3321	TrainAcc 0.1291	ValidAcc 0.1137	TestAcc 0.1158	BestValid 0.2903
	Epoch 2950:	Loss 3.4175	TrainAcc 0.0869	ValidAcc 0.0728	TestAcc 0.0730	BestValid 0.2903
	Epoch 3000:	Loss 3.0792	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 3050:	Loss 3.0722	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 3100:	Loss 3.2427	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 3150:	Loss 3.4168	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 3200:	Loss 3.4425	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2903
	Epoch 3250:	Loss 3.4092	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 3300:	Loss 3.3923	TrainAcc 0.0587	ValidAcc 0.0628	TestAcc 0.0607	BestValid 0.2903
	Epoch 3350:	Loss 3.0667	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 3400:	Loss 3.0038	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 3450:	Loss 3.0128	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 3500:	Loss 3.3274	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 3550:	Loss 3.3545	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 3600:	Loss 3.2843	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.2903
	Epoch 3650:	Loss 3.2384	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2903
	Epoch 3700:	Loss 3.2625	TrainAcc 0.1239	ValidAcc 0.1186	TestAcc 0.1150	BestValid 0.2903
	Epoch 3750:	Loss 3.4213	TrainAcc 0.1221	ValidAcc 0.1170	TestAcc 0.1132	BestValid 0.2903
	Epoch 3800:	Loss 3.1093	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 3850:	Loss 3.0572	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 3900:	Loss 3.0415	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 3950:	Loss 3.1636	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 4000:	Loss 3.2673	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2903
	Epoch 4050:	Loss 3.7306	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 4100:	Loss 3.4208	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 4150:	Loss 3.2863	TrainAcc 0.1185	ValidAcc 0.0997	TestAcc 0.1024	BestValid 0.2903
	Epoch 4200:	Loss 3.2280	TrainAcc 0.1196	ValidAcc 0.1009	TestAcc 0.1036	BestValid 0.2903
	Epoch 4250:	Loss 3.1974	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2903
	Epoch 4300:	Loss 3.1146	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 4350:	Loss 3.2913	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 4400:	Loss 3.2961	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2903
	Epoch 4450:	Loss 3.4121	TrainAcc 0.1191	ValidAcc 0.1000	TestAcc 0.1026	BestValid 0.2903
	Epoch 4500:	Loss 3.3949	TrainAcc 0.1190	ValidAcc 0.1000	TestAcc 0.1026	BestValid 0.2903
	Epoch 4550:	Loss 3.3048	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2903
	Epoch 4600:	Loss 3.2720	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2903
	Epoch 4650:	Loss 3.4004	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 4700:	Loss 3.3999	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 4750:	Loss 3.3111	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2903
	Epoch 4800:	Loss 3.4672	TrainAcc 0.1190	ValidAcc 0.1000	TestAcc 0.1026	BestValid 0.2903
	Epoch 4850:	Loss 3.5947	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 4900:	Loss 3.5302	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
	Epoch 4950:	Loss 3.2989	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2903
	Epoch 5000:	Loss 3.2811	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2903
****** Epoch Time (Excluding Evaluation Cost): 0.392 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 58.342 ms (Max: 60.356, Min: 52.846, Sum: 466.735)
Cluster-Wide Average, Compute: 235.842 ms (Max: 241.138, Min: 228.524, Sum: 1886.736)
Cluster-Wide Average, Communication-Layer: 16.520 ms (Max: 19.757, Min: 13.711, Sum: 132.163)
Cluster-Wide Average, Bubble-Imbalance: 12.248 ms (Max: 15.732, Min: 7.834, Sum: 97.986)
Cluster-Wide Average, Communication-Graph: 62.415 ms (Max: 65.879, Min: 59.549, Sum: 499.318)
Cluster-Wide Average, Optimization: 1.670 ms (Max: 1.815, Min: 1.392, Sum: 13.361)
Cluster-Wide Average, Others: 5.474 ms (Max: 11.561, Min: 3.443, Sum: 43.791)
****** Breakdown Sum: 392.511 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.054 GB (Max: 8.198, Min: 6.696, Sum: 56.435)
Cluster-Wide Average, Graph-Level Communication Throughput: 114.225 Gbps (Max: 127.928, Min: 100.784, Sum: 913.800)
Cluster-Wide Average, Layer-Level Communication Throughput: 33.104 Gbps (Max: 42.686, Min: 23.615, Sum: 264.828)
Layer-level communication (cluster-wide, per-epoch): 0.521 GB
Graph-level communication (cluster-wide, per-epoch): 5.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 5.868 GB
****** Accuracy Results ******
Highest valid_acc: 0.2903
Target test_acc: 0.2863
Epoch to reach the target acc: 599
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
