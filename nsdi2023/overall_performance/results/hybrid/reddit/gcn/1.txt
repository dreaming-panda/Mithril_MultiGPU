Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.832 seconds.
Building the CSC structure...
        It takes 1.890 seconds.
Building the CSC structure...
        It takes 1.912 seconds.
Building the CSC structure...
        It takes 1.928 seconds.
Building the CSC structure...
        It takes 1.989 seconds.
Building the CSC structure...
        It takes 2.054 seconds.
Building the CSC structure...
        It takes 2.064 seconds.
Building the CSC structure...
        It takes 2.364 seconds.
Building the CSC structure...
        It takes 1.851 seconds.
        It takes 1.793 seconds.
        It takes 1.851 seconds.
        It takes 1.839 seconds.
        It takes 1.867 seconds.
        It takes 1.890 seconds.
        It takes 1.944 seconds.
        It takes 2.198 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.305 seconds.
Building the Label Vector...
        It takes 0.044 seconds.
        It takes 0.035 seconds.
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.306 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.040 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.260 seconds.
Building the Label Vector...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.031 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 0.289 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.037 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.079 Gbps (per GPU), 448.630 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.832 Gbps (per GPU), 446.656 Gbps (aggregated)
The layer-level communication performance: 55.825 Gbps (per GPU), 446.600 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.626 Gbps (per GPU), 445.008 Gbps (aggregated)
The layer-level communication performance: 55.589 Gbps (per GPU), 444.712 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.421 Gbps (per GPU), 443.371 Gbps (aggregated)
The layer-level communication performance: 55.387 Gbps (per GPU), 443.097 Gbps (aggregated)
The layer-level communication performance: 55.352 Gbps (per GPU), 442.815 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.175 Gbps (per GPU), 1249.397 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.178 Gbps (per GPU), 1249.420 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.276 Gbps (per GPU), 1250.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.180 Gbps (per GPU), 1249.444 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.163 Gbps (per GPU), 1249.304 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.175 Gbps (per GPU), 1249.397 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.294 Gbps (per GPU), 1250.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.180 Gbps (per GPU), 1249.444 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.282 Gbps (per GPU), 810.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.281 Gbps (per GPU), 810.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.282 Gbps (per GPU), 810.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.280 Gbps (per GPU), 810.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.282 Gbps (per GPU), 810.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.281 Gbps (per GPU), 810.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.282 Gbps (per GPU), 810.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.282 Gbps (per GPU), 810.259 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.582 Gbps (per GPU), 284.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.581 Gbps (per GPU), 284.650 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.582 Gbps (per GPU), 284.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.581 Gbps (per GPU), 284.649 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.580 Gbps (per GPU), 284.641 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.582 Gbps (per GPU), 284.655 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.580 Gbps (per GPU), 284.644 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.582 Gbps (per GPU), 284.652 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.41ms  2.40ms  2.29ms  1.05  8.38K  3.53M
 chk_1  2.77ms  2.61ms  2.54ms  1.09  6.74K  3.60M
 chk_2  2.68ms  2.52ms  2.43ms  1.10  7.27K  3.53M
 chk_3  2.68ms  2.52ms  2.44ms  1.10  7.92K  3.61M
 chk_4  2.59ms  2.50ms  2.44ms  1.06  5.33K  3.68M
 chk_5  2.58ms  2.40ms  2.27ms  1.14 10.07K  3.45M
 chk_6  2.76ms  2.57ms  2.45ms  1.13  9.41K  3.48M
 chk_7  2.62ms  2.46ms  2.37ms  1.10  8.12K  3.60M
 chk_8  2.70ms  2.60ms  2.54ms  1.06  6.09K  3.64M
 chk_9  2.54ms  2.28ms  2.17ms  1.17 11.10K  3.38M
chk_10  2.78ms  2.65ms  2.57ms  1.08  5.67K  3.63M
chk_11  2.62ms  2.46ms  2.36ms  1.11  8.16K  3.54M
chk_12  2.83ms  2.67ms  2.58ms  1.10  7.24K  3.55M
chk_13  2.64ms  2.54ms  2.46ms  1.07  5.41K  3.68M
chk_14  2.90ms  2.75ms  2.93ms  1.07  7.14K  3.53M
chk_15  2.74ms  2.55ms  2.43ms  1.13  9.25K  3.49M
chk_16  2.57ms  2.47ms  2.40ms  1.07  4.78K  3.77M
chk_17  2.71ms  2.57ms  2.47ms  1.10  6.85K  3.60M
chk_18  2.54ms  2.38ms  2.28ms  1.11  7.47K  3.57M
chk_19  2.59ms  2.49ms  2.41ms  1.07  4.88K  3.75M
chk_20  2.61ms  2.45ms  2.36ms  1.11  7.00K  3.63M
chk_21  2.56ms  2.47ms  2.39ms  1.07  5.41K  3.68M
chk_22  2.78ms  2.55ms  2.40ms  1.16 11.07K  3.39M
chk_23  2.69ms  2.53ms  2.46ms  1.09  7.23K  3.64M
chk_24  2.74ms  2.48ms  2.40ms  1.14 10.13K  3.43M
chk_25  2.54ms  2.38ms  2.32ms  1.10  6.40K  3.57M
chk_26  2.75ms  2.63ms  2.55ms  1.08  5.78K  3.55M
chk_27  2.61ms  2.43ms  2.32ms  1.13  9.34K  3.48M
chk_28  2.94ms  2.78ms  2.69ms  1.09  6.37K  3.57M
chk_29  2.74ms  2.61ms  2.55ms  1.08  5.16K  3.78M
chk_30  2.62ms  2.51ms  2.44ms  1.08  5.44K  3.67M
chk_31  2.77ms  2.63ms  2.53ms  1.09  6.33K  3.63M
   Avg  2.68  2.53  2.45
   Max  2.94  2.78  2.93
   Min  2.41  2.28  2.17
 Ratio  1.22  1.22  1.35
   Var  0.01  0.01  0.02
Profiling takes 2.839 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 328.116 ms
Partition 0 [0, 4) has cost: 328.116 ms
Partition 1 [4, 8) has cost: 323.350 ms
Partition 2 [8, 12) has cost: 323.350 ms
Partition 3 [12, 16) has cost: 323.350 ms
Partition 4 [16, 20) has cost: 323.350 ms
Partition 5 [20, 24) has cost: 323.350 ms
Partition 6 [24, 28) has cost: 323.350 ms
Partition 7 [28, 32) has cost: 320.756 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 156.128 ms
GPU 0, Compute+Comm Time: 123.437 ms, Bubble Time: 27.943 ms, Imbalance Overhead: 4.748 ms
GPU 1, Compute+Comm Time: 121.347 ms, Bubble Time: 27.580 ms, Imbalance Overhead: 7.202 ms
GPU 2, Compute+Comm Time: 121.347 ms, Bubble Time: 27.416 ms, Imbalance Overhead: 7.365 ms
GPU 3, Compute+Comm Time: 121.347 ms, Bubble Time: 27.184 ms, Imbalance Overhead: 7.598 ms
GPU 4, Compute+Comm Time: 121.347 ms, Bubble Time: 27.000 ms, Imbalance Overhead: 7.781 ms
GPU 5, Compute+Comm Time: 121.347 ms, Bubble Time: 26.964 ms, Imbalance Overhead: 7.818 ms
GPU 6, Compute+Comm Time: 121.347 ms, Bubble Time: 26.927 ms, Imbalance Overhead: 7.855 ms
GPU 7, Compute+Comm Time: 120.145 ms, Bubble Time: 27.112 ms, Imbalance Overhead: 8.871 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 294.502 ms
GPU 0, Compute+Comm Time: 227.198 ms, Bubble Time: 51.267 ms, Imbalance Overhead: 16.037 ms
GPU 1, Compute+Comm Time: 228.591 ms, Bubble Time: 51.074 ms, Imbalance Overhead: 14.837 ms
GPU 2, Compute+Comm Time: 228.591 ms, Bubble Time: 51.071 ms, Imbalance Overhead: 14.840 ms
GPU 3, Compute+Comm Time: 228.591 ms, Bubble Time: 51.068 ms, Imbalance Overhead: 14.844 ms
GPU 4, Compute+Comm Time: 228.591 ms, Bubble Time: 51.423 ms, Imbalance Overhead: 14.489 ms
GPU 5, Compute+Comm Time: 228.591 ms, Bubble Time: 51.830 ms, Imbalance Overhead: 14.082 ms
GPU 6, Compute+Comm Time: 228.591 ms, Bubble Time: 52.151 ms, Imbalance Overhead: 13.760 ms
GPU 7, Compute+Comm Time: 231.266 ms, Bubble Time: 53.102 ms, Imbalance Overhead: 10.133 ms
The estimated cost of the whole pipeline: 473.161 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 651.466 ms
Partition 0 [0, 8) has cost: 651.466 ms
Partition 1 [8, 16) has cost: 646.700 ms
Partition 2 [16, 24) has cost: 646.700 ms
Partition 3 [24, 32) has cost: 644.106 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.103 ms
GPU 0, Compute+Comm Time: 134.954 ms, Bubble Time: 26.072 ms, Imbalance Overhead: 3.076 ms
GPU 1, Compute+Comm Time: 133.917 ms, Bubble Time: 25.478 ms, Imbalance Overhead: 4.708 ms
GPU 2, Compute+Comm Time: 133.917 ms, Bubble Time: 25.120 ms, Imbalance Overhead: 5.066 ms
GPU 3, Compute+Comm Time: 133.347 ms, Bubble Time: 24.736 ms, Imbalance Overhead: 6.020 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 297.639 ms
GPU 0, Compute+Comm Time: 242.348 ms, Bubble Time: 45.344 ms, Imbalance Overhead: 9.947 ms
GPU 1, Compute+Comm Time: 242.781 ms, Bubble Time: 45.640 ms, Imbalance Overhead: 9.218 ms
GPU 2, Compute+Comm Time: 242.781 ms, Bubble Time: 46.404 ms, Imbalance Overhead: 8.454 ms
GPU 3, Compute+Comm Time: 244.109 ms, Bubble Time: 47.691 ms, Imbalance Overhead: 5.839 ms
    The estimated cost with 2 DP ways is 484.829 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1298.167 ms
Partition 0 [0, 16) has cost: 1298.167 ms
Partition 1 [16, 32) has cost: 1290.806 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 211.961 ms
GPU 0, Compute+Comm Time: 184.293 ms, Bubble Time: 22.849 ms, Imbalance Overhead: 4.819 ms
GPU 1, Compute+Comm Time: 183.498 ms, Bubble Time: 23.336 ms, Imbalance Overhead: 5.127 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 339.063 ms
GPU 0, Compute+Comm Time: 295.935 ms, Bubble Time: 37.077 ms, Imbalance Overhead: 6.051 ms
GPU 1, Compute+Comm Time: 296.669 ms, Bubble Time: 36.744 ms, Imbalance Overhead: 5.649 ms
    The estimated cost with 4 DP ways is 578.575 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2588.972 ms
Partition 0 [0, 32) has cost: 2588.972 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 534.980 ms
GPU 0, Compute+Comm Time: 534.980 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 649.700 ms
GPU 0, Compute+Comm Time: 649.700 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1243.914 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 41)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [81, 121)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 41)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [81, 121)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 81)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [121, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [41, 81)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [121, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 41)...
+++++++++ Node 4 initializing the weights for op[81, 121)...
+++++++++ Node 1 initializing the weights for op[0, 41)...
+++++++++ Node 7 initializing the weights for op[121, 160)...
+++++++++ Node 3 initializing the weights for op[41, 81)...
+++++++++ Node 6 initializing the weights for op[121, 160)...
+++++++++ Node 2 initializing the weights for op[41, 81)...
+++++++++ Node 5 initializing the weights for op[81, 121)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7136	TrainAcc 0.0404	ValidAcc 0.0352	TestAcc 0.0374	BestValid 0.0352
	Epoch 50:	Loss 3.3872	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.2889	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.0657	TrainAcc 0.1237	ValidAcc 0.1184	TestAcc 0.1148	BestValid 0.1184
	Epoch 200:	Loss 2.8974	TrainAcc 0.1151	ValidAcc 0.1146	TestAcc 0.1101	BestValid 0.1184
	Epoch 250:	Loss 2.6373	TrainAcc 0.0555	ValidAcc 0.0606	TestAcc 0.0579	BestValid 0.1184
	Epoch 300:	Loss 2.6162	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1184
	Epoch 350:	Loss 2.5745	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1184
	Epoch 400:	Loss 2.7187	TrainAcc 0.1247	ValidAcc 0.1193	TestAcc 0.1155	BestValid 0.1193
	Epoch 450:	Loss 2.4924	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.1193
	Epoch 500:	Loss 2.7403	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1193
	Epoch 550:	Loss 2.9088	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1193
	Epoch 600:	Loss 3.0549	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1193
	Epoch 650:	Loss 2.9931	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1193
	Epoch 700:	Loss 3.3144	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1193
	Epoch 750:	Loss 3.3709	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1193
	Epoch 800:	Loss 3.3009	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1193
	Epoch 850:	Loss 3.2593	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1193
	Epoch 900:	Loss 3.3210	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1193
	Epoch 950:	Loss 3.3643	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1193
	Epoch 1000:	Loss 3.3215	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1193
	Epoch 1050:	Loss 3.3833	TrainAcc 0.0594	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1193
	Epoch 1100:	Loss 3.4387	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1193
	Epoch 1150:	Loss 3.3714	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1193
	Epoch 1200:	Loss 3.3454	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1193
	Epoch 1250:	Loss 3.3227	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1193
	Epoch 1300:	Loss 3.0442	TrainAcc 0.0595	ValidAcc 0.0616	TestAcc 0.0617	BestValid 0.1193
	Epoch 1350:	Loss 3.1601	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1193
	Epoch 1400:	Loss 2.9680	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1193
	Epoch 1450:	Loss 3.1452	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 1500:	Loss 3.2138	TrainAcc 0.0001	ValidAcc 0.0003	TestAcc 0.0001	BestValid 0.1466
	Epoch 1550:	Loss 3.1332	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1466
	Epoch 1600:	Loss 3.0431	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 1650:	Loss 3.1544	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1466
	Epoch 1700:	Loss 3.2457	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1750:	Loss 3.3422	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1800:	Loss 3.3423	TrainAcc 0.1288	ValidAcc 0.1212	TestAcc 0.1201	BestValid 0.1466
	Epoch 1850:	Loss 3.2442	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 1900:	Loss 3.2205	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 1950:	Loss 3.1659	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2000:	Loss 3.2102	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2050:	Loss 3.2281	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2100:	Loss 3.4248	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2150:	Loss 3.4520	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2200:	Loss 3.3625	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 2250:	Loss 3.1398	TrainAcc 0.1247	ValidAcc 0.1203	TestAcc 0.1165	BestValid 0.1466
	Epoch 2300:	Loss 3.1286	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1466
	Epoch 2350:	Loss 3.0572	TrainAcc 0.2156	ValidAcc 0.2107	TestAcc 0.2087	BestValid 0.2107
	Epoch 2400:	Loss 3.0577	TrainAcc 0.1240	ValidAcc 0.1188	TestAcc 0.1152	BestValid 0.2107
	Epoch 2450:	Loss 3.0749	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2107
	Epoch 2500:	Loss 3.0681	TrainAcc 0.1260	ValidAcc 0.1219	TestAcc 0.1188	BestValid 0.2107
	Epoch 2550:	Loss 3.0558	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2107
	Epoch 2600:	Loss 3.0520	TrainAcc 0.1258	ValidAcc 0.1216	TestAcc 0.1184	BestValid 0.2107
	Epoch 2650:	Loss 3.0522	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2107
	Epoch 2700:	Loss 3.0427	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2107
	Epoch 2750:	Loss 3.0342	TrainAcc 0.1267	ValidAcc 0.1228	TestAcc 0.1199	BestValid 0.2107
	Epoch 2800:	Loss 3.0185	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2107
	Epoch 2850:	Loss 3.0349	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2107
	Epoch 2900:	Loss 3.3053	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2107
	Epoch 2950:	Loss 3.2266	TrainAcc 0.1260	ValidAcc 0.1204	TestAcc 0.1164	BestValid 0.2107
	Epoch 3000:	Loss 3.1995	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2107
	Epoch 3050:	Loss 2.9952	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2107
	Epoch 3100:	Loss 3.0809	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2107
	Epoch 3150:	Loss 3.2130	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2107
	Epoch 3200:	Loss 3.4253	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2107
	Epoch 3250:	Loss 3.2967	TrainAcc 0.0724	ValidAcc 0.0618	TestAcc 0.0644	BestValid 0.2107
	Epoch 3300:	Loss 3.2583	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2107
	Epoch 3350:	Loss 3.1522	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2107
	Epoch 3400:	Loss 3.1260	TrainAcc 0.1264	ValidAcc 0.1229	TestAcc 0.1196	BestValid 0.2107
	Epoch 3450:	Loss 3.1370	TrainAcc 0.2179	ValidAcc 0.2530	TestAcc 0.2498	BestValid 0.2530
	Epoch 3500:	Loss 3.1113	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 3550:	Loss 3.0964	TrainAcc 0.1462	ValidAcc 0.1553	TestAcc 0.1511	BestValid 0.2530
	Epoch 3600:	Loss 3.0772	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 3650:	Loss 3.1129	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 3700:	Loss 3.2416	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 3750:	Loss 3.3531	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 3800:	Loss 3.1361	TrainAcc 0.2169	ValidAcc 0.2496	TestAcc 0.2450	BestValid 0.2530
	Epoch 3850:	Loss 2.9176	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2530
	Epoch 3900:	Loss 2.9594	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 3950:	Loss 3.1596	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4000:	Loss 3.2575	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4050:	Loss 3.4263	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4100:	Loss 3.3426	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4150:	Loss 3.2530	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4200:	Loss 3.2105	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4250:	Loss 3.1858	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2530
	Epoch 4300:	Loss 3.1628	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4350:	Loss 3.2845	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4400:	Loss 3.2938	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2530
	Epoch 4450:	Loss 3.2532	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4500:	Loss 3.2086	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4550:	Loss 3.1616	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4600:	Loss 3.1279	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4650:	Loss 3.2809	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4700:	Loss 3.2647	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4750:	Loss 3.3394	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4800:	Loss 3.2685	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4850:	Loss 3.1474	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4900:	Loss 3.1704	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
	Epoch 4950:	Loss 3.1743	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2530
	Epoch 5000:	Loss 3.3217	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.2530
****** Epoch Time (Excluding Evaluation Cost): 0.393 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 58.430 ms (Max: 60.478, Min: 52.994, Sum: 467.440)
Cluster-Wide Average, Compute: 235.982 ms (Max: 241.511, Min: 228.409, Sum: 1887.852)
Cluster-Wide Average, Communication-Layer: 16.516 ms (Max: 19.768, Min: 13.752, Sum: 132.127)
Cluster-Wide Average, Bubble-Imbalance: 12.276 ms (Max: 15.830, Min: 7.791, Sum: 98.211)
Cluster-Wide Average, Communication-Graph: 62.409 ms (Max: 66.075, Min: 59.700, Sum: 499.276)
Cluster-Wide Average, Optimization: 1.797 ms (Max: 1.990, Min: 1.646, Sum: 14.375)
Cluster-Wide Average, Others: 5.579 ms (Max: 11.755, Min: 3.454, Sum: 44.631)
****** Breakdown Sum: 392.989 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.055 GB (Max: 8.198, Min: 6.696, Sum: 56.438)
Cluster-Wide Average, Graph-Level Communication Throughput: 114.190 Gbps (Max: 127.310, Min: 100.448, Sum: 913.521)
Cluster-Wide Average, Layer-Level Communication Throughput: 33.116 Gbps (Max: 42.798, Min: 23.572, Sum: 264.931)
Layer-level communication (cluster-wide, per-epoch): 0.521 GB
Graph-level communication (cluster-wide, per-epoch): 5.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 5.868 GB
****** Accuracy Results ******
Highest valid_acc: 0.2530
Target test_acc: 0.2498
Epoch to reach the target acc: 3449
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
