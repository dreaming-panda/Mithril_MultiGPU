Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.916 seconds.
Building the CSC structure...
        It takes 2.356 seconds.
Building the CSC structure...
        It takes 2.392 seconds.
Building the CSC structure...
        It takes 2.422 seconds.
Building the CSC structure...
        It takes 2.433 seconds.
Building the CSC structure...
        It takes 2.587 seconds.
Building the CSC structure...
        It takes 2.615 seconds.
Building the CSC structure...
        It takes 2.622 seconds.
Building the CSC structure...
        It takes 1.850 seconds.
        It takes 2.350 seconds.
        It takes 2.331 seconds.
        It takes 2.317 seconds.
Building the Feature Vector...
        It takes 2.326 seconds.
        It takes 2.248 seconds.
        It takes 2.299 seconds.
        It takes 2.303 seconds.
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 0.042 seconds.
        It takes 0.034 seconds.
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.242 seconds.
Building the Label Vector...
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
        It takes 0.035 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 56.696 Gbps (per GPU), 453.571 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.418 Gbps (per GPU), 451.348 Gbps (aggregated)
The layer-level communication performance: 56.413 Gbps (per GPU), 451.306 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.199 Gbps (per GPU), 449.592 Gbps (aggregated)
The layer-level communication performance: 56.174 Gbps (per GPU), 449.392 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.997 Gbps (per GPU), 447.978 Gbps (aggregated)
The layer-level communication performance: 55.950 Gbps (per GPU), 447.599 Gbps (aggregated)
The layer-level communication performance: 55.922 Gbps (per GPU), 447.377 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.023 Gbps (per GPU), 1272.182 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.032 Gbps (per GPU), 1272.254 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.023 Gbps (per GPU), 1272.182 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.029 Gbps (per GPU), 1272.230 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.020 Gbps (per GPU), 1272.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.032 Gbps (per GPU), 1272.254 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.020 Gbps (per GPU), 1272.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.032 Gbps (per GPU), 1272.254 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.813 Gbps (per GPU), 806.506 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.814 Gbps (per GPU), 806.513 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.812 Gbps (per GPU), 806.499 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.813 Gbps (per GPU), 806.506 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.812 Gbps (per GPU), 806.500 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.812 Gbps (per GPU), 806.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.813 Gbps (per GPU), 806.506 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.814 Gbps (per GPU), 806.513 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.129 Gbps (per GPU), 265.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.129 Gbps (per GPU), 265.033 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.129 Gbps (per GPU), 265.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.129 Gbps (per GPU), 265.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.128 Gbps (per GPU), 265.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.129 Gbps (per GPU), 265.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.129 Gbps (per GPU), 265.031 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.128 Gbps (per GPU), 265.026 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.43ms  2.71ms  3.03  8.38K  3.53M
 chk_1  0.75ms  2.73ms  2.91ms  3.86  6.74K  3.60M
 chk_2  0.79ms  2.62ms  2.76ms  3.48  7.27K  3.53M
 chk_3  0.80ms  2.64ms  2.81ms  3.49  7.92K  3.61M
 chk_4  0.63ms  2.58ms  2.72ms  4.34  5.33K  3.68M
 chk_5  1.01ms  2.57ms  2.76ms  2.75 10.07K  3.45M
 chk_6  0.96ms  2.74ms  2.92ms  3.04  9.41K  3.48M
 chk_7  0.82ms  2.56ms  2.73ms  3.34  8.12K  3.60M
 chk_8  0.68ms  2.69ms  2.84ms  4.17  6.09K  3.64M
 chk_9  1.10ms  2.48ms  2.68ms  2.44 11.10K  3.38M
chk_10  0.65ms  2.73ms  3.03ms  4.65  5.67K  3.63M
chk_11  0.82ms  2.58ms  2.75ms  3.35  8.16K  3.54M
chk_12  0.80ms  2.78ms  2.95ms  3.70  7.24K  3.55M
chk_13  0.63ms  2.61ms  2.77ms  4.37  5.41K  3.68M
chk_14  0.78ms  2.85ms  3.03ms  3.90  7.14K  3.53M
chk_15  0.99ms  2.71ms  2.89ms  2.92  9.25K  3.49M
chk_16  0.59ms  2.53ms  2.68ms  4.52  4.78K  3.77M
chk_17  0.76ms  2.67ms  2.83ms  3.73  6.85K  3.60M
chk_18  0.81ms  2.47ms  2.65ms  3.28  7.47K  3.57M
chk_19  0.60ms  2.56ms  2.69ms  4.47  4.88K  3.75M
chk_20  0.77ms  2.54ms  2.70ms  3.53  7.00K  3.63M
chk_21  0.63ms  2.54ms  2.68ms  4.25  5.41K  3.68M
chk_22  1.09ms  2.75ms  2.96ms  2.71 11.07K  3.39M
chk_23  0.79ms  2.64ms  2.81ms  3.56  7.23K  3.64M
chk_24  1.01ms  2.69ms  2.88ms  2.87 10.13K  3.43M
chk_25  0.72ms  2.53ms  2.66ms  3.67  6.40K  3.57M
chk_26  0.66ms  2.71ms  2.86ms  4.34  5.78K  3.55M
chk_27  0.95ms  2.61ms  2.78ms  2.92  9.34K  3.48M
chk_28  0.72ms  2.89ms  3.08ms  4.26  6.37K  3.57M
chk_29  0.63ms  2.72ms  2.85ms  4.55  5.16K  3.78M
chk_30  0.63ms  2.60ms  2.78ms  4.39  5.44K  3.67M
chk_31  0.72ms  2.73ms  2.89ms  4.00  6.33K  3.63M
   Avg  0.79  2.64  2.81
   Max  1.10  2.89  3.08
   Min  0.59  2.43  2.65
 Ratio  1.85  1.19  1.16
   Var  0.02  0.01  0.01
Profiling takes 2.398 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 363.194 ms
Partition 0 [0, 5) has cost: 363.194 ms
Partition 1 [5, 9) has cost: 338.010 ms
Partition 2 [9, 13) has cost: 338.010 ms
Partition 3 [13, 17) has cost: 338.010 ms
Partition 4 [17, 21) has cost: 338.010 ms
Partition 5 [21, 25) has cost: 338.010 ms
Partition 6 [25, 29) has cost: 338.010 ms
Partition 7 [29, 33) has cost: 343.510 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.165 ms
GPU 0, Compute+Comm Time: 132.304 ms, Bubble Time: 28.851 ms, Imbalance Overhead: 3.010 ms
GPU 1, Compute+Comm Time: 125.260 ms, Bubble Time: 28.583 ms, Imbalance Overhead: 10.322 ms
GPU 2, Compute+Comm Time: 125.260 ms, Bubble Time: 28.553 ms, Imbalance Overhead: 10.352 ms
GPU 3, Compute+Comm Time: 125.260 ms, Bubble Time: 28.468 ms, Imbalance Overhead: 10.438 ms
GPU 4, Compute+Comm Time: 125.260 ms, Bubble Time: 28.369 ms, Imbalance Overhead: 10.537 ms
GPU 5, Compute+Comm Time: 125.260 ms, Bubble Time: 28.337 ms, Imbalance Overhead: 10.568 ms
GPU 6, Compute+Comm Time: 125.260 ms, Bubble Time: 28.574 ms, Imbalance Overhead: 10.332 ms
GPU 7, Compute+Comm Time: 126.587 ms, Bubble Time: 28.971 ms, Imbalance Overhead: 8.608 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 316.630 ms
GPU 0, Compute+Comm Time: 243.221 ms, Bubble Time: 56.478 ms, Imbalance Overhead: 16.931 ms
GPU 1, Compute+Comm Time: 239.048 ms, Bubble Time: 55.687 ms, Imbalance Overhead: 21.895 ms
GPU 2, Compute+Comm Time: 239.048 ms, Bubble Time: 55.137 ms, Imbalance Overhead: 22.445 ms
GPU 3, Compute+Comm Time: 239.048 ms, Bubble Time: 55.083 ms, Imbalance Overhead: 22.499 ms
GPU 4, Compute+Comm Time: 239.048 ms, Bubble Time: 55.200 ms, Imbalance Overhead: 22.382 ms
GPU 5, Compute+Comm Time: 239.048 ms, Bubble Time: 55.316 ms, Imbalance Overhead: 22.265 ms
GPU 6, Compute+Comm Time: 239.048 ms, Bubble Time: 55.159 ms, Imbalance Overhead: 22.423 ms
GPU 7, Compute+Comm Time: 257.188 ms, Bubble Time: 55.623 ms, Imbalance Overhead: 3.819 ms
The estimated cost of the whole pipeline: 504.834 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 701.205 ms
Partition 0 [0, 9) has cost: 701.205 ms
Partition 1 [9, 17) has cost: 676.020 ms
Partition 2 [17, 25) has cost: 676.020 ms
Partition 3 [25, 33) has cost: 681.520 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.190 ms
GPU 0, Compute+Comm Time: 141.297 ms, Bubble Time: 26.702 ms, Imbalance Overhead: 2.191 ms
GPU 1, Compute+Comm Time: 137.470 ms, Bubble Time: 26.232 ms, Imbalance Overhead: 6.487 ms
GPU 2, Compute+Comm Time: 137.470 ms, Bubble Time: 26.023 ms, Imbalance Overhead: 6.697 ms
GPU 3, Compute+Comm Time: 138.179 ms, Bubble Time: 25.723 ms, Imbalance Overhead: 6.288 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 315.223 ms
GPU 0, Compute+Comm Time: 254.935 ms, Bubble Time: 48.092 ms, Imbalance Overhead: 12.196 ms
GPU 1, Compute+Comm Time: 252.813 ms, Bubble Time: 48.440 ms, Imbalance Overhead: 13.970 ms
GPU 2, Compute+Comm Time: 252.813 ms, Bubble Time: 48.582 ms, Imbalance Overhead: 13.829 ms
GPU 3, Compute+Comm Time: 262.849 ms, Bubble Time: 49.576 ms, Imbalance Overhead: 2.798 ms
    The estimated cost with 2 DP ways is 509.684 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1377.225 ms
Partition 0 [0, 17) has cost: 1377.225 ms
Partition 1 [17, 33) has cost: 1357.540 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 217.824 ms
GPU 0, Compute+Comm Time: 189.735 ms, Bubble Time: 23.349 ms, Imbalance Overhead: 4.739 ms
GPU 1, Compute+Comm Time: 188.051 ms, Bubble Time: 24.021 ms, Imbalance Overhead: 5.751 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 354.876 ms
GPU 0, Compute+Comm Time: 307.038 ms, Bubble Time: 38.970 ms, Imbalance Overhead: 8.868 ms
GPU 1, Compute+Comm Time: 311.630 ms, Bubble Time: 38.137 ms, Imbalance Overhead: 5.110 ms
    The estimated cost with 4 DP ways is 601.334 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2734.765 ms
Partition 0 [0, 33) has cost: 2734.765 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 571.279 ms
GPU 0, Compute+Comm Time: 571.279 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 694.220 ms
GPU 0, Compute+Comm Time: 694.220 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1328.774 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7843	TrainAcc 0.0464	ValidAcc 0.0436	TestAcc 0.0433	BestValid 0.0436
	Epoch 50:	Loss 1.8561	TrainAcc 0.6972	ValidAcc 0.7132	TestAcc 0.7095	BestValid 0.7132
	Epoch 100:	Loss 1.2657	TrainAcc 0.6582	ValidAcc 0.6628	TestAcc 0.6553	BestValid 0.7132
	Epoch 150:	Loss 1.0205	TrainAcc 0.6918	ValidAcc 0.6936	TestAcc 0.6884	BestValid 0.7132
	Epoch 200:	Loss 0.8908	TrainAcc 0.8486	ValidAcc 0.8604	TestAcc 0.8536	BestValid 0.8604
	Epoch 250:	Loss 0.8038	TrainAcc 0.8721	ValidAcc 0.8794	TestAcc 0.8747	BestValid 0.8794
	Epoch 300:	Loss 0.7553	TrainAcc 0.8649	ValidAcc 0.8734	TestAcc 0.8692	BestValid 0.8794
	Epoch 350:	Loss 0.7131	TrainAcc 0.8633	ValidAcc 0.8715	TestAcc 0.8672	BestValid 0.8794
	Epoch 400:	Loss 0.6743	TrainAcc 0.8772	ValidAcc 0.8829	TestAcc 0.8787	BestValid 0.8829
	Epoch 450:	Loss 0.6436	TrainAcc 0.8949	ValidAcc 0.8993	TestAcc 0.8946	BestValid 0.8993
	Epoch 500:	Loss 0.6237	TrainAcc 0.9047	ValidAcc 0.9076	TestAcc 0.9030	BestValid 0.9076
	Epoch 550:	Loss 0.6066	TrainAcc 0.9074	ValidAcc 0.9099	TestAcc 0.9061	BestValid 0.9099
	Epoch 600:	Loss 0.5869	TrainAcc 0.9134	ValidAcc 0.9166	TestAcc 0.9116	BestValid 0.9166
	Epoch 650:	Loss 0.5741	TrainAcc 0.9163	ValidAcc 0.9188	TestAcc 0.9141	BestValid 0.9188
	Epoch 700:	Loss 0.5644	TrainAcc 0.9189	ValidAcc 0.9214	TestAcc 0.9166	BestValid 0.9214
	Epoch 750:	Loss 0.5491	TrainAcc 0.9213	ValidAcc 0.9237	TestAcc 0.9187	BestValid 0.9237
	Epoch 800:	Loss 0.5438	TrainAcc 0.9236	ValidAcc 0.9253	TestAcc 0.9210	BestValid 0.9253
	Epoch 850:	Loss 0.5327	TrainAcc 0.9238	ValidAcc 0.9255	TestAcc 0.9209	BestValid 0.9255
	Epoch 900:	Loss 0.5238	TrainAcc 0.9249	ValidAcc 0.9259	TestAcc 0.9218	BestValid 0.9259
	Epoch 950:	Loss 0.5170	TrainAcc 0.9273	ValidAcc 0.9284	TestAcc 0.9238	BestValid 0.9284
	Epoch 1000:	Loss 0.5114	TrainAcc 0.9293	ValidAcc 0.9295	TestAcc 0.9252	BestValid 0.9295
	Epoch 1050:	Loss 0.5050	TrainAcc 0.9309	ValidAcc 0.9308	TestAcc 0.9268	BestValid 0.9308
	Epoch 1100:	Loss 0.4985	TrainAcc 0.9303	ValidAcc 0.9295	TestAcc 0.9262	BestValid 0.9308
	Epoch 1150:	Loss 0.4860	TrainAcc 0.9299	ValidAcc 0.9287	TestAcc 0.9259	BestValid 0.9308
	Epoch 1200:	Loss 0.4817	TrainAcc 0.9318	ValidAcc 0.9303	TestAcc 0.9273	BestValid 0.9308
	Epoch 1250:	Loss 0.4786	TrainAcc 0.9332	ValidAcc 0.9321	TestAcc 0.9281	BestValid 0.9321
	Epoch 1300:	Loss 0.4680	TrainAcc 0.9353	ValidAcc 0.9334	TestAcc 0.9307	BestValid 0.9334
	Epoch 1350:	Loss 0.4660	TrainAcc 0.9331	ValidAcc 0.9310	TestAcc 0.9283	BestValid 0.9334
	Epoch 1400:	Loss 0.4628	TrainAcc 0.9361	ValidAcc 0.9338	TestAcc 0.9315	BestValid 0.9338
	Epoch 1450:	Loss 0.4601	TrainAcc 0.9338	ValidAcc 0.9323	TestAcc 0.9283	BestValid 0.9338
	Epoch 1500:	Loss 0.4514	TrainAcc 0.9369	ValidAcc 0.9346	TestAcc 0.9313	BestValid 0.9346
	Epoch 1550:	Loss 0.4513	TrainAcc 0.9354	ValidAcc 0.9329	TestAcc 0.9301	BestValid 0.9346
	Epoch 1600:	Loss 0.4474	TrainAcc 0.9346	ValidAcc 0.9316	TestAcc 0.9289	BestValid 0.9346
	Epoch 1650:	Loss 0.4433	TrainAcc 0.9374	ValidAcc 0.9363	TestAcc 0.9328	BestValid 0.9363
	Epoch 1700:	Loss 0.4389	TrainAcc 0.9376	ValidAcc 0.9347	TestAcc 0.9313	BestValid 0.9363
	Epoch 1750:	Loss 0.4347	TrainAcc 0.9337	ValidAcc 0.9314	TestAcc 0.9277	BestValid 0.9363
	Epoch 1800:	Loss 0.4323	TrainAcc 0.9315	ValidAcc 0.9281	TestAcc 0.9252	BestValid 0.9363
	Epoch 1850:	Loss 0.4298	TrainAcc 0.9389	ValidAcc 0.9370	TestAcc 0.9329	BestValid 0.9370
	Epoch 1900:	Loss 0.4293	TrainAcc 0.9376	ValidAcc 0.9354	TestAcc 0.9316	BestValid 0.9370
	Epoch 1950:	Loss 0.4198	TrainAcc 0.9361	ValidAcc 0.9327	TestAcc 0.9298	BestValid 0.9370
	Epoch 2000:	Loss 0.4186	TrainAcc 0.9320	ValidAcc 0.9283	TestAcc 0.9245	BestValid 0.9370
	Epoch 2050:	Loss 0.4181	TrainAcc 0.9335	ValidAcc 0.9298	TestAcc 0.9267	BestValid 0.9370
	Epoch 2100:	Loss 0.4134	TrainAcc 0.9376	ValidAcc 0.9339	TestAcc 0.9306	BestValid 0.9370
	Epoch 2150:	Loss 0.4126	TrainAcc 0.9374	ValidAcc 0.9339	TestAcc 0.9309	BestValid 0.9370
	Epoch 2200:	Loss 0.4102	TrainAcc 0.9364	ValidAcc 0.9331	TestAcc 0.9287	BestValid 0.9370
	Epoch 2250:	Loss 0.4094	TrainAcc 0.9331	ValidAcc 0.9292	TestAcc 0.9261	BestValid 0.9370
	Epoch 2300:	Loss 0.4061	TrainAcc 0.9261	ValidAcc 0.9225	TestAcc 0.9185	BestValid 0.9370
	Epoch 2350:	Loss 0.4042	TrainAcc 0.9233	ValidAcc 0.9183	TestAcc 0.9145	BestValid 0.9370
	Epoch 2400:	Loss 0.4040	TrainAcc 0.9373	ValidAcc 0.9335	TestAcc 0.9307	BestValid 0.9370
	Epoch 2450:	Loss 0.3992	TrainAcc 0.9374	ValidAcc 0.9331	TestAcc 0.9292	BestValid 0.9370
	Epoch 2500:	Loss 0.3965	TrainAcc 0.9414	ValidAcc 0.9377	TestAcc 0.9343	BestValid 0.9377
	Epoch 2550:	Loss 0.3961	TrainAcc 0.9312	ValidAcc 0.9267	TestAcc 0.9232	BestValid 0.9377
	Epoch 2600:	Loss 0.3980	TrainAcc 0.9212	ValidAcc 0.9152	TestAcc 0.9114	BestValid 0.9377
	Epoch 2650:	Loss 0.3953	TrainAcc 0.9255	ValidAcc 0.9198	TestAcc 0.9165	BestValid 0.9377
	Epoch 2700:	Loss 0.3944	TrainAcc 0.9457	ValidAcc 0.9421	TestAcc 0.9382	BestValid 0.9421
	Epoch 2750:	Loss 0.3919	TrainAcc 0.9426	ValidAcc 0.9392	TestAcc 0.9356	BestValid 0.9421
	Epoch 2800:	Loss 0.3890	TrainAcc 0.9428	ValidAcc 0.9398	TestAcc 0.9354	BestValid 0.9421
	Epoch 2850:	Loss 0.3858	TrainAcc 0.9187	ValidAcc 0.9118	TestAcc 0.9080	BestValid 0.9421
	Epoch 2900:	Loss 0.3877	TrainAcc 0.9220	ValidAcc 0.9148	TestAcc 0.9112	BestValid 0.9421
	Epoch 2950:	Loss 0.3864	TrainAcc 0.9364	ValidAcc 0.9303	TestAcc 0.9286	BestValid 0.9421
	Epoch 3000:	Loss 0.3880	TrainAcc 0.9467	ValidAcc 0.9431	TestAcc 0.9397	BestValid 0.9431
	Epoch 3050:	Loss 0.3843	TrainAcc 0.9379	ValidAcc 0.9327	TestAcc 0.9295	BestValid 0.9431
	Epoch 3100:	Loss 0.3837	TrainAcc 0.9295	ValidAcc 0.9230	TestAcc 0.9201	BestValid 0.9431
	Epoch 3150:	Loss 0.3786	TrainAcc 0.8985	ValidAcc 0.8882	TestAcc 0.8858	BestValid 0.9431
	Epoch 3200:	Loss 0.3816	TrainAcc 0.9127	ValidAcc 0.9037	TestAcc 0.9007	BestValid 0.9431
	Epoch 3250:	Loss 0.3794	TrainAcc 0.9366	ValidAcc 0.9306	TestAcc 0.9279	BestValid 0.9431
	Epoch 3300:	Loss 0.3777	TrainAcc 0.9490	ValidAcc 0.9433	TestAcc 0.9411	BestValid 0.9433
	Epoch 3350:	Loss 0.3766	TrainAcc 0.9453	ValidAcc 0.9403	TestAcc 0.9368	BestValid 0.9433
	Epoch 3400:	Loss 0.3746	TrainAcc 0.9303	ValidAcc 0.9233	TestAcc 0.9199	BestValid 0.9433
	Epoch 3450:	Loss 0.3755	TrainAcc 0.9169	ValidAcc 0.9094	TestAcc 0.9054	BestValid 0.9433
	Epoch 3500:	Loss 0.3778	TrainAcc 0.9042	ValidAcc 0.8933	TestAcc 0.8898	BestValid 0.9433
	Epoch 3550:	Loss 0.3741	TrainAcc 0.9461	ValidAcc 0.9409	TestAcc 0.9384	BestValid 0.9433
	Epoch 3600:	Loss 0.3714	TrainAcc 0.9451	ValidAcc 0.9398	TestAcc 0.9354	BestValid 0.9433
	Epoch 3650:	Loss 0.3696	TrainAcc 0.9431	ValidAcc 0.9381	TestAcc 0.9348	BestValid 0.9433
	Epoch 3700:	Loss 0.3722	TrainAcc 0.9107	ValidAcc 0.9012	TestAcc 0.8984	BestValid 0.9433
	Epoch 3750:	Loss 0.3684	TrainAcc 0.9055	ValidAcc 0.8943	TestAcc 0.8915	BestValid 0.9433
	Epoch 3800:	Loss 0.3699	TrainAcc 0.9271	ValidAcc 0.9180	TestAcc 0.9153	BestValid 0.9433
	Epoch 3850:	Loss 0.3695	TrainAcc 0.9481	ValidAcc 0.9434	TestAcc 0.9398	BestValid 0.9434
	Epoch 3900:	Loss 0.3668	TrainAcc 0.9504	ValidAcc 0.9455	TestAcc 0.9431	BestValid 0.9455
	Epoch 3950:	Loss 0.3692	TrainAcc 0.9314	ValidAcc 0.9238	TestAcc 0.9210	BestValid 0.9455
	Epoch 4000:	Loss 0.3689	TrainAcc 0.9152	ValidAcc 0.9047	TestAcc 0.9025	BestValid 0.9455
	Epoch 4050:	Loss 0.3712	TrainAcc 0.9073	ValidAcc 0.8966	TestAcc 0.8938	BestValid 0.9455
	Epoch 4100:	Loss 0.3649	TrainAcc 0.9284	ValidAcc 0.9206	TestAcc 0.9175	BestValid 0.9455
	Epoch 4150:	Loss 0.3642	TrainAcc 0.9437	ValidAcc 0.9386	TestAcc 0.9355	BestValid 0.9455
	Epoch 4200:	Loss 0.3659	TrainAcc 0.9433	ValidAcc 0.9375	TestAcc 0.9335	BestValid 0.9455
	Epoch 4250:	Loss 0.3635	TrainAcc 0.9206	ValidAcc 0.9105	TestAcc 0.9079	BestValid 0.9455
	Epoch 4300:	Loss 0.3663	TrainAcc 0.9054	ValidAcc 0.8951	TestAcc 0.8915	BestValid 0.9455
	Epoch 4350:	Loss 0.3651	TrainAcc 0.9231	ValidAcc 0.9137	TestAcc 0.9114	BestValid 0.9455
	Epoch 4400:	Loss 0.3608	TrainAcc 0.9477	ValidAcc 0.9422	TestAcc 0.9389	BestValid 0.9455
	Epoch 4450:	Loss 0.3614	TrainAcc 0.9457	ValidAcc 0.9397	TestAcc 0.9367	BestValid 0.9455
	Epoch 4500:	Loss 0.3580	TrainAcc 0.9347	ValidAcc 0.9278	TestAcc 0.9246	BestValid 0.9455
	Epoch 4550:	Loss 0.3551	TrainAcc 0.9002	ValidAcc 0.8874	TestAcc 0.8843	BestValid 0.9455
	Epoch 4600:	Loss 0.3581	TrainAcc 0.9219	ValidAcc 0.9131	TestAcc 0.9096	BestValid 0.9455
	Epoch 4650:	Loss 0.3599	TrainAcc 0.9303	ValidAcc 0.9230	TestAcc 0.9196	BestValid 0.9455
	Epoch 4700:	Loss 0.3670	TrainAcc 0.9454	ValidAcc 0.9398	TestAcc 0.9367	BestValid 0.9455
	Epoch 4750:	Loss 0.3650	TrainAcc 0.9492	ValidAcc 0.9447	TestAcc 0.9430	BestValid 0.9455
	Epoch 4800:	Loss 0.3601	TrainAcc 0.9340	ValidAcc 0.9261	TestAcc 0.9236	BestValid 0.9455
	Epoch 4850:	Loss 0.3593	TrainAcc 0.9175	ValidAcc 0.9104	TestAcc 0.9064	BestValid 0.9455
	Epoch 4900:	Loss 0.3615	TrainAcc 0.9057	ValidAcc 0.8969	TestAcc 0.8939	BestValid 0.9455
	Epoch 4950:	Loss 0.3562	TrainAcc 0.9374	ValidAcc 0.9302	TestAcc 0.9264	BestValid 0.9455
	Epoch 5000:	Loss 0.3596	TrainAcc 0.9409	ValidAcc 0.9350	TestAcc 0.9307	BestValid 0.9455
****** Epoch Time (Excluding Evaluation Cost): 0.447 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 66.655 ms (Max: 68.647, Min: 62.662, Sum: 533.236)
Cluster-Wide Average, Compute: 261.002 ms (Max: 269.715, Min: 250.165, Sum: 2088.018)
Cluster-Wide Average, Communication-Layer: 32.127 ms (Max: 37.563, Min: 25.317, Sum: 257.014)
Cluster-Wide Average, Bubble-Imbalance: 16.139 ms (Max: 22.372, Min: 10.909, Sum: 129.109)
Cluster-Wide Average, Communication-Graph: 64.456 ms (Max: 70.287, Min: 59.696, Sum: 515.652)
Cluster-Wide Average, Optimization: 1.755 ms (Max: 2.068, Min: 1.479, Sum: 14.039)
Cluster-Wide Average, Others: 5.440 ms (Max: 11.355, Min: 3.431, Sum: 43.523)
****** Breakdown Sum: 447.574 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.587 GB (Max: 9.405, Min: 7.096, Sum: 60.696)
Cluster-Wide Average, Graph-Level Communication Throughput: 110.224 Gbps (Max: 126.839, Min: 93.122, Sum: 881.791)
Cluster-Wide Average, Layer-Level Communication Throughput: 34.159 Gbps (Max: 44.343, Min: 23.857, Sum: 273.271)
Layer-level communication (cluster-wide, per-epoch): 1.041 GB
Graph-level communication (cluster-wide, per-epoch): 5.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 6.388 GB
****** Accuracy Results ******
Highest valid_acc: 0.9455
Target test_acc: 0.9431
Epoch to reach the target acc: 3899
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
