Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.886 seconds.
Building the CSC structure...
        It takes 1.915 seconds.
Building the CSC structure...
        It takes 2.126 seconds.
Building the CSC structure...
        It takes 2.290 seconds.
Building the CSC structure...
        It takes 2.367 seconds.
Building the CSC structure...
        It takes 2.387 seconds.
Building the CSC structure...
        It takes 2.542 seconds.
Building the CSC structure...
        It takes 2.718 seconds.
Building the CSC structure...
        It takes 1.784 seconds.
        It takes 1.845 seconds.
        It takes 1.890 seconds.
        It takes 2.254 seconds.
Building the Feature Vector...
        It takes 2.277 seconds.
        It takes 2.365 seconds.
Building the Feature Vector...
        It takes 0.241 seconds.
Building the Label Vector...
        It takes 2.360 seconds.
        It takes 0.036 seconds.
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 2.315 seconds.
        It takes 0.038 seconds.
Building the Feature Vector...
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.281 seconds.
Building the Label Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.032 seconds.
        It takes 0.295 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.290 seconds.
Building the Label Vector...
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.031 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.940 Gbps (per GPU), 471.517 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.657 Gbps (per GPU), 469.257 Gbps (aggregated)
The layer-level communication performance: 58.665 Gbps (per GPU), 469.322 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.400 Gbps (per GPU), 467.198 Gbps (aggregated)
The layer-level communication performance: 58.432 Gbps (per GPU), 467.459 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.204 Gbps (per GPU), 465.631 Gbps (aggregated)
The layer-level communication performance: 58.167 Gbps (per GPU), 465.333 Gbps (aggregated)
The layer-level communication performance: 58.134 Gbps (per GPU), 465.074 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.999 Gbps (per GPU), 1255.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.005 Gbps (per GPU), 1256.038 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.999 Gbps (per GPU), 1255.991 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.005 Gbps (per GPU), 1256.038 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.999 Gbps (per GPU), 1255.991 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.899 Gbps (per GPU), 1255.192 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.996 Gbps (per GPU), 1255.971 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.893 Gbps (per GPU), 1255.145 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.204 Gbps (per GPU), 801.632 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.203 Gbps (per GPU), 801.625 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.202 Gbps (per GPU), 801.619 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.203 Gbps (per GPU), 801.625 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.202 Gbps (per GPU), 801.619 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.204 Gbps (per GPU), 801.632 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.203 Gbps (per GPU), 801.625 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.204 Gbps (per GPU), 801.632 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.415 Gbps (per GPU), 283.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.415 Gbps (per GPU), 283.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.414 Gbps (per GPU), 283.314 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.413 Gbps (per GPU), 283.304 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.415 Gbps (per GPU), 283.322 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.415 Gbps (per GPU), 283.320 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.415 Gbps (per GPU), 283.318 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.413 Gbps (per GPU), 283.305 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.42ms  2.71ms  3.04  8.38K  3.53M
 chk_1  0.75ms  2.76ms  2.90ms  3.85  6.74K  3.60M
 chk_2  0.80ms  2.65ms  2.80ms  3.51  7.27K  3.53M
 chk_3  0.80ms  2.68ms  2.83ms  3.53  7.92K  3.61M
 chk_4  0.63ms  2.61ms  2.75ms  4.38  5.33K  3.68M
 chk_5  1.00ms  2.61ms  2.78ms  2.77 10.07K  3.45M
 chk_6  0.96ms  2.77ms  2.94ms  3.06  9.41K  3.48M
 chk_7  0.82ms  2.59ms  2.76ms  3.37  8.12K  3.60M
 chk_8  0.68ms  2.70ms  2.86ms  4.23  6.09K  3.64M
 chk_9  1.10ms  2.51ms  2.72ms  2.48 11.10K  3.38M
chk_10  0.65ms  2.73ms  2.89ms  4.44  5.67K  3.63M
chk_11  0.82ms  2.61ms  2.77ms  3.37  8.16K  3.54M
chk_12  0.79ms  2.79ms  2.95ms  3.72  7.24K  3.55M
chk_13  0.63ms  2.63ms  2.77ms  4.37  5.41K  3.68M
chk_14  0.78ms  2.86ms  3.04ms  3.91  7.14K  3.53M
chk_15  0.95ms  2.72ms  2.91ms  3.06  9.25K  3.49M
chk_16  0.59ms  2.55ms  2.70ms  4.56  4.78K  3.77M
chk_17  0.76ms  2.67ms  2.84ms  3.74  6.85K  3.60M
chk_18  0.81ms  2.48ms  2.65ms  3.28  7.47K  3.57M
chk_19  0.61ms  2.54ms  2.69ms  4.44  4.88K  3.75M
chk_20  0.77ms  2.56ms  2.70ms  3.51  7.00K  3.63M
chk_21  0.63ms  2.54ms  2.69ms  4.26  5.41K  3.68M
chk_22  1.10ms  2.75ms  2.96ms  2.70 11.07K  3.39M
chk_23  0.79ms  2.65ms  2.82ms  3.56  7.23K  3.64M
chk_24  1.01ms  2.72ms  2.93ms  2.91 10.13K  3.43M
chk_25  0.72ms  2.53ms  2.70ms  3.72  6.40K  3.57M
chk_26  0.66ms  2.76ms  2.90ms  4.40  5.78K  3.55M
chk_27  0.96ms  2.63ms  2.83ms  2.96  9.34K  3.48M
chk_28  0.75ms  2.94ms  3.09ms  4.14  6.37K  3.57M
chk_29  0.63ms  2.73ms  2.87ms  4.59  5.16K  3.78M
chk_30  0.63ms  2.60ms  2.75ms  4.35  5.44K  3.67M
chk_31  0.72ms  2.73ms  2.91ms  4.03  6.33K  3.63M
   Avg  0.79  2.66  2.83
   Max  1.10  2.94  3.09
   Min  0.59  2.42  2.65
 Ratio  1.85  1.21  1.17
   Var  0.02  0.01  0.01
Profiling takes 2.407 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 365.295 ms
Partition 0 [0, 5) has cost: 365.295 ms
Partition 1 [5, 9) has cost: 340.104 ms
Partition 2 [9, 13) has cost: 340.104 ms
Partition 3 [13, 17) has cost: 340.104 ms
Partition 4 [17, 21) has cost: 340.104 ms
Partition 5 [21, 25) has cost: 340.104 ms
Partition 6 [25, 29) has cost: 340.104 ms
Partition 7 [29, 33) has cost: 345.495 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.534 ms
GPU 0, Compute+Comm Time: 132.578 ms, Bubble Time: 29.020 ms, Imbalance Overhead: 2.936 ms
GPU 1, Compute+Comm Time: 125.539 ms, Bubble Time: 28.675 ms, Imbalance Overhead: 10.320 ms
GPU 2, Compute+Comm Time: 125.539 ms, Bubble Time: 28.650 ms, Imbalance Overhead: 10.345 ms
GPU 3, Compute+Comm Time: 125.539 ms, Bubble Time: 28.540 ms, Imbalance Overhead: 10.455 ms
GPU 4, Compute+Comm Time: 125.539 ms, Bubble Time: 28.426 ms, Imbalance Overhead: 10.569 ms
GPU 5, Compute+Comm Time: 125.539 ms, Bubble Time: 28.477 ms, Imbalance Overhead: 10.518 ms
GPU 6, Compute+Comm Time: 125.539 ms, Bubble Time: 28.760 ms, Imbalance Overhead: 10.235 ms
GPU 7, Compute+Comm Time: 126.672 ms, Bubble Time: 29.174 ms, Imbalance Overhead: 8.688 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 318.034 ms
GPU 0, Compute+Comm Time: 244.120 ms, Bubble Time: 56.803 ms, Imbalance Overhead: 17.111 ms
GPU 1, Compute+Comm Time: 239.861 ms, Bubble Time: 55.968 ms, Imbalance Overhead: 22.205 ms
GPU 2, Compute+Comm Time: 239.861 ms, Bubble Time: 55.328 ms, Imbalance Overhead: 22.845 ms
GPU 3, Compute+Comm Time: 239.861 ms, Bubble Time: 55.221 ms, Imbalance Overhead: 22.952 ms
GPU 4, Compute+Comm Time: 239.861 ms, Bubble Time: 55.344 ms, Imbalance Overhead: 22.830 ms
GPU 5, Compute+Comm Time: 239.861 ms, Bubble Time: 55.488 ms, Imbalance Overhead: 22.685 ms
GPU 6, Compute+Comm Time: 239.861 ms, Bubble Time: 55.378 ms, Imbalance Overhead: 22.795 ms
GPU 7, Compute+Comm Time: 258.013 ms, Bubble Time: 55.997 ms, Imbalance Overhead: 4.024 ms
The estimated cost of the whole pipeline: 506.697 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 705.399 ms
Partition 0 [0, 9) has cost: 705.399 ms
Partition 1 [9, 17) has cost: 680.207 ms
Partition 2 [17, 25) has cost: 680.207 ms
Partition 3 [25, 33) has cost: 685.599 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.936 ms
GPU 0, Compute+Comm Time: 141.764 ms, Bubble Time: 26.630 ms, Imbalance Overhead: 2.542 ms
GPU 1, Compute+Comm Time: 137.957 ms, Bubble Time: 26.216 ms, Imbalance Overhead: 6.762 ms
GPU 2, Compute+Comm Time: 137.957 ms, Bubble Time: 26.151 ms, Imbalance Overhead: 6.827 ms
GPU 3, Compute+Comm Time: 138.514 ms, Bubble Time: 25.955 ms, Imbalance Overhead: 6.467 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 317.197 ms
GPU 0, Compute+Comm Time: 256.522 ms, Bubble Time: 48.452 ms, Imbalance Overhead: 12.223 ms
GPU 1, Compute+Comm Time: 254.399 ms, Bubble Time: 48.646 ms, Imbalance Overhead: 14.151 ms
GPU 2, Compute+Comm Time: 254.399 ms, Bubble Time: 48.695 ms, Imbalance Overhead: 14.103 ms
GPU 3, Compute+Comm Time: 264.427 ms, Bubble Time: 49.593 ms, Imbalance Overhead: 3.177 ms
    The estimated cost with 2 DP ways is 512.539 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1385.606 ms
Partition 0 [0, 17) has cost: 1385.606 ms
Partition 1 [17, 33) has cost: 1365.806 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 218.559 ms
GPU 0, Compute+Comm Time: 190.399 ms, Bubble Time: 23.310 ms, Imbalance Overhead: 4.849 ms
GPU 1, Compute+Comm Time: 188.589 ms, Bubble Time: 23.984 ms, Imbalance Overhead: 5.986 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 356.687 ms
GPU 0, Compute+Comm Time: 308.935 ms, Bubble Time: 39.336 ms, Imbalance Overhead: 8.417 ms
GPU 1, Compute+Comm Time: 313.531 ms, Bubble Time: 38.134 ms, Imbalance Overhead: 5.022 ms
    The estimated cost with 4 DP ways is 604.009 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2751.412 ms
Partition 0 [0, 33) has cost: 2751.412 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 542.660 ms
GPU 0, Compute+Comm Time: 542.660 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 666.654 ms
GPU 0, Compute+Comm Time: 666.654 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1269.780 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7632	TrainAcc 0.0847	ValidAcc 0.0764	TestAcc 0.0765	BestValid 0.0764
	Epoch 50:	Loss 1.8775	TrainAcc 0.6617	ValidAcc 0.6732	TestAcc 0.6682	BestValid 0.6732
	Epoch 100:	Loss 1.2628	TrainAcc 0.6196	ValidAcc 0.6176	TestAcc 0.6109	BestValid 0.6732
	Epoch 150:	Loss 1.0087	TrainAcc 0.6830	ValidAcc 0.6804	TestAcc 0.6762	BestValid 0.6804
	Epoch 200:	Loss 0.8786	TrainAcc 0.8647	ValidAcc 0.8741	TestAcc 0.8644	BestValid 0.8741
	Epoch 250:	Loss 0.7977	TrainAcc 0.8664	ValidAcc 0.8745	TestAcc 0.8689	BestValid 0.8745
	Epoch 300:	Loss 0.7446	TrainAcc 0.8600	ValidAcc 0.8682	TestAcc 0.8638	BestValid 0.8745
	Epoch 350:	Loss 0.7070	TrainAcc 0.8678	ValidAcc 0.8759	TestAcc 0.8710	BestValid 0.8759
	Epoch 400:	Loss 0.6688	TrainAcc 0.8850	ValidAcc 0.8921	TestAcc 0.8859	BestValid 0.8921
	Epoch 450:	Loss 0.6402	TrainAcc 0.8989	ValidAcc 0.9026	TestAcc 0.8979	BestValid 0.9026
	Epoch 500:	Loss 0.6192	TrainAcc 0.9039	ValidAcc 0.9075	TestAcc 0.9024	BestValid 0.9075
	Epoch 550:	Loss 0.6019	TrainAcc 0.9050	ValidAcc 0.9074	TestAcc 0.9036	BestValid 0.9075
	Epoch 600:	Loss 0.5888	TrainAcc 0.9126	ValidAcc 0.9148	TestAcc 0.9105	BestValid 0.9148
	Epoch 650:	Loss 0.5704	TrainAcc 0.9157	ValidAcc 0.9179	TestAcc 0.9140	BestValid 0.9179
	Epoch 700:	Loss 0.5585	TrainAcc 0.9196	ValidAcc 0.9208	TestAcc 0.9173	BestValid 0.9208
	Epoch 750:	Loss 0.5489	TrainAcc 0.9193	ValidAcc 0.9203	TestAcc 0.9170	BestValid 0.9208
	Epoch 800:	Loss 0.5418	TrainAcc 0.9227	ValidAcc 0.9232	TestAcc 0.9199	BestValid 0.9232
	Epoch 850:	Loss 0.5285	TrainAcc 0.9245	ValidAcc 0.9254	TestAcc 0.9214	BestValid 0.9254
	Epoch 900:	Loss 0.5223	TrainAcc 0.9256	ValidAcc 0.9260	TestAcc 0.9227	BestValid 0.9260
	Epoch 950:	Loss 0.5129	TrainAcc 0.9279	ValidAcc 0.9287	TestAcc 0.9245	BestValid 0.9287
	Epoch 1000:	Loss 0.5054	TrainAcc 0.9283	ValidAcc 0.9282	TestAcc 0.9250	BestValid 0.9287
	Epoch 1050:	Loss 0.5034	TrainAcc 0.9298	ValidAcc 0.9299	TestAcc 0.9263	BestValid 0.9299
	Epoch 1100:	Loss 0.4908	TrainAcc 0.9300	ValidAcc 0.9298	TestAcc 0.9259	BestValid 0.9299
	Epoch 1150:	Loss 0.4885	TrainAcc 0.9304	ValidAcc 0.9304	TestAcc 0.9265	BestValid 0.9304
	Epoch 1200:	Loss 0.4843	TrainAcc 0.9328	ValidAcc 0.9313	TestAcc 0.9282	BestValid 0.9313
	Epoch 1250:	Loss 0.4733	TrainAcc 0.9333	ValidAcc 0.9320	TestAcc 0.9289	BestValid 0.9320
	Epoch 1300:	Loss 0.4669	TrainAcc 0.9325	ValidAcc 0.9309	TestAcc 0.9284	BestValid 0.9320
	Epoch 1350:	Loss 0.4627	TrainAcc 0.9341	ValidAcc 0.9324	TestAcc 0.9295	BestValid 0.9324
	Epoch 1400:	Loss 0.4572	TrainAcc 0.9348	ValidAcc 0.9338	TestAcc 0.9307	BestValid 0.9338
	Epoch 1450:	Loss 0.4601	TrainAcc 0.9330	ValidAcc 0.9307	TestAcc 0.9279	BestValid 0.9338
	Epoch 1500:	Loss 0.4498	TrainAcc 0.9329	ValidAcc 0.9306	TestAcc 0.9272	BestValid 0.9338
	Epoch 1550:	Loss 0.4485	TrainAcc 0.9372	ValidAcc 0.9345	TestAcc 0.9315	BestValid 0.9345
	Epoch 1600:	Loss 0.4449	TrainAcc 0.9383	ValidAcc 0.9356	TestAcc 0.9330	BestValid 0.9356
	Epoch 1650:	Loss 0.4408	TrainAcc 0.9365	ValidAcc 0.9336	TestAcc 0.9307	BestValid 0.9356
	Epoch 1700:	Loss 0.4374	TrainAcc 0.9364	ValidAcc 0.9333	TestAcc 0.9308	BestValid 0.9356
	Epoch 1750:	Loss 0.4340	TrainAcc 0.9350	ValidAcc 0.9318	TestAcc 0.9289	BestValid 0.9356
	Epoch 1800:	Loss 0.4301	TrainAcc 0.9377	ValidAcc 0.9341	TestAcc 0.9313	BestValid 0.9356
	Epoch 1850:	Loss 0.4296	TrainAcc 0.9372	ValidAcc 0.9332	TestAcc 0.9304	BestValid 0.9356
	Epoch 1900:	Loss 0.4228	TrainAcc 0.9366	ValidAcc 0.9333	TestAcc 0.9303	BestValid 0.9356
	Epoch 1950:	Loss 0.4207	TrainAcc 0.9336	ValidAcc 0.9303	TestAcc 0.9271	BestValid 0.9356
	Epoch 2000:	Loss 0.4191	TrainAcc 0.9315	ValidAcc 0.9275	TestAcc 0.9250	BestValid 0.9356
	Epoch 2050:	Loss 0.4140	TrainAcc 0.9364	ValidAcc 0.9326	TestAcc 0.9297	BestValid 0.9356
	Epoch 2100:	Loss 0.4144	TrainAcc 0.9367	ValidAcc 0.9329	TestAcc 0.9296	BestValid 0.9356
	Epoch 2150:	Loss 0.4078	TrainAcc 0.9283	ValidAcc 0.9235	TestAcc 0.9202	BestValid 0.9356
	Epoch 2200:	Loss 0.4079	TrainAcc 0.9400	ValidAcc 0.9360	TestAcc 0.9327	BestValid 0.9360
	Epoch 2250:	Loss 0.4074	TrainAcc 0.9381	ValidAcc 0.9337	TestAcc 0.9308	BestValid 0.9360
	Epoch 2300:	Loss 0.4062	TrainAcc 0.9214	ValidAcc 0.9163	TestAcc 0.9123	BestValid 0.9360
	Epoch 2350:	Loss 0.4036	TrainAcc 0.9281	ValidAcc 0.9239	TestAcc 0.9203	BestValid 0.9360
	Epoch 2400:	Loss 0.4000	TrainAcc 0.9367	ValidAcc 0.9317	TestAcc 0.9291	BestValid 0.9360
	Epoch 2450:	Loss 0.3991	TrainAcc 0.9438	ValidAcc 0.9394	TestAcc 0.9365	BestValid 0.9394
	Epoch 2500:	Loss 0.3987	TrainAcc 0.9279	ValidAcc 0.9227	TestAcc 0.9194	BestValid 0.9394
	Epoch 2550:	Loss 0.3943	TrainAcc 0.9158	ValidAcc 0.9092	TestAcc 0.9051	BestValid 0.9394
	Epoch 2600:	Loss 0.3955	TrainAcc 0.9141	ValidAcc 0.9084	TestAcc 0.9034	BestValid 0.9394
	Epoch 2650:	Loss 0.3939	TrainAcc 0.9263	ValidAcc 0.9208	TestAcc 0.9168	BestValid 0.9394
	Epoch 2700:	Loss 0.3922	TrainAcc 0.9399	ValidAcc 0.9345	TestAcc 0.9313	BestValid 0.9394
	Epoch 2750:	Loss 0.3919	TrainAcc 0.9443	ValidAcc 0.9395	TestAcc 0.9373	BestValid 0.9395
	Epoch 2800:	Loss 0.3885	TrainAcc 0.9287	ValidAcc 0.9232	TestAcc 0.9191	BestValid 0.9395
	Epoch 2850:	Loss 0.3866	TrainAcc 0.9095	ValidAcc 0.9018	TestAcc 0.8979	BestValid 0.9395
	Epoch 2900:	Loss 0.3871	TrainAcc 0.8937	ValidAcc 0.8870	TestAcc 0.8825	BestValid 0.9395
	Epoch 2950:	Loss 0.3834	TrainAcc 0.9358	ValidAcc 0.9307	TestAcc 0.9271	BestValid 0.9395
	Epoch 3000:	Loss 0.3827	TrainAcc 0.9470	ValidAcc 0.9420	TestAcc 0.9394	BestValid 0.9420
	Epoch 3050:	Loss 0.3821	TrainAcc 0.9376	ValidAcc 0.9319	TestAcc 0.9293	BestValid 0.9420
	Epoch 3100:	Loss 0.3812	TrainAcc 0.9400	ValidAcc 0.9350	TestAcc 0.9313	BestValid 0.9420
	Epoch 3150:	Loss 0.3767	TrainAcc 0.9089	ValidAcc 0.8997	TestAcc 0.8967	BestValid 0.9420
	Epoch 3200:	Loss 0.3816	TrainAcc 0.9188	ValidAcc 0.9115	TestAcc 0.9071	BestValid 0.9420
	Epoch 3250:	Loss 0.3801	TrainAcc 0.9338	ValidAcc 0.9288	TestAcc 0.9243	BestValid 0.9420
	Epoch 3300:	Loss 0.3754	TrainAcc 0.9436	ValidAcc 0.9381	TestAcc 0.9351	BestValid 0.9420
	Epoch 3350:	Loss 0.3763	TrainAcc 0.9362	ValidAcc 0.9311	TestAcc 0.9273	BestValid 0.9420
	Epoch 3400:	Loss 0.3736	TrainAcc 0.9021	ValidAcc 0.8945	TestAcc 0.8897	BestValid 0.9420
	Epoch 3450:	Loss 0.3766	TrainAcc 0.9319	ValidAcc 0.9262	TestAcc 0.9221	BestValid 0.9420
	Epoch 3500:	Loss 0.3734	TrainAcc 0.9342	ValidAcc 0.9290	TestAcc 0.9250	BestValid 0.9420
	Epoch 3550:	Loss 0.3747	TrainAcc 0.9299	ValidAcc 0.9225	TestAcc 0.9190	BestValid 0.9420
	Epoch 3600:	Loss 0.3748	TrainAcc 0.9504	ValidAcc 0.9457	TestAcc 0.9431	BestValid 0.9457
	Epoch 3650:	Loss 0.3723	TrainAcc 0.9387	ValidAcc 0.9330	TestAcc 0.9302	BestValid 0.9457
	Epoch 3700:	Loss 0.3713	TrainAcc 0.9276	ValidAcc 0.9225	TestAcc 0.9175	BestValid 0.9457
	Epoch 3750:	Loss 0.3650	TrainAcc 0.9259	ValidAcc 0.9193	TestAcc 0.9153	BestValid 0.9457
	Epoch 3800:	Loss 0.3715	TrainAcc 0.9339	ValidAcc 0.9268	TestAcc 0.9239	BestValid 0.9457
	Epoch 3850:	Loss 0.3636	TrainAcc 0.9336	ValidAcc 0.9279	TestAcc 0.9243	BestValid 0.9457
	Epoch 3900:	Loss 0.3685	TrainAcc 0.9437	ValidAcc 0.9383	TestAcc 0.9349	BestValid 0.9457
	Epoch 3950:	Loss 0.3652	TrainAcc 0.9495	ValidAcc 0.9442	TestAcc 0.9412	BestValid 0.9457
	Epoch 4000:	Loss 0.3613	TrainAcc 0.9355	ValidAcc 0.9287	TestAcc 0.9256	BestValid 0.9457
	Epoch 4050:	Loss 0.3618	TrainAcc 0.9415	ValidAcc 0.9355	TestAcc 0.9327	BestValid 0.9457
	Epoch 4100:	Loss 0.3607	TrainAcc 0.9473	ValidAcc 0.9413	TestAcc 0.9386	BestValid 0.9457
	Epoch 4150:	Loss 0.3568	TrainAcc 0.9509	ValidAcc 0.9453	TestAcc 0.9431	BestValid 0.9457
	Epoch 4200:	Loss 0.3600	TrainAcc 0.9467	ValidAcc 0.9398	TestAcc 0.9378	BestValid 0.9457
	Epoch 4250:	Loss 0.3580	TrainAcc 0.9439	ValidAcc 0.9380	TestAcc 0.9347	BestValid 0.9457
	Epoch 4300:	Loss 0.3566	TrainAcc 0.9402	ValidAcc 0.9335	TestAcc 0.9293	BestValid 0.9457
	Epoch 4350:	Loss 0.3579	TrainAcc 0.9422	ValidAcc 0.9356	TestAcc 0.9325	BestValid 0.9457
	Epoch 4400:	Loss 0.3554	TrainAcc 0.9422	ValidAcc 0.9353	TestAcc 0.9326	BestValid 0.9457
	Epoch 4450:	Loss 0.3565	TrainAcc 0.9513	ValidAcc 0.9452	TestAcc 0.9436	BestValid 0.9457
	Epoch 4500:	Loss 0.3487	TrainAcc 0.9510	ValidAcc 0.9455	TestAcc 0.9438	BestValid 0.9457
	Epoch 4550:	Loss 0.3496	TrainAcc 0.9437	ValidAcc 0.9375	TestAcc 0.9345	BestValid 0.9457
	Epoch 4600:	Loss 0.3487	TrainAcc 0.9402	ValidAcc 0.9341	TestAcc 0.9304	BestValid 0.9457
	Epoch 4650:	Loss 0.3507	TrainAcc 0.9487	ValidAcc 0.9423	TestAcc 0.9398	BestValid 0.9457
	Epoch 4700:	Loss 0.3525	TrainAcc 0.9498	ValidAcc 0.9444	TestAcc 0.9418	BestValid 0.9457
	Epoch 4750:	Loss 0.3440	TrainAcc 0.9378	ValidAcc 0.9315	TestAcc 0.9278	BestValid 0.9457
	Epoch 4800:	Loss 0.3468	TrainAcc 0.9494	ValidAcc 0.9441	TestAcc 0.9419	BestValid 0.9457
	Epoch 4850:	Loss 0.3442	TrainAcc 0.9465	ValidAcc 0.9400	TestAcc 0.9380	BestValid 0.9457
	Epoch 4900:	Loss 0.3473	TrainAcc 0.9496	ValidAcc 0.9425	TestAcc 0.9406	BestValid 0.9457
	Epoch 4950:	Loss 0.3464	TrainAcc 0.9533	ValidAcc 0.9469	TestAcc 0.9454	BestValid 0.9469
	Epoch 5000:	Loss 0.3477	TrainAcc 0.9523	ValidAcc 0.9458	TestAcc 0.9436	BestValid 0.9469
****** Epoch Time (Excluding Evaluation Cost): 0.450 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 67.241 ms (Max: 69.717, Min: 62.641, Sum: 537.925)
Cluster-Wide Average, Compute: 261.883 ms (Max: 272.099, Min: 249.529, Sum: 2095.066)
Cluster-Wide Average, Communication-Layer: 32.105 ms (Max: 37.816, Min: 25.114, Sum: 256.841)
Cluster-Wide Average, Bubble-Imbalance: 16.829 ms (Max: 25.210, Min: 10.501, Sum: 134.636)
Cluster-Wide Average, Communication-Graph: 64.757 ms (Max: 69.953, Min: 58.906, Sum: 518.052)
Cluster-Wide Average, Optimization: 2.222 ms (Max: 2.987, Min: 1.666, Sum: 17.779)
Cluster-Wide Average, Others: 5.757 ms (Max: 12.022, Min: 3.435, Sum: 46.057)
****** Breakdown Sum: 450.795 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.588 GB (Max: 9.405, Min: 7.096, Sum: 60.700)
Cluster-Wide Average, Graph-Level Communication Throughput: 109.846 Gbps (Max: 128.931, Min: 93.102, Sum: 878.769)
Cluster-Wide Average, Layer-Level Communication Throughput: 34.179 Gbps (Max: 44.368, Min: 23.888, Sum: 273.435)
Layer-level communication (cluster-wide, per-epoch): 1.041 GB
Graph-level communication (cluster-wide, per-epoch): 5.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 6.388 GB
****** Accuracy Results ******
Highest valid_acc: 0.9469
Target test_acc: 0.9454
Epoch to reach the target acc: 4949
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
