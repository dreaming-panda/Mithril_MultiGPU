Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 1 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 2 on machine gnerv2
Initialized node 7 on machine gnerv3
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.911 seconds.
Building the CSC structure...
        It takes 2.053 seconds.
Building the CSC structure...
        It takes 2.299 seconds.
Building the CSC structure...
        It takes 2.375 seconds.
Building the CSC structure...
        It takes 2.382 seconds.
Building the CSC structure...
        It takes 2.420 seconds.
Building the CSC structure...
        It takes 2.424 seconds.
Building the CSC structure...
        It takes 2.573 seconds.
Building the CSC structure...
        It takes 1.851 seconds.
        It takes 1.853 seconds.
        It takes 2.246 seconds.
        It takes 2.229 seconds.
Building the Feature Vector...
        It takes 2.366 seconds.
        It takes 2.329 seconds.
        It takes 2.356 seconds.
        It takes 2.371 seconds.
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.290 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.042 seconds.
        It takes 0.032 seconds.
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.270 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.030 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.206 Gbps (per GPU), 449.649 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.932 Gbps (per GPU), 447.454 Gbps (aggregated)
The layer-level communication performance: 55.937 Gbps (per GPU), 447.493 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.712 Gbps (per GPU), 445.700 Gbps (aggregated)
The layer-level communication performance: 55.678 Gbps (per GPU), 445.422 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.475 Gbps (per GPU), 443.803 Gbps (aggregated)
The layer-level communication performance: 55.438 Gbps (per GPU), 443.502 Gbps (aggregated)
The layer-level communication performance: 55.406 Gbps (per GPU), 443.249 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.287 Gbps (per GPU), 1258.299 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.287 Gbps (per GPU), 1258.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.276 Gbps (per GPU), 1258.205 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.231 Gbps (per GPU), 1257.851 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.288 Gbps (per GPU), 1258.303 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.284 Gbps (per GPU), 1258.274 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.281 Gbps (per GPU), 1258.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.235 Gbps (per GPU), 1257.878 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.381 Gbps (per GPU), 795.047 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.380 Gbps (per GPU), 795.041 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.380 Gbps (per GPU), 795.041 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.379 Gbps (per GPU), 795.028 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.378 Gbps (per GPU), 795.022 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.381 Gbps (per GPU), 795.046 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.380 Gbps (per GPU), 795.041 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.380 Gbps (per GPU), 795.041 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.932 Gbps (per GPU), 271.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.934 Gbps (per GPU), 271.470 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.934 Gbps (per GPU), 271.468 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.932 Gbps (per GPU), 271.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.933 Gbps (per GPU), 271.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.934 Gbps (per GPU), 271.470 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.920 Gbps (per GPU), 271.357 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.932 Gbps (per GPU), 271.454 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.43ms  2.71ms  3.03  8.38K  3.53M
 chk_1  0.76ms  2.73ms  2.90ms  3.83  6.74K  3.60M
 chk_2  0.80ms  2.63ms  2.80ms  3.51  7.27K  3.53M
 chk_3  0.81ms  2.65ms  2.83ms  3.51  7.92K  3.61M
 chk_4  0.63ms  2.58ms  2.74ms  4.34  5.33K  3.68M
 chk_5  1.01ms  2.60ms  2.78ms  2.76 10.07K  3.45M
 chk_6  0.96ms  2.74ms  2.93ms  3.05  9.41K  3.48M
 chk_7  0.82ms  2.59ms  2.75ms  3.35  8.12K  3.60M
 chk_8  0.68ms  2.69ms  2.85ms  4.19  6.09K  3.64M
 chk_9  1.10ms  2.51ms  2.71ms  2.46 11.10K  3.38M
chk_10  0.65ms  2.73ms  2.89ms  4.42  5.67K  3.63M
chk_11  0.82ms  2.59ms  2.77ms  3.36  8.16K  3.54M
chk_12  0.80ms  2.79ms  2.96ms  3.72  7.24K  3.55M
chk_13  0.64ms  2.63ms  2.79ms  4.38  5.41K  3.68M
chk_14  0.78ms  2.87ms  3.04ms  3.90  7.14K  3.53M
chk_15  0.95ms  2.72ms  2.91ms  3.06  9.25K  3.49M
chk_16  0.60ms  2.56ms  2.70ms  4.51  4.78K  3.77M
chk_17  0.76ms  2.67ms  2.84ms  3.72  6.85K  3.60M
chk_18  0.81ms  2.49ms  2.66ms  3.28  7.47K  3.57M
chk_19  0.61ms  2.55ms  2.71ms  4.45  4.88K  3.75M
chk_20  0.77ms  2.57ms  2.72ms  3.52  7.00K  3.63M
chk_21  0.64ms  2.55ms  2.71ms  4.26  5.41K  3.68M
chk_22  1.10ms  2.78ms  2.97ms  2.70 11.07K  3.39M
chk_23  0.80ms  2.65ms  2.82ms  3.54  7.23K  3.64M
chk_24  1.01ms  2.75ms  2.91ms  2.87 10.13K  3.43M
chk_25  0.73ms  2.55ms  2.68ms  3.66  6.40K  3.57M
chk_26  0.66ms  2.75ms  2.88ms  4.34  5.78K  3.55M
chk_27  0.96ms  2.59ms  2.80ms  2.93  9.34K  3.48M
chk_28  0.73ms  2.91ms  3.07ms  4.22  6.37K  3.57M
chk_29  0.63ms  2.70ms  2.86ms  4.51  5.16K  3.78M
chk_30  0.64ms  2.58ms  2.75ms  4.31  5.44K  3.67M
chk_31  0.73ms  2.72ms  2.91ms  4.00  6.33K  3.63M
   Avg  0.79  2.65  2.82
   Max  1.10  2.91  3.07
   Min  0.60  2.43  2.66
 Ratio  1.84  1.20  1.15
   Var  0.02  0.01  0.01
Profiling takes 2.398 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 364.829 ms
Partition 0 [0, 5) has cost: 364.829 ms
Partition 1 [5, 9) has cost: 339.556 ms
Partition 2 [9, 13) has cost: 339.556 ms
Partition 3 [13, 17) has cost: 339.556 ms
Partition 4 [17, 21) has cost: 339.556 ms
Partition 5 [21, 25) has cost: 339.556 ms
Partition 6 [25, 29) has cost: 339.556 ms
Partition 7 [29, 33) has cost: 345.024 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.033 ms
GPU 0, Compute+Comm Time: 132.968 ms, Bubble Time: 28.940 ms, Imbalance Overhead: 3.124 ms
GPU 1, Compute+Comm Time: 125.898 ms, Bubble Time: 28.657 ms, Imbalance Overhead: 10.478 ms
GPU 2, Compute+Comm Time: 125.898 ms, Bubble Time: 28.569 ms, Imbalance Overhead: 10.566 ms
GPU 3, Compute+Comm Time: 125.898 ms, Bubble Time: 28.469 ms, Imbalance Overhead: 10.666 ms
GPU 4, Compute+Comm Time: 125.898 ms, Bubble Time: 28.366 ms, Imbalance Overhead: 10.769 ms
GPU 5, Compute+Comm Time: 125.898 ms, Bubble Time: 28.345 ms, Imbalance Overhead: 10.790 ms
GPU 6, Compute+Comm Time: 125.898 ms, Bubble Time: 28.643 ms, Imbalance Overhead: 10.492 ms
GPU 7, Compute+Comm Time: 127.070 ms, Bubble Time: 29.040 ms, Imbalance Overhead: 8.923 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 318.247 ms
GPU 0, Compute+Comm Time: 244.481 ms, Bubble Time: 56.708 ms, Imbalance Overhead: 17.058 ms
GPU 1, Compute+Comm Time: 240.185 ms, Bubble Time: 55.901 ms, Imbalance Overhead: 22.161 ms
GPU 2, Compute+Comm Time: 240.185 ms, Bubble Time: 55.270 ms, Imbalance Overhead: 22.792 ms
GPU 3, Compute+Comm Time: 240.185 ms, Bubble Time: 55.140 ms, Imbalance Overhead: 22.921 ms
GPU 4, Compute+Comm Time: 240.185 ms, Bubble Time: 55.265 ms, Imbalance Overhead: 22.796 ms
GPU 5, Compute+Comm Time: 240.185 ms, Bubble Time: 55.414 ms, Imbalance Overhead: 22.647 ms
GPU 6, Compute+Comm Time: 240.185 ms, Bubble Time: 55.253 ms, Imbalance Overhead: 22.809 ms
GPU 7, Compute+Comm Time: 258.388 ms, Bubble Time: 55.785 ms, Imbalance Overhead: 4.074 ms
The estimated cost of the whole pipeline: 507.444 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 704.386 ms
Partition 0 [0, 9) has cost: 704.386 ms
Partition 1 [9, 17) has cost: 679.113 ms
Partition 2 [17, 25) has cost: 679.113 ms
Partition 3 [25, 33) has cost: 684.581 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 171.199 ms
GPU 0, Compute+Comm Time: 141.903 ms, Bubble Time: 26.770 ms, Imbalance Overhead: 2.525 ms
GPU 1, Compute+Comm Time: 138.076 ms, Bubble Time: 26.382 ms, Imbalance Overhead: 6.740 ms
GPU 2, Compute+Comm Time: 138.076 ms, Bubble Time: 26.078 ms, Imbalance Overhead: 7.045 ms
GPU 3, Compute+Comm Time: 138.646 ms, Bubble Time: 25.833 ms, Imbalance Overhead: 6.719 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 317.938 ms
GPU 0, Compute+Comm Time: 256.397 ms, Bubble Time: 48.478 ms, Imbalance Overhead: 13.064 ms
GPU 1, Compute+Comm Time: 254.254 ms, Bubble Time: 48.724 ms, Imbalance Overhead: 14.960 ms
GPU 2, Compute+Comm Time: 254.254 ms, Bubble Time: 48.855 ms, Imbalance Overhead: 14.829 ms
GPU 3, Compute+Comm Time: 264.306 ms, Bubble Time: 49.780 ms, Imbalance Overhead: 3.852 ms
    The estimated cost with 2 DP ways is 513.594 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1383.498 ms
Partition 0 [0, 17) has cost: 1383.498 ms
Partition 1 [17, 33) has cost: 1363.693 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 218.930 ms
GPU 0, Compute+Comm Time: 190.847 ms, Bubble Time: 23.403 ms, Imbalance Overhead: 4.680 ms
GPU 1, Compute+Comm Time: 189.042 ms, Bubble Time: 24.305 ms, Imbalance Overhead: 5.583 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 357.503 ms
GPU 0, Compute+Comm Time: 309.343 ms, Bubble Time: 39.765 ms, Imbalance Overhead: 8.396 ms
GPU 1, Compute+Comm Time: 313.948 ms, Bubble Time: 38.176 ms, Imbalance Overhead: 5.379 ms
    The estimated cost with 4 DP ways is 605.255 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2747.192 ms
Partition 0 [0, 33) has cost: 2747.192 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 560.336 ms
GPU 0, Compute+Comm Time: 560.336 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 684.522 ms
GPU 0, Compute+Comm Time: 684.522 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1307.101 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7957	TrainAcc 0.0082	ValidAcc 0.0077	TestAcc 0.0083	BestValid 0.0077
	Epoch 50:	Loss 1.8639	TrainAcc 0.6506	ValidAcc 0.6696	TestAcc 0.6669	BestValid 0.6696
	Epoch 100:	Loss 1.2566	TrainAcc 0.4054	ValidAcc 0.4144	TestAcc 0.4080	BestValid 0.6696
	Epoch 150:	Loss 1.0040	TrainAcc 0.7131	ValidAcc 0.7113	TestAcc 0.7046	BestValid 0.7113
	Epoch 200:	Loss 0.8774	TrainAcc 0.8672	ValidAcc 0.8715	TestAcc 0.8672	BestValid 0.8715
	Epoch 250:	Loss 0.7994	TrainAcc 0.8619	ValidAcc 0.8678	TestAcc 0.8623	BestValid 0.8715
	Epoch 300:	Loss 0.7435	TrainAcc 0.8504	ValidAcc 0.8563	TestAcc 0.8518	BestValid 0.8715
	Epoch 350:	Loss 0.7025	TrainAcc 0.8564	ValidAcc 0.8623	TestAcc 0.8567	BestValid 0.8715
	Epoch 400:	Loss 0.6676	TrainAcc 0.8745	ValidAcc 0.8781	TestAcc 0.8752	BestValid 0.8781
	Epoch 450:	Loss 0.6436	TrainAcc 0.8923	ValidAcc 0.8957	TestAcc 0.8920	BestValid 0.8957
	Epoch 500:	Loss 0.6215	TrainAcc 0.9008	ValidAcc 0.9027	TestAcc 0.8992	BestValid 0.9027
	Epoch 550:	Loss 0.6037	TrainAcc 0.9029	ValidAcc 0.9050	TestAcc 0.9016	BestValid 0.9050
	Epoch 600:	Loss 0.5883	TrainAcc 0.9113	ValidAcc 0.9126	TestAcc 0.9090	BestValid 0.9126
	Epoch 650:	Loss 0.5699	TrainAcc 0.9162	ValidAcc 0.9165	TestAcc 0.9121	BestValid 0.9165
	Epoch 700:	Loss 0.5652	TrainAcc 0.9189	ValidAcc 0.9194	TestAcc 0.9148	BestValid 0.9194
	Epoch 750:	Loss 0.5531	TrainAcc 0.9213	ValidAcc 0.9209	TestAcc 0.9172	BestValid 0.9209
	Epoch 800:	Loss 0.5417	TrainAcc 0.9214	ValidAcc 0.9209	TestAcc 0.9174	BestValid 0.9209
	Epoch 850:	Loss 0.5308	TrainAcc 0.9235	ValidAcc 0.9223	TestAcc 0.9192	BestValid 0.9223
	Epoch 900:	Loss 0.5253	TrainAcc 0.9255	ValidAcc 0.9242	TestAcc 0.9211	BestValid 0.9242
	Epoch 950:	Loss 0.5149	TrainAcc 0.9275	ValidAcc 0.9253	TestAcc 0.9225	BestValid 0.9253
	Epoch 1000:	Loss 0.5076	TrainAcc 0.9284	ValidAcc 0.9261	TestAcc 0.9234	BestValid 0.9261
	Epoch 1050:	Loss 0.5011	TrainAcc 0.9293	ValidAcc 0.9273	TestAcc 0.9241	BestValid 0.9273
	Epoch 1100:	Loss 0.4969	TrainAcc 0.9307	ValidAcc 0.9284	TestAcc 0.9255	BestValid 0.9284
	Epoch 1150:	Loss 0.4897	TrainAcc 0.9317	ValidAcc 0.9292	TestAcc 0.9261	BestValid 0.9292
	Epoch 1200:	Loss 0.4833	TrainAcc 0.9324	ValidAcc 0.9293	TestAcc 0.9262	BestValid 0.9293
	Epoch 1250:	Loss 0.4799	TrainAcc 0.9326	ValidAcc 0.9293	TestAcc 0.9266	BestValid 0.9293
	Epoch 1300:	Loss 0.4747	TrainAcc 0.9339	ValidAcc 0.9306	TestAcc 0.9280	BestValid 0.9306
	Epoch 1350:	Loss 0.4689	TrainAcc 0.9354	ValidAcc 0.9327	TestAcc 0.9295	BestValid 0.9327
	Epoch 1400:	Loss 0.4624	TrainAcc 0.9354	ValidAcc 0.9316	TestAcc 0.9291	BestValid 0.9327
	Epoch 1450:	Loss 0.4612	TrainAcc 0.9345	ValidAcc 0.9309	TestAcc 0.9280	BestValid 0.9327
	Epoch 1500:	Loss 0.4554	TrainAcc 0.9364	ValidAcc 0.9332	TestAcc 0.9296	BestValid 0.9332
	Epoch 1550:	Loss 0.4488	TrainAcc 0.9374	ValidAcc 0.9344	TestAcc 0.9309	BestValid 0.9344
	Epoch 1600:	Loss 0.4477	TrainAcc 0.9382	ValidAcc 0.9353	TestAcc 0.9326	BestValid 0.9353
	Epoch 1650:	Loss 0.4464	TrainAcc 0.9367	ValidAcc 0.9331	TestAcc 0.9301	BestValid 0.9353
	Epoch 1700:	Loss 0.4399	TrainAcc 0.9365	ValidAcc 0.9332	TestAcc 0.9298	BestValid 0.9353
	Epoch 1750:	Loss 0.4361	TrainAcc 0.9375	ValidAcc 0.9331	TestAcc 0.9308	BestValid 0.9353
	Epoch 1800:	Loss 0.4359	TrainAcc 0.9384	ValidAcc 0.9346	TestAcc 0.9320	BestValid 0.9353
	Epoch 1850:	Loss 0.4323	TrainAcc 0.9392	ValidAcc 0.9353	TestAcc 0.9335	BestValid 0.9353
	Epoch 1900:	Loss 0.4277	TrainAcc 0.9385	ValidAcc 0.9343	TestAcc 0.9311	BestValid 0.9353
	Epoch 1950:	Loss 0.4240	TrainAcc 0.9384	ValidAcc 0.9354	TestAcc 0.9315	BestValid 0.9354
	Epoch 2000:	Loss 0.4234	TrainAcc 0.9386	ValidAcc 0.9342	TestAcc 0.9315	BestValid 0.9354
	Epoch 2050:	Loss 0.4216	TrainAcc 0.9382	ValidAcc 0.9337	TestAcc 0.9313	BestValid 0.9354
	Epoch 2100:	Loss 0.4182	TrainAcc 0.9370	ValidAcc 0.9325	TestAcc 0.9296	BestValid 0.9354
	Epoch 2150:	Loss 0.4180	TrainAcc 0.9418	ValidAcc 0.9382	TestAcc 0.9358	BestValid 0.9382
	Epoch 2200:	Loss 0.4139	TrainAcc 0.9423	ValidAcc 0.9383	TestAcc 0.9360	BestValid 0.9383
	Epoch 2250:	Loss 0.4109	TrainAcc 0.9382	ValidAcc 0.9340	TestAcc 0.9309	BestValid 0.9383
	Epoch 2300:	Loss 0.4073	TrainAcc 0.9301	ValidAcc 0.9250	TestAcc 0.9212	BestValid 0.9383
	Epoch 2350:	Loss 0.4062	TrainAcc 0.9312	ValidAcc 0.9265	TestAcc 0.9226	BestValid 0.9383
	Epoch 2400:	Loss 0.4002	TrainAcc 0.9416	ValidAcc 0.9373	TestAcc 0.9343	BestValid 0.9383
	Epoch 2450:	Loss 0.4034	TrainAcc 0.9457	ValidAcc 0.9414	TestAcc 0.9388	BestValid 0.9414
	Epoch 2500:	Loss 0.3984	TrainAcc 0.9446	ValidAcc 0.9411	TestAcc 0.9382	BestValid 0.9414
	Epoch 2550:	Loss 0.3998	TrainAcc 0.9316	ValidAcc 0.9253	TestAcc 0.9227	BestValid 0.9414
	Epoch 2600:	Loss 0.3960	TrainAcc 0.9272	ValidAcc 0.9204	TestAcc 0.9173	BestValid 0.9414
	Epoch 2650:	Loss 0.3968	TrainAcc 0.9417	ValidAcc 0.9362	TestAcc 0.9339	BestValid 0.9414
	Epoch 2700:	Loss 0.3946	TrainAcc 0.9421	ValidAcc 0.9367	TestAcc 0.9346	BestValid 0.9414
	Epoch 2750:	Loss 0.3938	TrainAcc 0.9446	ValidAcc 0.9395	TestAcc 0.9368	BestValid 0.9414
	Epoch 2800:	Loss 0.3934	TrainAcc 0.9391	ValidAcc 0.9335	TestAcc 0.9311	BestValid 0.9414
	Epoch 2850:	Loss 0.3889	TrainAcc 0.9334	ValidAcc 0.9272	TestAcc 0.9246	BestValid 0.9414
	Epoch 2900:	Loss 0.3821	TrainAcc 0.9343	ValidAcc 0.9277	TestAcc 0.9248	BestValid 0.9414
	Epoch 2950:	Loss 0.3844	TrainAcc 0.9445	ValidAcc 0.9399	TestAcc 0.9367	BestValid 0.9414
	Epoch 3000:	Loss 0.3825	TrainAcc 0.9366	ValidAcc 0.9303	TestAcc 0.9285	BestValid 0.9414
	Epoch 3050:	Loss 0.3843	TrainAcc 0.9413	ValidAcc 0.9360	TestAcc 0.9333	BestValid 0.9414
	Epoch 3100:	Loss 0.3832	TrainAcc 0.9425	ValidAcc 0.9363	TestAcc 0.9339	BestValid 0.9414
	Epoch 3150:	Loss 0.3775	TrainAcc 0.9181	ValidAcc 0.9103	TestAcc 0.9059	BestValid 0.9414
	Epoch 3200:	Loss 0.3815	TrainAcc 0.9392	ValidAcc 0.9327	TestAcc 0.9301	BestValid 0.9414
	Epoch 3250:	Loss 0.3804	TrainAcc 0.9443	ValidAcc 0.9369	TestAcc 0.9356	BestValid 0.9414
	Epoch 3300:	Loss 0.3765	TrainAcc 0.9477	ValidAcc 0.9428	TestAcc 0.9406	BestValid 0.9428
	Epoch 3350:	Loss 0.3777	TrainAcc 0.9468	ValidAcc 0.9422	TestAcc 0.9401	BestValid 0.9428
	Epoch 3400:	Loss 0.3745	TrainAcc 0.9405	ValidAcc 0.9349	TestAcc 0.9326	BestValid 0.9428
	Epoch 3450:	Loss 0.3764	TrainAcc 0.9297	ValidAcc 0.9219	TestAcc 0.9196	BestValid 0.9428
	Epoch 3500:	Loss 0.3729	TrainAcc 0.9447	ValidAcc 0.9383	TestAcc 0.9368	BestValid 0.9428
	Epoch 3550:	Loss 0.3718	TrainAcc 0.9473	ValidAcc 0.9418	TestAcc 0.9395	BestValid 0.9428
	Epoch 3600:	Loss 0.3760	TrainAcc 0.9466	ValidAcc 0.9406	TestAcc 0.9390	BestValid 0.9428
	Epoch 3650:	Loss 0.3678	TrainAcc 0.9481	ValidAcc 0.9427	TestAcc 0.9411	BestValid 0.9428
	Epoch 3700:	Loss 0.3681	TrainAcc 0.9462	ValidAcc 0.9402	TestAcc 0.9384	BestValid 0.9428
	Epoch 3750:	Loss 0.3727	TrainAcc 0.9441	ValidAcc 0.9379	TestAcc 0.9359	BestValid 0.9428
	Epoch 3800:	Loss 0.3655	TrainAcc 0.9466	ValidAcc 0.9407	TestAcc 0.9389	BestValid 0.9428
	Epoch 3850:	Loss 0.3632	TrainAcc 0.9490	ValidAcc 0.9430	TestAcc 0.9416	BestValid 0.9430
	Epoch 3900:	Loss 0.3657	TrainAcc 0.9504	ValidAcc 0.9450	TestAcc 0.9430	BestValid 0.9450
	Epoch 3950:	Loss 0.3618	TrainAcc 0.9441	ValidAcc 0.9376	TestAcc 0.9358	BestValid 0.9450
	Epoch 4000:	Loss 0.3610	TrainAcc 0.9363	ValidAcc 0.9282	TestAcc 0.9272	BestValid 0.9450
	Epoch 4050:	Loss 0.3610	TrainAcc 0.9455	ValidAcc 0.9389	TestAcc 0.9373	BestValid 0.9450
	Epoch 4100:	Loss 0.3623	TrainAcc 0.9465	ValidAcc 0.9394	TestAcc 0.9384	BestValid 0.9450
	Epoch 4150:	Loss 0.3626	TrainAcc 0.9504	ValidAcc 0.9440	TestAcc 0.9430	BestValid 0.9450
	Epoch 4200:	Loss 0.3606	TrainAcc 0.9500	ValidAcc 0.9432	TestAcc 0.9430	BestValid 0.9450
	Epoch 4250:	Loss 0.3578	TrainAcc 0.9467	ValidAcc 0.9397	TestAcc 0.9389	BestValid 0.9450
	Epoch 4300:	Loss 0.3554	TrainAcc 0.9461	ValidAcc 0.9384	TestAcc 0.9374	BestValid 0.9450
	Epoch 4350:	Loss 0.3600	TrainAcc 0.9465	ValidAcc 0.9397	TestAcc 0.9382	BestValid 0.9450
	Epoch 4400:	Loss 0.3557	TrainAcc 0.9474	ValidAcc 0.9406	TestAcc 0.9388	BestValid 0.9450
	Epoch 4450:	Loss 0.3514	TrainAcc 0.9500	ValidAcc 0.9433	TestAcc 0.9423	BestValid 0.9450
	Epoch 4500:	Loss 0.3544	TrainAcc 0.9410	ValidAcc 0.9329	TestAcc 0.9317	BestValid 0.9450
	Epoch 4550:	Loss 0.3534	TrainAcc 0.9476	ValidAcc 0.9397	TestAcc 0.9392	BestValid 0.9450
	Epoch 4600:	Loss 0.3538	TrainAcc 0.9417	ValidAcc 0.9332	TestAcc 0.9322	BestValid 0.9450
	Epoch 4650:	Loss 0.3494	TrainAcc 0.9425	ValidAcc 0.9356	TestAcc 0.9329	BestValid 0.9450
	Epoch 4700:	Loss 0.3495	TrainAcc 0.9514	ValidAcc 0.9441	TestAcc 0.9432	BestValid 0.9450
	Epoch 4750:	Loss 0.3503	TrainAcc 0.9492	ValidAcc 0.9421	TestAcc 0.9407	BestValid 0.9450
	Epoch 4800:	Loss 0.3472	TrainAcc 0.9478	ValidAcc 0.9407	TestAcc 0.9399	BestValid 0.9450
	Epoch 4850:	Loss 0.3448	TrainAcc 0.9510	ValidAcc 0.9441	TestAcc 0.9433	BestValid 0.9450
	Epoch 4900:	Loss 0.3437	TrainAcc 0.9430	ValidAcc 0.9351	TestAcc 0.9329	BestValid 0.9450
	Epoch 4950:	Loss 0.3422	TrainAcc 0.9384	ValidAcc 0.9300	TestAcc 0.9288	BestValid 0.9450
	Epoch 5000:	Loss 0.3449	TrainAcc 0.9528	ValidAcc 0.9467	TestAcc 0.9454	BestValid 0.9467
****** Epoch Time (Excluding Evaluation Cost): 0.449 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 66.973 ms (Max: 69.102, Min: 63.108, Sum: 535.784)
Cluster-Wide Average, Compute: 261.782 ms (Max: 270.136, Min: 251.029, Sum: 2094.257)
Cluster-Wide Average, Communication-Layer: 32.129 ms (Max: 37.656, Min: 25.280, Sum: 257.030)
Cluster-Wide Average, Bubble-Imbalance: 16.183 ms (Max: 22.342, Min: 10.504, Sum: 129.464)
Cluster-Wide Average, Communication-Graph: 64.734 ms (Max: 70.603, Min: 59.926, Sum: 517.874)
Cluster-Wide Average, Optimization: 2.155 ms (Max: 2.883, Min: 1.557, Sum: 17.243)
Cluster-Wide Average, Others: 5.719 ms (Max: 11.945, Min: 3.438, Sum: 45.756)
****** Breakdown Sum: 449.676 ms ******
Cluster-Wide Average, GPU Memory Consumption: 7.587 GB (Max: 9.405, Min: 7.094, Sum: 60.692)
Cluster-Wide Average, Graph-Level Communication Throughput: 109.918 Gbps (Max: 126.855, Min: 92.660, Sum: 879.343)
Cluster-Wide Average, Layer-Level Communication Throughput: 34.159 Gbps (Max: 44.390, Min: 23.856, Sum: 273.276)
Layer-level communication (cluster-wide, per-epoch): 1.041 GB
Graph-level communication (cluster-wide, per-epoch): 5.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 6.388 GB
****** Accuracy Results ******
Highest valid_acc: 0.9467
Target test_acc: 0.9454
Epoch to reach the target acc: 4999
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
