Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.910 seconds.
Building the CSC structure...
        It takes 2.058 seconds.
Building the CSC structure...
        It takes 2.352 seconds.
Building the CSC structure...
        It takes 2.392 seconds.
Building the CSC structure...
        It takes 2.429 seconds.
Building the CSC structure...
        It takes 2.497 seconds.
Building the CSC structure...
        It takes 2.561 seconds.
Building the CSC structure...
        It takes 2.671 seconds.
Building the CSC structure...
        It takes 1.848 seconds.
        It takes 1.860 seconds.
        It takes 2.227 seconds.
        It takes 2.313 seconds.
        It takes 2.374 seconds.
Building the Feature Vector...
        It takes 2.286 seconds.
        It takes 2.410 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 2.394 seconds.
        It takes 0.040 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.309 seconds.
Building the Label Vector...
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.295 seconds.
Building the Label Vector...
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.030 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.248 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.133 Gbps (per GPU), 449.062 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.885 Gbps (per GPU), 447.078 Gbps (aggregated)
The layer-level communication performance: 55.874 Gbps (per GPU), 446.995 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.668 Gbps (per GPU), 445.348 Gbps (aggregated)
The layer-level communication performance: 55.635 Gbps (per GPU), 445.076 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.440 Gbps (per GPU), 443.523 Gbps (aggregated)
The layer-level communication performance: 55.405 Gbps (per GPU), 443.242 Gbps (aggregated)
The layer-level communication performance: 55.372 Gbps (per GPU), 442.976 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.378 Gbps (per GPU), 1251.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.402 Gbps (per GPU), 1251.215 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.378 Gbps (per GPU), 1251.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.294 Gbps (per GPU), 1250.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.378 Gbps (per GPU), 1251.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.393 Gbps (per GPU), 1251.145 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.376 Gbps (per GPU), 1251.005 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.297 Gbps (per GPU), 1250.375 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.102 Gbps (per GPU), 808.820 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.102 Gbps (per GPU), 808.820 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.101 Gbps (per GPU), 808.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.101 Gbps (per GPU), 808.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.102 Gbps (per GPU), 808.814 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.102 Gbps (per GPU), 808.813 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.101 Gbps (per GPU), 808.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.102 Gbps (per GPU), 808.820 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.106 Gbps (per GPU), 288.850 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.107 Gbps (per GPU), 288.855 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.105 Gbps (per GPU), 288.843 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.106 Gbps (per GPU), 288.850 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.107 Gbps (per GPU), 288.857 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.107 Gbps (per GPU), 288.855 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.106 Gbps (per GPU), 288.848 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.106 Gbps (per GPU), 288.849 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.32ms  2.57ms  2.37ms  1.40  8.38K  3.53M
 chk_1  3.47ms  2.81ms  2.68ms  1.30  6.74K  3.60M
 chk_2  3.39ms  2.70ms  2.54ms  1.33  7.27K  3.53M
 chk_3  3.43ms  2.71ms  2.56ms  1.34  7.92K  3.61M
 chk_4  3.14ms  2.64ms  2.54ms  1.24  5.33K  3.68M
 chk_5  3.53ms  2.64ms  2.42ms  1.46 10.07K  3.45M
 chk_6  3.67ms  2.82ms  2.59ms  1.42  9.41K  3.48M
 chk_7  3.35ms  2.66ms  2.48ms  1.35  8.12K  3.60M
 chk_8  3.34ms  2.77ms  2.63ms  1.27  6.09K  3.64M
 chk_9  3.58ms  2.56ms  2.31ms  1.55 11.10K  3.38M
chk_10  3.33ms  2.80ms  2.69ms  1.24  5.67K  3.63M
chk_11  3.37ms  2.66ms  2.49ms  1.35  8.16K  3.54M
chk_12  3.55ms  2.85ms  2.71ms  1.31  7.24K  3.55M
chk_13  3.20ms  2.69ms  2.58ms  1.24  5.41K  3.68M
chk_14  3.65ms  2.92ms  2.78ms  1.31  7.14K  3.53M
chk_15  3.65ms  2.80ms  2.58ms  1.42  9.25K  3.49M
chk_16  3.10ms  2.63ms  2.51ms  1.23  4.78K  3.77M
chk_17  3.40ms  2.74ms  2.61ms  1.30  6.85K  3.60M
chk_18  3.27ms  2.54ms  2.41ms  1.36  7.47K  3.57M
chk_19  3.11ms  2.64ms  2.52ms  1.23  4.88K  3.75M
chk_20  3.33ms  2.63ms  2.51ms  1.33  7.00K  3.63M
chk_21  3.15ms  2.63ms  2.51ms  1.26  5.41K  3.68M
chk_22  3.87ms  2.82ms  2.57ms  1.51 11.07K  3.39M
chk_23  3.45ms  2.72ms  2.60ms  1.33  7.23K  3.64M
chk_24  3.69ms  2.78ms  2.56ms  1.44 10.13K  3.43M
chk_25  3.20ms  2.60ms  2.46ms  1.30  6.40K  3.57M
chk_26  3.38ms  2.80ms  2.68ms  1.26  5.78K  3.55M
chk_27  3.52ms  2.67ms  2.47ms  1.42  9.34K  3.48M
chk_28  3.60ms  2.99ms  2.82ms  1.27  6.37K  3.57M
chk_29  3.31ms  2.79ms  2.66ms  1.24  5.16K  3.78M
chk_30  3.19ms  2.65ms  2.58ms  1.24  5.44K  3.67M
chk_31  3.44ms  2.80ms  2.72ms  1.26  6.33K  3.63M
   Avg  3.41  2.72  2.57
   Max  3.87  2.99  2.82
   Min  3.10  2.54  2.31
 Ratio  1.25  1.18  1.22
   Var  0.04  0.01  0.01
Profiling takes 3.240 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 370.046 ms
Partition 0 [0, 4) has cost: 370.046 ms
Partition 1 [4, 8) has cost: 348.073 ms
Partition 2 [8, 12) has cost: 348.073 ms
Partition 3 [12, 16) has cost: 348.073 ms
Partition 4 [16, 20) has cost: 348.073 ms
Partition 5 [20, 24) has cost: 348.073 ms
Partition 6 [24, 28) has cost: 348.073 ms
Partition 7 [28, 32) has cost: 343.210 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 167.018 ms
GPU 0, Compute+Comm Time: 134.524 ms, Bubble Time: 29.361 ms, Imbalance Overhead: 3.133 ms
GPU 1, Compute+Comm Time: 128.168 ms, Bubble Time: 29.112 ms, Imbalance Overhead: 9.738 ms
GPU 2, Compute+Comm Time: 128.168 ms, Bubble Time: 29.060 ms, Imbalance Overhead: 9.790 ms
GPU 3, Compute+Comm Time: 128.168 ms, Bubble Time: 28.906 ms, Imbalance Overhead: 9.943 ms
GPU 4, Compute+Comm Time: 128.168 ms, Bubble Time: 28.862 ms, Imbalance Overhead: 9.988 ms
GPU 5, Compute+Comm Time: 128.168 ms, Bubble Time: 28.875 ms, Imbalance Overhead: 9.975 ms
GPU 6, Compute+Comm Time: 128.168 ms, Bubble Time: 29.095 ms, Imbalance Overhead: 9.755 ms
GPU 7, Compute+Comm Time: 126.391 ms, Bubble Time: 29.552 ms, Imbalance Overhead: 11.075 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 323.213 ms
GPU 0, Compute+Comm Time: 243.380 ms, Bubble Time: 57.701 ms, Imbalance Overhead: 22.132 ms
GPU 1, Compute+Comm Time: 246.467 ms, Bubble Time: 56.790 ms, Imbalance Overhead: 19.957 ms
GPU 2, Compute+Comm Time: 246.467 ms, Bubble Time: 56.330 ms, Imbalance Overhead: 20.416 ms
GPU 3, Compute+Comm Time: 246.467 ms, Bubble Time: 56.236 ms, Imbalance Overhead: 20.511 ms
GPU 4, Compute+Comm Time: 246.467 ms, Bubble Time: 56.240 ms, Imbalance Overhead: 20.507 ms
GPU 5, Compute+Comm Time: 246.467 ms, Bubble Time: 56.468 ms, Imbalance Overhead: 20.279 ms
GPU 6, Compute+Comm Time: 246.467 ms, Bubble Time: 56.423 ms, Imbalance Overhead: 20.323 ms
GPU 7, Compute+Comm Time: 262.083 ms, Bubble Time: 56.887 ms, Imbalance Overhead: 4.243 ms
The estimated cost of the whole pipeline: 514.742 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 718.119 ms
Partition 0 [0, 8) has cost: 718.119 ms
Partition 1 [8, 16) has cost: 696.146 ms
Partition 2 [16, 24) has cost: 696.146 ms
Partition 3 [24, 32) has cost: 691.283 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.199 ms
GPU 0, Compute+Comm Time: 143.854 ms, Bubble Time: 27.049 ms, Imbalance Overhead: 2.296 ms
GPU 1, Compute+Comm Time: 140.388 ms, Bubble Time: 26.615 ms, Imbalance Overhead: 6.196 ms
GPU 2, Compute+Comm Time: 140.388 ms, Bubble Time: 26.494 ms, Imbalance Overhead: 6.318 ms
GPU 3, Compute+Comm Time: 139.570 ms, Bubble Time: 26.256 ms, Imbalance Overhead: 7.373 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 322.233 ms
GPU 0, Compute+Comm Time: 258.703 ms, Bubble Time: 49.417 ms, Imbalance Overhead: 14.114 ms
GPU 1, Compute+Comm Time: 260.141 ms, Bubble Time: 49.572 ms, Imbalance Overhead: 12.520 ms
GPU 2, Compute+Comm Time: 260.141 ms, Bubble Time: 49.496 ms, Imbalance Overhead: 12.596 ms
GPU 3, Compute+Comm Time: 268.817 ms, Bubble Time: 50.431 ms, Imbalance Overhead: 2.985 ms
    The estimated cost with 2 DP ways is 520.204 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1414.265 ms
Partition 0 [0, 16) has cost: 1414.265 ms
Partition 1 [16, 32) has cost: 1387.428 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 219.959 ms
GPU 0, Compute+Comm Time: 191.830 ms, Bubble Time: 23.520 ms, Imbalance Overhead: 4.609 ms
GPU 1, Compute+Comm Time: 189.734 ms, Bubble Time: 24.291 ms, Imbalance Overhead: 5.934 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 361.390 ms
GPU 0, Compute+Comm Time: 312.494 ms, Bubble Time: 39.914 ms, Imbalance Overhead: 8.982 ms
GPU 1, Compute+Comm Time: 317.699 ms, Bubble Time: 38.816 ms, Imbalance Overhead: 4.874 ms
    The estimated cost with 4 DP ways is 610.416 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2801.693 ms
Partition 0 [0, 32) has cost: 2801.693 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 535.685 ms
GPU 0, Compute+Comm Time: 535.685 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 662.804 ms
GPU 0, Compute+Comm Time: 662.804 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1258.414 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 5 initializing the weights for op[130, 194)...
+++++++++ Node 2 initializing the weights for op[66, 130)...
+++++++++ Node 6 initializing the weights for op[194, 257)...
+++++++++ Node 7 initializing the weights for op[194, 257)...
+++++++++ Node 0 initializing the weights for op[0, 66)...
+++++++++ Node 1 initializing the weights for op[0, 66)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.9917	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 50:	Loss 3.3530	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.2528	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.0418	TrainAcc 0.1762	ValidAcc 0.2046	TestAcc 0.2053	BestValid 0.2046
	Epoch 200:	Loss 2.9200	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2046
	Epoch 250:	Loss 2.7550	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2046
	Epoch 300:	Loss 2.8040	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2046
	Epoch 350:	Loss 2.7198	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2046
	Epoch 400:	Loss 2.5273	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2046
	Epoch 450:	Loss 2.3683	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2046
	Epoch 500:	Loss 2.1956	TrainAcc 0.0523	ValidAcc 0.0443	TestAcc 0.0470	BestValid 0.2046
	Epoch 550:	Loss 2.0992	TrainAcc 0.1490	ValidAcc 0.1788	TestAcc 0.1834	BestValid 0.2046
	Epoch 600:	Loss 2.1826	TrainAcc 0.1339	ValidAcc 0.1665	TestAcc 0.1678	BestValid 0.2046
	Epoch 650:	Loss 2.1487	TrainAcc 0.0526	ValidAcc 0.0440	TestAcc 0.0477	BestValid 0.2046
	Epoch 700:	Loss 2.1161	TrainAcc 0.0815	ValidAcc 0.0698	TestAcc 0.0728	BestValid 0.2046
	Epoch 750:	Loss 2.0224	TrainAcc 0.0604	ValidAcc 0.0503	TestAcc 0.0547	BestValid 0.2046
	Epoch 800:	Loss 1.9311	TrainAcc 0.1100	ValidAcc 0.1027	TestAcc 0.1064	BestValid 0.2046
	Epoch 850:	Loss 1.8536	TrainAcc 0.2487	ValidAcc 0.2797	TestAcc 0.2861	BestValid 0.2797
	Epoch 900:	Loss 1.9652	TrainAcc 0.0546	ValidAcc 0.0461	TestAcc 0.0497	BestValid 0.2797
	Epoch 950:	Loss 2.0211	TrainAcc 0.0512	ValidAcc 0.0426	TestAcc 0.0466	BestValid 0.2797
	Epoch 1000:	Loss 1.9207	TrainAcc 0.2420	ValidAcc 0.2774	TestAcc 0.2786	BestValid 0.2797
	Epoch 1050:	Loss 1.9540	TrainAcc 0.0551	ValidAcc 0.0455	TestAcc 0.0484	BestValid 0.2797
	Epoch 1100:	Loss 1.7726	TrainAcc 0.2194	ValidAcc 0.2461	TestAcc 0.2444	BestValid 0.2797
	Epoch 1150:	Loss 1.6934	TrainAcc 0.0629	ValidAcc 0.0629	TestAcc 0.0601	BestValid 0.2797
	Epoch 1200:	Loss 1.6345	TrainAcc 0.1346	ValidAcc 0.1282	TestAcc 0.1248	BestValid 0.2797
	Epoch 1250:	Loss 1.6078	TrainAcc 0.1210	ValidAcc 0.1091	TestAcc 0.1061	BestValid 0.2797
	Epoch 1300:	Loss 1.5255	TrainAcc 0.1453	ValidAcc 0.1403	TestAcc 0.1338	BestValid 0.2797
	Epoch 1350:	Loss 1.4905	TrainAcc 0.0535	ValidAcc 0.0524	TestAcc 0.0502	BestValid 0.2797
	Epoch 1400:	Loss 1.4541	TrainAcc 0.0530	ValidAcc 0.0520	TestAcc 0.0498	BestValid 0.2797
	Epoch 1450:	Loss 1.4278	TrainAcc 0.0531	ValidAcc 0.0522	TestAcc 0.0499	BestValid 0.2797
	Epoch 1500:	Loss 1.4990	TrainAcc 0.0586	ValidAcc 0.0544	TestAcc 0.0530	BestValid 0.2797
	Epoch 1550:	Loss 1.4729	TrainAcc 0.0530	ValidAcc 0.0520	TestAcc 0.0499	BestValid 0.2797
	Epoch 1600:	Loss 1.4292	TrainAcc 0.0530	ValidAcc 0.0521	TestAcc 0.0499	BestValid 0.2797
	Epoch 1650:	Loss 1.3742	TrainAcc 0.0534	ValidAcc 0.0543	TestAcc 0.0517	BestValid 0.2797
	Epoch 1700:	Loss 1.3500	TrainAcc 0.0530	ValidAcc 0.0520	TestAcc 0.0499	BestValid 0.2797
	Epoch 1750:	Loss 1.2985	TrainAcc 0.0530	ValidAcc 0.0521	TestAcc 0.0499	BestValid 0.2797
	Epoch 1800:	Loss 1.4171	TrainAcc 0.0530	ValidAcc 0.0520	TestAcc 0.0499	BestValid 0.2797
	Epoch 1850:	Loss 1.3909	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.2797
	Epoch 1900:	Loss 1.3593	TrainAcc 0.0606	ValidAcc 0.0534	TestAcc 0.0536	BestValid 0.2797
	Epoch 1950:	Loss 1.2477	TrainAcc 0.0317	ValidAcc 0.0285	TestAcc 0.0268	BestValid 0.2797
	Epoch 2000:	Loss 1.1750	TrainAcc 0.0479	ValidAcc 0.0424	TestAcc 0.0435	BestValid 0.2797
	Epoch 2050:	Loss 1.2360	TrainAcc 0.0461	ValidAcc 0.0399	TestAcc 0.0411	BestValid 0.2797
	Epoch 2100:	Loss 1.1874	TrainAcc 0.0472	ValidAcc 0.0747	TestAcc 0.0726	BestValid 0.2797
	Epoch 2150:	Loss 1.1398	TrainAcc 0.0857	ValidAcc 0.0689	TestAcc 0.0677	BestValid 0.2797
	Epoch 2200:	Loss 1.1195	TrainAcc 0.0876	ValidAcc 0.0709	TestAcc 0.0697	BestValid 0.2797
	Epoch 2250:	Loss 1.1323	TrainAcc 0.0030	ValidAcc 0.0030	TestAcc 0.0024	BestValid 0.2797
	Epoch 2300:	Loss 1.1172	TrainAcc 0.0890	ValidAcc 0.0734	TestAcc 0.0721	BestValid 0.2797
	Epoch 2350:	Loss 1.1608	TrainAcc 0.0009	ValidAcc 0.0008	TestAcc 0.0007	BestValid 0.2797
	Epoch 2400:	Loss 1.1319	TrainAcc 0.0106	ValidAcc 0.0111	TestAcc 0.0117	BestValid 0.2797
	Epoch 2450:	Loss 1.1136	TrainAcc 0.0170	ValidAcc 0.0136	TestAcc 0.0132	BestValid 0.2797
	Epoch 2500:	Loss 1.0617	TrainAcc 0.0811	ValidAcc 0.0732	TestAcc 0.0719	BestValid 0.2797
	Epoch 2550:	Loss 1.0560	TrainAcc 0.0153	ValidAcc 0.0133	TestAcc 0.0131	BestValid 0.2797
	Epoch 2600:	Loss 1.1048	TrainAcc 0.0267	ValidAcc 0.0241	TestAcc 0.0235	BestValid 0.2797
	Epoch 2650:	Loss 1.0392	TrainAcc 0.0126	ValidAcc 0.0126	TestAcc 0.0123	BestValid 0.2797
	Epoch 2700:	Loss 1.0072	TrainAcc 0.0236	ValidAcc 0.0212	TestAcc 0.0209	BestValid 0.2797
	Epoch 2750:	Loss 0.9983	TrainAcc 0.0956	ValidAcc 0.0872	TestAcc 0.0853	BestValid 0.2797
	Epoch 2800:	Loss 0.9640	TrainAcc 0.1222	ValidAcc 0.1026	TestAcc 0.0999	BestValid 0.2797
	Epoch 2850:	Loss 0.9410	TrainAcc 0.0919	ValidAcc 0.0815	TestAcc 0.0796	BestValid 0.2797
	Epoch 2900:	Loss 0.9564	TrainAcc 0.1110	ValidAcc 0.0944	TestAcc 0.0931	BestValid 0.2797
	Epoch 2950:	Loss 0.9286	TrainAcc 0.0891	ValidAcc 0.0739	TestAcc 0.0725	BestValid 0.2797
	Epoch 3000:	Loss 0.9551	TrainAcc 0.0290	ValidAcc 0.0233	TestAcc 0.0234	BestValid 0.2797
	Epoch 3050:	Loss 0.9427	TrainAcc 0.0119	ValidAcc 0.0123	TestAcc 0.0120	BestValid 0.2797
	Epoch 3100:	Loss 0.9370	TrainAcc 0.0843	ValidAcc 0.0750	TestAcc 0.0735	BestValid 0.2797
	Epoch 3150:	Loss 0.9221	TrainAcc 0.0386	ValidAcc 0.0339	TestAcc 0.0328	BestValid 0.2797
	Epoch 3200:	Loss 1.0440	TrainAcc 0.0100	ValidAcc 0.0107	TestAcc 0.0097	BestValid 0.2797
	Epoch 3250:	Loss 0.9414	TrainAcc 0.0291	ValidAcc 0.0245	TestAcc 0.0243	BestValid 0.2797
	Epoch 3300:	Loss 0.9107	TrainAcc 0.0890	ValidAcc 0.0739	TestAcc 0.0725	BestValid 0.2797
	Epoch 3350:	Loss 0.8829	TrainAcc 0.0875	ValidAcc 0.0718	TestAcc 0.0701	BestValid 0.2797
	Epoch 3400:	Loss 0.8686	TrainAcc 0.0885	ValidAcc 0.0738	TestAcc 0.0723	BestValid 0.2797
	Epoch 3450:	Loss 0.8522	TrainAcc 0.0886	ValidAcc 0.0737	TestAcc 0.0723	BestValid 0.2797
	Epoch 3500:	Loss 0.8495	TrainAcc 0.0881	ValidAcc 0.0734	TestAcc 0.0721	BestValid 0.2797
	Epoch 3550:	Loss 0.8439	TrainAcc 0.1065	ValidAcc 0.0907	TestAcc 0.0882	BestValid 0.2797
	Epoch 3600:	Loss 0.8576	TrainAcc 0.0788	ValidAcc 0.0689	TestAcc 0.0669	BestValid 0.2797
	Epoch 3650:	Loss 0.8606	TrainAcc 0.0881	ValidAcc 0.0744	TestAcc 0.0727	BestValid 0.2797
	Epoch 3700:	Loss 0.8665	TrainAcc 0.1128	ValidAcc 0.0948	TestAcc 0.0928	BestValid 0.2797
	Epoch 3750:	Loss 0.9079	TrainAcc 0.1067	ValidAcc 0.0906	TestAcc 0.0880	BestValid 0.2797
	Epoch 3800:	Loss 0.8928	TrainAcc 0.0372	ValidAcc 0.0334	TestAcc 0.0320	BestValid 0.2797
	Epoch 3850:	Loss 0.8742	TrainAcc 0.0215	ValidAcc 0.0177	TestAcc 0.0172	BestValid 0.2797
	Epoch 3900:	Loss 0.8439	TrainAcc 0.1039	ValidAcc 0.0857	TestAcc 0.0845	BestValid 0.2797
	Epoch 3950:	Loss 0.8427	TrainAcc 0.0554	ValidAcc 0.0433	TestAcc 0.0450	BestValid 0.2797
	Epoch 4000:	Loss 0.8237	TrainAcc 0.1139	ValidAcc 0.0962	TestAcc 0.0943	BestValid 0.2797
	Epoch 4050:	Loss 0.8581	TrainAcc 0.0346	ValidAcc 0.0578	TestAcc 0.0565	BestValid 0.2797
	Epoch 4100:	Loss 0.7898	TrainAcc 0.0695	ValidAcc 0.0754	TestAcc 0.0736	BestValid 0.2797
	Epoch 4150:	Loss 0.8347	TrainAcc 0.1060	ValidAcc 0.0894	TestAcc 0.0869	BestValid 0.2797
	Epoch 4200:	Loss 0.8115	TrainAcc 0.1062	ValidAcc 0.0898	TestAcc 0.0872	BestValid 0.2797
	Epoch 4250:	Loss 0.8196	TrainAcc 0.0883	ValidAcc 0.0729	TestAcc 0.0714	BestValid 0.2797
	Epoch 4300:	Loss 0.8104	TrainAcc 0.1065	ValidAcc 0.0906	TestAcc 0.0881	BestValid 0.2797
	Epoch 4350:	Loss 0.8384	TrainAcc 0.0935	ValidAcc 0.0769	TestAcc 0.0761	BestValid 0.2797
	Epoch 4400:	Loss 0.8210	TrainAcc 0.0098	ValidAcc 0.0081	TestAcc 0.0071	BestValid 0.2797
	Epoch 4450:	Loss 0.8206	TrainAcc 0.0042	ValidAcc 0.0059	TestAcc 0.0057	BestValid 0.2797
	Epoch 4500:	Loss 0.8115	TrainAcc 0.0012	ValidAcc 0.0031	TestAcc 0.0025	BestValid 0.2797
	Epoch 4550:	Loss 0.8058	TrainAcc 0.0331	ValidAcc 0.0315	TestAcc 0.0315	BestValid 0.2797
	Epoch 4600:	Loss 0.7960	TrainAcc 0.0092	ValidAcc 0.0068	TestAcc 0.0065	BestValid 0.2797
	Epoch 4650:	Loss 0.8121	TrainAcc 0.1075	ValidAcc 0.0914	TestAcc 0.0891	BestValid 0.2797
	Epoch 4700:	Loss 0.7892	TrainAcc 0.0470	ValidAcc 0.0737	TestAcc 0.0714	BestValid 0.2797
	Epoch 4750:	Loss 0.7674	TrainAcc 0.0199	ValidAcc 0.0178	TestAcc 0.0167	BestValid 0.2797
	Epoch 4800:	Loss 0.7615	TrainAcc 0.0825	ValidAcc 0.0692	TestAcc 0.0698	BestValid 0.2797
	Epoch 4850:	Loss 0.7485	TrainAcc 0.0150	ValidAcc 0.0111	TestAcc 0.0110	BestValid 0.2797
	Epoch 4900:	Loss 0.7997	TrainAcc 0.0273	ValidAcc 0.0269	TestAcc 0.0258	BestValid 0.2797
	Epoch 4950:	Loss 0.7798	TrainAcc 0.1058	ValidAcc 0.0896	TestAcc 0.0870	BestValid 0.2797
	Epoch 5000:	Loss 0.7508	TrainAcc 0.0603	ValidAcc 0.0833	TestAcc 0.0814	BestValid 0.2797
****** Epoch Time (Excluding Evaluation Cost): 0.567 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 86.347 ms (Max: 89.156, Min: 81.980, Sum: 690.775)
Cluster-Wide Average, Compute: 373.263 ms (Max: 384.654, Min: 355.844, Sum: 2986.103)
Cluster-Wide Average, Communication-Layer: 16.798 ms (Max: 20.209, Min: 13.654, Sum: 134.381)
Cluster-Wide Average, Bubble-Imbalance: 17.766 ms (Max: 26.937, Min: 8.791, Sum: 142.129)
Cluster-Wide Average, Communication-Graph: 65.489 ms (Max: 72.932, Min: 59.670, Sum: 523.910)
Cluster-Wide Average, Optimization: 2.610 ms (Max: 3.271, Min: 2.338, Sum: 20.883)
Cluster-Wide Average, Others: 5.748 ms (Max: 11.868, Min: 3.581, Sum: 45.987)
****** Breakdown Sum: 568.021 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.716 GB (Max: 8.128, Min: 6.327, Sum: 53.731)
Cluster-Wide Average, Graph-Level Communication Throughput: 108.005 Gbps (Max: 127.489, Min: 88.213, Sum: 864.041)
Cluster-Wide Average, Layer-Level Communication Throughput: 32.524 Gbps (Max: 41.164, Min: 23.343, Sum: 260.189)
Layer-level communication (cluster-wide, per-epoch): 0.521 GB
Graph-level communication (cluster-wide, per-epoch): 5.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 5.870 GB
****** Accuracy Results ******
Highest valid_acc: 0.2797
Target test_acc: 0.2861
Epoch to reach the target acc: 849
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 0] Success 
[MPI Rank 6] Success 
[MPI Rank 1] Success 
[MPI Rank 7] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
