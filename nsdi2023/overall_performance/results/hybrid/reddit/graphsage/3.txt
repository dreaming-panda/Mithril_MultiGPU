Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.882 seconds.
Building the CSC structure...
        It takes 2.077 seconds.
Building the CSC structure...
        It takes 2.375 seconds.
Building the CSC structure...
        It takes 2.382 seconds.
Building the CSC structure...
        It takes 2.407 seconds.
Building the CSC structure...
        It takes 2.592 seconds.
Building the CSC structure...
        It takes 2.643 seconds.
Building the CSC structure...
        It takes 2.662 seconds.
Building the CSC structure...
        It takes 1.841 seconds.
        It takes 1.913 seconds.
        It takes 2.304 seconds.
Building the Feature Vector...
        It takes 2.347 seconds.
        It takes 2.327 seconds.
        It takes 2.284 seconds.
        It takes 2.328 seconds.
        It takes 0.278 seconds.
Building the Label Vector...
        It takes 2.350 seconds.
        It takes 0.037 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.294 seconds.
        It takes 0.043 seconds.
Building the Label Vector...
        It takes 0.295 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
        It takes 0.040 seconds.
        It takes 0.043 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.158 Gbps (per GPU), 465.268 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.888 Gbps (per GPU), 463.105 Gbps (aggregated)
The layer-level communication performance: 57.972 Gbps (per GPU), 463.772 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.610 Gbps (per GPU), 460.878 Gbps (aggregated)
The layer-level communication performance: 57.641 Gbps (per GPU), 461.127 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.391 Gbps (per GPU), 459.127 Gbps (aggregated)
The layer-level communication performance: 57.345 Gbps (per GPU), 458.761 Gbps (aggregated)
The layer-level communication performance: 57.310 Gbps (per GPU), 458.480 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.779 Gbps (per GPU), 1270.231 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.791 Gbps (per GPU), 1270.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.776 Gbps (per GPU), 1270.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.836 Gbps (per GPU), 1270.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.776 Gbps (per GPU), 1270.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.785 Gbps (per GPU), 1270.279 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.773 Gbps (per GPU), 1270.184 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.842 Gbps (per GPU), 1270.736 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.162 Gbps (per GPU), 801.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.162 Gbps (per GPU), 801.300 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.159 Gbps (per GPU), 801.274 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.162 Gbps (per GPU), 801.300 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.159 Gbps (per GPU), 801.274 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.162 Gbps (per GPU), 801.299 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.159 Gbps (per GPU), 801.274 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.163 Gbps (per GPU), 801.306 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.044 Gbps (per GPU), 288.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.044 Gbps (per GPU), 288.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.043 Gbps (per GPU), 288.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.043 Gbps (per GPU), 288.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.044 Gbps (per GPU), 288.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.044 Gbps (per GPU), 288.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.043 Gbps (per GPU), 288.343 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.043 Gbps (per GPU), 288.344 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.27ms  2.56ms  2.40ms  1.36  8.38K  3.53M
 chk_1  3.48ms  2.81ms  2.66ms  1.31  6.74K  3.60M
 chk_2  3.38ms  2.67ms  2.53ms  1.33  7.27K  3.53M
 chk_3  3.45ms  2.70ms  2.55ms  1.35  7.92K  3.61M
 chk_4  3.14ms  2.63ms  2.53ms  1.24  5.33K  3.68M
 chk_5  3.52ms  2.63ms  2.41ms  1.46 10.07K  3.45M
 chk_6  3.66ms  2.80ms  2.59ms  1.42  9.41K  3.48M
 chk_7  3.36ms  2.64ms  2.47ms  1.36  8.12K  3.60M
 chk_8  3.32ms  2.76ms  2.62ms  1.26  6.09K  3.64M
 chk_9  3.57ms  2.54ms  2.30ms  1.55 11.10K  3.38M
chk_10  3.30ms  2.80ms  2.68ms  1.23  5.67K  3.63M
chk_11  3.35ms  2.66ms  2.48ms  1.35  8.16K  3.54M
chk_12  3.56ms  2.85ms  2.70ms  1.32  7.24K  3.55M
chk_13  3.19ms  2.67ms  2.57ms  1.24  5.41K  3.68M
chk_14  3.64ms  2.91ms  2.77ms  1.31  7.14K  3.53M
chk_15  3.59ms  2.77ms  2.57ms  1.40  9.25K  3.49M
chk_16  3.07ms  2.62ms  2.50ms  1.23  4.78K  3.77M
chk_17  3.39ms  2.74ms  2.60ms  1.31  6.85K  3.60M
chk_18  3.26ms  2.54ms  2.40ms  1.36  7.47K  3.57M
chk_19  3.11ms  2.61ms  2.51ms  1.24  4.88K  3.75M
chk_20  3.32ms  2.75ms  2.48ms  1.34  7.00K  3.63M
chk_21  3.13ms  2.58ms  2.50ms  1.26  5.41K  3.68M
chk_22  3.82ms  2.80ms  2.56ms  1.49 11.07K  3.39M
chk_23  3.42ms  2.71ms  2.56ms  1.33  7.23K  3.64M
chk_24  3.67ms  2.75ms  2.53ms  1.45 10.13K  3.43M
chk_25  3.17ms  2.58ms  2.44ms  1.30  6.40K  3.57M
chk_26  3.36ms  2.78ms  2.66ms  1.26  5.78K  3.55M
chk_27  3.49ms  2.66ms  2.46ms  1.42  9.34K  3.48M
chk_28  3.60ms  2.97ms  2.82ms  1.28  6.37K  3.57M
chk_29  3.31ms  2.78ms  2.65ms  1.25  5.16K  3.78M
chk_30  3.18ms  2.66ms  2.54ms  1.25  5.44K  3.67M
chk_31  3.44ms  2.80ms  2.67ms  1.29  6.33K  3.63M
   Avg  3.39  2.71  2.55
   Max  3.82  2.97  2.82
   Min  3.07  2.54  2.30
 Ratio  1.24  1.17  1.22
   Var  0.03  0.01  0.01
Profiling takes 3.231 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 368.704 ms
Partition 0 [0, 4) has cost: 368.704 ms
Partition 1 [4, 8) has cost: 346.912 ms
Partition 2 [8, 12) has cost: 346.912 ms
Partition 3 [12, 16) has cost: 346.912 ms
Partition 4 [16, 20) has cost: 346.912 ms
Partition 5 [20, 24) has cost: 346.912 ms
Partition 6 [24, 28) has cost: 346.912 ms
Partition 7 [28, 32) has cost: 341.902 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.441 ms
GPU 0, Compute+Comm Time: 133.391 ms, Bubble Time: 29.101 ms, Imbalance Overhead: 2.949 ms
GPU 1, Compute+Comm Time: 126.998 ms, Bubble Time: 28.871 ms, Imbalance Overhead: 9.571 ms
GPU 2, Compute+Comm Time: 126.998 ms, Bubble Time: 28.883 ms, Imbalance Overhead: 9.559 ms
GPU 3, Compute+Comm Time: 126.998 ms, Bubble Time: 28.741 ms, Imbalance Overhead: 9.702 ms
GPU 4, Compute+Comm Time: 126.998 ms, Bubble Time: 28.723 ms, Imbalance Overhead: 9.719 ms
GPU 5, Compute+Comm Time: 126.998 ms, Bubble Time: 28.741 ms, Imbalance Overhead: 9.701 ms
GPU 6, Compute+Comm Time: 126.998 ms, Bubble Time: 28.935 ms, Imbalance Overhead: 9.507 ms
GPU 7, Compute+Comm Time: 125.258 ms, Bubble Time: 29.368 ms, Imbalance Overhead: 10.815 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 321.435 ms
GPU 0, Compute+Comm Time: 242.280 ms, Bubble Time: 57.356 ms, Imbalance Overhead: 21.800 ms
GPU 1, Compute+Comm Time: 245.550 ms, Bubble Time: 56.465 ms, Imbalance Overhead: 19.419 ms
GPU 2, Compute+Comm Time: 245.550 ms, Bubble Time: 56.039 ms, Imbalance Overhead: 19.845 ms
GPU 3, Compute+Comm Time: 245.550 ms, Bubble Time: 55.978 ms, Imbalance Overhead: 19.907 ms
GPU 4, Compute+Comm Time: 245.550 ms, Bubble Time: 55.951 ms, Imbalance Overhead: 19.933 ms
GPU 5, Compute+Comm Time: 245.550 ms, Bubble Time: 56.200 ms, Imbalance Overhead: 19.685 ms
GPU 6, Compute+Comm Time: 245.550 ms, Bubble Time: 56.129 ms, Imbalance Overhead: 19.756 ms
GPU 7, Compute+Comm Time: 260.950 ms, Bubble Time: 56.582 ms, Imbalance Overhead: 3.903 ms
The estimated cost of the whole pipeline: 511.220 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 715.616 ms
Partition 0 [0, 8) has cost: 715.616 ms
Partition 1 [8, 16) has cost: 693.825 ms
Partition 2 [16, 24) has cost: 693.825 ms
Partition 3 [24, 32) has cost: 688.814 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 171.619 ms
GPU 0, Compute+Comm Time: 142.480 ms, Bubble Time: 26.952 ms, Imbalance Overhead: 2.186 ms
GPU 1, Compute+Comm Time: 139.000 ms, Bubble Time: 26.457 ms, Imbalance Overhead: 6.161 ms
GPU 2, Compute+Comm Time: 139.000 ms, Bubble Time: 26.358 ms, Imbalance Overhead: 6.260 ms
GPU 3, Compute+Comm Time: 138.192 ms, Bubble Time: 26.097 ms, Imbalance Overhead: 7.329 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 320.485 ms
GPU 0, Compute+Comm Time: 257.950 ms, Bubble Time: 49.113 ms, Imbalance Overhead: 13.422 ms
GPU 1, Compute+Comm Time: 259.539 ms, Bubble Time: 49.260 ms, Imbalance Overhead: 11.686 ms
GPU 2, Compute+Comm Time: 259.539 ms, Bubble Time: 49.193 ms, Imbalance Overhead: 11.753 ms
GPU 3, Compute+Comm Time: 268.000 ms, Bubble Time: 50.164 ms, Imbalance Overhead: 2.321 ms
    The estimated cost with 2 DP ways is 516.709 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1409.441 ms
Partition 0 [0, 16) has cost: 1409.441 ms
Partition 1 [16, 32) has cost: 1382.638 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 219.767 ms
GPU 0, Compute+Comm Time: 191.501 ms, Bubble Time: 23.538 ms, Imbalance Overhead: 4.728 ms
GPU 1, Compute+Comm Time: 189.422 ms, Bubble Time: 24.219 ms, Imbalance Overhead: 6.127 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 360.274 ms
GPU 0, Compute+Comm Time: 311.570 ms, Bubble Time: 39.663 ms, Imbalance Overhead: 9.041 ms
GPU 1, Compute+Comm Time: 316.719 ms, Bubble Time: 38.720 ms, Imbalance Overhead: 4.835 ms
    The estimated cost with 4 DP ways is 609.044 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2792.079 ms
Partition 0 [0, 32) has cost: 2792.079 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 536.041 ms
GPU 0, Compute+Comm Time: 536.041 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 662.469 ms
GPU 0, Compute+Comm Time: 662.469 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1258.436 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 66)...
+++++++++ Node 0 initializing the weights for op[0, 66)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 2 initializing the weights for op[66, 130)...
+++++++++ Node 5 initializing the weights for op[130, 194)...
+++++++++ Node 6 initializing the weights for op[194, 257)...
+++++++++ Node 7 initializing the weights for op[194, 257)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.4062	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.0166
	Epoch 50:	Loss 3.4988	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.2793	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 3.1895	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 200:	Loss 3.0374	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 250:	Loss 2.9627	TrainAcc 0.1075	ValidAcc 0.1465	TestAcc 0.1483	BestValid 0.1466
	Epoch 300:	Loss 2.9261	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 350:	Loss 2.9771	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 400:	Loss 2.7284	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 450:	Loss 2.6550	TrainAcc 0.0808	ValidAcc 0.0687	TestAcc 0.0681	BestValid 0.1466
	Epoch 500:	Loss 2.5626	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 550:	Loss 2.3765	TrainAcc 0.0618	ValidAcc 0.0536	TestAcc 0.0565	BestValid 0.1466
	Epoch 600:	Loss 2.2267	TrainAcc 0.0722	ValidAcc 0.0709	TestAcc 0.0667	BestValid 0.1466
	Epoch 650:	Loss 2.1523	TrainAcc 0.1076	ValidAcc 0.1467	TestAcc 0.1485	BestValid 0.1467
	Epoch 700:	Loss 2.0870	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1467
	Epoch 750:	Loss 2.0455	TrainAcc 0.1079	ValidAcc 0.1467	TestAcc 0.1485	BestValid 0.1467
	Epoch 800:	Loss 2.0178	TrainAcc 0.1328	ValidAcc 0.1595	TestAcc 0.1581	BestValid 0.1595
	Epoch 850:	Loss 2.0188	TrainAcc 0.1756	ValidAcc 0.2162	TestAcc 0.2175	BestValid 0.2162
	Epoch 900:	Loss 2.0072	TrainAcc 0.1421	ValidAcc 0.1741	TestAcc 0.1748	BestValid 0.2162
	Epoch 950:	Loss 2.0162	TrainAcc 0.0600	ValidAcc 0.0630	TestAcc 0.0628	BestValid 0.2162
	Epoch 1000:	Loss 1.9825	TrainAcc 0.0592	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.2162
	Epoch 1050:	Loss 1.9487	TrainAcc 0.0590	ValidAcc 0.0612	TestAcc 0.0613	BestValid 0.2162
	Epoch 1100:	Loss 1.9585	TrainAcc 0.1712	ValidAcc 0.2151	TestAcc 0.2149	BestValid 0.2162
	Epoch 1150:	Loss 2.0006	TrainAcc 0.1560	ValidAcc 0.1737	TestAcc 0.1731	BestValid 0.2162
	Epoch 1200:	Loss 2.3318	TrainAcc 0.1620	ValidAcc 0.2032	TestAcc 0.2056	BestValid 0.2162
	Epoch 1250:	Loss 2.1205	TrainAcc 0.1051	ValidAcc 0.1224	TestAcc 0.1231	BestValid 0.2162
	Epoch 1300:	Loss 1.9782	TrainAcc 0.1744	ValidAcc 0.2226	TestAcc 0.2220	BestValid 0.2226
	Epoch 1350:	Loss 1.8777	TrainAcc 0.0969	ValidAcc 0.1088	TestAcc 0.1068	BestValid 0.2226
	Epoch 1400:	Loss 1.8310	TrainAcc 0.1318	ValidAcc 0.1681	TestAcc 0.1691	BestValid 0.2226
	Epoch 1450:	Loss 1.8458	TrainAcc 0.0861	ValidAcc 0.0772	TestAcc 0.0795	BestValid 0.2226
	Epoch 1500:	Loss 1.9586	TrainAcc 0.1994	ValidAcc 0.2343	TestAcc 0.2348	BestValid 0.2343
	Epoch 1550:	Loss 1.8616	TrainAcc 0.0861	ValidAcc 0.0974	TestAcc 0.0928	BestValid 0.2343
	Epoch 1600:	Loss 1.8259	TrainAcc 0.2006	ValidAcc 0.2151	TestAcc 0.2103	BestValid 0.2343
	Epoch 1650:	Loss 1.7671	TrainAcc 0.0992	ValidAcc 0.1355	TestAcc 0.1370	BestValid 0.2343
	Epoch 1700:	Loss 1.7126	TrainAcc 0.0613	ValidAcc 0.0634	TestAcc 0.0645	BestValid 0.2343
	Epoch 1750:	Loss 1.6654	TrainAcc 0.1636	ValidAcc 0.2027	TestAcc 0.2000	BestValid 0.2343
	Epoch 1800:	Loss 1.6488	TrainAcc 0.1470	ValidAcc 0.1877	TestAcc 0.1857	BestValid 0.2343
	Epoch 1850:	Loss 1.5728	TrainAcc 0.1716	ValidAcc 0.2156	TestAcc 0.2133	BestValid 0.2343
	Epoch 1900:	Loss 1.5623	TrainAcc 0.2518	ValidAcc 0.2445	TestAcc 0.2419	BestValid 0.2445
	Epoch 1950:	Loss 1.5022	TrainAcc 0.0656	ValidAcc 0.0639	TestAcc 0.0640	BestValid 0.2445
	Epoch 2000:	Loss 1.5444	TrainAcc 0.0672	ValidAcc 0.0600	TestAcc 0.0619	BestValid 0.2445
	Epoch 2050:	Loss 1.5257	TrainAcc 0.0544	ValidAcc 0.0467	TestAcc 0.0496	BestValid 0.2445
	Epoch 2100:	Loss 1.4575	TrainAcc 0.0711	ValidAcc 0.0586	TestAcc 0.0604	BestValid 0.2445
	Epoch 2150:	Loss 1.4686	TrainAcc 0.1959	ValidAcc 0.1821	TestAcc 0.1803	BestValid 0.2445
	Epoch 2200:	Loss 1.4934	TrainAcc 0.1919	ValidAcc 0.2344	TestAcc 0.2246	BestValid 0.2445
	Epoch 2250:	Loss 1.4806	TrainAcc 0.1760	ValidAcc 0.2151	TestAcc 0.2095	BestValid 0.2445
	Epoch 2300:	Loss 1.4717	TrainAcc 0.0397	ValidAcc 0.0394	TestAcc 0.0359	BestValid 0.2445
	Epoch 2350:	Loss 1.3917	TrainAcc 0.1161	ValidAcc 0.0978	TestAcc 0.1016	BestValid 0.2445
	Epoch 2400:	Loss 1.3841	TrainAcc 0.0558	ValidAcc 0.0465	TestAcc 0.0497	BestValid 0.2445
	Epoch 2450:	Loss 1.3681	TrainAcc 0.0543	ValidAcc 0.0448	TestAcc 0.0482	BestValid 0.2445
	Epoch 2500:	Loss 1.3679	TrainAcc 0.0851	ValidAcc 0.0783	TestAcc 0.0785	BestValid 0.2445
	Epoch 2550:	Loss 1.3177	TrainAcc 0.0264	ValidAcc 0.0240	TestAcc 0.0229	BestValid 0.2445
	Epoch 2600:	Loss 1.3080	TrainAcc 0.0678	ValidAcc 0.0577	TestAcc 0.0605	BestValid 0.2445
	Epoch 2650:	Loss 1.3306	TrainAcc 0.1156	ValidAcc 0.1068	TestAcc 0.1039	BestValid 0.2445
	Epoch 2700:	Loss 1.3099	TrainAcc 0.1244	ValidAcc 0.1191	TestAcc 0.1132	BestValid 0.2445
	Epoch 2750:	Loss 1.2614	TrainAcc 0.0666	ValidAcc 0.0544	TestAcc 0.0581	BestValid 0.2445
	Epoch 2800:	Loss 1.2509	TrainAcc 0.0513	ValidAcc 0.0420	TestAcc 0.0461	BestValid 0.2445
	Epoch 2850:	Loss 1.2492	TrainAcc 0.0707	ValidAcc 0.0589	TestAcc 0.0625	BestValid 0.2445
	Epoch 2900:	Loss 1.2899	TrainAcc 0.0281	ValidAcc 0.0212	TestAcc 0.0213	BestValid 0.2445
	Epoch 2950:	Loss 1.2580	TrainAcc 0.0515	ValidAcc 0.0415	TestAcc 0.0458	BestValid 0.2445
	Epoch 3000:	Loss 1.2176	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0451	BestValid 0.2445
	Epoch 3050:	Loss 1.2347	TrainAcc 0.0184	ValidAcc 0.0169	TestAcc 0.0159	BestValid 0.2445
	Epoch 3100:	Loss 1.1393	TrainAcc 0.0652	ValidAcc 0.0553	TestAcc 0.0582	BestValid 0.2445
	Epoch 3150:	Loss 1.0897	TrainAcc 0.0698	ValidAcc 0.0590	TestAcc 0.0581	BestValid 0.2445
	Epoch 3200:	Loss 1.1376	TrainAcc 0.0693	ValidAcc 0.0586	TestAcc 0.0577	BestValid 0.2445
	Epoch 3250:	Loss 1.1237	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2445
	Epoch 3300:	Loss 1.1007	TrainAcc 0.0244	ValidAcc 0.0203	TestAcc 0.0196	BestValid 0.2445
	Epoch 3350:	Loss 1.0952	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0174	BestValid 0.2445
	Epoch 3400:	Loss 1.1299	TrainAcc 0.0741	ValidAcc 0.0629	TestAcc 0.0616	BestValid 0.2445
	Epoch 3450:	Loss 1.1491	TrainAcc 0.0268	ValidAcc 0.0200	TestAcc 0.0201	BestValid 0.2445
	Epoch 3500:	Loss 1.1854	TrainAcc 0.0216	ValidAcc 0.0185	TestAcc 0.0176	BestValid 0.2445
	Epoch 3550:	Loss 1.1172	TrainAcc 0.0230	ValidAcc 0.0183	TestAcc 0.0176	BestValid 0.2445
	Epoch 3600:	Loss 1.0854	TrainAcc 0.0213	ValidAcc 0.0186	TestAcc 0.0177	BestValid 0.2445
	Epoch 3650:	Loss 1.0815	TrainAcc 0.0212	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2445
	Epoch 3700:	Loss 1.0676	TrainAcc 0.0214	ValidAcc 0.0184	TestAcc 0.0174	BestValid 0.2445
	Epoch 3750:	Loss 1.1999	TrainAcc 0.0134	ValidAcc 0.0092	TestAcc 0.0088	BestValid 0.2445
	Epoch 3800:	Loss 1.1014	TrainAcc 0.0118	ValidAcc 0.0078	TestAcc 0.0078	BestValid 0.2445
	Epoch 3850:	Loss 1.0931	TrainAcc 0.0015	ValidAcc 0.0007	TestAcc 0.0008	BestValid 0.2445
	Epoch 3900:	Loss 1.1301	TrainAcc 0.0025	ValidAcc 0.0012	TestAcc 0.0013	BestValid 0.2445
	Epoch 3950:	Loss 1.0667	TrainAcc 0.0037	ValidAcc 0.0020	TestAcc 0.0020	BestValid 0.2445
	Epoch 4000:	Loss 1.0439	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2445
	Epoch 4050:	Loss 1.0784	TrainAcc 0.0213	ValidAcc 0.0180	TestAcc 0.0169	BestValid 0.2445
	Epoch 4100:	Loss 1.0207	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2445
	Epoch 4150:	Loss 1.0839	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2445
	Epoch 4200:	Loss 1.3421	TrainAcc 0.0214	ValidAcc 0.0184	TestAcc 0.0174	BestValid 0.2445
	Epoch 4250:	Loss 1.2269	TrainAcc 0.0219	ValidAcc 0.0185	TestAcc 0.0178	BestValid 0.2445
	Epoch 4300:	Loss 1.1418	TrainAcc 0.0218	ValidAcc 0.0185	TestAcc 0.0176	BestValid 0.2445
	Epoch 4350:	Loss 1.1013	TrainAcc 0.0234	ValidAcc 0.0204	TestAcc 0.0191	BestValid 0.2445
	Epoch 4400:	Loss 1.0564	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0174	BestValid 0.2445
	Epoch 4450:	Loss 1.0209	TrainAcc 0.0216	ValidAcc 0.0185	TestAcc 0.0175	BestValid 0.2445
	Epoch 4500:	Loss 1.0237	TrainAcc 0.0226	ValidAcc 0.0196	TestAcc 0.0186	BestValid 0.2445
	Epoch 4550:	Loss 1.0252	TrainAcc 0.0215	ValidAcc 0.0185	TestAcc 0.0174	BestValid 0.2445
	Epoch 4600:	Loss 0.9848	TrainAcc 0.0136	ValidAcc 0.0087	TestAcc 0.0079	BestValid 0.2445
	Epoch 4650:	Loss 0.9900	TrainAcc 0.0210	ValidAcc 0.0180	TestAcc 0.0170	BestValid 0.2445
	Epoch 4700:	Loss 0.9872	TrainAcc 0.0194	ValidAcc 0.0170	TestAcc 0.0162	BestValid 0.2445
	Epoch 4750:	Loss 0.9721	TrainAcc 0.0212	ValidAcc 0.0180	TestAcc 0.0172	BestValid 0.2445
	Epoch 4800:	Loss 0.9428	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2445
	Epoch 4850:	Loss 0.9287	TrainAcc 0.0199	ValidAcc 0.0176	TestAcc 0.0168	BestValid 0.2445
	Epoch 4900:	Loss 0.9333	TrainAcc 0.0218	ValidAcc 0.0186	TestAcc 0.0176	BestValid 0.2445
	Epoch 4950:	Loss 0.9015	TrainAcc 0.0217	ValidAcc 0.0184	TestAcc 0.0175	BestValid 0.2445
	Epoch 5000:	Loss 0.8866	TrainAcc 0.0224	ValidAcc 0.0193	TestAcc 0.0184	BestValid 0.2445
****** Epoch Time (Excluding Evaluation Cost): 0.569 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 86.292 ms (Max: 89.059, Min: 82.000, Sum: 690.339)
Cluster-Wide Average, Compute: 375.376 ms (Max: 385.741, Min: 358.860, Sum: 3003.010)
Cluster-Wide Average, Communication-Layer: 16.873 ms (Max: 20.423, Min: 13.689, Sum: 134.984)
Cluster-Wide Average, Bubble-Imbalance: 16.948 ms (Max: 26.659, Min: 10.417, Sum: 135.580)
Cluster-Wide Average, Communication-Graph: 65.489 ms (Max: 71.368, Min: 60.467, Sum: 523.912)
Cluster-Wide Average, Optimization: 2.721 ms (Max: 3.276, Min: 2.360, Sum: 21.764)
Cluster-Wide Average, Others: 5.831 ms (Max: 12.051, Min: 3.585, Sum: 46.650)
****** Breakdown Sum: 569.530 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.717 GB (Max: 8.128, Min: 6.329, Sum: 53.737)
Cluster-Wide Average, Graph-Level Communication Throughput: 107.864 Gbps (Max: 124.920, Min: 90.760, Sum: 862.912)
Cluster-Wide Average, Layer-Level Communication Throughput: 32.401 Gbps (Max: 41.478, Min: 23.088, Sum: 259.206)
Layer-level communication (cluster-wide, per-epoch): 0.521 GB
Graph-level communication (cluster-wide, per-epoch): 5.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 5.870 GB
****** Accuracy Results ******
Highest valid_acc: 0.2445
Target test_acc: 0.2419
Epoch to reach the target acc: 1899
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
