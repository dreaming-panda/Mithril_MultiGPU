Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...Building the CSR structure...
Building the CSR structure...
Building the CSR structure...

        It takes 1.889 seconds.
Building the CSC structure...
        It takes 1.939 seconds.
Building the CSC structure...
        It takes 1.982 seconds.
Building the CSC structure...
        It takes 2.333 seconds.
Building the CSC structure...
        It takes 2.356 seconds.
Building the CSC structure...
        It takes 2.419 seconds.
Building the CSC structure...
        It takes 2.460 seconds.
Building the CSC structure...
        It takes 2.629 seconds.
Building the CSC structure...
        It takes 1.814 seconds.
        It takes 1.863 seconds.
        It takes 1.908 seconds.
        It takes 2.196 seconds.
        It takes 2.270 seconds.
        It takes 2.252 seconds.
Building the Feature Vector...
        It takes 2.365 seconds.
Building the Feature Vector...
        It takes 0.283 seconds.
Building the Feature Vector...
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 2.381 seconds.
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.292 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.281 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.263 seconds.
Building the Label Vector...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.031 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.875 Gbps (per GPU), 446.998 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.615 Gbps (per GPU), 444.924 Gbps (aggregated)
The layer-level communication performance: 55.606 Gbps (per GPU), 444.850 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.362 Gbps (per GPU), 442.894 Gbps (aggregated)
The layer-level communication performance: 55.394 Gbps (per GPU), 443.153 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.184 Gbps (per GPU), 441.474 Gbps (aggregated)
The layer-level communication performance: 55.149 Gbps (per GPU), 441.194 Gbps (aggregated)
The layer-level communication performance: 55.117 Gbps (per GPU), 440.937 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.704 Gbps (per GPU), 1261.634 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.716 Gbps (per GPU), 1261.729 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.704 Gbps (per GPU), 1261.634 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.669 Gbps (per GPU), 1261.350 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.707 Gbps (per GPU), 1261.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.719 Gbps (per GPU), 1261.753 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.707 Gbps (per GPU), 1261.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.672 Gbps (per GPU), 1261.374 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.223 Gbps (per GPU), 809.783 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.221 Gbps (per GPU), 809.770 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.220 Gbps (per GPU), 809.763 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.224 Gbps (per GPU), 809.788 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.222 Gbps (per GPU), 809.776 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.224 Gbps (per GPU), 809.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.222 Gbps (per GPU), 809.776 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.224 Gbps (per GPU), 809.795 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.272 Gbps (per GPU), 250.174 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.272 Gbps (per GPU), 250.174 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.271 Gbps (per GPU), 250.167 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.270 Gbps (per GPU), 250.162 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.272 Gbps (per GPU), 250.173 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.272 Gbps (per GPU), 250.174 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.271 Gbps (per GPU), 250.169 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.270 Gbps (per GPU), 250.161 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.29ms  2.57ms  2.42ms  1.36  8.38K  3.53M
 chk_1  3.50ms  2.84ms  2.68ms  1.31  6.74K  3.60M
 chk_2  3.41ms  2.69ms  2.55ms  1.34  7.27K  3.53M
 chk_3  3.44ms  2.73ms  2.56ms  1.34  7.92K  3.61M
 chk_4  3.16ms  2.66ms  2.55ms  1.24  5.33K  3.68M
 chk_5  3.54ms  2.65ms  2.43ms  1.46 10.07K  3.45M
 chk_6  3.68ms  2.82ms  2.61ms  1.41  9.41K  3.48M
 chk_7  3.39ms  2.66ms  2.50ms  1.35  8.12K  3.60M
 chk_8  3.34ms  2.77ms  2.65ms  1.26  6.09K  3.64M
 chk_9  3.60ms  2.57ms  2.33ms  1.54 11.10K  3.38M
chk_10  3.34ms  2.80ms  2.71ms  1.24  5.67K  3.63M
chk_11  3.38ms  2.68ms  2.51ms  1.35  8.16K  3.54M
chk_12  3.58ms  2.87ms  2.73ms  1.31  7.24K  3.55M
chk_13  3.21ms  2.71ms  2.58ms  1.24  5.41K  3.68M
chk_14  3.67ms  2.94ms  2.78ms  1.32  7.14K  3.53M
chk_15  3.64ms  2.80ms  2.58ms  1.41  9.25K  3.49M
chk_16  3.12ms  2.64ms  2.53ms  1.23  4.78K  3.77M
chk_17  3.43ms  2.75ms  2.62ms  1.31  6.85K  3.60M
chk_18  3.27ms  2.56ms  2.42ms  1.35  7.47K  3.57M
chk_19  3.14ms  2.64ms  2.53ms  1.24  4.88K  3.75M
chk_20  3.32ms  2.62ms  2.50ms  1.33  7.00K  3.63M
chk_21  3.14ms  2.61ms  2.51ms  1.25  5.41K  3.68M
chk_22  3.84ms  2.81ms  2.57ms  1.49 11.07K  3.39M
chk_23  3.45ms  2.71ms  2.57ms  1.34  7.23K  3.64M
chk_24  3.69ms  2.79ms  2.56ms  1.44 10.13K  3.43M
chk_25  3.20ms  2.58ms  2.46ms  1.30  6.40K  3.57M
chk_26  3.38ms  2.80ms  2.69ms  1.26  5.78K  3.55M
chk_27  3.52ms  2.69ms  2.48ms  1.42  9.34K  3.48M
chk_28  3.60ms  2.99ms  2.83ms  1.27  6.37K  3.57M
chk_29  3.31ms  2.81ms  2.68ms  1.24  5.16K  3.78M
chk_30  3.21ms  2.68ms  2.57ms  1.25  5.44K  3.67M
chk_31  3.45ms  2.83ms  2.72ms  1.27  6.33K  3.63M
   Avg  3.41  2.73  2.58
   Max  3.84  2.99  2.83
   Min  3.12  2.56  2.33
 Ratio  1.23  1.17  1.21
   Var  0.03  0.01  0.01
Profiling takes 3.249 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 371.061 ms
Partition 0 [0, 4) has cost: 371.061 ms
Partition 1 [4, 8) has cost: 349.100 ms
Partition 2 [8, 12) has cost: 349.100 ms
Partition 3 [12, 16) has cost: 349.100 ms
Partition 4 [16, 20) has cost: 349.100 ms
Partition 5 [20, 24) has cost: 349.100 ms
Partition 6 [24, 28) has cost: 349.100 ms
Partition 7 [28, 32) has cost: 344.245 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 167.676 ms
GPU 0, Compute+Comm Time: 135.106 ms, Bubble Time: 29.498 ms, Imbalance Overhead: 3.072 ms
GPU 1, Compute+Comm Time: 128.675 ms, Bubble Time: 29.305 ms, Imbalance Overhead: 9.696 ms
GPU 2, Compute+Comm Time: 128.675 ms, Bubble Time: 29.333 ms, Imbalance Overhead: 9.668 ms
GPU 3, Compute+Comm Time: 128.675 ms, Bubble Time: 29.198 ms, Imbalance Overhead: 9.803 ms
GPU 4, Compute+Comm Time: 128.675 ms, Bubble Time: 29.178 ms, Imbalance Overhead: 9.823 ms
GPU 5, Compute+Comm Time: 128.675 ms, Bubble Time: 29.212 ms, Imbalance Overhead: 9.789 ms
GPU 6, Compute+Comm Time: 128.675 ms, Bubble Time: 29.394 ms, Imbalance Overhead: 9.607 ms
GPU 7, Compute+Comm Time: 126.889 ms, Bubble Time: 29.806 ms, Imbalance Overhead: 10.981 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 324.298 ms
GPU 0, Compute+Comm Time: 244.041 ms, Bubble Time: 57.804 ms, Imbalance Overhead: 22.453 ms
GPU 1, Compute+Comm Time: 247.110 ms, Bubble Time: 56.949 ms, Imbalance Overhead: 20.239 ms
GPU 2, Compute+Comm Time: 247.110 ms, Bubble Time: 56.528 ms, Imbalance Overhead: 20.660 ms
GPU 3, Compute+Comm Time: 247.110 ms, Bubble Time: 56.477 ms, Imbalance Overhead: 20.712 ms
GPU 4, Compute+Comm Time: 247.110 ms, Bubble Time: 56.460 ms, Imbalance Overhead: 20.728 ms
GPU 5, Compute+Comm Time: 247.110 ms, Bubble Time: 56.707 ms, Imbalance Overhead: 20.482 ms
GPU 6, Compute+Comm Time: 247.110 ms, Bubble Time: 56.646 ms, Imbalance Overhead: 20.542 ms
GPU 7, Compute+Comm Time: 262.639 ms, Bubble Time: 57.207 ms, Imbalance Overhead: 4.452 ms
The estimated cost of the whole pipeline: 516.573 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 720.161 ms
Partition 0 [0, 8) has cost: 720.161 ms
Partition 1 [8, 16) has cost: 698.200 ms
Partition 2 [16, 24) has cost: 698.200 ms
Partition 3 [24, 32) has cost: 693.345 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.964 ms
GPU 0, Compute+Comm Time: 144.271 ms, Bubble Time: 27.366 ms, Imbalance Overhead: 2.327 ms
GPU 1, Compute+Comm Time: 140.802 ms, Bubble Time: 26.812 ms, Imbalance Overhead: 6.350 ms
GPU 2, Compute+Comm Time: 140.802 ms, Bubble Time: 26.728 ms, Imbalance Overhead: 6.434 ms
GPU 3, Compute+Comm Time: 139.962 ms, Bubble Time: 26.486 ms, Imbalance Overhead: 7.516 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 323.361 ms
GPU 0, Compute+Comm Time: 259.416 ms, Bubble Time: 49.591 ms, Imbalance Overhead: 14.354 ms
GPU 1, Compute+Comm Time: 260.855 ms, Bubble Time: 49.776 ms, Imbalance Overhead: 12.731 ms
GPU 2, Compute+Comm Time: 260.855 ms, Bubble Time: 49.719 ms, Imbalance Overhead: 12.787 ms
GPU 3, Compute+Comm Time: 269.391 ms, Bubble Time: 50.739 ms, Imbalance Overhead: 3.232 ms
    The estimated cost with 2 DP ways is 522.192 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1418.361 ms
Partition 0 [0, 16) has cost: 1418.361 ms
Partition 1 [16, 32) has cost: 1391.545 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 221.412 ms
GPU 0, Compute+Comm Time: 192.856 ms, Bubble Time: 23.731 ms, Imbalance Overhead: 4.825 ms
GPU 1, Compute+Comm Time: 190.762 ms, Bubble Time: 24.278 ms, Imbalance Overhead: 6.372 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 362.616 ms
GPU 0, Compute+Comm Time: 313.478 ms, Bubble Time: 39.966 ms, Imbalance Overhead: 9.172 ms
GPU 1, Compute+Comm Time: 318.613 ms, Bubble Time: 39.048 ms, Imbalance Overhead: 4.954 ms
    The estimated cost with 4 DP ways is 613.229 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2809.906 ms
Partition 0 [0, 32) has cost: 2809.906 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 600.931 ms
GPU 0, Compute+Comm Time: 600.931 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 728.036 ms
GPU 0, Compute+Comm Time: 728.036 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1395.415 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [130, 194)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 66)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [130, 194)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 66)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [194, 257)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [66, 130)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [194, 257)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [66, 130)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[130, 194)...
+++++++++ Node 1 initializing the weights for op[0, 66)...
+++++++++ Node 6 initializing the weights for op[194, 257)...
+++++++++ Node 2 initializing the weights for op[66, 130)...
+++++++++ Node 5 initializing the weights for op[130, 194)...
+++++++++ Node 3 initializing the weights for op[66, 130)...
+++++++++ Node 7 initializing the weights for op[194, 257)...
+++++++++ Node 0 initializing the weights for op[0, 66)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.7145	TrainAcc 0.0317	ValidAcc 0.0278	TestAcc 0.0277	BestValid 0.0278
	Epoch 50:	Loss 3.3204	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 100:	Loss 3.1624	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 150:	Loss 2.8861	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 200:	Loss 3.0353	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 250:	Loss 2.9737	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 300:	Loss 3.2473	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 350:	Loss 3.0666	TrainAcc 0.1076	ValidAcc 0.1466	TestAcc 0.1483	BestValid 0.1466
	Epoch 400:	Loss 2.9662	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 450:	Loss 2.8456	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 500:	Loss 2.7696	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 550:	Loss 2.7018	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1466
	Epoch 600:	Loss 2.7135	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 650:	Loss 2.6375	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 700:	Loss 2.5505	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 750:	Loss 2.4430	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 800:	Loss 2.3365	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 850:	Loss 2.2954	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 900:	Loss 2.2860	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1466
	Epoch 950:	Loss 2.2362	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0452	BestValid 0.1466
	Epoch 1000:	Loss 2.1426	TrainAcc 0.0503	ValidAcc 0.0421	TestAcc 0.0458	BestValid 0.1466
	Epoch 1050:	Loss 2.0999	TrainAcc 0.0516	ValidAcc 0.0437	TestAcc 0.0472	BestValid 0.1466
	Epoch 1100:	Loss 2.0671	TrainAcc 0.0593	ValidAcc 0.0504	TestAcc 0.0550	BestValid 0.1466
	Epoch 1150:	Loss 2.0598	TrainAcc 0.0693	ValidAcc 0.0645	TestAcc 0.0685	BestValid 0.1466
	Epoch 1200:	Loss 2.0607	TrainAcc 0.0580	ValidAcc 0.0499	TestAcc 0.0537	BestValid 0.1466
	Epoch 1250:	Loss 1.9601	TrainAcc 0.0688	ValidAcc 0.0606	TestAcc 0.0653	BestValid 0.1466
	Epoch 1300:	Loss 1.9609	TrainAcc 0.0666	ValidAcc 0.0617	TestAcc 0.0650	BestValid 0.1466
	Epoch 1350:	Loss 1.9295	TrainAcc 0.0703	ValidAcc 0.0696	TestAcc 0.0727	BestValid 0.1466
	Epoch 1400:	Loss 1.9038	TrainAcc 0.0713	ValidAcc 0.0655	TestAcc 0.0695	BestValid 0.1466
	Epoch 1450:	Loss 1.9585	TrainAcc 0.0942	ValidAcc 0.1021	TestAcc 0.1030	BestValid 0.1466
	Epoch 1500:	Loss 1.9560	TrainAcc 0.1926	ValidAcc 0.1908	TestAcc 0.1949	BestValid 0.1908
	Epoch 1550:	Loss 1.8762	TrainAcc 0.1526	ValidAcc 0.1663	TestAcc 0.1649	BestValid 0.1908
	Epoch 1600:	Loss 1.8555	TrainAcc 0.2571	ValidAcc 0.2669	TestAcc 0.2674	BestValid 0.2669
	Epoch 1650:	Loss 1.8508	TrainAcc 0.1450	ValidAcc 0.1319	TestAcc 0.1384	BestValid 0.2669
	Epoch 1700:	Loss 1.8248	TrainAcc 0.1096	ValidAcc 0.1017	TestAcc 0.1065	BestValid 0.2669
	Epoch 1750:	Loss 1.8492	TrainAcc 0.0902	ValidAcc 0.0822	TestAcc 0.0876	BestValid 0.2669
	Epoch 1800:	Loss 1.7898	TrainAcc 0.0838	ValidAcc 0.0741	TestAcc 0.0798	BestValid 0.2669
	Epoch 1850:	Loss 1.7164	TrainAcc 0.0851	ValidAcc 0.0782	TestAcc 0.0848	BestValid 0.2669
	Epoch 1900:	Loss 1.6664	TrainAcc 0.0823	ValidAcc 0.0746	TestAcc 0.0799	BestValid 0.2669
	Epoch 1950:	Loss 1.6496	TrainAcc 0.0672	ValidAcc 0.0634	TestAcc 0.0650	BestValid 0.2669
	Epoch 2000:	Loss 1.6200	TrainAcc 0.0359	ValidAcc 0.0368	TestAcc 0.0382	BestValid 0.2669
	Epoch 2050:	Loss 1.5887	TrainAcc 0.0375	ValidAcc 0.0378	TestAcc 0.0399	BestValid 0.2669
	Epoch 2100:	Loss 1.5025	TrainAcc 0.0382	ValidAcc 0.0385	TestAcc 0.0409	BestValid 0.2669
	Epoch 2150:	Loss 1.4710	TrainAcc 0.0390	ValidAcc 0.0387	TestAcc 0.0406	BestValid 0.2669
	Epoch 2200:	Loss 1.4070	TrainAcc 0.0389	ValidAcc 0.0386	TestAcc 0.0409	BestValid 0.2669
	Epoch 2250:	Loss 1.3754	TrainAcc 0.0382	ValidAcc 0.0381	TestAcc 0.0405	BestValid 0.2669
	Epoch 2300:	Loss 1.2896	TrainAcc 0.0373	ValidAcc 0.0373	TestAcc 0.0398	BestValid 0.2669
	Epoch 2350:	Loss 1.2734	TrainAcc 0.0366	ValidAcc 0.0374	TestAcc 0.0391	BestValid 0.2669
	Epoch 2400:	Loss 1.1996	TrainAcc 0.0367	ValidAcc 0.0376	TestAcc 0.0393	BestValid 0.2669
	Epoch 2450:	Loss 1.1673	TrainAcc 0.0365	ValidAcc 0.0377	TestAcc 0.0393	BestValid 0.2669
	Epoch 2500:	Loss 1.2958	TrainAcc 0.0363	ValidAcc 0.0371	TestAcc 0.0387	BestValid 0.2669
	Epoch 2550:	Loss 1.3368	TrainAcc 0.0361	ValidAcc 0.0371	TestAcc 0.0385	BestValid 0.2669
	Epoch 2600:	Loss 1.3133	TrainAcc 0.0368	ValidAcc 0.0367	TestAcc 0.0391	BestValid 0.2669
	Epoch 2650:	Loss 1.1814	TrainAcc 0.0372	ValidAcc 0.0368	TestAcc 0.0392	BestValid 0.2669
	Epoch 2700:	Loss 1.1011	TrainAcc 0.0420	ValidAcc 0.0609	TestAcc 0.0638	BestValid 0.2669
	Epoch 2750:	Loss 1.1067	TrainAcc 0.1065	ValidAcc 0.0961	TestAcc 0.0978	BestValid 0.2669
	Epoch 2800:	Loss 1.1100	TrainAcc 0.1062	ValidAcc 0.0957	TestAcc 0.0971	BestValid 0.2669
	Epoch 2850:	Loss 1.0763	TrainAcc 0.1055	ValidAcc 0.0948	TestAcc 0.0960	BestValid 0.2669
	Epoch 2900:	Loss 1.2204	TrainAcc 0.1051	ValidAcc 0.0947	TestAcc 0.0954	BestValid 0.2669
	Epoch 2950:	Loss 1.1049	TrainAcc 0.1123	ValidAcc 0.1028	TestAcc 0.1026	BestValid 0.2669
	Epoch 3000:	Loss 1.0395	TrainAcc 0.1052	ValidAcc 0.0959	TestAcc 0.0966	BestValid 0.2669
	Epoch 3050:	Loss 1.0310	TrainAcc 0.1054	ValidAcc 0.0960	TestAcc 0.0967	BestValid 0.2669
	Epoch 3100:	Loss 0.9929	TrainAcc 0.1054	ValidAcc 0.0961	TestAcc 0.0967	BestValid 0.2669
	Epoch 3150:	Loss 0.9784	TrainAcc 0.1057	ValidAcc 0.0956	TestAcc 0.0965	BestValid 0.2669
	Epoch 3200:	Loss 0.9810	TrainAcc 0.1280	ValidAcc 0.1169	TestAcc 0.1166	BestValid 0.2669
	Epoch 3250:	Loss 1.0112	TrainAcc 0.1344	ValidAcc 0.1250	TestAcc 0.1231	BestValid 0.2669
	Epoch 3300:	Loss 0.9813	TrainAcc 0.1260	ValidAcc 0.1155	TestAcc 0.1146	BestValid 0.2669
	Epoch 3350:	Loss 0.9751	TrainAcc 0.1140	ValidAcc 0.1034	TestAcc 0.1032	BestValid 0.2669
	Epoch 3400:	Loss 0.9488	TrainAcc 0.1048	ValidAcc 0.0965	TestAcc 0.0961	BestValid 0.2669
	Epoch 3450:	Loss 0.9468	TrainAcc 0.1076	ValidAcc 0.0987	TestAcc 0.0986	BestValid 0.2669
	Epoch 3500:	Loss 0.9682	TrainAcc 0.1042	ValidAcc 0.0953	TestAcc 0.0956	BestValid 0.2669
	Epoch 3550:	Loss 0.9352	TrainAcc 0.1104	ValidAcc 0.1016	TestAcc 0.1008	BestValid 0.2669
	Epoch 3600:	Loss 0.9256	TrainAcc 0.1185	ValidAcc 0.1139	TestAcc 0.1122	BestValid 0.2669
	Epoch 3650:	Loss 0.9190	TrainAcc 0.1304	ValidAcc 0.1181	TestAcc 0.1174	BestValid 0.2669
	Epoch 3700:	Loss 0.9494	TrainAcc 0.1217	ValidAcc 0.1109	TestAcc 0.1106	BestValid 0.2669
	Epoch 3750:	Loss 1.0905	TrainAcc 0.0753	ValidAcc 0.1083	TestAcc 0.1058	BestValid 0.2669
	Epoch 3800:	Loss 0.9738	TrainAcc 0.0372	ValidAcc 0.0389	TestAcc 0.0396	BestValid 0.2669
	Epoch 3850:	Loss 0.9124	TrainAcc 0.0379	ValidAcc 0.0388	TestAcc 0.0395	BestValid 0.2669
	Epoch 3900:	Loss 0.8947	TrainAcc 0.0458	ValidAcc 0.0457	TestAcc 0.0464	BestValid 0.2669
	Epoch 3950:	Loss 0.8729	TrainAcc 0.0392	ValidAcc 0.0392	TestAcc 0.0398	BestValid 0.2669
	Epoch 4000:	Loss 0.8911	TrainAcc 0.0350	ValidAcc 0.0360	TestAcc 0.0372	BestValid 0.2669
	Epoch 4050:	Loss 0.8991	TrainAcc 0.0469	ValidAcc 0.0464	TestAcc 0.0474	BestValid 0.2669
	Epoch 4100:	Loss 0.8810	TrainAcc 0.0327	ValidAcc 0.0313	TestAcc 0.0314	BestValid 0.2669
	Epoch 4150:	Loss 0.8489	TrainAcc 0.0444	ValidAcc 0.0451	TestAcc 0.0433	BestValid 0.2669
	Epoch 4200:	Loss 0.8491	TrainAcc 0.0443	ValidAcc 0.0443	TestAcc 0.0447	BestValid 0.2669
	Epoch 4250:	Loss 0.8736	TrainAcc 0.0705	ValidAcc 0.1050	TestAcc 0.1009	BestValid 0.2669
	Epoch 4300:	Loss 0.8600	TrainAcc 0.0271	ValidAcc 0.0261	TestAcc 0.0261	BestValid 0.2669
	Epoch 4350:	Loss 0.8438	TrainAcc 0.0358	ValidAcc 0.0317	TestAcc 0.0323	BestValid 0.2669
	Epoch 4400:	Loss 0.8322	TrainAcc 0.0486	ValidAcc 0.0466	TestAcc 0.0466	BestValid 0.2669
	Epoch 4450:	Loss 0.8535	TrainAcc 0.0297	ValidAcc 0.0278	TestAcc 0.0280	BestValid 0.2669
	Epoch 4500:	Loss 0.8316	TrainAcc 0.0342	ValidAcc 0.0354	TestAcc 0.0372	BestValid 0.2669
	Epoch 4550:	Loss 0.8182	TrainAcc 0.0483	ValidAcc 0.0480	TestAcc 0.0493	BestValid 0.2669
	Epoch 4600:	Loss 0.8063	TrainAcc 0.0776	ValidAcc 0.0926	TestAcc 0.0924	BestValid 0.2669
	Epoch 4650:	Loss 0.8058	TrainAcc 0.0979	ValidAcc 0.0860	TestAcc 0.0851	BestValid 0.2669
	Epoch 4700:	Loss 0.8343	TrainAcc 0.0943	ValidAcc 0.0833	TestAcc 0.0818	BestValid 0.2669
	Epoch 4750:	Loss 0.8126	TrainAcc 0.0927	ValidAcc 0.0821	TestAcc 0.0812	BestValid 0.2669
	Epoch 4800:	Loss 0.8379	TrainAcc 0.0364	ValidAcc 0.0394	TestAcc 0.0392	BestValid 0.2669
	Epoch 4850:	Loss 0.8034	TrainAcc 0.0251	ValidAcc 0.0227	TestAcc 0.0237	BestValid 0.2669
	Epoch 4900:	Loss 0.8262	TrainAcc 0.0945	ValidAcc 0.1099	TestAcc 0.1072	BestValid 0.2669
	Epoch 4950:	Loss 0.7795	TrainAcc 0.0267	ValidAcc 0.0221	TestAcc 0.0226	BestValid 0.2669
	Epoch 5000:	Loss 0.8074	TrainAcc 0.0237	ValidAcc 0.0228	TestAcc 0.0227	BestValid 0.2669
****** Epoch Time (Excluding Evaluation Cost): 0.567 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 86.047 ms (Max: 88.886, Min: 81.951, Sum: 688.378)
Cluster-Wide Average, Compute: 373.614 ms (Max: 384.448, Min: 358.546, Sum: 2988.910)
Cluster-Wide Average, Communication-Layer: 16.790 ms (Max: 20.268, Min: 13.826, Sum: 134.318)
Cluster-Wide Average, Bubble-Imbalance: 17.148 ms (Max: 27.188, Min: 9.588, Sum: 137.188)
Cluster-Wide Average, Communication-Graph: 65.203 ms (Max: 69.362, Min: 60.523, Sum: 521.621)
Cluster-Wide Average, Optimization: 2.764 ms (Max: 3.486, Min: 2.355, Sum: 22.110)
Cluster-Wide Average, Others: 5.864 ms (Max: 12.090, Min: 3.568, Sum: 46.912)
****** Breakdown Sum: 567.430 ms ******
Cluster-Wide Average, GPU Memory Consumption: 6.717 GB (Max: 8.128, Min: 6.329, Sum: 53.737)
Cluster-Wide Average, Graph-Level Communication Throughput: 108.445 Gbps (Max: 123.757, Min: 94.615, Sum: 867.563)
Cluster-Wide Average, Layer-Level Communication Throughput: 32.552 Gbps (Max: 41.880, Min: 23.297, Sum: 260.417)
Layer-level communication (cluster-wide, per-epoch): 0.521 GB
Graph-level communication (cluster-wide, per-epoch): 5.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
Total communication (cluster-wide, per-epoch): 5.870 GB
****** Accuracy Results ******
Highest valid_acc: 0.2669
Target test_acc: 0.2674
Epoch to reach the target acc: 1599
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
