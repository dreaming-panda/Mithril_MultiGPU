Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.019 seconds.
Building the Feature Vector...
        It takes 0.030 seconds.
        It takes 0.021 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.029 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.111 seconds.
Building the Label Vector...
        It takes 0.114 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.010 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426)WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
Number of vertices per chunk: 2790
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.991 Gbps (per GPU), 455.928 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.725 Gbps (per GPU), 453.799 Gbps (aggregated)
The layer-level communication performance: 56.726 Gbps (per GPU), 453.808 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.497 Gbps (per GPU), 451.976 Gbps (aggregated)
The layer-level communication performance: 56.461 Gbps (per GPU), 451.686 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.275 Gbps (per GPU), 450.197 Gbps (aggregated)
The layer-level communication performance: 56.234 Gbps (per GPU), 449.875 Gbps (aggregated)
The layer-level communication performance: 56.204 Gbps (per GPU), 449.634 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.472 Gbps (per GPU), 1251.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.483 Gbps (per GPU), 1251.868 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.475 Gbps (per GPU), 1251.799 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.483 Gbps (per GPU), 1251.868 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.480 Gbps (per GPU), 1251.844 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.463 Gbps (per GPU), 1251.706 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.475 Gbps (per GPU), 1251.797 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.486 Gbps (per GPU), 1251.891 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.902 Gbps (per GPU), 807.218 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.901 Gbps (per GPU), 807.205 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.854 Gbps (per GPU), 806.830 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.855 Gbps (per GPU), 806.836 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.687 Gbps (per GPU), 805.493 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.682 Gbps (per GPU), 805.455 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.663 Gbps (per GPU), 805.306 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.633 Gbps (per GPU), 805.062 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.753 Gbps (per GPU), 246.026 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.756 Gbps (per GPU), 246.052 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.752 Gbps (per GPU), 246.017 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.756 Gbps (per GPU), 246.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.753 Gbps (per GPU), 246.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.756 Gbps (per GPU), 246.046 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.754 Gbps (per GPU), 246.032 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.751 Gbps (per GPU), 246.008 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.69ms  0.39ms  0.37ms  1.88  2.81K  0.03M
 chk_1  0.70ms  0.39ms  0.37ms  1.88  2.82K  0.03M
 chk_2  0.70ms  0.39ms  0.37ms  1.90  2.80K  0.03M
 chk_3  0.70ms  0.39ms  0.37ms  1.90  2.80K  0.03M
 chk_4  0.70ms  0.40ms  0.37ms  1.88  2.82K  0.03M
 chk_5  0.70ms  0.40ms  0.37ms  1.88  2.75K  0.03M
 chk_6  0.68ms  0.39ms  0.36ms  1.90  2.71K  0.03M
 chk_7  0.69ms  0.39ms  0.37ms  1.89  2.76K  0.03M
 chk_8  0.69ms  0.39ms  0.37ms  1.88  2.79K  0.03M
 chk_9  0.70ms  0.39ms  0.37ms  1.89  2.81K  0.03M
chk_10  0.69ms  0.39ms  0.42ms  1.77  2.81K  0.03M
chk_11  0.69ms  0.40ms  0.47ms  1.74  2.74K  0.03M
chk_12  0.70ms  0.40ms  0.46ms  1.75  2.76K  0.03M
chk_13  0.69ms  0.40ms  0.45ms  1.75  2.75K  0.03M
chk_14  0.69ms  0.39ms  0.37ms  1.89  2.81K  0.03M
chk_15  0.69ms  0.39ms  0.37ms  1.88  2.77K  0.03M
chk_16  0.69ms  0.39ms  0.37ms  1.88  2.78K  0.03M
chk_17  0.70ms  0.40ms  0.37ms  1.89  2.79K  0.03M
chk_18  0.70ms  0.40ms  0.37ms  1.90  2.82K  0.03M
chk_19  0.69ms  0.39ms  0.36ms  1.91  2.81K  0.03M
chk_20  0.69ms  0.40ms  0.37ms  1.88  2.77K  0.03M
chk_21  0.70ms  0.40ms  0.37ms  1.90  2.84K  0.02M
chk_22  0.69ms  0.40ms  0.37ms  1.88  2.78K  0.03M
chk_23  0.69ms  0.40ms  0.37ms  1.88  2.80K  0.03M
chk_24  0.69ms  0.40ms  0.37ms  1.87  2.80K  0.03M
chk_25  0.69ms  0.39ms  0.36ms  1.90  2.81K  0.03M
chk_26  0.69ms  0.39ms  0.36ms  1.90  2.81K  0.03M
chk_27  0.70ms  0.40ms  0.37ms  1.89  2.79K  0.03M
chk_28  0.70ms  0.40ms  0.37ms  1.89  2.77K  0.03M
chk_29  0.69ms  0.39ms  0.37ms  1.89  2.77K  0.03M
chk_30  0.70ms  0.40ms  0.37ms  1.89  2.80K  0.03M
chk_31  0.70ms  0.40ms  0.37ms  1.90  2.78K  0.03M
   Avg  0.69  0.39  0.38
   Max  0.70  0.40  0.47
   Min  0.68  0.39  0.36
 Ratio  1.03  1.03  1.30
   Var  0.00  0.00  0.00
Profiling takes 0.647 s
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [18, 26)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 10)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 6, starting model training...
Num Stages: 4 / 4
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 10)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [26, 33)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [26, 33)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [10, 18)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [18, 26)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [10, 18)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[18, 26)...
+++++++++ Node 0 initializing the weights for op[0, 10)...
+++++++++ Node 5 initializing the weights for op[18, 26)...
+++++++++ Node 1 initializing the weights for op[0, 10)...
+++++++++ Node 6 initializing the weights for op[26, 33)...
+++++++++ Node 2 initializing the weights for op[10, 18)...
+++++++++ Node 7 initializing the weights for op[26, 33)...
+++++++++ Node 3 initializing the weights for op[10, 18)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 6, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 5.7834	TrainAcc 0.4141	ValidAcc 0.4140	TestAcc 0.4152	BestValid 0.4140
	Epoch 50:	Loss 1.6229	TrainAcc 0.4214	ValidAcc 0.4238	TestAcc 0.4233	BestValid 0.4238
	Epoch 100:	Loss 1.5887	TrainAcc 0.4363	ValidAcc 0.4377	TestAcc 0.4367	BestValid 0.4377
	Epoch 150:	Loss 1.5514	TrainAcc 0.4941	ValidAcc 0.4940	TestAcc 0.4920	BestValid 0.4940
	Epoch 200:	Loss 1.5205	TrainAcc 0.4960	ValidAcc 0.4982	TestAcc 0.4951	BestValid 0.4982
	Epoch 250:	Loss 1.5018	TrainAcc 0.4996	ValidAcc 0.5021	TestAcc 0.5003	BestValid 0.5021
	Epoch 300:	Loss 1.4910	TrainAcc 0.5030	ValidAcc 0.5048	TestAcc 0.5032	BestValid 0.5048
	Epoch 350:	Loss 1.4731	TrainAcc 0.5059	ValidAcc 0.5074	TestAcc 0.5067	BestValid 0.5074
	Epoch 400:	Loss 1.4665	TrainAcc 0.5082	ValidAcc 0.5091	TestAcc 0.5078	BestValid 0.5091
	Epoch 450:	Loss 1.4572	TrainAcc 0.4994	ValidAcc 0.4975	TestAcc 0.4946	BestValid 0.5091
	Epoch 500:	Loss 1.4503	TrainAcc 0.5112	ValidAcc 0.5117	TestAcc 0.5090	BestValid 0.5117
	Epoch 550:	Loss 1.4458	TrainAcc 0.5117	ValidAcc 0.5114	TestAcc 0.5095	BestValid 0.5117
	Epoch 600:	Loss 1.4412	TrainAcc 0.5131	ValidAcc 0.5134	TestAcc 0.5102	BestValid 0.5134
	Epoch 650:	Loss 1.4386	TrainAcc 0.5142	ValidAcc 0.5132	TestAcc 0.5125	BestValid 0.5134
	Epoch 700:	Loss 1.4366	TrainAcc 0.5146	ValidAcc 0.5123	TestAcc 0.5089	BestValid 0.5134
	Epoch 750:	Loss 1.4305	TrainAcc 0.5140	ValidAcc 0.5117	TestAcc 0.5085	BestValid 0.5134
	Epoch 800:	Loss 1.4298	TrainAcc 0.5146	ValidAcc 0.5155	TestAcc 0.5131	BestValid 0.5155
	Epoch 850:	Loss 1.4266	TrainAcc 0.5170	ValidAcc 0.5144	TestAcc 0.5135	BestValid 0.5155
	Epoch 900:	Loss 1.4289	TrainAcc 0.5036	ValidAcc 0.5007	TestAcc 0.4978	BestValid 0.5155
	Epoch 950:	Loss 1.4240	TrainAcc 0.5165	ValidAcc 0.5165	TestAcc 0.5141	BestValid 0.5165
	Epoch 1000:	Loss 1.4227	TrainAcc 0.5173	ValidAcc 0.5165	TestAcc 0.5138	BestValid 0.5165
	Epoch 1050:	Loss 1.4233	TrainAcc 0.5177	ValidAcc 0.5151	TestAcc 0.5141	BestValid 0.5165
	Epoch 1100:	Loss 1.4245	TrainAcc 0.5178	ValidAcc 0.5154	TestAcc 0.5147	BestValid 0.5165
	Epoch 1150:	Loss 1.4201	TrainAcc 0.5193	ValidAcc 0.5173	TestAcc 0.5153	BestValid 0.5173
	Epoch 1200:	Loss 1.4207	TrainAcc 0.5172	ValidAcc 0.5113	TestAcc 0.5110	BestValid 0.5173
	Epoch 1250:	Loss 1.4196	TrainAcc 0.5209	ValidAcc 0.5171	TestAcc 0.5176	BestValid 0.5173
	Epoch 1300:	Loss 1.4170	TrainAcc 0.5210	ValidAcc 0.5177	TestAcc 0.5163	BestValid 0.5177
	Epoch 1350:	Loss 1.4173	TrainAcc 0.5197	ValidAcc 0.5149	TestAcc 0.5139	BestValid 0.5177
	Epoch 1400:	Loss 1.4171	TrainAcc 0.5209	ValidAcc 0.5165	TestAcc 0.5136	BestValid 0.5177
	Epoch 1450:	Loss 1.4151	TrainAcc 0.5219	ValidAcc 0.5173	TestAcc 0.5164	BestValid 0.5177
	Epoch 1500:	Loss 1.4158	TrainAcc 0.5182	ValidAcc 0.5146	TestAcc 0.5150	BestValid 0.5177
	Epoch 1550:	Loss 1.4123	TrainAcc 0.5226	ValidAcc 0.5175	TestAcc 0.5160	BestValid 0.5177
	Epoch 1600:	Loss 1.4135	TrainAcc 0.5206	ValidAcc 0.5164	TestAcc 0.5154	BestValid 0.5177
	Epoch 1650:	Loss 1.4122	TrainAcc 0.5209	ValidAcc 0.5141	TestAcc 0.5153	BestValid 0.5177
	Epoch 1700:	Loss 1.4145	TrainAcc 0.5225	ValidAcc 0.5182	TestAcc 0.5175	BestValid 0.5182
	Epoch 1750:	Loss 1.4100	TrainAcc 0.5236	ValidAcc 0.5190	TestAcc 0.5187	BestValid 0.5190
	Epoch 1800:	Loss 1.4083	TrainAcc 0.5201	ValidAcc 0.5167	TestAcc 0.5142	BestValid 0.5190
	Epoch 1850:	Loss 1.4104	TrainAcc 0.5234	ValidAcc 0.5188	TestAcc 0.5190	BestValid 0.5190
	Epoch 1900:	Loss 1.4056	TrainAcc 0.5236	ValidAcc 0.5181	TestAcc 0.5179	BestValid 0.5190
	Epoch 1950:	Loss 1.4080	TrainAcc 0.5247	ValidAcc 0.5179	TestAcc 0.5186	BestValid 0.5190
	Epoch 2000:	Loss 1.4060	TrainAcc 0.5255	ValidAcc 0.5195	TestAcc 0.5201	BestValid 0.5195
	Epoch 2050:	Loss 1.4057	TrainAcc 0.5256	ValidAcc 0.5190	TestAcc 0.5197	BestValid 0.5195
	Epoch 2100:	Loss 1.4043	TrainAcc 0.5253	ValidAcc 0.5191	TestAcc 0.5197	BestValid 0.5195
	Epoch 2150:	Loss 1.4029	TrainAcc 0.5246	ValidAcc 0.5186	TestAcc 0.5176	BestValid 0.5195
	Epoch 2200:	Loss 1.4085	TrainAcc 0.5225	ValidAcc 0.5156	TestAcc 0.5167	BestValid 0.5195
	Epoch 2250:	Loss 1.4021	TrainAcc 0.5248	ValidAcc 0.5198	TestAcc 0.5189	BestValid 0.5198
	Epoch 2300:	Loss 1.4012	TrainAcc 0.5231	ValidAcc 0.5177	TestAcc 0.5171	BestValid 0.5198
	Epoch 2350:	Loss 1.4001	TrainAcc 0.5250	ValidAcc 0.5193	TestAcc 0.5186	BestValid 0.5198
	Epoch 2400:	Loss 1.4028	TrainAcc 0.5245	ValidAcc 0.5176	TestAcc 0.5184	BestValid 0.5198
	Epoch 2450:	Loss 1.4049	TrainAcc 0.5250	ValidAcc 0.5205	TestAcc 0.5190	BestValid 0.5205
	Epoch 2500:	Loss 1.3944	TrainAcc 0.5266	ValidAcc 0.5204	TestAcc 0.5198	BestValid 0.5205
	Epoch 2550:	Loss 1.4035	TrainAcc 0.5230	ValidAcc 0.5192	TestAcc 0.5162	BestValid 0.5205
	Epoch 2600:	Loss 1.4030	TrainAcc 0.5284	ValidAcc 0.5201	TestAcc 0.5218	BestValid 0.5205
	Epoch 2650:	Loss 1.3988	TrainAcc 0.5260	ValidAcc 0.5197	TestAcc 0.5191	BestValid 0.5205
	Epoch 2700:	Loss 1.3982	TrainAcc 0.5286	ValidAcc 0.5206	TestAcc 0.5223	BestValid 0.5206
	Epoch 2750:	Loss 1.3947	TrainAcc 0.5289	ValidAcc 0.5204	TestAcc 0.5209	BestValid 0.5206
	Epoch 2800:	Loss 1.3949	TrainAcc 0.5284	ValidAcc 0.5208	TestAcc 0.5202	BestValid 0.5208
	Epoch 2850:	Loss 1.3975	TrainAcc 0.5246	ValidAcc 0.5176	TestAcc 0.5169	BestValid 0.5208
	Epoch 2900:	Loss 1.3940	TrainAcc 0.5286	ValidAcc 0.5201	TestAcc 0.5208	BestValid 0.5208
	Epoch 2950:	Loss 1.3940	TrainAcc 0.5264	ValidAcc 0.5216	TestAcc 0.5182	BestValid 0.5216
	Epoch 3000:	Loss 1.3947	TrainAcc 0.5317	ValidAcc 0.5233	TestAcc 0.5227	BestValid 0.5233
	Epoch 3050:	Loss 1.3916	TrainAcc 0.5287	ValidAcc 0.5224	TestAcc 0.5210	BestValid 0.5233
	Epoch 3100:	Loss 1.3922	TrainAcc 0.5313	ValidAcc 0.5233	TestAcc 0.5230	BestValid 0.5233
	Epoch 3150:	Loss 1.3922	TrainAcc 0.5297	ValidAcc 0.5226	TestAcc 0.5224	BestValid 0.5233
	Epoch 3200:	Loss 1.3887	TrainAcc 0.5291	ValidAcc 0.5221	TestAcc 0.5195	BestValid 0.5233
	Epoch 3250:	Loss 1.3894	TrainAcc 0.5295	ValidAcc 0.5201	TestAcc 0.5193	BestValid 0.5233
	Epoch 3300:	Loss 1.3905	TrainAcc 0.5288	ValidAcc 0.5191	TestAcc 0.5194	BestValid 0.5233
	Epoch 3350:	Loss 1.3886	TrainAcc 0.5325	ValidAcc 0.5242	TestAcc 0.5247	BestValid 0.5242
	Epoch 3400:	Loss 1.3887	TrainAcc 0.5320	ValidAcc 0.5229	TestAcc 0.5231	BestValid 0.5242
	Epoch 3450:	Loss 1.3890	TrainAcc 0.5333	ValidAcc 0.5237	TestAcc 0.5244	BestValid 0.5242
	Epoch 3500:	Loss 1.3899	TrainAcc 0.5330	ValidAcc 0.5244	TestAcc 0.5253	BestValid 0.5244
	Epoch 3550:	Loss 1.3866	TrainAcc 0.5316	ValidAcc 0.5205	TestAcc 0.5215	BestValid 0.5244
	Epoch 3600:	Loss 1.3879	TrainAcc 0.5342	ValidAcc 0.5266	TestAcc 0.5266	BestValid 0.5266
	Epoch 3650:	Loss 1.3852	TrainAcc 0.5265	ValidAcc 0.5195	TestAcc 0.5179	BestValid 0.5266
	Epoch 3700:	Loss 1.3853	TrainAcc 0.5323	ValidAcc 0.5213	TestAcc 0.5228	BestValid 0.5266
	Epoch 3750:	Loss 1.3859	TrainAcc 0.5378	ValidAcc 0.5287	TestAcc 0.5283	BestValid 0.5287
	Epoch 3800:	Loss 1.3843	TrainAcc 0.5262	ValidAcc 0.5185	TestAcc 0.5185	BestValid 0.5287
	Epoch 3850:	Loss 1.3795	TrainAcc 0.5348	ValidAcc 0.5227	TestAcc 0.5249	BestValid 0.5287
	Epoch 3900:	Loss 1.3816	TrainAcc 0.5309	ValidAcc 0.5205	TestAcc 0.5205	BestValid 0.5287
	Epoch 3950:	Loss 1.3778	TrainAcc 0.5375	ValidAcc 0.5247	TestAcc 0.5285	BestValid 0.5287
	Epoch 4000:	Loss 1.3803	TrainAcc 0.5376	ValidAcc 0.5256	TestAcc 0.5289	BestValid 0.5287
	Epoch 4050:	Loss 1.3798	TrainAcc 0.5346	ValidAcc 0.5233	TestAcc 0.5251	BestValid 0.5287
	Epoch 4100:	Loss 1.3798	TrainAcc 0.5351	ValidAcc 0.5225	TestAcc 0.5257	BestValid 0.5287
	Epoch 4150:	Loss 1.3806	TrainAcc 0.5351	ValidAcc 0.5232	TestAcc 0.5261	BestValid 0.5287
	Epoch 4200:	Loss 1.3771	TrainAcc 0.5374	ValidAcc 0.5239	TestAcc 0.5275	BestValid 0.5287
	Epoch 4250:	Loss 1.3765	TrainAcc 0.5380	ValidAcc 0.5261	TestAcc 0.5283	BestValid 0.5287
	Epoch 4300:	Loss 1.3782	TrainAcc 0.5382	ValidAcc 0.5249	TestAcc 0.5302	BestValid 0.5287
	Epoch 4350:	Loss 1.3760	TrainAcc 0.5378	ValidAcc 0.5257	TestAcc 0.5271	BestValid 0.5287
	Epoch 4400:	Loss 1.3757	TrainAcc 0.5385	ValidAcc 0.5255	TestAcc 0.5279	BestValid 0.5287
	Epoch 4450:	Loss 1.3779	TrainAcc 0.5408	ValidAcc 0.5268	TestAcc 0.5294	BestValid 0.5287
	Epoch 4500:	Loss 1.3710	TrainAcc 0.5412	ValidAcc 0.5278	TestAcc 0.5310	BestValid 0.5287
	Epoch 4550:	Loss 1.3703	TrainAcc 0.5392	ValidAcc 0.5251	TestAcc 0.5288	BestValid 0.5287
	Epoch 4600:	Loss 1.3716	TrainAcc 0.5389	ValidAcc 0.5257	TestAcc 0.5286	BestValid 0.5287
	Epoch 4650:	Loss 1.3688	TrainAcc 0.5410	ValidAcc 0.5257	TestAcc 0.5288	BestValid 0.5287
	Epoch 4700:	Loss 1.3696	TrainAcc 0.5422	ValidAcc 0.5275	TestAcc 0.5292	BestValid 0.5287
	Epoch 4750:	Loss 1.3710	TrainAcc 0.5423	ValidAcc 0.5302	TestAcc 0.5317	BestValid 0.5302
	Epoch 4800:	Loss 1.3681	TrainAcc 0.5383	ValidAcc 0.5246	TestAcc 0.5257	BestValid 0.5302
	Epoch 4850:	Loss 1.3669	TrainAcc 0.5324	ValidAcc 0.5211	TestAcc 0.5211	BestValid 0.5302
	Epoch 4900:	Loss 1.3653	TrainAcc 0.5361	ValidAcc 0.5232	TestAcc 0.5248	BestValid 0.5302
	Epoch 4950:	Loss 1.3700	TrainAcc 0.5449	ValidAcc 0.5307	TestAcc 0.5318	BestValid 0.5307
	Epoch 5000:	Loss 1.3674	TrainAcc 0.5431	ValidAcc 0.5284	TestAcc 0.5313	BestValid 0.5307
****** Epoch Time (Excluding Evaluation Cost): 0.028 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 4.234 ms (Max: 4.804, Min: 3.690, Sum: 33.871)
Cluster-Wide Average, Compute: 10.194 ms (Max: 13.993, Min: 8.747, Sum: 81.556)
Cluster-Wide Average, Communication-Layer: 4.423 ms (Max: 5.244, Min: 3.586, Sum: 35.386)
Cluster-Wide Average, Bubble-Imbalance: 3.341 ms (Max: 4.770, Min: 0.567, Sum: 26.730)
Cluster-Wide Average, Communication-Graph: 3.515 ms (Max: 3.739, Min: 3.372, Sum: 28.124)
Cluster-Wide Average, Optimization: 1.425 ms (Max: 2.195, Min: 0.076, Sum: 11.402)
Cluster-Wide Average, Others: 1.390 ms (Max: 3.864, Min: 0.277, Sum: 11.122)
****** Breakdown Sum: 28.524 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.372 GB (Max: 2.128, Min: 1.114, Sum: 10.978)
Cluster-Wide Average, Graph-Level Communication Throughput: 93.461 Gbps (Max: 99.176, Min: 85.931, Sum: 747.687)
Cluster-Wide Average, Layer-Level Communication Throughput: 47.103 Gbps (Max: 58.323, Min: 36.764, Sum: 376.824)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 0.224 GB
Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
Total communication (cluster-wide, per-epoch): 0.425 GB
****** Accuracy Results ******
Highest valid_acc: 0.5307
Target test_acc: 0.5318
Epoch to reach the target acc: 4949
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
