Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
        It takes 0.019 seconds.
        It takes 0.021 seconds.
        It takes 0.019 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.024 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.097 seconds.
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCNII layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.012 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
Chunks (number of global chunks: 32):WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.647 Gbps (per GPU), 477.174 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.357 Gbps (per GPU), 474.854 Gbps (aggregated)
The layer-level communication performance: 59.345 Gbps (per GPU), 474.758 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.107 Gbps (per GPU), 472.856 Gbps (aggregated)
The layer-level communication performance: 59.075 Gbps (per GPU), 472.598 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.869 Gbps (per GPU), 470.949 Gbps (aggregated)
The layer-level communication performance: 58.825 Gbps (per GPU), 470.602 Gbps (aggregated)
The layer-level communication performance: 58.794 Gbps (per GPU), 470.352 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.379 Gbps (per GPU), 1259.031 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.394 Gbps (per GPU), 1259.149 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.379 Gbps (per GPU), 1259.034 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.421 Gbps (per GPU), 1259.365 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.394 Gbps (per GPU), 1259.149 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.323 Gbps (per GPU), 1258.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.382 Gbps (per GPU), 1259.055 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.314 Gbps (per GPU), 1258.511 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.514 Gbps (per GPU), 812.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.514 Gbps (per GPU), 812.115 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.513 Gbps (per GPU), 812.102 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.514 Gbps (per GPU), 812.115 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.509 Gbps (per GPU), 812.076 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.511 Gbps (per GPU), 812.089 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.514 Gbps (per GPU), 812.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.512 Gbps (per GPU), 812.095 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.455 Gbps (per GPU), 259.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.456 Gbps (per GPU), 259.650 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.452 Gbps (per GPU), 259.619 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.456 Gbps (per GPU), 259.645 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.453 Gbps (per GPU), 259.621 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.455 Gbps (per GPU), 259.637 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.453 Gbps (per GPU), 259.625 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.447 Gbps (per GPU), 259.574 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.69ms  0.38ms  0.36ms  1.91  2.81K  0.03M
 chk_1  0.69ms  0.39ms  0.36ms  1.90  2.82K  0.03M
 chk_2  0.68ms  0.39ms  0.36ms  1.92  2.80K  0.03M
 chk_3  0.69ms  0.39ms  0.36ms  1.91  2.80K  0.03M
 chk_4  0.69ms  0.39ms  0.36ms  1.91  2.82K  0.03M
 chk_5  0.69ms  0.39ms  0.36ms  1.90  2.75K  0.03M
 chk_6  0.67ms  0.38ms  0.35ms  1.91  2.71K  0.03M
 chk_7  0.68ms  0.39ms  0.36ms  1.91  2.76K  0.03M
 chk_8  0.68ms  0.39ms  0.36ms  1.91  2.79K  0.03M
 chk_9  0.69ms  0.38ms  0.36ms  1.92  2.81K  0.03M
chk_10  0.69ms  0.38ms  0.36ms  1.93  2.81K  0.03M
chk_11  0.69ms  0.39ms  0.36ms  1.89  2.74K  0.03M
chk_12  0.69ms  0.39ms  0.36ms  1.91  2.76K  0.03M
chk_13  0.69ms  0.39ms  0.36ms  1.92  2.75K  0.03M
chk_14  0.69ms  0.38ms  0.36ms  1.92  2.81K  0.03M
chk_15  0.68ms  0.39ms  0.36ms  1.90  2.77K  0.03M
chk_16  0.68ms  0.38ms  0.36ms  1.91  2.78K  0.03M
chk_17  0.69ms  0.39ms  0.36ms  1.91  2.79K  0.03M
chk_18  0.69ms  0.39ms  0.36ms  1.92  2.82K  0.03M
chk_19  0.68ms  0.38ms  0.35ms  1.92  2.81K  0.03M
chk_20  0.69ms  0.39ms  0.36ms  1.91  2.77K  0.03M
chk_21  0.69ms  0.39ms  0.36ms  1.92  2.84K  0.02M
chk_22  0.69ms  0.39ms  0.36ms  1.91  2.78K  0.03M
chk_23  0.69ms  0.39ms  0.36ms  1.91  2.80K  0.03M
chk_24  0.68ms  0.39ms  0.36ms  1.90  2.80K  0.03M
chk_25  0.69ms  0.38ms  0.36ms  1.93  2.81K  0.03M
chk_26  0.69ms  0.38ms  0.36ms  1.92  2.81K  0.03M
chk_27  0.69ms  0.39ms  0.36ms  1.92  2.79K  0.03M
chk_28  0.69ms  0.39ms  0.36ms  1.91  2.77K  0.03M
chk_29  0.68ms  0.38ms  0.36ms  1.91  2.77K  0.03M
chk_30  0.69ms  0.39ms  0.36ms  1.91  2.80K  0.03M
chk_31  0.69ms  0.39ms  0.36ms  1.92  2.78K  0.03M
   Avg  0.69  0.39  0.36
   Max  0.69  0.39  0.36
   Min  0.67  0.38  0.35
 Ratio  1.03  1.03  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.635 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 10)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 10)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [10, 18)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [10, 18)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [18, 26)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [18, 26)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [26, 33)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [26, 33)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[18, 26)...
+++++++++ Node 6 initializing the weights for op[26, 33)...
+++++++++ Node 0 initializing the weights for op[0, 10)...
+++++++++ Node 2 initializing the weights for op[10, 18)...
+++++++++ Node 1 initializing the weights for op[0, 10)...
+++++++++ Node 3 initializing the weights for op[10, 18)...
+++++++++ Node 5 initializing the weights for op[18, 26)...
+++++++++ Node 7 initializing the weights for op[26, 33)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 6.2965	TrainAcc 0.3953	ValidAcc 0.3939	TestAcc 0.3954	BestValid 0.3939
	Epoch 50:	Loss 1.6181	TrainAcc 0.4221	ValidAcc 0.4246	TestAcc 0.4238	BestValid 0.4246
	Epoch 100:	Loss 1.5797	TrainAcc 0.4371	ValidAcc 0.4403	TestAcc 0.4388	BestValid 0.4403
	Epoch 150:	Loss 1.5299	TrainAcc 0.4936	ValidAcc 0.4941	TestAcc 0.4965	BestValid 0.4941
	Epoch 200:	Loss 1.5112	TrainAcc 0.4994	ValidAcc 0.5002	TestAcc 0.5006	BestValid 0.5002
	Epoch 250:	Loss 1.4906	TrainAcc 0.4842	ValidAcc 0.4849	TestAcc 0.4857	BestValid 0.5002
	Epoch 300:	Loss 1.4781	TrainAcc 0.4972	ValidAcc 0.4981	TestAcc 0.4959	BestValid 0.5002
	Epoch 350:	Loss 1.4706	TrainAcc 0.5038	ValidAcc 0.5032	TestAcc 0.5013	BestValid 0.5032
	Epoch 400:	Loss 1.4602	TrainAcc 0.5048	ValidAcc 0.5046	TestAcc 0.5015	BestValid 0.5046
	Epoch 450:	Loss 1.4551	TrainAcc 0.4959	ValidAcc 0.4952	TestAcc 0.4931	BestValid 0.5046
	Epoch 500:	Loss 1.4456	TrainAcc 0.4991	ValidAcc 0.4968	TestAcc 0.4970	BestValid 0.5046
	Epoch 550:	Loss 1.4408	TrainAcc 0.4869	ValidAcc 0.4855	TestAcc 0.4859	BestValid 0.5046
	Epoch 600:	Loss 1.4428	TrainAcc 0.4928	ValidAcc 0.4902	TestAcc 0.4912	BestValid 0.5046
	Epoch 650:	Loss 1.4386	TrainAcc 0.5059	ValidAcc 0.5026	TestAcc 0.5030	BestValid 0.5046
	Epoch 700:	Loss 1.4329	TrainAcc 0.5000	ValidAcc 0.4974	TestAcc 0.4974	BestValid 0.5046
	Epoch 750:	Loss 1.4331	TrainAcc 0.5112	ValidAcc 0.5075	TestAcc 0.5073	BestValid 0.5075
	Epoch 800:	Loss 1.4292	TrainAcc 0.5026	ValidAcc 0.4983	TestAcc 0.5002	BestValid 0.5075
	Epoch 850:	Loss 1.4268	TrainAcc 0.5028	ValidAcc 0.4990	TestAcc 0.5006	BestValid 0.5075
	Epoch 900:	Loss 1.4275	TrainAcc 0.5058	ValidAcc 0.5026	TestAcc 0.5024	BestValid 0.5075
	Epoch 950:	Loss 1.4234	TrainAcc 0.5065	ValidAcc 0.5026	TestAcc 0.5037	BestValid 0.5075
	Epoch 1000:	Loss 1.4205	TrainAcc 0.5094	ValidAcc 0.5053	TestAcc 0.5049	BestValid 0.5075
	Epoch 1050:	Loss 1.4219	TrainAcc 0.5116	ValidAcc 0.5071	TestAcc 0.5070	BestValid 0.5075
	Epoch 1100:	Loss 1.4234	TrainAcc 0.5018	ValidAcc 0.4979	TestAcc 0.4991	BestValid 0.5075
	Epoch 1150:	Loss 1.4179	TrainAcc 0.5135	ValidAcc 0.5077	TestAcc 0.5085	BestValid 0.5077
	Epoch 1200:	Loss 1.4207	TrainAcc 0.5018	ValidAcc 0.4978	TestAcc 0.4979	BestValid 0.5077
	Epoch 1250:	Loss 1.4148	TrainAcc 0.5070	ValidAcc 0.5028	TestAcc 0.5032	BestValid 0.5077
	Epoch 1300:	Loss 1.4132	TrainAcc 0.5130	ValidAcc 0.5089	TestAcc 0.5081	BestValid 0.5089
	Epoch 1350:	Loss 1.4175	TrainAcc 0.5106	ValidAcc 0.5061	TestAcc 0.5044	BestValid 0.5089
	Epoch 1400:	Loss 1.4133	TrainAcc 0.5067	ValidAcc 0.5020	TestAcc 0.5020	BestValid 0.5089
	Epoch 1450:	Loss 1.4157	TrainAcc 0.5179	ValidAcc 0.5127	TestAcc 0.5117	BestValid 0.5127
	Epoch 1500:	Loss 1.4126	TrainAcc 0.5177	ValidAcc 0.5117	TestAcc 0.5101	BestValid 0.5127
	Epoch 1550:	Loss 1.4145	TrainAcc 0.5110	ValidAcc 0.5059	TestAcc 0.5065	BestValid 0.5127
	Epoch 1600:	Loss 1.4138	TrainAcc 0.5013	ValidAcc 0.4974	TestAcc 0.4976	BestValid 0.5127
	Epoch 1650:	Loss 1.4102	TrainAcc 0.5225	ValidAcc 0.5177	TestAcc 0.5151	BestValid 0.5177
	Epoch 1700:	Loss 1.4110	TrainAcc 0.5088	ValidAcc 0.5033	TestAcc 0.5029	BestValid 0.5177
	Epoch 1750:	Loss 1.4063	TrainAcc 0.5096	ValidAcc 0.5037	TestAcc 0.5058	BestValid 0.5177
	Epoch 1800:	Loss 1.4075	TrainAcc 0.5224	ValidAcc 0.5173	TestAcc 0.5147	BestValid 0.5177
	Epoch 1850:	Loss 1.4043	TrainAcc 0.5131	ValidAcc 0.5070	TestAcc 0.5079	BestValid 0.5177
	Epoch 1900:	Loss 1.4049	TrainAcc 0.5144	ValidAcc 0.5076	TestAcc 0.5085	BestValid 0.5177
	Epoch 1950:	Loss 1.4066	TrainAcc 0.5105	ValidAcc 0.5043	TestAcc 0.5070	BestValid 0.5177
	Epoch 2000:	Loss 1.4008	TrainAcc 0.5184	ValidAcc 0.5121	TestAcc 0.5111	BestValid 0.5177
	Epoch 2050:	Loss 1.4012	TrainAcc 0.5096	ValidAcc 0.5033	TestAcc 0.5033	BestValid 0.5177
	Epoch 2100:	Loss 1.4058	TrainAcc 0.5247	ValidAcc 0.5188	TestAcc 0.5175	BestValid 0.5188
	Epoch 2150:	Loss 1.3991	TrainAcc 0.5034	ValidAcc 0.4966	TestAcc 0.4995	BestValid 0.5188
	Epoch 2200:	Loss 1.4026	TrainAcc 0.5285	ValidAcc 0.5221	TestAcc 0.5213	BestValid 0.5221
	Epoch 2250:	Loss 1.4020	TrainAcc 0.5260	ValidAcc 0.5189	TestAcc 0.5171	BestValid 0.5221
	Epoch 2300:	Loss 1.4027	TrainAcc 0.5119	ValidAcc 0.5042	TestAcc 0.5067	BestValid 0.5221
	Epoch 2350:	Loss 1.3981	TrainAcc 0.5204	ValidAcc 0.5141	TestAcc 0.5128	BestValid 0.5221
	Epoch 2400:	Loss 1.3999	TrainAcc 0.5167	ValidAcc 0.5093	TestAcc 0.5101	BestValid 0.5221
	Epoch 2450:	Loss 1.4022	TrainAcc 0.5134	ValidAcc 0.5053	TestAcc 0.5077	BestValid 0.5221
	Epoch 2500:	Loss 1.3947	TrainAcc 0.5141	ValidAcc 0.5063	TestAcc 0.5082	BestValid 0.5221
	Epoch 2550:	Loss 1.4003	TrainAcc 0.5262	ValidAcc 0.5182	TestAcc 0.5170	BestValid 0.5221
	Epoch 2600:	Loss 1.3961	TrainAcc 0.5177	ValidAcc 0.5099	TestAcc 0.5119	BestValid 0.5221
	Epoch 2650:	Loss 1.3941	TrainAcc 0.5152	ValidAcc 0.5076	TestAcc 0.5084	BestValid 0.5221
	Epoch 2700:	Loss 1.3970	TrainAcc 0.5283	ValidAcc 0.5207	TestAcc 0.5192	BestValid 0.5221
	Epoch 2750:	Loss 1.3959	TrainAcc 0.5291	ValidAcc 0.5214	TestAcc 0.5213	BestValid 0.5221
	Epoch 2800:	Loss 1.3909	TrainAcc 0.5250	ValidAcc 0.5167	TestAcc 0.5165	BestValid 0.5221
	Epoch 2850:	Loss 1.3944	TrainAcc 0.5133	ValidAcc 0.5048	TestAcc 0.5070	BestValid 0.5221
	Epoch 2900:	Loss 1.3947	TrainAcc 0.5179	ValidAcc 0.5096	TestAcc 0.5107	BestValid 0.5221
	Epoch 2950:	Loss 1.3948	TrainAcc 0.5252	ValidAcc 0.5178	TestAcc 0.5154	BestValid 0.5221
	Epoch 3000:	Loss 1.3957	TrainAcc 0.5199	ValidAcc 0.5120	TestAcc 0.5121	BestValid 0.5221
	Epoch 3050:	Loss 1.3920	TrainAcc 0.5233	ValidAcc 0.5149	TestAcc 0.5160	BestValid 0.5221
	Epoch 3100:	Loss 1.3932	TrainAcc 0.5126	ValidAcc 0.5044	TestAcc 0.5060	BestValid 0.5221
	Epoch 3150:	Loss 1.3917	TrainAcc 0.5117	ValidAcc 0.5041	TestAcc 0.5058	BestValid 0.5221
	Epoch 3200:	Loss 1.3902	TrainAcc 0.5067	ValidAcc 0.5003	TestAcc 0.5011	BestValid 0.5221
	Epoch 3250:	Loss 1.3925	TrainAcc 0.5246	ValidAcc 0.5165	TestAcc 0.5166	BestValid 0.5221
	Epoch 3300:	Loss 1.3879	TrainAcc 0.5245	ValidAcc 0.5168	TestAcc 0.5170	BestValid 0.5221
	Epoch 3350:	Loss 1.3895	TrainAcc 0.5150	ValidAcc 0.5059	TestAcc 0.5076	BestValid 0.5221
	Epoch 3400:	Loss 1.3873	TrainAcc 0.5261	ValidAcc 0.5167	TestAcc 0.5179	BestValid 0.5221
	Epoch 3450:	Loss 1.3890	TrainAcc 0.5212	ValidAcc 0.5115	TestAcc 0.5123	BestValid 0.5221
	Epoch 3500:	Loss 1.3868	TrainAcc 0.5167	ValidAcc 0.5078	TestAcc 0.5080	BestValid 0.5221
	Epoch 3550:	Loss 1.3859	TrainAcc 0.5234	ValidAcc 0.5135	TestAcc 0.5147	BestValid 0.5221
	Epoch 3600:	Loss 1.3845	TrainAcc 0.5123	ValidAcc 0.5049	TestAcc 0.5063	BestValid 0.5221
	Epoch 3650:	Loss 1.3830	TrainAcc 0.5095	ValidAcc 0.5019	TestAcc 0.5014	BestValid 0.5221
	Epoch 3700:	Loss 1.3870	TrainAcc 0.5313	ValidAcc 0.5231	TestAcc 0.5226	BestValid 0.5231
	Epoch 3750:	Loss 1.3856	TrainAcc 0.5357	ValidAcc 0.5254	TestAcc 0.5280	BestValid 0.5254
	Epoch 3800:	Loss 1.3871	TrainAcc 0.5093	ValidAcc 0.5012	TestAcc 0.5014	BestValid 0.5254
	Epoch 3850:	Loss 1.3840	TrainAcc 0.5261	ValidAcc 0.5170	TestAcc 0.5165	BestValid 0.5254
	Epoch 3900:	Loss 1.3788	TrainAcc 0.5276	ValidAcc 0.5174	TestAcc 0.5169	BestValid 0.5254
	Epoch 3950:	Loss 1.3811	TrainAcc 0.5183	ValidAcc 0.5105	TestAcc 0.5100	BestValid 0.5254
	Epoch 4000:	Loss 1.3804	TrainAcc 0.5262	ValidAcc 0.5172	TestAcc 0.5175	BestValid 0.5254
	Epoch 4050:	Loss 1.3829	TrainAcc 0.5308	ValidAcc 0.5214	TestAcc 0.5206	BestValid 0.5254
	Epoch 4100:	Loss 1.3780	TrainAcc 0.5329	ValidAcc 0.5240	TestAcc 0.5231	BestValid 0.5254
	Epoch 4150:	Loss 1.3774	TrainAcc 0.5262	ValidAcc 0.5169	TestAcc 0.5167	BestValid 0.5254
	Epoch 4200:	Loss 1.3765	TrainAcc 0.5262	ValidAcc 0.5170	TestAcc 0.5154	BestValid 0.5254
	Epoch 4250:	Loss 1.3817	TrainAcc 0.5357	ValidAcc 0.5264	TestAcc 0.5273	BestValid 0.5264
	Epoch 4300:	Loss 1.3757	TrainAcc 0.5273	ValidAcc 0.5178	TestAcc 0.5177	BestValid 0.5264
	Epoch 4350:	Loss 1.3769	TrainAcc 0.5223	ValidAcc 0.5130	TestAcc 0.5121	BestValid 0.5264
	Epoch 4400:	Loss 1.3750	TrainAcc 0.5371	ValidAcc 0.5264	TestAcc 0.5261	BestValid 0.5264
	Epoch 4450:	Loss 1.3748	TrainAcc 0.5346	ValidAcc 0.5249	TestAcc 0.5254	BestValid 0.5264
	Epoch 4500:	Loss 1.3752	TrainAcc 0.5378	ValidAcc 0.5272	TestAcc 0.5288	BestValid 0.5272
	Epoch 4550:	Loss 1.3735	TrainAcc 0.5236	ValidAcc 0.5138	TestAcc 0.5147	BestValid 0.5272
	Epoch 4600:	Loss 1.3704	TrainAcc 0.5245	ValidAcc 0.5147	TestAcc 0.5129	BestValid 0.5272
	Epoch 4650:	Loss 1.3709	TrainAcc 0.5131	ValidAcc 0.5036	TestAcc 0.5039	BestValid 0.5272
	Epoch 4700:	Loss 1.3677	TrainAcc 0.5297	ValidAcc 0.5197	TestAcc 0.5196	BestValid 0.5272
	Epoch 4750:	Loss 1.3665	TrainAcc 0.5268	ValidAcc 0.5170	TestAcc 0.5151	BestValid 0.5272
	Epoch 4800:	Loss 1.3679	TrainAcc 0.5277	ValidAcc 0.5175	TestAcc 0.5170	BestValid 0.5272
	Epoch 4850:	Loss 1.3673	TrainAcc 0.5322	ValidAcc 0.5213	TestAcc 0.5214	BestValid 0.5272
	Epoch 4900:	Loss 1.3701	TrainAcc 0.5375	ValidAcc 0.5240	TestAcc 0.5252	BestValid 0.5272
	Epoch 4950:	Loss 1.3685	TrainAcc 0.5408	ValidAcc 0.5296	TestAcc 0.5295	BestValid 0.5296
	Epoch 5000:	Loss 1.3646	TrainAcc 0.5411	ValidAcc 0.5303	TestAcc 0.5318	BestValid 0.5303
****** Epoch Time (Excluding Evaluation Cost): 0.028 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 4.200 ms (Max: 4.751, Min: 3.634, Sum: 33.601)
Cluster-Wide Average, Compute: 10.113 ms (Max: 13.908, Min: 8.677, Sum: 80.906)
Cluster-Wide Average, Communication-Layer: 4.425 ms (Max: 5.207, Min: 3.609, Sum: 35.404)
Cluster-Wide Average, Bubble-Imbalance: 3.330 ms (Max: 4.711, Min: 0.607, Sum: 26.636)
Cluster-Wide Average, Communication-Graph: 3.464 ms (Max: 3.612, Min: 3.380, Sum: 27.713)
Cluster-Wide Average, Optimization: 1.378 ms (Max: 2.035, Min: 0.077, Sum: 11.024)
Cluster-Wide Average, Others: 1.371 ms (Max: 3.798, Min: 0.356, Sum: 10.966)
****** Breakdown Sum: 28.281 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.372 GB (Max: 2.128, Min: 1.114, Sum: 10.978)
Cluster-Wide Average, Graph-Level Communication Throughput: 94.718 Gbps (Max: 99.649, Min: 86.703, Sum: 757.744)
Cluster-Wide Average, Layer-Level Communication Throughput: 47.078 Gbps (Max: 58.260, Min: 36.644, Sum: 376.626)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 0.224 GB
Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
Total communication (cluster-wide, per-epoch): 0.425 GB
****** Accuracy Results ******
Highest valid_acc: 0.5303
Target test_acc: 0.5318
Epoch to reach the target acc: 4999
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
