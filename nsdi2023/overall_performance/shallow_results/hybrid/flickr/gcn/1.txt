Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
        It takes 0.017 seconds.
        It takes 0.022 seconds.
        It takes 0.024 seconds.
Building the Feature Vector...
        It takes 0.017 seconds.
        It takes 0.026 seconds.
Building the Feature Vector...
        It takes 0.019 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCN layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626)WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.060 Gbps (per GPU), 480.477 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.768 Gbps (per GPU), 478.147 Gbps (aggregated)
The layer-level communication performance: 59.753 Gbps (per GPU), 478.022 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.492 Gbps (per GPU), 475.938 Gbps (aggregated)
The layer-level communication performance: 59.464 Gbps (per GPU), 475.710 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.248 Gbps (per GPU), 473.987 Gbps (aggregated)
The layer-level communication performance: 59.199 Gbps (per GPU), 473.594 Gbps (aggregated)
The layer-level communication performance: 59.167 Gbps (per GPU), 473.334 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.352 Gbps (per GPU), 1250.818 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.361 Gbps (per GPU), 1250.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.335 Gbps (per GPU), 1250.680 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.312 Gbps (per GPU), 1250.493 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.349 Gbps (per GPU), 1250.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.361 Gbps (per GPU), 1250.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.335 Gbps (per GPU), 1250.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.326 Gbps (per GPU), 1250.608 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.204 Gbps (per GPU), 809.633 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.196 Gbps (per GPU), 809.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.204 Gbps (per GPU), 809.633 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.203 Gbps (per GPU), 809.620 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.203 Gbps (per GPU), 809.627 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.207 Gbps (per GPU), 809.653 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.205 Gbps (per GPU), 809.640 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.182 Gbps (per GPU), 809.457 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.878 Gbps (per GPU), 255.023 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.876 Gbps (per GPU), 255.008 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.876 Gbps (per GPU), 255.008 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.873 Gbps (per GPU), 254.988 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.872 Gbps (per GPU), 254.975 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.874 Gbps (per GPU), 254.992 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.878 Gbps (per GPU), 255.024 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.877 Gbps (per GPU), 255.018 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.26ms  0.22ms  1.46  2.81K  0.03M
 chk_1  0.32ms  0.27ms  0.22ms  1.45  2.82K  0.03M
 chk_2  0.32ms  0.27ms  0.22ms  1.46  2.80K  0.03M
 chk_3  0.32ms  0.27ms  0.22ms  1.46  2.80K  0.03M
 chk_4  0.32ms  0.27ms  0.22ms  1.45  2.82K  0.03M
 chk_5  0.33ms  0.27ms  0.23ms  1.44  2.75K  0.03M
 chk_6  0.31ms  0.26ms  0.21ms  1.45  2.71K  0.03M
 chk_7  0.32ms  0.26ms  0.22ms  1.45  2.76K  0.03M
 chk_8  0.32ms  0.27ms  0.22ms  1.44  2.79K  0.03M
 chk_9  0.32ms  0.27ms  0.22ms  1.45  2.81K  0.03M
chk_10  0.35ms  0.26ms  0.21ms  1.64  2.81K  0.03M
chk_11  0.39ms  0.27ms  0.22ms  1.74  2.74K  0.03M
chk_12  0.39ms  0.27ms  0.22ms  1.74  2.76K  0.03M
chk_13  0.37ms  0.27ms  0.22ms  1.67  2.75K  0.03M
chk_14  0.32ms  0.26ms  0.22ms  1.46  2.81K  0.03M
chk_15  0.32ms  0.26ms  0.22ms  1.45  2.77K  0.03M
chk_16  0.32ms  0.26ms  0.22ms  1.44  2.78K  0.03M
chk_17  0.32ms  0.27ms  0.22ms  1.44  2.79K  0.03M
chk_18  0.32ms  0.27ms  0.22ms  1.45  2.82K  0.03M
chk_19  0.31ms  0.26ms  0.22ms  1.45  2.81K  0.03M
chk_20  0.32ms  0.27ms  0.22ms  1.45  2.77K  0.03M
chk_21  0.32ms  0.27ms  0.22ms  1.46  2.84K  0.02M
chk_22  0.32ms  0.26ms  0.22ms  1.44  2.78K  0.03M
chk_23  0.32ms  0.27ms  0.22ms  1.45  2.80K  0.03M
chk_24  0.32ms  0.26ms  0.22ms  1.45  2.80K  0.03M
chk_25  0.32ms  0.26ms  0.22ms  1.46  2.81K  0.03M
chk_26  0.32ms  0.26ms  0.22ms  1.46  2.81K  0.03M
chk_27  0.32ms  0.27ms  0.22ms  1.44  2.79K  0.03M
chk_28  0.32ms  0.27ms  0.22ms  1.44  2.77K  0.03M
chk_29  0.32ms  0.27ms  0.22ms  1.44  2.77K  0.03M
chk_30  0.32ms  0.27ms  0.22ms  1.45  2.80K  0.03M
chk_31  0.32ms  0.27ms  0.22ms  1.45  2.78K  0.03M
   Avg  0.33  0.27  0.22
   Max  0.39  0.27  0.23
   Min  0.31  0.26  0.21
 Ratio  1.25  1.04  1.05
   Var  0.00  0.00  0.00
Profiling takes 0.412 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 6)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 6)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [6, 11)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [6, 11)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [11, 16)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [11, 16)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 7, starting model training...
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [16, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [16, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[11, 16)...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 5 initializing the weights for op[11, 16)...
+++++++++ Node 1 initializing the weights for op[0, 6)...
+++++++++ Node 6 initializing the weights for op[16, 20)...
+++++++++ Node 2 initializing the weights for op[6, 11)...
+++++++++ Node 7 initializing the weights for op[16, 20)...
+++++++++ Node 3 initializing the weights for op[6, 11)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.0183	TrainAcc 0.4208	ValidAcc 0.4231	TestAcc 0.4228	BestValid 0.4231
	Epoch 50:	Loss 1.6025	TrainAcc 0.2596	ValidAcc 0.2592	TestAcc 0.2563	BestValid 0.4231
	Epoch 100:	Loss 1.5333	TrainAcc 0.4936	ValidAcc 0.4975	TestAcc 0.4921	BestValid 0.4975
	Epoch 150:	Loss 1.5151	TrainAcc 0.4816	ValidAcc 0.4847	TestAcc 0.4823	BestValid 0.4975
	Epoch 200:	Loss 1.5017	TrainAcc 0.4951	ValidAcc 0.4980	TestAcc 0.4933	BestValid 0.4980
	Epoch 250:	Loss 1.4683	TrainAcc 0.5008	ValidAcc 0.5001	TestAcc 0.4942	BestValid 0.5001
	Epoch 300:	Loss 1.4556	TrainAcc 0.5032	ValidAcc 0.5035	TestAcc 0.4974	BestValid 0.5035
	Epoch 350:	Loss 1.4488	TrainAcc 0.5043	ValidAcc 0.5039	TestAcc 0.4998	BestValid 0.5039
	Epoch 400:	Loss 1.4403	TrainAcc 0.4921	ValidAcc 0.4915	TestAcc 0.4890	BestValid 0.5039
	Epoch 450:	Loss 1.4415	TrainAcc 0.4953	ValidAcc 0.4944	TestAcc 0.4933	BestValid 0.5039
	Epoch 500:	Loss 1.4331	TrainAcc 0.5064	ValidAcc 0.5051	TestAcc 0.5033	BestValid 0.5051
	Epoch 550:	Loss 1.4291	TrainAcc 0.5068	ValidAcc 0.5054	TestAcc 0.5011	BestValid 0.5054
	Epoch 600:	Loss 1.4286	TrainAcc 0.5025	ValidAcc 0.5018	TestAcc 0.4994	BestValid 0.5054
	Epoch 650:	Loss 1.4258	TrainAcc 0.5062	ValidAcc 0.5048	TestAcc 0.5037	BestValid 0.5054
	Epoch 700:	Loss 1.4218	TrainAcc 0.4992	ValidAcc 0.4970	TestAcc 0.4939	BestValid 0.5054
	Epoch 750:	Loss 1.4192	TrainAcc 0.5121	ValidAcc 0.5105	TestAcc 0.5076	BestValid 0.5105
	Epoch 800:	Loss 1.4173	TrainAcc 0.5091	ValidAcc 0.5072	TestAcc 0.5070	BestValid 0.5105
	Epoch 850:	Loss 1.4161	TrainAcc 0.5131	ValidAcc 0.5103	TestAcc 0.5080	BestValid 0.5105
	Epoch 900:	Loss 1.4129	TrainAcc 0.4952	ValidAcc 0.4890	TestAcc 0.4866	BestValid 0.5105
	Epoch 950:	Loss 1.4109	TrainAcc 0.5105	ValidAcc 0.5083	TestAcc 0.5060	BestValid 0.5105
	Epoch 1000:	Loss 1.4106	TrainAcc 0.5132	ValidAcc 0.5078	TestAcc 0.5098	BestValid 0.5105
	Epoch 1050:	Loss 1.4056	TrainAcc 0.5182	ValidAcc 0.5135	TestAcc 0.5106	BestValid 0.5135
	Epoch 1100:	Loss 1.4058	TrainAcc 0.5200	ValidAcc 0.5143	TestAcc 0.5118	BestValid 0.5143
	Epoch 1150:	Loss 1.4012	TrainAcc 0.5210	ValidAcc 0.5130	TestAcc 0.5110	BestValid 0.5143
	Epoch 1200:	Loss 1.3986	TrainAcc 0.5161	ValidAcc 0.5087	TestAcc 0.5061	BestValid 0.5143
	Epoch 1250:	Loss 1.3990	TrainAcc 0.5182	ValidAcc 0.5109	TestAcc 0.5133	BestValid 0.5143
	Epoch 1300:	Loss 1.3985	TrainAcc 0.5101	ValidAcc 0.5013	TestAcc 0.5017	BestValid 0.5143
	Epoch 1350:	Loss 1.3946	TrainAcc 0.5179	ValidAcc 0.5079	TestAcc 0.5113	BestValid 0.5143
	Epoch 1400:	Loss 1.3921	TrainAcc 0.5264	ValidAcc 0.5173	TestAcc 0.5169	BestValid 0.5173
	Epoch 1450:	Loss 1.3912	TrainAcc 0.5168	ValidAcc 0.5060	TestAcc 0.5057	BestValid 0.5173
	Epoch 1500:	Loss 1.3898	TrainAcc 0.5234	ValidAcc 0.5112	TestAcc 0.5093	BestValid 0.5173
	Epoch 1550:	Loss 1.3870	TrainAcc 0.5263	ValidAcc 0.5135	TestAcc 0.5157	BestValid 0.5173
	Epoch 1600:	Loss 1.3880	TrainAcc 0.5245	ValidAcc 0.5118	TestAcc 0.5136	BestValid 0.5173
	Epoch 1650:	Loss 1.3843	TrainAcc 0.5256	ValidAcc 0.5151	TestAcc 0.5119	BestValid 0.5173
	Epoch 1700:	Loss 1.3837	TrainAcc 0.5273	ValidAcc 0.5132	TestAcc 0.5151	BestValid 0.5173
	Epoch 1750:	Loss 1.3816	TrainAcc 0.5088	ValidAcc 0.4939	TestAcc 0.4962	BestValid 0.5173
	Epoch 1800:	Loss 1.3769	TrainAcc 0.5246	ValidAcc 0.5089	TestAcc 0.5108	BestValid 0.5173
	Epoch 1850:	Loss 1.3778	TrainAcc 0.5306	ValidAcc 0.5119	TestAcc 0.5123	BestValid 0.5173
	Epoch 1900:	Loss 1.3748	TrainAcc 0.5327	ValidAcc 0.5158	TestAcc 0.5153	BestValid 0.5173
	Epoch 1950:	Loss 1.3767	TrainAcc 0.5341	ValidAcc 0.5183	TestAcc 0.5166	BestValid 0.5183
	Epoch 2000:	Loss 1.3742	TrainAcc 0.5269	ValidAcc 0.5096	TestAcc 0.5104	BestValid 0.5183
	Epoch 2050:	Loss 1.3728	TrainAcc 0.5285	ValidAcc 0.5125	TestAcc 0.5132	BestValid 0.5183
	Epoch 2100:	Loss 1.3700	TrainAcc 0.5337	ValidAcc 0.5173	TestAcc 0.5172	BestValid 0.5183
	Epoch 2150:	Loss 1.3695	TrainAcc 0.5384	ValidAcc 0.5199	TestAcc 0.5197	BestValid 0.5199
	Epoch 2200:	Loss 1.3686	TrainAcc 0.5303	ValidAcc 0.5092	TestAcc 0.5108	BestValid 0.5199
	Epoch 2250:	Loss 1.3666	TrainAcc 0.5387	ValidAcc 0.5188	TestAcc 0.5184	BestValid 0.5199
	Epoch 2300:	Loss 1.3683	TrainAcc 0.5389	ValidAcc 0.5166	TestAcc 0.5167	BestValid 0.5199
	Epoch 2350:	Loss 1.3688	TrainAcc 0.5351	ValidAcc 0.5169	TestAcc 0.5167	BestValid 0.5199
	Epoch 2400:	Loss 1.3674	TrainAcc 0.5229	ValidAcc 0.5035	TestAcc 0.5038	BestValid 0.5199
	Epoch 2450:	Loss 1.3644	TrainAcc 0.5326	ValidAcc 0.5137	TestAcc 0.5147	BestValid 0.5199
	Epoch 2500:	Loss 1.3635	TrainAcc 0.5258	ValidAcc 0.5026	TestAcc 0.5037	BestValid 0.5199
	Epoch 2550:	Loss 1.3601	TrainAcc 0.5400	ValidAcc 0.5201	TestAcc 0.5201	BestValid 0.5201
	Epoch 2600:	Loss 1.3587	TrainAcc 0.5309	ValidAcc 0.5096	TestAcc 0.5077	BestValid 0.5201
	Epoch 2650:	Loss 1.3587	TrainAcc 0.5409	ValidAcc 0.5161	TestAcc 0.5189	BestValid 0.5201
	Epoch 2700:	Loss 1.3565	TrainAcc 0.5407	ValidAcc 0.5187	TestAcc 0.5197	BestValid 0.5201
	Epoch 2750:	Loss 1.3555	TrainAcc 0.5343	ValidAcc 0.5072	TestAcc 0.5094	BestValid 0.5201
	Epoch 2800:	Loss 1.3524	TrainAcc 0.5441	ValidAcc 0.5209	TestAcc 0.5199	BestValid 0.5209
	Epoch 2850:	Loss 1.3527	TrainAcc 0.5435	ValidAcc 0.5184	TestAcc 0.5197	BestValid 0.5209
	Epoch 2900:	Loss 1.3517	TrainAcc 0.5420	ValidAcc 0.5128	TestAcc 0.5158	BestValid 0.5209
	Epoch 2950:	Loss 1.3519	TrainAcc 0.5057	ValidAcc 0.4823	TestAcc 0.4849	BestValid 0.5209
	Epoch 3000:	Loss 1.3471	TrainAcc 0.5364	ValidAcc 0.5096	TestAcc 0.5122	BestValid 0.5209
	Epoch 3050:	Loss 1.3472	TrainAcc 0.5416	ValidAcc 0.5143	TestAcc 0.5147	BestValid 0.5209
	Epoch 3100:	Loss 1.3485	TrainAcc 0.5416	ValidAcc 0.5161	TestAcc 0.5200	BestValid 0.5209
	Epoch 3150:	Loss 1.3456	TrainAcc 0.5400	ValidAcc 0.5094	TestAcc 0.5132	BestValid 0.5209
	Epoch 3200:	Loss 1.3464	TrainAcc 0.5519	ValidAcc 0.5249	TestAcc 0.5242	BestValid 0.5249
	Epoch 3250:	Loss 1.3454	TrainAcc 0.5468	ValidAcc 0.5195	TestAcc 0.5198	BestValid 0.5249
	Epoch 3300:	Loss 1.3390	TrainAcc 0.5403	ValidAcc 0.5169	TestAcc 0.5200	BestValid 0.5249
	Epoch 3350:	Loss 1.3382	TrainAcc 0.5459	ValidAcc 0.5170	TestAcc 0.5201	BestValid 0.5249
	Epoch 3400:	Loss 1.3417	TrainAcc 0.5445	ValidAcc 0.5134	TestAcc 0.5162	BestValid 0.5249
	Epoch 3450:	Loss 1.3369	TrainAcc 0.5485	ValidAcc 0.5119	TestAcc 0.5150	BestValid 0.5249
	Epoch 3500:	Loss 1.3359	TrainAcc 0.5529	ValidAcc 0.5185	TestAcc 0.5220	BestValid 0.5249
	Epoch 3550:	Loss 1.3361	TrainAcc 0.5385	ValidAcc 0.5025	TestAcc 0.5054	BestValid 0.5249
	Epoch 3600:	Loss 1.3348	TrainAcc 0.5507	ValidAcc 0.5144	TestAcc 0.5184	BestValid 0.5249
	Epoch 3650:	Loss 1.3339	TrainAcc 0.5397	ValidAcc 0.4998	TestAcc 0.5022	BestValid 0.5249
	Epoch 3700:	Loss 1.3319	TrainAcc 0.5556	ValidAcc 0.5195	TestAcc 0.5216	BestValid 0.5249
	Epoch 3750:	Loss 1.3302	TrainAcc 0.5579	ValidAcc 0.5276	TestAcc 0.5278	BestValid 0.5276
	Epoch 3800:	Loss 1.3277	TrainAcc 0.5565	ValidAcc 0.5191	TestAcc 0.5229	BestValid 0.5276
	Epoch 3850:	Loss 1.3272	TrainAcc 0.5582	ValidAcc 0.5245	TestAcc 0.5251	BestValid 0.5276
	Epoch 3900:	Loss 1.3238	TrainAcc 0.5524	ValidAcc 0.5160	TestAcc 0.5187	BestValid 0.5276
	Epoch 3950:	Loss 1.3265	TrainAcc 0.5572	ValidAcc 0.5203	TestAcc 0.5229	BestValid 0.5276
	Epoch 4000:	Loss 1.3248	TrainAcc 0.5546	ValidAcc 0.5124	TestAcc 0.5163	BestValid 0.5276
	Epoch 4050:	Loss 1.3247	TrainAcc 0.4917	ValidAcc 0.4573	TestAcc 0.4628	BestValid 0.5276
	Epoch 4100:	Loss 1.3217	TrainAcc 0.5614	ValidAcc 0.5262	TestAcc 0.5268	BestValid 0.5276
	Epoch 4150:	Loss 1.3205	TrainAcc 0.5418	ValidAcc 0.5024	TestAcc 0.5032	BestValid 0.5276
	Epoch 4200:	Loss 1.3191	TrainAcc 0.5569	ValidAcc 0.5117	TestAcc 0.5172	BestValid 0.5276
	Epoch 4250:	Loss 1.3179	TrainAcc 0.5552	ValidAcc 0.5111	TestAcc 0.5148	BestValid 0.5276
	Epoch 4300:	Loss 1.3162	TrainAcc 0.5579	ValidAcc 0.5147	TestAcc 0.5198	BestValid 0.5276
	Epoch 4350:	Loss 1.3159	TrainAcc 0.5592	ValidAcc 0.5134	TestAcc 0.5189	BestValid 0.5276
	Epoch 4400:	Loss 1.3151	TrainAcc 0.5389	ValidAcc 0.5095	TestAcc 0.5123	BestValid 0.5276
	Epoch 4450:	Loss 1.3133	TrainAcc 0.5649	ValidAcc 0.5209	TestAcc 0.5240	BestValid 0.5276
	Epoch 4500:	Loss 1.3100	TrainAcc 0.5509	ValidAcc 0.5013	TestAcc 0.5050	BestValid 0.5276
	Epoch 4550:	Loss 1.3074	TrainAcc 0.5610	ValidAcc 0.5130	TestAcc 0.5184	BestValid 0.5276
	Epoch 4600:	Loss 1.3080	TrainAcc 0.5585	ValidAcc 0.5130	TestAcc 0.5204	BestValid 0.5276
	Epoch 4650:	Loss 1.3079	TrainAcc 0.5500	ValidAcc 0.5089	TestAcc 0.5166	BestValid 0.5276
	Epoch 4700:	Loss 1.3071	TrainAcc 0.5645	ValidAcc 0.5198	TestAcc 0.5227	BestValid 0.5276
	Epoch 4750:	Loss 1.3069	TrainAcc 0.5597	ValidAcc 0.5080	TestAcc 0.5110	BestValid 0.5276
	Epoch 4800:	Loss 1.3048	TrainAcc 0.5647	ValidAcc 0.5163	TestAcc 0.5192	BestValid 0.5276
	Epoch 4850:	Loss 1.3031	TrainAcc 0.5498	ValidAcc 0.5042	TestAcc 0.5116	BestValid 0.5276
	Epoch 4900:	Loss 1.3036	TrainAcc 0.5453	ValidAcc 0.5047	TestAcc 0.5103	BestValid 0.5276
	Epoch 4950:	Loss 1.3002	TrainAcc 0.5638	ValidAcc 0.5087	TestAcc 0.5149	BestValid 0.5276
	Epoch 5000:	Loss 1.2978	TrainAcc 0.5518	ValidAcc 0.5071	TestAcc 0.5133	BestValid 0.5276
****** Epoch Time (Excluding Evaluation Cost): 0.020 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 3.447 ms (Max: 3.834, Min: 2.463, Sum: 27.580)
Cluster-Wide Average, Compute: 5.879 ms (Max: 6.941, Min: 5.320, Sum: 47.035)
Cluster-Wide Average, Communication-Layer: 4.240 ms (Max: 4.987, Min: 3.462, Sum: 33.920)
Cluster-Wide Average, Bubble-Imbalance: 1.825 ms (Max: 2.483, Min: 1.226, Sum: 14.598)
Cluster-Wide Average, Communication-Graph: 3.449 ms (Max: 3.624, Min: 3.312, Sum: 27.593)
Cluster-Wide Average, Optimization: 0.610 ms (Max: 0.988, Min: 0.044, Sum: 4.879)
Cluster-Wide Average, Others: 1.079 ms (Max: 2.830, Min: 0.333, Sum: 8.630)
****** Breakdown Sum: 20.529 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.339 GB (Max: 1.850, Min: 1.130, Sum: 10.708)
Cluster-Wide Average, Graph-Level Communication Throughput: 95.660 Gbps (Max: 100.321, Min: 88.776, Sum: 765.277)
Cluster-Wide Average, Layer-Level Communication Throughput: 49.176 Gbps (Max: 62.049, Min: 37.939, Sum: 393.405)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 0.224 GB
Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
Total communication (cluster-wide, per-epoch): 0.424 GB
****** Accuracy Results ******
Highest valid_acc: 0.5276
Target test_acc: 0.5278
Epoch to reach the target acc: 3749
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
