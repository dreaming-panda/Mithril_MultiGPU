Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.017 seconds.
        It takes 0.017 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
        It takes 0.022 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.095 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.097 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/flickr/32_parts
The number of GCN layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.115 seconds.
Building the Label Vector...
        It takes 0.112 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800)WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.580 Gbps (per GPU), 484.638 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.298 Gbps (per GPU), 482.382 Gbps (aggregated)
The layer-level communication performance: 60.287 Gbps (per GPU), 482.295 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.057 Gbps (per GPU), 480.458 Gbps (aggregated)
The layer-level communication performance: 60.026 Gbps (per GPU), 480.210 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.799 Gbps (per GPU), 478.393 Gbps (aggregated)
The layer-level communication performance: 59.750 Gbps (per GPU), 478.000 Gbps (aggregated)
The layer-level communication performance: 59.719 Gbps (per GPU), 477.753 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.974 Gbps (per GPU), 1247.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.971 Gbps (per GPU), 1247.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.980 Gbps (per GPU), 1247.841 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.974 Gbps (per GPU), 1247.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.966 Gbps (per GPU), 1247.725 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.000 Gbps (per GPU), 1248.003 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.963 Gbps (per GPU), 1247.701 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 154.983 Gbps (per GPU), 1239.864 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.414 Gbps (per GPU), 803.314 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.420 Gbps (per GPU), 803.359 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.417 Gbps (per GPU), 803.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.417 Gbps (per GPU), 803.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.413 Gbps (per GPU), 803.308 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.417 Gbps (per GPU), 803.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.417 Gbps (per GPU), 803.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.413 Gbps (per GPU), 803.308 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.670 Gbps (per GPU), 253.363 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.671 Gbps (per GPU), 253.368 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.670 Gbps (per GPU), 253.363 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.669 Gbps (per GPU), 253.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.667 Gbps (per GPU), 253.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.666 Gbps (per GPU), 253.325 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.667 Gbps (per GPU), 253.334 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.668 Gbps (per GPU), 253.342 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.27ms  0.22ms  1.45  2.81K  0.03M
 chk_1  0.32ms  0.27ms  0.22ms  1.46  2.82K  0.03M
 chk_2  0.32ms  0.27ms  0.22ms  1.45  2.80K  0.03M
 chk_3  0.32ms  0.27ms  0.22ms  1.45  2.80K  0.03M
 chk_4  0.32ms  0.27ms  0.22ms  1.44  2.82K  0.03M
 chk_5  0.32ms  0.27ms  0.23ms  1.43  2.75K  0.03M
 chk_6  0.31ms  0.26ms  0.22ms  1.44  2.71K  0.03M
 chk_7  0.32ms  0.27ms  0.22ms  1.44  2.76K  0.03M
 chk_8  0.32ms  0.27ms  0.22ms  1.44  2.79K  0.03M
 chk_9  0.32ms  0.27ms  0.22ms  1.45  2.81K  0.03M
chk_10  0.32ms  0.26ms  0.22ms  1.47  2.81K  0.03M
chk_11  0.32ms  0.27ms  0.22ms  1.43  2.74K  0.03M
chk_12  0.32ms  0.27ms  0.22ms  1.44  2.76K  0.03M
chk_13  0.32ms  0.27ms  0.22ms  1.44  2.75K  0.03M
chk_14  0.32ms  0.26ms  0.22ms  1.45  2.81K  0.03M
chk_15  0.32ms  0.27ms  0.22ms  1.45  2.77K  0.03M
chk_16  0.32ms  0.26ms  0.22ms  1.45  2.78K  0.03M
chk_17  0.32ms  0.27ms  0.22ms  1.44  2.79K  0.03M
chk_18  0.32ms  0.27ms  0.22ms  1.45  2.82K  0.03M
chk_19  0.32ms  0.26ms  0.22ms  1.46  2.81K  0.03M
chk_20  0.32ms  0.27ms  0.22ms  1.45  2.77K  0.03M
chk_21  0.32ms  0.27ms  0.22ms  1.46  2.84K  0.02M
chk_22  0.32ms  0.27ms  0.22ms  1.44  2.78K  0.03M
chk_23  0.32ms  0.27ms  0.22ms  1.45  2.80K  0.03M
chk_24  0.32ms  0.27ms  0.22ms  1.45  2.80K  0.03M
chk_25  0.32ms  0.26ms  0.22ms  1.46  2.81K  0.03M
chk_26  0.31ms  0.26ms  0.22ms  1.46  2.81K  0.03M
chk_27  0.32ms  0.27ms  0.22ms  1.44  2.79K  0.03M
chk_28  0.32ms  0.27ms  0.22ms  1.44  2.77K  0.03M
chk_29  0.32ms  0.27ms  0.22ms  1.44  2.77K  0.03M
chk_30  0.32ms  0.27ms  0.22ms  1.45  2.80K  0.03M
chk_31  0.32ms  0.27ms  0.22ms  1.45  2.78K  0.03M
   Avg  0.32  0.27  0.22
   Max  0.32  0.27  0.23
   Min  0.31  0.26  0.22
 Ratio  1.04  1.04  1.05
   Var  0.00  0.00  0.00
Profiling takes 0.391 s
*** Node 4, starting model training...
Num Stages: 4 / 4
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 6)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [11, 16)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 6)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [16, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [6, 11)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [16, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 44517, Num Local Vertices: 44733
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [6, 11)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 44517, Num Local Vertices: 44733
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [11, 16)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 44517
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[11, 16)...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 5 initializing the weights for op[11, 16)...
+++++++++ Node 2 initializing the weights for op[6, 11)...
+++++++++ Node 7 initializing the weights for op[16, 20)...
+++++++++ Node 3 initializing the weights for op[6, 11)...
+++++++++ Node 1 initializing the weights for op[0, 6)...
+++++++++ Node 6 initializing the weights for op[16, 20)...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 300780
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 6, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 7, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.0194	TrainAcc 0.2578	ValidAcc 0.2591	TestAcc 0.2552	BestValid 0.2591
	Epoch 50:	Loss 1.6089	TrainAcc 0.4319	ValidAcc 0.4352	TestAcc 0.4354	BestValid 0.4352
	Epoch 100:	Loss 1.5377	TrainAcc 0.4953	ValidAcc 0.4988	TestAcc 0.4934	BestValid 0.4988
	Epoch 150:	Loss 1.5026	TrainAcc 0.4971	ValidAcc 0.4993	TestAcc 0.4969	BestValid 0.4993
	Epoch 200:	Loss 1.4680	TrainAcc 0.4858	ValidAcc 0.4848	TestAcc 0.4814	BestValid 0.4993
	Epoch 250:	Loss 1.4542	TrainAcc 0.4992	ValidAcc 0.5000	TestAcc 0.4954	BestValid 0.5000
	Epoch 300:	Loss 1.4508	TrainAcc 0.4963	ValidAcc 0.4974	TestAcc 0.4925	BestValid 0.5000
	Epoch 350:	Loss 1.4455	TrainAcc 0.4867	ValidAcc 0.4858	TestAcc 0.4828	BestValid 0.5000
	Epoch 400:	Loss 1.4411	TrainAcc 0.4996	ValidAcc 0.5002	TestAcc 0.4954	BestValid 0.5002
	Epoch 450:	Loss 1.4359	TrainAcc 0.5016	ValidAcc 0.4997	TestAcc 0.4985	BestValid 0.5002
	Epoch 500:	Loss 1.4324	TrainAcc 0.5023	ValidAcc 0.5000	TestAcc 0.4992	BestValid 0.5002
	Epoch 550:	Loss 1.4269	TrainAcc 0.5078	ValidAcc 0.5070	TestAcc 0.5068	BestValid 0.5070
	Epoch 600:	Loss 1.4256	TrainAcc 0.4771	ValidAcc 0.4738	TestAcc 0.4709	BestValid 0.5070
	Epoch 650:	Loss 1.4221	TrainAcc 0.4883	ValidAcc 0.4866	TestAcc 0.4814	BestValid 0.5070
	Epoch 700:	Loss 1.4190	TrainAcc 0.4934	ValidAcc 0.4905	TestAcc 0.4889	BestValid 0.5070
	Epoch 750:	Loss 1.4185	TrainAcc 0.5036	ValidAcc 0.4998	TestAcc 0.4965	BestValid 0.5070
	Epoch 800:	Loss 1.4142	TrainAcc 0.5072	ValidAcc 0.5052	TestAcc 0.4998	BestValid 0.5070
	Epoch 850:	Loss 1.4092	TrainAcc 0.4936	ValidAcc 0.4905	TestAcc 0.4864	BestValid 0.5070
	Epoch 900:	Loss 1.4069	TrainAcc 0.5051	ValidAcc 0.5046	TestAcc 0.4969	BestValid 0.5070
	Epoch 950:	Loss 1.4075	TrainAcc 0.5094	ValidAcc 0.5061	TestAcc 0.5032	BestValid 0.5070
	Epoch 1000:	Loss 1.4033	TrainAcc 0.5181	ValidAcc 0.5107	TestAcc 0.5076	BestValid 0.5107
	Epoch 1050:	Loss 1.4028	TrainAcc 0.5202	ValidAcc 0.5131	TestAcc 0.5098	BestValid 0.5131
	Epoch 1100:	Loss 1.3995	TrainAcc 0.5225	ValidAcc 0.5141	TestAcc 0.5106	BestValid 0.5141
	Epoch 1150:	Loss 1.3988	TrainAcc 0.5215	ValidAcc 0.5140	TestAcc 0.5109	BestValid 0.5141
	Epoch 1200:	Loss 1.3972	TrainAcc 0.5248	ValidAcc 0.5186	TestAcc 0.5141	BestValid 0.5186
	Epoch 1250:	Loss 1.3986	TrainAcc 0.4641	ValidAcc 0.4598	TestAcc 0.4588	BestValid 0.5186
	Epoch 1300:	Loss 1.3928	TrainAcc 0.5227	ValidAcc 0.5143	TestAcc 0.5131	BestValid 0.5186
	Epoch 1350:	Loss 1.3927	TrainAcc 0.5183	ValidAcc 0.5099	TestAcc 0.5078	BestValid 0.5186
	Epoch 1400:	Loss 1.3884	TrainAcc 0.5229	ValidAcc 0.5149	TestAcc 0.5128	BestValid 0.5186
	Epoch 1450:	Loss 1.3878	TrainAcc 0.5232	ValidAcc 0.5134	TestAcc 0.5106	BestValid 0.5186
	Epoch 1500:	Loss 1.3860	TrainAcc 0.5185	ValidAcc 0.5085	TestAcc 0.5078	BestValid 0.5186
	Epoch 1550:	Loss 1.3815	TrainAcc 0.5296	ValidAcc 0.5186	TestAcc 0.5165	BestValid 0.5186
	Epoch 1600:	Loss 1.3820	TrainAcc 0.5315	ValidAcc 0.5212	TestAcc 0.5172	BestValid 0.5212
	Epoch 1650:	Loss 1.3788	TrainAcc 0.5289	ValidAcc 0.5189	TestAcc 0.5141	BestValid 0.5212
	Epoch 1700:	Loss 1.3794	TrainAcc 0.5288	ValidAcc 0.5171	TestAcc 0.5167	BestValid 0.5212
	Epoch 1750:	Loss 1.3759	TrainAcc 0.5076	ValidAcc 0.4960	TestAcc 0.4984	BestValid 0.5212
	Epoch 1800:	Loss 1.3768	TrainAcc 0.5179	ValidAcc 0.5038	TestAcc 0.5050	BestValid 0.5212
	Epoch 1850:	Loss 1.3733	TrainAcc 0.5322	ValidAcc 0.5156	TestAcc 0.5161	BestValid 0.5212
	Epoch 1900:	Loss 1.3740	TrainAcc 0.5354	ValidAcc 0.5192	TestAcc 0.5168	BestValid 0.5212
	Epoch 1950:	Loss 1.3716	TrainAcc 0.5329	ValidAcc 0.5180	TestAcc 0.5165	BestValid 0.5212
	Epoch 2000:	Loss 1.3698	TrainAcc 0.5297	ValidAcc 0.5130	TestAcc 0.5128	BestValid 0.5212
	Epoch 2050:	Loss 1.3703	TrainAcc 0.5339	ValidAcc 0.5205	TestAcc 0.5184	BestValid 0.5212
	Epoch 2100:	Loss 1.3657	TrainAcc 0.5337	ValidAcc 0.5138	TestAcc 0.5142	BestValid 0.5212
	Epoch 2150:	Loss 1.3652	TrainAcc 0.5388	ValidAcc 0.5201	TestAcc 0.5205	BestValid 0.5212
	Epoch 2200:	Loss 1.3635	TrainAcc 0.5317	ValidAcc 0.5160	TestAcc 0.5160	BestValid 0.5212
	Epoch 2250:	Loss 1.3639	TrainAcc 0.5341	ValidAcc 0.5143	TestAcc 0.5155	BestValid 0.5212
	Epoch 2300:	Loss 1.3626	TrainAcc 0.5382	ValidAcc 0.5169	TestAcc 0.5179	BestValid 0.5212
	Epoch 2350:	Loss 1.3599	TrainAcc 0.5384	ValidAcc 0.5183	TestAcc 0.5193	BestValid 0.5212
	Epoch 2400:	Loss 1.3590	TrainAcc 0.5399	ValidAcc 0.5195	TestAcc 0.5189	BestValid 0.5212
	Epoch 2450:	Loss 1.3580	TrainAcc 0.5433	ValidAcc 0.5237	TestAcc 0.5231	BestValid 0.5237
	Epoch 2500:	Loss 1.3550	TrainAcc 0.5401	ValidAcc 0.5221	TestAcc 0.5218	BestValid 0.5237
	Epoch 2550:	Loss 1.3547	TrainAcc 0.5302	ValidAcc 0.5134	TestAcc 0.5141	BestValid 0.5237
	Epoch 2600:	Loss 1.3541	TrainAcc 0.5341	ValidAcc 0.5121	TestAcc 0.5142	BestValid 0.5237
	Epoch 2650:	Loss 1.3506	TrainAcc 0.5376	ValidAcc 0.5170	TestAcc 0.5175	BestValid 0.5237
	Epoch 2700:	Loss 1.3495	TrainAcc 0.5412	ValidAcc 0.5176	TestAcc 0.5184	BestValid 0.5237
	Epoch 2750:	Loss 1.3491	TrainAcc 0.5419	ValidAcc 0.5169	TestAcc 0.5201	BestValid 0.5237
	Epoch 2800:	Loss 1.3473	TrainAcc 0.5443	ValidAcc 0.5229	TestAcc 0.5230	BestValid 0.5237
	Epoch 2850:	Loss 1.3490	TrainAcc 0.5379	ValidAcc 0.5124	TestAcc 0.5155	BestValid 0.5237
	Epoch 2900:	Loss 1.3445	TrainAcc 0.5166	ValidAcc 0.4933	TestAcc 0.4937	BestValid 0.5237
	Epoch 2950:	Loss 1.3481	TrainAcc 0.5472	ValidAcc 0.5191	TestAcc 0.5217	BestValid 0.5237
	Epoch 3000:	Loss 1.3454	TrainAcc 0.5328	ValidAcc 0.5069	TestAcc 0.5074	BestValid 0.5237
	Epoch 3050:	Loss 1.3424	TrainAcc 0.5450	ValidAcc 0.5201	TestAcc 0.5197	BestValid 0.5237
	Epoch 3100:	Loss 1.3417	TrainAcc 0.5390	ValidAcc 0.5118	TestAcc 0.5129	BestValid 0.5237
	Epoch 3150:	Loss 1.3381	TrainAcc 0.5446	ValidAcc 0.5203	TestAcc 0.5190	BestValid 0.5237
	Epoch 3200:	Loss 1.3371	TrainAcc 0.5479	ValidAcc 0.5206	TestAcc 0.5225	BestValid 0.5237
	Epoch 3250:	Loss 1.3370	TrainAcc 0.5436	ValidAcc 0.5200	TestAcc 0.5197	BestValid 0.5237
	Epoch 3300:	Loss 1.3358	TrainAcc 0.5299	ValidAcc 0.5044	TestAcc 0.5023	BestValid 0.5237
	Epoch 3350:	Loss 1.3365	TrainAcc 0.5283	ValidAcc 0.4979	TestAcc 0.5032	BestValid 0.5237
	Epoch 3400:	Loss 1.3355	TrainAcc 0.5546	ValidAcc 0.5261	TestAcc 0.5263	BestValid 0.5261
	Epoch 3450:	Loss 1.3333	TrainAcc 0.5516	ValidAcc 0.5215	TestAcc 0.5224	BestValid 0.5261
	Epoch 3500:	Loss 1.3305	TrainAcc 0.5480	ValidAcc 0.5171	TestAcc 0.5215	BestValid 0.5261
	Epoch 3550:	Loss 1.3301	TrainAcc 0.5432	ValidAcc 0.5096	TestAcc 0.5125	BestValid 0.5261
	Epoch 3600:	Loss 1.3293	TrainAcc 0.5408	ValidAcc 0.5156	TestAcc 0.5171	BestValid 0.5261
	Epoch 3650:	Loss 1.3274	TrainAcc 0.5446	ValidAcc 0.5117	TestAcc 0.5104	BestValid 0.5261
	Epoch 3700:	Loss 1.3293	TrainAcc 0.5349	ValidAcc 0.5034	TestAcc 0.5083	BestValid 0.5261
	Epoch 3750:	Loss 1.3283	TrainAcc 0.5306	ValidAcc 0.5118	TestAcc 0.5083	BestValid 0.5261
	Epoch 3800:	Loss 1.3259	TrainAcc 0.5512	ValidAcc 0.5201	TestAcc 0.5218	BestValid 0.5261
	Epoch 3850:	Loss 1.3244	TrainAcc 0.5188	ValidAcc 0.4792	TestAcc 0.4853	BestValid 0.5261
	Epoch 3900:	Loss 1.3220	TrainAcc 0.5466	ValidAcc 0.5087	TestAcc 0.5111	BestValid 0.5261
	Epoch 3950:	Loss 1.3210	TrainAcc 0.5585	ValidAcc 0.5209	TestAcc 0.5213	BestValid 0.5261
	Epoch 4000:	Loss 1.3182	TrainAcc 0.5600	ValidAcc 0.5228	TestAcc 0.5233	BestValid 0.5261
	Epoch 4050:	Loss 1.3230	TrainAcc 0.4914	ValidAcc 0.4628	TestAcc 0.4662	BestValid 0.5261
	Epoch 4100:	Loss 1.3226	TrainAcc 0.5377	ValidAcc 0.4976	TestAcc 0.4998	BestValid 0.5261
	Epoch 4150:	Loss 1.3158	TrainAcc 0.5432	ValidAcc 0.5037	TestAcc 0.5025	BestValid 0.5261
	Epoch 4200:	Loss 1.3152	TrainAcc 0.5511	ValidAcc 0.5096	TestAcc 0.5121	BestValid 0.5261
	Epoch 4250:	Loss 1.3171	TrainAcc 0.5407	ValidAcc 0.4977	TestAcc 0.5032	BestValid 0.5261
	Epoch 4300:	Loss 1.3156	TrainAcc 0.5574	ValidAcc 0.5178	TestAcc 0.5191	BestValid 0.5261
	Epoch 4350:	Loss 1.3136	TrainAcc 0.5542	ValidAcc 0.5134	TestAcc 0.5178	BestValid 0.5261
	Epoch 4400:	Loss 1.3120	TrainAcc 0.5512	ValidAcc 0.5141	TestAcc 0.5131	BestValid 0.5261
	Epoch 4450:	Loss 1.3098	TrainAcc 0.5648	ValidAcc 0.5251	TestAcc 0.5254	BestValid 0.5261
	Epoch 4500:	Loss 1.3109	TrainAcc 0.5487	ValidAcc 0.5094	TestAcc 0.5127	BestValid 0.5261
	Epoch 4550:	Loss 1.3070	TrainAcc 0.5558	ValidAcc 0.5097	TestAcc 0.5126	BestValid 0.5261
	Epoch 4600:	Loss 1.3100	TrainAcc 0.5493	ValidAcc 0.5039	TestAcc 0.5091	BestValid 0.5261
	Epoch 4650:	Loss 1.3039	TrainAcc 0.5657	ValidAcc 0.5222	TestAcc 0.5246	BestValid 0.5261
	Epoch 4700:	Loss 1.3084	TrainAcc 0.5537	ValidAcc 0.5063	TestAcc 0.5079	BestValid 0.5261
	Epoch 4750:	Loss 1.3008	TrainAcc 0.5649	ValidAcc 0.5195	TestAcc 0.5205	BestValid 0.5261
	Epoch 4800:	Loss 1.3029	TrainAcc 0.5558	ValidAcc 0.5038	TestAcc 0.5073	BestValid 0.5261
	Epoch 4850:	Loss 1.3031	TrainAcc 0.5647	ValidAcc 0.5224	TestAcc 0.5231	BestValid 0.5261
	Epoch 4900:	Loss 1.3044	TrainAcc 0.5630	ValidAcc 0.5118	TestAcc 0.5151	BestValid 0.5261
	Epoch 4950:	Loss 1.3027	TrainAcc 0.5451	ValidAcc 0.4986	TestAcc 0.5006	BestValid 0.5261
	Epoch 5000:	Loss 1.3013	TrainAcc 0.5577	ValidAcc 0.5055	TestAcc 0.5094	BestValid 0.5261
****** Epoch Time (Excluding Evaluation Cost): 0.020 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 3.449 ms (Max: 3.851, Min: 2.478, Sum: 27.588)
Cluster-Wide Average, Compute: 5.887 ms (Max: 6.967, Min: 5.335, Sum: 47.097)
Cluster-Wide Average, Communication-Layer: 4.223 ms (Max: 4.945, Min: 3.466, Sum: 33.782)
Cluster-Wide Average, Bubble-Imbalance: 1.795 ms (Max: 2.398, Min: 1.118, Sum: 14.359)
Cluster-Wide Average, Communication-Graph: 3.433 ms (Max: 3.497, Min: 3.366, Sum: 27.465)
Cluster-Wide Average, Optimization: 0.560 ms (Max: 0.772, Min: 0.048, Sum: 4.484)
Cluster-Wide Average, Others: 1.062 ms (Max: 2.744, Min: 0.457, Sum: 8.493)
****** Breakdown Sum: 20.408 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.339 GB (Max: 1.850, Min: 1.130, Sum: 10.708)
Cluster-Wide Average, Graph-Level Communication Throughput: 96.286 Gbps (Max: 101.546, Min: 92.649, Sum: 770.288)
Cluster-Wide Average, Layer-Level Communication Throughput: 49.375 Gbps (Max: 61.796, Min: 37.857, Sum: 395.001)
Layer-level communication (cluster-wide, per-epoch): 0.199 GB
Graph-level communication (cluster-wide, per-epoch): 0.224 GB
Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
Total communication (cluster-wide, per-epoch): 0.424 GB
****** Accuracy Results ******
Highest valid_acc: 0.5261
Target test_acc: 0.5263
Epoch to reach the target acc: 3399
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
