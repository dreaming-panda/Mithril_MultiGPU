Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.043 seconds.
Building the CSC structure...
        It takes 2.048 seconds.
Building the CSC structure...
        It takes 2.304 seconds.
Building the CSC structure...
        It takes 2.374 seconds.
Building the CSC structure...
        It takes 2.437 seconds.
Building the CSC structure...
        It takes 2.438 seconds.
Building the CSC structure...
        It takes 2.445 seconds.
Building the CSC structure...
        It takes 2.651 seconds.
Building the CSC structure...
        It takes 1.844 seconds.
        It takes 1.874 seconds.
        It takes 2.249 seconds.
        It takes 2.298 seconds.
        It takes 2.278 seconds.
        It takes 2.358 seconds.
        It takes 2.370 seconds.
        It takes 2.312 seconds.
Building the Feature Vector...
        It takes 0.249 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCN layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.031 seconds.
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.295 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.041 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.058 Gbps (per GPU), 448.460 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.796 Gbps (per GPU), 446.364 Gbps (aggregated)
The layer-level communication performance: 55.806 Gbps (per GPU), 446.449 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.556 Gbps (per GPU), 444.445 Gbps (aggregated)
The layer-level communication performance: 55.542 Gbps (per GPU), 444.339 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.361 Gbps (per GPU), 442.887 Gbps (aggregated)
The layer-level communication performance: 55.304 Gbps (per GPU), 442.428 Gbps (aggregated)
The layer-level communication performance: 55.292 Gbps (per GPU), 442.335 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.492 Gbps (per GPU), 1251.938 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.434 Gbps (per GPU), 1251.471 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.460 Gbps (per GPU), 1251.684 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.419 Gbps (per GPU), 1251.352 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.457 Gbps (per GPU), 1251.657 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.460 Gbps (per GPU), 1251.681 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.460 Gbps (per GPU), 1251.681 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.428 Gbps (per GPU), 1251.424 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.523 Gbps (per GPU), 812.181 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.509 Gbps (per GPU), 812.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.509 Gbps (per GPU), 812.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.509 Gbps (per GPU), 812.075 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.509 Gbps (per GPU), 812.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.509 Gbps (per GPU), 812.070 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.509 Gbps (per GPU), 812.076 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.509 Gbps (per GPU), 812.076 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.595 Gbps (per GPU), 268.757 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.594 Gbps (per GPU), 268.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.594 Gbps (per GPU), 268.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.594 Gbps (per GPU), 268.749 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.594 Gbps (per GPU), 268.752 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.595 Gbps (per GPU), 268.757 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.594 Gbps (per GPU), 268.754 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.594 Gbps (per GPU), 268.755 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.42ms  2.40ms  2.26ms  1.07  8.38K  3.53M
 chk_1  2.83ms  2.64ms  2.54ms  1.11  6.74K  3.60M
 chk_2  2.67ms  2.54ms  2.46ms  1.09  7.27K  3.53M
 chk_3  2.71ms  2.54ms  2.47ms  1.10  7.92K  3.61M
 chk_4  2.63ms  2.57ms  2.46ms  1.07  5.33K  3.68M
 chk_5  2.62ms  2.42ms  2.30ms  1.14 10.07K  3.45M
 chk_6  2.78ms  2.60ms  2.47ms  1.13  9.41K  3.48M
 chk_7  2.64ms  2.48ms  2.39ms  1.11  8.12K  3.60M
 chk_8  2.60ms  2.60ms  2.55ms  1.02  6.09K  3.64M
 chk_9  2.56ms  2.31ms  2.18ms  1.17 11.10K  3.38M
chk_10  2.77ms  2.65ms  2.60ms  1.07  5.67K  3.63M
chk_11  2.63ms  2.48ms  2.39ms  1.10  8.16K  3.54M
chk_12  2.84ms  2.68ms  2.61ms  1.09  7.24K  3.55M
chk_13  2.65ms  2.55ms  2.49ms  1.07  5.41K  3.68M
chk_14  2.90ms  2.75ms  2.66ms  1.09  7.14K  3.53M
chk_15  2.73ms  2.56ms  2.44ms  1.12  9.25K  3.49M
chk_16  2.59ms  2.49ms  2.42ms  1.07  4.78K  3.77M
chk_17  2.73ms  2.58ms  2.49ms  1.09  6.85K  3.60M
chk_18  2.54ms  2.38ms  2.30ms  1.10  7.47K  3.57M
chk_19  2.57ms  2.49ms  2.44ms  1.06  4.88K  3.75M
chk_20  2.62ms  2.49ms  2.38ms  1.10  7.00K  3.63M
chk_21  2.59ms  2.47ms  2.41ms  1.08  5.41K  3.68M
chk_22  2.80ms  2.54ms  2.42ms  1.16 11.07K  3.39M
chk_23  2.71ms  2.54ms  2.46ms  1.10  7.23K  3.64M
chk_24  2.74ms  2.51ms  2.38ms  1.15 10.13K  3.43M
chk_25  2.56ms  2.42ms  2.32ms  1.10  6.40K  3.57M
chk_26  2.75ms  2.63ms  2.54ms  1.08  5.78K  3.55M
chk_27  2.65ms  2.44ms  2.32ms  1.14  9.34K  3.48M
chk_28  2.92ms  2.79ms  2.69ms  1.09  6.37K  3.57M
chk_29  2.74ms  2.63ms  2.69ms  1.05  5.16K  3.78M
chk_30  2.62ms  2.54ms  2.42ms  1.08  5.44K  3.67M
chk_31  2.78ms  2.68ms  2.53ms  1.10  6.33K  3.63M
   Avg  2.68  2.54  2.45
   Max  2.92  2.79  2.69
   Min  2.42  2.31  2.18
 Ratio  1.21  1.21  1.23
   Var  0.01  0.01  0.01
Profiling takes 2.867 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 6)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 6)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [6, 11)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [11, 16)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [16, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [16, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [11, 16)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [6, 11)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[11, 16)...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 5 initializing the weights for op[11, 16)...
+++++++++ Node 2 initializing the weights for op[6, 11)...
+++++++++ Node 6 initializing the weights for op[16, 20)...
+++++++++ Node 3 initializing the weights for op[6, 11)...
+++++++++ Node 7 initializing the weights for op[16, 20)...
+++++++++ Node 1 initializing the weights for op[0, 6)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7061	TrainAcc 0.0815	ValidAcc 0.0678	TestAcc 0.0659	BestValid 0.0678
	Epoch 50:	Loss 1.1381	TrainAcc 0.7614	ValidAcc 0.7842	TestAcc 0.7788	BestValid 0.7842
	Epoch 100:	Loss 0.6069	TrainAcc 0.8821	ValidAcc 0.8930	TestAcc 0.8897	BestValid 0.8930
	Epoch 150:	Loss 0.4934	TrainAcc 0.9071	ValidAcc 0.9157	TestAcc 0.9141	BestValid 0.9157
	Epoch 200:	Loss 0.4419	TrainAcc 0.9175	ValidAcc 0.9233	TestAcc 0.9234	BestValid 0.9233
	Epoch 250:	Loss 0.4110	TrainAcc 0.9209	ValidAcc 0.9261	TestAcc 0.9265	BestValid 0.9261
	Epoch 300:	Loss 0.3886	TrainAcc 0.9247	ValidAcc 0.9287	TestAcc 0.9299	BestValid 0.9287
	Epoch 350:	Loss 0.3716	TrainAcc 0.9273	ValidAcc 0.9306	TestAcc 0.9326	BestValid 0.9306
	Epoch 400:	Loss 0.3620	TrainAcc 0.9285	ValidAcc 0.9317	TestAcc 0.9337	BestValid 0.9317
	Epoch 450:	Loss 0.3515	TrainAcc 0.9302	ValidAcc 0.9337	TestAcc 0.9347	BestValid 0.9337
	Epoch 500:	Loss 0.3443	TrainAcc 0.9314	ValidAcc 0.9340	TestAcc 0.9357	BestValid 0.9340
	Epoch 550:	Loss 0.3386	TrainAcc 0.9326	ValidAcc 0.9350	TestAcc 0.9368	BestValid 0.9350
	Epoch 600:	Loss 0.3353	TrainAcc 0.9330	ValidAcc 0.9355	TestAcc 0.9370	BestValid 0.9355
	Epoch 650:	Loss 0.3315	TrainAcc 0.9342	ValidAcc 0.9366	TestAcc 0.9380	BestValid 0.9366
	Epoch 700:	Loss 0.3249	TrainAcc 0.9347	ValidAcc 0.9365	TestAcc 0.9386	BestValid 0.9366
	Epoch 750:	Loss 0.3217	TrainAcc 0.9352	ValidAcc 0.9370	TestAcc 0.9388	BestValid 0.9370
	Epoch 800:	Loss 0.3179	TrainAcc 0.9359	ValidAcc 0.9374	TestAcc 0.9391	BestValid 0.9374
	Epoch 850:	Loss 0.3170	TrainAcc 0.9362	ValidAcc 0.9369	TestAcc 0.9392	BestValid 0.9374
	Epoch 900:	Loss 0.3137	TrainAcc 0.9367	ValidAcc 0.9377	TestAcc 0.9397	BestValid 0.9377
	Epoch 950:	Loss 0.3095	TrainAcc 0.9371	ValidAcc 0.9380	TestAcc 0.9399	BestValid 0.9380
	Epoch 1000:	Loss 0.3078	TrainAcc 0.9377	ValidAcc 0.9386	TestAcc 0.9406	BestValid 0.9386
	Epoch 1050:	Loss 0.3043	TrainAcc 0.9379	ValidAcc 0.9384	TestAcc 0.9403	BestValid 0.9386
	Epoch 1100:	Loss 0.3031	TrainAcc 0.9383	ValidAcc 0.9387	TestAcc 0.9404	BestValid 0.9387
	Epoch 1150:	Loss 0.3014	TrainAcc 0.9386	ValidAcc 0.9391	TestAcc 0.9412	BestValid 0.9391
	Epoch 1200:	Loss 0.2996	TrainAcc 0.9388	ValidAcc 0.9391	TestAcc 0.9408	BestValid 0.9391
	Epoch 1250:	Loss 0.2951	TrainAcc 0.9395	ValidAcc 0.9391	TestAcc 0.9411	BestValid 0.9391
	Epoch 1300:	Loss 0.2948	TrainAcc 0.9397	ValidAcc 0.9396	TestAcc 0.9413	BestValid 0.9396
	Epoch 1350:	Loss 0.2933	TrainAcc 0.9400	ValidAcc 0.9394	TestAcc 0.9416	BestValid 0.9396
	Epoch 1400:	Loss 0.2913	TrainAcc 0.9405	ValidAcc 0.9402	TestAcc 0.9416	BestValid 0.9402
	Epoch 1450:	Loss 0.2897	TrainAcc 0.9410	ValidAcc 0.9405	TestAcc 0.9422	BestValid 0.9405
	Epoch 1500:	Loss 0.2878	TrainAcc 0.9412	ValidAcc 0.9415	TestAcc 0.9424	BestValid 0.9415
	Epoch 1550:	Loss 0.2881	TrainAcc 0.9412	ValidAcc 0.9407	TestAcc 0.9421	BestValid 0.9415
	Epoch 1600:	Loss 0.2857	TrainAcc 0.9415	ValidAcc 0.9413	TestAcc 0.9422	BestValid 0.9415
	Epoch 1650:	Loss 0.2838	TrainAcc 0.9418	ValidAcc 0.9419	TestAcc 0.9428	BestValid 0.9419
	Epoch 1700:	Loss 0.2823	TrainAcc 0.9419	ValidAcc 0.9416	TestAcc 0.9431	BestValid 0.9419
	Epoch 1750:	Loss 0.2809	TrainAcc 0.9424	ValidAcc 0.9417	TestAcc 0.9431	BestValid 0.9419
	Epoch 1800:	Loss 0.2795	TrainAcc 0.9423	ValidAcc 0.9413	TestAcc 0.9427	BestValid 0.9419
	Epoch 1850:	Loss 0.2788	TrainAcc 0.9426	ValidAcc 0.9420	TestAcc 0.9429	BestValid 0.9420
	Epoch 1900:	Loss 0.2780	TrainAcc 0.9430	ValidAcc 0.9419	TestAcc 0.9433	BestValid 0.9420
	Epoch 1950:	Loss 0.2762	TrainAcc 0.9431	ValidAcc 0.9421	TestAcc 0.9435	BestValid 0.9421
	Epoch 2000:	Loss 0.2754	TrainAcc 0.9433	ValidAcc 0.9421	TestAcc 0.9436	BestValid 0.9421
	Epoch 2050:	Loss 0.2734	TrainAcc 0.9430	ValidAcc 0.9418	TestAcc 0.9430	BestValid 0.9421
	Epoch 2100:	Loss 0.2727	TrainAcc 0.9436	ValidAcc 0.9425	TestAcc 0.9435	BestValid 0.9425
	Epoch 2150:	Loss 0.2713	TrainAcc 0.9436	ValidAcc 0.9423	TestAcc 0.9439	BestValid 0.9425
	Epoch 2200:	Loss 0.2716	TrainAcc 0.9439	ValidAcc 0.9429	TestAcc 0.9440	BestValid 0.9429
	Epoch 2250:	Loss 0.2692	TrainAcc 0.9440	ValidAcc 0.9428	TestAcc 0.9444	BestValid 0.9429
	Epoch 2300:	Loss 0.2702	TrainAcc 0.9429	ValidAcc 0.9420	TestAcc 0.9435	BestValid 0.9429
	Epoch 2350:	Loss 0.2679	TrainAcc 0.9442	ValidAcc 0.9427	TestAcc 0.9438	BestValid 0.9429
	Epoch 2400:	Loss 0.2668	TrainAcc 0.9444	ValidAcc 0.9431	TestAcc 0.9443	BestValid 0.9431
	Epoch 2450:	Loss 0.2660	TrainAcc 0.9442	ValidAcc 0.9423	TestAcc 0.9441	BestValid 0.9431
	Epoch 2500:	Loss 0.2656	TrainAcc 0.9447	ValidAcc 0.9428	TestAcc 0.9446	BestValid 0.9431
	Epoch 2550:	Loss 0.2647	TrainAcc 0.9445	ValidAcc 0.9429	TestAcc 0.9440	BestValid 0.9431
	Epoch 2600:	Loss 0.2644	TrainAcc 0.9450	ValidAcc 0.9432	TestAcc 0.9442	BestValid 0.9432
	Epoch 2650:	Loss 0.2623	TrainAcc 0.9453	ValidAcc 0.9434	TestAcc 0.9447	BestValid 0.9434
	Epoch 2700:	Loss 0.2612	TrainAcc 0.9449	ValidAcc 0.9434	TestAcc 0.9442	BestValid 0.9434
	Epoch 2750:	Loss 0.2627	TrainAcc 0.9452	ValidAcc 0.9437	TestAcc 0.9446	BestValid 0.9437
	Epoch 2800:	Loss 0.2605	TrainAcc 0.9453	ValidAcc 0.9434	TestAcc 0.9443	BestValid 0.9437
	Epoch 2850:	Loss 0.2605	TrainAcc 0.9451	ValidAcc 0.9438	TestAcc 0.9444	BestValid 0.9438
	Epoch 2900:	Loss 0.2605	TrainAcc 0.9451	ValidAcc 0.9436	TestAcc 0.9444	BestValid 0.9438
	Epoch 2950:	Loss 0.2598	TrainAcc 0.9452	ValidAcc 0.9432	TestAcc 0.9447	BestValid 0.9438
	Epoch 3000:	Loss 0.2593	TrainAcc 0.9460	ValidAcc 0.9438	TestAcc 0.9447	BestValid 0.9438
	Epoch 3050:	Loss 0.2586	TrainAcc 0.9457	ValidAcc 0.9436	TestAcc 0.9447	BestValid 0.9438
	Epoch 3100:	Loss 0.2578	TrainAcc 0.9462	ValidAcc 0.9439	TestAcc 0.9454	BestValid 0.9439
	Epoch 3150:	Loss 0.2558	TrainAcc 0.9456	ValidAcc 0.9441	TestAcc 0.9453	BestValid 0.9441
	Epoch 3200:	Loss 0.2557	TrainAcc 0.9451	ValidAcc 0.9426	TestAcc 0.9442	BestValid 0.9441
	Epoch 3250:	Loss 0.2552	TrainAcc 0.9457	ValidAcc 0.9436	TestAcc 0.9450	BestValid 0.9441
	Epoch 3300:	Loss 0.2537	TrainAcc 0.9463	ValidAcc 0.9439	TestAcc 0.9452	BestValid 0.9441
	Epoch 3350:	Loss 0.2534	TrainAcc 0.9468	ValidAcc 0.9443	TestAcc 0.9451	BestValid 0.9443
	Epoch 3400:	Loss 0.2537	TrainAcc 0.9466	ValidAcc 0.9447	TestAcc 0.9449	BestValid 0.9447
	Epoch 3450:	Loss 0.2529	TrainAcc 0.9462	ValidAcc 0.9439	TestAcc 0.9449	BestValid 0.9447
	Epoch 3500:	Loss 0.2535	TrainAcc 0.9459	ValidAcc 0.9429	TestAcc 0.9444	BestValid 0.9447
	Epoch 3550:	Loss 0.2515	TrainAcc 0.9468	ValidAcc 0.9442	TestAcc 0.9451	BestValid 0.9447
	Epoch 3600:	Loss 0.2507	TrainAcc 0.9459	ValidAcc 0.9441	TestAcc 0.9447	BestValid 0.9447
	Epoch 3650:	Loss 0.2511	TrainAcc 0.9470	ValidAcc 0.9446	TestAcc 0.9453	BestValid 0.9447
	Epoch 3700:	Loss 0.2497	TrainAcc 0.9469	ValidAcc 0.9444	TestAcc 0.9452	BestValid 0.9447
	Epoch 3750:	Loss 0.2490	TrainAcc 0.9471	ValidAcc 0.9442	TestAcc 0.9450	BestValid 0.9447
	Epoch 3800:	Loss 0.2492	TrainAcc 0.9454	ValidAcc 0.9426	TestAcc 0.9438	BestValid 0.9447
	Epoch 3850:	Loss 0.2487	TrainAcc 0.9471	ValidAcc 0.9439	TestAcc 0.9448	BestValid 0.9447
	Epoch 3900:	Loss 0.2489	TrainAcc 0.9471	ValidAcc 0.9443	TestAcc 0.9448	BestValid 0.9447
	Epoch 3950:	Loss 0.2477	TrainAcc 0.9472	ValidAcc 0.9446	TestAcc 0.9450	BestValid 0.9447
	Epoch 4000:	Loss 0.2464	TrainAcc 0.9465	ValidAcc 0.9434	TestAcc 0.9440	BestValid 0.9447
	Epoch 4050:	Loss 0.2469	TrainAcc 0.9477	ValidAcc 0.9441	TestAcc 0.9453	BestValid 0.9447
	Epoch 4100:	Loss 0.2465	TrainAcc 0.9474	ValidAcc 0.9444	TestAcc 0.9453	BestValid 0.9447
	Epoch 4150:	Loss 0.2457	TrainAcc 0.9474	ValidAcc 0.9445	TestAcc 0.9452	BestValid 0.9447
	Epoch 4200:	Loss 0.2453	TrainAcc 0.9477	ValidAcc 0.9442	TestAcc 0.9453	BestValid 0.9447
	Epoch 4250:	Loss 0.2445	TrainAcc 0.9477	ValidAcc 0.9444	TestAcc 0.9450	BestValid 0.9447
	Epoch 4300:	Loss 0.2449	TrainAcc 0.9477	ValidAcc 0.9445	TestAcc 0.9451	BestValid 0.9447
	Epoch 4350:	Loss 0.2440	TrainAcc 0.9480	ValidAcc 0.9450	TestAcc 0.9459	BestValid 0.9450
	Epoch 4400:	Loss 0.2432	TrainAcc 0.9475	ValidAcc 0.9439	TestAcc 0.9448	BestValid 0.9450
	Epoch 4450:	Loss 0.2434	TrainAcc 0.9483	ValidAcc 0.9443	TestAcc 0.9452	BestValid 0.9450
	Epoch 4500:	Loss 0.2430	TrainAcc 0.9478	ValidAcc 0.9446	TestAcc 0.9455	BestValid 0.9450
	Epoch 4550:	Loss 0.2428	TrainAcc 0.9482	ValidAcc 0.9441	TestAcc 0.9453	BestValid 0.9450
	Epoch 4600:	Loss 0.2416	TrainAcc 0.9483	ValidAcc 0.9440	TestAcc 0.9452	BestValid 0.9450
	Epoch 4650:	Loss 0.2422	TrainAcc 0.9469	ValidAcc 0.9428	TestAcc 0.9438	BestValid 0.9450
	Epoch 4700:	Loss 0.2411	TrainAcc 0.9486	ValidAcc 0.9444	TestAcc 0.9457	BestValid 0.9450
	Epoch 4750:	Loss 0.2415	TrainAcc 0.9483	ValidAcc 0.9444	TestAcc 0.9449	BestValid 0.9450
	Epoch 4800:	Loss 0.2412	TrainAcc 0.9486	ValidAcc 0.9440	TestAcc 0.9451	BestValid 0.9450
	Epoch 4850:	Loss 0.2402	TrainAcc 0.9488	ValidAcc 0.9448	TestAcc 0.9455	BestValid 0.9450
	Epoch 4900:	Loss 0.2407	TrainAcc 0.9484	ValidAcc 0.9441	TestAcc 0.9447	BestValid 0.9450
	Epoch 4950:	Loss 0.2398	TrainAcc 0.9482	ValidAcc 0.9442	TestAcc 0.9447	BestValid 0.9450
	Epoch 5000:	Loss 0.2394	TrainAcc 0.9481	ValidAcc 0.9447	TestAcc 0.9450	BestValid 0.9450
****** Epoch Time (Excluding Evaluation Cost): 0.074 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 14.275 ms (Max: 16.301, Min: 8.945, Sum: 114.202)
Cluster-Wide Average, Compute: 32.023 ms (Max: 35.148, Min: 29.445, Sum: 256.183)
Cluster-Wide Average, Communication-Layer: 11.210 ms (Max: 13.535, Min: 8.701, Sum: 89.679)
Cluster-Wide Average, Bubble-Imbalance: 4.950 ms (Max: 6.888, Min: 3.228, Sum: 39.603)
Cluster-Wide Average, Communication-Graph: 8.678 ms (Max: 9.870, Min: 7.403, Sum: 69.423)
Cluster-Wide Average, Optimization: 0.580 ms (Max: 0.816, Min: 0.059, Sum: 4.642)
Cluster-Wide Average, Others: 2.726 ms (Max: 8.937, Min: 0.620, Sum: 21.805)
****** Breakdown Sum: 74.442 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.521 GB (Max: 4.434, Min: 3.227, Sum: 28.167)
Cluster-Wide Average, Graph-Level Communication Throughput: 101.530 Gbps (Max: 125.697, Min: 80.075, Sum: 812.243)
Cluster-Wide Average, Layer-Level Communication Throughput: 48.706 Gbps (Max: 62.563, Min: 36.309, Sum: 389.651)
Layer-level communication (cluster-wide, per-epoch): 0.521 GB
Graph-level communication (cluster-wide, per-epoch): 0.668 GB
Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
Total communication (cluster-wide, per-epoch): 1.189 GB
****** Accuracy Results ******
Highest valid_acc: 0.9450
Target test_acc: 0.9459
Epoch to reach the target acc: 4349
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
