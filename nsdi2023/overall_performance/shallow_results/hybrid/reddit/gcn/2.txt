Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.015 seconds.
Building the CSC structure...
        It takes 2.205 seconds.
Building the CSC structure...
        It takes 2.289 seconds.
Building the CSC structure...
        It takes 2.403 seconds.
Building the CSC structure...
        It takes 2.417 seconds.
Building the CSC structure...
        It takes 2.520 seconds.
Building the CSC structure...
        It takes 2.653 seconds.
Building the CSC structure...
        It takes 2.653 seconds.
Building the CSC structure...
        It takes 1.862 seconds.
        It takes 2.212 seconds.
        It takes 2.317 seconds.
        It takes 2.302 seconds.
        It takes 2.350 seconds.
        It takes 2.314 seconds.
        It takes 2.320 seconds.
        It takes 2.351 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.287 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.043 seconds.
        It takes 0.290 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.306 seconds.
Building the Label Vector...
        It takes 0.304 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCN layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.044 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 58.547 Gbps (per GPU), 468.374 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.274 Gbps (per GPU), 466.190 Gbps (aggregated)
The layer-level communication performance: 58.260 Gbps (per GPU), 466.082 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.029 Gbps (per GPU), 464.231 Gbps (aggregated)
The layer-level communication performance: 57.996 Gbps (per GPU), 463.972 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.793 Gbps (per GPU), 462.340 Gbps (aggregated)
The layer-level communication performance: 57.753 Gbps (per GPU), 462.027 Gbps (aggregated)
The layer-level communication performance: 57.718 Gbps (per GPU), 461.744 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.559 Gbps (per GPU), 1252.475 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.580 Gbps (per GPU), 1252.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.562 Gbps (per GPU), 1252.498 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.510 Gbps (per GPU), 1252.078 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.562 Gbps (per GPU), 1252.498 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.580 Gbps (per GPU), 1252.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.554 Gbps (per GPU), 1252.428 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.510 Gbps (per GPU), 1252.078 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.882 Gbps (per GPU), 799.055 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.881 Gbps (per GPU), 799.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.882 Gbps (per GPU), 799.055 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.872 Gbps (per GPU), 798.978 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.883 Gbps (per GPU), 799.061 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.882 Gbps (per GPU), 799.053 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.883 Gbps (per GPU), 799.060 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.861 Gbps (per GPU), 798.890 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.666 Gbps (per GPU), 277.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.666 Gbps (per GPU), 277.330 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.666 Gbps (per GPU), 277.326 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.664 Gbps (per GPU), 277.312 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.667 Gbps (per GPU), 277.332 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.667 Gbps (per GPU), 277.335 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.666 Gbps (per GPU), 277.330 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.663 Gbps (per GPU), 277.307 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.42ms  2.36ms  2.31ms  1.05  8.38K  3.53M
 chk_1  2.82ms  2.64ms  2.53ms  1.11  6.74K  3.60M
 chk_2  2.67ms  2.51ms  2.45ms  1.09  7.27K  3.53M
 chk_3  2.71ms  2.53ms  2.46ms  1.10  7.92K  3.61M
 chk_4  2.63ms  2.50ms  2.45ms  1.07  5.33K  3.68M
 chk_5  2.62ms  2.39ms  2.28ms  1.15 10.07K  3.45M
 chk_6  2.77ms  2.58ms  2.46ms  1.13  9.41K  3.48M
 chk_7  2.63ms  2.47ms  2.37ms  1.11  8.12K  3.60M
 chk_8  2.73ms  2.61ms  2.53ms  1.08  6.09K  3.64M
 chk_9  2.54ms  2.30ms  2.16ms  1.17 11.10K  3.38M
chk_10  2.77ms  2.65ms  2.58ms  1.07  5.67K  3.63M
chk_11  2.63ms  2.47ms  2.38ms  1.11  8.16K  3.54M
chk_12  2.85ms  2.68ms  2.61ms  1.09  7.24K  3.55M
chk_13  2.64ms  2.55ms  2.50ms  1.06  5.41K  3.68M
chk_14  2.91ms  2.75ms  2.66ms  1.09  7.14K  3.53M
chk_15  2.75ms  2.57ms  2.44ms  1.13  9.25K  3.49M
chk_16  2.58ms  2.49ms  2.44ms  1.05  4.78K  3.77M
chk_17  2.73ms  2.57ms  2.51ms  1.09  6.85K  3.60M
chk_18  2.53ms  2.38ms  2.31ms  1.10  7.47K  3.57M
chk_19  2.61ms  2.51ms  2.44ms  1.07  4.88K  3.75M
chk_20  2.61ms  2.45ms  2.38ms  1.10  7.00K  3.63M
chk_21  2.59ms  2.48ms  2.41ms  1.07  5.41K  3.68M
chk_22  2.80ms  2.56ms  2.42ms  1.16 11.07K  3.39M
chk_23  2.71ms  2.55ms  2.46ms  1.10  7.23K  3.64M
chk_24  2.72ms  2.52ms  2.39ms  1.13 10.13K  3.43M
chk_25  2.54ms  2.41ms  2.33ms  1.09  6.40K  3.57M
chk_26  2.74ms  2.62ms  2.54ms  1.08  5.78K  3.55M
chk_27  2.63ms  2.44ms  2.32ms  1.13  9.34K  3.48M
chk_28  2.97ms  2.79ms  2.69ms  1.10  6.37K  3.57M
chk_29  2.77ms  2.63ms  2.55ms  1.09  5.16K  3.78M
chk_30  2.65ms  2.57ms  2.40ms  1.10  5.44K  3.67M
chk_31  2.78ms  2.69ms  2.55ms  1.09  6.33K  3.63M
   Avg  2.69  2.54  2.45
   Max  2.97  2.79  2.69
   Min  2.42  2.30  2.16
 Ratio  1.23  1.21  1.24
   Var  0.01  0.01  0.01
Profiling takes 2.852 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 6)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 6)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [6, 11)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [6, 11)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [11, 16)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [11, 16)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [16, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [16, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[11, 16)...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 5 initializing the weights for op[11, 16)...
+++++++++ Node 1 initializing the weights for op[0, 6)...
+++++++++ Node 6 initializing the weights for op[16, 20)...
+++++++++ Node 2 initializing the weights for op[6, 11)...
+++++++++ Node 7 initializing the weights for op[16, 20)...
+++++++++ Node 3 initializing the weights for op[6, 11)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7267	TrainAcc 0.0393	ValidAcc 0.0397	TestAcc 0.0377	BestValid 0.0397
	Epoch 50:	Loss 1.3631	TrainAcc 0.7123	ValidAcc 0.7326	TestAcc 0.7276	BestValid 0.7326
	Epoch 100:	Loss 0.6417	TrainAcc 0.8743	ValidAcc 0.8821	TestAcc 0.8800	BestValid 0.8821
	Epoch 150:	Loss 0.4907	TrainAcc 0.9099	ValidAcc 0.9181	TestAcc 0.9173	BestValid 0.9181
	Epoch 200:	Loss 0.4384	TrainAcc 0.9190	ValidAcc 0.9246	TestAcc 0.9247	BestValid 0.9246
	Epoch 250:	Loss 0.4059	TrainAcc 0.9228	ValidAcc 0.9277	TestAcc 0.9287	BestValid 0.9277
	Epoch 300:	Loss 0.3839	TrainAcc 0.9257	ValidAcc 0.9305	TestAcc 0.9309	BestValid 0.9305
	Epoch 350:	Loss 0.3706	TrainAcc 0.9268	ValidAcc 0.9308	TestAcc 0.9320	BestValid 0.9308
	Epoch 400:	Loss 0.3621	TrainAcc 0.9293	ValidAcc 0.9328	TestAcc 0.9341	BestValid 0.9328
	Epoch 450:	Loss 0.3509	TrainAcc 0.9310	ValidAcc 0.9336	TestAcc 0.9353	BestValid 0.9336
	Epoch 500:	Loss 0.3455	TrainAcc 0.9315	ValidAcc 0.9345	TestAcc 0.9359	BestValid 0.9345
	Epoch 550:	Loss 0.3374	TrainAcc 0.9324	ValidAcc 0.9351	TestAcc 0.9368	BestValid 0.9351
	Epoch 600:	Loss 0.3358	TrainAcc 0.9337	ValidAcc 0.9361	TestAcc 0.9374	BestValid 0.9361
	Epoch 650:	Loss 0.3297	TrainAcc 0.9344	ValidAcc 0.9358	TestAcc 0.9377	BestValid 0.9361
	Epoch 700:	Loss 0.3239	TrainAcc 0.9352	ValidAcc 0.9368	TestAcc 0.9384	BestValid 0.9368
	Epoch 750:	Loss 0.3215	TrainAcc 0.9354	ValidAcc 0.9367	TestAcc 0.9388	BestValid 0.9368
	Epoch 800:	Loss 0.3195	TrainAcc 0.9358	ValidAcc 0.9375	TestAcc 0.9394	BestValid 0.9375
	Epoch 850:	Loss 0.3154	TrainAcc 0.9369	ValidAcc 0.9379	TestAcc 0.9398	BestValid 0.9379
	Epoch 900:	Loss 0.3122	TrainAcc 0.9368	ValidAcc 0.9376	TestAcc 0.9396	BestValid 0.9379
	Epoch 950:	Loss 0.3096	TrainAcc 0.9372	ValidAcc 0.9376	TestAcc 0.9402	BestValid 0.9379
	Epoch 1000:	Loss 0.3065	TrainAcc 0.9378	ValidAcc 0.9382	TestAcc 0.9404	BestValid 0.9382
	Epoch 1050:	Loss 0.3052	TrainAcc 0.9383	ValidAcc 0.9386	TestAcc 0.9406	BestValid 0.9386
	Epoch 1100:	Loss 0.3021	TrainAcc 0.9388	ValidAcc 0.9382	TestAcc 0.9408	BestValid 0.9386
	Epoch 1150:	Loss 0.3002	TrainAcc 0.9387	ValidAcc 0.9386	TestAcc 0.9409	BestValid 0.9386
	Epoch 1200:	Loss 0.2971	TrainAcc 0.9392	ValidAcc 0.9392	TestAcc 0.9413	BestValid 0.9392
	Epoch 1250:	Loss 0.2959	TrainAcc 0.9397	ValidAcc 0.9396	TestAcc 0.9416	BestValid 0.9396
	Epoch 1300:	Loss 0.2922	TrainAcc 0.9401	ValidAcc 0.9395	TestAcc 0.9418	BestValid 0.9396
	Epoch 1350:	Loss 0.2918	TrainAcc 0.9405	ValidAcc 0.9402	TestAcc 0.9423	BestValid 0.9402
	Epoch 1400:	Loss 0.2911	TrainAcc 0.9409	ValidAcc 0.9403	TestAcc 0.9423	BestValid 0.9403
	Epoch 1450:	Loss 0.2904	TrainAcc 0.9410	ValidAcc 0.9407	TestAcc 0.9426	BestValid 0.9407
	Epoch 1500:	Loss 0.2873	TrainAcc 0.9413	ValidAcc 0.9411	TestAcc 0.9431	BestValid 0.9411
	Epoch 1550:	Loss 0.2854	TrainAcc 0.9414	ValidAcc 0.9410	TestAcc 0.9429	BestValid 0.9411
	Epoch 1600:	Loss 0.2847	TrainAcc 0.9416	ValidAcc 0.9404	TestAcc 0.9428	BestValid 0.9411
	Epoch 1650:	Loss 0.2821	TrainAcc 0.9413	ValidAcc 0.9405	TestAcc 0.9430	BestValid 0.9411
	Epoch 1700:	Loss 0.2814	TrainAcc 0.9414	ValidAcc 0.9408	TestAcc 0.9427	BestValid 0.9411
	Epoch 1750:	Loss 0.2807	TrainAcc 0.9421	ValidAcc 0.9418	TestAcc 0.9434	BestValid 0.9418
	Epoch 1800:	Loss 0.2794	TrainAcc 0.9426	ValidAcc 0.9412	TestAcc 0.9432	BestValid 0.9418
	Epoch 1850:	Loss 0.2764	TrainAcc 0.9425	ValidAcc 0.9413	TestAcc 0.9431	BestValid 0.9418
	Epoch 1900:	Loss 0.2755	TrainAcc 0.9429	ValidAcc 0.9420	TestAcc 0.9436	BestValid 0.9420
	Epoch 1950:	Loss 0.2745	TrainAcc 0.9429	ValidAcc 0.9418	TestAcc 0.9434	BestValid 0.9420
	Epoch 2000:	Loss 0.2735	TrainAcc 0.9434	ValidAcc 0.9421	TestAcc 0.9437	BestValid 0.9421
	Epoch 2050:	Loss 0.2717	TrainAcc 0.9435	ValidAcc 0.9424	TestAcc 0.9438	BestValid 0.9424
	Epoch 2100:	Loss 0.2723	TrainAcc 0.9438	ValidAcc 0.9424	TestAcc 0.9442	BestValid 0.9424
	Epoch 2150:	Loss 0.2709	TrainAcc 0.9439	ValidAcc 0.9431	TestAcc 0.9442	BestValid 0.9431
	Epoch 2200:	Loss 0.2699	TrainAcc 0.9438	ValidAcc 0.9421	TestAcc 0.9437	BestValid 0.9431
	Epoch 2250:	Loss 0.2686	TrainAcc 0.9443	ValidAcc 0.9426	TestAcc 0.9445	BestValid 0.9431
	Epoch 2300:	Loss 0.2677	TrainAcc 0.9442	ValidAcc 0.9428	TestAcc 0.9441	BestValid 0.9431
	Epoch 2350:	Loss 0.2675	TrainAcc 0.9447	ValidAcc 0.9430	TestAcc 0.9443	BestValid 0.9431
	Epoch 2400:	Loss 0.2649	TrainAcc 0.9442	ValidAcc 0.9426	TestAcc 0.9442	BestValid 0.9431
	Epoch 2450:	Loss 0.2648	TrainAcc 0.9450	ValidAcc 0.9432	TestAcc 0.9446	BestValid 0.9432
	Epoch 2500:	Loss 0.2642	TrainAcc 0.9443	ValidAcc 0.9428	TestAcc 0.9446	BestValid 0.9432
	Epoch 2550:	Loss 0.2633	TrainAcc 0.9446	ValidAcc 0.9430	TestAcc 0.9448	BestValid 0.9432
	Epoch 2600:	Loss 0.2624	TrainAcc 0.9453	ValidAcc 0.9428	TestAcc 0.9448	BestValid 0.9432
	Epoch 2650:	Loss 0.2620	TrainAcc 0.9455	ValidAcc 0.9436	TestAcc 0.9450	BestValid 0.9436
	Epoch 2700:	Loss 0.2616	TrainAcc 0.9455	ValidAcc 0.9436	TestAcc 0.9447	BestValid 0.9436
	Epoch 2750:	Loss 0.2611	TrainAcc 0.9457	ValidAcc 0.9434	TestAcc 0.9448	BestValid 0.9436
	Epoch 2800:	Loss 0.2591	TrainAcc 0.9458	ValidAcc 0.9436	TestAcc 0.9448	BestValid 0.9436
	Epoch 2850:	Loss 0.2591	TrainAcc 0.9450	ValidAcc 0.9430	TestAcc 0.9443	BestValid 0.9436
	Epoch 2900:	Loss 0.2577	TrainAcc 0.9455	ValidAcc 0.9432	TestAcc 0.9447	BestValid 0.9436
	Epoch 2950:	Loss 0.2585	TrainAcc 0.9450	ValidAcc 0.9428	TestAcc 0.9444	BestValid 0.9436
	Epoch 3000:	Loss 0.2573	TrainAcc 0.9454	ValidAcc 0.9432	TestAcc 0.9447	BestValid 0.9436
	Epoch 3050:	Loss 0.2556	TrainAcc 0.9461	ValidAcc 0.9435	TestAcc 0.9450	BestValid 0.9436
	Epoch 3100:	Loss 0.2567	TrainAcc 0.9463	ValidAcc 0.9434	TestAcc 0.9454	BestValid 0.9436
	Epoch 3150:	Loss 0.2548	TrainAcc 0.9456	ValidAcc 0.9436	TestAcc 0.9447	BestValid 0.9436
	Epoch 3200:	Loss 0.2548	TrainAcc 0.9459	ValidAcc 0.9438	TestAcc 0.9448	BestValid 0.9438
	Epoch 3250:	Loss 0.2546	TrainAcc 0.9462	ValidAcc 0.9432	TestAcc 0.9445	BestValid 0.9438
	Epoch 3300:	Loss 0.2539	TrainAcc 0.9470	ValidAcc 0.9439	TestAcc 0.9450	BestValid 0.9439
	Epoch 3350:	Loss 0.2527	TrainAcc 0.9467	ValidAcc 0.9436	TestAcc 0.9450	BestValid 0.9439
	Epoch 3400:	Loss 0.2531	TrainAcc 0.9466	ValidAcc 0.9434	TestAcc 0.9448	BestValid 0.9439
	Epoch 3450:	Loss 0.2520	TrainAcc 0.9462	ValidAcc 0.9430	TestAcc 0.9447	BestValid 0.9439
	Epoch 3500:	Loss 0.2520	TrainAcc 0.9472	ValidAcc 0.9437	TestAcc 0.9457	BestValid 0.9439
	Epoch 3550:	Loss 0.2510	TrainAcc 0.9472	ValidAcc 0.9436	TestAcc 0.9456	BestValid 0.9439
	Epoch 3600:	Loss 0.2501	TrainAcc 0.9474	ValidAcc 0.9440	TestAcc 0.9453	BestValid 0.9440
	Epoch 3650:	Loss 0.2497	TrainAcc 0.9470	ValidAcc 0.9441	TestAcc 0.9453	BestValid 0.9441
	Epoch 3700:	Loss 0.2491	TrainAcc 0.9468	ValidAcc 0.9433	TestAcc 0.9449	BestValid 0.9441
	Epoch 3750:	Loss 0.2488	TrainAcc 0.9467	ValidAcc 0.9429	TestAcc 0.9448	BestValid 0.9441
	Epoch 3800:	Loss 0.2480	TrainAcc 0.9476	ValidAcc 0.9438	TestAcc 0.9457	BestValid 0.9441
	Epoch 3850:	Loss 0.2474	TrainAcc 0.9480	ValidAcc 0.9443	TestAcc 0.9453	BestValid 0.9443
	Epoch 3900:	Loss 0.2476	TrainAcc 0.9477	ValidAcc 0.9439	TestAcc 0.9456	BestValid 0.9443
	Epoch 3950:	Loss 0.2468	TrainAcc 0.9477	ValidAcc 0.9437	TestAcc 0.9454	BestValid 0.9443
	Epoch 4000:	Loss 0.2450	TrainAcc 0.9483	ValidAcc 0.9443	TestAcc 0.9461	BestValid 0.9443
	Epoch 4050:	Loss 0.2453	TrainAcc 0.9481	ValidAcc 0.9444	TestAcc 0.9462	BestValid 0.9444
	Epoch 4100:	Loss 0.2450	TrainAcc 0.9479	ValidAcc 0.9436	TestAcc 0.9460	BestValid 0.9444
	Epoch 4150:	Loss 0.2441	TrainAcc 0.9472	ValidAcc 0.9439	TestAcc 0.9450	BestValid 0.9444
	Epoch 4200:	Loss 0.2443	TrainAcc 0.9478	ValidAcc 0.9443	TestAcc 0.9456	BestValid 0.9444
	Epoch 4250:	Loss 0.2437	TrainAcc 0.9485	ValidAcc 0.9441	TestAcc 0.9459	BestValid 0.9444
	Epoch 4300:	Loss 0.2430	TrainAcc 0.9483	ValidAcc 0.9440	TestAcc 0.9459	BestValid 0.9444
	Epoch 4350:	Loss 0.2436	TrainAcc 0.9487	ValidAcc 0.9443	TestAcc 0.9463	BestValid 0.9444
	Epoch 4400:	Loss 0.2424	TrainAcc 0.9486	ValidAcc 0.9440	TestAcc 0.9458	BestValid 0.9444
	Epoch 4450:	Loss 0.2423	TrainAcc 0.9485	ValidAcc 0.9443	TestAcc 0.9462	BestValid 0.9444
	Epoch 4500:	Loss 0.2424	TrainAcc 0.9490	ValidAcc 0.9452	TestAcc 0.9467	BestValid 0.9452
	Epoch 4550:	Loss 0.2415	TrainAcc 0.9471	ValidAcc 0.9431	TestAcc 0.9444	BestValid 0.9452
	Epoch 4600:	Loss 0.2406	TrainAcc 0.9478	ValidAcc 0.9435	TestAcc 0.9454	BestValid 0.9452
	Epoch 4650:	Loss 0.2402	TrainAcc 0.9482	ValidAcc 0.9438	TestAcc 0.9456	BestValid 0.9452
	Epoch 4700:	Loss 0.2405	TrainAcc 0.9492	ValidAcc 0.9446	TestAcc 0.9463	BestValid 0.9452
	Epoch 4750:	Loss 0.2400	TrainAcc 0.9486	ValidAcc 0.9437	TestAcc 0.9455	BestValid 0.9452
	Epoch 4800:	Loss 0.2404	TrainAcc 0.9489	ValidAcc 0.9438	TestAcc 0.9456	BestValid 0.9452
	Epoch 4850:	Loss 0.2391	TrainAcc 0.9482	ValidAcc 0.9440	TestAcc 0.9456	BestValid 0.9452
	Epoch 4900:	Loss 0.2388	TrainAcc 0.9495	ValidAcc 0.9443	TestAcc 0.9464	BestValid 0.9452
	Epoch 4950:	Loss 0.2393	TrainAcc 0.9490	ValidAcc 0.9437	TestAcc 0.9461	BestValid 0.9452
	Epoch 5000:	Loss 0.2378	TrainAcc 0.9488	ValidAcc 0.9446	TestAcc 0.9465	BestValid 0.9452
****** Epoch Time (Excluding Evaluation Cost): 0.075 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 14.341 ms (Max: 16.431, Min: 8.929, Sum: 114.730)
Cluster-Wide Average, Compute: 32.081 ms (Max: 35.324, Min: 29.427, Sum: 256.650)
Cluster-Wide Average, Communication-Layer: 11.230 ms (Max: 13.483, Min: 8.702, Sum: 89.843)
Cluster-Wide Average, Bubble-Imbalance: 4.951 ms (Max: 7.007, Min: 3.070, Sum: 39.608)
Cluster-Wide Average, Communication-Graph: 8.738 ms (Max: 9.772, Min: 7.550, Sum: 69.906)
Cluster-Wide Average, Optimization: 0.629 ms (Max: 0.877, Min: 0.065, Sum: 5.035)
Cluster-Wide Average, Others: 2.750 ms (Max: 9.038, Min: 0.614, Sum: 22.003)
****** Breakdown Sum: 74.722 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.521 GB (Max: 4.434, Min: 3.227, Sum: 28.167)
Cluster-Wide Average, Graph-Level Communication Throughput: 100.827 Gbps (Max: 123.750, Min: 81.235, Sum: 806.618)
Cluster-Wide Average, Layer-Level Communication Throughput: 48.620 Gbps (Max: 62.611, Min: 36.501, Sum: 388.963)
Layer-level communication (cluster-wide, per-epoch): 0.521 GB
Graph-level communication (cluster-wide, per-epoch): 0.668 GB
Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
Total communication (cluster-wide, per-epoch): 1.189 GB
****** Accuracy Results ******
Highest valid_acc: 0.9452
Target test_acc: 0.9467
Epoch to reach the target acc: 4499
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
