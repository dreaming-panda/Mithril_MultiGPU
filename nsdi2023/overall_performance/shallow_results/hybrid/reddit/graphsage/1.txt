Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.917 seconds.
Building the CSC structure...
        It takes 1.921 seconds.
Building the CSC structure...
        It takes 1.937 seconds.
Building the CSC structure...
        It takes 2.056 seconds.
Building the CSC structure...
        It takes 2.311 seconds.
Building the CSC structure...
        It takes 2.392 seconds.
Building the CSC structure...
        It takes 2.414 seconds.
Building the CSC structure...
        It takes 2.469 seconds.
Building the CSC structure...
        It takes 1.839 seconds.
        It takes 1.871 seconds.
        It takes 1.943 seconds.
        It takes 1.879 seconds.
        It takes 2.168 seconds.
        It takes 2.306 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.329 seconds.
        It takes 2.487 seconds.
        It takes 0.298 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.040 seconds.
        It takes 0.304 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.255 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.255 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.262 seconds.
Building the Label Vector...
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
        It takes 0.030 seconds.
Building the Feature Vector...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.047 seconds.
        It takes 0.263 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.031 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.610 Gbps (per GPU), 460.878 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.333 Gbps (per GPU), 458.661 Gbps (aggregated)
The layer-level communication performance: 57.332 Gbps (per GPU), 458.653 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.065 Gbps (per GPU), 456.519 Gbps (aggregated)
The layer-level communication performance: 57.102 Gbps (per GPU), 456.813 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.862 Gbps (per GPU), 454.897 Gbps (aggregated)
The layer-level communication performance: 56.788 Gbps (per GPU), 454.300 Gbps (aggregated)
The layer-level communication performance: 56.831 Gbps (per GPU), 454.647 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.805 Gbps (per GPU), 1262.441 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.811 Gbps (per GPU), 1262.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.811 Gbps (per GPU), 1262.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.743 Gbps (per GPU), 1261.943 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.838 Gbps (per GPU), 1262.701 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.790 Gbps (per GPU), 1262.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.811 Gbps (per GPU), 1262.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.758 Gbps (per GPU), 1262.062 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.724 Gbps (per GPU), 813.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.722 Gbps (per GPU), 813.776 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.720 Gbps (per GPU), 813.763 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.721 Gbps (per GPU), 813.770 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.723 Gbps (per GPU), 813.783 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.722 Gbps (per GPU), 813.776 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.721 Gbps (per GPU), 813.770 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.722 Gbps (per GPU), 813.776 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.923 Gbps (per GPU), 263.384 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.922 Gbps (per GPU), 263.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.923 Gbps (per GPU), 263.382 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.922 Gbps (per GPU), 263.375 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.924 Gbps (per GPU), 263.389 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.922 Gbps (per GPU), 263.373 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.923 Gbps (per GPU), 263.385 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.923 Gbps (per GPU), 263.385 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.27ms  2.56ms  2.42ms  1.35  8.38K  3.53M
 chk_1  3.58ms  2.83ms  2.69ms  1.33  6.74K  3.60M
 chk_2  3.42ms  2.68ms  2.55ms  1.34  7.27K  3.53M
 chk_3  3.45ms  2.72ms  2.57ms  1.35  7.92K  3.61M
 chk_4  3.15ms  2.65ms  2.55ms  1.24  5.33K  3.68M
 chk_5  3.54ms  2.64ms  2.43ms  1.46 10.07K  3.45M
 chk_6  3.70ms  2.81ms  2.60ms  1.42  9.41K  3.48M
 chk_7  3.36ms  2.67ms  2.49ms  1.35  8.12K  3.60M
 chk_8  3.36ms  2.78ms  2.64ms  1.27  6.09K  3.64M
 chk_9  3.60ms  2.57ms  2.32ms  1.55 11.10K  3.38M
chk_10  3.32ms  2.81ms  2.70ms  1.23  5.67K  3.63M
chk_11  3.38ms  2.68ms  2.50ms  1.35  8.16K  3.54M
chk_12  3.60ms  2.87ms  2.72ms  1.32  7.24K  3.55M
chk_13  3.21ms  2.70ms  2.59ms  1.24  5.41K  3.68M
chk_14  3.66ms  2.95ms  2.79ms  1.31  7.14K  3.53M
chk_15  3.64ms  2.79ms  2.59ms  1.41  9.25K  3.49M
chk_16  3.09ms  2.62ms  2.52ms  1.23  4.78K  3.77M
chk_17  3.43ms  2.75ms  2.61ms  1.31  6.85K  3.60M
chk_18  3.27ms  2.56ms  2.40ms  1.36  7.47K  3.57M
chk_19  3.13ms  2.63ms  2.53ms  1.24  4.88K  3.75M
chk_20  3.33ms  2.63ms  2.51ms  1.33  7.00K  3.63M
chk_21  3.15ms  2.62ms  2.50ms  1.26  5.41K  3.68M
chk_22  3.85ms  2.81ms  2.57ms  1.50 11.07K  3.39M
chk_23  3.45ms  2.71ms  2.57ms  1.34  7.23K  3.64M
chk_24  3.71ms  2.75ms  2.55ms  1.45 10.13K  3.43M
chk_25  3.20ms  2.59ms  2.47ms  1.30  6.40K  3.57M
chk_26  3.40ms  2.79ms  2.68ms  1.27  5.78K  3.55M
chk_27  3.55ms  2.68ms  2.48ms  1.43  9.34K  3.48M
chk_28  3.61ms  3.00ms  2.83ms  1.27  6.37K  3.57M
chk_29  3.32ms  2.81ms  2.68ms  1.24  5.16K  3.78M
chk_30  3.21ms  2.69ms  2.57ms  1.25  5.44K  3.67M
chk_31  3.45ms  2.83ms  2.71ms  1.27  6.33K  3.63M
   Avg  3.42  2.72  2.57
   Max  3.85  3.00  2.83
   Min  3.09  2.56  2.32
 Ratio  1.24  1.17  1.22
   Var  0.04  0.01  0.01
Profiling takes 3.247 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 10)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [18, 26)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 10)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [18, 26)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [10, 18)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [26, 33)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [10, 18)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [26, 33)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[18, 26)...
+++++++++ Node 0 initializing the weights for op[0, 10)...
+++++++++ Node 5 initializing the weights for op[18, 26)...
+++++++++ Node 2 initializing the weights for op[10, 18)...
+++++++++ Node 6 initializing the weights for op[26, 33)...
+++++++++ Node 3 initializing the weights for op[10, 18)...
+++++++++ Node 7 initializing the weights for op[26, 33)...
+++++++++ Node 1 initializing the weights for op[0, 10)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 5.0358	TrainAcc 0.1633	ValidAcc 0.1673	TestAcc 0.1614	BestValid 0.1673
	Epoch 50:	Loss 1.1219	TrainAcc 0.8003	ValidAcc 0.8163	TestAcc 0.8125	BestValid 0.8163
	Epoch 100:	Loss 0.5119	TrainAcc 0.9160	ValidAcc 0.9243	TestAcc 0.9222	BestValid 0.9243
	Epoch 150:	Loss 0.4040	TrainAcc 0.9278	ValidAcc 0.9332	TestAcc 0.9332	BestValid 0.9332
	Epoch 200:	Loss 0.3563	TrainAcc 0.9339	ValidAcc 0.9381	TestAcc 0.9381	BestValid 0.9381
	Epoch 250:	Loss 0.3343	TrainAcc 0.9381	ValidAcc 0.9422	TestAcc 0.9419	BestValid 0.9422
	Epoch 300:	Loss 0.3191	TrainAcc 0.9410	ValidAcc 0.9448	TestAcc 0.9447	BestValid 0.9448
	Epoch 350:	Loss 0.3050	TrainAcc 0.9432	ValidAcc 0.9463	TestAcc 0.9465	BestValid 0.9463
	Epoch 400:	Loss 0.2937	TrainAcc 0.9449	ValidAcc 0.9475	TestAcc 0.9477	BestValid 0.9475
	Epoch 450:	Loss 0.2843	TrainAcc 0.9467	ValidAcc 0.9493	TestAcc 0.9493	BestValid 0.9493
	Epoch 500:	Loss 0.2785	TrainAcc 0.9483	ValidAcc 0.9510	TestAcc 0.9507	BestValid 0.9510
	Epoch 550:	Loss 0.2711	TrainAcc 0.9493	ValidAcc 0.9517	TestAcc 0.9519	BestValid 0.9517
	Epoch 600:	Loss 0.2663	TrainAcc 0.9501	ValidAcc 0.9524	TestAcc 0.9524	BestValid 0.9524
	Epoch 650:	Loss 0.2610	TrainAcc 0.9508	ValidAcc 0.9532	TestAcc 0.9529	BestValid 0.9532
	Epoch 700:	Loss 0.2554	TrainAcc 0.9516	ValidAcc 0.9539	TestAcc 0.9534	BestValid 0.9539
	Epoch 750:	Loss 0.2525	TrainAcc 0.9522	ValidAcc 0.9543	TestAcc 0.9536	BestValid 0.9543
	Epoch 800:	Loss 0.2503	TrainAcc 0.9528	ValidAcc 0.9546	TestAcc 0.9539	BestValid 0.9546
	Epoch 850:	Loss 0.2453	TrainAcc 0.9533	ValidAcc 0.9553	TestAcc 0.9545	BestValid 0.9553
	Epoch 900:	Loss 0.2410	TrainAcc 0.9542	ValidAcc 0.9554	TestAcc 0.9547	BestValid 0.9554
	Epoch 950:	Loss 0.2422	TrainAcc 0.9546	ValidAcc 0.9556	TestAcc 0.9549	BestValid 0.9556
	Epoch 1000:	Loss 0.2355	TrainAcc 0.9550	ValidAcc 0.9561	TestAcc 0.9548	BestValid 0.9561
	Epoch 1050:	Loss 0.2347	TrainAcc 0.9557	ValidAcc 0.9561	TestAcc 0.9555	BestValid 0.9561
	Epoch 1100:	Loss 0.2338	TrainAcc 0.9561	ValidAcc 0.9563	TestAcc 0.9556	BestValid 0.9563
	Epoch 1150:	Loss 0.2308	TrainAcc 0.9564	ValidAcc 0.9568	TestAcc 0.9560	BestValid 0.9568
	Epoch 1200:	Loss 0.2265	TrainAcc 0.9570	ValidAcc 0.9571	TestAcc 0.9565	BestValid 0.9571
	Epoch 1250:	Loss 0.2255	TrainAcc 0.9574	ValidAcc 0.9574	TestAcc 0.9566	BestValid 0.9574
	Epoch 1300:	Loss 0.2221	TrainAcc 0.9575	ValidAcc 0.9576	TestAcc 0.9565	BestValid 0.9576
	Epoch 1350:	Loss 0.2217	TrainAcc 0.9581	ValidAcc 0.9578	TestAcc 0.9569	BestValid 0.9578
	Epoch 1400:	Loss 0.2190	TrainAcc 0.9583	ValidAcc 0.9579	TestAcc 0.9572	BestValid 0.9579
	Epoch 1450:	Loss 0.2172	TrainAcc 0.9590	ValidAcc 0.9583	TestAcc 0.9578	BestValid 0.9583
	Epoch 1500:	Loss 0.2143	TrainAcc 0.9587	ValidAcc 0.9585	TestAcc 0.9577	BestValid 0.9585
	Epoch 1550:	Loss 0.2136	TrainAcc 0.9594	ValidAcc 0.9583	TestAcc 0.9578	BestValid 0.9585
	Epoch 1600:	Loss 0.2137	TrainAcc 0.9599	ValidAcc 0.9587	TestAcc 0.9582	BestValid 0.9587
	Epoch 1650:	Loss 0.2091	TrainAcc 0.9603	ValidAcc 0.9596	TestAcc 0.9588	BestValid 0.9596
	Epoch 1700:	Loss 0.2093	TrainAcc 0.9607	ValidAcc 0.9594	TestAcc 0.9587	BestValid 0.9596
	Epoch 1750:	Loss 0.2088	TrainAcc 0.9607	ValidAcc 0.9595	TestAcc 0.9586	BestValid 0.9596
	Epoch 1800:	Loss 0.2058	TrainAcc 0.9611	ValidAcc 0.9591	TestAcc 0.9589	BestValid 0.9596
	Epoch 1850:	Loss 0.2045	TrainAcc 0.9612	ValidAcc 0.9599	TestAcc 0.9592	BestValid 0.9599
	Epoch 1900:	Loss 0.2037	TrainAcc 0.9617	ValidAcc 0.9599	TestAcc 0.9597	BestValid 0.9599
	Epoch 1950:	Loss 0.2005	TrainAcc 0.9620	ValidAcc 0.9598	TestAcc 0.9598	BestValid 0.9599
	Epoch 2000:	Loss 0.1982	TrainAcc 0.9624	ValidAcc 0.9607	TestAcc 0.9601	BestValid 0.9607
	Epoch 2050:	Loss 0.1972	TrainAcc 0.9626	ValidAcc 0.9605	TestAcc 0.9601	BestValid 0.9607
	Epoch 2100:	Loss 0.1976	TrainAcc 0.9626	ValidAcc 0.9606	TestAcc 0.9596	BestValid 0.9607
	Epoch 2150:	Loss 0.1940	TrainAcc 0.9628	ValidAcc 0.9599	TestAcc 0.9598	BestValid 0.9607
	Epoch 2200:	Loss 0.1949	TrainAcc 0.9632	ValidAcc 0.9609	TestAcc 0.9601	BestValid 0.9609
	Epoch 2250:	Loss 0.1929	TrainAcc 0.9636	ValidAcc 0.9607	TestAcc 0.9606	BestValid 0.9609
	Epoch 2300:	Loss 0.1926	TrainAcc 0.9637	ValidAcc 0.9608	TestAcc 0.9601	BestValid 0.9609
	Epoch 2350:	Loss 0.1926	TrainAcc 0.9637	ValidAcc 0.9609	TestAcc 0.9601	BestValid 0.9609
	Epoch 2400:	Loss 0.1933	TrainAcc 0.9637	ValidAcc 0.9606	TestAcc 0.9602	BestValid 0.9609
	Epoch 2450:	Loss 0.1892	TrainAcc 0.9642	ValidAcc 0.9611	TestAcc 0.9605	BestValid 0.9611
	Epoch 2500:	Loss 0.1889	TrainAcc 0.9646	ValidAcc 0.9612	TestAcc 0.9606	BestValid 0.9612
	Epoch 2550:	Loss 0.1852	TrainAcc 0.9649	ValidAcc 0.9615	TestAcc 0.9612	BestValid 0.9615
	Epoch 2600:	Loss 0.1876	TrainAcc 0.9650	ValidAcc 0.9618	TestAcc 0.9612	BestValid 0.9618
	Epoch 2650:	Loss 0.1867	TrainAcc 0.9646	ValidAcc 0.9612	TestAcc 0.9603	BestValid 0.9618
	Epoch 2700:	Loss 0.1853	TrainAcc 0.9654	ValidAcc 0.9614	TestAcc 0.9611	BestValid 0.9618
	Epoch 2750:	Loss 0.1845	TrainAcc 0.9656	ValidAcc 0.9618	TestAcc 0.9609	BestValid 0.9618
	Epoch 2800:	Loss 0.1834	TrainAcc 0.9658	ValidAcc 0.9616	TestAcc 0.9609	BestValid 0.9618
	Epoch 2850:	Loss 0.1807	TrainAcc 0.9660	ValidAcc 0.9621	TestAcc 0.9613	BestValid 0.9621
	Epoch 2900:	Loss 0.1811	TrainAcc 0.9660	ValidAcc 0.9622	TestAcc 0.9610	BestValid 0.9622
	Epoch 2950:	Loss 0.1802	TrainAcc 0.9663	ValidAcc 0.9617	TestAcc 0.9611	BestValid 0.9622
	Epoch 3000:	Loss 0.1799	TrainAcc 0.9665	ValidAcc 0.9617	TestAcc 0.9614	BestValid 0.9622
	Epoch 3050:	Loss 0.1769	TrainAcc 0.9666	ValidAcc 0.9620	TestAcc 0.9611	BestValid 0.9622
	Epoch 3100:	Loss 0.1778	TrainAcc 0.9670	ValidAcc 0.9622	TestAcc 0.9615	BestValid 0.9622
	Epoch 3150:	Loss 0.1783	TrainAcc 0.9668	ValidAcc 0.9619	TestAcc 0.9614	BestValid 0.9622
	Epoch 3200:	Loss 0.1761	TrainAcc 0.9672	ValidAcc 0.9620	TestAcc 0.9613	BestValid 0.9622
	Epoch 3250:	Loss 0.1746	TrainAcc 0.9670	ValidAcc 0.9616	TestAcc 0.9610	BestValid 0.9622
	Epoch 3300:	Loss 0.1745	TrainAcc 0.9677	ValidAcc 0.9622	TestAcc 0.9613	BestValid 0.9622
	Epoch 3350:	Loss 0.1724	TrainAcc 0.9676	ValidAcc 0.9628	TestAcc 0.9614	BestValid 0.9628
	Epoch 3400:	Loss 0.1736	TrainAcc 0.9678	ValidAcc 0.9627	TestAcc 0.9612	BestValid 0.9628
	Epoch 3450:	Loss 0.1732	TrainAcc 0.9681	ValidAcc 0.9623	TestAcc 0.9616	BestValid 0.9628
	Epoch 3500:	Loss 0.1712	TrainAcc 0.9677	ValidAcc 0.9619	TestAcc 0.9613	BestValid 0.9628
	Epoch 3550:	Loss 0.1710	TrainAcc 0.9679	ValidAcc 0.9620	TestAcc 0.9613	BestValid 0.9628
	Epoch 3600:	Loss 0.1705	TrainAcc 0.9681	ValidAcc 0.9624	TestAcc 0.9616	BestValid 0.9628
	Epoch 3650:	Loss 0.1693	TrainAcc 0.9684	ValidAcc 0.9626	TestAcc 0.9614	BestValid 0.9628
	Epoch 3700:	Loss 0.1710	TrainAcc 0.9687	ValidAcc 0.9619	TestAcc 0.9616	BestValid 0.9628
	Epoch 3750:	Loss 0.1690	TrainAcc 0.9687	ValidAcc 0.9628	TestAcc 0.9618	BestValid 0.9628
	Epoch 3800:	Loss 0.1701	TrainAcc 0.9688	ValidAcc 0.9626	TestAcc 0.9620	BestValid 0.9628
	Epoch 3850:	Loss 0.1693	TrainAcc 0.9690	ValidAcc 0.9626	TestAcc 0.9620	BestValid 0.9628
	Epoch 3900:	Loss 0.1687	TrainAcc 0.9691	ValidAcc 0.9626	TestAcc 0.9614	BestValid 0.9628
	Epoch 3950:	Loss 0.1657	TrainAcc 0.9693	ValidAcc 0.9634	TestAcc 0.9620	BestValid 0.9634
	Epoch 4000:	Loss 0.1677	TrainAcc 0.9693	ValidAcc 0.9632	TestAcc 0.9620	BestValid 0.9634
	Epoch 4050:	Loss 0.1658	TrainAcc 0.9691	ValidAcc 0.9631	TestAcc 0.9620	BestValid 0.9634
	Epoch 4100:	Loss 0.1652	TrainAcc 0.9692	ValidAcc 0.9631	TestAcc 0.9621	BestValid 0.9634
	Epoch 4150:	Loss 0.1662	TrainAcc 0.9692	ValidAcc 0.9631	TestAcc 0.9620	BestValid 0.9634
	Epoch 4200:	Loss 0.1643	TrainAcc 0.9697	ValidAcc 0.9636	TestAcc 0.9621	BestValid 0.9636
	Epoch 4250:	Loss 0.1637	TrainAcc 0.9700	ValidAcc 0.9635	TestAcc 0.9620	BestValid 0.9636
	Epoch 4300:	Loss 0.1648	TrainAcc 0.9698	ValidAcc 0.9631	TestAcc 0.9623	BestValid 0.9636
	Epoch 4350:	Loss 0.1631	TrainAcc 0.9701	ValidAcc 0.9634	TestAcc 0.9621	BestValid 0.9636
	Epoch 4400:	Loss 0.1627	TrainAcc 0.9705	ValidAcc 0.9637	TestAcc 0.9622	BestValid 0.9637
	Epoch 4450:	Loss 0.1607	TrainAcc 0.9701	ValidAcc 0.9632	TestAcc 0.9619	BestValid 0.9637
	Epoch 4500:	Loss 0.1611	TrainAcc 0.9702	ValidAcc 0.9635	TestAcc 0.9619	BestValid 0.9637
	Epoch 4550:	Loss 0.1624	TrainAcc 0.9703	ValidAcc 0.9637	TestAcc 0.9623	BestValid 0.9637
	Epoch 4600:	Loss 0.1595	TrainAcc 0.9698	ValidAcc 0.9632	TestAcc 0.9620	BestValid 0.9637
	Epoch 4650:	Loss 0.1611	TrainAcc 0.9702	ValidAcc 0.9637	TestAcc 0.9620	BestValid 0.9637
	Epoch 4700:	Loss 0.1601	TrainAcc 0.9704	ValidAcc 0.9629	TestAcc 0.9624	BestValid 0.9637
	Epoch 4750:	Loss 0.1584	TrainAcc 0.9706	ValidAcc 0.9635	TestAcc 0.9621	BestValid 0.9637
	Epoch 4800:	Loss 0.1583	TrainAcc 0.9708	ValidAcc 0.9635	TestAcc 0.9623	BestValid 0.9637
	Epoch 4850:	Loss 0.1589	TrainAcc 0.9711	ValidAcc 0.9636	TestAcc 0.9626	BestValid 0.9637
	Epoch 4900:	Loss 0.1591	TrainAcc 0.9707	ValidAcc 0.9637	TestAcc 0.9622	BestValid 0.9637
	Epoch 4950:	Loss 0.1559	TrainAcc 0.9704	ValidAcc 0.9632	TestAcc 0.9614	BestValid 0.9637
	Epoch 5000:	Loss 0.1594	TrainAcc 0.9708	ValidAcc 0.9634	TestAcc 0.9618	BestValid 0.9637
****** Epoch Time (Excluding Evaluation Cost): 0.102 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.358 ms (Max: 19.826, Min: 13.091, Sum: 138.862)
Cluster-Wide Average, Compute: 50.199 ms (Max: 59.889, Min: 44.856, Sum: 401.589)
Cluster-Wide Average, Communication-Layer: 11.856 ms (Max: 14.522, Min: 9.182, Sum: 94.851)
Cluster-Wide Average, Bubble-Imbalance: 9.253 ms (Max: 13.411, Min: 1.355, Sum: 74.022)
Cluster-Wide Average, Communication-Graph: 9.019 ms (Max: 10.611, Min: 7.661, Sum: 72.153)
Cluster-Wide Average, Optimization: 1.478 ms (Max: 2.378, Min: 0.109, Sum: 11.827)
Cluster-Wide Average, Others: 3.177 ms (Max: 10.309, Min: 0.498, Sum: 25.415)
****** Breakdown Sum: 102.340 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.548 GB (Max: 4.762, Min: 3.184, Sum: 28.386)
Cluster-Wide Average, Graph-Level Communication Throughput: 97.437 Gbps (Max: 120.696, Min: 73.861, Sum: 779.498)
Cluster-Wide Average, Layer-Level Communication Throughput: 46.010 Gbps (Max: 58.667, Min: 34.811, Sum: 368.081)
Layer-level communication (cluster-wide, per-epoch): 0.521 GB
Graph-level communication (cluster-wide, per-epoch): 0.668 GB
Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
Total communication (cluster-wide, per-epoch): 1.190 GB
****** Accuracy Results ******
Highest valid_acc: 0.9637
Target test_acc: 0.9622
Epoch to reach the target acc: 4399
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
