Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.917 seconds.
Building the CSC structure...
        It takes 2.362 seconds.
Building the CSC structure...
        It takes 2.366 seconds.
Building the CSC structure...
        It takes 2.403 seconds.
Building the CSC structure...
        It takes 2.418 seconds.
Building the CSC structure...
        It takes 2.470 seconds.
Building the CSC structure...
        It takes 2.623 seconds.
Building the CSC structure...
        It takes 2.664 seconds.
Building the CSC structure...
        It takes 1.853 seconds.
        It takes 2.181 seconds.
        It takes 2.322 seconds.
        It takes 2.377 seconds.
        It takes 2.338 seconds.
        It takes 2.273 seconds.
Building the Feature Vector...
        It takes 2.356 seconds.
        It takes 0.287 seconds.
Building the Label Vector...
        It takes 0.043 seconds.
        It takes 2.444 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.258 seconds.
Building the Label Vector...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.029 seconds.
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.034 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.260 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/reddit/32_parts
The number of GCNII layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
        It takes 0.251 seconds.
Building the Label Vector...
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
        It takes 0.037 seconds.
        It takes 0.255 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.764 Gbps (per GPU), 462.113 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.496 Gbps (per GPU), 459.968 Gbps (aggregated)
The layer-level communication performance: 57.503 Gbps (per GPU), 460.027 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.229 Gbps (per GPU), 457.830 Gbps (aggregated)
The layer-level communication performance: 57.256 Gbps (per GPU), 458.045 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.032 Gbps (per GPU), 456.254 Gbps (aggregated)
The layer-level communication performance: 56.991 Gbps (per GPU), 455.931 Gbps (aggregated)
The layer-level communication performance: 56.957 Gbps (per GPU), 455.657 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.073 Gbps (per GPU), 1248.586 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.102 Gbps (per GPU), 1248.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.076 Gbps (per GPU), 1248.607 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.087 Gbps (per GPU), 1248.700 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.096 Gbps (per GPU), 1248.769 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.090 Gbps (per GPU), 1248.723 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.082 Gbps (per GPU), 1248.656 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.091 Gbps (per GPU), 1248.727 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.623 Gbps (per GPU), 804.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.620 Gbps (per GPU), 804.959 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.618 Gbps (per GPU), 804.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.624 Gbps (per GPU), 804.990 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.621 Gbps (per GPU), 804.965 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.623 Gbps (per GPU), 804.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.622 Gbps (per GPU), 804.978 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.624 Gbps (per GPU), 804.991 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.763 Gbps (per GPU), 254.107 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.762 Gbps (per GPU), 254.096 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.763 Gbps (per GPU), 254.103 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.762 Gbps (per GPU), 254.100 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.763 Gbps (per GPU), 254.107 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.763 Gbps (per GPU), 254.106 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.763 Gbps (per GPU), 254.108 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.763 Gbps (per GPU), 254.103 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.29ms  2.58ms  2.41ms  1.36  8.38K  3.53M
 chk_1  3.51ms  2.83ms  2.70ms  1.30  6.74K  3.60M
 chk_2  3.43ms  2.72ms  2.56ms  1.34  7.27K  3.53M
 chk_3  3.48ms  2.75ms  2.57ms  1.35  7.92K  3.61M
 chk_4  3.16ms  2.79ms  2.56ms  1.24  5.33K  3.68M
 chk_5  3.56ms  2.65ms  2.44ms  1.46 10.07K  3.45M
 chk_6  3.70ms  2.83ms  2.61ms  1.42  9.41K  3.48M
 chk_7  3.37ms  2.67ms  2.52ms  1.34  8.12K  3.60M
 chk_8  3.36ms  2.79ms  2.65ms  1.27  6.09K  3.64M
 chk_9  3.60ms  2.58ms  2.34ms  1.54 11.10K  3.38M
chk_10  3.34ms  2.84ms  2.71ms  1.23  5.67K  3.63M
chk_11  3.39ms  2.69ms  2.51ms  1.35  8.16K  3.54M
chk_12  3.59ms  2.87ms  2.73ms  1.32  7.24K  3.55M
chk_13  3.22ms  2.72ms  2.60ms  1.24  5.41K  3.68M
chk_14  3.67ms  2.94ms  2.79ms  1.31  7.14K  3.53M
chk_15  3.65ms  2.81ms  2.59ms  1.41  9.25K  3.49M
chk_16  3.11ms  2.65ms  2.53ms  1.23  4.78K  3.77M
chk_17  3.43ms  2.77ms  2.62ms  1.31  6.85K  3.60M
chk_18  3.28ms  2.58ms  2.45ms  1.34  7.47K  3.57M
chk_19  3.14ms  2.66ms  2.56ms  1.23  4.88K  3.75M
chk_20  3.33ms  2.65ms  2.53ms  1.32  7.00K  3.63M
chk_21  3.15ms  2.55ms  2.52ms  1.25  5.41K  3.68M
chk_22  3.90ms  2.81ms  2.60ms  1.50 11.07K  3.39M
chk_23  3.47ms  2.72ms  2.61ms  1.33  7.23K  3.64M
chk_24  3.70ms  2.78ms  2.56ms  1.45 10.13K  3.43M
chk_25  3.22ms  2.60ms  2.47ms  1.30  6.40K  3.57M
chk_26  3.41ms  2.80ms  2.69ms  1.27  5.78K  3.55M
chk_27  3.55ms  2.68ms  2.47ms  1.44  9.34K  3.48M
chk_28  3.62ms  2.98ms  2.82ms  1.28  6.37K  3.57M
chk_29  3.35ms  2.78ms  2.66ms  1.26  5.16K  3.78M
chk_30  3.21ms  2.67ms  2.57ms  1.25  5.44K  3.67M
chk_31  3.46ms  2.83ms  2.72ms  1.27  6.33K  3.63M
   Avg  3.43  2.74  2.58
   Max  3.90  2.98  2.82
   Min  3.11  2.55  2.34
 Ratio  1.25  1.17  1.21
   Var  0.04  0.01  0.01
Profiling takes 3.275 s
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [18, 26)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 10)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [18, 26)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 10)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [26, 33)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [10, 18)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [26, 33)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [10, 18)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[18, 26)...
+++++++++ Node 0 initializing the weights for op[0, 10)...
+++++++++ Node 5 initializing the weights for op[18, 26)...
+++++++++ Node 1 initializing the weights for op[0, 10)...
+++++++++ Node 6 initializing the weights for op[26, 33)...
+++++++++ Node 2 initializing the weights for op[10, 18)...
+++++++++ Node 7 initializing the weights for op[26, 33)...
+++++++++ Node 3 initializing the weights for op[10, 18)...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 896608
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 5.0411	TrainAcc 0.0877	ValidAcc 0.0748	TestAcc 0.0769	BestValid 0.0748
	Epoch 50:	Loss 1.1205	TrainAcc 0.7943	ValidAcc 0.8128	TestAcc 0.8090	BestValid 0.8128
	Epoch 100:	Loss 0.5158	TrainAcc 0.9141	ValidAcc 0.9225	TestAcc 0.9213	BestValid 0.9225
	Epoch 150:	Loss 0.4069	TrainAcc 0.9274	ValidAcc 0.9333	TestAcc 0.9326	BestValid 0.9333
	Epoch 200:	Loss 0.3624	TrainAcc 0.9337	ValidAcc 0.9385	TestAcc 0.9380	BestValid 0.9385
	Epoch 250:	Loss 0.3336	TrainAcc 0.9379	ValidAcc 0.9427	TestAcc 0.9419	BestValid 0.9427
	Epoch 300:	Loss 0.3152	TrainAcc 0.9414	ValidAcc 0.9453	TestAcc 0.9449	BestValid 0.9453
	Epoch 350:	Loss 0.3017	TrainAcc 0.9436	ValidAcc 0.9475	TestAcc 0.9470	BestValid 0.9475
	Epoch 400:	Loss 0.2911	TrainAcc 0.9455	ValidAcc 0.9494	TestAcc 0.9487	BestValid 0.9494
	Epoch 450:	Loss 0.2811	TrainAcc 0.9474	ValidAcc 0.9506	TestAcc 0.9502	BestValid 0.9506
	Epoch 500:	Loss 0.2753	TrainAcc 0.9490	ValidAcc 0.9525	TestAcc 0.9515	BestValid 0.9525
	Epoch 550:	Loss 0.2686	TrainAcc 0.9500	ValidAcc 0.9535	TestAcc 0.9521	BestValid 0.9535
	Epoch 600:	Loss 0.2618	TrainAcc 0.9509	ValidAcc 0.9543	TestAcc 0.9527	BestValid 0.9543
	Epoch 650:	Loss 0.2586	TrainAcc 0.9517	ValidAcc 0.9545	TestAcc 0.9529	BestValid 0.9545
	Epoch 700:	Loss 0.2541	TrainAcc 0.9524	ValidAcc 0.9551	TestAcc 0.9539	BestValid 0.9551
	Epoch 750:	Loss 0.2494	TrainAcc 0.9534	ValidAcc 0.9561	TestAcc 0.9545	BestValid 0.9561
	Epoch 800:	Loss 0.2469	TrainAcc 0.9539	ValidAcc 0.9559	TestAcc 0.9547	BestValid 0.9561
	Epoch 850:	Loss 0.2425	TrainAcc 0.9545	ValidAcc 0.9565	TestAcc 0.9552	BestValid 0.9565
	Epoch 900:	Loss 0.2386	TrainAcc 0.9553	ValidAcc 0.9570	TestAcc 0.9557	BestValid 0.9570
	Epoch 950:	Loss 0.2352	TrainAcc 0.9557	ValidAcc 0.9570	TestAcc 0.9558	BestValid 0.9570
	Epoch 1000:	Loss 0.2329	TrainAcc 0.9562	ValidAcc 0.9568	TestAcc 0.9559	BestValid 0.9570
	Epoch 1050:	Loss 0.2295	TrainAcc 0.9570	ValidAcc 0.9573	TestAcc 0.9565	BestValid 0.9573
	Epoch 1100:	Loss 0.2272	TrainAcc 0.9576	ValidAcc 0.9574	TestAcc 0.9568	BestValid 0.9574
	Epoch 1150:	Loss 0.2245	TrainAcc 0.9578	ValidAcc 0.9577	TestAcc 0.9569	BestValid 0.9577
	Epoch 1200:	Loss 0.2240	TrainAcc 0.9580	ValidAcc 0.9582	TestAcc 0.9571	BestValid 0.9582
	Epoch 1250:	Loss 0.2224	TrainAcc 0.9583	ValidAcc 0.9587	TestAcc 0.9573	BestValid 0.9587
	Epoch 1300:	Loss 0.2198	TrainAcc 0.9590	ValidAcc 0.9588	TestAcc 0.9576	BestValid 0.9588
	Epoch 1350:	Loss 0.2169	TrainAcc 0.9590	ValidAcc 0.9589	TestAcc 0.9579	BestValid 0.9589
	Epoch 1400:	Loss 0.2130	TrainAcc 0.9597	ValidAcc 0.9595	TestAcc 0.9580	BestValid 0.9595
	Epoch 1450:	Loss 0.2140	TrainAcc 0.9599	ValidAcc 0.9590	TestAcc 0.9579	BestValid 0.9595
	Epoch 1500:	Loss 0.2111	TrainAcc 0.9601	ValidAcc 0.9589	TestAcc 0.9581	BestValid 0.9595
	Epoch 1550:	Loss 0.2098	TrainAcc 0.9605	ValidAcc 0.9592	TestAcc 0.9582	BestValid 0.9595
	Epoch 1600:	Loss 0.2091	TrainAcc 0.9611	ValidAcc 0.9597	TestAcc 0.9590	BestValid 0.9597
	Epoch 1650:	Loss 0.2062	TrainAcc 0.9614	ValidAcc 0.9596	TestAcc 0.9589	BestValid 0.9597
	Epoch 1700:	Loss 0.2043	TrainAcc 0.9617	ValidAcc 0.9601	TestAcc 0.9593	BestValid 0.9601
	Epoch 1750:	Loss 0.2052	TrainAcc 0.9617	ValidAcc 0.9599	TestAcc 0.9590	BestValid 0.9601
	Epoch 1800:	Loss 0.2038	TrainAcc 0.9620	ValidAcc 0.9601	TestAcc 0.9592	BestValid 0.9601
	Epoch 1850:	Loss 0.2008	TrainAcc 0.9621	ValidAcc 0.9598	TestAcc 0.9596	BestValid 0.9601
	Epoch 1900:	Loss 0.2003	TrainAcc 0.9625	ValidAcc 0.9602	TestAcc 0.9593	BestValid 0.9602
	Epoch 1950:	Loss 0.1994	TrainAcc 0.9629	ValidAcc 0.9605	TestAcc 0.9598	BestValid 0.9605
	Epoch 2000:	Loss 0.1965	TrainAcc 0.9631	ValidAcc 0.9606	TestAcc 0.9599	BestValid 0.9606
	Epoch 2050:	Loss 0.1956	TrainAcc 0.9632	ValidAcc 0.9603	TestAcc 0.9598	BestValid 0.9606
	Epoch 2100:	Loss 0.1941	TrainAcc 0.9638	ValidAcc 0.9606	TestAcc 0.9600	BestValid 0.9606
	Epoch 2150:	Loss 0.1939	TrainAcc 0.9640	ValidAcc 0.9614	TestAcc 0.9603	BestValid 0.9614
	Epoch 2200:	Loss 0.1913	TrainAcc 0.9641	ValidAcc 0.9606	TestAcc 0.9599	BestValid 0.9614
	Epoch 2250:	Loss 0.1934	TrainAcc 0.9643	ValidAcc 0.9603	TestAcc 0.9602	BestValid 0.9614
	Epoch 2300:	Loss 0.1903	TrainAcc 0.9644	ValidAcc 0.9608	TestAcc 0.9603	BestValid 0.9614
	Epoch 2350:	Loss 0.1912	TrainAcc 0.9647	ValidAcc 0.9611	TestAcc 0.9607	BestValid 0.9614
	Epoch 2400:	Loss 0.1866	TrainAcc 0.9646	ValidAcc 0.9604	TestAcc 0.9602	BestValid 0.9614
	Epoch 2450:	Loss 0.1883	TrainAcc 0.9653	ValidAcc 0.9613	TestAcc 0.9606	BestValid 0.9614
	Epoch 2500:	Loss 0.1854	TrainAcc 0.9652	ValidAcc 0.9611	TestAcc 0.9608	BestValid 0.9614
	Epoch 2550:	Loss 0.1864	TrainAcc 0.9656	ValidAcc 0.9610	TestAcc 0.9609	BestValid 0.9614
	Epoch 2600:	Loss 0.1860	TrainAcc 0.9659	ValidAcc 0.9616	TestAcc 0.9612	BestValid 0.9616
	Epoch 2650:	Loss 0.1845	TrainAcc 0.9655	ValidAcc 0.9610	TestAcc 0.9610	BestValid 0.9616
	Epoch 2700:	Loss 0.1832	TrainAcc 0.9660	ValidAcc 0.9614	TestAcc 0.9610	BestValid 0.9616
	Epoch 2750:	Loss 0.1827	TrainAcc 0.9663	ValidAcc 0.9616	TestAcc 0.9612	BestValid 0.9616
	Epoch 2800:	Loss 0.1789	TrainAcc 0.9664	ValidAcc 0.9614	TestAcc 0.9613	BestValid 0.9616
	Epoch 2850:	Loss 0.1804	TrainAcc 0.9665	ValidAcc 0.9611	TestAcc 0.9612	BestValid 0.9616
	Epoch 2900:	Loss 0.1791	TrainAcc 0.9670	ValidAcc 0.9618	TestAcc 0.9613	BestValid 0.9618
	Epoch 2950:	Loss 0.1796	TrainAcc 0.9669	ValidAcc 0.9612	TestAcc 0.9612	BestValid 0.9618
	Epoch 3000:	Loss 0.1780	TrainAcc 0.9670	ValidAcc 0.9616	TestAcc 0.9613	BestValid 0.9618
	Epoch 3050:	Loss 0.1778	TrainAcc 0.9674	ValidAcc 0.9611	TestAcc 0.9620	BestValid 0.9618
	Epoch 3100:	Loss 0.1779	TrainAcc 0.9673	ValidAcc 0.9619	TestAcc 0.9615	BestValid 0.9619
	Epoch 3150:	Loss 0.1760	TrainAcc 0.9675	ValidAcc 0.9616	TestAcc 0.9616	BestValid 0.9619
	Epoch 3200:	Loss 0.1765	TrainAcc 0.9677	ValidAcc 0.9614	TestAcc 0.9616	BestValid 0.9619
	Epoch 3250:	Loss 0.1743	TrainAcc 0.9674	ValidAcc 0.9610	TestAcc 0.9619	BestValid 0.9619
	Epoch 3300:	Loss 0.1749	TrainAcc 0.9679	ValidAcc 0.9621	TestAcc 0.9620	BestValid 0.9621
	Epoch 3350:	Loss 0.1740	TrainAcc 0.9684	ValidAcc 0.9623	TestAcc 0.9622	BestValid 0.9623
	Epoch 3400:	Loss 0.1729	TrainAcc 0.9684	ValidAcc 0.9619	TestAcc 0.9621	BestValid 0.9623
	Epoch 3450:	Loss 0.1707	TrainAcc 0.9688	ValidAcc 0.9617	TestAcc 0.9622	BestValid 0.9623
	Epoch 3500:	Loss 0.1692	TrainAcc 0.9688	ValidAcc 0.9615	TestAcc 0.9624	BestValid 0.9623
	Epoch 3550:	Loss 0.1695	TrainAcc 0.9686	ValidAcc 0.9622	TestAcc 0.9621	BestValid 0.9623
	Epoch 3600:	Loss 0.1696	TrainAcc 0.9687	ValidAcc 0.9620	TestAcc 0.9622	BestValid 0.9623
	Epoch 3650:	Loss 0.1691	TrainAcc 0.9691	ValidAcc 0.9620	TestAcc 0.9625	BestValid 0.9623
	Epoch 3700:	Loss 0.1693	TrainAcc 0.9691	ValidAcc 0.9622	TestAcc 0.9624	BestValid 0.9623
	Epoch 3750:	Loss 0.1694	TrainAcc 0.9692	ValidAcc 0.9622	TestAcc 0.9625	BestValid 0.9623
	Epoch 3800:	Loss 0.1694	TrainAcc 0.9695	ValidAcc 0.9627	TestAcc 0.9625	BestValid 0.9627
	Epoch 3850:	Loss 0.1681	TrainAcc 0.9695	ValidAcc 0.9622	TestAcc 0.9623	BestValid 0.9627
	Epoch 3900:	Loss 0.1667	TrainAcc 0.9696	ValidAcc 0.9621	TestAcc 0.9624	BestValid 0.9627
	Epoch 3950:	Loss 0.1680	TrainAcc 0.9696	ValidAcc 0.9622	TestAcc 0.9625	BestValid 0.9627
	Epoch 4000:	Loss 0.1658	TrainAcc 0.9700	ValidAcc 0.9625	TestAcc 0.9627	BestValid 0.9627
	Epoch 4050:	Loss 0.1641	TrainAcc 0.9703	ValidAcc 0.9624	TestAcc 0.9627	BestValid 0.9627
	Epoch 4100:	Loss 0.1658	TrainAcc 0.9699	ValidAcc 0.9627	TestAcc 0.9626	BestValid 0.9627
	Epoch 4150:	Loss 0.1631	TrainAcc 0.9699	ValidAcc 0.9620	TestAcc 0.9625	BestValid 0.9627
	Epoch 4200:	Loss 0.1663	TrainAcc 0.9704	ValidAcc 0.9625	TestAcc 0.9627	BestValid 0.9627
	Epoch 4250:	Loss 0.1638	TrainAcc 0.9705	ValidAcc 0.9622	TestAcc 0.9626	BestValid 0.9627
	Epoch 4300:	Loss 0.1643	TrainAcc 0.9705	ValidAcc 0.9624	TestAcc 0.9629	BestValid 0.9627
	Epoch 4350:	Loss 0.1622	TrainAcc 0.9707	ValidAcc 0.9626	TestAcc 0.9628	BestValid 0.9627
	Epoch 4400:	Loss 0.1630	TrainAcc 0.9706	ValidAcc 0.9616	TestAcc 0.9623	BestValid 0.9627
	Epoch 4450:	Loss 0.1608	TrainAcc 0.9709	ValidAcc 0.9628	TestAcc 0.9627	BestValid 0.9628
	Epoch 4500:	Loss 0.1616	TrainAcc 0.9709	ValidAcc 0.9624	TestAcc 0.9628	BestValid 0.9628
	Epoch 4550:	Loss 0.1586	TrainAcc 0.9712	ValidAcc 0.9628	TestAcc 0.9627	BestValid 0.9628
	Epoch 4600:	Loss 0.1601	TrainAcc 0.9712	ValidAcc 0.9630	TestAcc 0.9631	BestValid 0.9630
	Epoch 4650:	Loss 0.1594	TrainAcc 0.9711	ValidAcc 0.9627	TestAcc 0.9631	BestValid 0.9630
	Epoch 4700:	Loss 0.1579	TrainAcc 0.9714	ValidAcc 0.9627	TestAcc 0.9627	BestValid 0.9630
	Epoch 4750:	Loss 0.1582	TrainAcc 0.9713	ValidAcc 0.9627	TestAcc 0.9627	BestValid 0.9630
	Epoch 4800:	Loss 0.1598	TrainAcc 0.9715	ValidAcc 0.9624	TestAcc 0.9626	BestValid 0.9630
	Epoch 4850:	Loss 0.1590	TrainAcc 0.9716	ValidAcc 0.9629	TestAcc 0.9630	BestValid 0.9630
	Epoch 4900:	Loss 0.1572	TrainAcc 0.9715	ValidAcc 0.9629	TestAcc 0.9629	BestValid 0.9630
	Epoch 4950:	Loss 0.1561	TrainAcc 0.9717	ValidAcc 0.9636	TestAcc 0.9631	BestValid 0.9636
	Epoch 5000:	Loss 0.1583	TrainAcc 0.9716	ValidAcc 0.9629	TestAcc 0.9631	BestValid 0.9636
****** Epoch Time (Excluding Evaluation Cost): 0.102 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.303 ms (Max: 19.790, Min: 13.103, Sum: 138.421)
Cluster-Wide Average, Compute: 50.139 ms (Max: 60.174, Min: 44.843, Sum: 401.114)
Cluster-Wide Average, Communication-Layer: 11.817 ms (Max: 14.456, Min: 9.088, Sum: 94.537)
Cluster-Wide Average, Bubble-Imbalance: 9.563 ms (Max: 13.689, Min: 1.309, Sum: 76.502)
Cluster-Wide Average, Communication-Graph: 9.000 ms (Max: 10.648, Min: 7.638, Sum: 72.000)
Cluster-Wide Average, Optimization: 1.472 ms (Max: 2.210, Min: 0.106, Sum: 11.777)
Cluster-Wide Average, Others: 3.171 ms (Max: 10.279, Min: 0.561, Sum: 25.365)
****** Breakdown Sum: 102.464 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.548 GB (Max: 4.762, Min: 3.184, Sum: 28.384)
Cluster-Wide Average, Graph-Level Communication Throughput: 97.551 Gbps (Max: 121.167, Min: 74.018, Sum: 780.405)
Cluster-Wide Average, Layer-Level Communication Throughput: 46.158 Gbps (Max: 58.960, Min: 35.290, Sum: 369.263)
Layer-level communication (cluster-wide, per-epoch): 0.521 GB
Graph-level communication (cluster-wide, per-epoch): 0.668 GB
Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
Total communication (cluster-wide, per-epoch): 1.190 GB
****** Accuracy Results ******
Highest valid_acc: 0.9636
Target test_acc: 0.9631
Epoch to reach the target acc: 4949
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
