Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.006 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.012 seconds.
        It takes 0.012 seconds.
        It takes 0.009 seconds.
        It takes 0.013 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 4
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.034 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 163
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.671 Gbps (per GPU), 485.366 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.366 Gbps (per GPU), 482.927 Gbps (aggregated)
The layer-level communication performance: 60.364 Gbps (per GPU), 482.909 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.120 Gbps (per GPU), 480.961 Gbps (aggregated)
The layer-level communication performance: 60.090 Gbps (per GPU), 480.719 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.851 Gbps (per GPU), 478.808 Gbps (aggregated)
The layer-level communication performance: 59.808 Gbps (per GPU), 478.462 Gbps (aggregated)
The layer-level communication performance: 59.776 Gbps (per GPU), 478.207 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.552 Gbps (per GPU), 1244.416 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.558 Gbps (per GPU), 1244.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.540 Gbps (per GPU), 1244.324 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.503 Gbps (per GPU), 1244.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.546 Gbps (per GPU), 1244.370 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.573 Gbps (per GPU), 1244.580 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.552 Gbps (per GPU), 1244.416 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.509 Gbps (per GPU), 1244.070 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.278 Gbps (per GPU), 810.226 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.275 Gbps (per GPU), 810.200 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.277 Gbps (per GPU), 810.220 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.276 Gbps (per GPU), 810.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.275 Gbps (per GPU), 810.200 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.278 Gbps (per GPU), 810.226 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.277 Gbps (per GPU), 810.213 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.278 Gbps (per GPU), 810.227 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.748 Gbps (per GPU), 245.982 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.746 Gbps (per GPU), 245.966 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.744 Gbps (per GPU), 245.953 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.745 Gbps (per GPU), 245.962 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.741 Gbps (per GPU), 245.924 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.744 Gbps (per GPU), 245.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.741 Gbps (per GPU), 245.927 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.739 Gbps (per GPU), 245.911 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.72ms  0.53ms  0.37ms  1.95  0.08K  0.02M
 chk_1  0.85ms  0.61ms  0.36ms  2.37  0.16K  0.01M
 chk_2  0.77ms  0.56ms  0.35ms  2.21  0.13K  0.01M
 chk_3  0.87ms  0.68ms  0.35ms  2.44  0.18K  0.01M
 chk_4  0.84ms  0.61ms  0.35ms  2.37  0.15K  0.01M
 chk_5  0.96ms  0.69ms  0.35ms  2.75  0.25K  0.01M
 chk_6  0.73ms  0.54ms  0.37ms  1.99  0.10K  0.02M
 chk_7  0.78ms  0.57ms  0.36ms  2.17  0.11K  0.02M
 chk_8  0.94ms  0.68ms  0.35ms  2.69  0.23K  0.01M
 chk_9  0.82ms  0.60ms  0.34ms  2.38  0.14K  0.01M
chk_10  0.93ms  0.67ms  0.36ms  2.61  0.20K  0.01M
chk_11  0.71ms  0.53ms  0.37ms  1.95  0.09K  0.02M
chk_12  0.84ms  0.61ms  0.36ms  2.35  0.16K  0.01M
chk_13  0.83ms  0.61ms  0.35ms  2.36  0.16K  0.01M
chk_14  0.82ms  0.61ms  0.35ms  2.38  0.14K  0.01M
chk_15  0.94ms  0.68ms  0.36ms  2.65  0.21K  0.01M
chk_16  0.86ms  0.69ms  0.35ms  2.43  0.18K  0.01M
chk_17  1.02ms  0.70ms  0.31ms  3.28  0.29K  0.01M
chk_18  1.03ms  0.71ms  0.31ms  3.31  0.31K  0.00M
chk_19  0.77ms  0.56ms  0.35ms  2.17  0.13K  0.01M
chk_20  0.77ms  0.56ms  0.35ms  2.20  0.13K  0.01M
chk_21  0.86ms  0.68ms  0.36ms  2.42  0.18K  0.01M
chk_22  0.77ms  0.55ms  0.35ms  2.21  0.13K  0.01M
chk_23  0.85ms  0.67ms  0.35ms  2.41  0.16K  0.01M
chk_24  0.72ms  0.53ms  0.36ms  1.99  0.09K  0.02M
chk_25  0.72ms  0.54ms  0.37ms  1.95  0.09K  0.02M
chk_26  0.86ms  0.68ms  0.35ms  2.45  0.18K  0.01M
chk_27  0.77ms  0.55ms  0.35ms  2.21  0.13K  0.01M
chk_28  0.85ms  0.68ms  0.35ms  2.42  0.17K  0.01M
chk_29  0.83ms  0.61ms  0.36ms  2.31  0.15K  0.01M
chk_30  0.95ms  0.69ms  0.35ms  2.70  0.24K  0.01M
chk_31  0.93ms  0.67ms  0.35ms  2.63  0.20K  0.01M
   Avg  0.84  0.62  0.35
   Max  1.03  0.71  0.37
   Min  0.71  0.53  0.31
 Ratio  1.45  1.34  1.20
   Var  0.01  0.00  0.00
Profiling takes 0.770 s
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 10)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [18, 26)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 10)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [10, 18)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [26, 33)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [18, 26)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [10, 18)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [26, 33)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 10)...
+++++++++ Node 6 initializing the weights for op[26, 33)...
+++++++++ Node 4 initializing the weights for op[18, 26)...
+++++++++ Node 5 initializing the weights for op[18, 26)...
+++++++++ Node 7 initializing the weights for op[26, 33)...
+++++++++ Node 1 initializing the weights for op[0, 10)...
+++++++++ Node 2 initializing the weights for op[10, 18)...
+++++++++ Node 3 initializing the weights for op[10, 18)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 3, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6266	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 0.6787	TrainAcc 0.8898	ValidAcc 0.4447	TestAcc 0.4198	BestValid 0.4447
	Epoch 100:	Loss 0.1898	TrainAcc 0.9816	ValidAcc 0.4802	TestAcc 0.4688	BestValid 0.4802
	Epoch 150:	Loss 0.1179	TrainAcc 0.9944	ValidAcc 0.5042	TestAcc 0.4947	BestValid 0.5042
	Epoch 200:	Loss 0.1112	TrainAcc 0.9952	ValidAcc 0.5096	TestAcc 0.4890	BestValid 0.5096
	Epoch 250:	Loss 0.0842	TrainAcc 0.9956	ValidAcc 0.5072	TestAcc 0.5053	BestValid 0.5096
	Epoch 300:	Loss 0.0797	TrainAcc 0.9936	ValidAcc 0.5120	TestAcc 0.4995	BestValid 0.5120
	Epoch 350:	Loss 0.0606	TrainAcc 0.9964	ValidAcc 0.5144	TestAcc 0.5043	BestValid 0.5144
	Epoch 400:	Loss 0.0639	TrainAcc 0.9956	ValidAcc 0.5144	TestAcc 0.5130	BestValid 0.5144
	Epoch 450:	Loss 0.0509	TrainAcc 0.9964	ValidAcc 0.5186	TestAcc 0.5187	BestValid 0.5186
	Epoch 500:	Loss 0.0626	TrainAcc 0.9984	ValidAcc 0.5240	TestAcc 0.5207	BestValid 0.5240
	Epoch 550:	Loss 0.0501	TrainAcc 0.9976	ValidAcc 0.5300	TestAcc 0.5264	BestValid 0.5300
	Epoch 600:	Loss 0.0602	TrainAcc 0.9972	ValidAcc 0.5294	TestAcc 0.5312	BestValid 0.5300
	Epoch 650:	Loss 0.0457	TrainAcc 0.9980	ValidAcc 0.5198	TestAcc 0.5303	BestValid 0.5300
	Epoch 700:	Loss 0.0437	TrainAcc 0.9980	ValidAcc 0.5270	TestAcc 0.5226	BestValid 0.5300
	Epoch 750:	Loss 0.0394	TrainAcc 0.9976	ValidAcc 0.5312	TestAcc 0.5216	BestValid 0.5312
	Epoch 800:	Loss 0.0470	TrainAcc 0.9980	ValidAcc 0.5270	TestAcc 0.5274	BestValid 0.5312
	Epoch 850:	Loss 0.0455	TrainAcc 0.9984	ValidAcc 0.5240	TestAcc 0.5303	BestValid 0.5312
	Epoch 900:	Loss 0.0385	TrainAcc 0.9980	ValidAcc 0.5270	TestAcc 0.5283	BestValid 0.5312
	Epoch 950:	Loss 0.0350	TrainAcc 0.9988	ValidAcc 0.5198	TestAcc 0.5274	BestValid 0.5312
	Epoch 1000:	Loss 0.0413	TrainAcc 0.9988	ValidAcc 0.5234	TestAcc 0.5303	BestValid 0.5312
	Epoch 1050:	Loss 0.0330	TrainAcc 0.9988	ValidAcc 0.5228	TestAcc 0.5283	BestValid 0.5312
	Epoch 1100:	Loss 0.0397	TrainAcc 0.9980	ValidAcc 0.5240	TestAcc 0.5245	BestValid 0.5312
	Epoch 1150:	Loss 0.0439	TrainAcc 0.9984	ValidAcc 0.5210	TestAcc 0.5216	BestValid 0.5312
	Epoch 1200:	Loss 0.0502	TrainAcc 0.9988	ValidAcc 0.5294	TestAcc 0.5293	BestValid 0.5312
	Epoch 1250:	Loss 0.0435	TrainAcc 0.9988	ValidAcc 0.5349	TestAcc 0.5351	BestValid 0.5349
	Epoch 1300:	Loss 0.0374	TrainAcc 0.9988	ValidAcc 0.5331	TestAcc 0.5274	BestValid 0.5349
	Epoch 1350:	Loss 0.0365	TrainAcc 0.9988	ValidAcc 0.5282	TestAcc 0.5264	BestValid 0.5349
	Epoch 1400:	Loss 0.0308	TrainAcc 0.9988	ValidAcc 0.5282	TestAcc 0.5351	BestValid 0.5349
	Epoch 1450:	Loss 0.0276	TrainAcc 0.9988	ValidAcc 0.5361	TestAcc 0.5399	BestValid 0.5361
	Epoch 1500:	Loss 0.0297	TrainAcc 0.9988	ValidAcc 0.5355	TestAcc 0.5437	BestValid 0.5361
	Epoch 1550:	Loss 0.0285	TrainAcc 0.9988	ValidAcc 0.5270	TestAcc 0.5437	BestValid 0.5361
	Epoch 1600:	Loss 0.0288	TrainAcc 0.9992	ValidAcc 0.5312	TestAcc 0.5485	BestValid 0.5361
	Epoch 1650:	Loss 0.0288	TrainAcc 0.9988	ValidAcc 0.5397	TestAcc 0.5485	BestValid 0.5397
	Epoch 1700:	Loss 0.0344	TrainAcc 0.9988	ValidAcc 0.5282	TestAcc 0.5447	BestValid 0.5397
	Epoch 1750:	Loss 0.0288	TrainAcc 0.9988	ValidAcc 0.5312	TestAcc 0.5514	BestValid 0.5397
	Epoch 1800:	Loss 0.0295	TrainAcc 0.9992	ValidAcc 0.5306	TestAcc 0.5408	BestValid 0.5397
	Epoch 1850:	Loss 0.0249	TrainAcc 0.9988	ValidAcc 0.5288	TestAcc 0.5514	BestValid 0.5397
	Epoch 1900:	Loss 0.0224	TrainAcc 0.9984	ValidAcc 0.5306	TestAcc 0.5447	BestValid 0.5397
	Epoch 1950:	Loss 0.0285	TrainAcc 0.9984	ValidAcc 0.5300	TestAcc 0.5427	BestValid 0.5397
	Epoch 2000:	Loss 0.0259	TrainAcc 0.9988	ValidAcc 0.5264	TestAcc 0.5427	BestValid 0.5397
	Epoch 2050:	Loss 0.0283	TrainAcc 0.9988	ValidAcc 0.5331	TestAcc 0.5437	BestValid 0.5397
	Epoch 2100:	Loss 0.0290	TrainAcc 0.9988	ValidAcc 0.5409	TestAcc 0.5466	BestValid 0.5409
	Epoch 2150:	Loss 0.0297	TrainAcc 0.9992	ValidAcc 0.5331	TestAcc 0.5495	BestValid 0.5409
	Epoch 2200:	Loss 0.0214	TrainAcc 0.9992	ValidAcc 0.5312	TestAcc 0.5447	BestValid 0.5409
	Epoch 2250:	Loss 0.0284	TrainAcc 0.9992	ValidAcc 0.5300	TestAcc 0.5476	BestValid 0.5409
	Epoch 2300:	Loss 0.0253	TrainAcc 0.9992	ValidAcc 0.5397	TestAcc 0.5504	BestValid 0.5409
	Epoch 2350:	Loss 0.0281	TrainAcc 0.9988	ValidAcc 0.5343	TestAcc 0.5476	BestValid 0.5409
	Epoch 2400:	Loss 0.0210	TrainAcc 0.9988	ValidAcc 0.5319	TestAcc 0.5514	BestValid 0.5409
	Epoch 2450:	Loss 0.0256	TrainAcc 0.9988	ValidAcc 0.5361	TestAcc 0.5591	BestValid 0.5409
	Epoch 2500:	Loss 0.0195	TrainAcc 0.9988	ValidAcc 0.5355	TestAcc 0.5562	BestValid 0.5409
	Epoch 2550:	Loss 0.0304	TrainAcc 0.9988	ValidAcc 0.5331	TestAcc 0.5437	BestValid 0.5409
	Epoch 2600:	Loss 0.0248	TrainAcc 0.9988	ValidAcc 0.5391	TestAcc 0.5524	BestValid 0.5409
	Epoch 2650:	Loss 0.0307	TrainAcc 0.9992	ValidAcc 0.5325	TestAcc 0.5399	BestValid 0.5409
	Epoch 2700:	Loss 0.0206	TrainAcc 0.9988	ValidAcc 0.5343	TestAcc 0.5504	BestValid 0.5409
	Epoch 2750:	Loss 0.0233	TrainAcc 0.9992	ValidAcc 0.5397	TestAcc 0.5466	BestValid 0.5409
	Epoch 2800:	Loss 0.0237	TrainAcc 0.9992	ValidAcc 0.5349	TestAcc 0.5408	BestValid 0.5409
	Epoch 2850:	Loss 0.0200	TrainAcc 0.9996	ValidAcc 0.5367	TestAcc 0.5514	BestValid 0.5409
	Epoch 2900:	Loss 0.0251	TrainAcc 0.9992	ValidAcc 0.5421	TestAcc 0.5456	BestValid 0.5421
	Epoch 2950:	Loss 0.0215	TrainAcc 0.9988	ValidAcc 0.5361	TestAcc 0.5447	BestValid 0.5421
	Epoch 3000:	Loss 0.0204	TrainAcc 0.9992	ValidAcc 0.5391	TestAcc 0.5543	BestValid 0.5421
	Epoch 3050:	Loss 0.0259	TrainAcc 0.9988	ValidAcc 0.5337	TestAcc 0.5495	BestValid 0.5421
	Epoch 3100:	Loss 0.0147	TrainAcc 0.9992	ValidAcc 0.5397	TestAcc 0.5466	BestValid 0.5421
	Epoch 3150:	Loss 0.0182	TrainAcc 0.9988	ValidAcc 0.5379	TestAcc 0.5466	BestValid 0.5421
	Epoch 3200:	Loss 0.0138	TrainAcc 0.9988	ValidAcc 0.5463	TestAcc 0.5495	BestValid 0.5463
	Epoch 3250:	Loss 0.0168	TrainAcc 0.9988	ValidAcc 0.5439	TestAcc 0.5476	BestValid 0.5463
	Epoch 3300:	Loss 0.0272	TrainAcc 0.9992	ValidAcc 0.5433	TestAcc 0.5504	BestValid 0.5463
	Epoch 3350:	Loss 0.0265	TrainAcc 0.9988	ValidAcc 0.5439	TestAcc 0.5514	BestValid 0.5463
	Epoch 3400:	Loss 0.0218	TrainAcc 0.9988	ValidAcc 0.5421	TestAcc 0.5418	BestValid 0.5463
	Epoch 3450:	Loss 0.0154	TrainAcc 0.9992	ValidAcc 0.5433	TestAcc 0.5476	BestValid 0.5463
	Epoch 3500:	Loss 0.0176	TrainAcc 0.9992	ValidAcc 0.5343	TestAcc 0.5466	BestValid 0.5463
	Epoch 3550:	Loss 0.0173	TrainAcc 0.9988	ValidAcc 0.5355	TestAcc 0.5543	BestValid 0.5463
	Epoch 3600:	Loss 0.0156	TrainAcc 0.9992	ValidAcc 0.5373	TestAcc 0.5543	BestValid 0.5463
	Epoch 3650:	Loss 0.0189	TrainAcc 1.0000	ValidAcc 0.5282	TestAcc 0.5524	BestValid 0.5463
	Epoch 3700:	Loss 0.0251	TrainAcc 0.9992	ValidAcc 0.5343	TestAcc 0.5581	BestValid 0.5463
	Epoch 3750:	Loss 0.0240	TrainAcc 0.9988	ValidAcc 0.5355	TestAcc 0.5581	BestValid 0.5463
	Epoch 3800:	Loss 0.0170	TrainAcc 0.9992	ValidAcc 0.5294	TestAcc 0.5581	BestValid 0.5463
	Epoch 3850:	Loss 0.0249	TrainAcc 0.9992	ValidAcc 0.5433	TestAcc 0.5620	BestValid 0.5463
	Epoch 3900:	Loss 0.0170	TrainAcc 0.9992	ValidAcc 0.5403	TestAcc 0.5639	BestValid 0.5463
	Epoch 3950:	Loss 0.0218	TrainAcc 0.9992	ValidAcc 0.5433	TestAcc 0.5524	BestValid 0.5463
	Epoch 4000:	Loss 0.0211	TrainAcc 0.9988	ValidAcc 0.5457	TestAcc 0.5572	BestValid 0.5463
	Epoch 4050:	Loss 0.0180	TrainAcc 0.9988	ValidAcc 0.5421	TestAcc 0.5591	BestValid 0.5463
	Epoch 4100:	Loss 0.0142	TrainAcc 0.9988	ValidAcc 0.5439	TestAcc 0.5600	BestValid 0.5463
	Epoch 4150:	Loss 0.0140	TrainAcc 0.9988	ValidAcc 0.5385	TestAcc 0.5562	BestValid 0.5463
	Epoch 4200:	Loss 0.0178	TrainAcc 0.9988	ValidAcc 0.5379	TestAcc 0.5572	BestValid 0.5463
	Epoch 4250:	Loss 0.0146	TrainAcc 0.9988	ValidAcc 0.5373	TestAcc 0.5591	BestValid 0.5463
	Epoch 4300:	Loss 0.0140	TrainAcc 0.9988	ValidAcc 0.5355	TestAcc 0.5552	BestValid 0.5463
	Epoch 4350:	Loss 0.0179	TrainAcc 0.9988	ValidAcc 0.5331	TestAcc 0.5543	BestValid 0.5463
	Epoch 4400:	Loss 0.0166	TrainAcc 0.9988	ValidAcc 0.5403	TestAcc 0.5648	BestValid 0.5463
	Epoch 4450:	Loss 0.0121	TrainAcc 0.9988	ValidAcc 0.5391	TestAcc 0.5562	BestValid 0.5463
	Epoch 4500:	Loss 0.0142	TrainAcc 0.9988	ValidAcc 0.5457	TestAcc 0.5543	BestValid 0.5463
	Epoch 4550:	Loss 0.0126	TrainAcc 0.9988	ValidAcc 0.5379	TestAcc 0.5562	BestValid 0.5463
	Epoch 4600:	Loss 0.0190	TrainAcc 0.9988	ValidAcc 0.5367	TestAcc 0.5668	BestValid 0.5463
	Epoch 4650:	Loss 0.0204	TrainAcc 0.9988	ValidAcc 0.5421	TestAcc 0.5648	BestValid 0.5463
	Epoch 4700:	Loss 0.0196	TrainAcc 0.9988	ValidAcc 0.5415	TestAcc 0.5600	BestValid 0.5463
	Epoch 4750:	Loss 0.0158	TrainAcc 0.9992	ValidAcc 0.5403	TestAcc 0.5591	BestValid 0.5463
	Epoch 4800:	Loss 0.0186	TrainAcc 0.9988	ValidAcc 0.5385	TestAcc 0.5677	BestValid 0.5463
	Epoch 4850:	Loss 0.0213	TrainAcc 0.9992	ValidAcc 0.5403	TestAcc 0.5600	BestValid 0.5463
	Epoch 4900:	Loss 0.0177	TrainAcc 0.9988	ValidAcc 0.5397	TestAcc 0.5687	BestValid 0.5463
	Epoch 4950:	Loss 0.0183	TrainAcc 0.9992	ValidAcc 0.5403	TestAcc 0.5668	BestValid 0.5463
	Epoch 5000:	Loss 0.0171	TrainAcc 0.9988	ValidAcc 0.5433	TestAcc 0.5668	BestValid 0.5463
****** Epoch Time (Excluding Evaluation Cost): 0.035 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 4.168 ms (Max: 4.769, Min: 3.483, Sum: 33.347)
Cluster-Wide Average, Compute: 12.797 ms (Max: 17.848, Min: 7.861, Sum: 102.375)
Cluster-Wide Average, Communication-Layer: 3.097 ms (Max: 3.828, Min: 2.257, Sum: 24.777)
Cluster-Wide Average, Bubble-Imbalance: 4.410 ms (Max: 8.485, Min: 0.784, Sum: 35.284)
Cluster-Wide Average, Communication-Graph: 3.420 ms (Max: 3.653, Min: 3.110, Sum: 27.358)
Cluster-Wide Average, Optimization: 3.857 ms (Max: 6.569, Min: 1.041, Sum: 30.860)
Cluster-Wide Average, Others: 2.949 ms (Max: 6.092, Min: 0.238, Sum: 23.594)
****** Breakdown Sum: 34.699 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.393 GB (Max: 2.047, Min: 1.130, Sum: 11.144)
Cluster-Wide Average, Graph-Level Communication Throughput: 53.630 Gbps (Max: 61.899, Min: 46.815, Sum: 429.042)
Cluster-Wide Average, Layer-Level Communication Throughput: 39.400 Gbps (Max: 49.897, Min: 27.530, Sum: 315.201)
Layer-level communication (cluster-wide, per-epoch): 0.116 GB
Graph-level communication (cluster-wide, per-epoch): 0.128 GB
Weight-sync communication (cluster-wide, per-epoch): 0.061 GB
Total communication (cluster-wide, per-epoch): 0.305 GB
****** Accuracy Results ******
Highest valid_acc: 0.5463
Target test_acc: 0.5495
Epoch to reach the target acc: 3199
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
