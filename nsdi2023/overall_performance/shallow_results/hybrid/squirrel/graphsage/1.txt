Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 0 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.006 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.009 seconds.
        It takes 0.010 seconds.
        It takes 0.009 seconds.
        It takes 0.006 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCNII layers: 4
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.029 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.032 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546)WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.175 Gbps (per GPU), 481.399 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.881 Gbps (per GPU), 479.047 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.884 Gbps (per GPU), 479.073 Gbps (aggregated)
The layer-level communication performance: 59.639 Gbps (per GPU), 477.115 Gbps (aggregated)
The layer-level communication performance: 59.610 Gbps (per GPU), 476.878 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.407 Gbps (per GPU), 475.253 Gbps (aggregated)
The layer-level communication performance: 59.363 Gbps (per GPU), 474.903 Gbps (aggregated)
The layer-level communication performance: 59.333 Gbps (per GPU), 474.661 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.838 Gbps (per GPU), 1246.705 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.844 Gbps (per GPU), 1246.752 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.835 Gbps (per GPU), 1246.681 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.841 Gbps (per GPU), 1246.728 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.818 Gbps (per GPU), 1246.542 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.832 Gbps (per GPU), 1246.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.835 Gbps (per GPU), 1246.681 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.847 Gbps (per GPU), 1246.774 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 98.871 Gbps (per GPU), 790.968 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.869 Gbps (per GPU), 790.949 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.869 Gbps (per GPU), 790.949 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.872 Gbps (per GPU), 790.974 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.870 Gbps (per GPU), 790.961 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.872 Gbps (per GPU), 790.979 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.868 Gbps (per GPU), 790.943 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.870 Gbps (per GPU), 790.961 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.182 Gbps (per GPU), 241.457 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.183 Gbps (per GPU), 241.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.177 Gbps (per GPU), 241.414 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.178 Gbps (per GPU), 241.427 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.179 Gbps (per GPU), 241.433 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.178 Gbps (per GPU), 241.423 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.179 Gbps (per GPU), 241.436 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.178 Gbps (per GPU), 241.422 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.72ms  0.53ms  0.37ms  1.93  0.08K  0.02M
 chk_1  0.85ms  0.60ms  0.36ms  2.36  0.16K  0.01M
 chk_2  0.77ms  0.55ms  0.35ms  2.21  0.13K  0.01M
 chk_3  0.86ms  0.66ms  0.35ms  2.44  0.18K  0.01M
 chk_4  0.84ms  0.60ms  0.35ms  2.36  0.15K  0.01M
 chk_5  0.98ms  0.81ms  0.35ms  2.83  0.25K  0.01M
 chk_6  0.72ms  0.53ms  0.37ms  1.97  0.10K  0.02M
 chk_7  0.76ms  0.57ms  0.36ms  2.14  0.11K  0.02M
 chk_8  0.94ms  0.68ms  0.35ms  2.68  0.23K  0.01M
 chk_9  0.82ms  0.61ms  0.34ms  2.37  0.14K  0.01M
chk_10  0.93ms  0.67ms  0.36ms  2.61  0.20K  0.01M
chk_11  0.71ms  0.53ms  0.36ms  1.96  0.09K  0.02M
chk_12  0.83ms  0.61ms  0.35ms  2.35  0.16K  0.01M
chk_13  0.83ms  0.61ms  0.35ms  2.37  0.16K  0.01M
chk_14  0.82ms  0.61ms  0.35ms  2.38  0.14K  0.01M
chk_15  0.94ms  0.68ms  0.35ms  2.64  0.21K  0.01M
chk_16  0.86ms  0.69ms  0.35ms  2.43  0.18K  0.01M
chk_17  1.01ms  0.70ms  0.31ms  3.26  0.29K  0.01M
chk_18  1.03ms  0.71ms  0.31ms  3.31  0.31K  0.00M
chk_19  0.75ms  0.56ms  0.35ms  2.14  0.13K  0.01M
chk_20  0.75ms  0.56ms  0.35ms  2.16  0.13K  0.01M
chk_21  0.86ms  0.68ms  0.35ms  2.43  0.18K  0.01M
chk_22  0.76ms  0.56ms  0.35ms  2.19  0.13K  0.01M
chk_23  0.85ms  0.68ms  0.35ms  2.41  0.16K  0.01M
chk_24  0.72ms  0.53ms  0.36ms  1.98  0.09K  0.02M
chk_25  0.72ms  0.54ms  0.37ms  1.95  0.09K  0.02M
chk_26  0.86ms  0.68ms  0.35ms  2.44  0.18K  0.01M
chk_27  0.76ms  0.56ms  0.35ms  2.21  0.13K  0.01M
chk_28  0.85ms  0.68ms  0.35ms  2.41  0.17K  0.01M
chk_29  0.83ms  0.61ms  0.36ms  2.30  0.15K  0.01M
chk_30  1.07ms  0.69ms  0.35ms  3.04  0.24K  0.01M
chk_31  0.95ms  0.67ms  0.35ms  2.69  0.20K  0.01M
   Avg  0.84  0.62  0.35
   Max  1.07  0.81  0.37
   Min  0.71  0.53  0.31
 Ratio  1.50  1.54  1.19
   Var  0.01  0.00  0.00
Profiling takes 0.777 s
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [18, 26)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 10)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [18, 26)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 10)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [26, 33)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [10, 18)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [26, 33)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [10, 18)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 7 initializing the weights for op[26, 33)...
+++++++++ Node 0 initializing the weights for op[0, 10)...
+++++++++ Node 4 initializing the weights for op[18, 26)...
+++++++++ Node 3 initializing the weights for op[10, 18)...
+++++++++ Node 5 initializing the weights for op[18, 26)...
+++++++++ Node 6 initializing the weights for op[26, 33)...
+++++++++ Node 1 initializing the weights for op[0, 10)...
+++++++++ Node 2 initializing the weights for op[10, 18)...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6489	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 0.7201	TrainAcc 0.8722	ValidAcc 0.4453	TestAcc 0.4217	BestValid 0.4453
	Epoch 100:	Loss 0.2408	TrainAcc 0.9832	ValidAcc 0.4814	TestAcc 0.4659	BestValid 0.4814
	Epoch 150:	Loss 0.1394	TrainAcc 0.9912	ValidAcc 0.5018	TestAcc 0.4918	BestValid 0.5018
	Epoch 200:	Loss 0.1080	TrainAcc 0.9944	ValidAcc 0.5102	TestAcc 0.4957	BestValid 0.5102
	Epoch 250:	Loss 0.0841	TrainAcc 0.9956	ValidAcc 0.5162	TestAcc 0.5005	BestValid 0.5162
	Epoch 300:	Loss 0.0629	TrainAcc 0.9960	ValidAcc 0.5162	TestAcc 0.5120	BestValid 0.5162
	Epoch 350:	Loss 0.0727	TrainAcc 0.9968	ValidAcc 0.5216	TestAcc 0.5197	BestValid 0.5216
	Epoch 400:	Loss 0.0619	TrainAcc 0.9976	ValidAcc 0.5270	TestAcc 0.5130	BestValid 0.5270
	Epoch 450:	Loss 0.0637	TrainAcc 0.9972	ValidAcc 0.5331	TestAcc 0.5110	BestValid 0.5331
	Epoch 500:	Loss 0.0534	TrainAcc 0.9972	ValidAcc 0.5276	TestAcc 0.5139	BestValid 0.5331
	Epoch 550:	Loss 0.0503	TrainAcc 0.9980	ValidAcc 0.5288	TestAcc 0.5226	BestValid 0.5331
	Epoch 600:	Loss 0.0535	TrainAcc 0.9980	ValidAcc 0.5349	TestAcc 0.5139	BestValid 0.5349
	Epoch 650:	Loss 0.0496	TrainAcc 0.9984	ValidAcc 0.5367	TestAcc 0.5120	BestValid 0.5367
	Epoch 700:	Loss 0.0422	TrainAcc 0.9984	ValidAcc 0.5391	TestAcc 0.5235	BestValid 0.5391
	Epoch 750:	Loss 0.0365	TrainAcc 0.9976	ValidAcc 0.5391	TestAcc 0.5264	BestValid 0.5391
	Epoch 800:	Loss 0.0436	TrainAcc 0.9984	ValidAcc 0.5288	TestAcc 0.5255	BestValid 0.5391
	Epoch 850:	Loss 0.0410	TrainAcc 0.9980	ValidAcc 0.5373	TestAcc 0.5274	BestValid 0.5391
	Epoch 900:	Loss 0.0434	TrainAcc 0.9984	ValidAcc 0.5331	TestAcc 0.5341	BestValid 0.5391
	Epoch 950:	Loss 0.0358	TrainAcc 0.9984	ValidAcc 0.5379	TestAcc 0.5293	BestValid 0.5391
	Epoch 1000:	Loss 0.0397	TrainAcc 0.9988	ValidAcc 0.5331	TestAcc 0.5264	BestValid 0.5391
	Epoch 1050:	Loss 0.0258	TrainAcc 0.9984	ValidAcc 0.5421	TestAcc 0.5245	BestValid 0.5421
	Epoch 1100:	Loss 0.0326	TrainAcc 0.9984	ValidAcc 0.5337	TestAcc 0.5312	BestValid 0.5421
	Epoch 1150:	Loss 0.0285	TrainAcc 0.9988	ValidAcc 0.5409	TestAcc 0.5303	BestValid 0.5421
	Epoch 1200:	Loss 0.0325	TrainAcc 0.9992	ValidAcc 0.5529	TestAcc 0.5283	BestValid 0.5529
	Epoch 1250:	Loss 0.0396	TrainAcc 0.9988	ValidAcc 0.5421	TestAcc 0.5216	BestValid 0.5529
	Epoch 1300:	Loss 0.0248	TrainAcc 0.9988	ValidAcc 0.5439	TestAcc 0.5322	BestValid 0.5529
	Epoch 1350:	Loss 0.0245	TrainAcc 0.9988	ValidAcc 0.5397	TestAcc 0.5303	BestValid 0.5529
	Epoch 1400:	Loss 0.0287	TrainAcc 0.9988	ValidAcc 0.5475	TestAcc 0.5264	BestValid 0.5529
	Epoch 1450:	Loss 0.0306	TrainAcc 0.9988	ValidAcc 0.5439	TestAcc 0.5235	BestValid 0.5529
	Epoch 1500:	Loss 0.0379	TrainAcc 0.9988	ValidAcc 0.5463	TestAcc 0.5341	BestValid 0.5529
	Epoch 1550:	Loss 0.0304	TrainAcc 0.9988	ValidAcc 0.5433	TestAcc 0.5351	BestValid 0.5529
	Epoch 1600:	Loss 0.0325	TrainAcc 0.9988	ValidAcc 0.5445	TestAcc 0.5370	BestValid 0.5529
	Epoch 1650:	Loss 0.0319	TrainAcc 0.9988	ValidAcc 0.5451	TestAcc 0.5322	BestValid 0.5529
	Epoch 1700:	Loss 0.0219	TrainAcc 0.9988	ValidAcc 0.5475	TestAcc 0.5399	BestValid 0.5529
	Epoch 1750:	Loss 0.0284	TrainAcc 0.9988	ValidAcc 0.5499	TestAcc 0.5408	BestValid 0.5529
	Epoch 1800:	Loss 0.0335	TrainAcc 0.9988	ValidAcc 0.5505	TestAcc 0.5322	BestValid 0.5529
	Epoch 1850:	Loss 0.0229	TrainAcc 0.9988	ValidAcc 0.5535	TestAcc 0.5370	BestValid 0.5535
	Epoch 1900:	Loss 0.0210	TrainAcc 0.9988	ValidAcc 0.5421	TestAcc 0.5331	BestValid 0.5535
	Epoch 1950:	Loss 0.0293	TrainAcc 0.9988	ValidAcc 0.5457	TestAcc 0.5447	BestValid 0.5535
	Epoch 2000:	Loss 0.0240	TrainAcc 0.9988	ValidAcc 0.5463	TestAcc 0.5427	BestValid 0.5535
	Epoch 2050:	Loss 0.0275	TrainAcc 0.9992	ValidAcc 0.5559	TestAcc 0.5447	BestValid 0.5559
	Epoch 2100:	Loss 0.0306	TrainAcc 0.9988	ValidAcc 0.5481	TestAcc 0.5456	BestValid 0.5559
	Epoch 2150:	Loss 0.0252	TrainAcc 0.9992	ValidAcc 0.5505	TestAcc 0.5389	BestValid 0.5559
	Epoch 2200:	Loss 0.0258	TrainAcc 0.9988	ValidAcc 0.5529	TestAcc 0.5408	BestValid 0.5559
	Epoch 2250:	Loss 0.0263	TrainAcc 0.9988	ValidAcc 0.5517	TestAcc 0.5379	BestValid 0.5559
	Epoch 2300:	Loss 0.0228	TrainAcc 0.9992	ValidAcc 0.5493	TestAcc 0.5418	BestValid 0.5559
	Epoch 2350:	Loss 0.0253	TrainAcc 0.9988	ValidAcc 0.5547	TestAcc 0.5485	BestValid 0.5559
	Epoch 2400:	Loss 0.0268	TrainAcc 0.9988	ValidAcc 0.5511	TestAcc 0.5437	BestValid 0.5559
	Epoch 2450:	Loss 0.0254	TrainAcc 0.9992	ValidAcc 0.5535	TestAcc 0.5379	BestValid 0.5559
	Epoch 2500:	Loss 0.0228	TrainAcc 0.9992	ValidAcc 0.5589	TestAcc 0.5437	BestValid 0.5589
	Epoch 2550:	Loss 0.0218	TrainAcc 0.9988	ValidAcc 0.5541	TestAcc 0.5456	BestValid 0.5589
	Epoch 2600:	Loss 0.0272	TrainAcc 0.9988	ValidAcc 0.5511	TestAcc 0.5456	BestValid 0.5589
	Epoch 2650:	Loss 0.0219	TrainAcc 0.9988	ValidAcc 0.5529	TestAcc 0.5456	BestValid 0.5589
	Epoch 2700:	Loss 0.0180	TrainAcc 0.9992	ValidAcc 0.5517	TestAcc 0.5447	BestValid 0.5589
	Epoch 2750:	Loss 0.0227	TrainAcc 0.9992	ValidAcc 0.5541	TestAcc 0.5533	BestValid 0.5589
	Epoch 2800:	Loss 0.0192	TrainAcc 0.9988	ValidAcc 0.5475	TestAcc 0.5485	BestValid 0.5589
	Epoch 2850:	Loss 0.0185	TrainAcc 0.9988	ValidAcc 0.5505	TestAcc 0.5514	BestValid 0.5589
	Epoch 2900:	Loss 0.0217	TrainAcc 0.9988	ValidAcc 0.5535	TestAcc 0.5437	BestValid 0.5589
	Epoch 2950:	Loss 0.0303	TrainAcc 0.9988	ValidAcc 0.5583	TestAcc 0.5504	BestValid 0.5589
	Epoch 3000:	Loss 0.0205	TrainAcc 0.9988	ValidAcc 0.5505	TestAcc 0.5476	BestValid 0.5589
	Epoch 3050:	Loss 0.0188	TrainAcc 0.9992	ValidAcc 0.5463	TestAcc 0.5466	BestValid 0.5589
	Epoch 3100:	Loss 0.0259	TrainAcc 0.9988	ValidAcc 0.5535	TestAcc 0.5514	BestValid 0.5589
	Epoch 3150:	Loss 0.0198	TrainAcc 0.9988	ValidAcc 0.5523	TestAcc 0.5437	BestValid 0.5589
	Epoch 3200:	Loss 0.0240	TrainAcc 0.9988	ValidAcc 0.5535	TestAcc 0.5370	BestValid 0.5589
	Epoch 3250:	Loss 0.0227	TrainAcc 0.9988	ValidAcc 0.5493	TestAcc 0.5437	BestValid 0.5589
	Epoch 3300:	Loss 0.0181	TrainAcc 0.9992	ValidAcc 0.5601	TestAcc 0.5427	BestValid 0.5601
	Epoch 3350:	Loss 0.0177	TrainAcc 0.9988	ValidAcc 0.5517	TestAcc 0.5456	BestValid 0.5601
	Epoch 3400:	Loss 0.0215	TrainAcc 0.9988	ValidAcc 0.5559	TestAcc 0.5476	BestValid 0.5601
	Epoch 3450:	Loss 0.0159	TrainAcc 0.9996	ValidAcc 0.5625	TestAcc 0.5466	BestValid 0.5625
	Epoch 3500:	Loss 0.0115	TrainAcc 0.9988	ValidAcc 0.5607	TestAcc 0.5485	BestValid 0.5625
	Epoch 3550:	Loss 0.0169	TrainAcc 0.9988	ValidAcc 0.5625	TestAcc 0.5466	BestValid 0.5625
	Epoch 3600:	Loss 0.0168	TrainAcc 0.9988	ValidAcc 0.5625	TestAcc 0.5495	BestValid 0.5625
	Epoch 3650:	Loss 0.0139	TrainAcc 0.9992	ValidAcc 0.5583	TestAcc 0.5456	BestValid 0.5625
	Epoch 3700:	Loss 0.0171	TrainAcc 0.9992	ValidAcc 0.5565	TestAcc 0.5466	BestValid 0.5625
	Epoch 3750:	Loss 0.0190	TrainAcc 0.9988	ValidAcc 0.5559	TestAcc 0.5456	BestValid 0.5625
	Epoch 3800:	Loss 0.0175	TrainAcc 0.9992	ValidAcc 0.5571	TestAcc 0.5456	BestValid 0.5625
	Epoch 3850:	Loss 0.0149	TrainAcc 0.9996	ValidAcc 0.5613	TestAcc 0.5379	BestValid 0.5625
	Epoch 3900:	Loss 0.0228	TrainAcc 0.9988	ValidAcc 0.5583	TestAcc 0.5389	BestValid 0.5625
	Epoch 3950:	Loss 0.0151	TrainAcc 0.9992	ValidAcc 0.5583	TestAcc 0.5447	BestValid 0.5625
	Epoch 4000:	Loss 0.0200	TrainAcc 0.9992	ValidAcc 0.5529	TestAcc 0.5466	BestValid 0.5625
	Epoch 4050:	Loss 0.0256	TrainAcc 0.9988	ValidAcc 0.5505	TestAcc 0.5504	BestValid 0.5625
	Epoch 4100:	Loss 0.0158	TrainAcc 0.9992	ValidAcc 0.5529	TestAcc 0.5524	BestValid 0.5625
	Epoch 4150:	Loss 0.0144	TrainAcc 0.9992	ValidAcc 0.5535	TestAcc 0.5591	BestValid 0.5625
	Epoch 4200:	Loss 0.0202	TrainAcc 0.9988	ValidAcc 0.5511	TestAcc 0.5437	BestValid 0.5625
	Epoch 4250:	Loss 0.0172	TrainAcc 0.9996	ValidAcc 0.5535	TestAcc 0.5456	BestValid 0.5625
	Epoch 4300:	Loss 0.0133	TrainAcc 0.9988	ValidAcc 0.5541	TestAcc 0.5456	BestValid 0.5625
	Epoch 4350:	Loss 0.0179	TrainAcc 0.9992	ValidAcc 0.5565	TestAcc 0.5456	BestValid 0.5625
	Epoch 4400:	Loss 0.0136	TrainAcc 0.9992	ValidAcc 0.5463	TestAcc 0.5466	BestValid 0.5625
	Epoch 4450:	Loss 0.0219	TrainAcc 0.9992	ValidAcc 0.5541	TestAcc 0.5533	BestValid 0.5625
	Epoch 4500:	Loss 0.0161	TrainAcc 0.9988	ValidAcc 0.5565	TestAcc 0.5572	BestValid 0.5625
	Epoch 4550:	Loss 0.0220	TrainAcc 0.9992	ValidAcc 0.5547	TestAcc 0.5543	BestValid 0.5625
	Epoch 4600:	Loss 0.0196	TrainAcc 0.9996	ValidAcc 0.5571	TestAcc 0.5572	BestValid 0.5625
	Epoch 4650:	Loss 0.0149	TrainAcc 0.9996	ValidAcc 0.5595	TestAcc 0.5524	BestValid 0.5625
	Epoch 4700:	Loss 0.0220	TrainAcc 0.9996	ValidAcc 0.5529	TestAcc 0.5456	BestValid 0.5625
	Epoch 4750:	Loss 0.0139	TrainAcc 0.9992	ValidAcc 0.5643	TestAcc 0.5581	BestValid 0.5643
	Epoch 4800:	Loss 0.0119	TrainAcc 0.9992	ValidAcc 0.5607	TestAcc 0.5591	BestValid 0.5643
	Epoch 4850:	Loss 0.0158	TrainAcc 0.9992	ValidAcc 0.5607	TestAcc 0.5610	BestValid 0.5643
	Epoch 4900:	Loss 0.0137	TrainAcc 0.9988	ValidAcc 0.5583	TestAcc 0.5581	BestValid 0.5643
	Epoch 4950:	Loss 0.0141	TrainAcc 0.9996	ValidAcc 0.5643	TestAcc 0.5658	BestValid 0.5643
	Epoch 5000:	Loss 0.0173	TrainAcc 0.9992	ValidAcc 0.5631	TestAcc 0.5629	BestValid 0.5643
****** Epoch Time (Excluding Evaluation Cost): 0.035 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 4.162 ms (Max: 4.780, Min: 3.489, Sum: 33.297)
Cluster-Wide Average, Compute: 12.762 ms (Max: 17.769, Min: 7.768, Sum: 102.092)
Cluster-Wide Average, Communication-Layer: 3.140 ms (Max: 3.890, Min: 2.248, Sum: 25.123)
Cluster-Wide Average, Bubble-Imbalance: 4.309 ms (Max: 8.511, Min: 0.848, Sum: 34.470)
Cluster-Wide Average, Communication-Graph: 3.416 ms (Max: 3.584, Min: 3.275, Sum: 27.326)
Cluster-Wide Average, Optimization: 4.120 ms (Max: 7.012, Min: 1.041, Sum: 32.957)
Cluster-Wide Average, Others: 3.124 ms (Max: 6.518, Min: 0.249, Sum: 24.993)
****** Breakdown Sum: 35.032 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.393 GB (Max: 2.047, Min: 1.130, Sum: 11.146)
Cluster-Wide Average, Graph-Level Communication Throughput: 53.220 Gbps (Max: 57.550, Min: 48.482, Sum: 425.760)
Cluster-Wide Average, Layer-Level Communication Throughput: 38.953 Gbps (Max: 50.146, Min: 26.622, Sum: 311.625)
Layer-level communication (cluster-wide, per-epoch): 0.116 GB
Graph-level communication (cluster-wide, per-epoch): 0.128 GB
Weight-sync communication (cluster-wide, per-epoch): 0.061 GB
Total communication (cluster-wide, per-epoch): 0.305 GB
****** Accuracy Results ******
Highest valid_acc: 0.5643
Target test_acc: 0.5581
Epoch to reach the target acc: 4749
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
