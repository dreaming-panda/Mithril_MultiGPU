Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.014 seconds.
        It takes 0.015 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCN layers: 4
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.028 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.754 Gbps (per GPU), 454.034 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.489 Gbps (per GPU), 451.913 Gbps (aggregated)
The layer-level communication performance: 56.485 Gbps (per GPU), 451.882 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.248 Gbps (per GPU), 449.986 Gbps (aggregated)
The layer-level communication performance: 56.225 Gbps (per GPU), 449.796 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.024 Gbps (per GPU), 448.189 Gbps (aggregated)
The layer-level communication performance: 55.980 Gbps (per GPU), 447.844 Gbps (aggregated)
The layer-level communication performance: 55.955 Gbps (per GPU), 447.637 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.820 Gbps (per GPU), 1262.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.829 Gbps (per GPU), 1262.632 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.826 Gbps (per GPU), 1262.608 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.799 Gbps (per GPU), 1262.394 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.824 Gbps (per GPU), 1262.588 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.835 Gbps (per GPU), 1262.679 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.820 Gbps (per GPU), 1262.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.811 Gbps (per GPU), 1262.489 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 97.438 Gbps (per GPU), 779.502 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.441 Gbps (per GPU), 779.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.435 Gbps (per GPU), 779.478 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.439 Gbps (per GPU), 779.514 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.435 Gbps (per GPU), 779.484 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.442 Gbps (per GPU), 779.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.437 Gbps (per GPU), 779.496 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.429 Gbps (per GPU), 779.428 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.825 Gbps (per GPU), 270.597 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.820 Gbps (per GPU), 270.564 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.820 Gbps (per GPU), 270.563 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.821 Gbps (per GPU), 270.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.821 Gbps (per GPU), 270.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.817 Gbps (per GPU), 270.540 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.821 Gbps (per GPU), 270.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.815 Gbps (per GPU), 270.520 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.38ms  0.34ms  0.25ms  1.52  0.08K  0.02M
 chk_1  0.41ms  0.37ms  0.23ms  1.82  0.16K  0.01M
 chk_2  0.39ms  0.35ms  0.23ms  1.72  0.13K  0.01M
 chk_3  0.42ms  0.41ms  0.22ms  1.87  0.18K  0.01M
 chk_4  0.41ms  0.37ms  0.22ms  1.83  0.15K  0.01M
 chk_5  0.46ms  0.41ms  0.22ms  2.10  0.25K  0.01M
 chk_6  0.38ms  0.35ms  0.24ms  1.56  0.10K  0.02M
 chk_7  0.39ms  0.36ms  0.24ms  1.67  0.11K  0.02M
 chk_8  0.45ms  0.41ms  0.22ms  2.07  0.23K  0.01M
 chk_9  0.40ms  0.37ms  0.22ms  1.82  0.14K  0.01M
chk_10  0.46ms  0.41ms  0.23ms  2.04  0.20K  0.01M
chk_11  0.38ms  0.35ms  0.24ms  1.55  0.09K  0.02M
chk_12  0.41ms  0.37ms  0.22ms  1.83  0.16K  0.01M
chk_13  0.41ms  0.37ms  0.22ms  1.84  0.16K  0.01M
chk_14  0.40ms  0.37ms  0.22ms  1.83  0.14K  0.01M
chk_15  0.45ms  0.41ms  0.22ms  2.01  0.21K  0.01M
chk_16  0.42ms  0.41ms  0.22ms  1.87  0.18K  0.01M
chk_17  0.46ms  0.40ms  0.18ms  2.50  0.29K  0.01M
chk_18  0.47ms  0.41ms  0.18ms  2.57  0.31K  0.00M
chk_19  0.38ms  0.34ms  0.23ms  1.68  0.13K  0.01M
chk_20  0.38ms  0.35ms  0.23ms  1.70  0.13K  0.01M
chk_21  0.42ms  0.41ms  0.22ms  1.87  0.18K  0.01M
chk_22  0.38ms  0.34ms  0.22ms  1.71  0.13K  0.01M
chk_23  0.41ms  0.41ms  0.22ms  1.83  0.16K  0.01M
chk_24  0.37ms  0.34ms  0.24ms  1.55  0.09K  0.02M
chk_25  0.38ms  0.35ms  0.25ms  1.52  0.09K  0.02M
chk_26  0.41ms  0.41ms  0.22ms  1.86  0.18K  0.01M
chk_27  0.38ms  0.34ms  0.23ms  1.69  0.13K  0.01M
chk_28  0.41ms  0.41ms  0.22ms  1.85  0.17K  0.01M
chk_29  0.41ms  0.37ms  0.23ms  1.78  0.15K  0.01M
chk_30  0.45ms  0.41ms  0.22ms  2.06  0.24K  0.01M
chk_31  0.44ms  0.40ms  0.22ms  1.99  0.20K  0.01M
   Avg  0.41  0.38  0.22
   Max  0.47  0.41  0.25
   Min  0.37  0.34  0.18
 Ratio  1.25  1.20  1.36
   Var  0.00  0.00  0.00
Profiling takes 0.460 s
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [11, 16)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 6)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [11, 16)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 6)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [16, 20)
*** Node 6, constructing the helper classes...
*** Node 3, starting model training...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [6, 11)
*** Node 2, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [6, 11)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [16, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[16, 20)...
+++++++++ Node 7 initializing the weights for op[16, 20)...
+++++++++ Node 1 initializing the weights for op[0, 6)...
+++++++++ Node 3 initializing the weights for op[6, 11)...
+++++++++ Node 2 initializing the weights for op[6, 11)...
+++++++++ Node 4 initializing the weights for op[11, 16)...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 5 initializing the weights for op[11, 16)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6101	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 1.3211	TrainAcc 0.4103	ValidAcc 0.3089	TestAcc 0.3218	BestValid 0.3089
	Epoch 100:	Loss 1.1727	TrainAcc 0.4696	ValidAcc 0.3419	TestAcc 0.3449	BestValid 0.3419
	Epoch 150:	Loss 1.0879	TrainAcc 0.5493	ValidAcc 0.3960	TestAcc 0.3987	BestValid 0.3960
	Epoch 200:	Loss 1.0325	TrainAcc 0.5597	ValidAcc 0.3948	TestAcc 0.4102	BestValid 0.3960
	Epoch 250:	Loss 0.9776	TrainAcc 0.5236	ValidAcc 0.3642	TestAcc 0.3641	BestValid 0.3960
	Epoch 300:	Loss 0.9803	TrainAcc 0.5204	ValidAcc 0.3582	TestAcc 0.3727	BestValid 0.3960
	Epoch 350:	Loss 0.9345	TrainAcc 0.6022	ValidAcc 0.3876	TestAcc 0.4227	BestValid 0.3960
	Epoch 400:	Loss 0.9209	TrainAcc 0.5994	ValidAcc 0.3996	TestAcc 0.4150	BestValid 0.3996
	Epoch 450:	Loss 0.8965	TrainAcc 0.5974	ValidAcc 0.3906	TestAcc 0.4092	BestValid 0.3996
	Epoch 500:	Loss 0.8679	TrainAcc 0.5905	ValidAcc 0.3864	TestAcc 0.4006	BestValid 0.3996
	Epoch 550:	Loss 0.8490	TrainAcc 0.6390	ValidAcc 0.4225	TestAcc 0.4419	BestValid 0.4225
	Epoch 600:	Loss 0.8727	TrainAcc 0.6238	ValidAcc 0.4159	TestAcc 0.4025	BestValid 0.4225
	Epoch 650:	Loss 0.8538	TrainAcc 0.6226	ValidAcc 0.3954	TestAcc 0.4121	BestValid 0.4225
	Epoch 700:	Loss 0.8188	TrainAcc 0.6470	ValidAcc 0.4339	TestAcc 0.4457	BestValid 0.4339
	Epoch 750:	Loss 0.8064	TrainAcc 0.6338	ValidAcc 0.4099	TestAcc 0.4207	BestValid 0.4339
	Epoch 800:	Loss 0.8059	TrainAcc 0.6759	ValidAcc 0.4285	TestAcc 0.4515	BestValid 0.4339
	Epoch 850:	Loss 0.7830	TrainAcc 0.6583	ValidAcc 0.4231	TestAcc 0.4352	BestValid 0.4339
	Epoch 900:	Loss 0.8257	TrainAcc 0.6106	ValidAcc 0.4099	TestAcc 0.3939	BestValid 0.4339
	Epoch 950:	Loss 0.7803	TrainAcc 0.6695	ValidAcc 0.4123	TestAcc 0.4438	BestValid 0.4339
	Epoch 1000:	Loss 0.7688	TrainAcc 0.6727	ValidAcc 0.4249	TestAcc 0.4457	BestValid 0.4339
	Epoch 1050:	Loss 0.7531	TrainAcc 0.6502	ValidAcc 0.4141	TestAcc 0.4265	BestValid 0.4339
	Epoch 1100:	Loss 0.7476	TrainAcc 0.7051	ValidAcc 0.4417	TestAcc 0.4563	BestValid 0.4417
	Epoch 1150:	Loss 0.7437	TrainAcc 0.6711	ValidAcc 0.4183	TestAcc 0.4304	BestValid 0.4417
	Epoch 1200:	Loss 0.7670	TrainAcc 0.6583	ValidAcc 0.3996	TestAcc 0.4236	BestValid 0.4417
	Epoch 1250:	Loss 0.7372	TrainAcc 0.6595	ValidAcc 0.4183	TestAcc 0.4284	BestValid 0.4417
	Epoch 1300:	Loss 0.7318	TrainAcc 0.6883	ValidAcc 0.4309	TestAcc 0.4544	BestValid 0.4417
	Epoch 1350:	Loss 0.7241	TrainAcc 0.6562	ValidAcc 0.4123	TestAcc 0.4227	BestValid 0.4417
	Epoch 1400:	Loss 0.7160	TrainAcc 0.6807	ValidAcc 0.4123	TestAcc 0.4371	BestValid 0.4417
	Epoch 1450:	Loss 0.7634	TrainAcc 0.6975	ValidAcc 0.4363	TestAcc 0.4457	BestValid 0.4417
	Epoch 1500:	Loss 0.7179	TrainAcc 0.7220	ValidAcc 0.4447	TestAcc 0.4515	BestValid 0.4447
	Epoch 1550:	Loss 0.7216	TrainAcc 0.6619	ValidAcc 0.4237	TestAcc 0.4217	BestValid 0.4447
	Epoch 1600:	Loss 0.7147	TrainAcc 0.7280	ValidAcc 0.4507	TestAcc 0.4611	BestValid 0.4507
	Epoch 1650:	Loss 0.6927	TrainAcc 0.7083	ValidAcc 0.4321	TestAcc 0.4630	BestValid 0.4507
	Epoch 1700:	Loss 0.7031	TrainAcc 0.6747	ValidAcc 0.4213	TestAcc 0.4448	BestValid 0.4507
	Epoch 1750:	Loss 0.7175	TrainAcc 0.7159	ValidAcc 0.4507	TestAcc 0.4563	BestValid 0.4507
	Epoch 1800:	Loss 0.7028	TrainAcc 0.7276	ValidAcc 0.4483	TestAcc 0.4630	BestValid 0.4507
	Epoch 1850:	Loss 0.6913	TrainAcc 0.7087	ValidAcc 0.4327	TestAcc 0.4352	BestValid 0.4507
	Epoch 1900:	Loss 0.6749	TrainAcc 0.7119	ValidAcc 0.4405	TestAcc 0.4553	BestValid 0.4507
	Epoch 1950:	Loss 0.6847	TrainAcc 0.6987	ValidAcc 0.4261	TestAcc 0.4246	BestValid 0.4507
	Epoch 2000:	Loss 0.6802	TrainAcc 0.7143	ValidAcc 0.4495	TestAcc 0.4592	BestValid 0.4507
	Epoch 2050:	Loss 0.6808	TrainAcc 0.7424	ValidAcc 0.4423	TestAcc 0.4563	BestValid 0.4507
	Epoch 2100:	Loss 0.6826	TrainAcc 0.7384	ValidAcc 0.4507	TestAcc 0.4601	BestValid 0.4507
	Epoch 2150:	Loss 0.6623	TrainAcc 0.7268	ValidAcc 0.4549	TestAcc 0.4707	BestValid 0.4549
	Epoch 2200:	Loss 0.6563	TrainAcc 0.6931	ValidAcc 0.4159	TestAcc 0.4256	BestValid 0.4549
	Epoch 2250:	Loss 0.6589	TrainAcc 0.7316	ValidAcc 0.4483	TestAcc 0.4621	BestValid 0.4549
	Epoch 2300:	Loss 0.6704	TrainAcc 0.7131	ValidAcc 0.4333	TestAcc 0.4582	BestValid 0.4549
	Epoch 2350:	Loss 0.6817	TrainAcc 0.7308	ValidAcc 0.4495	TestAcc 0.4553	BestValid 0.4549
	Epoch 2400:	Loss 0.6506	TrainAcc 0.7504	ValidAcc 0.4597	TestAcc 0.4717	BestValid 0.4597
	Epoch 2450:	Loss 0.6521	TrainAcc 0.6546	ValidAcc 0.4171	TestAcc 0.4227	BestValid 0.4597
	Epoch 2500:	Loss 0.6579	TrainAcc 0.6979	ValidAcc 0.4375	TestAcc 0.4419	BestValid 0.4597
	Epoch 2550:	Loss 0.6539	TrainAcc 0.7027	ValidAcc 0.4423	TestAcc 0.4544	BestValid 0.4597
	Epoch 2600:	Loss 0.6602	TrainAcc 0.7300	ValidAcc 0.4688	TestAcc 0.4534	BestValid 0.4688
	Epoch 2650:	Loss 0.6464	TrainAcc 0.7692	ValidAcc 0.4645	TestAcc 0.4822	BestValid 0.4688
	Epoch 2700:	Loss 0.6479	TrainAcc 0.7516	ValidAcc 0.4603	TestAcc 0.4765	BestValid 0.4688
	Epoch 2750:	Loss 0.6358	TrainAcc 0.7143	ValidAcc 0.4447	TestAcc 0.4467	BestValid 0.4688
	Epoch 2800:	Loss 0.6341	TrainAcc 0.7188	ValidAcc 0.4447	TestAcc 0.4467	BestValid 0.4688
	Epoch 2850:	Loss 0.6302	TrainAcc 0.7300	ValidAcc 0.4688	TestAcc 0.4640	BestValid 0.4688
	Epoch 2900:	Loss 0.6613	TrainAcc 0.6963	ValidAcc 0.4297	TestAcc 0.4313	BestValid 0.4688
	Epoch 2950:	Loss 0.6370	TrainAcc 0.7208	ValidAcc 0.4435	TestAcc 0.4428	BestValid 0.4688
	Epoch 3000:	Loss 0.6475	TrainAcc 0.7496	ValidAcc 0.4688	TestAcc 0.4793	BestValid 0.4688
	Epoch 3050:	Loss 0.6252	TrainAcc 0.7704	ValidAcc 0.4712	TestAcc 0.4832	BestValid 0.4712
	Epoch 3100:	Loss 0.6493	TrainAcc 0.7484	ValidAcc 0.4495	TestAcc 0.4630	BestValid 0.4712
	Epoch 3150:	Loss 0.6249	TrainAcc 0.7512	ValidAcc 0.4609	TestAcc 0.4669	BestValid 0.4712
	Epoch 3200:	Loss 0.6399	TrainAcc 0.6883	ValidAcc 0.4099	TestAcc 0.4313	BestValid 0.4712
	Epoch 3250:	Loss 0.6360	TrainAcc 0.7580	ValidAcc 0.4772	TestAcc 0.4909	BestValid 0.4772
	Epoch 3300:	Loss 0.6085	TrainAcc 0.7488	ValidAcc 0.4603	TestAcc 0.4601	BestValid 0.4772
	Epoch 3350:	Loss 0.6248	TrainAcc 0.7416	ValidAcc 0.4633	TestAcc 0.4640	BestValid 0.4772
	Epoch 3400:	Loss 0.5987	TrainAcc 0.6867	ValidAcc 0.4231	TestAcc 0.4342	BestValid 0.4772
	Epoch 3450:	Loss 0.6208	TrainAcc 0.7220	ValidAcc 0.4417	TestAcc 0.4524	BestValid 0.4772
	Epoch 3500:	Loss 0.6220	TrainAcc 0.7432	ValidAcc 0.4573	TestAcc 0.4707	BestValid 0.4772
	Epoch 3550:	Loss 0.6263	TrainAcc 0.7528	ValidAcc 0.4657	TestAcc 0.4755	BestValid 0.4772
	Epoch 3600:	Loss 0.6114	TrainAcc 0.7400	ValidAcc 0.4423	TestAcc 0.4505	BestValid 0.4772
	Epoch 3650:	Loss 0.6045	TrainAcc 0.7708	ValidAcc 0.4748	TestAcc 0.4841	BestValid 0.4772
	Epoch 3700:	Loss 0.6168	TrainAcc 0.7444	ValidAcc 0.4597	TestAcc 0.4649	BestValid 0.4772
	Epoch 3750:	Loss 0.6483	TrainAcc 0.7684	ValidAcc 0.4760	TestAcc 0.4697	BestValid 0.4772
	Epoch 3800:	Loss 0.6432	TrainAcc 0.7320	ValidAcc 0.4688	TestAcc 0.4745	BestValid 0.4772
	Epoch 3850:	Loss 0.6032	TrainAcc 0.7684	ValidAcc 0.4675	TestAcc 0.4688	BestValid 0.4772
	Epoch 3900:	Loss 0.5904	TrainAcc 0.7556	ValidAcc 0.4681	TestAcc 0.4707	BestValid 0.4772
	Epoch 3950:	Loss 0.6011	TrainAcc 0.7328	ValidAcc 0.4495	TestAcc 0.4505	BestValid 0.4772
	Epoch 4000:	Loss 0.5869	TrainAcc 0.7576	ValidAcc 0.4621	TestAcc 0.4563	BestValid 0.4772
	Epoch 4050:	Loss 0.5891	TrainAcc 0.6947	ValidAcc 0.4387	TestAcc 0.4409	BestValid 0.4772
	Epoch 4100:	Loss 0.6358	TrainAcc 0.7188	ValidAcc 0.4399	TestAcc 0.4428	BestValid 0.4772
	Epoch 4150:	Loss 0.5935	TrainAcc 0.6943	ValidAcc 0.4213	TestAcc 0.4256	BestValid 0.4772
	Epoch 4200:	Loss 0.6040	TrainAcc 0.7744	ValidAcc 0.4742	TestAcc 0.4793	BestValid 0.4772
	Epoch 4250:	Loss 0.5910	TrainAcc 0.7448	ValidAcc 0.4591	TestAcc 0.4649	BestValid 0.4772
	Epoch 4300:	Loss 0.5902	TrainAcc 0.7704	ValidAcc 0.4663	TestAcc 0.4784	BestValid 0.4772
	Epoch 4350:	Loss 0.5926	TrainAcc 0.7228	ValidAcc 0.4537	TestAcc 0.4361	BestValid 0.4772
	Epoch 4400:	Loss 0.5767	TrainAcc 0.7825	ValidAcc 0.4808	TestAcc 0.4861	BestValid 0.4808
	Epoch 4450:	Loss 0.5754	TrainAcc 0.7788	ValidAcc 0.4675	TestAcc 0.4726	BestValid 0.4808
	Epoch 4500:	Loss 0.5825	TrainAcc 0.7812	ValidAcc 0.4856	TestAcc 0.4976	BestValid 0.4856
	Epoch 4550:	Loss 0.5732	TrainAcc 0.7480	ValidAcc 0.4657	TestAcc 0.4688	BestValid 0.4856
	Epoch 4600:	Loss 0.5731	TrainAcc 0.7712	ValidAcc 0.4754	TestAcc 0.4976	BestValid 0.4856
	Epoch 4650:	Loss 0.5934	TrainAcc 0.7155	ValidAcc 0.4423	TestAcc 0.4428	BestValid 0.4856
	Epoch 4700:	Loss 0.5797	TrainAcc 0.7817	ValidAcc 0.4790	TestAcc 0.5034	BestValid 0.4856
	Epoch 4750:	Loss 0.5853	TrainAcc 0.7220	ValidAcc 0.4213	TestAcc 0.4486	BestValid 0.4856
	Epoch 4800:	Loss 0.5670	TrainAcc 0.7560	ValidAcc 0.4603	TestAcc 0.4649	BestValid 0.4856
	Epoch 4850:	Loss 0.5729	TrainAcc 0.7925	ValidAcc 0.4784	TestAcc 0.4918	BestValid 0.4856
	Epoch 4900:	Loss 0.6544	TrainAcc 0.7572	ValidAcc 0.4555	TestAcc 0.4544	BestValid 0.4856
	Epoch 4950:	Loss 0.5812	TrainAcc 0.7704	ValidAcc 0.4681	TestAcc 0.4880	BestValid 0.4856
	Epoch 5000:	Loss 0.5802	TrainAcc 0.7656	ValidAcc 0.4537	TestAcc 0.4582	BestValid 0.4856
****** Epoch Time (Excluding Evaluation Cost): 0.021 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 2.601 ms (Max: 2.750, Min: 2.426, Sum: 20.804)
Cluster-Wide Average, Compute: 6.832 ms (Max: 8.863, Min: 4.303, Sum: 54.652)
Cluster-Wide Average, Communication-Layer: 2.785 ms (Max: 3.314, Min: 2.235, Sum: 22.279)
Cluster-Wide Average, Bubble-Imbalance: 2.530 ms (Max: 4.946, Min: 1.300, Sum: 20.240)
Cluster-Wide Average, Communication-Graph: 3.125 ms (Max: 3.332, Min: 2.935, Sum: 25.003)
Cluster-Wide Average, Optimization: 1.686 ms (Max: 2.813, Min: 1.017, Sum: 13.488)
Cluster-Wide Average, Others: 1.336 ms (Max: 2.351, Min: 0.167, Sum: 10.691)
****** Breakdown Sum: 20.895 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.287 GB (Max: 1.700, Min: 1.092, Sum: 10.298)
Cluster-Wide Average, Graph-Level Communication Throughput: 59.957 Gbps (Max: 66.241, Min: 53.986, Sum: 479.658)
Cluster-Wide Average, Layer-Level Communication Throughput: 43.529 Gbps (Max: 53.475, Min: 33.516, Sum: 348.228)
Layer-level communication (cluster-wide, per-epoch): 0.116 GB
Graph-level communication (cluster-wide, per-epoch): 0.128 GB
Weight-sync communication (cluster-wide, per-epoch): 0.031 GB
Total communication (cluster-wide, per-epoch): 0.274 GB
****** Accuracy Results ******
Highest valid_acc: 0.4856
Target test_acc: 0.4976
Epoch to reach the target acc: 4499
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
