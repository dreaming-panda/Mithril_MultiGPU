Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
        It takes 0.006 seconds.
        It takes 0.006 seconds.
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.011 seconds.
Building the Feature Vector...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.007 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/pipeline_parallel_datasets/squirrel/32_parts
The number of GCN layers: 4
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
Chunks (number of global chunks: 32): 0-[0, 80) 1-[80, 237) 2-[237, 364) 3-[364, 546) 4-[546, 693) 5-[693, 945) 6-[945, 1041) 7-[1041, 1148) 8-[1148, 1376) ... 31-[5004, 5201)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 163
5201, 401907, 401907
Number of vertices per chunk: 163
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.788 Gbps (per GPU), 478.301 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.507 Gbps (per GPU), 476.052 Gbps (aggregated)
The layer-level communication performance: 59.496 Gbps (per GPU), 475.967 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.268 Gbps (per GPU), 474.142 Gbps (aggregated)
The layer-level communication performance: 59.235 Gbps (per GPU), 473.882 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.027 Gbps (per GPU), 472.212 Gbps (aggregated)
The layer-level communication performance: 58.979 Gbps (per GPU), 471.833 Gbps (aggregated)
The layer-level communication performance: 58.950 Gbps (per GPU), 471.598 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.939 Gbps (per GPU), 1247.516 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.919 Gbps (per GPU), 1247.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.945 Gbps (per GPU), 1247.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.928 Gbps (per GPU), 1247.423 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.824 Gbps (per GPU), 1246.589 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.934 Gbps (per GPU), 1247.469 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.824 Gbps (per GPU), 1246.589 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.920 Gbps (per GPU), 1247.359 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 97.920 Gbps (per GPU), 783.360 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.909 Gbps (per GPU), 783.268 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.923 Gbps (per GPU), 783.384 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.883 Gbps (per GPU), 783.061 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.918 Gbps (per GPU), 783.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.882 Gbps (per GPU), 783.055 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.921 Gbps (per GPU), 783.372 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 97.887 Gbps (per GPU), 783.098 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.950 Gbps (per GPU), 255.597 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.950 Gbps (per GPU), 255.598 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.946 Gbps (per GPU), 255.565 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.948 Gbps (per GPU), 255.588 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.946 Gbps (per GPU), 255.572 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.945 Gbps (per GPU), 255.561 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.942 Gbps (per GPU), 255.534 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.943 Gbps (per GPU), 255.544 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.39ms  0.35ms  0.26ms  1.51  0.08K  0.02M
 chk_1  0.42ms  0.38ms  0.24ms  1.78  0.16K  0.01M
 chk_2  0.40ms  0.35ms  0.24ms  1.69  0.13K  0.01M
 chk_3  0.43ms  0.42ms  0.24ms  1.81  0.18K  0.01M
 chk_4  0.42ms  0.38ms  0.24ms  1.78  0.15K  0.01M
 chk_5  0.47ms  0.42ms  0.23ms  2.04  0.25K  0.01M
 chk_6  0.39ms  0.36ms  0.26ms  1.52  0.10K  0.02M
 chk_7  0.40ms  0.37ms  0.25ms  1.64  0.11K  0.02M
 chk_8  0.46ms  0.42ms  0.23ms  1.99  0.23K  0.01M
 chk_9  0.41ms  0.38ms  0.23ms  1.78  0.14K  0.01M
chk_10  0.47ms  0.42ms  0.24ms  1.99  0.20K  0.01M
chk_11  0.39ms  0.36ms  0.25ms  1.52  0.09K  0.02M
chk_12  0.42ms  0.38ms  0.24ms  1.79  0.16K  0.01M
chk_13  0.42ms  0.38ms  0.23ms  1.81  0.16K  0.01M
chk_14  0.42ms  0.38ms  0.23ms  1.81  0.14K  0.01M
chk_15  0.47ms  0.42ms  0.23ms  2.03  0.21K  0.01M
chk_16  0.43ms  0.42ms  0.23ms  1.85  0.18K  0.01M
chk_17  0.47ms  0.41ms  0.20ms  2.39  0.29K  0.01M
chk_18  0.48ms  0.42ms  0.19ms  2.49  0.31K  0.00M
chk_19  0.39ms  0.36ms  0.24ms  1.65  0.13K  0.01M
chk_20  0.40ms  0.36ms  0.24ms  1.68  0.13K  0.01M
chk_21  0.43ms  0.42ms  0.23ms  1.82  0.18K  0.01M
chk_22  0.40ms  0.36ms  0.23ms  1.69  0.13K  0.01M
chk_23  0.42ms  0.42ms  0.24ms  1.79  0.16K  0.01M
chk_24  0.39ms  0.35ms  0.25ms  1.54  0.09K  0.02M
chk_25  0.39ms  0.36ms  0.26ms  1.52  0.09K  0.02M
chk_26  0.43ms  0.42ms  0.23ms  1.83  0.18K  0.01M
chk_27  0.39ms  0.36ms  0.24ms  1.68  0.13K  0.01M
chk_28  0.42ms  0.42ms  0.23ms  1.82  0.17K  0.01M
chk_29  0.42ms  0.38ms  0.24ms  1.75  0.15K  0.01M
chk_30  0.46ms  0.42ms  0.23ms  2.01  0.24K  0.01M
chk_31  0.45ms  0.41ms  0.23ms  1.95  0.20K  0.01M
   Avg  0.42  0.39  0.24
   Max  0.48  0.42  0.26
   Min  0.39  0.35  0.19
 Ratio  1.23  1.20  1.35
   Var  0.00  0.00  0.00
Profiling takes 0.473 s
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [11, 16)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [6, 11)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [11, 16)
*** Node 5, constructing the helper classes...
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 6)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2460
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [16, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 2460
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 6)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [16, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 5, Local Vertex Begin: 2460, Num Local Vertices: 2741
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [6, 11)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2460, Num Local Vertices: 2741
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 6 initializing the weights for op[16, 20)...
+++++++++ Node 7 initializing the weights for op[16, 20)...
+++++++++ Node 4 initializing the weights for op[11, 16)...
+++++++++ Node 5 initializing the weights for op[11, 16)...
+++++++++ Node 1 initializing the weights for op[0, 6)...
+++++++++ Node 2 initializing the weights for op[6, 11)...
+++++++++ Node 3 initializing the weights for op[6, 11)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 17128
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6096	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 1.2808	TrainAcc 0.3922	ValidAcc 0.3131	TestAcc 0.3189	BestValid 0.3131
	Epoch 100:	Loss 1.1565	TrainAcc 0.5140	ValidAcc 0.3780	TestAcc 0.3958	BestValid 0.3780
	Epoch 150:	Loss 1.0763	TrainAcc 0.5172	ValidAcc 0.3798	TestAcc 0.3727	BestValid 0.3798
	Epoch 200:	Loss 1.0287	TrainAcc 0.4916	ValidAcc 0.3558	TestAcc 0.3516	BestValid 0.3798
	Epoch 250:	Loss 0.9812	TrainAcc 0.5321	ValidAcc 0.3756	TestAcc 0.3881	BestValid 0.3798
	Epoch 300:	Loss 0.9640	TrainAcc 0.6006	ValidAcc 0.4069	TestAcc 0.4207	BestValid 0.4069
	Epoch 350:	Loss 0.9670	TrainAcc 0.5893	ValidAcc 0.4056	TestAcc 0.3910	BestValid 0.4069
	Epoch 400:	Loss 0.9181	TrainAcc 0.5753	ValidAcc 0.3900	TestAcc 0.3948	BestValid 0.4069
	Epoch 450:	Loss 0.9033	TrainAcc 0.5401	ValidAcc 0.3588	TestAcc 0.3708	BestValid 0.4069
	Epoch 500:	Loss 0.8791	TrainAcc 0.5986	ValidAcc 0.4026	TestAcc 0.4169	BestValid 0.4069
	Epoch 550:	Loss 0.8615	TrainAcc 0.6466	ValidAcc 0.4345	TestAcc 0.4476	BestValid 0.4345
	Epoch 600:	Loss 0.8661	TrainAcc 0.5224	ValidAcc 0.3750	TestAcc 0.3900	BestValid 0.4345
	Epoch 650:	Loss 0.8459	TrainAcc 0.6611	ValidAcc 0.4309	TestAcc 0.4371	BestValid 0.4345
	Epoch 700:	Loss 0.8357	TrainAcc 0.6691	ValidAcc 0.4357	TestAcc 0.4496	BestValid 0.4357
	Epoch 750:	Loss 0.8165	TrainAcc 0.6506	ValidAcc 0.4291	TestAcc 0.4265	BestValid 0.4357
	Epoch 800:	Loss 0.8084	TrainAcc 0.6807	ValidAcc 0.4435	TestAcc 0.4524	BestValid 0.4435
	Epoch 850:	Loss 0.8066	TrainAcc 0.6398	ValidAcc 0.4117	TestAcc 0.4198	BestValid 0.4435
	Epoch 900:	Loss 0.8648	TrainAcc 0.6390	ValidAcc 0.4243	TestAcc 0.4275	BestValid 0.4435
	Epoch 950:	Loss 0.7830	TrainAcc 0.6759	ValidAcc 0.4285	TestAcc 0.4419	BestValid 0.4435
	Epoch 1000:	Loss 0.8015	TrainAcc 0.6663	ValidAcc 0.4243	TestAcc 0.4150	BestValid 0.4435
	Epoch 1050:	Loss 0.7747	TrainAcc 0.6595	ValidAcc 0.4243	TestAcc 0.4486	BestValid 0.4435
	Epoch 1100:	Loss 0.7609	TrainAcc 0.6498	ValidAcc 0.4153	TestAcc 0.4207	BestValid 0.4435
	Epoch 1150:	Loss 0.7542	TrainAcc 0.6811	ValidAcc 0.4327	TestAcc 0.4284	BestValid 0.4435
	Epoch 1200:	Loss 0.7702	TrainAcc 0.7019	ValidAcc 0.4387	TestAcc 0.4544	BestValid 0.4435
	Epoch 1250:	Loss 0.7528	TrainAcc 0.6931	ValidAcc 0.4327	TestAcc 0.4467	BestValid 0.4435
	Epoch 1300:	Loss 0.7358	TrainAcc 0.6530	ValidAcc 0.4195	TestAcc 0.4304	BestValid 0.4435
	Epoch 1350:	Loss 0.7295	TrainAcc 0.6903	ValidAcc 0.4285	TestAcc 0.4553	BestValid 0.4435
	Epoch 1400:	Loss 0.7361	TrainAcc 0.6755	ValidAcc 0.4231	TestAcc 0.4332	BestValid 0.4435
	Epoch 1450:	Loss 0.7767	TrainAcc 0.6783	ValidAcc 0.4393	TestAcc 0.4304	BestValid 0.4435
	Epoch 1500:	Loss 0.7257	TrainAcc 0.7111	ValidAcc 0.4381	TestAcc 0.4448	BestValid 0.4435
	Epoch 1550:	Loss 0.7379	TrainAcc 0.6699	ValidAcc 0.4171	TestAcc 0.4275	BestValid 0.4435
	Epoch 1600:	Loss 0.7217	TrainAcc 0.7244	ValidAcc 0.4621	TestAcc 0.4717	BestValid 0.4621
	Epoch 1650:	Loss 0.7220	TrainAcc 0.6775	ValidAcc 0.4243	TestAcc 0.4169	BestValid 0.4621
	Epoch 1700:	Loss 0.7138	TrainAcc 0.6631	ValidAcc 0.4183	TestAcc 0.3996	BestValid 0.4621
	Epoch 1750:	Loss 0.7283	TrainAcc 0.6434	ValidAcc 0.4147	TestAcc 0.4121	BestValid 0.4621
	Epoch 1800:	Loss 0.7014	TrainAcc 0.7292	ValidAcc 0.4471	TestAcc 0.4621	BestValid 0.4621
	Epoch 1850:	Loss 0.7064	TrainAcc 0.6887	ValidAcc 0.4291	TestAcc 0.4428	BestValid 0.4621
	Epoch 1900:	Loss 0.7109	TrainAcc 0.7079	ValidAcc 0.4351	TestAcc 0.4476	BestValid 0.4621
	Epoch 1950:	Loss 0.6983	TrainAcc 0.7163	ValidAcc 0.4459	TestAcc 0.4582	BestValid 0.4621
	Epoch 2000:	Loss 0.6879	TrainAcc 0.6851	ValidAcc 0.4129	TestAcc 0.4323	BestValid 0.4621
	Epoch 2050:	Loss 0.7373	TrainAcc 0.5745	ValidAcc 0.3768	TestAcc 0.3852	BestValid 0.4621
	Epoch 2100:	Loss 0.7227	TrainAcc 0.7288	ValidAcc 0.4531	TestAcc 0.4649	BestValid 0.4621
	Epoch 2150:	Loss 0.6871	TrainAcc 0.6843	ValidAcc 0.4243	TestAcc 0.4371	BestValid 0.4621
	Epoch 2200:	Loss 0.7188	TrainAcc 0.7143	ValidAcc 0.4315	TestAcc 0.4476	BestValid 0.4621
	Epoch 2250:	Loss 0.6828	TrainAcc 0.7328	ValidAcc 0.4579	TestAcc 0.4774	BestValid 0.4621
	Epoch 2300:	Loss 0.6710	TrainAcc 0.7179	ValidAcc 0.4447	TestAcc 0.4582	BestValid 0.4621
	Epoch 2350:	Loss 0.7237	TrainAcc 0.6266	ValidAcc 0.4056	TestAcc 0.3890	BestValid 0.4621
	Epoch 2400:	Loss 0.6828	TrainAcc 0.7444	ValidAcc 0.4549	TestAcc 0.4697	BestValid 0.4621
	Epoch 2450:	Loss 0.6643	TrainAcc 0.6727	ValidAcc 0.4219	TestAcc 0.4284	BestValid 0.4621
	Epoch 2500:	Loss 0.6643	TrainAcc 0.7147	ValidAcc 0.4339	TestAcc 0.4486	BestValid 0.4621
	Epoch 2550:	Loss 0.6976	TrainAcc 0.6979	ValidAcc 0.4279	TestAcc 0.4371	BestValid 0.4621
	Epoch 2600:	Loss 0.6899	TrainAcc 0.7344	ValidAcc 0.4435	TestAcc 0.4621	BestValid 0.4621
	Epoch 2650:	Loss 0.6649	TrainAcc 0.7276	ValidAcc 0.4405	TestAcc 0.4534	BestValid 0.4621
	Epoch 2700:	Loss 0.6754	TrainAcc 0.7344	ValidAcc 0.4567	TestAcc 0.4563	BestValid 0.4621
	Epoch 2750:	Loss 0.6506	TrainAcc 0.7448	ValidAcc 0.4603	TestAcc 0.4803	BestValid 0.4621
	Epoch 2800:	Loss 0.6457	TrainAcc 0.7492	ValidAcc 0.4573	TestAcc 0.4726	BestValid 0.4621
	Epoch 2850:	Loss 0.6427	TrainAcc 0.6995	ValidAcc 0.4183	TestAcc 0.4323	BestValid 0.4621
	Epoch 2900:	Loss 0.6750	TrainAcc 0.6619	ValidAcc 0.4231	TestAcc 0.4275	BestValid 0.4621
	Epoch 2950:	Loss 0.6570	TrainAcc 0.7388	ValidAcc 0.4603	TestAcc 0.4697	BestValid 0.4621
	Epoch 3000:	Loss 0.6534	TrainAcc 0.6979	ValidAcc 0.4297	TestAcc 0.4448	BestValid 0.4621
	Epoch 3050:	Loss 0.6551	TrainAcc 0.6779	ValidAcc 0.4333	TestAcc 0.4390	BestValid 0.4621
	Epoch 3100:	Loss 0.6634	TrainAcc 0.7308	ValidAcc 0.4465	TestAcc 0.4515	BestValid 0.4621
	Epoch 3150:	Loss 0.6305	TrainAcc 0.7436	ValidAcc 0.4597	TestAcc 0.4736	BestValid 0.4621
	Epoch 3200:	Loss 0.6782	TrainAcc 0.7071	ValidAcc 0.4393	TestAcc 0.4467	BestValid 0.4621
	Epoch 3250:	Loss 0.6538	TrainAcc 0.7348	ValidAcc 0.4555	TestAcc 0.4649	BestValid 0.4621
	Epoch 3300:	Loss 0.6306	TrainAcc 0.7564	ValidAcc 0.4657	TestAcc 0.4909	BestValid 0.4657
	Epoch 3350:	Loss 0.6351	TrainAcc 0.7528	ValidAcc 0.4465	TestAcc 0.4544	BestValid 0.4657
	Epoch 3400:	Loss 0.6213	TrainAcc 0.7276	ValidAcc 0.4597	TestAcc 0.4745	BestValid 0.4657
	Epoch 3450:	Loss 0.6228	TrainAcc 0.6811	ValidAcc 0.4279	TestAcc 0.4198	BestValid 0.4657
	Epoch 3500:	Loss 0.6724	TrainAcc 0.6659	ValidAcc 0.4081	TestAcc 0.4159	BestValid 0.4657
	Epoch 3550:	Loss 0.6237	TrainAcc 0.7576	ValidAcc 0.4597	TestAcc 0.4736	BestValid 0.4657
	Epoch 3600:	Loss 0.6239	TrainAcc 0.7712	ValidAcc 0.4712	TestAcc 0.4841	BestValid 0.4712
	Epoch 3650:	Loss 0.6357	TrainAcc 0.7348	ValidAcc 0.4387	TestAcc 0.4630	BestValid 0.4712
	Epoch 3700:	Loss 0.6431	TrainAcc 0.7432	ValidAcc 0.4537	TestAcc 0.4496	BestValid 0.4712
	Epoch 3750:	Loss 0.6399	TrainAcc 0.7492	ValidAcc 0.4507	TestAcc 0.4717	BestValid 0.4712
	Epoch 3800:	Loss 0.6304	TrainAcc 0.7660	ValidAcc 0.4627	TestAcc 0.4697	BestValid 0.4712
	Epoch 3850:	Loss 0.6176	TrainAcc 0.7500	ValidAcc 0.4579	TestAcc 0.4726	BestValid 0.4712
	Epoch 3900:	Loss 0.6216	TrainAcc 0.7460	ValidAcc 0.4579	TestAcc 0.4736	BestValid 0.4712
	Epoch 3950:	Loss 0.6363	TrainAcc 0.6983	ValidAcc 0.4399	TestAcc 0.4419	BestValid 0.4712
	Epoch 4000:	Loss 0.6098	TrainAcc 0.6959	ValidAcc 0.4351	TestAcc 0.4592	BestValid 0.4712
	Epoch 4050:	Loss 0.6297	TrainAcc 0.7504	ValidAcc 0.4501	TestAcc 0.4640	BestValid 0.4712
	Epoch 4100:	Loss 0.6465	TrainAcc 0.7620	ValidAcc 0.4603	TestAcc 0.4707	BestValid 0.4712
	Epoch 4150:	Loss 0.6067	TrainAcc 0.7200	ValidAcc 0.4393	TestAcc 0.4428	BestValid 0.4712
	Epoch 4200:	Loss 0.5927	TrainAcc 0.7744	ValidAcc 0.4609	TestAcc 0.4736	BestValid 0.4712
	Epoch 4250:	Loss 0.6034	TrainAcc 0.7500	ValidAcc 0.4519	TestAcc 0.4755	BestValid 0.4712
	Epoch 4300:	Loss 0.6064	TrainAcc 0.7340	ValidAcc 0.4573	TestAcc 0.4784	BestValid 0.4712
	Epoch 4350:	Loss 0.6360	TrainAcc 0.6771	ValidAcc 0.4387	TestAcc 0.4515	BestValid 0.4712
	Epoch 4400:	Loss 0.6029	TrainAcc 0.7720	ValidAcc 0.4669	TestAcc 0.4899	BestValid 0.4712
	Epoch 4450:	Loss 0.5980	TrainAcc 0.7572	ValidAcc 0.4615	TestAcc 0.4688	BestValid 0.4712
	Epoch 4500:	Loss 0.6233	TrainAcc 0.7288	ValidAcc 0.4651	TestAcc 0.4649	BestValid 0.4712
	Epoch 4550:	Loss 0.6413	TrainAcc 0.6823	ValidAcc 0.4183	TestAcc 0.4323	BestValid 0.4712
	Epoch 4600:	Loss 0.5835	TrainAcc 0.7083	ValidAcc 0.4273	TestAcc 0.4380	BestValid 0.4712
	Epoch 4650:	Loss 0.6261	TrainAcc 0.7404	ValidAcc 0.4519	TestAcc 0.4640	BestValid 0.4712
	Epoch 4700:	Loss 0.5917	TrainAcc 0.7821	ValidAcc 0.4688	TestAcc 0.4803	BestValid 0.4712
	Epoch 4750:	Loss 0.5788	TrainAcc 0.7584	ValidAcc 0.4627	TestAcc 0.4793	BestValid 0.4712
	Epoch 4800:	Loss 0.5728	TrainAcc 0.7496	ValidAcc 0.4399	TestAcc 0.4486	BestValid 0.4712
	Epoch 4850:	Loss 0.5778	TrainAcc 0.7432	ValidAcc 0.4459	TestAcc 0.4611	BestValid 0.4712
	Epoch 4900:	Loss 0.5998	TrainAcc 0.7668	ValidAcc 0.4531	TestAcc 0.4678	BestValid 0.4712
	Epoch 4950:	Loss 0.6435	TrainAcc 0.7216	ValidAcc 0.4435	TestAcc 0.4573	BestValid 0.4712
	Epoch 5000:	Loss 0.5756	TrainAcc 0.7772	ValidAcc 0.4615	TestAcc 0.4793	BestValid 0.4712
****** Epoch Time (Excluding Evaluation Cost): 0.021 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 2.631 ms (Max: 2.808, Min: 2.414, Sum: 21.045)
Cluster-Wide Average, Compute: 6.880 ms (Max: 9.094, Min: 4.311, Sum: 55.039)
Cluster-Wide Average, Communication-Layer: 2.783 ms (Max: 3.350, Min: 2.225, Sum: 22.261)
Cluster-Wide Average, Bubble-Imbalance: 2.542 ms (Max: 4.966, Min: 1.117, Sum: 20.333)
Cluster-Wide Average, Communication-Graph: 3.164 ms (Max: 3.443, Min: 2.925, Sum: 25.314)
Cluster-Wide Average, Optimization: 1.639 ms (Max: 2.783, Min: 0.801, Sum: 13.115)
Cluster-Wide Average, Others: 1.376 ms (Max: 2.570, Min: 0.195, Sum: 11.011)
****** Breakdown Sum: 21.015 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.287 GB (Max: 1.700, Min: 1.092, Sum: 10.298)
Cluster-Wide Average, Graph-Level Communication Throughput: 59.632 Gbps (Max: 69.377, Min: 50.134, Sum: 477.058)
Cluster-Wide Average, Layer-Level Communication Throughput: 43.566 Gbps (Max: 52.992, Min: 33.752, Sum: 348.527)
Layer-level communication (cluster-wide, per-epoch): 0.116 GB
Graph-level communication (cluster-wide, per-epoch): 0.128 GB
Weight-sync communication (cluster-wide, per-epoch): 0.031 GB
Total communication (cluster-wide, per-epoch): 0.274 GB
****** Accuracy Results ******
Highest valid_acc: 0.4712
Target test_acc: 0.4841
Epoch to reach the target acc: 3599
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
