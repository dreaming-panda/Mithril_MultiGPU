Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.017 seconds.
        It takes 0.023 seconds.
        It takes 0.018 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.024 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.100 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.106 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCNII layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 11157
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.803 Gbps (per GPU), 478.423 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.515 Gbps (per GPU), 476.122 Gbps (aggregated)
The layer-level communication performance: 59.503 Gbps (per GPU), 476.026 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.247 Gbps (per GPU), 473.973 Gbps (aggregated)
The layer-level communication performance: 59.207 Gbps (per GPU), 473.657 Gbps (aggregated)
The layer-level communication performance: 58.996 Gbps (per GPU), 471.970 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.952 Gbps (per GPU), 471.614 Gbps (aggregated)
The layer-level communication performance: 58.918 Gbps (per GPU), 471.344 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.804 Gbps (per GPU), 1246.430 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.803 Gbps (per GPU), 1246.425 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.795 Gbps (per GPU), 1246.357 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.760 Gbps (per GPU), 1246.080 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.809 Gbps (per GPU), 1246.473 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.824 Gbps (per GPU), 1246.589 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.792 Gbps (per GPU), 1246.334 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.766 Gbps (per GPU), 1246.126 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.289 Gbps (per GPU), 802.315 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.292 Gbps (per GPU), 802.335 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.292 Gbps (per GPU), 802.335 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.276 Gbps (per GPU), 802.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.291 Gbps (per GPU), 802.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.234 Gbps (per GPU), 801.874 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.292 Gbps (per GPU), 802.335 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.208 Gbps (per GPU), 801.664 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.538 Gbps (per GPU), 284.306 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.537 Gbps (per GPU), 284.292 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.537 Gbps (per GPU), 284.294 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.532 Gbps (per GPU), 284.256 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.539 Gbps (per GPU), 284.309 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.530 Gbps (per GPU), 284.238 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.533 Gbps (per GPU), 284.267 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.528 Gbps (per GPU), 284.225 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.73ms  0.84ms  0.51ms  3.41 11.16K  0.12M
 chk_1  1.74ms  0.84ms  0.50ms  3.48 11.16K  0.11M
 chk_2  1.67ms  0.81ms  0.47ms  3.57 11.16K  0.11M
 chk_3  1.61ms  0.77ms  0.43ms  3.78 11.16K  0.12M
 chk_4  1.69ms  0.84ms  0.50ms  3.41 11.16K  0.11M
 chk_5  1.82ms  0.83ms  0.48ms  3.76 11.16K  0.10M
 chk_6  1.67ms  0.83ms  0.49ms  3.43 11.16K  0.12M
 chk_7  1.77ms  0.84ms  0.50ms  3.55 11.16K  0.11M
   Avg  1.71  0.83  0.48
   Max  1.82  0.84  0.51
   Min  1.61  0.77  0.43
 Ratio  1.13  1.09  1.19
   Var  0.00  0.00  0.00
Profiling takes 0.361 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 33)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 33)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 33)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 33)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 33)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 33)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 33)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 33)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 33)...
+++++++++ Node 5 initializing the weights for op[0, 33)...
+++++++++ Node 2 initializing the weights for op[0, 33)...
+++++++++ Node 6 initializing the weights for op[0, 33)...
+++++++++ Node 7 initializing the weights for op[0, 33)...
+++++++++ Node 4 initializing the weights for op[0, 33)...
+++++++++ Node 3 initializing the weights for op[0, 33)...
+++++++++ Node 0 initializing the weights for op[0, 33)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 7.3177	TrainAcc 0.3892	ValidAcc 0.3870	TestAcc 0.3883	BestValid 0.3870
	Epoch 50:	Loss 1.6124	TrainAcc 0.4216	ValidAcc 0.4240	TestAcc 0.4237	BestValid 0.4240
	Epoch 100:	Loss 1.5517	TrainAcc 0.4974	ValidAcc 0.4971	TestAcc 0.4981	BestValid 0.4971
	Epoch 150:	Loss 1.5229	TrainAcc 0.5016	ValidAcc 0.5017	TestAcc 0.5008	BestValid 0.5017
	Epoch 200:	Loss 1.5130	TrainAcc 0.5032	ValidAcc 0.5045	TestAcc 0.5028	BestValid 0.5045
	Epoch 250:	Loss 1.5030	TrainAcc 0.5048	ValidAcc 0.5049	TestAcc 0.5032	BestValid 0.5049
	Epoch 300:	Loss 1.4920	TrainAcc 0.5040	ValidAcc 0.5040	TestAcc 0.5030	BestValid 0.5049
	Epoch 350:	Loss 1.4886	TrainAcc 0.5062	ValidAcc 0.5058	TestAcc 0.5054	BestValid 0.5058
	Epoch 400:	Loss 1.4850	TrainAcc 0.5062	ValidAcc 0.5075	TestAcc 0.5053	BestValid 0.5075
	Epoch 450:	Loss 1.4797	TrainAcc 0.5065	ValidAcc 0.5067	TestAcc 0.5048	BestValid 0.5075
	Epoch 500:	Loss 1.4753	TrainAcc 0.5066	ValidAcc 0.5077	TestAcc 0.5048	BestValid 0.5077
	Epoch 550:	Loss 1.4654	TrainAcc 0.5093	ValidAcc 0.5084	TestAcc 0.5083	BestValid 0.5084
	Epoch 600:	Loss 1.4589	TrainAcc 0.5102	ValidAcc 0.5105	TestAcc 0.5097	BestValid 0.5105
	Epoch 650:	Loss 1.4537	TrainAcc 0.5113	ValidAcc 0.5103	TestAcc 0.5074	BestValid 0.5105
	Epoch 700:	Loss 1.4488	TrainAcc 0.5153	ValidAcc 0.5137	TestAcc 0.5111	BestValid 0.5137
	Epoch 750:	Loss 1.4456	TrainAcc 0.5141	ValidAcc 0.5115	TestAcc 0.5089	BestValid 0.5137
	Epoch 800:	Loss 1.4479	TrainAcc 0.5165	ValidAcc 0.5136	TestAcc 0.5121	BestValid 0.5137
	Epoch 850:	Loss 1.4388	TrainAcc 0.5174	ValidAcc 0.5144	TestAcc 0.5117	BestValid 0.5144
	Epoch 900:	Loss 1.4345	TrainAcc 0.5177	ValidAcc 0.5146	TestAcc 0.5123	BestValid 0.5146
	Epoch 950:	Loss 1.4360	TrainAcc 0.5174	ValidAcc 0.5133	TestAcc 0.5119	BestValid 0.5146
	Epoch 1000:	Loss 1.4363	TrainAcc 0.5183	ValidAcc 0.5155	TestAcc 0.5130	BestValid 0.5155
	Epoch 1050:	Loss 1.4384	TrainAcc 0.5182	ValidAcc 0.5147	TestAcc 0.5129	BestValid 0.5155
	Epoch 1100:	Loss 1.4291	TrainAcc 0.5175	ValidAcc 0.5144	TestAcc 0.5134	BestValid 0.5155
	Epoch 1150:	Loss 1.4281	TrainAcc 0.5193	ValidAcc 0.5167	TestAcc 0.5141	BestValid 0.5167
	Epoch 1200:	Loss 1.4280	TrainAcc 0.5194	ValidAcc 0.5166	TestAcc 0.5149	BestValid 0.5167
	Epoch 1250:	Loss 1.4272	TrainAcc 0.5185	ValidAcc 0.5139	TestAcc 0.5119	BestValid 0.5167
	Epoch 1300:	Loss 1.4250	TrainAcc 0.5199	ValidAcc 0.5157	TestAcc 0.5143	BestValid 0.5167
	Epoch 1350:	Loss 1.4243	TrainAcc 0.5195	ValidAcc 0.5164	TestAcc 0.5156	BestValid 0.5167
	Epoch 1400:	Loss 1.4285	TrainAcc 0.5190	ValidAcc 0.5143	TestAcc 0.5121	BestValid 0.5167
	Epoch 1450:	Loss 1.4220	TrainAcc 0.5208	ValidAcc 0.5160	TestAcc 0.5153	BestValid 0.5167
	Epoch 1500:	Loss 1.4224	TrainAcc 0.5213	ValidAcc 0.5182	TestAcc 0.5169	BestValid 0.5182
	Epoch 1550:	Loss 1.4215	TrainAcc 0.5212	ValidAcc 0.5186	TestAcc 0.5147	BestValid 0.5186
	Epoch 1600:	Loss 1.4196	TrainAcc 0.5213	ValidAcc 0.5174	TestAcc 0.5170	BestValid 0.5186
	Epoch 1650:	Loss 1.4190	TrainAcc 0.5211	ValidAcc 0.5174	TestAcc 0.5154	BestValid 0.5186
	Epoch 1700:	Loss 1.4124	TrainAcc 0.5218	ValidAcc 0.5177	TestAcc 0.5172	BestValid 0.5186
	Epoch 1750:	Loss 1.4206	TrainAcc 0.5229	ValidAcc 0.5185	TestAcc 0.5181	BestValid 0.5186
	Epoch 1800:	Loss 1.4152	TrainAcc 0.5163	ValidAcc 0.5147	TestAcc 0.5134	BestValid 0.5186
	Epoch 1850:	Loss 1.4142	TrainAcc 0.5244	ValidAcc 0.5185	TestAcc 0.5188	BestValid 0.5186
	Epoch 1900:	Loss 1.4102	TrainAcc 0.5236	ValidAcc 0.5194	TestAcc 0.5193	BestValid 0.5194
	Epoch 1950:	Loss 1.4091	TrainAcc 0.5232	ValidAcc 0.5187	TestAcc 0.5186	BestValid 0.5194
	Epoch 2000:	Loss 1.4117	TrainAcc 0.5235	ValidAcc 0.5191	TestAcc 0.5186	BestValid 0.5194
	Epoch 2050:	Loss 1.4092	TrainAcc 0.5240	ValidAcc 0.5183	TestAcc 0.5199	BestValid 0.5194
	Epoch 2100:	Loss 1.4099	TrainAcc 0.5252	ValidAcc 0.5195	TestAcc 0.5203	BestValid 0.5195
	Epoch 2150:	Loss 1.4158	TrainAcc 0.5250	ValidAcc 0.5200	TestAcc 0.5201	BestValid 0.5200
	Epoch 2200:	Loss 1.4116	TrainAcc 0.5237	ValidAcc 0.5190	TestAcc 0.5193	BestValid 0.5200
	Epoch 2250:	Loss 1.4119	TrainAcc 0.5236	ValidAcc 0.5184	TestAcc 0.5190	BestValid 0.5200
	Epoch 2300:	Loss 1.4099	TrainAcc 0.5252	ValidAcc 0.5189	TestAcc 0.5212	BestValid 0.5200
	Epoch 2350:	Loss 1.4065	TrainAcc 0.5256	ValidAcc 0.5204	TestAcc 0.5205	BestValid 0.5204
	Epoch 2400:	Loss 1.4075	TrainAcc 0.5252	ValidAcc 0.5200	TestAcc 0.5210	BestValid 0.5204
	Epoch 2450:	Loss 1.4074	TrainAcc 0.5257	ValidAcc 0.5208	TestAcc 0.5212	BestValid 0.5208
	Epoch 2500:	Loss 1.4053	TrainAcc 0.5240	ValidAcc 0.5189	TestAcc 0.5185	BestValid 0.5208
	Epoch 2550:	Loss 1.4058	TrainAcc 0.5262	ValidAcc 0.5197	TestAcc 0.5217	BestValid 0.5208
	Epoch 2600:	Loss 1.4048	TrainAcc 0.5267	ValidAcc 0.5197	TestAcc 0.5220	BestValid 0.5208
	Epoch 2650:	Loss 1.4054	TrainAcc 0.5263	ValidAcc 0.5199	TestAcc 0.5217	BestValid 0.5208
	Epoch 2700:	Loss 1.4013	TrainAcc 0.5265	ValidAcc 0.5212	TestAcc 0.5216	BestValid 0.5212
	Epoch 2750:	Loss 1.4046	TrainAcc 0.5271	ValidAcc 0.5216	TestAcc 0.5216	BestValid 0.5216
	Epoch 2800:	Loss 1.4075	TrainAcc 0.5265	ValidAcc 0.5208	TestAcc 0.5218	BestValid 0.5216
	Epoch 2850:	Loss 1.4025	TrainAcc 0.5273	ValidAcc 0.5216	TestAcc 0.5219	BestValid 0.5216
	Epoch 2900:	Loss 1.4036	TrainAcc 0.5260	ValidAcc 0.5211	TestAcc 0.5216	BestValid 0.5216
	Epoch 2950:	Loss 1.3985	TrainAcc 0.5267	ValidAcc 0.5204	TestAcc 0.5220	BestValid 0.5216
	Epoch 3000:	Loss 1.4079	TrainAcc 0.5277	ValidAcc 0.5213	TestAcc 0.5221	BestValid 0.5216
	Epoch 3050:	Loss 1.4010	TrainAcc 0.5276	ValidAcc 0.5212	TestAcc 0.5228	BestValid 0.5216
	Epoch 3100:	Loss 1.3973	TrainAcc 0.5273	ValidAcc 0.5212	TestAcc 0.5223	BestValid 0.5216
	Epoch 3150:	Loss 1.4019	TrainAcc 0.5276	ValidAcc 0.5214	TestAcc 0.5226	BestValid 0.5216
	Epoch 3200:	Loss 1.3974	TrainAcc 0.5275	ValidAcc 0.5222	TestAcc 0.5228	BestValid 0.5222
	Epoch 3250:	Loss 1.4005	TrainAcc 0.5252	ValidAcc 0.5218	TestAcc 0.5205	BestValid 0.5222
	Epoch 3300:	Loss 1.3961	TrainAcc 0.5284	ValidAcc 0.5227	TestAcc 0.5231	BestValid 0.5227
	Epoch 3350:	Loss 1.3954	TrainAcc 0.5286	ValidAcc 0.5222	TestAcc 0.5236	BestValid 0.5227
	Epoch 3400:	Loss 1.3949	TrainAcc 0.5285	ValidAcc 0.5225	TestAcc 0.5236	BestValid 0.5227
	Epoch 3450:	Loss 1.3948	TrainAcc 0.5239	ValidAcc 0.5164	TestAcc 0.5168	BestValid 0.5227
	Epoch 3500:	Loss 1.3929	TrainAcc 0.5287	ValidAcc 0.5230	TestAcc 0.5234	BestValid 0.5230
	Epoch 3550:	Loss 1.3994	TrainAcc 0.5289	ValidAcc 0.5234	TestAcc 0.5249	BestValid 0.5234
	Epoch 3600:	Loss 1.3907	TrainAcc 0.5302	ValidAcc 0.5235	TestAcc 0.5234	BestValid 0.5235
	Epoch 3650:	Loss 1.3925	TrainAcc 0.5305	ValidAcc 0.5243	TestAcc 0.5230	BestValid 0.5243
	Epoch 3700:	Loss 1.3921	TrainAcc 0.5293	ValidAcc 0.5249	TestAcc 0.5236	BestValid 0.5249
	Epoch 3750:	Loss 1.3945	TrainAcc 0.5266	ValidAcc 0.5192	TestAcc 0.5209	BestValid 0.5249
	Epoch 3800:	Loss 1.3907	TrainAcc 0.5306	ValidAcc 0.5249	TestAcc 0.5237	BestValid 0.5249
	Epoch 3850:	Loss 1.3881	TrainAcc 0.5300	ValidAcc 0.5246	TestAcc 0.5229	BestValid 0.5249
	Epoch 3900:	Loss 1.3894	TrainAcc 0.5300	ValidAcc 0.5255	TestAcc 0.5244	BestValid 0.5255
	Epoch 3950:	Loss 1.3888	TrainAcc 0.5298	ValidAcc 0.5242	TestAcc 0.5249	BestValid 0.5255
	Epoch 4000:	Loss 1.3876	TrainAcc 0.5316	ValidAcc 0.5260	TestAcc 0.5254	BestValid 0.5260
	Epoch 4050:	Loss 1.3891	TrainAcc 0.5314	ValidAcc 0.5252	TestAcc 0.5239	BestValid 0.5260
	Epoch 4100:	Loss 1.3879	TrainAcc 0.5313	ValidAcc 0.5254	TestAcc 0.5244	BestValid 0.5260
	Epoch 4150:	Loss 1.3965	TrainAcc 0.5321	ValidAcc 0.5259	TestAcc 0.5249	BestValid 0.5260
	Epoch 4200:	Loss 1.3885	TrainAcc 0.5328	ValidAcc 0.5268	TestAcc 0.5256	BestValid 0.5268
	Epoch 4250:	Loss 1.3849	TrainAcc 0.5334	ValidAcc 0.5265	TestAcc 0.5263	BestValid 0.5268
	Epoch 4300:	Loss 1.3852	TrainAcc 0.5319	ValidAcc 0.5274	TestAcc 0.5248	BestValid 0.5274
	Epoch 4350:	Loss 1.3844	TrainAcc 0.5333	ValidAcc 0.5268	TestAcc 0.5255	BestValid 0.5274
	Epoch 4400:	Loss 1.3824	TrainAcc 0.5333	ValidAcc 0.5274	TestAcc 0.5260	BestValid 0.5274
	Epoch 4450:	Loss 1.3859	TrainAcc 0.5327	ValidAcc 0.5282	TestAcc 0.5257	BestValid 0.5282
	Epoch 4500:	Loss 1.3796	TrainAcc 0.5360	ValidAcc 0.5285	TestAcc 0.5281	BestValid 0.5285
	Epoch 4550:	Loss 1.3778	TrainAcc 0.5347	ValidAcc 0.5280	TestAcc 0.5275	BestValid 0.5285
	Epoch 4600:	Loss 1.3881	TrainAcc 0.5327	ValidAcc 0.5258	TestAcc 0.5231	BestValid 0.5285
	Epoch 4650:	Loss 1.3793	TrainAcc 0.5343	ValidAcc 0.5266	TestAcc 0.5276	BestValid 0.5285
	Epoch 4700:	Loss 1.3801	TrainAcc 0.5365	ValidAcc 0.5294	TestAcc 0.5288	BestValid 0.5294
	Epoch 4750:	Loss 1.3856	TrainAcc 0.5348	ValidAcc 0.5260	TestAcc 0.5266	BestValid 0.5294
	Epoch 4800:	Loss 1.3742	TrainAcc 0.5345	ValidAcc 0.5265	TestAcc 0.5259	BestValid 0.5294
	Epoch 4850:	Loss 1.3794	TrainAcc 0.5358	ValidAcc 0.5285	TestAcc 0.5285	BestValid 0.5294
	Epoch 4900:	Loss 1.3757	TrainAcc 0.5375	ValidAcc 0.5294	TestAcc 0.5295	BestValid 0.5294
	Epoch 4950:	Loss 1.3770	TrainAcc 0.5381	ValidAcc 0.5287	TestAcc 0.5294	BestValid 0.5294
	Epoch 5000:	Loss 1.3771	TrainAcc 0.5369	ValidAcc 0.5284	TestAcc 0.5290	BestValid 0.5294
****** Epoch Time (Excluding Evaluation Cost): 0.022 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.049 ms (Max: 0.060, Min: 0.033, Sum: 0.389)
Cluster-Wide Average, Compute: 5.395 ms (Max: 5.565, Min: 5.227, Sum: 43.164)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.122)
Cluster-Wide Average, Communication-Graph: 15.348 ms (Max: 15.528, Min: 15.168, Sum: 122.785)
Cluster-Wide Average, Optimization: 0.767 ms (Max: 0.773, Min: 0.760, Sum: 6.139)
Cluster-Wide Average, Others: 0.810 ms (Max: 0.814, Min: 0.803, Sum: 6.483)
****** Breakdown Sum: 22.393 ms ******
Cluster-Wide Average, GPU Memory Consumption: 2.013 GB (Max: 2.194, Min: 1.973, Sum: 16.101)
Cluster-Wide Average, Graph-Level Communication Throughput: 43.039 Gbps (Max: 52.352, Min: 16.914, Sum: 344.313)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.573 GB
Weight-sync communication (cluster-wide, per-epoch): 0.007 GB
Total communication (cluster-wide, per-epoch): 0.580 GB
****** Accuracy Results ******
Highest valid_acc: 0.5294
Target test_acc: 0.5288
Epoch to reach the target acc: 4699
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
