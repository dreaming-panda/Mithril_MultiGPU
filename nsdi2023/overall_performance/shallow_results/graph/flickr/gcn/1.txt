Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.031 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.025 seconds.
        It takes 0.028 seconds.
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCN layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.012 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.306 Gbps (per GPU), 482.450 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.993 Gbps (per GPU), 479.944 Gbps (aggregated)
The layer-level communication performance: 59.983 Gbps (per GPU), 479.865 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.752 Gbps (per GPU), 478.017 Gbps (aggregated)
The layer-level communication performance: 59.721 Gbps (per GPU), 477.768 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.495 Gbps (per GPU), 475.960 Gbps (aggregated)
The layer-level communication performance: 59.449 Gbps (per GPU), 475.595 Gbps (aggregated)
The layer-level communication performance: 59.416 Gbps (per GPU), 475.325 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.955 Gbps (per GPU), 1255.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.958 Gbps (per GPU), 1255.661 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.952 Gbps (per GPU), 1255.615 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.925 Gbps (per GPU), 1255.404 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.952 Gbps (per GPU), 1255.619 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.961 Gbps (per GPU), 1255.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.952 Gbps (per GPU), 1255.614 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.920 Gbps (per GPU), 1255.357 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.034 Gbps (per GPU), 816.271 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.039 Gbps (per GPU), 816.310 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.018 Gbps (per GPU), 816.145 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.033 Gbps (per GPU), 816.264 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.995 Gbps (per GPU), 815.960 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.042 Gbps (per GPU), 816.337 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.978 Gbps (per GPU), 815.820 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.010 Gbps (per GPU), 816.079 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.611 Gbps (per GPU), 244.884 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.608 Gbps (per GPU), 244.867 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.609 Gbps (per GPU), 244.872 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.609 Gbps (per GPU), 244.872 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.609 Gbps (per GPU), 244.874 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.610 Gbps (per GPU), 244.877 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.609 Gbps (per GPU), 244.873 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.607 Gbps (per GPU), 244.859 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.77ms  0.56ms  0.36ms  2.12 11.16K  0.12M
 chk_1  0.78ms  0.57ms  0.37ms  2.09 11.16K  0.11M
 chk_2  0.74ms  0.53ms  0.34ms  2.20 11.16K  0.11M
 chk_3  0.69ms  0.50ms  0.30ms  2.29 11.16K  0.12M
 chk_4  0.75ms  0.56ms  0.37ms  2.03 11.16K  0.11M
 chk_5  0.73ms  0.55ms  0.36ms  2.06 11.16K  0.10M
 chk_6  0.74ms  0.56ms  0.36ms  2.04 11.16K  0.12M
 chk_7  0.74ms  0.56ms  0.37ms  2.01 11.16K  0.11M
   Avg  0.74  0.55  0.35
   Max  0.78  0.57  0.37
   Min  0.69  0.50  0.30
 Ratio  1.13  1.14  1.24
   Var  0.00  0.00  0.00
Profiling takes 0.220 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 20)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 20)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 20)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 20)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 20)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 20)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 20)...
+++++++++ Node 6 initializing the weights for op[0, 20)...
+++++++++ Node 3 initializing the weights for op[0, 20)...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 2 initializing the weights for op[0, 20)...
+++++++++ Node 7 initializing the weights for op[0, 20)...
+++++++++ Node 4 initializing the weights for op[0, 20)...
+++++++++ Node 5 initializing the weights for op[0, 20)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2301	TrainAcc 0.4211	ValidAcc 0.4230	TestAcc 0.4227	BestValid 0.4230
	Epoch 50:	Loss 1.5319	TrainAcc 0.4963	ValidAcc 0.4987	TestAcc 0.4945	BestValid 0.4987
	Epoch 100:	Loss 1.5111	TrainAcc 0.4901	ValidAcc 0.4926	TestAcc 0.4894	BestValid 0.4987
	Epoch 150:	Loss 1.4714	TrainAcc 0.4898	ValidAcc 0.4899	TestAcc 0.4855	BestValid 0.4987
	Epoch 200:	Loss 1.4532	TrainAcc 0.4992	ValidAcc 0.5009	TestAcc 0.4955	BestValid 0.5009
	Epoch 250:	Loss 1.4488	TrainAcc 0.4962	ValidAcc 0.4968	TestAcc 0.4930	BestValid 0.5009
	Epoch 300:	Loss 1.4431	TrainAcc 0.4923	ValidAcc 0.4915	TestAcc 0.4887	BestValid 0.5009
	Epoch 350:	Loss 1.4414	TrainAcc 0.4954	ValidAcc 0.4946	TestAcc 0.4916	BestValid 0.5009
	Epoch 400:	Loss 1.4343	TrainAcc 0.5039	ValidAcc 0.5038	TestAcc 0.4987	BestValid 0.5038
	Epoch 450:	Loss 1.4293	TrainAcc 0.5032	ValidAcc 0.5024	TestAcc 0.4985	BestValid 0.5038
	Epoch 500:	Loss 1.4244	TrainAcc 0.5012	ValidAcc 0.4994	TestAcc 0.4980	BestValid 0.5038
	Epoch 550:	Loss 1.4237	TrainAcc 0.4997	ValidAcc 0.4966	TestAcc 0.4965	BestValid 0.5038
	Epoch 600:	Loss 1.4204	TrainAcc 0.4962	ValidAcc 0.4914	TestAcc 0.4920	BestValid 0.5038
	Epoch 650:	Loss 1.4163	TrainAcc 0.5014	ValidAcc 0.4955	TestAcc 0.4960	BestValid 0.5038
	Epoch 700:	Loss 1.4129	TrainAcc 0.4984	ValidAcc 0.4911	TestAcc 0.4948	BestValid 0.5038
	Epoch 750:	Loss 1.4131	TrainAcc 0.5010	ValidAcc 0.4960	TestAcc 0.4953	BestValid 0.5038
	Epoch 800:	Loss 1.4096	TrainAcc 0.5024	ValidAcc 0.4957	TestAcc 0.4985	BestValid 0.5038
	Epoch 850:	Loss 1.4076	TrainAcc 0.5104	ValidAcc 0.5026	TestAcc 0.5042	BestValid 0.5038
	Epoch 900:	Loss 1.4041	TrainAcc 0.5095	ValidAcc 0.5027	TestAcc 0.5022	BestValid 0.5038
	Epoch 950:	Loss 1.4027	TrainAcc 0.5063	ValidAcc 0.4993	TestAcc 0.5006	BestValid 0.5038
	Epoch 1000:	Loss 1.4075	TrainAcc 0.5114	ValidAcc 0.5022	TestAcc 0.5028	BestValid 0.5038
	Epoch 1050:	Loss 1.3955	TrainAcc 0.5027	ValidAcc 0.4938	TestAcc 0.4954	BestValid 0.5038
	Epoch 1100:	Loss 1.3972	TrainAcc 0.5128	ValidAcc 0.5050	TestAcc 0.5038	BestValid 0.5050
	Epoch 1150:	Loss 1.3955	TrainAcc 0.5078	ValidAcc 0.4996	TestAcc 0.4998	BestValid 0.5050
	Epoch 1200:	Loss 1.3961	TrainAcc 0.5070	ValidAcc 0.4992	TestAcc 0.4998	BestValid 0.5050
	Epoch 1250:	Loss 1.3916	TrainAcc 0.5054	ValidAcc 0.4955	TestAcc 0.4976	BestValid 0.5050
	Epoch 1300:	Loss 1.3910	TrainAcc 0.4950	ValidAcc 0.4862	TestAcc 0.4862	BestValid 0.5050
	Epoch 1350:	Loss 1.3886	TrainAcc 0.5178	ValidAcc 0.5089	TestAcc 0.5081	BestValid 0.5089
	Epoch 1400:	Loss 1.3892	TrainAcc 0.5175	ValidAcc 0.5065	TestAcc 0.5080	BestValid 0.5089
	Epoch 1450:	Loss 1.3888	TrainAcc 0.5026	ValidAcc 0.4920	TestAcc 0.4933	BestValid 0.5089
	Epoch 1500:	Loss 1.3831	TrainAcc 0.5173	ValidAcc 0.5059	TestAcc 0.5056	BestValid 0.5089
	Epoch 1550:	Loss 1.3870	TrainAcc 0.5163	ValidAcc 0.5033	TestAcc 0.5037	BestValid 0.5089
	Epoch 1600:	Loss 1.3850	TrainAcc 0.5207	ValidAcc 0.5079	TestAcc 0.5089	BestValid 0.5089
	Epoch 1650:	Loss 1.3818	TrainAcc 0.5212	ValidAcc 0.5087	TestAcc 0.5100	BestValid 0.5089
	Epoch 1700:	Loss 1.3838	TrainAcc 0.5197	ValidAcc 0.5070	TestAcc 0.5081	BestValid 0.5089
	Epoch 1750:	Loss 1.3816	TrainAcc 0.5194	ValidAcc 0.5043	TestAcc 0.5063	BestValid 0.5089
	Epoch 1800:	Loss 1.3777	TrainAcc 0.5209	ValidAcc 0.5064	TestAcc 0.5078	BestValid 0.5089
	Epoch 1850:	Loss 1.3748	TrainAcc 0.5235	ValidAcc 0.5080	TestAcc 0.5103	BestValid 0.5089
	Epoch 1900:	Loss 1.3768	TrainAcc 0.5179	ValidAcc 0.5030	TestAcc 0.5057	BestValid 0.5089
	Epoch 1950:	Loss 1.3760	TrainAcc 0.5202	ValidAcc 0.5038	TestAcc 0.5067	BestValid 0.5089
	Epoch 2000:	Loss 1.3750	TrainAcc 0.5079	ValidAcc 0.4920	TestAcc 0.4936	BestValid 0.5089
	Epoch 2050:	Loss 1.3712	TrainAcc 0.5188	ValidAcc 0.5027	TestAcc 0.5048	BestValid 0.5089
	Epoch 2100:	Loss 1.3721	TrainAcc 0.5147	ValidAcc 0.4979	TestAcc 0.4988	BestValid 0.5089
	Epoch 2150:	Loss 1.3732	TrainAcc 0.5065	ValidAcc 0.4900	TestAcc 0.4927	BestValid 0.5089
	Epoch 2200:	Loss 1.3729	TrainAcc 0.5245	ValidAcc 0.5076	TestAcc 0.5110	BestValid 0.5089
	Epoch 2250:	Loss 1.3711	TrainAcc 0.5131	ValidAcc 0.4959	TestAcc 0.4977	BestValid 0.5089
	Epoch 2300:	Loss 1.3685	TrainAcc 0.5143	ValidAcc 0.4968	TestAcc 0.4985	BestValid 0.5089
	Epoch 2350:	Loss 1.3655	TrainAcc 0.5185	ValidAcc 0.4999	TestAcc 0.5018	BestValid 0.5089
	Epoch 2400:	Loss 1.3650	TrainAcc 0.5177	ValidAcc 0.4985	TestAcc 0.5017	BestValid 0.5089
	Epoch 2450:	Loss 1.3646	TrainAcc 0.5189	ValidAcc 0.4991	TestAcc 0.5019	BestValid 0.5089
	Epoch 2500:	Loss 1.3623	TrainAcc 0.5297	ValidAcc 0.5131	TestAcc 0.5141	BestValid 0.5131
	Epoch 2550:	Loss 1.3642	TrainAcc 0.5248	ValidAcc 0.5052	TestAcc 0.5082	BestValid 0.5131
	Epoch 2600:	Loss 1.3630	TrainAcc 0.5181	ValidAcc 0.4972	TestAcc 0.4995	BestValid 0.5131
	Epoch 2650:	Loss 1.3611	TrainAcc 0.5168	ValidAcc 0.4960	TestAcc 0.4980	BestValid 0.5131
	Epoch 2700:	Loss 1.3589	TrainAcc 0.5149	ValidAcc 0.4939	TestAcc 0.4952	BestValid 0.5131
	Epoch 2750:	Loss 1.3621	TrainAcc 0.5080	ValidAcc 0.4877	TestAcc 0.4887	BestValid 0.5131
	Epoch 2800:	Loss 1.3656	TrainAcc 0.5237	ValidAcc 0.5034	TestAcc 0.5049	BestValid 0.5131
	Epoch 2850:	Loss 1.3574	TrainAcc 0.5196	ValidAcc 0.4990	TestAcc 0.5003	BestValid 0.5131
	Epoch 2900:	Loss 1.3558	TrainAcc 0.5177	ValidAcc 0.4961	TestAcc 0.4970	BestValid 0.5131
	Epoch 2950:	Loss 1.3560	TrainAcc 0.5252	ValidAcc 0.5012	TestAcc 0.5044	BestValid 0.5131
	Epoch 3000:	Loss 1.3529	TrainAcc 0.5323	ValidAcc 0.5094	TestAcc 0.5119	BestValid 0.5131
	Epoch 3050:	Loss 1.3561	TrainAcc 0.5153	ValidAcc 0.4913	TestAcc 0.4927	BestValid 0.5131
	Epoch 3100:	Loss 1.3527	TrainAcc 0.5280	ValidAcc 0.5048	TestAcc 0.5064	BestValid 0.5131
	Epoch 3150:	Loss 1.3533	TrainAcc 0.5272	ValidAcc 0.5027	TestAcc 0.5044	BestValid 0.5131
	Epoch 3200:	Loss 1.3504	TrainAcc 0.5119	ValidAcc 0.4885	TestAcc 0.4907	BestValid 0.5131
	Epoch 3250:	Loss 1.3467	TrainAcc 0.5218	ValidAcc 0.4962	TestAcc 0.5002	BestValid 0.5131
	Epoch 3300:	Loss 1.3479	TrainAcc 0.5341	ValidAcc 0.5091	TestAcc 0.5101	BestValid 0.5131
	Epoch 3350:	Loss 1.3489	TrainAcc 0.5351	ValidAcc 0.5091	TestAcc 0.5123	BestValid 0.5131
	Epoch 3400:	Loss 1.3452	TrainAcc 0.5330	ValidAcc 0.5056	TestAcc 0.5079	BestValid 0.5131
	Epoch 3450:	Loss 1.3456	TrainAcc 0.5272	ValidAcc 0.4996	TestAcc 0.5012	BestValid 0.5131
	Epoch 3500:	Loss 1.3450	TrainAcc 0.5282	ValidAcc 0.5020	TestAcc 0.5041	BestValid 0.5131
	Epoch 3550:	Loss 1.3455	TrainAcc 0.5154	ValidAcc 0.4896	TestAcc 0.4918	BestValid 0.5131
	Epoch 3600:	Loss 1.3417	TrainAcc 0.5276	ValidAcc 0.4995	TestAcc 0.5016	BestValid 0.5131
	Epoch 3650:	Loss 1.3432	TrainAcc 0.5199	ValidAcc 0.4923	TestAcc 0.4940	BestValid 0.5131
	Epoch 3700:	Loss 1.3450	TrainAcc 0.5201	ValidAcc 0.4931	TestAcc 0.4946	BestValid 0.5131
	Epoch 3750:	Loss 1.3401	TrainAcc 0.5237	ValidAcc 0.4974	TestAcc 0.4978	BestValid 0.5131
	Epoch 3800:	Loss 1.3384	TrainAcc 0.5192	ValidAcc 0.4925	TestAcc 0.4931	BestValid 0.5131
	Epoch 3850:	Loss 1.3373	TrainAcc 0.5263	ValidAcc 0.4976	TestAcc 0.4980	BestValid 0.5131
	Epoch 3900:	Loss 1.3412	TrainAcc 0.5343	ValidAcc 0.5039	TestAcc 0.5060	BestValid 0.5131
	Epoch 3950:	Loss 1.3403	TrainAcc 0.5207	ValidAcc 0.4927	TestAcc 0.4934	BestValid 0.5131
	Epoch 4000:	Loss 1.3346	TrainAcc 0.5297	ValidAcc 0.4996	TestAcc 0.5021	BestValid 0.5131
	Epoch 4050:	Loss 1.3342	TrainAcc 0.5271	ValidAcc 0.4958	TestAcc 0.4979	BestValid 0.5131
	Epoch 4100:	Loss 1.3414	TrainAcc 0.5406	ValidAcc 0.5083	TestAcc 0.5111	BestValid 0.5131
	Epoch 4150:	Loss 1.3314	TrainAcc 0.5194	ValidAcc 0.4920	TestAcc 0.4908	BestValid 0.5131
	Epoch 4200:	Loss 1.3323	TrainAcc 0.5337	ValidAcc 0.5006	TestAcc 0.5025	BestValid 0.5131
	Epoch 4250:	Loss 1.3304	TrainAcc 0.5340	ValidAcc 0.5022	TestAcc 0.5041	BestValid 0.5131
	Epoch 4300:	Loss 1.3318	TrainAcc 0.5304	ValidAcc 0.4996	TestAcc 0.4996	BestValid 0.5131
	Epoch 4350:	Loss 1.3302	TrainAcc 0.5264	ValidAcc 0.4950	TestAcc 0.4942	BestValid 0.5131
	Epoch 4400:	Loss 1.3296	TrainAcc 0.5354	ValidAcc 0.5005	TestAcc 0.5032	BestValid 0.5131
	Epoch 4450:	Loss 1.3254	TrainAcc 0.5203	ValidAcc 0.4894	TestAcc 0.4883	BestValid 0.5131
	Epoch 4500:	Loss 1.3269	TrainAcc 0.5359	ValidAcc 0.5003	TestAcc 0.5006	BestValid 0.5131
	Epoch 4550:	Loss 1.3237	TrainAcc 0.5300	ValidAcc 0.4991	TestAcc 0.4962	BestValid 0.5131
	Epoch 4600:	Loss 1.3277	TrainAcc 0.5296	ValidAcc 0.4972	TestAcc 0.4982	BestValid 0.5131
	Epoch 4650:	Loss 1.3262	TrainAcc 0.5269	ValidAcc 0.4929	TestAcc 0.4908	BestValid 0.5131
	Epoch 4700:	Loss 1.3244	TrainAcc 0.5106	ValidAcc 0.4750	TestAcc 0.4755	BestValid 0.5131
	Epoch 4750:	Loss 1.3228	TrainAcc 0.5341	ValidAcc 0.4993	TestAcc 0.5000	BestValid 0.5131
	Epoch 4800:	Loss 1.3187	TrainAcc 0.5409	ValidAcc 0.5026	TestAcc 0.5042	BestValid 0.5131
	Epoch 4850:	Loss 1.3254	TrainAcc 0.5354	ValidAcc 0.4980	TestAcc 0.4984	BestValid 0.5131
	Epoch 4900:	Loss 1.3210	TrainAcc 0.5255	ValidAcc 0.4916	TestAcc 0.4886	BestValid 0.5131
	Epoch 4950:	Loss 1.3178	TrainAcc 0.5236	ValidAcc 0.4873	TestAcc 0.4862	BestValid 0.5131
	Epoch 5000:	Loss 1.3192	TrainAcc 0.5093	ValidAcc 0.4737	TestAcc 0.4735	BestValid 0.5131
****** Epoch Time (Excluding Evaluation Cost): 0.020 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.055 ms (Max: 0.066, Min: 0.033, Sum: 0.438)
Cluster-Wide Average, Compute: 3.176 ms (Max: 3.324, Min: 3.044, Sum: 25.406)
Cluster-Wide Average, Communication-Layer: 0.007 ms (Max: 0.008, Min: 0.007, Sum: 0.059)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.119)
Cluster-Wide Average, Communication-Graph: 15.282 ms (Max: 15.411, Min: 15.122, Sum: 122.259)
Cluster-Wide Average, Optimization: 0.376 ms (Max: 0.380, Min: 0.372, Sum: 3.009)
Cluster-Wide Average, Others: 0.783 ms (Max: 0.788, Min: 0.780, Sum: 6.268)
****** Breakdown Sum: 19.695 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.864 GB (Max: 1.979, Min: 1.833, Sum: 14.913)
Cluster-Wide Average, Graph-Level Communication Throughput: 43.207 Gbps (Max: 52.552, Min: 17.143, Sum: 345.658)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.573 GB
Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
Total communication (cluster-wide, per-epoch): 0.577 GB
****** Accuracy Results ******
Highest valid_acc: 0.5131
Target test_acc: 0.5141
Epoch to reach the target acc: 2499
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
