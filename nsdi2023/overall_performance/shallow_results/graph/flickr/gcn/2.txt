Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
        It takes 0.026 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.030 seconds.
        It takes 0.024 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.101 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
        It takes 0.006 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.108 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/flickr/8_parts
The number of GCN layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.009 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 11156) 1-[11156, 22313) 2-[22313, 33469) 3-[33469, 44625) 4-[44625, 55781) 5-[55781, 66937) 6-[66937, 78093) 7-[78093, 89250)
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.262 Gbps (per GPU), 474.094 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.962 Gbps (per GPU), 471.694 Gbps (aggregated)
The layer-level communication performance: 58.948 Gbps (per GPU), 471.585 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.717 Gbps (per GPU), 469.732 Gbps (aggregated)
The layer-level communication performance: 58.685 Gbps (per GPU), 469.477 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.462 Gbps (per GPU), 467.699 Gbps (aggregated)
The layer-level communication performance: 58.420 Gbps (per GPU), 467.358 Gbps (aggregated)
The layer-level communication performance: 58.382 Gbps (per GPU), 467.057 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 151.745 Gbps (per GPU), 1213.959 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.750 Gbps (per GPU), 1214.003 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.742 Gbps (per GPU), 1213.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.764 Gbps (per GPU), 1214.113 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.742 Gbps (per GPU), 1213.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.762 Gbps (per GPU), 1214.096 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.742 Gbps (per GPU), 1213.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 151.602 Gbps (per GPU), 1212.817 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.545 Gbps (per GPU), 812.358 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.545 Gbps (per GPU), 812.357 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.544 Gbps (per GPU), 812.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.545 Gbps (per GPU), 812.358 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.544 Gbps (per GPU), 812.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.536 Gbps (per GPU), 812.286 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.545 Gbps (per GPU), 812.359 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.524 Gbps (per GPU), 812.194 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.151 Gbps (per GPU), 257.205 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.152 Gbps (per GPU), 257.217 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.151 Gbps (per GPU), 257.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.151 Gbps (per GPU), 257.209 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.146 Gbps (per GPU), 257.170 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.150 Gbps (per GPU), 257.204 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.146 Gbps (per GPU), 257.169 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.148 Gbps (per GPU), 257.181 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.76ms  0.55ms  0.36ms  2.13 11.16K  0.12M
 chk_1  0.77ms  0.56ms  0.36ms  2.12 11.16K  0.11M
 chk_2  0.74ms  0.52ms  0.33ms  2.24 11.16K  0.11M
 chk_3  0.70ms  0.49ms  0.29ms  2.40 11.16K  0.12M
 chk_4  0.74ms  0.55ms  0.36ms  2.04 11.16K  0.11M
 chk_5  0.73ms  0.54ms  0.35ms  2.08 11.16K  0.10M
 chk_6  0.73ms  0.55ms  0.35ms  2.06 11.16K  0.12M
 chk_7  0.74ms  0.56ms  0.36ms  2.05 11.16K  0.11M
   Avg  0.74  0.54  0.35
   Max  0.77  0.56  0.36
   Min  0.70  0.49  0.29
 Ratio  1.09  1.15  1.24
   Var  0.00  0.00  0.00
Profiling takes 0.218 s
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 20)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44625, Num Local Vertices: 11156
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 20)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11156
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 20)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55781, Num Local Vertices: 11156
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 20)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11156, Num Local Vertices: 11157
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66937, Num Local Vertices: 11156
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 20)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22313, Num Local Vertices: 11156
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78093, Num Local Vertices: 11157
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 20)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33469, Num Local Vertices: 11156
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 20)...
+++++++++ Node 1 initializing the weights for op[0, 20)...
+++++++++ Node 6 initializing the weights for op[0, 20)...
+++++++++ Node 3 initializing the weights for op[0, 20)...
+++++++++ Node 7 initializing the weights for op[0, 20)...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 4 initializing the weights for op[0, 20)...
+++++++++ Node 2 initializing the weights for op[0, 20)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 192232
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 6, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 2.2387	TrainAcc 0.2718	ValidAcc 0.2721	TestAcc 0.2697	BestValid 0.2721
	Epoch 50:	Loss 1.5394	TrainAcc 0.4934	ValidAcc 0.4966	TestAcc 0.4906	BestValid 0.4966
	Epoch 100:	Loss 1.4932	TrainAcc 0.4917	ValidAcc 0.4933	TestAcc 0.4881	BestValid 0.4966
	Epoch 150:	Loss 1.4611	TrainAcc 0.4948	ValidAcc 0.4956	TestAcc 0.4904	BestValid 0.4966
	Epoch 200:	Loss 1.4539	TrainAcc 0.4902	ValidAcc 0.4915	TestAcc 0.4879	BestValid 0.4966
	Epoch 250:	Loss 1.4475	TrainAcc 0.4991	ValidAcc 0.5006	TestAcc 0.4954	BestValid 0.5006
	Epoch 300:	Loss 1.4461	TrainAcc 0.4962	ValidAcc 0.4953	TestAcc 0.4920	BestValid 0.5006
	Epoch 350:	Loss 1.4383	TrainAcc 0.4936	ValidAcc 0.4928	TestAcc 0.4926	BestValid 0.5006
	Epoch 400:	Loss 1.4324	TrainAcc 0.5050	ValidAcc 0.5041	TestAcc 0.5002	BestValid 0.5041
	Epoch 450:	Loss 1.4267	TrainAcc 0.5020	ValidAcc 0.4995	TestAcc 0.4973	BestValid 0.5041
	Epoch 500:	Loss 1.4251	TrainAcc 0.4924	ValidAcc 0.4896	TestAcc 0.4898	BestValid 0.5041
	Epoch 550:	Loss 1.4193	TrainAcc 0.5008	ValidAcc 0.4974	TestAcc 0.4961	BestValid 0.5041
	Epoch 600:	Loss 1.4185	TrainAcc 0.5040	ValidAcc 0.5012	TestAcc 0.4985	BestValid 0.5041
	Epoch 650:	Loss 1.4159	TrainAcc 0.5076	ValidAcc 0.5043	TestAcc 0.5006	BestValid 0.5043
	Epoch 700:	Loss 1.4138	TrainAcc 0.5094	ValidAcc 0.5037	TestAcc 0.5020	BestValid 0.5043
	Epoch 750:	Loss 1.4088	TrainAcc 0.5076	ValidAcc 0.5017	TestAcc 0.4990	BestValid 0.5043
	Epoch 800:	Loss 1.4069	TrainAcc 0.5029	ValidAcc 0.4983	TestAcc 0.4962	BestValid 0.5043
	Epoch 850:	Loss 1.4019	TrainAcc 0.5065	ValidAcc 0.5020	TestAcc 0.4998	BestValid 0.5043
	Epoch 900:	Loss 1.4014	TrainAcc 0.5069	ValidAcc 0.5003	TestAcc 0.4983	BestValid 0.5043
	Epoch 950:	Loss 1.4022	TrainAcc 0.5075	ValidAcc 0.4984	TestAcc 0.4983	BestValid 0.5043
	Epoch 1000:	Loss 1.3981	TrainAcc 0.5145	ValidAcc 0.5085	TestAcc 0.5045	BestValid 0.5085
	Epoch 1050:	Loss 1.3959	TrainAcc 0.5082	ValidAcc 0.4983	TestAcc 0.4996	BestValid 0.5085
	Epoch 1100:	Loss 1.3930	TrainAcc 0.5077	ValidAcc 0.4984	TestAcc 0.5002	BestValid 0.5085
	Epoch 1150:	Loss 1.3945	TrainAcc 0.5097	ValidAcc 0.5011	TestAcc 0.5026	BestValid 0.5085
	Epoch 1200:	Loss 1.3940	TrainAcc 0.5112	ValidAcc 0.5017	TestAcc 0.5034	BestValid 0.5085
	Epoch 1250:	Loss 1.3915	TrainAcc 0.5149	ValidAcc 0.5059	TestAcc 0.5071	BestValid 0.5085
	Epoch 1300:	Loss 1.3890	TrainAcc 0.5191	ValidAcc 0.5090	TestAcc 0.5090	BestValid 0.5090
	Epoch 1350:	Loss 1.3881	TrainAcc 0.5103	ValidAcc 0.4996	TestAcc 0.5016	BestValid 0.5090
	Epoch 1400:	Loss 1.3859	TrainAcc 0.5118	ValidAcc 0.5005	TestAcc 0.5022	BestValid 0.5090
	Epoch 1450:	Loss 1.3843	TrainAcc 0.5068	ValidAcc 0.4944	TestAcc 0.4975	BestValid 0.5090
	Epoch 1500:	Loss 1.3828	TrainAcc 0.5089	ValidAcc 0.4968	TestAcc 0.4999	BestValid 0.5090
	Epoch 1550:	Loss 1.3797	TrainAcc 0.5005	ValidAcc 0.4853	TestAcc 0.4880	BestValid 0.5090
	Epoch 1600:	Loss 1.3794	TrainAcc 0.5122	ValidAcc 0.4985	TestAcc 0.5020	BestValid 0.5090
	Epoch 1650:	Loss 1.3794	TrainAcc 0.5140	ValidAcc 0.4996	TestAcc 0.5024	BestValid 0.5090
	Epoch 1700:	Loss 1.3782	TrainAcc 0.5174	ValidAcc 0.5034	TestAcc 0.5050	BestValid 0.5090
	Epoch 1750:	Loss 1.3755	TrainAcc 0.5091	ValidAcc 0.4936	TestAcc 0.4964	BestValid 0.5090
	Epoch 1800:	Loss 1.3749	TrainAcc 0.5160	ValidAcc 0.5011	TestAcc 0.5032	BestValid 0.5090
	Epoch 1850:	Loss 1.3736	TrainAcc 0.5170	ValidAcc 0.5019	TestAcc 0.5040	BestValid 0.5090
	Epoch 1900:	Loss 1.3765	TrainAcc 0.5159	ValidAcc 0.5003	TestAcc 0.5024	BestValid 0.5090
	Epoch 1950:	Loss 1.3673	TrainAcc 0.5195	ValidAcc 0.5034	TestAcc 0.5050	BestValid 0.5090
	Epoch 2000:	Loss 1.3717	TrainAcc 0.5199	ValidAcc 0.5040	TestAcc 0.5072	BestValid 0.5090
	Epoch 2050:	Loss 1.3706	TrainAcc 0.5193	ValidAcc 0.5025	TestAcc 0.5047	BestValid 0.5090
	Epoch 2100:	Loss 1.3694	TrainAcc 0.5135	ValidAcc 0.4975	TestAcc 0.4979	BestValid 0.5090
	Epoch 2150:	Loss 1.3648	TrainAcc 0.5147	ValidAcc 0.4976	TestAcc 0.4996	BestValid 0.5090
	Epoch 2200:	Loss 1.3634	TrainAcc 0.5196	ValidAcc 0.5010	TestAcc 0.5041	BestValid 0.5090
	Epoch 2250:	Loss 1.3673	TrainAcc 0.5226	ValidAcc 0.5031	TestAcc 0.5054	BestValid 0.5090
	Epoch 2300:	Loss 1.3661	TrainAcc 0.5208	ValidAcc 0.5020	TestAcc 0.5041	BestValid 0.5090
	Epoch 2350:	Loss 1.3667	TrainAcc 0.5261	ValidAcc 0.5069	TestAcc 0.5085	BestValid 0.5090
	Epoch 2400:	Loss 1.3610	TrainAcc 0.5236	ValidAcc 0.5039	TestAcc 0.5042	BestValid 0.5090
	Epoch 2450:	Loss 1.3589	TrainAcc 0.5215	ValidAcc 0.5016	TestAcc 0.5032	BestValid 0.5090
	Epoch 2500:	Loss 1.3584	TrainAcc 0.5296	ValidAcc 0.5091	TestAcc 0.5105	BestValid 0.5091
	Epoch 2550:	Loss 1.3596	TrainAcc 0.5132	ValidAcc 0.4935	TestAcc 0.4957	BestValid 0.5091
	Epoch 2600:	Loss 1.3573	TrainAcc 0.5194	ValidAcc 0.4977	TestAcc 0.4998	BestValid 0.5091
	Epoch 2650:	Loss 1.3562	TrainAcc 0.5231	ValidAcc 0.5013	TestAcc 0.5023	BestValid 0.5091
	Epoch 2700:	Loss 1.3548	TrainAcc 0.5333	ValidAcc 0.5099	TestAcc 0.5121	BestValid 0.5099
	Epoch 2750:	Loss 1.3531	TrainAcc 0.5154	ValidAcc 0.4917	TestAcc 0.4947	BestValid 0.5099
	Epoch 2800:	Loss 1.3518	TrainAcc 0.5208	ValidAcc 0.4983	TestAcc 0.4986	BestValid 0.5099
	Epoch 2850:	Loss 1.3512	TrainAcc 0.5096	ValidAcc 0.4880	TestAcc 0.4881	BestValid 0.5099
	Epoch 2900:	Loss 1.3490	TrainAcc 0.5291	ValidAcc 0.5038	TestAcc 0.5058	BestValid 0.5099
	Epoch 2950:	Loss 1.3486	TrainAcc 0.5223	ValidAcc 0.4980	TestAcc 0.4980	BestValid 0.5099
	Epoch 3000:	Loss 1.3485	TrainAcc 0.5137	ValidAcc 0.4897	TestAcc 0.4911	BestValid 0.5099
	Epoch 3050:	Loss 1.3475	TrainAcc 0.5349	ValidAcc 0.5105	TestAcc 0.5111	BestValid 0.5105
	Epoch 3100:	Loss 1.3480	TrainAcc 0.5271	ValidAcc 0.5009	TestAcc 0.5035	BestValid 0.5105
	Epoch 3150:	Loss 1.3455	TrainAcc 0.5347	ValidAcc 0.5073	TestAcc 0.5107	BestValid 0.5105
	Epoch 3200:	Loss 1.3441	TrainAcc 0.5129	ValidAcc 0.4880	TestAcc 0.4872	BestValid 0.5105
	Epoch 3250:	Loss 1.3485	TrainAcc 0.5095	ValidAcc 0.4852	TestAcc 0.4858	BestValid 0.5105
	Epoch 3300:	Loss 1.3424	TrainAcc 0.5268	ValidAcc 0.4982	TestAcc 0.5014	BestValid 0.5105
	Epoch 3350:	Loss 1.3420	TrainAcc 0.5217	ValidAcc 0.4957	TestAcc 0.4956	BestValid 0.5105
	Epoch 3400:	Loss 1.3424	TrainAcc 0.5217	ValidAcc 0.4956	TestAcc 0.4961	BestValid 0.5105
	Epoch 3450:	Loss 1.3414	TrainAcc 0.5178	ValidAcc 0.4907	TestAcc 0.4913	BestValid 0.5105
	Epoch 3500:	Loss 1.3420	TrainAcc 0.5310	ValidAcc 0.5013	TestAcc 0.5034	BestValid 0.5105
	Epoch 3550:	Loss 1.3385	TrainAcc 0.5247	ValidAcc 0.4944	TestAcc 0.4969	BestValid 0.5105
	Epoch 3600:	Loss 1.3395	TrainAcc 0.5306	ValidAcc 0.5020	TestAcc 0.5037	BestValid 0.5105
	Epoch 3650:	Loss 1.3377	TrainAcc 0.5181	ValidAcc 0.4909	TestAcc 0.4902	BestValid 0.5105
	Epoch 3700:	Loss 1.3357	TrainAcc 0.5328	ValidAcc 0.5017	TestAcc 0.5045	BestValid 0.5105
	Epoch 3750:	Loss 1.3366	TrainAcc 0.5242	ValidAcc 0.4951	TestAcc 0.4953	BestValid 0.5105
	Epoch 3800:	Loss 1.3340	TrainAcc 0.5355	ValidAcc 0.5033	TestAcc 0.5049	BestValid 0.5105
	Epoch 3850:	Loss 1.3326	TrainAcc 0.5148	ValidAcc 0.4860	TestAcc 0.4863	BestValid 0.5105
	Epoch 3900:	Loss 1.3352	TrainAcc 0.5144	ValidAcc 0.4842	TestAcc 0.4856	BestValid 0.5105
	Epoch 3950:	Loss 1.3318	TrainAcc 0.5211	ValidAcc 0.4902	TestAcc 0.4911	BestValid 0.5105
	Epoch 4000:	Loss 1.3319	TrainAcc 0.5250	ValidAcc 0.4913	TestAcc 0.4933	BestValid 0.5105
	Epoch 4050:	Loss 1.3298	TrainAcc 0.5372	ValidAcc 0.5025	TestAcc 0.5053	BestValid 0.5105
	Epoch 4100:	Loss 1.3281	TrainAcc 0.5153	ValidAcc 0.4834	TestAcc 0.4852	BestValid 0.5105
	Epoch 4150:	Loss 1.3310	TrainAcc 0.5293	ValidAcc 0.4950	TestAcc 0.4981	BestValid 0.5105
	Epoch 4200:	Loss 1.3278	TrainAcc 0.5422	ValidAcc 0.5077	TestAcc 0.5094	BestValid 0.5105
	Epoch 4250:	Loss 1.3320	TrainAcc 0.5324	ValidAcc 0.4989	TestAcc 0.5006	BestValid 0.5105
	Epoch 4300:	Loss 1.3284	TrainAcc 0.5277	ValidAcc 0.4941	TestAcc 0.4966	BestValid 0.5105
	Epoch 4350:	Loss 1.3249	TrainAcc 0.5398	ValidAcc 0.5033	TestAcc 0.5049	BestValid 0.5105
	Epoch 4400:	Loss 1.3273	TrainAcc 0.5352	ValidAcc 0.4975	TestAcc 0.5002	BestValid 0.5105
	Epoch 4450:	Loss 1.3226	TrainAcc 0.5255	ValidAcc 0.4914	TestAcc 0.4940	BestValid 0.5105
	Epoch 4500:	Loss 1.3254	TrainAcc 0.5330	ValidAcc 0.4957	TestAcc 0.4979	BestValid 0.5105
	Epoch 4550:	Loss 1.3201	TrainAcc 0.5234	ValidAcc 0.4853	TestAcc 0.4898	BestValid 0.5105
	Epoch 4600:	Loss 1.3244	TrainAcc 0.5292	ValidAcc 0.4902	TestAcc 0.4915	BestValid 0.5105
	Epoch 4650:	Loss 1.3252	TrainAcc 0.5286	ValidAcc 0.4902	TestAcc 0.4936	BestValid 0.5105
	Epoch 4700:	Loss 1.3167	TrainAcc 0.5315	ValidAcc 0.4931	TestAcc 0.4942	BestValid 0.5105
	Epoch 4750:	Loss 1.3208	TrainAcc 0.5347	ValidAcc 0.4935	TestAcc 0.4956	BestValid 0.5105
	Epoch 4800:	Loss 1.3175	TrainAcc 0.5310	ValidAcc 0.4904	TestAcc 0.4921	BestValid 0.5105
	Epoch 4850:	Loss 1.3275	TrainAcc 0.5327	ValidAcc 0.4914	TestAcc 0.4936	BestValid 0.5105
	Epoch 4900:	Loss 1.3197	TrainAcc 0.5459	ValidAcc 0.5054	TestAcc 0.5050	BestValid 0.5105
	Epoch 4950:	Loss 1.3140	TrainAcc 0.5212	ValidAcc 0.4824	TestAcc 0.4816	BestValid 0.5105
	Epoch 5000:	Loss 1.3172	TrainAcc 0.5441	ValidAcc 0.5034	TestAcc 0.5046	BestValid 0.5105
****** Epoch Time (Excluding Evaluation Cost): 0.020 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.052 ms (Max: 0.058, Min: 0.032, Sum: 0.414)
Cluster-Wide Average, Compute: 3.177 ms (Max: 3.329, Min: 3.042, Sum: 25.419)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.010, Min: 0.007, Sum: 0.060)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.013, Sum: 0.120)
Cluster-Wide Average, Communication-Graph: 15.328 ms (Max: 15.463, Min: 15.165, Sum: 122.625)
Cluster-Wide Average, Optimization: 0.373 ms (Max: 0.377, Min: 0.368, Sum: 2.981)
Cluster-Wide Average, Others: 0.784 ms (Max: 0.789, Min: 0.779, Sum: 6.269)
****** Breakdown Sum: 19.736 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.864 GB (Max: 1.979, Min: 1.833, Sum: 14.913)
Cluster-Wide Average, Graph-Level Communication Throughput: 43.052 Gbps (Max: 52.414, Min: 17.186, Sum: 344.415)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.573 GB
Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
Total communication (cluster-wide, per-epoch): 0.577 GB
****** Accuracy Results ******
Highest valid_acc: 0.5105
Target test_acc: 0.5111
Epoch to reach the target acc: 3049
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
