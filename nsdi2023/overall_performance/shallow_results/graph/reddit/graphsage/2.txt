Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...Building the CSR structure...
Building the CSR structure...

Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.867 seconds.
Building the CSC structure...
        It takes 1.869 seconds.
Building the CSC structure...
        It takes 2.011 seconds.
Building the CSC structure...
        It takes 2.019 seconds.
Building the CSC structure...
        It takes 2.260 seconds.
Building the CSC structure...
        It takes 2.376 seconds.
Building the CSC structure...
        It takes 2.517 seconds.
Building the CSC structure...
        It takes 2.635 seconds.
Building the CSC structure...
        It takes 1.787 seconds.
        It takes 1.826 seconds.
        It takes 1.865 seconds.
        It takes 1.874 seconds.
        It takes 2.177 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.355 seconds.
        It takes 2.329 seconds.
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.234 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.036 seconds.
Building the Feature Vector...
        It takes 0.231 seconds.
Building the Label Vector...
        It takes 2.572 seconds.
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.273 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.037 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.033 seconds.
Building the Feature Vector...
        It takes 0.287 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
        It takes 0.278 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.038 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.255 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.031 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.124 Gbps (per GPU), 440.994 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.938 Gbps (per GPU), 439.504 Gbps (aggregated)
The layer-level communication performance: 54.864 Gbps (per GPU), 438.913 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.630 Gbps (per GPU), 437.036 Gbps (aggregated)
The layer-level communication performance: 54.665 Gbps (per GPU), 437.321 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.436 Gbps (per GPU), 435.486 Gbps (aggregated)
The layer-level communication performance: 54.398 Gbps (per GPU), 435.188 Gbps (aggregated)
The layer-level communication performance: 54.365 Gbps (per GPU), 434.918 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.040 Gbps (per GPU), 1256.320 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.049 Gbps (per GPU), 1256.391 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.040 Gbps (per GPU), 1256.320 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.996 Gbps (per GPU), 1255.968 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.046 Gbps (per GPU), 1256.367 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.994 Gbps (per GPU), 1255.949 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.043 Gbps (per GPU), 1256.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.949 Gbps (per GPU), 1255.593 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.510 Gbps (per GPU), 812.082 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.508 Gbps (per GPU), 812.063 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.509 Gbps (per GPU), 812.076 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.507 Gbps (per GPU), 812.055 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.483 Gbps (per GPU), 811.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.509 Gbps (per GPU), 812.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.435 Gbps (per GPU), 811.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.163 Gbps (per GPU), 809.307 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.336 Gbps (per GPU), 314.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.335 Gbps (per GPU), 314.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.336 Gbps (per GPU), 314.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.335 Gbps (per GPU), 314.679 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.334 Gbps (per GPU), 314.673 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.334 Gbps (per GPU), 314.674 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.335 Gbps (per GPU), 314.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.335 Gbps (per GPU), 314.683 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.19ms  9.62ms  9.06ms  1.35 29.12K 14.23M
 chk_1  7.78ms  5.22ms  4.64ms  1.67 29.12K  6.56M
 chk_2 19.29ms 16.82ms 16.34ms  1.18 29.12K 24.68M
 chk_3 19.39ms 16.86ms 16.28ms  1.19 29.12K 22.95M
 chk_4  7.58ms  5.05ms  4.47ms  1.70 29.12K  6.33M
 chk_5 11.62ms  9.07ms  8.43ms  1.38 29.12K 12.05M
 chk_6 12.88ms 10.24ms  9.65ms  1.34 29.12K 14.60M
 chk_7 11.94ms  9.50ms  8.80ms  1.36 29.12K 13.21M
   Avg 12.83 10.30  9.71
   Max 19.39 16.86 16.34
   Min  7.58  5.05  4.47
 Ratio  2.56  3.34  3.66
   Var 17.57 17.69 17.90
Profiling takes 2.998 s
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 33)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 33)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 33)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 33)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 33)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 33)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 33)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 33)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 33)...
+++++++++ Node 4 initializing the weights for op[0, 33)...
+++++++++ Node 2 initializing the weights for op[0, 33)...
+++++++++ Node 3 initializing the weights for op[0, 33)...
+++++++++ Node 5 initializing the weights for op[0, 33)...
+++++++++ Node 6 initializing the weights for op[0, 33)...
+++++++++ Node 0 initializing the weights for op[0, 33)...
+++++++++ Node 7 initializing the weights for op[0, 33)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 6, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 5.4701	TrainAcc 0.0990	ValidAcc 0.0850	TestAcc 0.0867	BestValid 0.0850
	Epoch 50:	Loss 0.9378	TrainAcc 0.8338	ValidAcc 0.8495	TestAcc 0.8472	BestValid 0.8495
	Epoch 100:	Loss 0.4718	TrainAcc 0.9205	ValidAcc 0.9280	TestAcc 0.9267	BestValid 0.9280
	Epoch 150:	Loss 0.3860	TrainAcc 0.9304	ValidAcc 0.9352	TestAcc 0.9351	BestValid 0.9352
	Epoch 200:	Loss 0.3458	TrainAcc 0.9355	ValidAcc 0.9406	TestAcc 0.9397	BestValid 0.9406
	Epoch 250:	Loss 0.3244	TrainAcc 0.9387	ValidAcc 0.9435	TestAcc 0.9427	BestValid 0.9435
	Epoch 300:	Loss 0.3091	TrainAcc 0.9414	ValidAcc 0.9459	TestAcc 0.9450	BestValid 0.9459
	Epoch 350:	Loss 0.2971	TrainAcc 0.9437	ValidAcc 0.9477	TestAcc 0.9471	BestValid 0.9477
	Epoch 400:	Loss 0.2909	TrainAcc 0.9453	ValidAcc 0.9491	TestAcc 0.9481	BestValid 0.9491
	Epoch 450:	Loss 0.2807	TrainAcc 0.9467	ValidAcc 0.9498	TestAcc 0.9496	BestValid 0.9498
	Epoch 500:	Loss 0.2744	TrainAcc 0.9478	ValidAcc 0.9508	TestAcc 0.9506	BestValid 0.9508
	Epoch 550:	Loss 0.2681	TrainAcc 0.9489	ValidAcc 0.9515	TestAcc 0.9513	BestValid 0.9515
	Epoch 600:	Loss 0.2647	TrainAcc 0.9501	ValidAcc 0.9522	TestAcc 0.9521	BestValid 0.9522
	Epoch 650:	Loss 0.2596	TrainAcc 0.9507	ValidAcc 0.9529	TestAcc 0.9526	BestValid 0.9529
	Epoch 700:	Loss 0.2552	TrainAcc 0.9515	ValidAcc 0.9536	TestAcc 0.9530	BestValid 0.9536
	Epoch 750:	Loss 0.2495	TrainAcc 0.9522	ValidAcc 0.9543	TestAcc 0.9536	BestValid 0.9543
	Epoch 800:	Loss 0.2466	TrainAcc 0.9527	ValidAcc 0.9546	TestAcc 0.9541	BestValid 0.9546
	Epoch 850:	Loss 0.2434	TrainAcc 0.9532	ValidAcc 0.9551	TestAcc 0.9546	BestValid 0.9551
	Epoch 900:	Loss 0.2417	TrainAcc 0.9536	ValidAcc 0.9554	TestAcc 0.9548	BestValid 0.9554
	Epoch 950:	Loss 0.2363	TrainAcc 0.9542	ValidAcc 0.9557	TestAcc 0.9551	BestValid 0.9557
	Epoch 1000:	Loss 0.2344	TrainAcc 0.9548	ValidAcc 0.9565	TestAcc 0.9555	BestValid 0.9565
	Epoch 1050:	Loss 0.2353	TrainAcc 0.9554	ValidAcc 0.9569	TestAcc 0.9557	BestValid 0.9569
	Epoch 1100:	Loss 0.2299	TrainAcc 0.9557	ValidAcc 0.9568	TestAcc 0.9559	BestValid 0.9569
	Epoch 1150:	Loss 0.2291	TrainAcc 0.9561	ValidAcc 0.9570	TestAcc 0.9560	BestValid 0.9570
	Epoch 1200:	Loss 0.2264	TrainAcc 0.9566	ValidAcc 0.9576	TestAcc 0.9563	BestValid 0.9576
	Epoch 1250:	Loss 0.2256	TrainAcc 0.9569	ValidAcc 0.9575	TestAcc 0.9566	BestValid 0.9576
	Epoch 1300:	Loss 0.2223	TrainAcc 0.9574	ValidAcc 0.9579	TestAcc 0.9566	BestValid 0.9579
	Epoch 1350:	Loss 0.2223	TrainAcc 0.9576	ValidAcc 0.9581	TestAcc 0.9570	BestValid 0.9581
	Epoch 1400:	Loss 0.2203	TrainAcc 0.9581	ValidAcc 0.9583	TestAcc 0.9571	BestValid 0.9583
	Epoch 1450:	Loss 0.2179	TrainAcc 0.9583	ValidAcc 0.9584	TestAcc 0.9570	BestValid 0.9584
	Epoch 1500:	Loss 0.2170	TrainAcc 0.9588	ValidAcc 0.9588	TestAcc 0.9574	BestValid 0.9588
	Epoch 1550:	Loss 0.2156	TrainAcc 0.9592	ValidAcc 0.9594	TestAcc 0.9575	BestValid 0.9594
	Epoch 1600:	Loss 0.2133	TrainAcc 0.9596	ValidAcc 0.9594	TestAcc 0.9579	BestValid 0.9594
	Epoch 1650:	Loss 0.2117	TrainAcc 0.9599	ValidAcc 0.9596	TestAcc 0.9580	BestValid 0.9596
	Epoch 1700:	Loss 0.2106	TrainAcc 0.9601	ValidAcc 0.9597	TestAcc 0.9581	BestValid 0.9597
	Epoch 1750:	Loss 0.2095	TrainAcc 0.9604	ValidAcc 0.9598	TestAcc 0.9582	BestValid 0.9598
	Epoch 1800:	Loss 0.2070	TrainAcc 0.9607	ValidAcc 0.9600	TestAcc 0.9585	BestValid 0.9600
	Epoch 1850:	Loss 0.2077	TrainAcc 0.9609	ValidAcc 0.9602	TestAcc 0.9586	BestValid 0.9602
	Epoch 1900:	Loss 0.2035	TrainAcc 0.9611	ValidAcc 0.9599	TestAcc 0.9584	BestValid 0.9602
	Epoch 1950:	Loss 0.2039	TrainAcc 0.9614	ValidAcc 0.9604	TestAcc 0.9591	BestValid 0.9604
	Epoch 2000:	Loss 0.2016	TrainAcc 0.9618	ValidAcc 0.9605	TestAcc 0.9593	BestValid 0.9605
	Epoch 2050:	Loss 0.2017	TrainAcc 0.9619	ValidAcc 0.9608	TestAcc 0.9592	BestValid 0.9608
	Epoch 2100:	Loss 0.2024	TrainAcc 0.9622	ValidAcc 0.9606	TestAcc 0.9594	BestValid 0.9608
	Epoch 2150:	Loss 0.2001	TrainAcc 0.9624	ValidAcc 0.9605	TestAcc 0.9595	BestValid 0.9608
	Epoch 2200:	Loss 0.1999	TrainAcc 0.9625	ValidAcc 0.9611	TestAcc 0.9596	BestValid 0.9611
	Epoch 2250:	Loss 0.1995	TrainAcc 0.9628	ValidAcc 0.9608	TestAcc 0.9599	BestValid 0.9611
	Epoch 2300:	Loss 0.1963	TrainAcc 0.9629	ValidAcc 0.9610	TestAcc 0.9602	BestValid 0.9611
	Epoch 2350:	Loss 0.1958	TrainAcc 0.9633	ValidAcc 0.9611	TestAcc 0.9604	BestValid 0.9611
	Epoch 2400:	Loss 0.1947	TrainAcc 0.9635	ValidAcc 0.9614	TestAcc 0.9605	BestValid 0.9614
	Epoch 2450:	Loss 0.1924	TrainAcc 0.9637	ValidAcc 0.9613	TestAcc 0.9606	BestValid 0.9614
	Epoch 2500:	Loss 0.1931	TrainAcc 0.9636	ValidAcc 0.9611	TestAcc 0.9606	BestValid 0.9614
	Epoch 2550:	Loss 0.1936	TrainAcc 0.9641	ValidAcc 0.9615	TestAcc 0.9609	BestValid 0.9615
	Epoch 2600:	Loss 0.1900	TrainAcc 0.9642	ValidAcc 0.9615	TestAcc 0.9611	BestValid 0.9615
	Epoch 2650:	Loss 0.1907	TrainAcc 0.9643	ValidAcc 0.9616	TestAcc 0.9610	BestValid 0.9616
	Epoch 2700:	Loss 0.1897	TrainAcc 0.9646	ValidAcc 0.9620	TestAcc 0.9612	BestValid 0.9620
	Epoch 2750:	Loss 0.1875	TrainAcc 0.9648	ValidAcc 0.9617	TestAcc 0.9613	BestValid 0.9620
	Epoch 2800:	Loss 0.1885	TrainAcc 0.9652	ValidAcc 0.9618	TestAcc 0.9615	BestValid 0.9620
	Epoch 2850:	Loss 0.1864	TrainAcc 0.9652	ValidAcc 0.9620	TestAcc 0.9615	BestValid 0.9620
	Epoch 2900:	Loss 0.1859	TrainAcc 0.9653	ValidAcc 0.9619	TestAcc 0.9614	BestValid 0.9620
	Epoch 2950:	Loss 0.1851	TrainAcc 0.9656	ValidAcc 0.9624	TestAcc 0.9617	BestValid 0.9624
	Epoch 3000:	Loss 0.1847	TrainAcc 0.9659	ValidAcc 0.9621	TestAcc 0.9616	BestValid 0.9624
	Epoch 3050:	Loss 0.1837	TrainAcc 0.9660	ValidAcc 0.9625	TestAcc 0.9618	BestValid 0.9625
	Epoch 3100:	Loss 0.1821	TrainAcc 0.9661	ValidAcc 0.9626	TestAcc 0.9617	BestValid 0.9626
	Epoch 3150:	Loss 0.1822	TrainAcc 0.9663	ValidAcc 0.9625	TestAcc 0.9617	BestValid 0.9626
	Epoch 3200:	Loss 0.1807	TrainAcc 0.9663	ValidAcc 0.9625	TestAcc 0.9619	BestValid 0.9626
	Epoch 3250:	Loss 0.1814	TrainAcc 0.9664	ValidAcc 0.9623	TestAcc 0.9617	BestValid 0.9626
	Epoch 3300:	Loss 0.1795	TrainAcc 0.9665	ValidAcc 0.9626	TestAcc 0.9620	BestValid 0.9626
	Epoch 3350:	Loss 0.1796	TrainAcc 0.9668	ValidAcc 0.9629	TestAcc 0.9621	BestValid 0.9629
	Epoch 3400:	Loss 0.1766	TrainAcc 0.9669	ValidAcc 0.9631	TestAcc 0.9622	BestValid 0.9631
	Epoch 3450:	Loss 0.1784	TrainAcc 0.9671	ValidAcc 0.9630	TestAcc 0.9623	BestValid 0.9631
	Epoch 3500:	Loss 0.1773	TrainAcc 0.9671	ValidAcc 0.9628	TestAcc 0.9621	BestValid 0.9631
	Epoch 3550:	Loss 0.1779	TrainAcc 0.9673	ValidAcc 0.9628	TestAcc 0.9621	BestValid 0.9631
	Epoch 3600:	Loss 0.1772	TrainAcc 0.9675	ValidAcc 0.9633	TestAcc 0.9626	BestValid 0.9633
	Epoch 3650:	Loss 0.1756	TrainAcc 0.9678	ValidAcc 0.9632	TestAcc 0.9627	BestValid 0.9633
	Epoch 3700:	Loss 0.1752	TrainAcc 0.9676	ValidAcc 0.9633	TestAcc 0.9626	BestValid 0.9633
	Epoch 3750:	Loss 0.1739	TrainAcc 0.9678	ValidAcc 0.9631	TestAcc 0.9625	BestValid 0.9633
	Epoch 3800:	Loss 0.1735	TrainAcc 0.9681	ValidAcc 0.9634	TestAcc 0.9627	BestValid 0.9634
	Epoch 3850:	Loss 0.1739	TrainAcc 0.9680	ValidAcc 0.9634	TestAcc 0.9625	BestValid 0.9634
	Epoch 3900:	Loss 0.1740	TrainAcc 0.9682	ValidAcc 0.9634	TestAcc 0.9626	BestValid 0.9634
	Epoch 3950:	Loss 0.1732	TrainAcc 0.9684	ValidAcc 0.9637	TestAcc 0.9625	BestValid 0.9637
	Epoch 4000:	Loss 0.1713	TrainAcc 0.9686	ValidAcc 0.9636	TestAcc 0.9629	BestValid 0.9637
	Epoch 4050:	Loss 0.1712	TrainAcc 0.9685	ValidAcc 0.9638	TestAcc 0.9627	BestValid 0.9638
	Epoch 4100:	Loss 0.1681	TrainAcc 0.9688	ValidAcc 0.9636	TestAcc 0.9631	BestValid 0.9638
	Epoch 4150:	Loss 0.1718	TrainAcc 0.9690	ValidAcc 0.9638	TestAcc 0.9630	BestValid 0.9638
	Epoch 4200:	Loss 0.1711	TrainAcc 0.9690	ValidAcc 0.9638	TestAcc 0.9630	BestValid 0.9638
	Epoch 4250:	Loss 0.1688	TrainAcc 0.9691	ValidAcc 0.9643	TestAcc 0.9629	BestValid 0.9643
	Epoch 4300:	Loss 0.1681	TrainAcc 0.9692	ValidAcc 0.9638	TestAcc 0.9631	BestValid 0.9643
	Epoch 4350:	Loss 0.1686	TrainAcc 0.9694	ValidAcc 0.9641	TestAcc 0.9631	BestValid 0.9643
	Epoch 4400:	Loss 0.1669	TrainAcc 0.9695	ValidAcc 0.9639	TestAcc 0.9632	BestValid 0.9643
	Epoch 4450:	Loss 0.1664	TrainAcc 0.9698	ValidAcc 0.9642	TestAcc 0.9629	BestValid 0.9643
	Epoch 4500:	Loss 0.1657	TrainAcc 0.9699	ValidAcc 0.9642	TestAcc 0.9631	BestValid 0.9643
	Epoch 4550:	Loss 0.1660	TrainAcc 0.9700	ValidAcc 0.9645	TestAcc 0.9631	BestValid 0.9645
	Epoch 4600:	Loss 0.1652	TrainAcc 0.9701	ValidAcc 0.9645	TestAcc 0.9633	BestValid 0.9645
	Epoch 4650:	Loss 0.1665	TrainAcc 0.9701	ValidAcc 0.9645	TestAcc 0.9631	BestValid 0.9645
	Epoch 4700:	Loss 0.1652	TrainAcc 0.9704	ValidAcc 0.9642	TestAcc 0.9634	BestValid 0.9645
	Epoch 4750:	Loss 0.1655	TrainAcc 0.9704	ValidAcc 0.9642	TestAcc 0.9633	BestValid 0.9645
	Epoch 4800:	Loss 0.1631	TrainAcc 0.9707	ValidAcc 0.9642	TestAcc 0.9633	BestValid 0.9645
	Epoch 4850:	Loss 0.1639	TrainAcc 0.9707	ValidAcc 0.9644	TestAcc 0.9632	BestValid 0.9645
	Epoch 4900:	Loss 0.1665	TrainAcc 0.9708	ValidAcc 0.9650	TestAcc 0.9634	BestValid 0.9650
	Epoch 4950:	Loss 0.1629	TrainAcc 0.9710	ValidAcc 0.9646	TestAcc 0.9636	BestValid 0.9650
	Epoch 5000:	Loss 0.1630	TrainAcc 0.9712	ValidAcc 0.9650	TestAcc 0.9636	BestValid 0.9650
****** Epoch Time (Excluding Evaluation Cost): 0.140 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.238 ms (Max: 0.316, Min: 0.036, Sum: 1.900)
Cluster-Wide Average, Compute: 48.013 ms (Max: 72.851, Min: 29.367, Sum: 384.103)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.121)
Cluster-Wide Average, Communication-Graph: 88.229 ms (Max: 106.798, Min: 63.553, Sum: 705.832)
Cluster-Wide Average, Optimization: 0.957 ms (Max: 0.966, Min: 0.943, Sum: 7.656)
Cluster-Wide Average, Others: 2.189 ms (Max: 2.230, Min: 2.171, Sum: 17.512)
****** Breakdown Sum: 139.649 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.314 GB (Max: 5.641, Min: 5.251, Sum: 42.513)
Cluster-Wide Average, Graph-Level Communication Throughput: 24.570 Gbps (Max: 48.667, Min: 9.991, Sum: 196.556)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 1.810 GB
Weight-sync communication (cluster-wide, per-epoch): 0.009 GB
Total communication (cluster-wide, per-epoch): 1.819 GB
****** Accuracy Results ******
Highest valid_acc: 0.9650
Target test_acc: 0.9634
Epoch to reach the target acc: 4899
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
