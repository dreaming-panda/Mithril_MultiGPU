Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...Building the CSR structure...
Building the CSR structure...

Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.834 seconds.
Building the CSC structure...
        It takes 2.021 seconds.
Building the CSC structure...
        It takes 2.055 seconds.
Building the CSC structure...
        It takes 2.058 seconds.
Building the CSC structure...
        It takes 2.379 seconds.
Building the CSC structure...
        It takes 2.384 seconds.
Building the CSC structure...
        It takes 2.409 seconds.
Building the CSC structure...
        It takes 2.575 seconds.
Building the CSC structure...
        It takes 1.806 seconds.
        It takes 1.868 seconds.
        It takes 1.858 seconds.
        It takes 1.857 seconds.
Building the Feature Vector...
        It takes 2.303 seconds.
        It takes 2.351 seconds.
        It takes 2.367 seconds.
        It takes 2.293 seconds.
        It takes 0.303 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
Building the Feature Vector...
        It takes 0.266 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.039 seconds.
        It takes 0.258 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCNII layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.532 Gbps (per GPU), 460.255 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.267 Gbps (per GPU), 458.133 Gbps (aggregated)
The layer-level communication performance: 57.297 Gbps (per GPU), 458.376 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.035 Gbps (per GPU), 456.277 Gbps (aggregated)
The layer-level communication performance: 56.997 Gbps (per GPU), 455.973 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.840 Gbps (per GPU), 454.720 Gbps (aggregated)
The layer-level communication performance: 56.764 Gbps (per GPU), 454.110 Gbps (aggregated)
The layer-level communication performance: 56.724 Gbps (per GPU), 453.789 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.952 Gbps (per GPU), 1255.615 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.975 Gbps (per GPU), 1255.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.952 Gbps (per GPU), 1255.615 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.978 Gbps (per GPU), 1255.827 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.949 Gbps (per GPU), 1255.593 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.975 Gbps (per GPU), 1255.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.949 Gbps (per GPU), 1255.592 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.981 Gbps (per GPU), 1255.850 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.520 Gbps (per GPU), 804.161 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.521 Gbps (per GPU), 804.168 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.519 Gbps (per GPU), 804.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.521 Gbps (per GPU), 804.168 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.520 Gbps (per GPU), 804.161 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.519 Gbps (per GPU), 804.154 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.521 Gbps (per GPU), 804.167 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.523 Gbps (per GPU), 804.181 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.324 Gbps (per GPU), 282.590 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.324 Gbps (per GPU), 282.591 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.323 Gbps (per GPU), 282.581 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.323 Gbps (per GPU), 282.585 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.324 Gbps (per GPU), 282.592 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.324 Gbps (per GPU), 282.590 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.324 Gbps (per GPU), 282.593 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.324 Gbps (per GPU), 282.588 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.23ms  9.62ms  9.06ms  1.35 29.12K 14.23M
 chk_1  7.77ms  5.24ms  4.65ms  1.67 29.12K  6.56M
 chk_2 19.27ms 16.82ms 16.34ms  1.18 29.12K 24.68M
 chk_3 19.37ms 16.84ms 16.33ms  1.19 29.12K 22.95M
 chk_4  7.60ms  5.06ms  4.50ms  1.69 29.12K  6.33M
 chk_5 11.60ms  9.02ms  8.47ms  1.37 29.12K 12.05M
 chk_6 12.84ms 10.24ms  9.66ms  1.33 29.12K 14.60M
 chk_7 11.94ms  9.58ms  8.77ms  1.36 29.12K 13.21M
   Avg 12.83 10.30  9.72
   Max 19.37 16.84 16.34
   Min  7.60  5.06  4.50
 Ratio  2.55  3.33  3.63
   Var 17.49 17.62 17.93
Profiling takes 2.999 s
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 33)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 33)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 33)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 33)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 33)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 33)
*** Node 2, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 33)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 33)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 33)...
+++++++++ Node 1 initializing the weights for op[0, 33)...
+++++++++ Node 5 initializing the weights for op[0, 33)...
+++++++++ Node 2 initializing the weights for op[0, 33)...
+++++++++ Node 7 initializing the weights for op[0, 33)...
+++++++++ Node 3 initializing the weights for op[0, 33)...
+++++++++ Node 4 initializing the weights for op[0, 33)...
+++++++++ Node 6 initializing the weights for op[0, 33)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 5.3204	TrainAcc 0.1704	ValidAcc 0.1758	TestAcc 0.1695	BestValid 0.1758
	Epoch 50:	Loss 0.8737	TrainAcc 0.8451	ValidAcc 0.8601	TestAcc 0.8561	BestValid 0.8601
	Epoch 100:	Loss 0.4478	TrainAcc 0.9232	ValidAcc 0.9307	TestAcc 0.9291	BestValid 0.9307
	Epoch 150:	Loss 0.3713	TrainAcc 0.9314	ValidAcc 0.9370	TestAcc 0.9370	BestValid 0.9370
	Epoch 200:	Loss 0.3388	TrainAcc 0.9360	ValidAcc 0.9407	TestAcc 0.9408	BestValid 0.9407
	Epoch 250:	Loss 0.3212	TrainAcc 0.9395	ValidAcc 0.9440	TestAcc 0.9435	BestValid 0.9440
	Epoch 300:	Loss 0.3059	TrainAcc 0.9422	ValidAcc 0.9466	TestAcc 0.9460	BestValid 0.9466
	Epoch 350:	Loss 0.2946	TrainAcc 0.9441	ValidAcc 0.9482	TestAcc 0.9474	BestValid 0.9482
	Epoch 400:	Loss 0.2859	TrainAcc 0.9455	ValidAcc 0.9488	TestAcc 0.9486	BestValid 0.9488
	Epoch 450:	Loss 0.2804	TrainAcc 0.9470	ValidAcc 0.9501	TestAcc 0.9498	BestValid 0.9501
	Epoch 500:	Loss 0.2723	TrainAcc 0.9482	ValidAcc 0.9504	TestAcc 0.9507	BestValid 0.9504
	Epoch 550:	Loss 0.2682	TrainAcc 0.9491	ValidAcc 0.9518	TestAcc 0.9517	BestValid 0.9518
	Epoch 600:	Loss 0.2612	TrainAcc 0.9500	ValidAcc 0.9522	TestAcc 0.9524	BestValid 0.9522
	Epoch 650:	Loss 0.2564	TrainAcc 0.9509	ValidAcc 0.9527	TestAcc 0.9527	BestValid 0.9527
	Epoch 700:	Loss 0.2544	TrainAcc 0.9517	ValidAcc 0.9539	TestAcc 0.9532	BestValid 0.9539
	Epoch 750:	Loss 0.2484	TrainAcc 0.9524	ValidAcc 0.9542	TestAcc 0.9539	BestValid 0.9542
	Epoch 800:	Loss 0.2482	TrainAcc 0.9529	ValidAcc 0.9544	TestAcc 0.9541	BestValid 0.9544
	Epoch 850:	Loss 0.2451	TrainAcc 0.9536	ValidAcc 0.9552	TestAcc 0.9543	BestValid 0.9552
	Epoch 900:	Loss 0.2409	TrainAcc 0.9538	ValidAcc 0.9553	TestAcc 0.9545	BestValid 0.9553
	Epoch 950:	Loss 0.2374	TrainAcc 0.9544	ValidAcc 0.9554	TestAcc 0.9550	BestValid 0.9554
	Epoch 1000:	Loss 0.2362	TrainAcc 0.9548	ValidAcc 0.9562	TestAcc 0.9551	BestValid 0.9562
	Epoch 1050:	Loss 0.2334	TrainAcc 0.9554	ValidAcc 0.9567	TestAcc 0.9555	BestValid 0.9567
	Epoch 1100:	Loss 0.2323	TrainAcc 0.9559	ValidAcc 0.9571	TestAcc 0.9558	BestValid 0.9571
	Epoch 1150:	Loss 0.2285	TrainAcc 0.9562	ValidAcc 0.9572	TestAcc 0.9561	BestValid 0.9572
	Epoch 1200:	Loss 0.2273	TrainAcc 0.9567	ValidAcc 0.9573	TestAcc 0.9565	BestValid 0.9573
	Epoch 1250:	Loss 0.2256	TrainAcc 0.9568	ValidAcc 0.9578	TestAcc 0.9565	BestValid 0.9578
	Epoch 1300:	Loss 0.2245	TrainAcc 0.9574	ValidAcc 0.9577	TestAcc 0.9568	BestValid 0.9578
	Epoch 1350:	Loss 0.2209	TrainAcc 0.9577	ValidAcc 0.9579	TestAcc 0.9571	BestValid 0.9579
	Epoch 1400:	Loss 0.2212	TrainAcc 0.9581	ValidAcc 0.9584	TestAcc 0.9573	BestValid 0.9584
	Epoch 1450:	Loss 0.2169	TrainAcc 0.9584	ValidAcc 0.9586	TestAcc 0.9577	BestValid 0.9586
	Epoch 1500:	Loss 0.2141	TrainAcc 0.9591	ValidAcc 0.9589	TestAcc 0.9580	BestValid 0.9589
	Epoch 1550:	Loss 0.2151	TrainAcc 0.9590	ValidAcc 0.9590	TestAcc 0.9580	BestValid 0.9590
	Epoch 1600:	Loss 0.2147	TrainAcc 0.9595	ValidAcc 0.9592	TestAcc 0.9582	BestValid 0.9592
	Epoch 1650:	Loss 0.2118	TrainAcc 0.9598	ValidAcc 0.9592	TestAcc 0.9585	BestValid 0.9592
	Epoch 1700:	Loss 0.2098	TrainAcc 0.9601	ValidAcc 0.9593	TestAcc 0.9588	BestValid 0.9593
	Epoch 1750:	Loss 0.2099	TrainAcc 0.9603	ValidAcc 0.9597	TestAcc 0.9590	BestValid 0.9597
	Epoch 1800:	Loss 0.2090	TrainAcc 0.9605	ValidAcc 0.9598	TestAcc 0.9590	BestValid 0.9598
	Epoch 1850:	Loss 0.2048	TrainAcc 0.9609	ValidAcc 0.9602	TestAcc 0.9594	BestValid 0.9602
	Epoch 1900:	Loss 0.2078	TrainAcc 0.9611	ValidAcc 0.9603	TestAcc 0.9597	BestValid 0.9603
	Epoch 1950:	Loss 0.2036	TrainAcc 0.9614	ValidAcc 0.9600	TestAcc 0.9598	BestValid 0.9603
	Epoch 2000:	Loss 0.2028	TrainAcc 0.9614	ValidAcc 0.9601	TestAcc 0.9598	BestValid 0.9603
	Epoch 2050:	Loss 0.1987	TrainAcc 0.9616	ValidAcc 0.9604	TestAcc 0.9600	BestValid 0.9604
	Epoch 2100:	Loss 0.2007	TrainAcc 0.9621	ValidAcc 0.9604	TestAcc 0.9601	BestValid 0.9604
	Epoch 2150:	Loss 0.1982	TrainAcc 0.9623	ValidAcc 0.9608	TestAcc 0.9602	BestValid 0.9608
	Epoch 2200:	Loss 0.1986	TrainAcc 0.9626	ValidAcc 0.9608	TestAcc 0.9603	BestValid 0.9608
	Epoch 2250:	Loss 0.1967	TrainAcc 0.9625	ValidAcc 0.9613	TestAcc 0.9604	BestValid 0.9613
	Epoch 2300:	Loss 0.1949	TrainAcc 0.9628	ValidAcc 0.9608	TestAcc 0.9603	BestValid 0.9613
	Epoch 2350:	Loss 0.1954	TrainAcc 0.9631	ValidAcc 0.9611	TestAcc 0.9606	BestValid 0.9613
	Epoch 2400:	Loss 0.1937	TrainAcc 0.9635	ValidAcc 0.9613	TestAcc 0.9608	BestValid 0.9613
	Epoch 2450:	Loss 0.1951	TrainAcc 0.9636	ValidAcc 0.9612	TestAcc 0.9610	BestValid 0.9613
	Epoch 2500:	Loss 0.1900	TrainAcc 0.9636	ValidAcc 0.9615	TestAcc 0.9610	BestValid 0.9615
	Epoch 2550:	Loss 0.1915	TrainAcc 0.9639	ValidAcc 0.9615	TestAcc 0.9610	BestValid 0.9615
	Epoch 2600:	Loss 0.1905	TrainAcc 0.9643	ValidAcc 0.9619	TestAcc 0.9613	BestValid 0.9619
	Epoch 2650:	Loss 0.1898	TrainAcc 0.9645	ValidAcc 0.9621	TestAcc 0.9614	BestValid 0.9621
	Epoch 2700:	Loss 0.1888	TrainAcc 0.9647	ValidAcc 0.9618	TestAcc 0.9612	BestValid 0.9621
	Epoch 2750:	Loss 0.1876	TrainAcc 0.9649	ValidAcc 0.9621	TestAcc 0.9615	BestValid 0.9621
	Epoch 2800:	Loss 0.1866	TrainAcc 0.9651	ValidAcc 0.9627	TestAcc 0.9616	BestValid 0.9627
	Epoch 2850:	Loss 0.1861	TrainAcc 0.9651	ValidAcc 0.9620	TestAcc 0.9617	BestValid 0.9627
	Epoch 2900:	Loss 0.1852	TrainAcc 0.9652	ValidAcc 0.9620	TestAcc 0.9616	BestValid 0.9627
	Epoch 2950:	Loss 0.1840	TrainAcc 0.9654	ValidAcc 0.9622	TestAcc 0.9619	BestValid 0.9627
	Epoch 3000:	Loss 0.1859	TrainAcc 0.9655	ValidAcc 0.9621	TestAcc 0.9618	BestValid 0.9627
	Epoch 3050:	Loss 0.1829	TrainAcc 0.9657	ValidAcc 0.9625	TestAcc 0.9622	BestValid 0.9627
	Epoch 3100:	Loss 0.1817	TrainAcc 0.9658	ValidAcc 0.9622	TestAcc 0.9620	BestValid 0.9627
	Epoch 3150:	Loss 0.1822	TrainAcc 0.9659	ValidAcc 0.9621	TestAcc 0.9620	BestValid 0.9627
	Epoch 3200:	Loss 0.1802	TrainAcc 0.9661	ValidAcc 0.9624	TestAcc 0.9621	BestValid 0.9627
	Epoch 3250:	Loss 0.1794	TrainAcc 0.9664	ValidAcc 0.9625	TestAcc 0.9622	BestValid 0.9627
	Epoch 3300:	Loss 0.1784	TrainAcc 0.9664	ValidAcc 0.9627	TestAcc 0.9624	BestValid 0.9627
	Epoch 3350:	Loss 0.1785	TrainAcc 0.9664	ValidAcc 0.9624	TestAcc 0.9622	BestValid 0.9627
	Epoch 3400:	Loss 0.1786	TrainAcc 0.9668	ValidAcc 0.9627	TestAcc 0.9624	BestValid 0.9627
	Epoch 3450:	Loss 0.1783	TrainAcc 0.9671	ValidAcc 0.9631	TestAcc 0.9626	BestValid 0.9631
	Epoch 3500:	Loss 0.1775	TrainAcc 0.9672	ValidAcc 0.9632	TestAcc 0.9627	BestValid 0.9632
	Epoch 3550:	Loss 0.1763	TrainAcc 0.9672	ValidAcc 0.9627	TestAcc 0.9626	BestValid 0.9632
	Epoch 3600:	Loss 0.1758	TrainAcc 0.9673	ValidAcc 0.9631	TestAcc 0.9627	BestValid 0.9632
	Epoch 3650:	Loss 0.1748	TrainAcc 0.9676	ValidAcc 0.9632	TestAcc 0.9628	BestValid 0.9632
	Epoch 3700:	Loss 0.1763	TrainAcc 0.9676	ValidAcc 0.9630	TestAcc 0.9629	BestValid 0.9632
	Epoch 3750:	Loss 0.1747	TrainAcc 0.9678	ValidAcc 0.9634	TestAcc 0.9628	BestValid 0.9634
	Epoch 3800:	Loss 0.1745	TrainAcc 0.9679	ValidAcc 0.9634	TestAcc 0.9629	BestValid 0.9634
	Epoch 3850:	Loss 0.1732	TrainAcc 0.9678	ValidAcc 0.9633	TestAcc 0.9628	BestValid 0.9634
	Epoch 3900:	Loss 0.1724	TrainAcc 0.9682	ValidAcc 0.9635	TestAcc 0.9630	BestValid 0.9635
	Epoch 3950:	Loss 0.1719	TrainAcc 0.9682	ValidAcc 0.9635	TestAcc 0.9629	BestValid 0.9635
	Epoch 4000:	Loss 0.1705	TrainAcc 0.9685	ValidAcc 0.9636	TestAcc 0.9631	BestValid 0.9636
	Epoch 4050:	Loss 0.1713	TrainAcc 0.9684	ValidAcc 0.9636	TestAcc 0.9629	BestValid 0.9636
	Epoch 4100:	Loss 0.1707	TrainAcc 0.9686	ValidAcc 0.9636	TestAcc 0.9629	BestValid 0.9636
	Epoch 4150:	Loss 0.1697	TrainAcc 0.9686	ValidAcc 0.9635	TestAcc 0.9631	BestValid 0.9636
	Epoch 4200:	Loss 0.1679	TrainAcc 0.9689	ValidAcc 0.9638	TestAcc 0.9632	BestValid 0.9638
	Epoch 4250:	Loss 0.1704	TrainAcc 0.9690	ValidAcc 0.9636	TestAcc 0.9630	BestValid 0.9638
	Epoch 4300:	Loss 0.1668	TrainAcc 0.9690	ValidAcc 0.9640	TestAcc 0.9632	BestValid 0.9640
	Epoch 4350:	Loss 0.1665	TrainAcc 0.9692	ValidAcc 0.9642	TestAcc 0.9631	BestValid 0.9642
	Epoch 4400:	Loss 0.1687	TrainAcc 0.9692	ValidAcc 0.9636	TestAcc 0.9630	BestValid 0.9642
	Epoch 4450:	Loss 0.1689	TrainAcc 0.9696	ValidAcc 0.9645	TestAcc 0.9635	BestValid 0.9645
	Epoch 4500:	Loss 0.1675	TrainAcc 0.9696	ValidAcc 0.9640	TestAcc 0.9634	BestValid 0.9645
	Epoch 4550:	Loss 0.1648	TrainAcc 0.9696	ValidAcc 0.9639	TestAcc 0.9633	BestValid 0.9645
	Epoch 4600:	Loss 0.1668	TrainAcc 0.9696	ValidAcc 0.9640	TestAcc 0.9633	BestValid 0.9645
	Epoch 4650:	Loss 0.1668	TrainAcc 0.9699	ValidAcc 0.9642	TestAcc 0.9634	BestValid 0.9645
	Epoch 4700:	Loss 0.1671	TrainAcc 0.9699	ValidAcc 0.9642	TestAcc 0.9633	BestValid 0.9645
	Epoch 4750:	Loss 0.1657	TrainAcc 0.9701	ValidAcc 0.9642	TestAcc 0.9634	BestValid 0.9645
	Epoch 4800:	Loss 0.1649	TrainAcc 0.9702	ValidAcc 0.9644	TestAcc 0.9634	BestValid 0.9645
	Epoch 4850:	Loss 0.1625	TrainAcc 0.9703	ValidAcc 0.9644	TestAcc 0.9634	BestValid 0.9645
	Epoch 4900:	Loss 0.1629	TrainAcc 0.9705	ValidAcc 0.9647	TestAcc 0.9636	BestValid 0.9647
	Epoch 4950:	Loss 0.1642	TrainAcc 0.9708	ValidAcc 0.9646	TestAcc 0.9635	BestValid 0.9647
	Epoch 5000:	Loss 0.1634	TrainAcc 0.9706	ValidAcc 0.9643	TestAcc 0.9637	BestValid 0.9647
****** Epoch Time (Excluding Evaluation Cost): 0.140 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.236 ms (Max: 0.315, Min: 0.034, Sum: 1.884)
Cluster-Wide Average, Compute: 48.139 ms (Max: 72.886, Min: 29.451, Sum: 385.109)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.012, Sum: 0.117)
Cluster-Wide Average, Communication-Graph: 88.134 ms (Max: 106.751, Min: 63.558, Sum: 705.072)
Cluster-Wide Average, Optimization: 0.960 ms (Max: 0.970, Min: 0.944, Sum: 7.680)
Cluster-Wide Average, Others: 2.190 ms (Max: 2.228, Min: 2.171, Sum: 17.517)
****** Breakdown Sum: 139.680 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.314 GB (Max: 5.641, Min: 5.251, Sum: 42.513)
Cluster-Wide Average, Graph-Level Communication Throughput: 24.598 Gbps (Max: 48.657, Min: 9.995, Sum: 196.786)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 1.810 GB
Weight-sync communication (cluster-wide, per-epoch): 0.009 GB
Total communication (cluster-wide, per-epoch): 1.819 GB
****** Accuracy Results ******
Highest valid_acc: 0.9647
Target test_acc: 0.9636
Epoch to reach the target acc: 4899
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
