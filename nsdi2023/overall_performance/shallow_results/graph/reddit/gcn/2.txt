Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.855 seconds.
Building the CSC structure...
        It takes 1.884 seconds.
Building the CSC structure...
        It takes 2.022 seconds.
Building the CSC structure...
        It takes 2.387 seconds.
Building the CSC structure...
        It takes 2.426 seconds.
Building the CSC structure...
        It takes 2.447 seconds.
Building the CSC structure...
        It takes 2.620 seconds.
Building the CSC structure...
        It takes 2.627 seconds.
Building the CSC structure...
        It takes 1.837 seconds.
        It takes 1.813 seconds.
        It takes 1.982 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.295 seconds.
        It takes 2.350 seconds.
        It takes 2.334 seconds.
        It takes 2.242 seconds.
        It takes 2.306 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.043 seconds.
        It takes 0.043 seconds.
Building the Feature Vector...
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCN layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.289 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.037 seconds.
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.256 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.029 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
        It takes 0.255 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.454 Gbps (per GPU), 459.632 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.200 Gbps (per GPU), 457.597 Gbps (aggregated)
The layer-level communication performance: 57.192 Gbps (per GPU), 457.536 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.963 Gbps (per GPU), 455.703 Gbps (aggregated)
The layer-level communication performance: 56.925 Gbps (per GPU), 455.402 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.830 Gbps (per GPU), 454.638 Gbps (aggregated)
The layer-level communication performance: 56.700 Gbps (per GPU), 453.603 Gbps (aggregated)
The layer-level communication performance: 56.662 Gbps (per GPU), 453.293 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.990 Gbps (per GPU), 1255.921 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.005 Gbps (per GPU), 1256.038 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.993 Gbps (per GPU), 1255.944 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.931 Gbps (per GPU), 1255.451 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.981 Gbps (per GPU), 1255.850 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.928 Gbps (per GPU), 1255.427 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.993 Gbps (per GPU), 1255.944 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.900 Gbps (per GPU), 1255.197 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.571 Gbps (per GPU), 804.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.572 Gbps (per GPU), 804.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.569 Gbps (per GPU), 804.553 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.570 Gbps (per GPU), 804.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.568 Gbps (per GPU), 804.540 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.571 Gbps (per GPU), 804.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.570 Gbps (per GPU), 804.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.572 Gbps (per GPU), 804.573 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.026 Gbps (per GPU), 264.205 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.026 Gbps (per GPU), 264.205 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.025 Gbps (per GPU), 264.199 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.026 Gbps (per GPU), 264.204 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.025 Gbps (per GPU), 264.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.025 Gbps (per GPU), 264.199 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.025 Gbps (per GPU), 264.201 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.025 Gbps (per GPU), 264.199 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.51ms  8.97ms  8.68ms  1.10 29.12K 14.23M
 chk_1  5.21ms  4.60ms  4.28ms  1.22 29.12K  6.56M
 chk_2 16.65ms 16.17ms 15.81ms  1.05 29.12K 24.68M
 chk_3 16.89ms 16.23ms 15.86ms  1.07 29.12K 22.95M
 chk_4  5.03ms  4.41ms  4.11ms  1.22 29.12K  6.33M
 chk_5  9.06ms  8.37ms  8.16ms  1.11 29.12K 12.05M
 chk_6 10.24ms  9.60ms  9.30ms  1.10 29.12K 14.60M
 chk_7  9.65ms  8.58ms  8.44ms  1.14 29.12K 13.21M
   Avg 10.28  9.62  9.33
   Max 16.89 16.23 15.86
   Min  5.03  4.41  4.11
 Ratio  3.36  3.68  3.86
   Var 17.52 17.75 17.49
Profiling takes 2.670 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 20)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 20)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 20)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 20)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 20)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 20)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 20)...
+++++++++ Node 5 initializing the weights for op[0, 20)...
+++++++++ Node 2 initializing the weights for op[0, 20)...
+++++++++ Node 6 initializing the weights for op[0, 20)...
+++++++++ Node 7 initializing the weights for op[0, 20)...
+++++++++ Node 3 initializing the weights for op[0, 20)...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 1 initializing the weights for op[0, 20)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7350	TrainAcc 0.0707	ValidAcc 0.0806	TestAcc 0.0792	BestValid 0.0806
	Epoch 50:	Loss 0.7327	TrainAcc 0.8702	ValidAcc 0.8797	TestAcc 0.8782	BestValid 0.8797
	Epoch 100:	Loss 0.4747	TrainAcc 0.9106	ValidAcc 0.9180	TestAcc 0.9174	BestValid 0.9180
	Epoch 150:	Loss 0.4157	TrainAcc 0.9202	ValidAcc 0.9259	TestAcc 0.9261	BestValid 0.9259
	Epoch 200:	Loss 0.3858	TrainAcc 0.9253	ValidAcc 0.9297	TestAcc 0.9308	BestValid 0.9297
	Epoch 250:	Loss 0.3681	TrainAcc 0.9281	ValidAcc 0.9324	TestAcc 0.9332	BestValid 0.9324
	Epoch 300:	Loss 0.3557	TrainAcc 0.9301	ValidAcc 0.9332	TestAcc 0.9350	BestValid 0.9332
	Epoch 350:	Loss 0.3457	TrainAcc 0.9317	ValidAcc 0.9347	TestAcc 0.9366	BestValid 0.9347
	Epoch 400:	Loss 0.3391	TrainAcc 0.9328	ValidAcc 0.9354	TestAcc 0.9371	BestValid 0.9354
	Epoch 450:	Loss 0.3323	TrainAcc 0.9337	ValidAcc 0.9358	TestAcc 0.9381	BestValid 0.9358
	Epoch 500:	Loss 0.3278	TrainAcc 0.9344	ValidAcc 0.9368	TestAcc 0.9385	BestValid 0.9368
	Epoch 550:	Loss 0.3235	TrainAcc 0.9355	ValidAcc 0.9382	TestAcc 0.9394	BestValid 0.9382
	Epoch 600:	Loss 0.3210	TrainAcc 0.9361	ValidAcc 0.9384	TestAcc 0.9397	BestValid 0.9384
	Epoch 650:	Loss 0.3168	TrainAcc 0.9363	ValidAcc 0.9384	TestAcc 0.9399	BestValid 0.9384
	Epoch 700:	Loss 0.3135	TrainAcc 0.9368	ValidAcc 0.9394	TestAcc 0.9404	BestValid 0.9394
	Epoch 750:	Loss 0.3087	TrainAcc 0.9374	ValidAcc 0.9397	TestAcc 0.9407	BestValid 0.9397
	Epoch 800:	Loss 0.3083	TrainAcc 0.9380	ValidAcc 0.9405	TestAcc 0.9411	BestValid 0.9405
	Epoch 850:	Loss 0.3038	TrainAcc 0.9384	ValidAcc 0.9398	TestAcc 0.9408	BestValid 0.9405
	Epoch 900:	Loss 0.3014	TrainAcc 0.9389	ValidAcc 0.9410	TestAcc 0.9416	BestValid 0.9410
	Epoch 950:	Loss 0.2998	TrainAcc 0.9390	ValidAcc 0.9408	TestAcc 0.9413	BestValid 0.9410
	Epoch 1000:	Loss 0.2983	TrainAcc 0.9394	ValidAcc 0.9412	TestAcc 0.9415	BestValid 0.9412
	Epoch 1050:	Loss 0.2966	TrainAcc 0.9397	ValidAcc 0.9413	TestAcc 0.9420	BestValid 0.9413
	Epoch 1100:	Loss 0.2940	TrainAcc 0.9399	ValidAcc 0.9416	TestAcc 0.9422	BestValid 0.9416
	Epoch 1150:	Loss 0.2921	TrainAcc 0.9404	ValidAcc 0.9419	TestAcc 0.9424	BestValid 0.9419
	Epoch 1200:	Loss 0.2893	TrainAcc 0.9405	ValidAcc 0.9415	TestAcc 0.9423	BestValid 0.9419
	Epoch 1250:	Loss 0.2881	TrainAcc 0.9408	ValidAcc 0.9419	TestAcc 0.9427	BestValid 0.9419
	Epoch 1300:	Loss 0.2869	TrainAcc 0.9411	ValidAcc 0.9418	TestAcc 0.9432	BestValid 0.9419
	Epoch 1350:	Loss 0.2844	TrainAcc 0.9413	ValidAcc 0.9415	TestAcc 0.9432	BestValid 0.9419
	Epoch 1400:	Loss 0.2842	TrainAcc 0.9415	ValidAcc 0.9418	TestAcc 0.9431	BestValid 0.9419
	Epoch 1450:	Loss 0.2824	TrainAcc 0.9416	ValidAcc 0.9419	TestAcc 0.9433	BestValid 0.9419
	Epoch 1500:	Loss 0.2819	TrainAcc 0.9419	ValidAcc 0.9421	TestAcc 0.9435	BestValid 0.9421
	Epoch 1550:	Loss 0.2805	TrainAcc 0.9422	ValidAcc 0.9423	TestAcc 0.9438	BestValid 0.9423
	Epoch 1600:	Loss 0.2800	TrainAcc 0.9425	ValidAcc 0.9423	TestAcc 0.9438	BestValid 0.9423
	Epoch 1650:	Loss 0.2778	TrainAcc 0.9427	ValidAcc 0.9425	TestAcc 0.9438	BestValid 0.9425
	Epoch 1700:	Loss 0.2772	TrainAcc 0.9428	ValidAcc 0.9426	TestAcc 0.9444	BestValid 0.9426
	Epoch 1750:	Loss 0.2755	TrainAcc 0.9429	ValidAcc 0.9429	TestAcc 0.9441	BestValid 0.9429
	Epoch 1800:	Loss 0.2747	TrainAcc 0.9432	ValidAcc 0.9432	TestAcc 0.9447	BestValid 0.9432
	Epoch 1850:	Loss 0.2740	TrainAcc 0.9435	ValidAcc 0.9429	TestAcc 0.9448	BestValid 0.9432
	Epoch 1900:	Loss 0.2712	TrainAcc 0.9435	ValidAcc 0.9428	TestAcc 0.9444	BestValid 0.9432
	Epoch 1950:	Loss 0.2714	TrainAcc 0.9438	ValidAcc 0.9436	TestAcc 0.9449	BestValid 0.9436
	Epoch 2000:	Loss 0.2699	TrainAcc 0.9437	ValidAcc 0.9432	TestAcc 0.9447	BestValid 0.9436
	Epoch 2050:	Loss 0.2701	TrainAcc 0.9441	ValidAcc 0.9441	TestAcc 0.9452	BestValid 0.9441
	Epoch 2100:	Loss 0.2692	TrainAcc 0.9441	ValidAcc 0.9436	TestAcc 0.9452	BestValid 0.9441
	Epoch 2150:	Loss 0.2679	TrainAcc 0.9442	ValidAcc 0.9436	TestAcc 0.9452	BestValid 0.9441
	Epoch 2200:	Loss 0.2668	TrainAcc 0.9445	ValidAcc 0.9436	TestAcc 0.9451	BestValid 0.9441
	Epoch 2250:	Loss 0.2648	TrainAcc 0.9446	ValidAcc 0.9436	TestAcc 0.9454	BestValid 0.9441
	Epoch 2300:	Loss 0.2660	TrainAcc 0.9448	ValidAcc 0.9441	TestAcc 0.9453	BestValid 0.9441
	Epoch 2350:	Loss 0.2652	TrainAcc 0.9447	ValidAcc 0.9441	TestAcc 0.9453	BestValid 0.9441
	Epoch 2400:	Loss 0.2633	TrainAcc 0.9449	ValidAcc 0.9441	TestAcc 0.9455	BestValid 0.9441
	Epoch 2450:	Loss 0.2626	TrainAcc 0.9451	ValidAcc 0.9442	TestAcc 0.9457	BestValid 0.9442
	Epoch 2500:	Loss 0.2628	TrainAcc 0.9449	ValidAcc 0.9440	TestAcc 0.9455	BestValid 0.9442
	Epoch 2550:	Loss 0.2618	TrainAcc 0.9452	ValidAcc 0.9441	TestAcc 0.9456	BestValid 0.9442
	Epoch 2600:	Loss 0.2617	TrainAcc 0.9456	ValidAcc 0.9448	TestAcc 0.9460	BestValid 0.9448
	Epoch 2650:	Loss 0.2608	TrainAcc 0.9455	ValidAcc 0.9444	TestAcc 0.9458	BestValid 0.9448
	Epoch 2700:	Loss 0.2595	TrainAcc 0.9454	ValidAcc 0.9444	TestAcc 0.9455	BestValid 0.9448
	Epoch 2750:	Loss 0.2585	TrainAcc 0.9457	ValidAcc 0.9446	TestAcc 0.9459	BestValid 0.9448
	Epoch 2800:	Loss 0.2586	TrainAcc 0.9455	ValidAcc 0.9450	TestAcc 0.9458	BestValid 0.9450
	Epoch 2850:	Loss 0.2587	TrainAcc 0.9458	ValidAcc 0.9447	TestAcc 0.9460	BestValid 0.9450
	Epoch 2900:	Loss 0.2565	TrainAcc 0.9452	ValidAcc 0.9443	TestAcc 0.9456	BestValid 0.9450
	Epoch 2950:	Loss 0.2561	TrainAcc 0.9460	ValidAcc 0.9449	TestAcc 0.9459	BestValid 0.9450
	Epoch 3000:	Loss 0.2551	TrainAcc 0.9463	ValidAcc 0.9449	TestAcc 0.9462	BestValid 0.9450
	Epoch 3050:	Loss 0.2565	TrainAcc 0.9462	ValidAcc 0.9446	TestAcc 0.9460	BestValid 0.9450
	Epoch 3100:	Loss 0.2543	TrainAcc 0.9464	ValidAcc 0.9450	TestAcc 0.9461	BestValid 0.9450
	Epoch 3150:	Loss 0.2542	TrainAcc 0.9465	ValidAcc 0.9450	TestAcc 0.9463	BestValid 0.9450
	Epoch 3200:	Loss 0.2527	TrainAcc 0.9466	ValidAcc 0.9452	TestAcc 0.9467	BestValid 0.9452
	Epoch 3250:	Loss 0.2531	TrainAcc 0.9467	ValidAcc 0.9454	TestAcc 0.9463	BestValid 0.9454
	Epoch 3300:	Loss 0.2539	TrainAcc 0.9465	ValidAcc 0.9451	TestAcc 0.9463	BestValid 0.9454
	Epoch 3350:	Loss 0.2526	TrainAcc 0.9468	ValidAcc 0.9451	TestAcc 0.9464	BestValid 0.9454
	Epoch 3400:	Loss 0.2524	TrainAcc 0.9470	ValidAcc 0.9450	TestAcc 0.9465	BestValid 0.9454
	Epoch 3450:	Loss 0.2519	TrainAcc 0.9469	ValidAcc 0.9452	TestAcc 0.9465	BestValid 0.9454
	Epoch 3500:	Loss 0.2508	TrainAcc 0.9471	ValidAcc 0.9453	TestAcc 0.9466	BestValid 0.9454
	Epoch 3550:	Loss 0.2506	TrainAcc 0.9469	ValidAcc 0.9451	TestAcc 0.9466	BestValid 0.9454
	Epoch 3600:	Loss 0.2503	TrainAcc 0.9473	ValidAcc 0.9456	TestAcc 0.9469	BestValid 0.9456
	Epoch 3650:	Loss 0.2501	TrainAcc 0.9471	ValidAcc 0.9451	TestAcc 0.9467	BestValid 0.9456
	Epoch 3700:	Loss 0.2484	TrainAcc 0.9473	ValidAcc 0.9457	TestAcc 0.9469	BestValid 0.9457
	Epoch 3750:	Loss 0.2484	TrainAcc 0.9475	ValidAcc 0.9460	TestAcc 0.9472	BestValid 0.9460
	Epoch 3800:	Loss 0.2488	TrainAcc 0.9472	ValidAcc 0.9454	TestAcc 0.9468	BestValid 0.9460
	Epoch 3850:	Loss 0.2473	TrainAcc 0.9476	ValidAcc 0.9451	TestAcc 0.9469	BestValid 0.9460
	Epoch 3900:	Loss 0.2469	TrainAcc 0.9479	ValidAcc 0.9457	TestAcc 0.9470	BestValid 0.9460
	Epoch 3950:	Loss 0.2482	TrainAcc 0.9478	ValidAcc 0.9457	TestAcc 0.9471	BestValid 0.9460
	Epoch 4000:	Loss 0.2465	TrainAcc 0.9478	ValidAcc 0.9456	TestAcc 0.9470	BestValid 0.9460
	Epoch 4050:	Loss 0.2463	TrainAcc 0.9479	ValidAcc 0.9454	TestAcc 0.9472	BestValid 0.9460
	Epoch 4100:	Loss 0.2457	TrainAcc 0.9479	ValidAcc 0.9459	TestAcc 0.9471	BestValid 0.9460
	Epoch 4150:	Loss 0.2439	TrainAcc 0.9480	ValidAcc 0.9450	TestAcc 0.9472	BestValid 0.9460
	Epoch 4200:	Loss 0.2449	TrainAcc 0.9479	ValidAcc 0.9457	TestAcc 0.9469	BestValid 0.9460
	Epoch 4250:	Loss 0.2442	TrainAcc 0.9483	ValidAcc 0.9459	TestAcc 0.9474	BestValid 0.9460
	Epoch 4300:	Loss 0.2429	TrainAcc 0.9481	ValidAcc 0.9455	TestAcc 0.9472	BestValid 0.9460
	Epoch 4350:	Loss 0.2443	TrainAcc 0.9482	ValidAcc 0.9458	TestAcc 0.9471	BestValid 0.9460
	Epoch 4400:	Loss 0.2436	TrainAcc 0.9482	ValidAcc 0.9458	TestAcc 0.9471	BestValid 0.9460
	Epoch 4450:	Loss 0.2433	TrainAcc 0.9484	ValidAcc 0.9464	TestAcc 0.9477	BestValid 0.9464
	Epoch 4500:	Loss 0.2423	TrainAcc 0.9484	ValidAcc 0.9460	TestAcc 0.9471	BestValid 0.9464
	Epoch 4550:	Loss 0.2417	TrainAcc 0.9483	ValidAcc 0.9457	TestAcc 0.9470	BestValid 0.9464
	Epoch 4600:	Loss 0.2416	TrainAcc 0.9484	ValidAcc 0.9463	TestAcc 0.9477	BestValid 0.9464
	Epoch 4650:	Loss 0.2419	TrainAcc 0.9485	ValidAcc 0.9462	TestAcc 0.9475	BestValid 0.9464
	Epoch 4700:	Loss 0.2417	TrainAcc 0.9486	ValidAcc 0.9461	TestAcc 0.9475	BestValid 0.9464
	Epoch 4750:	Loss 0.2412	TrainAcc 0.9486	ValidAcc 0.9459	TestAcc 0.9475	BestValid 0.9464
	Epoch 4800:	Loss 0.2405	TrainAcc 0.9486	ValidAcc 0.9460	TestAcc 0.9474	BestValid 0.9464
	Epoch 4850:	Loss 0.2397	TrainAcc 0.9489	ValidAcc 0.9460	TestAcc 0.9475	BestValid 0.9464
	Epoch 4900:	Loss 0.2398	TrainAcc 0.9488	ValidAcc 0.9465	TestAcc 0.9473	BestValid 0.9465
	Epoch 4950:	Loss 0.2391	TrainAcc 0.9488	ValidAcc 0.9462	TestAcc 0.9477	BestValid 0.9465
	Epoch 5000:	Loss 0.2395	TrainAcc 0.9491	ValidAcc 0.9466	TestAcc 0.9476	BestValid 0.9466
****** Epoch Time (Excluding Evaluation Cost): 0.111 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.170 ms (Max: 0.230, Min: 0.043, Sum: 1.363)
Cluster-Wide Average, Compute: 31.605 ms (Max: 46.250, Min: 19.758, Sum: 252.838)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.010, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.014, Sum: 0.122)
Cluster-Wide Average, Communication-Graph: 76.521 ms (Max: 88.312, Min: 61.990, Sum: 612.166)
Cluster-Wide Average, Optimization: 0.474 ms (Max: 0.483, Min: 0.465, Sum: 3.793)
Cluster-Wide Average, Others: 2.151 ms (Max: 2.172, Min: 2.141, Sum: 17.211)
****** Breakdown Sum: 110.945 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.935 GB (Max: 5.169, Min: 4.885, Sum: 39.483)
Cluster-Wide Average, Graph-Level Communication Throughput: 27.489 Gbps (Max: 49.812, Min: 12.117, Sum: 219.909)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 1.810 GB
Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
Total communication (cluster-wide, per-epoch): 1.815 GB
****** Accuracy Results ******
Highest valid_acc: 0.9466
Target test_acc: 0.9476
Epoch to reach the target acc: 4999
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
