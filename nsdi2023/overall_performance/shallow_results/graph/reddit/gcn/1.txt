Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.875 seconds.
Building the CSC structure...
        It takes 1.973 seconds.
Building the CSC structure...
        It takes 2.359 seconds.
Building the CSC structure...
        It takes 2.372 seconds.
Building the CSC structure...
        It takes 2.601 seconds.
Building the CSC structure...
        It takes 2.618 seconds.
Building the CSC structure...
        It takes 2.651 seconds.
Building the CSC structure...
        It takes 2.673 seconds.
Building the CSC structure...
        It takes 1.825 seconds.
        It takes 1.905 seconds.
        It takes 2.289 seconds.
        It takes 2.285 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.233 seconds.
Building the Label Vector...
        It takes 2.319 seconds.
        It takes 2.317 seconds.
        It takes 0.032 seconds.
        It takes 2.348 seconds.
        It takes 2.330 seconds.
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.247 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.316 seconds.
Building the Label Vector...
        It takes 0.045 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
        It takes 0.298 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/reddit/8_parts
The number of GCN layers: 4
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.236 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
        It takes 0.029 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.728 Gbps (per GPU), 445.821 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.476 Gbps (per GPU), 443.810 Gbps (aggregated)
The layer-level communication performance: 55.471 Gbps (per GPU), 443.771 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.243 Gbps (per GPU), 441.944 Gbps (aggregated)
The layer-level communication performance: 55.262 Gbps (per GPU), 442.100 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.044 Gbps (per GPU), 440.355 Gbps (aggregated)
The layer-level communication performance: 55.001 Gbps (per GPU), 440.005 Gbps (aggregated)
The layer-level communication performance: 54.975 Gbps (per GPU), 439.802 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.522 Gbps (per GPU), 1276.174 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.540 Gbps (per GPU), 1276.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.528 Gbps (per GPU), 1276.222 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.513 Gbps (per GPU), 1276.101 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.528 Gbps (per GPU), 1276.222 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.537 Gbps (per GPU), 1276.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.513 Gbps (per GPU), 1276.104 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.516 Gbps (per GPU), 1276.125 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.285 Gbps (per GPU), 802.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.285 Gbps (per GPU), 802.283 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.285 Gbps (per GPU), 802.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.285 Gbps (per GPU), 802.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.285 Gbps (per GPU), 802.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.285 Gbps (per GPU), 802.283 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.285 Gbps (per GPU), 802.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.285 Gbps (per GPU), 802.283 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.469 Gbps (per GPU), 267.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.469 Gbps (per GPU), 267.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.468 Gbps (per GPU), 267.745 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.469 Gbps (per GPU), 267.752 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.468 Gbps (per GPU), 267.746 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.467 Gbps (per GPU), 267.735 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.469 Gbps (per GPU), 267.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.467 Gbps (per GPU), 267.734 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.58ms  8.96ms  8.78ms  1.09 29.12K 14.23M
 chk_1  5.25ms  4.59ms  4.30ms  1.22 29.12K  6.56M
 chk_2 16.79ms 16.22ms 15.83ms  1.06 29.12K 24.68M
 chk_3 16.80ms 16.22ms 15.95ms  1.05 29.12K 22.95M
 chk_4  5.03ms  4.42ms  4.11ms  1.22 29.12K  6.33M
 chk_5  9.02ms  8.39ms  8.18ms  1.10 29.12K 12.05M
 chk_6 10.25ms  9.55ms  9.34ms  1.10 29.12K 14.60M
 chk_7  9.52ms  8.70ms  8.36ms  1.14 29.12K 13.21M
   Avg 10.28  9.63  9.36
   Max 16.80 16.22 15.95
   Min  5.03  4.42  4.11
 Ratio  3.34  3.67  3.88
   Var 17.55 17.78 17.63
Profiling takes 2.666 s
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 20)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 20)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 20)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 20)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 20)
*** Node 2, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 20)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[0, 20)...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 7 initializing the weights for op[0, 20)...
+++++++++ Node 1 initializing the weights for op[0, 20)...
+++++++++ Node 4 initializing the weights for op[0, 20)...
+++++++++ Node 3 initializing the weights for op[0, 20)...
+++++++++ Node 5 initializing the weights for op[0, 20)...
+++++++++ Node 2 initializing the weights for op[0, 20)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7051	TrainAcc 0.1261	ValidAcc 0.1065	TestAcc 0.1081	BestValid 0.1065
	Epoch 50:	Loss 0.7433	TrainAcc 0.8676	ValidAcc 0.8793	TestAcc 0.8767	BestValid 0.8793
	Epoch 100:	Loss 0.4758	TrainAcc 0.9082	ValidAcc 0.9179	TestAcc 0.9161	BestValid 0.9179
	Epoch 150:	Loss 0.4141	TrainAcc 0.9198	ValidAcc 0.9264	TestAcc 0.9259	BestValid 0.9264
	Epoch 200:	Loss 0.3843	TrainAcc 0.9246	ValidAcc 0.9287	TestAcc 0.9301	BestValid 0.9287
	Epoch 250:	Loss 0.3679	TrainAcc 0.9276	ValidAcc 0.9313	TestAcc 0.9326	BestValid 0.9313
	Epoch 300:	Loss 0.3553	TrainAcc 0.9296	ValidAcc 0.9332	TestAcc 0.9343	BestValid 0.9332
	Epoch 350:	Loss 0.3450	TrainAcc 0.9310	ValidAcc 0.9346	TestAcc 0.9354	BestValid 0.9346
	Epoch 400:	Loss 0.3397	TrainAcc 0.9325	ValidAcc 0.9357	TestAcc 0.9369	BestValid 0.9357
	Epoch 450:	Loss 0.3343	TrainAcc 0.9335	ValidAcc 0.9363	TestAcc 0.9375	BestValid 0.9363
	Epoch 500:	Loss 0.3295	TrainAcc 0.9345	ValidAcc 0.9368	TestAcc 0.9384	BestValid 0.9368
	Epoch 550:	Loss 0.3234	TrainAcc 0.9351	ValidAcc 0.9371	TestAcc 0.9392	BestValid 0.9371
	Epoch 600:	Loss 0.3211	TrainAcc 0.9354	ValidAcc 0.9373	TestAcc 0.9393	BestValid 0.9373
	Epoch 650:	Loss 0.3163	TrainAcc 0.9365	ValidAcc 0.9387	TestAcc 0.9401	BestValid 0.9387
	Epoch 700:	Loss 0.3109	TrainAcc 0.9368	ValidAcc 0.9387	TestAcc 0.9401	BestValid 0.9387
	Epoch 750:	Loss 0.3106	TrainAcc 0.9373	ValidAcc 0.9388	TestAcc 0.9405	BestValid 0.9388
	Epoch 800:	Loss 0.3073	TrainAcc 0.9381	ValidAcc 0.9395	TestAcc 0.9409	BestValid 0.9395
	Epoch 850:	Loss 0.3044	TrainAcc 0.9383	ValidAcc 0.9392	TestAcc 0.9411	BestValid 0.9395
	Epoch 900:	Loss 0.3028	TrainAcc 0.9388	ValidAcc 0.9399	TestAcc 0.9413	BestValid 0.9399
	Epoch 950:	Loss 0.3001	TrainAcc 0.9392	ValidAcc 0.9395	TestAcc 0.9414	BestValid 0.9399
	Epoch 1000:	Loss 0.2986	TrainAcc 0.9392	ValidAcc 0.9396	TestAcc 0.9417	BestValid 0.9399
	Epoch 1050:	Loss 0.2964	TrainAcc 0.9396	ValidAcc 0.9403	TestAcc 0.9419	BestValid 0.9403
	Epoch 1100:	Loss 0.2935	TrainAcc 0.9404	ValidAcc 0.9406	TestAcc 0.9420	BestValid 0.9406
	Epoch 1150:	Loss 0.2918	TrainAcc 0.9404	ValidAcc 0.9404	TestAcc 0.9424	BestValid 0.9406
	Epoch 1200:	Loss 0.2898	TrainAcc 0.9409	ValidAcc 0.9414	TestAcc 0.9427	BestValid 0.9414
	Epoch 1250:	Loss 0.2900	TrainAcc 0.9409	ValidAcc 0.9414	TestAcc 0.9423	BestValid 0.9414
	Epoch 1300:	Loss 0.2874	TrainAcc 0.9413	ValidAcc 0.9415	TestAcc 0.9428	BestValid 0.9415
	Epoch 1350:	Loss 0.2852	TrainAcc 0.9415	ValidAcc 0.9412	TestAcc 0.9429	BestValid 0.9415
	Epoch 1400:	Loss 0.2851	TrainAcc 0.9418	ValidAcc 0.9417	TestAcc 0.9432	BestValid 0.9417
	Epoch 1450:	Loss 0.2832	TrainAcc 0.9417	ValidAcc 0.9420	TestAcc 0.9435	BestValid 0.9420
	Epoch 1500:	Loss 0.2805	TrainAcc 0.9424	ValidAcc 0.9423	TestAcc 0.9433	BestValid 0.9423
	Epoch 1550:	Loss 0.2817	TrainAcc 0.9423	ValidAcc 0.9424	TestAcc 0.9436	BestValid 0.9424
	Epoch 1600:	Loss 0.2795	TrainAcc 0.9423	ValidAcc 0.9423	TestAcc 0.9432	BestValid 0.9424
	Epoch 1650:	Loss 0.2781	TrainAcc 0.9427	ValidAcc 0.9425	TestAcc 0.9441	BestValid 0.9425
	Epoch 1700:	Loss 0.2761	TrainAcc 0.9423	ValidAcc 0.9420	TestAcc 0.9436	BestValid 0.9425
	Epoch 1750:	Loss 0.2762	TrainAcc 0.9431	ValidAcc 0.9427	TestAcc 0.9442	BestValid 0.9427
	Epoch 1800:	Loss 0.2755	TrainAcc 0.9433	ValidAcc 0.9431	TestAcc 0.9443	BestValid 0.9431
	Epoch 1850:	Loss 0.2740	TrainAcc 0.9436	ValidAcc 0.9430	TestAcc 0.9446	BestValid 0.9431
	Epoch 1900:	Loss 0.2728	TrainAcc 0.9437	ValidAcc 0.9435	TestAcc 0.9446	BestValid 0.9435
	Epoch 1950:	Loss 0.2722	TrainAcc 0.9438	ValidAcc 0.9437	TestAcc 0.9446	BestValid 0.9437
	Epoch 2000:	Loss 0.2714	TrainAcc 0.9442	ValidAcc 0.9435	TestAcc 0.9449	BestValid 0.9437
	Epoch 2050:	Loss 0.2698	TrainAcc 0.9441	ValidAcc 0.9434	TestAcc 0.9448	BestValid 0.9437
	Epoch 2100:	Loss 0.2691	TrainAcc 0.9443	ValidAcc 0.9435	TestAcc 0.9450	BestValid 0.9437
	Epoch 2150:	Loss 0.2671	TrainAcc 0.9442	ValidAcc 0.9431	TestAcc 0.9446	BestValid 0.9437
	Epoch 2200:	Loss 0.2670	TrainAcc 0.9447	ValidAcc 0.9440	TestAcc 0.9452	BestValid 0.9440
	Epoch 2250:	Loss 0.2665	TrainAcc 0.9447	ValidAcc 0.9440	TestAcc 0.9452	BestValid 0.9440
	Epoch 2300:	Loss 0.2652	TrainAcc 0.9448	ValidAcc 0.9441	TestAcc 0.9454	BestValid 0.9441
	Epoch 2350:	Loss 0.2649	TrainAcc 0.9449	ValidAcc 0.9441	TestAcc 0.9453	BestValid 0.9441
	Epoch 2400:	Loss 0.2645	TrainAcc 0.9453	ValidAcc 0.9445	TestAcc 0.9457	BestValid 0.9445
	Epoch 2450:	Loss 0.2634	TrainAcc 0.9452	ValidAcc 0.9439	TestAcc 0.9456	BestValid 0.9445
	Epoch 2500:	Loss 0.2624	TrainAcc 0.9453	ValidAcc 0.9443	TestAcc 0.9456	BestValid 0.9445
	Epoch 2550:	Loss 0.2619	TrainAcc 0.9457	ValidAcc 0.9444	TestAcc 0.9459	BestValid 0.9445
	Epoch 2600:	Loss 0.2613	TrainAcc 0.9455	ValidAcc 0.9443	TestAcc 0.9456	BestValid 0.9445
	Epoch 2650:	Loss 0.2618	TrainAcc 0.9457	ValidAcc 0.9446	TestAcc 0.9458	BestValid 0.9446
	Epoch 2700:	Loss 0.2598	TrainAcc 0.9459	ValidAcc 0.9445	TestAcc 0.9459	BestValid 0.9446
	Epoch 2750:	Loss 0.2605	TrainAcc 0.9455	ValidAcc 0.9441	TestAcc 0.9458	BestValid 0.9446
	Epoch 2800:	Loss 0.2584	TrainAcc 0.9461	ValidAcc 0.9444	TestAcc 0.9458	BestValid 0.9446
	Epoch 2850:	Loss 0.2577	TrainAcc 0.9461	ValidAcc 0.9448	TestAcc 0.9461	BestValid 0.9448
	Epoch 2900:	Loss 0.2573	TrainAcc 0.9461	ValidAcc 0.9448	TestAcc 0.9459	BestValid 0.9448
	Epoch 2950:	Loss 0.2573	TrainAcc 0.9463	ValidAcc 0.9447	TestAcc 0.9463	BestValid 0.9448
	Epoch 3000:	Loss 0.2559	TrainAcc 0.9464	ValidAcc 0.9446	TestAcc 0.9463	BestValid 0.9448
	Epoch 3050:	Loss 0.2562	TrainAcc 0.9465	ValidAcc 0.9451	TestAcc 0.9465	BestValid 0.9451
	Epoch 3100:	Loss 0.2543	TrainAcc 0.9464	ValidAcc 0.9446	TestAcc 0.9462	BestValid 0.9451
	Epoch 3150:	Loss 0.2553	TrainAcc 0.9468	ValidAcc 0.9453	TestAcc 0.9464	BestValid 0.9453
	Epoch 3200:	Loss 0.2529	TrainAcc 0.9469	ValidAcc 0.9452	TestAcc 0.9464	BestValid 0.9453
	Epoch 3250:	Loss 0.2536	TrainAcc 0.9469	ValidAcc 0.9452	TestAcc 0.9466	BestValid 0.9453
	Epoch 3300:	Loss 0.2528	TrainAcc 0.9470	ValidAcc 0.9453	TestAcc 0.9465	BestValid 0.9453
	Epoch 3350:	Loss 0.2533	TrainAcc 0.9467	ValidAcc 0.9450	TestAcc 0.9465	BestValid 0.9453
	Epoch 3400:	Loss 0.2509	TrainAcc 0.9470	ValidAcc 0.9457	TestAcc 0.9467	BestValid 0.9457
	Epoch 3450:	Loss 0.2516	TrainAcc 0.9470	ValidAcc 0.9454	TestAcc 0.9468	BestValid 0.9457
	Epoch 3500:	Loss 0.2503	TrainAcc 0.9472	ValidAcc 0.9461	TestAcc 0.9468	BestValid 0.9461
	Epoch 3550:	Loss 0.2508	TrainAcc 0.9472	ValidAcc 0.9454	TestAcc 0.9468	BestValid 0.9461
	Epoch 3600:	Loss 0.2499	TrainAcc 0.9473	ValidAcc 0.9453	TestAcc 0.9466	BestValid 0.9461
	Epoch 3650:	Loss 0.2505	TrainAcc 0.9472	ValidAcc 0.9454	TestAcc 0.9469	BestValid 0.9461
	Epoch 3700:	Loss 0.2488	TrainAcc 0.9473	ValidAcc 0.9455	TestAcc 0.9466	BestValid 0.9461
	Epoch 3750:	Loss 0.2483	TrainAcc 0.9474	ValidAcc 0.9449	TestAcc 0.9468	BestValid 0.9461
	Epoch 3800:	Loss 0.2478	TrainAcc 0.9478	ValidAcc 0.9453	TestAcc 0.9469	BestValid 0.9461
	Epoch 3850:	Loss 0.2487	TrainAcc 0.9476	ValidAcc 0.9455	TestAcc 0.9469	BestValid 0.9461
	Epoch 3900:	Loss 0.2481	TrainAcc 0.9477	ValidAcc 0.9456	TestAcc 0.9472	BestValid 0.9461
	Epoch 3950:	Loss 0.2466	TrainAcc 0.9479	ValidAcc 0.9454	TestAcc 0.9468	BestValid 0.9461
	Epoch 4000:	Loss 0.2469	TrainAcc 0.9477	ValidAcc 0.9452	TestAcc 0.9471	BestValid 0.9461
	Epoch 4050:	Loss 0.2467	TrainAcc 0.9480	ValidAcc 0.9455	TestAcc 0.9471	BestValid 0.9461
	Epoch 4100:	Loss 0.2444	TrainAcc 0.9480	ValidAcc 0.9457	TestAcc 0.9473	BestValid 0.9461
	Epoch 4150:	Loss 0.2448	TrainAcc 0.9483	ValidAcc 0.9454	TestAcc 0.9472	BestValid 0.9461
	Epoch 4200:	Loss 0.2452	TrainAcc 0.9481	ValidAcc 0.9457	TestAcc 0.9469	BestValid 0.9461
	Epoch 4250:	Loss 0.2441	TrainAcc 0.9483	ValidAcc 0.9453	TestAcc 0.9474	BestValid 0.9461
	Epoch 4300:	Loss 0.2440	TrainAcc 0.9485	ValidAcc 0.9459	TestAcc 0.9474	BestValid 0.9461
	Epoch 4350:	Loss 0.2431	TrainAcc 0.9485	ValidAcc 0.9457	TestAcc 0.9474	BestValid 0.9461
	Epoch 4400:	Loss 0.2436	TrainAcc 0.9482	ValidAcc 0.9457	TestAcc 0.9469	BestValid 0.9461
	Epoch 4450:	Loss 0.2438	TrainAcc 0.9484	ValidAcc 0.9461	TestAcc 0.9474	BestValid 0.9461
	Epoch 4500:	Loss 0.2432	TrainAcc 0.9483	ValidAcc 0.9454	TestAcc 0.9470	BestValid 0.9461
	Epoch 4550:	Loss 0.2415	TrainAcc 0.9484	ValidAcc 0.9457	TestAcc 0.9471	BestValid 0.9461
	Epoch 4600:	Loss 0.2422	TrainAcc 0.9486	ValidAcc 0.9458	TestAcc 0.9472	BestValid 0.9461
	Epoch 4650:	Loss 0.2410	TrainAcc 0.9486	ValidAcc 0.9460	TestAcc 0.9478	BestValid 0.9461
	Epoch 4700:	Loss 0.2413	TrainAcc 0.9487	ValidAcc 0.9460	TestAcc 0.9477	BestValid 0.9461
	Epoch 4750:	Loss 0.2406	TrainAcc 0.9489	ValidAcc 0.9461	TestAcc 0.9477	BestValid 0.9461
	Epoch 4800:	Loss 0.2408	TrainAcc 0.9490	ValidAcc 0.9457	TestAcc 0.9474	BestValid 0.9461
	Epoch 4850:	Loss 0.2393	TrainAcc 0.9485	ValidAcc 0.9454	TestAcc 0.9469	BestValid 0.9461
	Epoch 4900:	Loss 0.2397	TrainAcc 0.9489	ValidAcc 0.9463	TestAcc 0.9473	BestValid 0.9463
	Epoch 4950:	Loss 0.2400	TrainAcc 0.9489	ValidAcc 0.9457	TestAcc 0.9476	BestValid 0.9463
	Epoch 5000:	Loss 0.2389	TrainAcc 0.9493	ValidAcc 0.9457	TestAcc 0.9475	BestValid 0.9463
****** Epoch Time (Excluding Evaluation Cost): 0.111 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.184 ms (Max: 0.247, Min: 0.047, Sum: 1.471)
Cluster-Wide Average, Compute: 31.664 ms (Max: 46.356, Min: 19.825, Sum: 253.315)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.065)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.018, Min: 0.014, Sum: 0.129)
Cluster-Wide Average, Communication-Graph: 76.850 ms (Max: 88.628, Min: 62.273, Sum: 614.797)
Cluster-Wide Average, Optimization: 0.474 ms (Max: 0.485, Min: 0.464, Sum: 3.793)
Cluster-Wide Average, Others: 2.153 ms (Max: 2.179, Min: 2.140, Sum: 17.223)
****** Breakdown Sum: 111.349 ms ******
Cluster-Wide Average, GPU Memory Consumption: 4.935 GB (Max: 5.169, Min: 4.885, Sum: 39.483)
Cluster-Wide Average, Graph-Level Communication Throughput: 27.366 Gbps (Max: 49.588, Min: 12.075, Sum: 218.928)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 1.810 GB
Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
Total communication (cluster-wide, per-epoch): 1.815 GB
****** Accuracy Results ******
Highest valid_acc: 0.9463
Target test_acc: 0.9473
Epoch to reach the target acc: 4899
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
