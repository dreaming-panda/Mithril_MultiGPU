Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.006 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.007 seconds.
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
        It takes 0.015 seconds.
Building the CSC structure...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
        It takes 0.007 seconds.
        It takes 0.009 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCN layers: 4
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 2
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 4)
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.

5201, 401907, 401907
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.789 Gbps (per GPU), 478.313 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.465 Gbps (per GPU), 475.722 Gbps (aggregated)
The layer-level communication performance: 59.460 Gbps (per GPU), 475.679 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.215 Gbps (per GPU), 473.717 Gbps (aggregated)
The layer-level communication performance: 59.180 Gbps (per GPU), 473.442 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.976 Gbps (per GPU), 471.806 Gbps (aggregated)
The layer-level communication performance: 58.933 Gbps (per GPU), 471.466 Gbps (aggregated)
The layer-level communication performance: 58.900 Gbps (per GPU), 471.198 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.891 Gbps (per GPU), 1263.131 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.897 Gbps (per GPU), 1263.176 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.889 Gbps (per GPU), 1263.111 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.847 Gbps (per GPU), 1262.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.897 Gbps (per GPU), 1263.178 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.912 Gbps (per GPU), 1263.297 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.892 Gbps (per GPU), 1263.133 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.853 Gbps (per GPU), 1262.822 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.448 Gbps (per GPU), 811.585 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.447 Gbps (per GPU), 811.578 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.444 Gbps (per GPU), 811.552 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.446 Gbps (per GPU), 811.572 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.442 Gbps (per GPU), 811.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.449 Gbps (per GPU), 811.591 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.446 Gbps (per GPU), 811.565 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.446 Gbps (per GPU), 811.572 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.752 Gbps (per GPU), 246.018 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.751 Gbps (per GPU), 246.009 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.748 Gbps (per GPU), 245.987 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.752 Gbps (per GPU), 246.016 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.747 Gbps (per GPU), 245.976 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.747 Gbps (per GPU), 245.973 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.748 Gbps (per GPU), 245.987 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.747 Gbps (per GPU), 245.978 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.67ms  1.56ms  1.13ms  1.47  0.65K  0.22M
 chk_1  0.99ms  0.83ms  0.41ms  2.43  0.65K  0.05M
 chk_2  0.89ms  0.70ms  0.27ms  3.34  0.65K  0.02M
 chk_3  0.90ms  0.72ms  0.37ms  2.44  0.65K  0.02M
 chk_4  0.85ms  0.65ms  0.22ms  3.87  0.65K  0.01M
 chk_5  0.84ms  0.65ms  0.22ms  3.87  0.65K  0.01M
 chk_6  1.04ms  0.85ms  0.42ms  2.47  0.65K  0.06M
 chk_7  0.86ms  0.67ms  0.24ms  3.58  0.65K  0.01M
   Avg  1.00  0.83  0.41
   Max  1.67  1.56  1.13
   Min  0.84  0.65  0.22
 Ratio  1.98  2.41  5.22
   Var  0.07  0.08  0.08
Profiling takes 0.267 s
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 20)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 20)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 20)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 20)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 20)
*** Node 6, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 20)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
*** Node 3, starting model training...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 20)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 20)...
+++++++++ Node 1 initializing the weights for op[0, 20)...
+++++++++ Node 7 initializing the weights for op[0, 20)...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 2 initializing the weights for op[0, 20)...
+++++++++ Node 6 initializing the weights for op[0, 20)...
+++++++++ Node 5 initializing the weights for op[0, 20)...
+++++++++ Node 3 initializing the weights for op[0, 20)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6096	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 1.3134	TrainAcc 0.4067	ValidAcc 0.3143	TestAcc 0.3314	BestValid 0.3143
	Epoch 100:	Loss 1.1707	TrainAcc 0.5116	ValidAcc 0.3684	TestAcc 0.3698	BestValid 0.3684
	Epoch 150:	Loss 1.0420	TrainAcc 0.5649	ValidAcc 0.4099	TestAcc 0.4169	BestValid 0.4099
	Epoch 200:	Loss 0.9769	TrainAcc 0.6106	ValidAcc 0.4153	TestAcc 0.4342	BestValid 0.4153
	Epoch 250:	Loss 0.9341	TrainAcc 0.6338	ValidAcc 0.4195	TestAcc 0.4438	BestValid 0.4195
	Epoch 300:	Loss 0.9230	TrainAcc 0.5994	ValidAcc 0.4014	TestAcc 0.4063	BestValid 0.4195
	Epoch 350:	Loss 0.8899	TrainAcc 0.6546	ValidAcc 0.4327	TestAcc 0.4582	BestValid 0.4327
	Epoch 400:	Loss 0.8666	TrainAcc 0.6667	ValidAcc 0.4303	TestAcc 0.4467	BestValid 0.4327
	Epoch 450:	Loss 0.8576	TrainAcc 0.6695	ValidAcc 0.4351	TestAcc 0.4582	BestValid 0.4351
	Epoch 500:	Loss 0.8276	TrainAcc 0.6867	ValidAcc 0.4303	TestAcc 0.4592	BestValid 0.4351
	Epoch 550:	Loss 0.8474	TrainAcc 0.6831	ValidAcc 0.4423	TestAcc 0.4534	BestValid 0.4423
	Epoch 600:	Loss 0.8124	TrainAcc 0.6919	ValidAcc 0.4423	TestAcc 0.4717	BestValid 0.4423
	Epoch 650:	Loss 0.7907	TrainAcc 0.6935	ValidAcc 0.4315	TestAcc 0.4553	BestValid 0.4423
	Epoch 700:	Loss 0.7801	TrainAcc 0.6827	ValidAcc 0.4309	TestAcc 0.4467	BestValid 0.4423
	Epoch 750:	Loss 0.7743	TrainAcc 0.7099	ValidAcc 0.4471	TestAcc 0.4544	BestValid 0.4471
	Epoch 800:	Loss 0.7976	TrainAcc 0.7031	ValidAcc 0.4417	TestAcc 0.4688	BestValid 0.4471
	Epoch 850:	Loss 0.7550	TrainAcc 0.7019	ValidAcc 0.4411	TestAcc 0.4611	BestValid 0.4471
	Epoch 900:	Loss 0.7524	TrainAcc 0.7091	ValidAcc 0.4429	TestAcc 0.4524	BestValid 0.4471
	Epoch 950:	Loss 0.7325	TrainAcc 0.7268	ValidAcc 0.4537	TestAcc 0.4707	BestValid 0.4537
	Epoch 1000:	Loss 0.7315	TrainAcc 0.7179	ValidAcc 0.4513	TestAcc 0.4697	BestValid 0.4537
	Epoch 1050:	Loss 0.7659	TrainAcc 0.7352	ValidAcc 0.4585	TestAcc 0.4669	BestValid 0.4585
	Epoch 1100:	Loss 0.7148	TrainAcc 0.7272	ValidAcc 0.4471	TestAcc 0.4659	BestValid 0.4585
	Epoch 1150:	Loss 0.7134	TrainAcc 0.7288	ValidAcc 0.4471	TestAcc 0.4717	BestValid 0.4585
	Epoch 1200:	Loss 0.7030	TrainAcc 0.7448	ValidAcc 0.4681	TestAcc 0.4822	BestValid 0.4681
	Epoch 1250:	Loss 0.7107	TrainAcc 0.7388	ValidAcc 0.4615	TestAcc 0.4765	BestValid 0.4681
	Epoch 1300:	Loss 0.7309	TrainAcc 0.7436	ValidAcc 0.4694	TestAcc 0.4861	BestValid 0.4694
	Epoch 1350:	Loss 0.7138	TrainAcc 0.7444	ValidAcc 0.4609	TestAcc 0.4688	BestValid 0.4694
	Epoch 1400:	Loss 0.6902	TrainAcc 0.7520	ValidAcc 0.4675	TestAcc 0.4841	BestValid 0.4694
	Epoch 1450:	Loss 0.7091	TrainAcc 0.7143	ValidAcc 0.4465	TestAcc 0.4649	BestValid 0.4694
	Epoch 1500:	Loss 0.6959	TrainAcc 0.7628	ValidAcc 0.4730	TestAcc 0.4774	BestValid 0.4730
	Epoch 1550:	Loss 0.6874	TrainAcc 0.7548	ValidAcc 0.4579	TestAcc 0.4813	BestValid 0.4730
	Epoch 1600:	Loss 0.6692	TrainAcc 0.7488	ValidAcc 0.4627	TestAcc 0.4717	BestValid 0.4730
	Epoch 1650:	Loss 0.6624	TrainAcc 0.7516	ValidAcc 0.4657	TestAcc 0.4861	BestValid 0.4730
	Epoch 1700:	Loss 0.6614	TrainAcc 0.7628	ValidAcc 0.4700	TestAcc 0.4909	BestValid 0.4730
	Epoch 1750:	Loss 0.6526	TrainAcc 0.7632	ValidAcc 0.4760	TestAcc 0.4938	BestValid 0.4760
	Epoch 1800:	Loss 0.6500	TrainAcc 0.7660	ValidAcc 0.4621	TestAcc 0.4755	BestValid 0.4760
	Epoch 1850:	Loss 0.6548	TrainAcc 0.7396	ValidAcc 0.4694	TestAcc 0.4784	BestValid 0.4760
	Epoch 1900:	Loss 0.6434	TrainAcc 0.7688	ValidAcc 0.4796	TestAcc 0.4918	BestValid 0.4796
	Epoch 1950:	Loss 0.6493	TrainAcc 0.7632	ValidAcc 0.4742	TestAcc 0.4947	BestValid 0.4796
	Epoch 2000:	Loss 0.6561	TrainAcc 0.7744	ValidAcc 0.4778	TestAcc 0.5043	BestValid 0.4796
	Epoch 2050:	Loss 0.6959	TrainAcc 0.7776	ValidAcc 0.4796	TestAcc 0.4986	BestValid 0.4796
	Epoch 2100:	Loss 0.6367	TrainAcc 0.7672	ValidAcc 0.4645	TestAcc 0.4841	BestValid 0.4796
	Epoch 2150:	Loss 0.6353	TrainAcc 0.7873	ValidAcc 0.4796	TestAcc 0.4928	BestValid 0.4796
	Epoch 2200:	Loss 0.6318	TrainAcc 0.7760	ValidAcc 0.4700	TestAcc 0.4851	BestValid 0.4796
	Epoch 2250:	Loss 0.6760	TrainAcc 0.7640	ValidAcc 0.4688	TestAcc 0.4813	BestValid 0.4796
	Epoch 2300:	Loss 0.6102	TrainAcc 0.7680	ValidAcc 0.4742	TestAcc 0.4957	BestValid 0.4796
	Epoch 2350:	Loss 0.6202	TrainAcc 0.7869	ValidAcc 0.4742	TestAcc 0.4966	BestValid 0.4796
	Epoch 2400:	Loss 0.6420	TrainAcc 0.7768	ValidAcc 0.4766	TestAcc 0.4928	BestValid 0.4796
	Epoch 2450:	Loss 0.6029	TrainAcc 0.7800	ValidAcc 0.4772	TestAcc 0.4928	BestValid 0.4796
	Epoch 2500:	Loss 0.6164	TrainAcc 0.7873	ValidAcc 0.4724	TestAcc 0.4890	BestValid 0.4796
	Epoch 2550:	Loss 0.6161	TrainAcc 0.7756	ValidAcc 0.4796	TestAcc 0.5091	BestValid 0.4796
	Epoch 2600:	Loss 0.5996	TrainAcc 0.7893	ValidAcc 0.4880	TestAcc 0.5120	BestValid 0.4880
	Epoch 2650:	Loss 0.6163	TrainAcc 0.7973	ValidAcc 0.4832	TestAcc 0.5024	BestValid 0.4880
	Epoch 2700:	Loss 0.5855	TrainAcc 0.8017	ValidAcc 0.4904	TestAcc 0.5082	BestValid 0.4904
	Epoch 2750:	Loss 0.6023	TrainAcc 0.7772	ValidAcc 0.4700	TestAcc 0.4899	BestValid 0.4904
	Epoch 2800:	Loss 0.5896	TrainAcc 0.7941	ValidAcc 0.4874	TestAcc 0.5043	BestValid 0.4904
	Epoch 2850:	Loss 0.5912	TrainAcc 0.7800	ValidAcc 0.4742	TestAcc 0.4890	BestValid 0.4904
	Epoch 2900:	Loss 0.5947	TrainAcc 0.7804	ValidAcc 0.4796	TestAcc 0.4966	BestValid 0.4904
	Epoch 2950:	Loss 0.6088	TrainAcc 0.7957	ValidAcc 0.4886	TestAcc 0.5072	BestValid 0.4904
	Epoch 3000:	Loss 0.5999	TrainAcc 0.7977	ValidAcc 0.4832	TestAcc 0.4995	BestValid 0.4904
	Epoch 3050:	Loss 0.6054	TrainAcc 0.8013	ValidAcc 0.4844	TestAcc 0.5072	BestValid 0.4904
	Epoch 3100:	Loss 0.5703	TrainAcc 0.7925	ValidAcc 0.4706	TestAcc 0.4861	BestValid 0.4904
	Epoch 3150:	Loss 0.5803	TrainAcc 0.7969	ValidAcc 0.4904	TestAcc 0.5110	BestValid 0.4904
	Epoch 3200:	Loss 0.6266	TrainAcc 0.7861	ValidAcc 0.4796	TestAcc 0.4995	BestValid 0.4904
	Epoch 3250:	Loss 0.5763	TrainAcc 0.8105	ValidAcc 0.4934	TestAcc 0.5207	BestValid 0.4934
	Epoch 3300:	Loss 0.5971	TrainAcc 0.8029	ValidAcc 0.4922	TestAcc 0.5072	BestValid 0.4934
	Epoch 3350:	Loss 0.6318	TrainAcc 0.7881	ValidAcc 0.4874	TestAcc 0.4986	BestValid 0.4934
	Epoch 3400:	Loss 0.5838	TrainAcc 0.8037	ValidAcc 0.4904	TestAcc 0.5130	BestValid 0.4934
	Epoch 3450:	Loss 0.5549	TrainAcc 0.7949	ValidAcc 0.4766	TestAcc 0.5034	BestValid 0.4934
	Epoch 3500:	Loss 0.5586	TrainAcc 0.8033	ValidAcc 0.4910	TestAcc 0.5101	BestValid 0.4934
	Epoch 3550:	Loss 0.5440	TrainAcc 0.8089	ValidAcc 0.4892	TestAcc 0.5082	BestValid 0.4934
	Epoch 3600:	Loss 0.5469	TrainAcc 0.8061	ValidAcc 0.4910	TestAcc 0.5130	BestValid 0.4934
	Epoch 3650:	Loss 0.5792	TrainAcc 0.7921	ValidAcc 0.4736	TestAcc 0.4861	BestValid 0.4934
	Epoch 3700:	Loss 0.5636	TrainAcc 0.8021	ValidAcc 0.4832	TestAcc 0.5062	BestValid 0.4934
	Epoch 3750:	Loss 0.5625	TrainAcc 0.7993	ValidAcc 0.4862	TestAcc 0.5053	BestValid 0.4934
	Epoch 3800:	Loss 0.5588	TrainAcc 0.8065	ValidAcc 0.4898	TestAcc 0.5072	BestValid 0.4934
	Epoch 3850:	Loss 0.5697	TrainAcc 0.8181	ValidAcc 0.4880	TestAcc 0.5187	BestValid 0.4934
	Epoch 3900:	Loss 0.5354	TrainAcc 0.8161	ValidAcc 0.4958	TestAcc 0.5130	BestValid 0.4958
	Epoch 3950:	Loss 0.5448	TrainAcc 0.8161	ValidAcc 0.4916	TestAcc 0.5091	BestValid 0.4958
	Epoch 4000:	Loss 0.5240	TrainAcc 0.8289	ValidAcc 0.5006	TestAcc 0.5207	BestValid 0.5006
	Epoch 4050:	Loss 0.5448	TrainAcc 0.8125	ValidAcc 0.4814	TestAcc 0.5034	BestValid 0.5006
	Epoch 4100:	Loss 0.5357	TrainAcc 0.8257	ValidAcc 0.4976	TestAcc 0.5207	BestValid 0.5006
	Epoch 4150:	Loss 0.5401	TrainAcc 0.8097	ValidAcc 0.4748	TestAcc 0.5024	BestValid 0.5006
	Epoch 4200:	Loss 0.5807	TrainAcc 0.8205	ValidAcc 0.4946	TestAcc 0.5187	BestValid 0.5006
	Epoch 4250:	Loss 0.5530	TrainAcc 0.8113	ValidAcc 0.4952	TestAcc 0.5130	BestValid 0.5006
	Epoch 4300:	Loss 0.5717	TrainAcc 0.8017	ValidAcc 0.4850	TestAcc 0.5101	BestValid 0.5006
	Epoch 4350:	Loss 0.5111	TrainAcc 0.8225	ValidAcc 0.4946	TestAcc 0.5168	BestValid 0.5006
	Epoch 4400:	Loss 0.5370	TrainAcc 0.8249	ValidAcc 0.4904	TestAcc 0.5207	BestValid 0.5006
	Epoch 4450:	Loss 0.5582	TrainAcc 0.8093	ValidAcc 0.4916	TestAcc 0.5053	BestValid 0.5006
	Epoch 4500:	Loss 0.5266	TrainAcc 0.8133	ValidAcc 0.4844	TestAcc 0.5101	BestValid 0.5006
	Epoch 4550:	Loss 0.5174	TrainAcc 0.8313	ValidAcc 0.5024	TestAcc 0.5197	BestValid 0.5024
	Epoch 4600:	Loss 0.5128	TrainAcc 0.8249	ValidAcc 0.4982	TestAcc 0.5149	BestValid 0.5024
	Epoch 4650:	Loss 0.5315	TrainAcc 0.8061	ValidAcc 0.4928	TestAcc 0.5091	BestValid 0.5024
	Epoch 4700:	Loss 0.5561	TrainAcc 0.8049	ValidAcc 0.4880	TestAcc 0.5005	BestValid 0.5024
	Epoch 4750:	Loss 0.5473	TrainAcc 0.8217	ValidAcc 0.4970	TestAcc 0.5139	BestValid 0.5024
	Epoch 4800:	Loss 0.5269	TrainAcc 0.8029	ValidAcc 0.4862	TestAcc 0.4995	BestValid 0.5024
	Epoch 4850:	Loss 0.5607	TrainAcc 0.8205	ValidAcc 0.4904	TestAcc 0.5110	BestValid 0.5024
	Epoch 4900:	Loss 0.5126	TrainAcc 0.8181	ValidAcc 0.4874	TestAcc 0.5091	BestValid 0.5024
	Epoch 4950:	Loss 0.5099	TrainAcc 0.8297	ValidAcc 0.4892	TestAcc 0.5178	BestValid 0.5024
	Epoch 5000:	Loss 0.5054	TrainAcc 0.8213	ValidAcc 0.4946	TestAcc 0.5139	BestValid 0.5024
****** Epoch Time (Excluding Evaluation Cost): 0.024 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.051 ms (Max: 0.057, Min: 0.036, Sum: 0.407)
Cluster-Wide Average, Compute: 3.744 ms (Max: 5.176, Min: 3.405, Sum: 29.951)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.013, Sum: 0.122)
Cluster-Wide Average, Communication-Graph: 13.906 ms (Max: 14.249, Min: 12.485, Sum: 111.247)
Cluster-Wide Average, Optimization: 5.885 ms (Max: 5.908, Min: 5.874, Sum: 47.077)
Cluster-Wide Average, Others: 0.566 ms (Max: 0.569, Min: 0.562, Sum: 4.528)
****** Breakdown Sum: 24.174 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.549 GB (Max: 1.655, Min: 1.520, Sum: 12.390)
Cluster-Wide Average, Graph-Level Communication Throughput: 28.453 Gbps (Max: 47.763, Min: 10.015, Sum: 227.622)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.214 GB
Total communication (cluster-wide, per-epoch): 0.558 GB
****** Accuracy Results ******
Highest valid_acc: 0.5024
Target test_acc: 0.5197
Epoch to reach the target acc: 4549
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
