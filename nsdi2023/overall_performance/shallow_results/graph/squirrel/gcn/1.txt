Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.011 seconds.
        It takes 0.012 seconds.
        It takes 0.010 seconds.
        It takes 0.012 seconds.
Building the Feature Vector...
        It takes 0.015 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCN layers: 4
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 4)
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
5201, 401907, 401907
Number of vertices per chunk: 651
Number of vertices per chunk: 651
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.714 Gbps (per GPU), 477.715 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.424 Gbps (per GPU), 475.392 Gbps (aggregated)
The layer-level communication performance: 59.424 Gbps (per GPU), 475.392 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.162 Gbps (per GPU), 473.297 Gbps (aggregated)
The layer-level communication performance: 59.137 Gbps (per GPU), 473.093 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.910 Gbps (per GPU), 471.284 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.864 Gbps (per GPU), 470.909 Gbps (aggregated)
The layer-level communication performance: 58.832 Gbps (per GPU), 470.654 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.559 Gbps (per GPU), 1252.475 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.568 Gbps (per GPU), 1252.545 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.554 Gbps (per GPU), 1252.428 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.518 Gbps (per GPU), 1252.146 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.556 Gbps (per GPU), 1252.452 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.571 Gbps (per GPU), 1252.569 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.556 Gbps (per GPU), 1252.452 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.521 Gbps (per GPU), 1252.171 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 98.328 Gbps (per GPU), 786.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.332 Gbps (per GPU), 786.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.328 Gbps (per GPU), 786.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.331 Gbps (per GPU), 786.647 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.327 Gbps (per GPU), 786.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.330 Gbps (per GPU), 786.641 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.328 Gbps (per GPU), 786.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.309 Gbps (per GPU), 786.469 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.292 Gbps (per GPU), 266.338 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.290 Gbps (per GPU), 266.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.287 Gbps (per GPU), 266.298 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.285 Gbps (per GPU), 266.281 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.287 Gbps (per GPU), 266.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.288 Gbps (per GPU), 266.301 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.286 Gbps (per GPU), 266.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.279 Gbps (per GPU), 266.231 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.64ms  1.53ms  1.09ms  1.50  0.65K  0.22M
 chk_1  0.97ms  0.82ms  0.40ms  2.42  0.65K  0.05M
 chk_2  0.87ms  0.69ms  0.27ms  3.23  0.65K  0.02M
 chk_3  0.88ms  0.79ms  0.28ms  3.11  0.65K  0.02M
 chk_4  0.82ms  0.64ms  0.22ms  3.72  0.65K  0.01M
 chk_5  0.82ms  0.64ms  0.22ms  3.72  0.65K  0.01M
 chk_6  1.01ms  0.83ms  0.41ms  2.44  0.65K  0.06M
 chk_7  0.84ms  0.66ms  0.24ms  3.47  0.65K  0.01M
   Avg  0.98  0.83  0.39
   Max  1.64  1.53  1.09
   Min  0.82  0.64  0.22
 Ratio  2.00  2.39  4.97
   Var  0.07  0.08  0.07
Profiling takes 0.272 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 20)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 20)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 20)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 20)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 20)
*** Node 2, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
*** Node 3, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 20)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 20)...
+++++++++ Node 2 initializing the weights for op[0, 20)...
+++++++++ Node 6 initializing the weights for op[0, 20)...
+++++++++ Node 7 initializing the weights for op[0, 20)...
+++++++++ Node 4 initializing the weights for op[0, 20)...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 3 initializing the weights for op[0, 20)...
+++++++++ Node 5 initializing the weights for op[0, 20)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6105	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 1.3228	TrainAcc 0.4583	ValidAcc 0.3600	TestAcc 0.3602	BestValid 0.3600
	Epoch 100:	Loss 1.2151	TrainAcc 0.4844	ValidAcc 0.3510	TestAcc 0.3622	BestValid 0.3600
	Epoch 150:	Loss 1.0851	TrainAcc 0.5333	ValidAcc 0.3672	TestAcc 0.3919	BestValid 0.3672
	Epoch 200:	Loss 1.0575	TrainAcc 0.5813	ValidAcc 0.3996	TestAcc 0.4131	BestValid 0.3996
	Epoch 250:	Loss 0.9681	TrainAcc 0.6250	ValidAcc 0.4171	TestAcc 0.4371	BestValid 0.4171
	Epoch 300:	Loss 0.9372	TrainAcc 0.6490	ValidAcc 0.4285	TestAcc 0.4515	BestValid 0.4285
	Epoch 350:	Loss 0.9012	TrainAcc 0.6410	ValidAcc 0.4291	TestAcc 0.4380	BestValid 0.4291
	Epoch 400:	Loss 0.8714	TrainAcc 0.6538	ValidAcc 0.4243	TestAcc 0.4476	BestValid 0.4291
	Epoch 450:	Loss 0.8770	TrainAcc 0.6538	ValidAcc 0.4141	TestAcc 0.4352	BestValid 0.4291
	Epoch 500:	Loss 0.8582	TrainAcc 0.6518	ValidAcc 0.4273	TestAcc 0.4544	BestValid 0.4291
	Epoch 550:	Loss 0.8476	TrainAcc 0.6799	ValidAcc 0.4315	TestAcc 0.4573	BestValid 0.4315
	Epoch 600:	Loss 0.8248	TrainAcc 0.6683	ValidAcc 0.4267	TestAcc 0.4496	BestValid 0.4315
	Epoch 650:	Loss 0.8186	TrainAcc 0.6807	ValidAcc 0.4327	TestAcc 0.4563	BestValid 0.4327
	Epoch 700:	Loss 0.7954	TrainAcc 0.6835	ValidAcc 0.4429	TestAcc 0.4515	BestValid 0.4429
	Epoch 750:	Loss 0.8114	TrainAcc 0.6783	ValidAcc 0.4297	TestAcc 0.4428	BestValid 0.4429
	Epoch 800:	Loss 0.7845	TrainAcc 0.6767	ValidAcc 0.4405	TestAcc 0.4400	BestValid 0.4429
	Epoch 850:	Loss 0.7914	TrainAcc 0.6999	ValidAcc 0.4507	TestAcc 0.4582	BestValid 0.4507
	Epoch 900:	Loss 0.7722	TrainAcc 0.7047	ValidAcc 0.4501	TestAcc 0.4601	BestValid 0.4507
	Epoch 950:	Loss 0.7811	TrainAcc 0.7151	ValidAcc 0.4483	TestAcc 0.4669	BestValid 0.4507
	Epoch 1000:	Loss 0.7319	TrainAcc 0.7055	ValidAcc 0.4489	TestAcc 0.4563	BestValid 0.4507
	Epoch 1050:	Loss 0.7516	TrainAcc 0.7087	ValidAcc 0.4441	TestAcc 0.4630	BestValid 0.4507
	Epoch 1100:	Loss 0.7537	TrainAcc 0.7192	ValidAcc 0.4429	TestAcc 0.4784	BestValid 0.4507
	Epoch 1150:	Loss 0.7392	TrainAcc 0.7183	ValidAcc 0.4579	TestAcc 0.4678	BestValid 0.4579
	Epoch 1200:	Loss 0.7355	TrainAcc 0.7252	ValidAcc 0.4537	TestAcc 0.4659	BestValid 0.4579
	Epoch 1250:	Loss 0.7624	TrainAcc 0.7288	ValidAcc 0.4525	TestAcc 0.4707	BestValid 0.4579
	Epoch 1300:	Loss 0.7170	TrainAcc 0.7147	ValidAcc 0.4465	TestAcc 0.4659	BestValid 0.4579
	Epoch 1350:	Loss 0.7158	TrainAcc 0.7348	ValidAcc 0.4549	TestAcc 0.4813	BestValid 0.4579
	Epoch 1400:	Loss 0.7057	TrainAcc 0.7192	ValidAcc 0.4549	TestAcc 0.4726	BestValid 0.4579
	Epoch 1450:	Loss 0.7024	TrainAcc 0.7268	ValidAcc 0.4561	TestAcc 0.4793	BestValid 0.4579
	Epoch 1500:	Loss 0.7060	TrainAcc 0.7504	ValidAcc 0.4597	TestAcc 0.4803	BestValid 0.4597
	Epoch 1550:	Loss 0.6914	TrainAcc 0.7388	ValidAcc 0.4591	TestAcc 0.4745	BestValid 0.4597
	Epoch 1600:	Loss 0.6795	TrainAcc 0.7520	ValidAcc 0.4669	TestAcc 0.4784	BestValid 0.4669
	Epoch 1650:	Loss 0.6853	TrainAcc 0.7528	ValidAcc 0.4615	TestAcc 0.4803	BestValid 0.4669
	Epoch 1700:	Loss 0.7046	TrainAcc 0.7216	ValidAcc 0.4561	TestAcc 0.4918	BestValid 0.4669
	Epoch 1750:	Loss 0.6671	TrainAcc 0.7528	ValidAcc 0.4694	TestAcc 0.4745	BestValid 0.4694
	Epoch 1800:	Loss 0.6835	TrainAcc 0.7588	ValidAcc 0.4724	TestAcc 0.4813	BestValid 0.4724
	Epoch 1850:	Loss 0.6911	TrainAcc 0.7175	ValidAcc 0.4501	TestAcc 0.4659	BestValid 0.4724
	Epoch 1900:	Loss 0.6839	TrainAcc 0.7608	ValidAcc 0.4688	TestAcc 0.4755	BestValid 0.4724
	Epoch 1950:	Loss 0.6505	TrainAcc 0.7644	ValidAcc 0.4700	TestAcc 0.4928	BestValid 0.4724
	Epoch 2000:	Loss 0.6834	TrainAcc 0.7684	ValidAcc 0.4669	TestAcc 0.4947	BestValid 0.4724
	Epoch 2050:	Loss 0.6529	TrainAcc 0.7300	ValidAcc 0.4609	TestAcc 0.4717	BestValid 0.4724
	Epoch 2100:	Loss 0.6623	TrainAcc 0.7596	ValidAcc 0.4591	TestAcc 0.4870	BestValid 0.4724
	Epoch 2150:	Loss 0.6683	TrainAcc 0.7624	ValidAcc 0.4669	TestAcc 0.4928	BestValid 0.4724
	Epoch 2200:	Loss 0.6618	TrainAcc 0.7520	ValidAcc 0.4621	TestAcc 0.4947	BestValid 0.4724
	Epoch 2250:	Loss 0.6633	TrainAcc 0.7612	ValidAcc 0.4760	TestAcc 0.4717	BestValid 0.4760
	Epoch 2300:	Loss 0.6301	TrainAcc 0.7704	ValidAcc 0.4712	TestAcc 0.4957	BestValid 0.4760
	Epoch 2350:	Loss 0.6723	TrainAcc 0.7488	ValidAcc 0.4651	TestAcc 0.4880	BestValid 0.4760
	Epoch 2400:	Loss 0.6331	TrainAcc 0.7812	ValidAcc 0.4754	TestAcc 0.5062	BestValid 0.4760
	Epoch 2450:	Loss 0.6588	TrainAcc 0.7812	ValidAcc 0.4820	TestAcc 0.4966	BestValid 0.4820
	Epoch 2500:	Loss 0.6510	TrainAcc 0.7724	ValidAcc 0.4808	TestAcc 0.4986	BestValid 0.4820
	Epoch 2550:	Loss 0.6177	TrainAcc 0.7756	ValidAcc 0.4718	TestAcc 0.4841	BestValid 0.4820
	Epoch 2600:	Loss 0.6634	TrainAcc 0.7656	ValidAcc 0.4724	TestAcc 0.4918	BestValid 0.4820
	Epoch 2650:	Loss 0.6074	TrainAcc 0.7837	ValidAcc 0.4844	TestAcc 0.4947	BestValid 0.4844
	Epoch 2700:	Loss 0.6521	TrainAcc 0.7873	ValidAcc 0.4808	TestAcc 0.5024	BestValid 0.4844
	Epoch 2750:	Loss 0.6186	TrainAcc 0.7845	ValidAcc 0.4778	TestAcc 0.5130	BestValid 0.4844
	Epoch 2800:	Loss 0.6129	TrainAcc 0.7676	ValidAcc 0.4748	TestAcc 0.4918	BestValid 0.4844
	Epoch 2850:	Loss 0.5971	TrainAcc 0.7788	ValidAcc 0.4796	TestAcc 0.4966	BestValid 0.4844
	Epoch 2900:	Loss 0.6113	TrainAcc 0.7925	ValidAcc 0.4784	TestAcc 0.5072	BestValid 0.4844
	Epoch 2950:	Loss 0.5882	TrainAcc 0.7688	ValidAcc 0.4651	TestAcc 0.4899	BestValid 0.4844
	Epoch 3000:	Loss 0.5979	TrainAcc 0.7893	ValidAcc 0.4784	TestAcc 0.4995	BestValid 0.4844
	Epoch 3050:	Loss 0.6202	TrainAcc 0.7917	ValidAcc 0.4814	TestAcc 0.4966	BestValid 0.4844
	Epoch 3100:	Loss 0.6027	TrainAcc 0.8025	ValidAcc 0.4838	TestAcc 0.5101	BestValid 0.4844
	Epoch 3150:	Loss 0.6364	TrainAcc 0.7716	ValidAcc 0.4663	TestAcc 0.4861	BestValid 0.4844
	Epoch 3200:	Loss 0.5879	TrainAcc 0.7973	ValidAcc 0.4868	TestAcc 0.5062	BestValid 0.4868
	Epoch 3250:	Loss 0.5789	TrainAcc 0.7989	ValidAcc 0.4832	TestAcc 0.5053	BestValid 0.4868
	Epoch 3300:	Loss 0.5887	TrainAcc 0.7901	ValidAcc 0.4736	TestAcc 0.4947	BestValid 0.4868
	Epoch 3350:	Loss 0.5900	TrainAcc 0.7929	ValidAcc 0.4820	TestAcc 0.5034	BestValid 0.4868
	Epoch 3400:	Loss 0.5783	TrainAcc 0.7969	ValidAcc 0.4814	TestAcc 0.5005	BestValid 0.4868
	Epoch 3450:	Loss 0.5977	TrainAcc 0.7993	ValidAcc 0.4850	TestAcc 0.5091	BestValid 0.4868
	Epoch 3500:	Loss 0.5866	TrainAcc 0.8001	ValidAcc 0.4898	TestAcc 0.5043	BestValid 0.4898
	Epoch 3550:	Loss 0.6372	TrainAcc 0.8081	ValidAcc 0.4922	TestAcc 0.5053	BestValid 0.4922
	Epoch 3600:	Loss 0.5747	TrainAcc 0.7937	ValidAcc 0.4778	TestAcc 0.4976	BestValid 0.4922
	Epoch 3650:	Loss 0.5915	TrainAcc 0.7985	ValidAcc 0.4826	TestAcc 0.4976	BestValid 0.4922
	Epoch 3700:	Loss 0.5982	TrainAcc 0.8057	ValidAcc 0.4904	TestAcc 0.5130	BestValid 0.4922
	Epoch 3750:	Loss 0.5663	TrainAcc 0.7921	ValidAcc 0.4880	TestAcc 0.5014	BestValid 0.4922
	Epoch 3800:	Loss 0.5588	TrainAcc 0.8113	ValidAcc 0.4904	TestAcc 0.5091	BestValid 0.4922
	Epoch 3850:	Loss 0.5756	TrainAcc 0.7821	ValidAcc 0.4706	TestAcc 0.4851	BestValid 0.4922
	Epoch 3900:	Loss 0.5797	TrainAcc 0.7829	ValidAcc 0.4694	TestAcc 0.5014	BestValid 0.4922
	Epoch 3950:	Loss 0.5594	TrainAcc 0.8109	ValidAcc 0.4952	TestAcc 0.5091	BestValid 0.4952
	Epoch 4000:	Loss 0.5709	TrainAcc 0.8069	ValidAcc 0.4898	TestAcc 0.5149	BestValid 0.4952
	Epoch 4050:	Loss 0.5460	TrainAcc 0.8217	ValidAcc 0.4964	TestAcc 0.5226	BestValid 0.4964
	Epoch 4100:	Loss 0.5500	TrainAcc 0.8117	ValidAcc 0.4850	TestAcc 0.5139	BestValid 0.4964
	Epoch 4150:	Loss 0.5530	TrainAcc 0.8093	ValidAcc 0.4814	TestAcc 0.5082	BestValid 0.4964
	Epoch 4200:	Loss 0.5322	TrainAcc 0.8189	ValidAcc 0.5000	TestAcc 0.5159	BestValid 0.5000
	Epoch 4250:	Loss 0.6173	TrainAcc 0.8017	ValidAcc 0.4880	TestAcc 0.5120	BestValid 0.5000
	Epoch 4300:	Loss 0.5540	TrainAcc 0.8109	ValidAcc 0.4988	TestAcc 0.5207	BestValid 0.5000
	Epoch 4350:	Loss 0.6007	TrainAcc 0.8101	ValidAcc 0.4910	TestAcc 0.5149	BestValid 0.5000
	Epoch 4400:	Loss 0.5666	TrainAcc 0.8133	ValidAcc 0.4910	TestAcc 0.5207	BestValid 0.5000
	Epoch 4450:	Loss 0.5656	TrainAcc 0.8237	ValidAcc 0.4940	TestAcc 0.5159	BestValid 0.5000
	Epoch 4500:	Loss 0.5232	TrainAcc 0.8165	ValidAcc 0.4862	TestAcc 0.5101	BestValid 0.5000
	Epoch 4550:	Loss 0.5809	TrainAcc 0.8241	ValidAcc 0.4946	TestAcc 0.5130	BestValid 0.5000
	Epoch 4600:	Loss 0.5324	TrainAcc 0.8049	ValidAcc 0.4916	TestAcc 0.5168	BestValid 0.5000
	Epoch 4650:	Loss 0.5338	TrainAcc 0.8249	ValidAcc 0.4898	TestAcc 0.5264	BestValid 0.5000
	Epoch 4700:	Loss 0.6295	TrainAcc 0.7949	ValidAcc 0.4778	TestAcc 0.5005	BestValid 0.5000
	Epoch 4750:	Loss 0.5854	TrainAcc 0.7921	ValidAcc 0.4874	TestAcc 0.5082	BestValid 0.5000
	Epoch 4800:	Loss 0.5240	TrainAcc 0.8109	ValidAcc 0.4796	TestAcc 0.5091	BestValid 0.5000
	Epoch 4850:	Loss 0.5560	TrainAcc 0.8065	ValidAcc 0.4922	TestAcc 0.5043	BestValid 0.5000
	Epoch 4900:	Loss 0.5200	TrainAcc 0.8265	ValidAcc 0.4940	TestAcc 0.5207	BestValid 0.5000
	Epoch 4950:	Loss 0.5235	TrainAcc 0.8313	ValidAcc 0.4952	TestAcc 0.5264	BestValid 0.5000
	Epoch 5000:	Loss 0.5304	TrainAcc 0.8373	ValidAcc 0.4952	TestAcc 0.5216	BestValid 0.5000
****** Epoch Time (Excluding Evaluation Cost): 0.024 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.049 ms (Max: 0.057, Min: 0.034, Sum: 0.392)
Cluster-Wide Average, Compute: 3.709 ms (Max: 5.162, Min: 3.349, Sum: 29.670)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.062)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.014, Sum: 0.123)
Cluster-Wide Average, Communication-Graph: 13.922 ms (Max: 14.287, Min: 12.470, Sum: 111.377)
Cluster-Wide Average, Optimization: 5.886 ms (Max: 5.909, Min: 5.866, Sum: 47.090)
Cluster-Wide Average, Others: 0.565 ms (Max: 0.567, Min: 0.561, Sum: 4.519)
****** Breakdown Sum: 24.154 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.549 GB (Max: 1.655, Min: 1.520, Sum: 12.390)
Cluster-Wide Average, Graph-Level Communication Throughput: 28.417 Gbps (Max: 47.882, Min: 9.960, Sum: 227.340)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.214 GB
Total communication (cluster-wide, per-epoch): 0.558 GB
****** Accuracy Results ******
Highest valid_acc: 0.5000
Target test_acc: 0.5159
Epoch to reach the target acc: 4199
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
