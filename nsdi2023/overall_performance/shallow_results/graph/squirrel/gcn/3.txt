Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.006 seconds.
        It takes 0.006 seconds.
        It takes 0.007 seconds.
        It takes 0.009 seconds.
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.012 seconds.
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCN layers: 4
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 3
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.029 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
train nodes 2496, valid nodes 1664, test nodes 1041
GPU 0, layer [0, 4)
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.

5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
5201, 401907, 401907
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.050 Gbps (per GPU), 480.400 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.768 Gbps (per GPU), 478.143 Gbps (aggregated)
The layer-level communication performance: 59.754 Gbps (per GPU), 478.035 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.466 Gbps (per GPU), 475.728 Gbps (aggregated)
The layer-level communication performance: 59.501 Gbps (per GPU), 476.006 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.264 Gbps (per GPU), 474.113 Gbps (aggregated)
The layer-level communication performance: 59.220 Gbps (per GPU), 473.756 Gbps (aggregated)
The layer-level communication performance: 59.186 Gbps (per GPU), 473.491 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.607 Gbps (per GPU), 1244.855 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.618 Gbps (per GPU), 1244.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.610 Gbps (per GPU), 1244.878 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.572 Gbps (per GPU), 1244.578 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.618 Gbps (per GPU), 1244.947 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.641 Gbps (per GPU), 1245.132 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.616 Gbps (per GPU), 1244.927 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.578 Gbps (per GPU), 1244.624 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.286 Gbps (per GPU), 810.291 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.291 Gbps (per GPU), 810.330 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.286 Gbps (per GPU), 810.291 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.290 Gbps (per GPU), 810.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.290 Gbps (per GPU), 810.317 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.290 Gbps (per GPU), 810.317 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.290 Gbps (per GPU), 810.324 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.290 Gbps (per GPU), 810.318 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.897 Gbps (per GPU), 255.178 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.897 Gbps (per GPU), 255.179 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.899 Gbps (per GPU), 255.190 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.897 Gbps (per GPU), 255.177 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.892 Gbps (per GPU), 255.137 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.895 Gbps (per GPU), 255.161 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.893 Gbps (per GPU), 255.146 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.894 Gbps (per GPU), 255.150 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.68ms  1.57ms  1.13ms  1.48  0.65K  0.22M
 chk_1  0.99ms  0.84ms  0.41ms  2.40  0.65K  0.05M
 chk_2  0.89ms  0.71ms  0.27ms  3.25  0.65K  0.02M
 chk_3  0.90ms  0.73ms  0.29ms  3.12  0.65K  0.02M
 chk_4  0.85ms  0.66ms  0.23ms  3.75  0.65K  0.01M
 chk_5  0.87ms  0.66ms  0.22ms  3.88  0.65K  0.01M
 chk_6  1.05ms  0.86ms  0.43ms  2.46  0.65K  0.06M
 chk_7  0.87ms  0.68ms  0.25ms  3.49  0.65K  0.01M
   Avg  1.01  0.84  0.40
   Max  1.68  1.57  1.13
   Min  0.85  0.66  0.22
 Ratio  1.98  2.39  5.08
   Var  0.07  0.08  0.08
Profiling takes 0.269 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 20)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 20)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 3, starting model training...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 20)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 20)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 20)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 20)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 20)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 20)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 7 initializing the weights for op[0, 20)...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 2 initializing the weights for op[0, 20)...
+++++++++ Node 1 initializing the weights for op[0, 20)...
+++++++++ Node 5 initializing the weights for op[0, 20)...
+++++++++ Node 3 initializing the weights for op[0, 20)...
+++++++++ Node 6 initializing the weights for op[0, 20)...
+++++++++ Node 4 initializing the weights for op[0, 20)...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6096	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 1.3020	TrainAcc 0.4455	ValidAcc 0.3383	TestAcc 0.3583	BestValid 0.3383
	Epoch 100:	Loss 1.1439	TrainAcc 0.5321	ValidAcc 0.3810	TestAcc 0.3794	BestValid 0.3810
	Epoch 150:	Loss 1.0873	TrainAcc 0.5869	ValidAcc 0.4026	TestAcc 0.4073	BestValid 0.4026
	Epoch 200:	Loss 1.0038	TrainAcc 0.5938	ValidAcc 0.4123	TestAcc 0.4265	BestValid 0.4123
	Epoch 250:	Loss 0.9503	TrainAcc 0.6310	ValidAcc 0.4135	TestAcc 0.4457	BestValid 0.4135
	Epoch 300:	Loss 0.9089	TrainAcc 0.6454	ValidAcc 0.4243	TestAcc 0.4486	BestValid 0.4243
	Epoch 350:	Loss 0.8914	TrainAcc 0.6350	ValidAcc 0.4285	TestAcc 0.4467	BestValid 0.4285
	Epoch 400:	Loss 0.8761	TrainAcc 0.6615	ValidAcc 0.4339	TestAcc 0.4649	BestValid 0.4339
	Epoch 450:	Loss 0.8498	TrainAcc 0.6675	ValidAcc 0.4375	TestAcc 0.4592	BestValid 0.4375
	Epoch 500:	Loss 0.8835	TrainAcc 0.6635	ValidAcc 0.4315	TestAcc 0.4438	BestValid 0.4375
	Epoch 550:	Loss 0.8281	TrainAcc 0.6899	ValidAcc 0.4423	TestAcc 0.4563	BestValid 0.4423
	Epoch 600:	Loss 0.7987	TrainAcc 0.6743	ValidAcc 0.4423	TestAcc 0.4505	BestValid 0.4423
	Epoch 650:	Loss 0.8344	TrainAcc 0.6795	ValidAcc 0.4369	TestAcc 0.4467	BestValid 0.4423
	Epoch 700:	Loss 0.7925	TrainAcc 0.6931	ValidAcc 0.4447	TestAcc 0.4601	BestValid 0.4447
	Epoch 750:	Loss 0.7928	TrainAcc 0.6783	ValidAcc 0.4423	TestAcc 0.4592	BestValid 0.4447
	Epoch 800:	Loss 0.7634	TrainAcc 0.7131	ValidAcc 0.4531	TestAcc 0.4736	BestValid 0.4531
	Epoch 850:	Loss 0.7609	TrainAcc 0.7027	ValidAcc 0.4423	TestAcc 0.4630	BestValid 0.4531
	Epoch 900:	Loss 0.7535	TrainAcc 0.7135	ValidAcc 0.4579	TestAcc 0.4726	BestValid 0.4579
	Epoch 950:	Loss 0.7459	TrainAcc 0.7095	ValidAcc 0.4513	TestAcc 0.4630	BestValid 0.4579
	Epoch 1000:	Loss 0.7308	TrainAcc 0.7115	ValidAcc 0.4549	TestAcc 0.4745	BestValid 0.4579
	Epoch 1050:	Loss 0.8119	TrainAcc 0.7296	ValidAcc 0.4513	TestAcc 0.4649	BestValid 0.4579
	Epoch 1100:	Loss 0.7832	TrainAcc 0.7216	ValidAcc 0.4531	TestAcc 0.4736	BestValid 0.4579
	Epoch 1150:	Loss 0.7402	TrainAcc 0.7179	ValidAcc 0.4591	TestAcc 0.4745	BestValid 0.4591
	Epoch 1200:	Loss 0.7151	TrainAcc 0.7200	ValidAcc 0.4513	TestAcc 0.4659	BestValid 0.4591
	Epoch 1250:	Loss 0.7118	TrainAcc 0.7368	ValidAcc 0.4561	TestAcc 0.4784	BestValid 0.4591
	Epoch 1300:	Loss 0.7049	TrainAcc 0.7416	ValidAcc 0.4603	TestAcc 0.4841	BestValid 0.4603
	Epoch 1350:	Loss 0.7144	TrainAcc 0.7360	ValidAcc 0.4573	TestAcc 0.4697	BestValid 0.4603
	Epoch 1400:	Loss 0.6868	TrainAcc 0.7344	ValidAcc 0.4549	TestAcc 0.4707	BestValid 0.4603
	Epoch 1450:	Loss 0.7163	TrainAcc 0.7292	ValidAcc 0.4609	TestAcc 0.4630	BestValid 0.4609
	Epoch 1500:	Loss 0.6749	TrainAcc 0.7600	ValidAcc 0.4663	TestAcc 0.4890	BestValid 0.4663
	Epoch 1550:	Loss 0.7287	TrainAcc 0.7416	ValidAcc 0.4591	TestAcc 0.4707	BestValid 0.4663
	Epoch 1600:	Loss 0.6817	TrainAcc 0.7368	ValidAcc 0.4579	TestAcc 0.4784	BestValid 0.4663
	Epoch 1650:	Loss 0.6741	TrainAcc 0.7600	ValidAcc 0.4706	TestAcc 0.4966	BestValid 0.4706
	Epoch 1700:	Loss 0.6691	TrainAcc 0.7212	ValidAcc 0.4579	TestAcc 0.4793	BestValid 0.4706
	Epoch 1750:	Loss 0.6653	TrainAcc 0.7548	ValidAcc 0.4681	TestAcc 0.4774	BestValid 0.4706
	Epoch 1800:	Loss 0.6799	TrainAcc 0.7612	ValidAcc 0.4609	TestAcc 0.4793	BestValid 0.4706
	Epoch 1850:	Loss 0.6643	TrainAcc 0.7648	ValidAcc 0.4802	TestAcc 0.4909	BestValid 0.4802
	Epoch 1900:	Loss 0.6578	TrainAcc 0.7704	ValidAcc 0.4748	TestAcc 0.4918	BestValid 0.4802
	Epoch 1950:	Loss 0.6624	TrainAcc 0.7752	ValidAcc 0.4784	TestAcc 0.4899	BestValid 0.4802
	Epoch 2000:	Loss 0.6493	TrainAcc 0.7676	ValidAcc 0.4663	TestAcc 0.4851	BestValid 0.4802
	Epoch 2050:	Loss 0.6544	TrainAcc 0.7692	ValidAcc 0.4832	TestAcc 0.4986	BestValid 0.4832
	Epoch 2100:	Loss 0.6442	TrainAcc 0.7724	ValidAcc 0.4718	TestAcc 0.4832	BestValid 0.4832
	Epoch 2150:	Loss 0.6614	TrainAcc 0.7644	ValidAcc 0.4760	TestAcc 0.4880	BestValid 0.4832
	Epoch 2200:	Loss 0.6504	TrainAcc 0.7764	ValidAcc 0.4784	TestAcc 0.4861	BestValid 0.4832
	Epoch 2250:	Loss 0.6454	TrainAcc 0.7857	ValidAcc 0.4844	TestAcc 0.4966	BestValid 0.4844
	Epoch 2300:	Loss 0.6365	TrainAcc 0.7416	ValidAcc 0.4603	TestAcc 0.4736	BestValid 0.4844
	Epoch 2350:	Loss 0.6166	TrainAcc 0.7780	ValidAcc 0.4832	TestAcc 0.4851	BestValid 0.4844
	Epoch 2400:	Loss 0.6153	TrainAcc 0.7837	ValidAcc 0.4826	TestAcc 0.5062	BestValid 0.4844
	Epoch 2450:	Loss 0.6518	TrainAcc 0.7580	ValidAcc 0.4742	TestAcc 0.4880	BestValid 0.4844
	Epoch 2500:	Loss 0.6060	TrainAcc 0.7821	ValidAcc 0.4802	TestAcc 0.4947	BestValid 0.4844
	Epoch 2550:	Loss 0.6662	TrainAcc 0.7885	ValidAcc 0.4862	TestAcc 0.4957	BestValid 0.4862
	Epoch 2600:	Loss 0.6281	TrainAcc 0.7740	ValidAcc 0.4772	TestAcc 0.4909	BestValid 0.4862
	Epoch 2650:	Loss 0.6204	TrainAcc 0.7897	ValidAcc 0.4862	TestAcc 0.5110	BestValid 0.4862
	Epoch 2700:	Loss 0.6252	TrainAcc 0.7744	ValidAcc 0.4778	TestAcc 0.4957	BestValid 0.4862
	Epoch 2750:	Loss 0.6291	TrainAcc 0.7989	ValidAcc 0.4850	TestAcc 0.5043	BestValid 0.4862
	Epoch 2800:	Loss 0.5973	TrainAcc 0.7961	ValidAcc 0.4886	TestAcc 0.5034	BestValid 0.4886
	Epoch 2850:	Loss 0.6015	TrainAcc 0.7909	ValidAcc 0.4808	TestAcc 0.4986	BestValid 0.4886
	Epoch 2900:	Loss 0.6043	TrainAcc 0.7953	ValidAcc 0.4838	TestAcc 0.5120	BestValid 0.4886
	Epoch 2950:	Loss 0.6133	TrainAcc 0.7829	ValidAcc 0.4862	TestAcc 0.4986	BestValid 0.4886
	Epoch 3000:	Loss 0.5918	TrainAcc 0.7961	ValidAcc 0.4868	TestAcc 0.5197	BestValid 0.4886
	Epoch 3050:	Loss 0.5975	TrainAcc 0.7937	ValidAcc 0.4856	TestAcc 0.5014	BestValid 0.4886
	Epoch 3100:	Loss 0.5997	TrainAcc 0.7881	ValidAcc 0.4760	TestAcc 0.4966	BestValid 0.4886
	Epoch 3150:	Loss 0.5918	TrainAcc 0.7885	ValidAcc 0.4808	TestAcc 0.5101	BestValid 0.4886
	Epoch 3200:	Loss 0.6015	TrainAcc 0.7969	ValidAcc 0.4904	TestAcc 0.5024	BestValid 0.4904
