Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.007 seconds.
Building the CSC structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
Building the Feature Vector...
        It takes 0.009 seconds.
        It takes 0.010 seconds.
Building the Feature Vector...
        It takes 0.011 seconds.
        It takes 0.008 seconds.
        It takes 0.014 seconds.
Building the CSC structure...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.012 seconds.
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Feature Vector...
        It takes 0.017 seconds.
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/squirrel/8_parts
The number of GCNII layers: 4
The number of hidden units: 1000
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights
The random seed: 1
Number of classes: 5
Number of feature dimensions: 2089
Number of vertices: 5201
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 2496, valid nodes 1664, test nodes 1041
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
Chunks (number of global chunks: 8): 0-[0, 651) 1-[651, 1301) 2-[1301, 1951) 3-[1951, 2601) 4-[2601, 3251) 5-[3251, 3901) 6-[3901, 4551) 7-[4551, 5201)WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.

5201, 401907, 401907
5201, 401907, 401907
Number of vertices per chunk: 651
Number of vertices per chunk: 651
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
5201, 401907, 401907
Number of vertices per chunk: 651
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.800 Gbps (per GPU), 446.402 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.552 Gbps (per GPU), 444.415 Gbps (aggregated)
The layer-level communication performance: 55.545 Gbps (per GPU), 444.364 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.336 Gbps (per GPU), 442.690 Gbps (aggregated)
The layer-level communication performance: 55.308 Gbps (per GPU), 442.460 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.134 Gbps (per GPU), 441.071 Gbps (aggregated)
The layer-level communication performance: 55.092 Gbps (per GPU), 440.733 Gbps (aggregated)
The layer-level communication performance: 55.062 Gbps (per GPU), 440.495 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.790 Gbps (per GPU), 1262.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.793 Gbps (per GPU), 1262.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.787 Gbps (per GPU), 1262.299 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.773 Gbps (per GPU), 1262.180 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.773 Gbps (per GPU), 1262.180 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.787 Gbps (per GPU), 1262.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.793 Gbps (per GPU), 1262.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.778 Gbps (per GPU), 1262.228 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.683 Gbps (per GPU), 813.461 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.687 Gbps (per GPU), 813.500 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.681 Gbps (per GPU), 813.447 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.685 Gbps (per GPU), 813.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.685 Gbps (per GPU), 813.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.687 Gbps (per GPU), 813.500 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.682 Gbps (per GPU), 813.454 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.687 Gbps (per GPU), 813.500 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.522 Gbps (per GPU), 244.178 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.524 Gbps (per GPU), 244.193 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.524 Gbps (per GPU), 244.189 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.521 Gbps (per GPU), 244.171 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.519 Gbps (per GPU), 244.156 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.520 Gbps (per GPU), 244.161 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.519 Gbps (per GPU), 244.154 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.518 Gbps (per GPU), 244.145 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.63ms  2.05ms  1.24ms  2.12  0.65K  0.22M
 chk_1  2.07ms  1.32ms  0.52ms  4.01  0.65K  0.05M
 chk_2  1.90ms  1.19ms  0.37ms  5.07  0.65K  0.02M
 chk_3  1.91ms  1.20ms  0.39ms  4.89  0.65K  0.02M
 chk_4  1.85ms  1.14ms  0.33ms  5.67  0.65K  0.01M
 chk_5  1.85ms  1.13ms  0.32ms  5.79  0.65K  0.01M
 chk_6  2.06ms  1.34ms  0.53ms  3.91  0.65K  0.06M
 chk_7  1.87ms  1.16ms  0.35ms  5.35  0.65K  0.01M
   Avg  2.02  1.31  0.51
   Max  2.63  2.05  1.24
   Min  1.85  1.13  0.32
 Ratio  1.43  1.82  3.89
   Var  0.06  0.08  0.08
Profiling takes 0.417 s
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 33)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 2601, Num Local Vertices: 650
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 33)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 651
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 33)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 3251, Num Local Vertices: 650
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 33)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 651, Num Local Vertices: 650
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 33)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 4551, Num Local Vertices: 650
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 33)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1301, Num Local Vertices: 650
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 33)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 3901, Num Local Vertices: 650
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 33)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 1951, Num Local Vertices: 650
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 33)...
+++++++++ Node 1 initializing the weights for op[0, 33)...
+++++++++ Node 7 initializing the weights for op[0, 33)...
+++++++++ Node 0 initializing the weights for op[0, 33)...
+++++++++ Node 5 initializing the weights for op[0, 33)...
+++++++++ Node 2 initializing the weights for op[0, 33)...
+++++++++ Node 6 initializing the weights for op[0, 33)...
+++++++++ Node 3 initializing the weights for op[0, 33)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 11547
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6398	TrainAcc 0.2003	ValidAcc 0.2007	TestAcc 0.1979	BestValid 0.2007
	Epoch 50:	Loss 0.8100	TrainAcc 0.8578	ValidAcc 0.4609	TestAcc 0.4236	BestValid 0.4609
	Epoch 100:	Loss 0.2558	TrainAcc 0.9796	ValidAcc 0.4958	TestAcc 0.4793	BestValid 0.4958
	Epoch 150:	Loss 0.1392	TrainAcc 0.9920	ValidAcc 0.5252	TestAcc 0.4890	BestValid 0.5252
	Epoch 200:	Loss 0.1208	TrainAcc 0.9928	ValidAcc 0.5282	TestAcc 0.4928	BestValid 0.5282
	Epoch 250:	Loss 0.0847	TrainAcc 0.9960	ValidAcc 0.5433	TestAcc 0.4947	BestValid 0.5433
	Epoch 300:	Loss 0.0705	TrainAcc 0.9964	ValidAcc 0.5433	TestAcc 0.4976	BestValid 0.5433
	Epoch 350:	Loss 0.0667	TrainAcc 0.9960	ValidAcc 0.5409	TestAcc 0.5053	BestValid 0.5433
	Epoch 400:	Loss 0.0658	TrainAcc 0.9976	ValidAcc 0.5451	TestAcc 0.5043	BestValid 0.5451
	Epoch 450:	Loss 0.0533	TrainAcc 0.9976	ValidAcc 0.5493	TestAcc 0.5216	BestValid 0.5493
	Epoch 500:	Loss 0.0633	TrainAcc 0.9976	ValidAcc 0.5457	TestAcc 0.5110	BestValid 0.5493
	Epoch 550:	Loss 0.0504	TrainAcc 0.9976	ValidAcc 0.5493	TestAcc 0.5120	BestValid 0.5493
	Epoch 600:	Loss 0.0422	TrainAcc 0.9980	ValidAcc 0.5517	TestAcc 0.5159	BestValid 0.5517
	Epoch 650:	Loss 0.0395	TrainAcc 0.9976	ValidAcc 0.5565	TestAcc 0.5226	BestValid 0.5565
	Epoch 700:	Loss 0.0486	TrainAcc 0.9980	ValidAcc 0.5577	TestAcc 0.5283	BestValid 0.5577
	Epoch 750:	Loss 0.0461	TrainAcc 0.9984	ValidAcc 0.5529	TestAcc 0.5207	BestValid 0.5577
	Epoch 800:	Loss 0.0376	TrainAcc 0.9984	ValidAcc 0.5475	TestAcc 0.5178	BestValid 0.5577
	Epoch 850:	Loss 0.0474	TrainAcc 0.9984	ValidAcc 0.5517	TestAcc 0.5235	BestValid 0.5577
	Epoch 900:	Loss 0.0454	TrainAcc 0.9984	ValidAcc 0.5511	TestAcc 0.5178	BestValid 0.5577
	Epoch 950:	Loss 0.0358	TrainAcc 0.9988	ValidAcc 0.5577	TestAcc 0.5226	BestValid 0.5577
	Epoch 1000:	Loss 0.0326	TrainAcc 0.9988	ValidAcc 0.5637	TestAcc 0.5226	BestValid 0.5637
	Epoch 1050:	Loss 0.0514	TrainAcc 0.9984	ValidAcc 0.5613	TestAcc 0.5216	BestValid 0.5637
	Epoch 1100:	Loss 0.0379	TrainAcc 0.9988	ValidAcc 0.5583	TestAcc 0.5226	BestValid 0.5637
	Epoch 1150:	Loss 0.0407	TrainAcc 0.9984	ValidAcc 0.5589	TestAcc 0.5235	BestValid 0.5637
	Epoch 1200:	Loss 0.0392	TrainAcc 0.9988	ValidAcc 0.5595	TestAcc 0.5178	BestValid 0.5637
	Epoch 1250:	Loss 0.0297	TrainAcc 0.9988	ValidAcc 0.5673	TestAcc 0.5274	BestValid 0.5673
	Epoch 1300:	Loss 0.0330	TrainAcc 0.9988	ValidAcc 0.5661	TestAcc 0.5255	BestValid 0.5673
	Epoch 1350:	Loss 0.0414	TrainAcc 0.9988	ValidAcc 0.5631	TestAcc 0.5303	BestValid 0.5673
	Epoch 1400:	Loss 0.0258	TrainAcc 0.9988	ValidAcc 0.5583	TestAcc 0.5341	BestValid 0.5673
	Epoch 1450:	Loss 0.0273	TrainAcc 0.9988	ValidAcc 0.5637	TestAcc 0.5322	BestValid 0.5673
	Epoch 1500:	Loss 0.0349	TrainAcc 0.9988	ValidAcc 0.5637	TestAcc 0.5274	BestValid 0.5673
	Epoch 1550:	Loss 0.0200	TrainAcc 0.9988	ValidAcc 0.5643	TestAcc 0.5331	BestValid 0.5673
	Epoch 1600:	Loss 0.0339	TrainAcc 0.9992	ValidAcc 0.5625	TestAcc 0.5341	BestValid 0.5673
	Epoch 1650:	Loss 0.0283	TrainAcc 0.9988	ValidAcc 0.5577	TestAcc 0.5312	BestValid 0.5673
	Epoch 1700:	Loss 0.0283	TrainAcc 0.9992	ValidAcc 0.5571	TestAcc 0.5264	BestValid 0.5673
	Epoch 1750:	Loss 0.0308	TrainAcc 0.9988	ValidAcc 0.5625	TestAcc 0.5322	BestValid 0.5673
	Epoch 1800:	Loss 0.0307	TrainAcc 0.9988	ValidAcc 0.5589	TestAcc 0.5341	BestValid 0.5673
	Epoch 1850:	Loss 0.0293	TrainAcc 0.9988	ValidAcc 0.5643	TestAcc 0.5245	BestValid 0.5673
	Epoch 1900:	Loss 0.0313	TrainAcc 0.9988	ValidAcc 0.5577	TestAcc 0.5389	BestValid 0.5673
	Epoch 1950:	Loss 0.0265	TrainAcc 0.9988	ValidAcc 0.5631	TestAcc 0.5341	BestValid 0.5673
	Epoch 2000:	Loss 0.0200	TrainAcc 0.9988	ValidAcc 0.5577	TestAcc 0.5283	BestValid 0.5673
	Epoch 2050:	Loss 0.0294	TrainAcc 0.9988	ValidAcc 0.5565	TestAcc 0.5389	BestValid 0.5673
	Epoch 2100:	Loss 0.0207	TrainAcc 0.9988	ValidAcc 0.5571	TestAcc 0.5360	BestValid 0.5673
	Epoch 2150:	Loss 0.0205	TrainAcc 0.9988	ValidAcc 0.5631	TestAcc 0.5389	BestValid 0.5673
	Epoch 2200:	Loss 0.0208	TrainAcc 0.9988	ValidAcc 0.5589	TestAcc 0.5370	BestValid 0.5673
	Epoch 2250:	Loss 0.0255	TrainAcc 0.9988	ValidAcc 0.5583	TestAcc 0.5418	BestValid 0.5673
	Epoch 2300:	Loss 0.0331	TrainAcc 0.9988	ValidAcc 0.5619	TestAcc 0.5360	BestValid 0.5673
	Epoch 2350:	Loss 0.0285	TrainAcc 0.9992	ValidAcc 0.5547	TestAcc 0.5264	BestValid 0.5673
	Epoch 2400:	Loss 0.0219	TrainAcc 0.9992	ValidAcc 0.5577	TestAcc 0.5427	BestValid 0.5673
	Epoch 2450:	Loss 0.0288	TrainAcc 0.9988	ValidAcc 0.5541	TestAcc 0.5437	BestValid 0.5673
	Epoch 2500:	Loss 0.0263	TrainAcc 0.9992	ValidAcc 0.5607	TestAcc 0.5399	BestValid 0.5673
	Epoch 2550:	Loss 0.0275	TrainAcc 0.9992	ValidAcc 0.5619	TestAcc 0.5427	BestValid 0.5673
	Epoch 2600:	Loss 0.0258	TrainAcc 0.9988	ValidAcc 0.5643	TestAcc 0.5456	BestValid 0.5673
	Epoch 2650:	Loss 0.0193	TrainAcc 0.9992	ValidAcc 0.5661	TestAcc 0.5418	BestValid 0.5673
	Epoch 2700:	Loss 0.0250	TrainAcc 0.9992	ValidAcc 0.5751	TestAcc 0.5418	BestValid 0.5751
	Epoch 2750:	Loss 0.0201	TrainAcc 0.9992	ValidAcc 0.5625	TestAcc 0.5418	BestValid 0.5751
	Epoch 2800:	Loss 0.0181	TrainAcc 0.9996	ValidAcc 0.5661	TestAcc 0.5293	BestValid 0.5751
	Epoch 2850:	Loss 0.0197	TrainAcc 0.9992	ValidAcc 0.5631	TestAcc 0.5389	BestValid 0.5751
	Epoch 2900:	Loss 0.0242	TrainAcc 0.9996	ValidAcc 0.5655	TestAcc 0.5351	BestValid 0.5751
	Epoch 2950:	Loss 0.0197	TrainAcc 0.9992	ValidAcc 0.5733	TestAcc 0.5418	BestValid 0.5751
	Epoch 3000:	Loss 0.0225	TrainAcc 0.9992	ValidAcc 0.5691	TestAcc 0.5408	BestValid 0.5751
	Epoch 3050:	Loss 0.0235	TrainAcc 0.9992	ValidAcc 0.5649	TestAcc 0.5370	BestValid 0.5751
	Epoch 3100:	Loss 0.0241	TrainAcc 0.9988	ValidAcc 0.5655	TestAcc 0.5418	BestValid 0.5751
	Epoch 3150:	Loss 0.0196	TrainAcc 0.9992	ValidAcc 0.5685	TestAcc 0.5447	BestValid 0.5751
	Epoch 3200:	Loss 0.0189	TrainAcc 0.9992	ValidAcc 0.5667	TestAcc 0.5447	BestValid 0.5751
	Epoch 3250:	Loss 0.0152	TrainAcc 0.9988	ValidAcc 0.5631	TestAcc 0.5389	BestValid 0.5751
	Epoch 3300:	Loss 0.0130	TrainAcc 0.9988	ValidAcc 0.5673	TestAcc 0.5408	BestValid 0.5751
	Epoch 3350:	Loss 0.0200	TrainAcc 1.0000	ValidAcc 0.5673	TestAcc 0.5476	BestValid 0.5751
	Epoch 3400:	Loss 0.0271	TrainAcc 0.9992	ValidAcc 0.5709	TestAcc 0.5399	BestValid 0.5751
	Epoch 3450:	Loss 0.0243	TrainAcc 0.9996	ValidAcc 0.5607	TestAcc 0.5427	BestValid 0.5751
	Epoch 3500:	Loss 0.0234	TrainAcc 0.9988	ValidAcc 0.5619	TestAcc 0.5456	BestValid 0.5751
	Epoch 3550:	Loss 0.0191	TrainAcc 0.9992	ValidAcc 0.5553	TestAcc 0.5476	BestValid 0.5751
	Epoch 3600:	Loss 0.0152	TrainAcc 0.9992	ValidAcc 0.5589	TestAcc 0.5485	BestValid 0.5751
	Epoch 3650:	Loss 0.0297	TrainAcc 0.9992	ValidAcc 0.5721	TestAcc 0.5408	BestValid 0.5751
	Epoch 3700:	Loss 0.0164	TrainAcc 0.9992	ValidAcc 0.5643	TestAcc 0.5408	BestValid 0.5751
	Epoch 3750:	Loss 0.0149	TrainAcc 0.9996	ValidAcc 0.5547	TestAcc 0.5437	BestValid 0.5751
	Epoch 3800:	Loss 0.0115	TrainAcc 0.9992	ValidAcc 0.5619	TestAcc 0.5476	BestValid 0.5751
	Epoch 3850:	Loss 0.0200	TrainAcc 0.9996	ValidAcc 0.5625	TestAcc 0.5466	BestValid 0.5751
	Epoch 3900:	Loss 0.0161	TrainAcc 0.9988	ValidAcc 0.5631	TestAcc 0.5427	BestValid 0.5751
	Epoch 3950:	Loss 0.0116	TrainAcc 0.9992	ValidAcc 0.5649	TestAcc 0.5399	BestValid 0.5751
	Epoch 4000:	Loss 0.0150	TrainAcc 0.9992	ValidAcc 0.5661	TestAcc 0.5341	BestValid 0.5751
	Epoch 4050:	Loss 0.0188	TrainAcc 0.9992	ValidAcc 0.5697	TestAcc 0.5476	BestValid 0.5751
	Epoch 4100:	Loss 0.0145	TrainAcc 0.9992	ValidAcc 0.5625	TestAcc 0.5476	BestValid 0.5751
	Epoch 4150:	Loss 0.0198	TrainAcc 0.9988	ValidAcc 0.5661	TestAcc 0.5427	BestValid 0.5751
	Epoch 4200:	Loss 0.0168	TrainAcc 0.9996	ValidAcc 0.5637	TestAcc 0.5447	BestValid 0.5751
	Epoch 4250:	Loss 0.0109	TrainAcc 0.9988	ValidAcc 0.5643	TestAcc 0.5427	BestValid 0.5751
	Epoch 4300:	Loss 0.0146	TrainAcc 0.9988	ValidAcc 0.5625	TestAcc 0.5485	BestValid 0.5751
	Epoch 4350:	Loss 0.0155	TrainAcc 1.0000	ValidAcc 0.5691	TestAcc 0.5476	BestValid 0.5751
	Epoch 4400:	Loss 0.0248	TrainAcc 0.9992	ValidAcc 0.5691	TestAcc 0.5447	BestValid 0.5751
	Epoch 4450:	Loss 0.0150	TrainAcc 0.9996	ValidAcc 0.5649	TestAcc 0.5418	BestValid 0.5751
	Epoch 4500:	Loss 0.0181	TrainAcc 1.0000	ValidAcc 0.5589	TestAcc 0.5495	BestValid 0.5751
	Epoch 4550:	Loss 0.0175	TrainAcc 0.9996	ValidAcc 0.5589	TestAcc 0.5476	BestValid 0.5751
	Epoch 4600:	Loss 0.0068	TrainAcc 0.9996	ValidAcc 0.5619	TestAcc 0.5447	BestValid 0.5751
	Epoch 4650:	Loss 0.0130	TrainAcc 0.9992	ValidAcc 0.5619	TestAcc 0.5524	BestValid 0.5751
	Epoch 4700:	Loss 0.0096	TrainAcc 0.9996	ValidAcc 0.5601	TestAcc 0.5456	BestValid 0.5751
	Epoch 4750:	Loss 0.0158	TrainAcc 0.9996	ValidAcc 0.5583	TestAcc 0.5572	BestValid 0.5751
	Epoch 4800:	Loss 0.0132	TrainAcc 0.9992	ValidAcc 0.5541	TestAcc 0.5504	BestValid 0.5751
	Epoch 4850:	Loss 0.0089	TrainAcc 0.9992	ValidAcc 0.5565	TestAcc 0.5495	BestValid 0.5751
	Epoch 4900:	Loss 0.0098	TrainAcc 0.9992	ValidAcc 0.5595	TestAcc 0.5581	BestValid 0.5751
	Epoch 4950:	Loss 0.0206	TrainAcc 1.0000	ValidAcc 0.5607	TestAcc 0.5533	BestValid 0.5751
	Epoch 5000:	Loss 0.0176	TrainAcc 0.9992	ValidAcc 0.5655	TestAcc 0.5476	BestValid 0.5751
****** Epoch Time (Excluding Evaluation Cost): 0.034 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.069 ms (Max: 0.092, Min: 0.045, Sum: 0.554)
Cluster-Wide Average, Compute: 6.682 ms (Max: 9.041, Min: 6.101, Sum: 53.453)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.063)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.016, Min: 0.013, Sum: 0.119)
Cluster-Wide Average, Communication-Graph: 14.829 ms (Max: 15.439, Min: 12.461, Sum: 118.635)
Cluster-Wide Average, Optimization: 11.835 ms (Max: 11.886, Min: 11.809, Sum: 94.680)
Cluster-Wide Average, Others: 0.676 ms (Max: 0.687, Min: 0.665, Sum: 5.410)
****** Breakdown Sum: 34.114 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.734 GB (Max: 1.897, Min: 1.698, Sum: 13.876)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.864 Gbps (Max: 47.836, Min: 9.212, Sum: 214.910)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.344 GB
Weight-sync communication (cluster-wide, per-epoch): 0.427 GB
Total communication (cluster-wide, per-epoch): 0.771 GB
****** Accuracy Results ******
Highest valid_acc: 0.5751
Target test_acc: 0.5418
Epoch to reach the target acc: 2699
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
