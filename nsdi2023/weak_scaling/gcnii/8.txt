Tue Sep 19 23:28:07 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 37%   55C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 31%   49C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 32%   50C    P8    15W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 31%   49C    P8    18W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 20%] Built target context
[ 20%] Built target core
[ 70%] Built target cudahelp
[ 75%] Built target OSDI2023_MULTI_NODES_resgcn_plus
[ 87%] Built target estimate_comm_volume
[ 87%] Built target OSDI2023_MULTI_NODES_gcn
[ 87%] Built target OSDI2023_MULTI_NODES_resgcn
[ 87%] Built target OSDI2023_MULTI_NODES_graphsage
[ 87%] Built target OSDI2023_MULTI_NODES_gcnii
Running experiments...
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
Initialized node 4 on machine gnerv4
DONE MPI INIT
Initialized node 7 on machine gnerv4
DONE MPI INIT
Initialized node 6 on machine gnerv4
DONE MPI INIT
Initialized node 5 on machine gnerv4
DONE MPI INITDONE MPI INIT
Initialized node 1 on machine gnerv1
DONE MPI INIT
DONE MPI INIT
Initialized node 3 on machine gnerv1

Initialized node 2 on machine gnerv1
Initialized node 0 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.880 seconds.
Building the CSC structure...
        It takes 2.369 seconds.
Building the CSC structure...
        It takes 2.373 seconds.
Building the CSC structure...
        It takes 2.410 seconds.
Building the CSC structure...
        It takes 2.408 seconds.
Building the CSC structure...
        It takes 2.597 seconds.
Building the CSC structure...
        It takes 2.632 seconds.
Building the CSC structure...
        It takes 2.653 seconds.
Building the CSC structure...
        It takes 1.857 seconds.
        It takes 2.287 seconds.
        It takes 2.343 seconds.
Building the Feature Vector...
        It takes 2.334 seconds.
        It takes 2.373 seconds.
        It takes 2.305 seconds.
        It takes 2.381 seconds.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 2.347 seconds.
        It takes 0.040 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.252 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
GPU 0, layer [0, 65)
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.289 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.243 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.031 seconds.
        It takes 0.295 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.037 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 64
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.233 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 65)
GPU 0, layer [0, 65)
GPU 0, layer [0, 65)
GPU 0, layer [0, 65)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 65)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 65)
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 65)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.223 Gbps (per GPU), 473.786 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.951 Gbps (per GPU), 471.610 Gbps (aggregated)
The layer-level communication performance: 58.941 Gbps (per GPU), 471.530 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.707 Gbps (per GPU), 469.658 Gbps (aggregated)
The layer-level communication performance: 58.680 Gbps (per GPU), 469.437 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.471 Gbps (per GPU), 467.771 Gbps (aggregated)
The layer-level communication performance: 58.427 Gbps (per GPU), 467.417 Gbps (aggregated)
The layer-level communication performance: 58.393 Gbps (per GPU), 467.142 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.043 Gbps (per GPU), 1264.343 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.040 Gbps (per GPU), 1264.320 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.031 Gbps (per GPU), 1264.249 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.031 Gbps (per GPU), 1264.249 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.957 Gbps (per GPU), 1263.654 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.031 Gbps (per GPU), 1264.249 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.829 Gbps (per GPU), 1262.632 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.031 Gbps (per GPU), 1264.249 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.470 Gbps (per GPU), 835.762 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.463 Gbps (per GPU), 835.705 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.473 Gbps (per GPU), 835.781 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.468 Gbps (per GPU), 835.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.474 Gbps (per GPU), 835.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.472 Gbps (per GPU), 835.775 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.471 Gbps (per GPU), 835.768 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.447 Gbps (per GPU), 835.574 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 40.789 Gbps (per GPU), 326.315 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.789 Gbps (per GPU), 326.315 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.790 Gbps (per GPU), 326.317 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.789 Gbps (per GPU), 326.315 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.789 Gbps (per GPU), 326.313 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.789 Gbps (per GPU), 326.310 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.786 Gbps (per GPU), 326.289 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 40.785 Gbps (per GPU), 326.278 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.53ms  9.72ms 10.17ms  4.02 29.12K 14.23M
 chk_1  2.54ms  5.26ms  5.64ms  2.22 29.12K  6.56M
 chk_2  2.55ms 17.02ms 17.48ms  6.85 29.12K 24.68M
 chk_3  2.56ms 17.04ms 17.43ms  6.81 29.12K 22.95M
 chk_4  2.56ms  5.12ms  5.48ms  2.14 29.12K  6.33M
 chk_5  2.57ms  9.46ms  9.67ms  3.77 29.12K 12.05M
 chk_6  2.56ms 10.40ms 10.66ms  4.16 29.12K 14.60M
 chk_7  2.56ms  9.53ms 10.02ms  3.91 29.12K 13.21M
   Avg  2.55 10.44 10.82
   Max  2.57 17.04 17.48
   Min  2.53  5.12  5.48
 Ratio  1.01  3.33  3.19
   Var  0.00 18.05 18.22
Profiling takes 2.210 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 457)...
+++++++++ Node 0 initializing the weights for op[0, 457)...
+++++++++ Node 2 initializing the weights for op[0, 457)...
+++++++++ Node 4 initializing the weights for op[0, 457)...
+++++++++ Node 3 initializing the weights for op[0, 457)...
+++++++++ Node 5 initializing the weights for op[0, 457)...
+++++++++ Node 7 initializing the weights for op[0, 457)...
+++++++++ Node 6 initializing the weights for op[0, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 607420
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.8846	TrainAcc 0.1058	ValidAcc 0.1078	TestAcc 0.1068	BestValid 0.1078
	Epoch 50:	Loss 1.7474	TrainAcc 0.6866	ValidAcc 0.7069	TestAcc 0.7000	BestValid 0.7069
	Epoch 100:	Loss 1.2439	TrainAcc 0.7853	ValidAcc 0.8012	TestAcc 0.7947	BestValid 0.8012
****** Epoch Time (Excluding Evaluation Cost): 1.747 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 3.134 ms (Max: 4.957, Min: 0.037, Sum: 25.069)
Cluster-Wide Average, Compute: 461.210 ms (Max: 741.657, Min: 243.707, Sum: 3689.677)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.008, Sum: 0.067)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.017, Min: 0.014, Sum: 0.127)
Cluster-Wide Average, Communication-Graph: 1275.102 ms (Max: 1490.942, Min: 997.719, Sum: 10200.818)
Cluster-Wide Average, Optimization: 5.621 ms (Max: 5.653, Min: 5.592, Sum: 44.970)
Cluster-Wide Average, Others: 1.181 ms (Max: 1.228, Min: 1.156, Sum: 9.449)
****** Breakdown Sum: 1746.272 ms ******
Cluster-Wide Average, GPU Memory Consumption: 13.669 GB (Max: 14.741, Min: 13.499, Sum: 109.351)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.574 Gbps (Max: 49.495, Min: 11.473, Sum: 212.589)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 28.964 GB
Weight-sync communication (cluster-wide, per-epoch): 0.037 GB
Total communication (cluster-wide, per-epoch): 29.001 GB
****** Accuracy Results ******
Highest valid_acc: 0.8012
Target test_acc: 0.7947
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
Tue Sep 19 23:33:08 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 40%   60C    P0   106W / 230W |      1MiB / 24564MiB |     26%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 31%   54C    P0    95W / 230W |      1MiB / 24564MiB |     53%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 32%   54C    P0    84W / 230W |      1MiB / 24564MiB |     48%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   52C    P0    83W / 230W |      1MiB / 24564MiB |     54%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
