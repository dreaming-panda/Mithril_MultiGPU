Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2

Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
        It takes 0.014 seconds.
        It takes 0.016 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 3.093 seconds.
        It takes 3.091 seconds.
        It takes 3.090 seconds.
        It takes 3.093 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.010 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/graph_parallel_datasets/physics/4_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 4
        It takes 0.010 seconds.
        It takes 0.011 seconds.
        It takes 0.012 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 4): 0-[0, 8623) 1-[8623, 17247) 2-[17247, 25870) 3-[25870, 34493)
34493, 530417, 530417
34493, 530417, 530417
Number of vertices per chunk: 8624
Number of vertices per chunk: 8624
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 8624
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 8624
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 114.146 Gbps (per GPU), 456.585 Gbps (aggregated)
The layer-level communication performance: 114.008 Gbps (per GPU), 456.033 Gbps (aggregated)
The layer-level communication performance: 113.850 Gbps (per GPU), 455.402 Gbps (aggregated)
The layer-level communication performance: 113.815 Gbps (per GPU), 455.260 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.419 Gbps (per GPU), 633.676 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.428 Gbps (per GPU), 633.712 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.422 Gbps (per GPU), 633.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.416 Gbps (per GPU), 633.666 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.079 Gbps (per GPU), 404.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.067 Gbps (per GPU), 404.267 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.080 Gbps (per GPU), 404.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.078 Gbps (per GPU), 404.313 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.32ms  0.49ms  0.31ms 10.87  8.62K  0.20M
 chk_1  3.48ms  0.45ms  0.27ms 12.76  8.62K  0.13M
 chk_2  3.47ms  0.42ms  0.25ms 14.16  8.62K  0.09M
 chk_3  3.47ms  0.41ms  0.24ms 14.16  8.62K  0.08M
   Avg  3.44  0.44  0.27
   Max  3.48  0.49  0.31
   Min  3.32  0.41  0.24
 Ratio  1.05  1.17  1.25
   Var  0.00  0.00  0.00
Profiling takes 0.260 s
*** Node 0, starting model training...
*** Node 2, starting model training...
*** Node 1, starting model training...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 8623
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 17247, Num Local Vertices: 8623
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 8623, Num Local Vertices: 8624
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 25870, Num Local Vertices: 8623
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 19661
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6094	TrainAcc 0.2100	ValidAcc 0.4840	TestAcc 0.4910	BestValid 0.4840
	Epoch 50:	Loss 1.6094	TrainAcc 0.2000	ValidAcc 0.0840	TestAcc 0.0920	BestValid 0.4840
****** Epoch Time (Excluding Evaluation Cost): 0.044 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.085 ms (Max: 0.133, Min: 0.030, Sum: 0.340)
Cluster-Wide Average, Compute: 18.063 ms (Max: 18.871, Min: 17.348, Sum: 72.254)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.008, Min: 0.007, Sum: 0.031)
Cluster-Wide Average, Bubble-Imbalance: 0.014 ms (Max: 0.015, Min: 0.013, Sum: 0.056)
Cluster-Wide Average, Communication-Graph: 23.844 ms (Max: 24.539, Min: 23.040, Sum: 95.377)
Cluster-Wide Average, Optimization: 1.997 ms (Max: 2.003, Min: 1.987, Sum: 7.987)
Cluster-Wide Average, Others: 0.367 ms (Max: 0.381, Min: 0.338, Sum: 1.467)
****** Breakdown Sum: 44.378 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.779 GB (Max: 4.016, Min: 3.674, Sum: 15.118)
Cluster-Wide Average, Graph-Level Communication Throughput: 47.267 Gbps (Max: 69.373, Min: 19.617, Sum: 189.068)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.469 GB
Weight-sync communication (cluster-wide, per-epoch): 0.026 GB
Total communication (cluster-wide, per-epoch): 0.494 GB
****** Accuracy Results ******
Highest valid_acc: 0.4840
Target test_acc: 0.4910
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
