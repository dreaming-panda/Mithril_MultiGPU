g006.anvil.rcac.purdue.edu
Wed May 17 22:55:48 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   34C    P0    55W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[ 11%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 83%] Built target estimate_comm_volume
[ 91%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target OSDI2023_MULTI_NODES_gcn
Initialized node 0 on machine g006.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 0.073 seconds.
Building the CSC structure...
        It takes 0.038 seconds.
Building the Feature Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCNII layers: 8
The number of hidden units: 16
The number of training epoches: 100
Learning rate: 0.010000
The partition strategy: model
The dropout rate: 0.000
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 59
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.000000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 1
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 10)
*** Node 0, starting model training...
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 65)
*** Node 0, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_DROPOUT, output tensors: 1
    Op 2: type OPERATOR_WEIGHT, output tensors: 2
    Op 3: type OPERATOR_MATMUL, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_ADD, output tensors: 7
    Op 8: type OPERATOR_WEIGHT, output tensors: 8
    Op 9: type OPERATOR_MATMUL, output tensors: 9
    Op 10: type OPERATOR_ADD, output tensors: 10
    Op 11: type OPERATOR_RELU, output tensors: 11
    Op 12: type OPERATOR_DROPOUT, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_ADD, output tensors: 14
    Op 15: type OPERATOR_WEIGHT, output tensors: 15
    Op 16: type OPERATOR_MATMUL, output tensors: 16
    Op 17: type OPERATOR_ADD, output tensors: 17
    Op 18: type OPERATOR_RELU, output tensors: 18
    Op 19: type OPERATOR_DROPOUT, output tensors: 19
    Op 20: type OPERATOR_AGGREGATION, output tensors: 20
    Op 21: type OPERATOR_ADD, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_ADD, output tensors: 24
    Op 25: type OPERATOR_RELU, output tensors: 25
    Op 26: type OPERATOR_DROPOUT, output tensors: 26
    Op 27: type OPERATOR_AGGREGATION, output tensors: 27
    Op 28: type OPERATOR_ADD, output tensors: 28
    Op 29: type OPERATOR_WEIGHT, output tensors: 29
    Op 30: type OPERATOR_MATMUL, output tensors: 30
    Op 31: type OPERATOR_ADD, output tensors: 31
    Op 32: type OPERATOR_RELU, output tensors: 32
    Op 33: type OPERATOR_DROPOUT, output tensors: 33
    Op 34: type OPERATOR_AGGREGATION, output tensors: 34
    Op 35: type OPERATOR_ADD, output tensors: 35
    Op 36: type OPERATOR_WEIGHT, output tensors: 36
    Op 37: type OPERATOR_MATMUL, output tensors: 37
    Op 38: type OPERATOR_ADD, output tensors: 38
    Op 39: type OPERATOR_RELU, output tensors: 39
    Op 40: type OPERATOR_DROPOUT, output tensors: 40
    Op 41: type OPERATOR_AGGREGATION, output tensors: 41
    Op 42: type OPERATOR_ADD, output tensors: 42
    Op 43: type OPERATOR_WEIGHT, output tensors: 43
    Op 44: type OPERATOR_MATMUL, output tensors: 44
    Op 45: type OPERATOR_ADD, output tensors: 45
    Op 46: type OPERATOR_RELU, output tensors: 46
    Op 47: type OPERATOR_DROPOUT, output tensors: 47
    Op 48: type OPERATOR_AGGREGATION, output tensors: 48
    Op 49: type OPERATOR_ADD, output tensors: 49
    Op 50: type OPERATOR_WEIGHT, output tensors: 50
    Op 51: type OPERATOR_MATMUL, output tensors: 51
    Op 52: type OPERATOR_ADD, output tensors: 52
    Op 53: type OPERATOR_RELU, output tensors: 53
    Op 54: type OPERATOR_DROPOUT, output tensors: 54
    Op 55: type OPERATOR_AGGREGATION, output tensors: 55
    Op 56: type OPERATOR_ADD, output tensors: 56
    Op 57: type OPERATOR_WEIGHT, output tensors: 57
    Op 58: type OPERATOR_MATMUL, output tensors: 58
    Op 59: type OPERATOR_ADD, output tensors: 59
    Op 60: type OPERATOR_RELU, output tensors: 60
    Op 61: type OPERATOR_DROPOUT, output tensors: 61
    Op 62: type OPERATOR_WEIGHT, output tensors: 62
    Op 63: type OPERATOR_MATMUL, output tensors: 63
    Op 64: type OPERATOR_SOFTMAX, output tensors: 64
Boundaries: 0 169343
Fragments: [0, 169343)
Chunks (number of global chunks: 16): 0-[0, 10584) 1-[10584, 21168) 2-[21168, 31752) 3-[31752, 42336) 4-[42336, 52920) 5-[52920, 63504) 6-[63504, 74088) 7-[74088, 84672) 8-[84672, 95256) ... 15-[158760, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 10584
csr in-out ready !*** Node 0, setting up some other necessary information...
*** Node 0, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 65)...
+++++++++ Node 0, mapping weight op 2
+++++++++ Node 0, mapping weight op 8
+++++++++ Node 0, mapping weight op 15
+++++++++ Node 0, mapping weight op 22
+++++++++ Node 0, mapping weight op 29
+++++++++ Node 0, mapping weight op 36
+++++++++ Node 0, mapping weight op 43
+++++++++ Node 0, mapping weight op 50
+++++++++ Node 0, mapping weight op 57
+++++++++ Node 0, mapping weight op 62
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.010000000
	Epoch 1:	Loss 3.7094	TrainAcc 0.0076	ValidAcc 0.0040	BestValid 0.0040
	Epoch 2:	Loss 3.6723	TrainAcc 0.1723	ValidAcc 0.0737	BestValid 0.0737
	Epoch 3:	Loss 3.6229	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 4:	Loss 3.5615	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 5:	Loss 3.4866	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 6:	Loss 3.4022	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 7:	Loss 3.3267	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 8:	Loss 3.2748	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 9:	Loss 3.2419	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 10:	Loss 3.2327	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 11:	Loss 3.2349	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 12:	Loss 3.2182	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 13:	Loss 3.1978	TrainAcc 0.1799	ValidAcc 0.0768	BestValid 0.0768
	Epoch 14:	Loss 3.1790	TrainAcc 0.1890	ValidAcc 0.0908	BestValid 0.0908
	Epoch 15:	Loss 3.1537	TrainAcc 0.1851	ValidAcc 0.0846	BestValid 0.0908
	Epoch 16:	Loss 3.1268	TrainAcc 0.1798	ValidAcc 0.0769	BestValid 0.0908
	Epoch 17:	Loss 3.0978	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0908
	Epoch 18:	Loss 3.0751	TrainAcc 0.1793	ValidAcc 0.0763	BestValid 0.0908
	Epoch 19:	Loss 3.0475	TrainAcc 0.1802	ValidAcc 0.0767	BestValid 0.0908
	Epoch 20:	Loss 2.9628	TrainAcc 0.1868	ValidAcc 0.0790	BestValid 0.0908
	Epoch 21:	Loss 2.8991	TrainAcc 0.2148	ValidAcc 0.1077	BestValid 0.1077
	Epoch 22:	Loss 2.9027	TrainAcc 0.2904	ValidAcc 0.2754	BestValid 0.2754
	Epoch 23:	Loss 2.8705	TrainAcc 0.3321	ValidAcc 0.3543	BestValid 0.3543
	Epoch 24:	Loss 2.8535	TrainAcc 0.3139	ValidAcc 0.3182	BestValid 0.3543
	Epoch 25:	Loss 2.8195	TrainAcc 0.3118	ValidAcc 0.3193	BestValid 0.3543
	Epoch 26:	Loss 2.7870	TrainAcc 0.3109	ValidAcc 0.3192	BestValid 0.3543
	Epoch 27:	Loss 2.7551	TrainAcc 0.3108	ValidAcc 0.3186	BestValid 0.3543
	Epoch 28:	Loss 2.7300	TrainAcc 0.3142	ValidAcc 0.3201	BestValid 0.3543
	Epoch 29:	Loss 2.7023	TrainAcc 0.3245	ValidAcc 0.3240	BestValid 0.3543
	Epoch 30:	Loss 2.6591	TrainAcc 0.3384	ValidAcc 0.3298	BestValid 0.3543
	Epoch 31:	Loss 2.6252	TrainAcc 0.3508	ValidAcc 0.3365	BestValid 0.3543
	Epoch 32:	Loss 2.5951	TrainAcc 0.3594	ValidAcc 0.3427	BestValid 0.3543
	Epoch 33:	Loss 2.5604	TrainAcc 0.3665	ValidAcc 0.3476	BestValid 0.3543
	Epoch 34:	Loss 2.5286	TrainAcc 0.3731	ValidAcc 0.3530	BestValid 0.3543
	Epoch 35:	Loss 2.5017	TrainAcc 0.3786	ValidAcc 0.3606	BestValid 0.3606
	Epoch 36:	Loss 2.4682	TrainAcc 0.3832	ValidAcc 0.3674	BestValid 0.3674
	Epoch 37:	Loss 2.4288	TrainAcc 0.3894	ValidAcc 0.3737	BestValid 0.3737
	Epoch 38:	Loss 2.4037	TrainAcc 0.3960	ValidAcc 0.3821	BestValid 0.3821
	Epoch 39:	Loss 2.3657	TrainAcc 0.4030	ValidAcc 0.3909	BestValid 0.3909
	Epoch 40:	Loss 2.2597	TrainAcc 0.4110	ValidAcc 0.3970	BestValid 0.3970
	Epoch 41:	Loss 2.1992	TrainAcc 0.4219	ValidAcc 0.4063	BestValid 0.4063
	Epoch 42:	Loss 2.1976	TrainAcc 0.4359	ValidAcc 0.4214	BestValid 0.4214
	Epoch 43:	Loss 2.1804	TrainAcc 0.4510	ValidAcc 0.4414	BestValid 0.4414
	Epoch 44:	Loss 2.1499	TrainAcc 0.4605	ValidAcc 0.4570	BestValid 0.4570
	Epoch 45:	Loss 2.1280	TrainAcc 0.4670	ValidAcc 0.4639	BestValid 0.4639
	Epoch 46:	Loss 2.1003	TrainAcc 0.4684	ValidAcc 0.4650	BestValid 0.4650
	Epoch 47:	Loss 2.0827	TrainAcc 0.4690	ValidAcc 0.4667	BestValid 0.4667
	Epoch 48:	Loss 2.0577	TrainAcc 0.4711	ValidAcc 0.4691	BestValid 0.4691
	Epoch 49:	Loss 2.0348	TrainAcc 0.4768	ValidAcc 0.4727	BestValid 0.4727
	Epoch 50:	Loss 2.0175	TrainAcc 0.4843	ValidAcc 0.4791	BestValid 0.4791
	Epoch 51:	Loss 1.9966	TrainAcc 0.4927	ValidAcc 0.4855	BestValid 0.4855
	Epoch 52:	Loss 1.9824	TrainAcc 0.5017	ValidAcc 0.4920	BestValid 0.4920
	Epoch 53:	Loss 1.9638	TrainAcc 0.5078	ValidAcc 0.4997	BestValid 0.4997
	Epoch 54:	Loss 1.9398	TrainAcc 0.5118	ValidAcc 0.5043	BestValid 0.5043
	Epoch 55:	Loss 1.9329	TrainAcc 0.5160	ValidAcc 0.5079	BestValid 0.5079
	Epoch 56:	Loss 1.9106	TrainAcc 0.5206	ValidAcc 0.5097	BestValid 0.5097
	Epoch 57:	Loss 1.8978	TrainAcc 0.5242	ValidAcc 0.5119	BestValid 0.5119
	Epoch 58:	Loss 1.8804	TrainAcc 0.5269	ValidAcc 0.5124	BestValid 0.5124
	Epoch 59:	Loss 1.8699	TrainAcc 0.5293	ValidAcc 0.5141	BestValid 0.5141
	Epoch 60:	Loss 1.8176	TrainAcc 0.5339	ValidAcc 0.5213	BestValid 0.5213
	Epoch 61:	Loss 1.7955	TrainAcc 0.5343	ValidAcc 0.5334	BestValid 0.5334
	Epoch 62:	Loss 1.7896	TrainAcc 0.5300	ValidAcc 0.5439	BestValid 0.5439
	Epoch 63:	Loss 1.7821	TrainAcc 0.5315	ValidAcc 0.5474	BestValid 0.5474
	Epoch 64:	Loss 1.7737	TrainAcc 0.5390	ValidAcc 0.5470	BestValid 0.5474
	Epoch 65:	Loss 1.7571	TrainAcc 0.5474	ValidAcc 0.5449	BestValid 0.5474
	Epoch 66:	Loss 1.7440	TrainAcc 0.5516	ValidAcc 0.5422	BestValid 0.5474
	Epoch 67:	Loss 1.7361	TrainAcc 0.5530	ValidAcc 0.5392	BestValid 0.5474
	Epoch 68:	Loss 1.7293	TrainAcc 0.5542	ValidAcc 0.5388	BestValid 0.5474
	Epoch 69:	Loss 1.7174	TrainAcc 0.5564	ValidAcc 0.5424	BestValid 0.5474
	Epoch 70:	Loss 1.7082	TrainAcc 0.5583	ValidAcc 0.5473	BestValid 0.5474
	Epoch 71:	Loss 1.7013	TrainAcc 0.5599	ValidAcc 0.5512	BestValid 0.5512
	Epoch 72:	Loss 1.6911	TrainAcc 0.5621	ValidAcc 0.5545	BestValid 0.5545
	Epoch 73:	Loss 1.6843	TrainAcc 0.5648	ValidAcc 0.5590	BestValid 0.5590
	Epoch 74:	Loss 1.6749	TrainAcc 0.5680	ValidAcc 0.5636	BestValid 0.5636
	Epoch 75:	Loss 1.6691	TrainAcc 0.5706	ValidAcc 0.5663	BestValid 0.5663
	Epoch 76:	Loss 1.6624	TrainAcc 0.5726	ValidAcc 0.5658	BestValid 0.5663
	Epoch 77:	Loss 1.6547	TrainAcc 0.5726	ValidAcc 0.5630	BestValid 0.5663
	Epoch 78:	Loss 1.6449	TrainAcc 0.5724	ValidAcc 0.5605	BestValid 0.5663
	Epoch 79:	Loss 1.6376	TrainAcc 0.5730	ValidAcc 0.5592	BestValid 0.5663
	Epoch 80:	Loss 1.6145	TrainAcc 0.5746	ValidAcc 0.5627	BestValid 0.5663
	Epoch 81:	Loss 1.5987	TrainAcc 0.5762	ValidAcc 0.5776	BestValid 0.5776
	Epoch 82:	Loss 1.5991	TrainAcc 0.5787	ValidAcc 0.5895	BestValid 0.5895
	Epoch 83:	Loss 1.5938	TrainAcc 0.5813	ValidAcc 0.5941	BestValid 0.5941
	Epoch 84:	Loss 1.5870	TrainAcc 0.5834	ValidAcc 0.5937	BestValid 0.5941
	Epoch 85:	Loss 1.5824	TrainAcc 0.5851	ValidAcc 0.5911	BestValid 0.5941
	Epoch 86:	Loss 1.5744	TrainAcc 0.5864	ValidAcc 0.5854	BestValid 0.5941
	Epoch 87:	Loss 1.5689	TrainAcc 0.5869	ValidAcc 0.5791	BestValid 0.5941
	Epoch 88:	Loss 1.5631	TrainAcc 0.5865	ValidAcc 0.5761	BestValid 0.5941
	Epoch 89:	Loss 1.5589	TrainAcc 0.5865	ValidAcc 0.5767	BestValid 0.5941
	Epoch 90:	Loss 1.5549	TrainAcc 0.5880	ValidAcc 0.5815	BestValid 0.5941
	Epoch 91:	Loss 1.5483	TrainAcc 0.5906	ValidAcc 0.5879	BestValid 0.5941
	Epoch 92:	Loss 1.5443	TrainAcc 0.5927	ValidAcc 0.5930	BestValid 0.5941
	Epoch 93:	Loss 1.5390	TrainAcc 0.5940	ValidAcc 0.5964	BestValid 0.5964
	Epoch 94:	Loss 1.5337	TrainAcc 0.5951	ValidAcc 0.5973	BestValid 0.5973
	Epoch 95:	Loss 1.5300	TrainAcc 0.5959	ValidAcc 0.5957	BestValid 0.5973
	Epoch 96:	Loss 1.5256	TrainAcc 0.5966	ValidAcc 0.5934	BestValid 0.5973
	Epoch 97:	Loss 1.5207	TrainAcc 0.5969	ValidAcc 0.5928	BestValid 0.5973
	Epoch 98:	Loss 1.5184	TrainAcc 0.5972	ValidAcc 0.5914	BestValid 0.5973
	Epoch 99:	Loss 1.5126	TrainAcc 0.5977	ValidAcc 0.5920	BestValid 0.5973
Node 0, Layer-level comm throughput (grad): -nan GBps
Node 0, Layer-level comm throughput (act): -nan GBps
	Epoch 100:	Loss 1.4987	TrainAcc 0.5990	ValidAcc 0.5945	BestValid 0.5973
Node 0, GPU memory consumption: 3.730 GB
Node 0, compression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, decompression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, pure compute time: 13.665 s, total compute time: 13.665 s
Node 0, wait_for_task_time: 0.001 s, wait_for_other_gpus_time: 0.000 s
------------------------node id 0,  per-epoch time: 0.144846 s---------------
************ Profiling Results ************
	Bubble: 0.563385 (s) (3.61 percentage)
	Compute: 14.999513 (s) (96.14 percentage)
	GradSync: 0.031782 (s) (0.20 percentage)
	GraphComm: 0.007166 (s) (0.05 percentage)
	Imbalance: 0.000250 (s) (0.00 percentage)
	LayerComm: 0.000000 (s) (0.00 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.000 GB
Highest valid_acc: 0.5973
Target test_acc: 0.5870
Epoch to reach the target acc: 94
[MPI Rank 0] Success 
