g010.anvil.rcac.purdue.edu
Sat Jun  3 22:53:29 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   36C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2       9) zlib/1.2.11
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0       10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0      11) openmpi/4.0.6
  4) gmp/6.2.1              8) numactl/2.0.14  12) boost/1.74.0

 

Consolidate compiler generated dependencies of target cudahelp
[ 11%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 83%] Built target estimate_comm_volume
[ 88%] Built target OSDI2023_MULTI_NODES_gcnii
[ 94%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target OSDI2023_MULTI_NODES_gcn
Initialized node 2 on machine g012.anvil.rcac.purdue.edu
Initialized node 3 on machine g013.anvil.rcac.purdue.edu
Initialized node 1 on machine g011.anvil.rcac.purdue.edu
Initialized node 0 on machine g010.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.950 seconds.
Building the CSC structure...
        It takes 1.963 seconds.
Building the CSC structure...
        It takes 1.991 seconds.
Building the CSC structure...
        It takes 2.394 seconds.
Building the CSC structure...
        It takes 1.918 seconds.
        It takes 1.929 seconds.
        It takes 1.957 seconds.
        It takes 2.318 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.633 seconds.
Building the Label Vector...
        It takes 0.630 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.784 seconds.
Building the Label Vector...
        It takes 0.342 seconds.
        It takes 0.342 seconds.
        It takes 0.401 seconds.
        It takes 1.022 seconds.
Building the Label Vector...
        It takes 0.534 seconds.
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/partitioned_graphs/ogbn_products/4_parts
The number of GCNII layers: 3
The number of hidden units: 128
The number of training epoches: 100
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 1
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
Number of GPUs: 4
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 3)
*** Node 2, starting model training...
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 25)
*** Node 2, constructing the helper classes...
GPU 0, layer [0, 3)
*** Node 1, starting model training...
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 25)
*** Node 1, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 3)
*** Node 3, starting model training...
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 25)
*** Node 3, constructing the helper classes...
2449029, 126167053, 126167053
Number of vertices per chunk: 612258
2449029, 126167053, 126167053
Number of vertices per chunk: 612258
2449029, 126167053, 126167053
Number of vertices per chunk: 612258
train nodes 196615, valid nodes 39323, test nodes 2213091
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 3)
*** Node 0, starting model training...
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 25)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_DROPOUT, output tensors: 1
    Op 2: type OPERATOR_WEIGHT, output tensors: 2
    Op 3: type OPERATOR_MATMUL, output tensors: 3
    Op 4: type OPERATOR_WEIGHT, output tensors: 4
    Op 5: type OPERATOR_MATMUL, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_ADD, output tensors: 7
    Op 8: type OPERATOR_RELU, output tensors: 8
    Op 9: type OPERATOR_DROPOUT, output tensors: 9
    Op 10: type OPERATOR_WEIGHT, output tensors: 10
    Op 11: type OPERATOR_MATMUL, output tensors: 11
    Op 12: type OPERATOR_AGGREGATION, output tensors: 12
    Op 13: type OPERATOR_WEIGHT, output tensors: 13
    Op 14: type OPERATOR_MATMUL, output tensors: 14
    Op 15: type OPERATOR_ADD, output tensors: 15
    Op 16: type OPERATOR_RELU, output tensors: 16
    Op 17: type OPERATOR_DROPOUT, output tensors: 17
    Op 18: type OPERATOR_WEIGHT, output tensors: 18
    Op 19: type OPERATOR_MATMUL, output tensors: 19
    Op 20: type OPERATOR_AGGREGATION, output tensors: 20
    Op 21: type OPERATOR_WEIGHT, output tensors: 21
    Op 22: type OPERATOR_MATMUL, output tensors: 22
    Op 23: type OPERATOR_ADD, output tensors: 23
    Op 24: type OPERATOR_SOFTMAX, output tensors: 24
Chunks (number of global chunks: 4): 0-[0, 612257) 1-[612257, 1224514) 2-[1224514, 1836771) 3-[1836771, 2449029)
2449029, 126167053, 126167053
Number of vertices per chunk: 612258
csr in-out ready !*** Node 0, setting up some other necessary information...
*** Node 0, starting the helper threads...
csr in-out ready !*** Node 2, setting up some other necessary information...
*** Node 2, starting the helper threads...
csr in-out ready !*** Node 3, setting up some other necessary information...
*** Node 3, starting the helper threads...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 1, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[0, 25)...
+++++++++ Node 1, mapping weight op 2
+++++++++ Node 1, mapping weight op 4
+++++++++ Node 0 initializing the weights for op[0, 25)...
+++++++++ Node 0, mapping weight op 2
+++++++++ Node 1, mapping weight op 10
+++++++++ Node 3 initializing the weights for op[0, 25)...
+++++++++ Node 3, mapping weight op 2
+++++++++ Node 1, mapping weight op 13
+++++++++ Node 0, mapping weight op 4
+++++++++ Node 0, mapping weight op 10
+++++++++ Node 2 initializing the weights for op[0, 25)...
+++++++++ Node 2, mapping weight op 2
+++++++++ Node 3, mapping weight op 4
+++++++++ Node 1, mapping weight op 18
+++++++++ Node 0, mapping weight op 13
+++++++++ Node 3, mapping weight op 10
+++++++++ Node 0, mapping weight op 18
+++++++++ Node 2, mapping weight op 4
+++++++++ Node 3, mapping weight op 13
+++++++++ Node 2, mapping weight op 10
+++++++++ Node 3, mapping weight op 18
+++++++++ Node 2, mapping weight op 13
+++++++++ Node 2, mapping weight op 18
+++++++++ Node 1, mapping weight op 21
+++++++++ Node 0, mapping weight op 21
+++++++++ Node 3, mapping weight op 21
+++++++++ Node 2, mapping weight op 21
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.003000000
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.003000000
The learning rate specified by the user: 0.003000000
The learning rate specified by the user: 0.003000000
	Epoch 10:	Loss 1.5749
	Epoch 20:	Loss 0.9213
	Epoch 30:	Loss 0.6866
	Epoch 40:	Loss 0.5809
	Epoch 50:	Loss 0.5236
	Epoch 60:	Loss 0.4872
	Epoch 70:	Loss 0.4626
	Epoch 80:	Loss 0.4442
	Epoch 90:	Loss 0.4294
	Epoch 100:	Loss 0.4171
Node 0, Layer-level comm throughput (act): -nan GBps
Node 3, Layer-level comm throughput (act): -nan GBps
Node 2, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (grad): -nan GBps
Node 2, Layer-level comm throughput (grad): -nan GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 0, Layer-level comm throughput (grad): -nan GBps
Node 0, GPU memory consumption: 24.021 GB
------------------------node id 0,  per-epoch time: 0.279604 s---------------
Node 1, GPU memory consumption: 23.830 GB
------------------------node id 1,  per-epoch time: 0.279603 s---------------
Node 2, GPU memory consumption: 23.820 GB
------------------------node id 2,  per-epoch time: 0.279603 s---------------
Node 3, GPU memory consumption: 23.816 GB
------------------------node id 3,  per-epoch time: 0.279603 s---------------
************ Profiling Results ************
	Bubble: 0.016315 (s) (0.06 percentage)
	Compression: 0.000000 (s) (0.00 percentage)
	Compute: 8.503589 (s) (30.30 percentage)
	GraphCommGPUCPU: 6.455447 (s) (23.00 percentage)
	GraphCommNetwork: 11.439523 (s) (40.76 percentage)
	LayerCommGPUCPU: 0.000000 (s) (0.00 percentage)
	LayerCommNetwork: 0.000000 (s) (0.00 percentage)
	Optimization: 0.053835 (s) (0.19 percentage)
	Other: 1.594580 (s) (5.68 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 2.719 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 2.720 GB
Highest valid_acc: 0.0000
Target test_acc: 0.0455
Epoch to reach the target acc: 0
[MPI Rank 3] Success 
[MPI Rank 2] Success 
[MPI Rank 1] Success 
[MPI Rank 0] Success 
