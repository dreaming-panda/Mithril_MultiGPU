amadeus-MS-7B86
Mon May  8 14:17:44 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:29:00.0 Off |                  N/A |
| 44%   44C    P8     5W / 120W |    357MiB /  6144MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1331      G   /usr/lib/xorg/Xorg                101MiB |
|    0   N/A  N/A      2412      G   /usr/lib/xorg/Xorg                200MiB |
|    0   N/A  N/A      2537      G   /usr/bin/gnome-shell               44MiB |
+-----------------------------------------------------------------------------+
[ 11%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 83%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_gcn
[ 94%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target OSDI2023_MULTI_NODES_graphsage
Initialized node 0 on machine amadeus-MS-7B86
Building the CSR structure...
        It takes 0.036 seconds.
Building the CSC structure...
        It takes 0.036 seconds.
Building the Feature Vector...
        It takes 0.034 seconds.
Building the Label Vector...
        It takes 0.015 seconds.
The graph dataset locates at /home/amadeus/ssd512/gnn_datasets/reordered/ogbn_arxiv
The number of GCNII layers: 4
The number of hidden units: 256
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 5
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 1
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 4)
*** Node 0, starting model training...
Number of operators: 20
0 169343 0 20
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the partition [0, 20) x [0, 169343)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_AGGREGATION, output tensors: 16
    Op 17: type OPERATOR_WEIGHT, output tensors: 17
    Op 18: type OPERATOR_MATMUL, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
Boundaries: 0 169343
Fragments: [0, 169343)
Chunks (number of global chunks: 16): 0-[0, 10584) 1-[10584, 21168) 2-[21168, 31752) 3-[31752, 42336) 4-[42336, 52920) 5-[52920, 63504) 6-[63504, 74088) 7-[74088, 84672) 8-[84672, 95256) ... 15-[158760, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 10584
csr in-out ready !*** Node 0, setting up some other necessary information...
*** Node 0, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 0, mapping weight op 1
using the Pytorch initialization method.
+++++++++ Node 0, mapping weight op 7
using the Pytorch initialization method.
+++++++++ Node 0, mapping weight op 12
using the Pytorch initialization method.
+++++++++ Node 0, mapping weight op 17
using the Pytorch initialization method.
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
	Epoch 10:	Loss 3.26528	TrainAcc 0.1797	ValidAcc 0.0763	TestAcc 0.0588
	Epoch 20:	Loss 2.76874	TrainAcc 0.3161	ValidAcc 0.3192	TestAcc 0.2868
	Epoch 30:	Loss 2.24517	TrainAcc 0.4641	ValidAcc 0.4672	TestAcc 0.4361
	Epoch 40:	Loss 1.92256	TrainAcc 0.5253	ValidAcc 0.5437	TestAcc 0.5265
	Epoch 50:	Loss 1.79562	TrainAcc 0.5510	ValidAcc 0.5660	TestAcc 0.5493
	Epoch 60:	Loss 1.67584	TrainAcc 0.5774	ValidAcc 0.5843	TestAcc 0.5647
	Epoch 70:	Loss 1.57408	TrainAcc 0.5989	ValidAcc 0.5967	TestAcc 0.5759
	Epoch 80:	Loss 1.48096	TrainAcc 0.6164	ValidAcc 0.6195	TestAcc 0.6040
	Epoch 90:	Loss 1.42190	TrainAcc 0.6262	ValidAcc 0.6273	TestAcc 0.6128
	Epoch 100:	Loss 1.38379	TrainAcc 0.6344	ValidAcc 0.6355	TestAcc 0.6250
	Epoch 110:	Loss 1.35939	TrainAcc 0.6404	ValidAcc 0.6433	TestAcc 0.6341
	Epoch 120:	Loss 1.33163	TrainAcc 0.6452	ValidAcc 0.6470	TestAcc 0.6366
	Epoch 130:	Loss 1.31094	TrainAcc 0.6500	ValidAcc 0.6516	TestAcc 0.6448
	Epoch 140:	Loss 1.28584	TrainAcc 0.6540	ValidAcc 0.6551	TestAcc 0.6477
	Epoch 150:	Loss 1.27016	TrainAcc 0.6581	ValidAcc 0.6597	TestAcc 0.6528
	Epoch 160:	Loss 1.25518	TrainAcc 0.6622	ValidAcc 0.6640	TestAcc 0.6583
	Epoch 170:	Loss 1.24092	TrainAcc 0.6662	ValidAcc 0.6672	TestAcc 0.6605
	Epoch 180:	Loss 1.22751	TrainAcc 0.6697	ValidAcc 0.6691	TestAcc 0.6591
	Epoch 190:	Loss 1.21422	TrainAcc 0.6727	ValidAcc 0.6726	TestAcc 0.6641
	Epoch 200:	Loss 1.20430	TrainAcc 0.6751	ValidAcc 0.6752	TestAcc 0.6664
	Epoch 210:	Loss 1.19606	TrainAcc 0.6773	ValidAcc 0.6762	TestAcc 0.6669
	Epoch 220:	Loss 1.18190	TrainAcc 0.6797	ValidAcc 0.6796	TestAcc 0.6713
	Epoch 230:	Loss 1.17731	TrainAcc 0.6815	ValidAcc 0.6807	TestAcc 0.6715
	Epoch 240:	Loss 1.16587	TrainAcc 0.6834	ValidAcc 0.6813	TestAcc 0.6712
	Epoch 250:	Loss 1.15770	TrainAcc 0.6852	ValidAcc 0.6830	TestAcc 0.6706
	Epoch 260:	Loss 1.15178	TrainAcc 0.6871	ValidAcc 0.6870	TestAcc 0.6776
	Epoch 270:	Loss 1.14513	TrainAcc 0.6892	ValidAcc 0.6865	TestAcc 0.6765
	Epoch 280:	Loss 1.14126	TrainAcc 0.6901	ValidAcc 0.6884	TestAcc 0.6786
	Epoch 290:	Loss 1.13310	TrainAcc 0.6913	ValidAcc 0.6863	TestAcc 0.6729
	Epoch 300:	Loss 1.12877	TrainAcc 0.6924	ValidAcc 0.6890	TestAcc 0.6797
	Epoch 310:	Loss 1.12235	TrainAcc 0.6945	ValidAcc 0.6916	TestAcc 0.6827
	Epoch 320:	Loss 1.11792	TrainAcc 0.6959	ValidAcc 0.6929	TestAcc 0.6844
	Epoch 330:	Loss 1.11361	TrainAcc 0.6966	ValidAcc 0.6922	TestAcc 0.6806
	Epoch 340:	Loss 1.10674	TrainAcc 0.6981	ValidAcc 0.6912	TestAcc 0.6778
	Epoch 350:	Loss 1.10608	TrainAcc 0.6988	ValidAcc 0.6925	TestAcc 0.6793
	Epoch 360:	Loss 1.09999	TrainAcc 0.6991	ValidAcc 0.6971	TestAcc 0.6910
	Epoch 370:	Loss 1.09556	TrainAcc 0.7006	ValidAcc 0.6942	TestAcc 0.6822
	Epoch 380:	Loss 1.09610	TrainAcc 0.7007	ValidAcc 0.6983	TestAcc 0.6896
	Epoch 390:	Loss 1.09009	TrainAcc 0.7019	ValidAcc 0.6977	TestAcc 0.6883
	Epoch 400:	Loss 1.08296	TrainAcc 0.7025	ValidAcc 0.6980	TestAcc 0.6885
	Epoch 410:	Loss 1.08128	TrainAcc 0.7030	ValidAcc 0.6974	TestAcc 0.6859
	Epoch 420:	Loss 1.07667	TrainAcc 0.7042	ValidAcc 0.6989	TestAcc 0.6896
	Epoch 430:	Loss 1.07717	TrainAcc 0.7043	ValidAcc 0.6998	TestAcc 0.6902
	Epoch 440:	Loss 1.07212	TrainAcc 0.7046	ValidAcc 0.7007	TestAcc 0.6901
	Epoch 450:	Loss 1.06753	TrainAcc 0.7059	ValidAcc 0.6997	TestAcc 0.6896
	Epoch 460:	Loss 1.06829	TrainAcc 0.7056	ValidAcc 0.7007	TestAcc 0.6902
	Epoch 470:	Loss 1.06322	TrainAcc 0.7066	ValidAcc 0.7008	TestAcc 0.6910
	Epoch 480:	Loss 1.06191	TrainAcc 0.7065	ValidAcc 0.7021	TestAcc 0.6907
	Epoch 490:	Loss 1.05778	TrainAcc 0.7075	ValidAcc 0.6997	TestAcc 0.6877
	Epoch 500:	Loss 1.05688	TrainAcc 0.7078	ValidAcc 0.7026	TestAcc 0.6928
	Epoch 510:	Loss 1.05715	TrainAcc 0.7087	ValidAcc 0.7028	TestAcc 0.6937
	Epoch 520:	Loss 1.05046	TrainAcc 0.7094	ValidAcc 0.7016	TestAcc 0.6906
	Epoch 530:	Loss 1.05047	TrainAcc 0.7100	ValidAcc 0.7006	TestAcc 0.6872
	Epoch 540:	Loss 1.05093	TrainAcc 0.7104	ValidAcc 0.7022	TestAcc 0.6916
	Epoch 550:	Loss 1.04432	TrainAcc 0.7107	ValidAcc 0.7032	TestAcc 0.6922
	Epoch 560:	Loss 1.04338	TrainAcc 0.7110	ValidAcc 0.7027	TestAcc 0.6909
	Epoch 570:	Loss 1.04144	TrainAcc 0.7115	ValidAcc 0.7036	TestAcc 0.6910
	Epoch 580:	Loss 1.03881	TrainAcc 0.7119	ValidAcc 0.7045	TestAcc 0.6958
	Epoch 590:	Loss 1.04370	TrainAcc 0.7126	ValidAcc 0.7054	TestAcc 0.6950
	Epoch 600:	Loss 1.04037	TrainAcc 0.7128	ValidAcc 0.7038	TestAcc 0.6936
	Epoch 610:	Loss 1.03734	TrainAcc 0.7136	ValidAcc 0.7042	TestAcc 0.6934
	Epoch 620:	Loss 1.03658	TrainAcc 0.7139	ValidAcc 0.7047	TestAcc 0.6915
	Epoch 630:	Loss 1.03173	TrainAcc 0.7141	ValidAcc 0.7058	TestAcc 0.6960
	Epoch 640:	Loss 1.03218	TrainAcc 0.7149	ValidAcc 0.7029	TestAcc 0.6904
	Epoch 650:	Loss 1.02960	TrainAcc 0.7156	ValidAcc 0.7060	TestAcc 0.6946
	Epoch 660:	Loss 1.02708	TrainAcc 0.7157	ValidAcc 0.7056	TestAcc 0.6944
	Epoch 670:	Loss 1.02542	TrainAcc 0.7160	ValidAcc 0.7059	TestAcc 0.6957
	Epoch 680:	Loss 1.02423	TrainAcc 0.7161	ValidAcc 0.7070	TestAcc 0.6963
	Epoch 690:	Loss 1.02150	TrainAcc 0.7169	ValidAcc 0.7056	TestAcc 0.6930
	Epoch 700:	Loss 1.01831	TrainAcc 0.7168	ValidAcc 0.7065	TestAcc 0.6954
	Epoch 710:	Loss 1.01663	TrainAcc 0.7172	ValidAcc 0.7064	TestAcc 0.6925
	Epoch 720:	Loss 1.01932	TrainAcc 0.7180	ValidAcc 0.7078	TestAcc 0.6977
	Epoch 730:	Loss 1.01587	TrainAcc 0.7185	ValidAcc 0.7066	TestAcc 0.6938
	Epoch 740:	Loss 1.01250	TrainAcc 0.7181	ValidAcc 0.7074	TestAcc 0.6952
	Epoch 750:	Loss 1.01499	TrainAcc 0.7183	ValidAcc 0.7072	TestAcc 0.6959
	Epoch 760:	Loss 1.01082	TrainAcc 0.7196	ValidAcc 0.7068	TestAcc 0.6948
	Epoch 770:	Loss 1.01214	TrainAcc 0.7190	ValidAcc 0.7083	TestAcc 0.6985
	Epoch 780:	Loss 1.00709	TrainAcc 0.7197	ValidAcc 0.7087	TestAcc 0.6955
	Epoch 790:	Loss 1.00882	TrainAcc 0.7201	ValidAcc 0.7082	TestAcc 0.6935
	Epoch 800:	Loss 1.00426	TrainAcc 0.7207	ValidAcc 0.7087	TestAcc 0.6967
	Epoch 810:	Loss 1.00320	TrainAcc 0.7207	ValidAcc 0.7077	TestAcc 0.6952
	Epoch 820:	Loss 1.00371	TrainAcc 0.7212	ValidAcc 0.7092	TestAcc 0.6978
	Epoch 830:	Loss 1.00175	TrainAcc 0.7213	ValidAcc 0.7079	TestAcc 0.6956
	Epoch 840:	Loss 1.00216	TrainAcc 0.7216	ValidAcc 0.7098	TestAcc 0.6985
	Epoch 850:	Loss 0.99889	TrainAcc 0.7217	ValidAcc 0.7089	TestAcc 0.6985
	Epoch 860:	Loss 0.99934	TrainAcc 0.7224	ValidAcc 0.7084	TestAcc 0.6953
	Epoch 870:	Loss 0.99768	TrainAcc 0.7215	ValidAcc 0.7090	TestAcc 0.7001
	Epoch 880:	Loss 0.99523	TrainAcc 0.7225	ValidAcc 0.7097	TestAcc 0.6975
	Epoch 890:	Loss 0.99589	TrainAcc 0.7231	ValidAcc 0.7086	TestAcc 0.6952
	Epoch 900:	Loss 0.99223	TrainAcc 0.7233	ValidAcc 0.7101	TestAcc 0.7007
	Epoch 910:	Loss 0.99375	TrainAcc 0.7235	ValidAcc 0.7088	TestAcc 0.6982
	Epoch 920:	Loss 0.99034	TrainAcc 0.7236	ValidAcc 0.7094	TestAcc 0.6981
	Epoch 930:	Loss 0.99053	TrainAcc 0.7242	ValidAcc 0.7092	TestAcc 0.6947
	Epoch 940:	Loss 0.99029	TrainAcc 0.7242	ValidAcc 0.7107	TestAcc 0.7013
	Epoch 950:	Loss 0.98919	TrainAcc 0.7244	ValidAcc 0.7106	TestAcc 0.7007
	Epoch 960:	Loss 0.98820	TrainAcc 0.7251	ValidAcc 0.7077	TestAcc 0.6942
	Epoch 970:	Loss 0.98508	TrainAcc 0.7248	ValidAcc 0.7086	TestAcc 0.6944
	Epoch 980:	Loss 0.98462	TrainAcc 0.7243	ValidAcc 0.7096	TestAcc 0.6993
	Epoch 990:	Loss 0.98619	TrainAcc 0.7254	ValidAcc 0.7095	TestAcc 0.6986
Node 0, Layer-level comm throughput (act): -nan GBps
Node 0, Layer-level comm throughput (grad): -nan GBps
	Epoch 1000:	Loss 0.98485	TrainAcc 0.7252	ValidAcc 0.7100	TestAcc 0.6975
Node 0, GPU memory consumption: 4.216 GB
Node 0, compression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, decompression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, pure compute time: 402.364 s, total compute time: 402.364 s
Node 0, wait_for_task_time: 22.008 s, wait_for_other_gpus_time: 0.000 s
------------------------node id 0,  per-epoch time: 0.511654 s---------------
************ Profiling Results ************
	Bubble: 31.194833 (s) (6.10 percentage)
	Compute: 447.608275 (s) (87.54 percentage)
	GradSync: 0.832455 (s) (0.16 percentage)
	GraphComm: 31.689933 (s) (6.20 percentage)
	Imbalance: 0.000304 (s) (0.00 percentage)
	LayerComm: 0.000000 (s) (0.00 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.000 GB
Highest valid_acc: 0.7107
Target test_acc: 0.7013
Epoch to reach the target acc: 940
[MPI Rank 0] Success 
