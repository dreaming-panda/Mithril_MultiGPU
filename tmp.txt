g013.anvil.rcac.purdue.edu
Mon May  8 13:26:39 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   29C    P0    55W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

Consolidate compiler generated dependencies of target cudahelp
[ 11%] Built target context
[ 36%] Built target core
[ 38%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 41%] Linking CXX static library libcudahelp.a
[ 77%] Built target cudahelp
[ 80%] Linking CXX executable estimate_comm_volume
[ 86%] Linking CXX executable gcn
[ 86%] Linking CXX executable gcnii
[ 88%] Linking CXX executable graphsage
[ 91%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target OSDI2023_MULTI_NODES_gcnii
Initialized node 1 on machine g014.anvil.rcac.purdue.edu
Initialized node 0 on machine g013.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
        It takes 0.076 seconds.
Building the CSC structure...
        It takes 0.079 seconds.
Building the CSC structure...
        It takes 0.037 seconds.
        It takes 0.041 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.052 seconds.
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCNII layers: 8
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 1
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 2
        It takes 0.124 seconds.
Building the Label Vector...
        It takes 0.026 seconds.
train nodes 90941, valid nodes 29799, test nodes 48603
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
*** Node 0, starting model training...
Number of operators: 40
0 169343 0 21
0 169343 21 40
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 21) x [0, 169343)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_AGGREGATION, output tensors: 16
    Op 17: type OPERATOR_WEIGHT, output tensors: 17
    Op 18: type OPERATOR_MATMUL, output tensors: 18
    Op 19: type OPERATOR_RELU, output tensors: 19
    Op 20: type OPERATOR_DROPOUT, output tensors: 20
    Op 21: type OPERATOR_AGGREGATION, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_RELU, output tensors: 24
    Op 25: type OPERATOR_DROPOUT, output tensors: 25
    Op 26: type OPERATOR_AGGREGATION, output tensors: 26
    Op 27: type OPERATOR_WEIGHT, output tensors: 27
    Op 28: type OPERATOR_MATMUL, output tensors: 28
    Op 29: type OPERATOR_RELU, output tensors: 29
    Op 30: type OPERATOR_DROPOUT, output tensors: 30
    Op 31: type OPERATOR_AGGREGATION, output tensors: 31
    Op 32: type OPERATOR_WEIGHT, output tensors: 32
    Op 33: type OPERATOR_MATMUL, output tensors: 33
    Op 34: type OPERATOR_RELU, output tensors: 34
    Op 35: type OPERATOR_DROPOUT, output tensors: 35
    Op 36: type OPERATOR_AGGREGATION, output tensors: 36
    Op 37: type OPERATOR_WEIGHT, output tensors: 37
    Op 38: type OPERATOR_MATMUL, output tensors: 38
    Op 39: type OPERATOR_SOFTMAX, output tensors: 39
Boundaries: 0 0 169343 169343
Fragments: [0, 169343)
Chunks (number of global chunks: 16): 0-[0, 10584) 1-[10584, 21168) 2-[21168, 31752) 3-[31752, 42336) 4-[42336, 52920) 5-[52920, 63504) 6-[63504, 74088) 7-[74088, 84672) 8-[84672, 95256) ... 15-[158760, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 10584
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
*** Node 1, starting model training...
Number of operators: 40
0 169343 0 21
0 169343 21 40
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the partition [21, 40) x [0, 169343)
*** Node 1, constructing the helper classes...
169343, 2484941, 2484941
Number of vertices per chunk: 10584
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 1, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[21, 40)...
+++++++++ Node 1, mapping weight op 22
using the Pytorch initialization method.
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 0, mapping weight op 1
using the Pytorch initialization method.
+++++++++ Node 1, mapping weight op 27
+++++++++ Node 0, mapping weight op 7
using the Pytorch initialization method.
using the Pytorch initialization method.
+++++++++ Node 0, mapping weight op 12
+++++++++ Node 1, mapping weight op 32
using the Pytorch initialization method.
using the Pytorch initialization method.
+++++++++ Node 0, mapping weight op 17
+++++++++ Node 1, mapping weight op 37
using the Pytorch initialization method.
using the Pytorch initialization method.
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 10:	Loss 3.41303	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299
	Epoch 20:	Loss 3.14747	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586
	Epoch 30:	Loss 2.97649	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586
	Epoch 40:	Loss 2.87110	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586
	Epoch 50:	Loss 2.82293	TrainAcc 0.2336	ValidAcc 0.1212	TestAcc 0.0912
	Epoch 60:	Loss 2.71845	TrainAcc 0.2435	ValidAcc 0.1176	TestAcc 0.0907
	Epoch 70:	Loss 2.54646	TrainAcc 0.3386	ValidAcc 0.3325	TestAcc 0.2987
	Epoch 80:	Loss 2.35001	TrainAcc 0.3444	ValidAcc 0.3439	TestAcc 0.3063
	Epoch 90:	Loss 2.21601	TrainAcc 0.4245	ValidAcc 0.4397	TestAcc 0.4098
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 9.481 GBps
Node 1, Layer-level comm throughput (grad): -nan GBps
Node 0, Layer-level comm throughput (grad): 9.637 GBps
	Epoch 100:	Loss 2.10430	TrainAcc 0.4691	ValidAcc 0.4924	TestAcc 0.4683
Node 0, GPU memory consumption: 5.417 GB
Node 0, compression time: 0.153s, compression size: 20.995GB, throughput: 137.233GBps
Node 0, decompression time: 0.325s, compression size: 20.995GB, throughput: 64.508GBps
Node 0, pure compute time: 8.372 s, total compute time: 8.851 s
Node 0, wait_for_task_time: 0.721 s, wait_for_other_gpus_time: 0.000 s
------------------------node id 0,  per-epoch time: 0.103474 s---------------
Node 1, GPU memory consumption: 5.195 GB
Node 1, compression time: 0.198s, compression size: 20.995GB, throughput: 106.095GBps
Node 1, decompression time: 0.329s, compression size: 20.995GB, throughput: 63.761GBps
Node 1, pure compute time: 6.990 s, total compute time: 7.517 s
Node 1, wait_for_task_time: 0.433 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 1,  per-epoch time: 0.103474 s---------------
************ Profiling Results ************
	Bubble: 0.545201 (s) (5.22 percentage)
	Compute: 8.950576 (s) (85.68 percentage)
	GradSync: 0.155579 (s) (1.49 percentage)
	GraphComm: 0.002899 (s) (0.03 percentage)
	Imbalance: 0.688195 (s) (6.59 percentage)
	LayerComm: 0.104024 (s) (1.00 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.130 GB
Highest valid_acc: 0.4924
Target test_acc: 0.4683
Epoch to reach the target acc: 100
[MPI Rank 1] Success 
[MPI Rank 0] Success 
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 8
The number of hidden units: 256
The number of training epoches: 0
Learning rate: 0.000000
Initialized node g013.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 0.038 seconds.
Building the CSC structure...
        It takes 0.038 seconds.
Building the Feature Vector...
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.021 seconds.
Number of classes: 40
Number of feature dimensions: 128
Dropout: 0.000 
train nodes 90941, valid nodes 29799, test nodes 48603
*** Allocating resources for all tensors...
    OP_TYPE: OPERATOR_INPUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_SOFTMAX
*** Done allocating resource.
*** Preparing the input tensor...
*** Done preparing the input tensor.
*** Preparing the STD tensor...
    Number of labels: 40
    Number of vertices: 169343
*** Done preparing the STD tensor.
Version 0	Loss 3.2628	TrainAcc 0.0570	ValidAcc 0.0413	TestAcc 0.0299
Version 1	Loss 3.0900	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586
Version 2	Loss 2.9610	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586
Version 3	Loss 3.0218	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586
Version 4	Loss 2.9226	TrainAcc 0.2325	ValidAcc 0.1188	TestAcc 0.0884
Version 5	Loss 2.6892	TrainAcc 0.2486	ValidAcc 0.1323	TestAcc 0.1022
Version 6	Loss 2.4707	TrainAcc 0.3386	ValidAcc 0.3326	TestAcc 0.2986
Version 7	Loss 2.4387	TrainAcc 0.3433	ValidAcc 0.3429	TestAcc 0.3055
Version 8	Loss 2.1372	TrainAcc 0.4259	ValidAcc 0.4430	TestAcc 0.4130
Version 9	Loss 2.0060	TrainAcc 0.4695	ValidAcc 0.4928	TestAcc 0.4682
Version 9 achieved the highest validation accuracy 0.4928 (test accuracy: 0.4682)
