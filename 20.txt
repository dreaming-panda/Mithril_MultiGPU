gnerv1
Mon Aug  7 18:13:30 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   28C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   27C    P8    23W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 36%] Built target core
[ 36%] Built target context
Scanning dependencies of target cudahelp
[ 38%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 41%] Linking CXX static library libcudahelp.a
[ 77%] Built target cudahelp
[ 88%] Linking CXX executable gcn
[ 88%] Linking CXX executable estimate_comm_volume
[ 88%] Linking CXX executable graphsage
[ 88%] Linking CXX executable gcnii
[ 91%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_gcnii
[ 97%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_MULTI_NODES_graphsage
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.028 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
        It takes 0.021 seconds.
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.025 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 200
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.109 seconds.
        It takes 0.109 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.112 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.008 seconds.
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
Number of vertices per chunk: 2790
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.110 Gbps (per GPU), 480.883 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.072 Gbps (per GPU), 480.576 Gbps (aggregated)
The layer-level communication performance: 59.778 Gbps (per GPU), 478.220 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.537 Gbps (per GPU), 476.300 Gbps (aggregated)
The layer-level communication performance: 59.501 Gbps (per GPU), 476.006 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.287 Gbps (per GPU), 474.295 Gbps (aggregated)
The layer-level communication performance: 59.510 Gbps (per GPU), 476.082 Gbps (aggregated)
The layer-level communication performance: 59.203 Gbps (per GPU), 473.625 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.405 Gbps (per GPU), 1251.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.396 Gbps (per GPU), 1251.167 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.405 Gbps (per GPU), 1251.237 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.381 Gbps (per GPU), 1251.051 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.405 Gbps (per GPU), 1251.237 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.384 Gbps (per GPU), 1251.074 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.402 Gbps (per GPU), 1251.214 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.381 Gbps (per GPU), 1251.051 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.353 Gbps (per GPU), 810.826 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.348 Gbps (per GPU), 810.781 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.350 Gbps (per GPU), 810.800 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.351 Gbps (per GPU), 810.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.350 Gbps (per GPU), 810.800 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.352 Gbps (per GPU), 810.813 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.352 Gbps (per GPU), 810.813 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.352 Gbps (per GPU), 810.820 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.605 Gbps (per GPU), 268.842 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.604 Gbps (per GPU), 268.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.605 Gbps (per GPU), 268.843 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.604 Gbps (per GPU), 268.835 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.605 Gbps (per GPU), 268.839 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.606 Gbps (per GPU), 268.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.604 Gbps (per GPU), 268.834 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.600 Gbps (per GPU), 268.801 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.88ms  0.70ms  0.34ms  2.61  2.81K  0.03M
 chk_1  0.90ms  0.71ms  0.34ms  2.63  2.82K  0.03M
 chk_2  0.89ms  0.71ms  0.35ms  2.52  2.80K  0.03M
 chk_3  0.88ms  0.70ms  0.34ms  2.57  2.80K  0.03M
 chk_4  0.90ms  0.72ms  0.34ms  2.63  2.82K  0.03M
 chk_5  0.91ms  0.73ms  0.37ms  2.46  2.75K  0.03M
 chk_6  0.86ms  0.70ms  0.33ms  2.60  2.71K  0.03M
 chk_7  0.88ms  0.71ms  0.35ms  2.54  2.76K  0.03M
 chk_8  0.88ms  0.71ms  0.35ms  2.56  2.79K  0.03M
 chk_9  0.88ms  0.71ms  0.34ms  2.59  2.81K  0.03M
chk_10  0.87ms  0.70ms  0.33ms  2.65  2.81K  0.03M
chk_11  0.90ms  0.73ms  0.36ms  2.46  2.74K  0.03M
chk_12  0.90ms  0.72ms  0.37ms  2.43  2.76K  0.03M
chk_13  0.89ms  0.72ms  0.35ms  2.50  2.75K  0.03M
chk_14  0.88ms  0.71ms  0.34ms  2.62  2.81K  0.03M
chk_15  0.88ms  0.71ms  0.34ms  2.58  2.77K  0.03M
chk_16  0.88ms  0.73ms  0.35ms  2.55  2.78K  0.03M
chk_17  0.89ms  0.71ms  0.35ms  2.57  2.79K  0.03M
chk_18  0.90ms  0.71ms  0.34ms  2.62  2.82K  0.03M
chk_19  0.86ms  0.69ms  0.33ms  2.65  2.81K  0.03M
chk_20  0.90ms  0.72ms  0.36ms  2.48  2.77K  0.03M
chk_21  0.90ms  0.71ms  0.34ms  2.64  2.84K  0.02M
chk_22  0.88ms  0.70ms  0.34ms  2.56  2.78K  0.03M
chk_23  0.88ms  0.71ms  0.34ms  2.58  2.80K  0.03M
chk_24  0.88ms  0.71ms  0.35ms  2.56  2.80K  0.03M
chk_25  0.88ms  0.70ms  0.33ms  2.62  2.81K  0.03M
chk_26  0.87ms  0.70ms  0.33ms  2.63  2.81K  0.03M
chk_27  0.89ms  0.71ms  0.35ms  2.56  2.79K  0.03M
chk_28  0.89ms  0.72ms  0.36ms  2.49  2.77K  0.03M
chk_29  0.89ms  0.71ms  0.35ms  2.54  2.77K  0.03M
chk_30  0.89ms  0.75ms  0.35ms  2.55  2.80K  0.03M
chk_31  0.89ms  0.75ms  0.35ms  2.52  2.78K  0.03M
   Avg  0.89  0.71  0.35
   Max  0.91  0.75  0.37
   Min  0.86  0.69  0.33
 Ratio  1.05  1.09  1.13
   Var  0.00  0.00  0.00
Profiling takes 0.849 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 96.848 ms
Partition 0 [0, 4) has cost: 96.848 ms
Partition 1 [4, 8) has cost: 91.349 ms
Partition 2 [8, 12) has cost: 91.349 ms
Partition 3 [12, 16) has cost: 91.349 ms
Partition 4 [16, 20) has cost: 91.349 ms
Partition 5 [20, 24) has cost: 91.349 ms
Partition 6 [24, 28) has cost: 91.349 ms
Partition 7 [28, 32) has cost: 79.570 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 48.221 ms
GPU 0, Compute+Comm Time: 39.805 ms, Bubble Time: 8.416 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.853 ms, Bubble Time: 8.427 ms, Imbalance Overhead: 2.941 ms
GPU 2, Compute+Comm Time: 36.853 ms, Bubble Time: 8.463 ms, Imbalance Overhead: 2.905 ms
GPU 3, Compute+Comm Time: 36.853 ms, Bubble Time: 8.489 ms, Imbalance Overhead: 2.879 ms
GPU 4, Compute+Comm Time: 36.853 ms, Bubble Time: 8.501 ms, Imbalance Overhead: 2.867 ms
GPU 5, Compute+Comm Time: 36.853 ms, Bubble Time: 8.540 ms, Imbalance Overhead: 2.828 ms
GPU 6, Compute+Comm Time: 36.853 ms, Bubble Time: 8.589 ms, Imbalance Overhead: 2.779 ms
GPU 7, Compute+Comm Time: 33.501 ms, Bubble Time: 8.696 ms, Imbalance Overhead: 6.024 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.567 ms
GPU 0, Compute+Comm Time: 65.073 ms, Bubble Time: 16.633 ms, Imbalance Overhead: 10.861 ms
GPU 1, Compute+Comm Time: 73.501 ms, Bubble Time: 16.383 ms, Imbalance Overhead: 2.683 ms
GPU 2, Compute+Comm Time: 73.501 ms, Bubble Time: 16.326 ms, Imbalance Overhead: 2.740 ms
GPU 3, Compute+Comm Time: 73.501 ms, Bubble Time: 16.337 ms, Imbalance Overhead: 2.729 ms
GPU 4, Compute+Comm Time: 73.501 ms, Bubble Time: 16.383 ms, Imbalance Overhead: 2.683 ms
GPU 5, Compute+Comm Time: 73.501 ms, Bubble Time: 16.402 ms, Imbalance Overhead: 2.664 ms
GPU 6, Compute+Comm Time: 73.501 ms, Bubble Time: 16.416 ms, Imbalance Overhead: 2.650 ms
GPU 7, Compute+Comm Time: 76.048 ms, Bubble Time: 16.465 ms, Imbalance Overhead: 0.054 ms
The estimated cost of the whole pipeline: 147.827 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 188.197 ms
Partition 0 [0, 8) has cost: 188.197 ms
Partition 1 [8, 16) has cost: 182.698 ms
Partition 2 [16, 24) has cost: 182.698 ms
Partition 3 [24, 32) has cost: 170.918 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 54.493 ms
GPU 0, Compute+Comm Time: 45.927 ms, Bubble Time: 8.560 ms, Imbalance Overhead: 0.007 ms
GPU 1, Compute+Comm Time: 44.460 ms, Bubble Time: 8.512 ms, Imbalance Overhead: 1.521 ms
GPU 2, Compute+Comm Time: 44.460 ms, Bubble Time: 8.486 ms, Imbalance Overhead: 1.547 ms
GPU 3, Compute+Comm Time: 42.776 ms, Bubble Time: 8.581 ms, Imbalance Overhead: 3.136 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 98.104 ms
GPU 0, Compute+Comm Time: 77.125 ms, Bubble Time: 15.556 ms, Imbalance Overhead: 5.424 ms
GPU 1, Compute+Comm Time: 81.343 ms, Bubble Time: 15.297 ms, Imbalance Overhead: 1.464 ms
GPU 2, Compute+Comm Time: 81.343 ms, Bubble Time: 15.312 ms, Imbalance Overhead: 1.449 ms
GPU 3, Compute+Comm Time: 82.590 ms, Bubble Time: 15.350 ms, Imbalance Overhead: 0.164 ms
    The estimated cost with 2 DP ways is 160.227 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 370.895 ms
Partition 0 [0, 16) has cost: 370.895 ms
Partition 1 [16, 32) has cost: 353.616 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 81.436 ms
GPU 0, Compute+Comm Time: 72.434 ms, Bubble Time: 9.002 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 70.846 ms, Bubble Time: 8.955 ms, Imbalance Overhead: 1.635 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 123.003 ms
GPU 0, Compute+Comm Time: 106.852 ms, Bubble Time: 13.732 ms, Imbalance Overhead: 2.418 ms
GPU 1, Compute+Comm Time: 109.583 ms, Bubble Time: 13.419 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 214.661 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 724.511 ms
Partition 0 [0, 32) has cost: 724.511 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 269.361 ms
GPU 0, Compute+Comm Time: 269.361 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 306.195 ms
GPU 0, Compute+Comm Time: 306.195 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 604.334 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 1.6205	TrainAcc 0.4404	ValidAcc 0.4418	TestAcc 0.4419	BestValid 0.4418
	Epoch 50:	Loss 1.5640	TrainAcc 0.4739	ValidAcc 0.4723	TestAcc 0.4742	BestValid 0.4723
	Epoch 75:	Loss 1.5337	TrainAcc 0.4686	ValidAcc 0.4693	TestAcc 0.4695	BestValid 0.4723
	Epoch 100:	Loss 1.5040	TrainAcc 0.4954	ValidAcc 0.4947	TestAcc 0.4989	BestValid 0.4947
	Epoch 125:	Loss 1.4926	TrainAcc 0.4464	ValidAcc 0.4428	TestAcc 0.4443	BestValid 0.4947
	Epoch 150:	Loss 1.4828	TrainAcc 0.4544	ValidAcc 0.4503	TestAcc 0.4516	BestValid 0.4947
	Epoch 175:	Loss 1.4698	TrainAcc 0.4967	ValidAcc 0.4931	TestAcc 0.4959	BestValid 0.4947
	Epoch 200:	Loss 1.4585	TrainAcc 0.4819	ValidAcc 0.4786	TestAcc 0.4784	BestValid 0.4947
	Epoch 225:	Loss 1.4508	TrainAcc 0.5008	ValidAcc 0.4970	TestAcc 0.4987	BestValid 0.4970
	Epoch 250:	Loss 1.4457	TrainAcc 0.5019	ValidAcc 0.4952	TestAcc 0.4988	BestValid 0.4970
