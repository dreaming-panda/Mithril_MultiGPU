gnerv1
Fri Aug  4 03:32:33 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   29C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   28C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 27%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 88%] Built target OSDI2023_MULTI_NODES_graphsage
[ 88%] Built target estimate_comm_volume
[ 88%] Built target OSDI2023_MULTI_NODES_gcn
[ 88%] Built target OSDI2023_MULTI_NODES_gcnii
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...Building the CSR structure...
Building the CSR structure...

        It takes 1.967 seconds.
Building the CSC structure...
        It takes 2.376 seconds.
Building the CSC structure...
        It takes 2.388 seconds.
Building the CSC structure...
        It takes 2.400 seconds.
Building the CSC structure...
        It takes 2.407 seconds.
Building the CSC structure...
        It takes 2.416 seconds.
Building the CSC structure...
        It takes 2.546 seconds.
Building the CSC structure...
        It takes 2.643 seconds.
Building the CSC structure...
        It takes 1.823 seconds.
        It takes 2.280 seconds.
        It takes 2.266 seconds.
        It takes 2.308 seconds.
        It takes 2.323 seconds.
        It takes 2.334 seconds.
Building the Feature Vector...
        It takes 2.326 seconds.
        It takes 2.246 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
Building the Feature Vector...
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.284 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
        It takes 0.041 seconds.
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
        It takes 0.245 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.029 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.462 Gbps (per GPU), 443.695 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.219 Gbps (per GPU), 441.752 Gbps (aggregated)
The layer-level communication performance: 55.223 Gbps (per GPU), 441.782 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.009 Gbps (per GPU), 440.073 Gbps (aggregated)
The layer-level communication performance: 54.983 Gbps (per GPU), 439.863 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.767 Gbps (per GPU), 438.139 Gbps (aggregated)
The layer-level communication performance: 54.724 Gbps (per GPU), 437.794 Gbps (aggregated)
The layer-level communication performance: 54.694 Gbps (per GPU), 437.549 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.931 Gbps (per GPU), 1247.446 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.896 Gbps (per GPU), 1247.168 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.789 Gbps (per GPU), 1246.311 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.896 Gbps (per GPU), 1247.168 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.931 Gbps (per GPU), 1247.446 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.890 Gbps (per GPU), 1247.122 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.795 Gbps (per GPU), 1246.357 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.896 Gbps (per GPU), 1247.168 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.904 Gbps (per GPU), 815.233 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.906 Gbps (per GPU), 815.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.904 Gbps (per GPU), 815.232 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.906 Gbps (per GPU), 815.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.913 Gbps (per GPU), 815.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.906 Gbps (per GPU), 815.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.904 Gbps (per GPU), 815.231 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.904 Gbps (per GPU), 815.233 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.266 Gbps (per GPU), 274.128 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.266 Gbps (per GPU), 274.131 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.265 Gbps (per GPU), 274.123 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.264 Gbps (per GPU), 274.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.265 Gbps (per GPU), 274.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.266 Gbps (per GPU), 274.129 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.266 Gbps (per GPU), 274.126 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.264 Gbps (per GPU), 274.109 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.31ms  2.66ms  2.36ms  1.40  8.38K  3.53M
 chk_1  3.51ms  2.86ms  2.66ms  1.32  6.74K  3.60M
 chk_2  3.35ms  2.74ms  2.53ms  1.33  7.27K  3.53M
 chk_3  3.43ms  2.78ms  2.54ms  1.35  7.92K  3.61M
 chk_4  3.12ms  2.68ms  2.52ms  1.24  5.33K  3.68M
 chk_5  3.51ms  2.72ms  2.40ms  1.46 10.07K  3.45M
 chk_6  3.65ms  2.89ms  2.58ms  1.41  9.41K  3.48M
 chk_7  3.33ms  2.72ms  2.46ms  1.35  8.12K  3.60M
 chk_8  3.31ms  2.82ms  2.62ms  1.26  6.09K  3.64M
 chk_9  3.56ms  2.67ms  2.29ms  1.55 11.10K  3.38M
chk_10  3.31ms  2.85ms  2.67ms  1.24  5.67K  3.63M
chk_11  3.36ms  2.73ms  2.47ms  1.36  8.16K  3.54M
chk_12  3.54ms  2.90ms  2.69ms  1.32  7.24K  3.55M
chk_13  3.17ms  2.73ms  2.56ms  1.24  5.41K  3.68M
chk_14  3.62ms  3.00ms  2.77ms  1.31  7.14K  3.53M
chk_15  3.61ms  2.88ms  2.56ms  1.41  9.25K  3.49M
chk_16  3.07ms  2.66ms  2.49ms  1.24  4.78K  3.77M
chk_17  3.38ms  2.81ms  2.59ms  1.31  6.85K  3.60M
chk_18  3.23ms  2.62ms  2.37ms  1.36  7.47K  3.57M
chk_19  3.08ms  2.67ms  2.49ms  1.24  4.88K  3.75M
chk_20  3.29ms  2.70ms  2.47ms  1.33  7.00K  3.63M
chk_21  3.11ms  2.65ms  2.46ms  1.26  5.41K  3.68M
chk_22  3.80ms  2.92ms  2.54ms  1.50 11.07K  3.39M
chk_23  3.41ms  2.77ms  2.53ms  1.35  7.23K  3.64M
chk_24  3.59ms  2.85ms  2.52ms  1.43 10.13K  3.43M
chk_25  3.17ms  2.64ms  2.43ms  1.30  6.40K  3.57M
chk_26  3.35ms  2.98ms  2.65ms  1.27  5.78K  3.55M
chk_27  3.48ms  2.73ms  2.44ms  1.43  9.34K  3.48M
chk_28  3.58ms  3.01ms  2.79ms  1.28  6.37K  3.57M
chk_29  3.29ms  2.79ms  2.62ms  1.26  5.16K  3.78M
chk_30  3.17ms  2.70ms  2.51ms  1.26  5.44K  3.67M
chk_31  3.42ms  2.87ms  2.68ms  1.28  6.33K  3.63M
   Avg  3.38  2.78  2.54
   Max  3.80  3.01  2.79
   Min  3.07  2.62  2.29
 Ratio  1.24  1.15  1.22
   Var  0.03  0.01  0.01
Profiling takes 3.248 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 375.054 ms
Partition 0 [0, 4) has cost: 375.054 ms
Partition 1 [4, 8) has cost: 355.943 ms
Partition 2 [8, 12) has cost: 355.943 ms
Partition 3 [12, 16) has cost: 355.943 ms
Partition 4 [16, 20) has cost: 355.943 ms
Partition 5 [20, 24) has cost: 355.943 ms
Partition 6 [24, 28) has cost: 355.943 ms
Partition 7 [28, 32) has cost: 348.195 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 171.092 ms
GPU 0, Compute+Comm Time: 135.943 ms, Bubble Time: 29.958 ms, Imbalance Overhead: 5.191 ms
GPU 1, Compute+Comm Time: 130.475 ms, Bubble Time: 29.463 ms, Imbalance Overhead: 11.155 ms
GPU 2, Compute+Comm Time: 130.475 ms, Bubble Time: 29.336 ms, Imbalance Overhead: 11.282 ms
GPU 3, Compute+Comm Time: 130.475 ms, Bubble Time: 29.190 ms, Imbalance Overhead: 11.428 ms
GPU 4, Compute+Comm Time: 130.475 ms, Bubble Time: 29.202 ms, Imbalance Overhead: 11.416 ms
GPU 5, Compute+Comm Time: 130.475 ms, Bubble Time: 29.193 ms, Imbalance Overhead: 11.425 ms
GPU 6, Compute+Comm Time: 130.475 ms, Bubble Time: 29.409 ms, Imbalance Overhead: 11.209 ms
GPU 7, Compute+Comm Time: 127.869 ms, Bubble Time: 29.877 ms, Imbalance Overhead: 13.347 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 329.811 ms
GPU 0, Compute+Comm Time: 247.210 ms, Bubble Time: 58.839 ms, Imbalance Overhead: 23.763 ms
GPU 1, Compute+Comm Time: 252.351 ms, Bubble Time: 57.898 ms, Imbalance Overhead: 19.562 ms
GPU 2, Compute+Comm Time: 252.351 ms, Bubble Time: 57.463 ms, Imbalance Overhead: 19.997 ms
GPU 3, Compute+Comm Time: 252.351 ms, Bubble Time: 57.420 ms, Imbalance Overhead: 20.040 ms
GPU 4, Compute+Comm Time: 252.351 ms, Bubble Time: 57.296 ms, Imbalance Overhead: 20.164 ms
GPU 5, Compute+Comm Time: 252.351 ms, Bubble Time: 57.504 ms, Imbalance Overhead: 19.956 ms
GPU 6, Compute+Comm Time: 252.351 ms, Bubble Time: 57.374 ms, Imbalance Overhead: 20.086 ms
GPU 7, Compute+Comm Time: 265.994 ms, Bubble Time: 57.704 ms, Imbalance Overhead: 6.113 ms
The estimated cost of the whole pipeline: 525.949 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 730.998 ms
Partition 0 [0, 8) has cost: 730.998 ms
Partition 1 [8, 16) has cost: 711.887 ms
Partition 2 [16, 24) has cost: 711.887 ms
Partition 3 [24, 32) has cost: 704.139 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 177.164 ms
GPU 0, Compute+Comm Time: 145.974 ms, Bubble Time: 27.539 ms, Imbalance Overhead: 3.651 ms
GPU 1, Compute+Comm Time: 143.026 ms, Bubble Time: 26.952 ms, Imbalance Overhead: 7.186 ms
GPU 2, Compute+Comm Time: 143.026 ms, Bubble Time: 26.741 ms, Imbalance Overhead: 7.396 ms
GPU 3, Compute+Comm Time: 141.763 ms, Bubble Time: 26.409 ms, Imbalance Overhead: 8.992 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 329.288 ms
GPU 0, Compute+Comm Time: 263.630 ms, Bubble Time: 50.276 ms, Imbalance Overhead: 15.382 ms
GPU 1, Compute+Comm Time: 266.078 ms, Bubble Time: 50.483 ms, Imbalance Overhead: 12.726 ms
GPU 2, Compute+Comm Time: 266.078 ms, Bubble Time: 50.508 ms, Imbalance Overhead: 12.702 ms
GPU 3, Compute+Comm Time: 273.759 ms, Bubble Time: 51.707 ms, Imbalance Overhead: 3.822 ms
    The estimated cost with 2 DP ways is 531.775 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1442.885 ms
Partition 0 [0, 16) has cost: 1442.885 ms
Partition 1 [16, 32) has cost: 1416.026 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 221.854 ms
GPU 0, Compute+Comm Time: 194.635 ms, Bubble Time: 23.775 ms, Imbalance Overhead: 3.443 ms
GPU 1, Compute+Comm Time: 192.613 ms, Bubble Time: 24.383 ms, Imbalance Overhead: 4.858 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 366.541 ms
GPU 0, Compute+Comm Time: 316.822 ms, Bubble Time: 40.575 ms, Imbalance Overhead: 9.145 ms
GPU 1, Compute+Comm Time: 321.950 ms, Bubble Time: 39.561 ms, Imbalance Overhead: 5.031 ms
    The estimated cost with 4 DP ways is 617.815 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2858.910 ms
Partition 0 [0, 32) has cost: 2858.910 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 562.043 ms
GPU 0, Compute+Comm Time: 562.043 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 689.909 ms
GPU 0, Compute+Comm Time: 689.909 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1314.549 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 1.5621	TrainAcc 0.6933	ValidAcc 0.7200	TestAcc 0.7162	BestValid 0.7200
	Epoch 50:	Loss 0.6880	TrainAcc 0.8671	ValidAcc 0.8808	TestAcc 0.8773	BestValid 0.8808
	Epoch 75:	Loss 0.4463	TrainAcc 0.9205	ValidAcc 0.9272	TestAcc 0.9256	BestValid 0.9272
	Epoch 100:	Loss 0.3683	TrainAcc 0.9358	ValidAcc 0.9423	TestAcc 0.9409	BestValid 0.9423
	Epoch 125:	Loss 0.3255	TrainAcc 0.9413	ValidAcc 0.9460	TestAcc 0.9450	BestValid 0.9460
	Epoch 150:	Loss 0.3028	TrainAcc 0.9439	ValidAcc 0.9487	TestAcc 0.9476	BestValid 0.9487
	Epoch 175:	Loss 0.2872	TrainAcc 0.9463	ValidAcc 0.9501	TestAcc 0.9492	BestValid 0.9501
	Epoch 200:	Loss 0.2704	TrainAcc 0.9483	ValidAcc 0.9516	TestAcc 0.9510	BestValid 0.9516
	Epoch 225:	Loss 0.2615	TrainAcc 0.9497	ValidAcc 0.9528	TestAcc 0.9525	BestValid 0.9528
	Epoch 250:	Loss 0.2530	TrainAcc 0.9509	ValidAcc 0.9535	TestAcc 0.9533	BestValid 0.9535
	Epoch 275:	Loss 0.2462	TrainAcc 0.9520	ValidAcc 0.9543	TestAcc 0.9541	BestValid 0.9543
	Epoch 300:	Loss 0.2411	TrainAcc 0.9529	ValidAcc 0.9548	TestAcc 0.9548	BestValid 0.9548
	Epoch 325:	Loss 0.2349	TrainAcc 0.9537	ValidAcc 0.9553	TestAcc 0.9550	BestValid 0.9553
	Epoch 350:	Loss 0.2308	TrainAcc 0.9543	ValidAcc 0.9560	TestAcc 0.9556	BestValid 0.9560
	Epoch 375:	Loss 0.2265	TrainAcc 0.9550	ValidAcc 0.9569	TestAcc 0.9561	BestValid 0.9569
	Epoch 400:	Loss 0.2258	TrainAcc 0.9555	ValidAcc 0.9565	TestAcc 0.9563	BestValid 0.9569
	Epoch 425:	Loss 0.2211	TrainAcc 0.9560	ValidAcc 0.9571	TestAcc 0.9568	BestValid 0.9571
	Epoch 450:	Loss 0.2178	TrainAcc 0.9565	ValidAcc 0.9577	TestAcc 0.9569	BestValid 0.9577
	Epoch 475:	Loss 0.2143	TrainAcc 0.9572	ValidAcc 0.9578	TestAcc 0.9576	BestValid 0.9578
	Epoch 500:	Loss 0.2123	TrainAcc 0.9575	ValidAcc 0.9584	TestAcc 0.9577	BestValid 0.9584
	Epoch 525:	Loss 0.2076	TrainAcc 0.9580	ValidAcc 0.9590	TestAcc 0.9580	BestValid 0.9590
	Epoch 550:	Loss 0.2080	TrainAcc 0.9584	ValidAcc 0.9587	TestAcc 0.9582	BestValid 0.9590
	Epoch 575:	Loss 0.2038	TrainAcc 0.9586	ValidAcc 0.9592	TestAcc 0.9586	BestValid 0.9592
	Epoch 600:	Loss 0.2035	TrainAcc 0.9591	ValidAcc 0.9596	TestAcc 0.9588	BestValid 0.9596
	Epoch 625:	Loss 0.1998	TrainAcc 0.9594	ValidAcc 0.9598	TestAcc 0.9589	BestValid 0.9598
	Epoch 650:	Loss 0.1989	TrainAcc 0.9599	ValidAcc 0.9601	TestAcc 0.9592	BestValid 0.9601
	Epoch 675:	Loss 0.1974	TrainAcc 0.9600	ValidAcc 0.9600	TestAcc 0.9593	BestValid 0.9601
	Epoch 700:	Loss 0.1953	TrainAcc 0.9603	ValidAcc 0.9603	TestAcc 0.9595	BestValid 0.9603
	Epoch 725:	Loss 0.1944	TrainAcc 0.9606	ValidAcc 0.9603	TestAcc 0.9596	BestValid 0.9603
	Epoch 750:	Loss 0.1936	TrainAcc 0.9608	ValidAcc 0.9604	TestAcc 0.9596	BestValid 0.9604
	Epoch 775:	Loss 0.1913	TrainAcc 0.9611	ValidAcc 0.9606	TestAcc 0.9602	BestValid 0.9606
	Epoch 800:	Loss 0.1910	TrainAcc 0.9614	ValidAcc 0.9611	TestAcc 0.9602	BestValid 0.9611
	Epoch 825:	Loss 0.1879	TrainAcc 0.9617	ValidAcc 0.9608	TestAcc 0.9602	BestValid 0.9611
	Epoch 850:	Loss 0.1869	TrainAcc 0.9619	ValidAcc 0.9608	TestAcc 0.9605	BestValid 0.9611
	Epoch 875:	Loss 0.1877	TrainAcc 0.9620	ValidAcc 0.9609	TestAcc 0.9605	BestValid 0.9611
	Epoch 900:	Loss 0.1845	TrainAcc 0.9621	ValidAcc 0.9611	TestAcc 0.9606	BestValid 0.9611
	Epoch 925:	Loss 0.1847	TrainAcc 0.9625	ValidAcc 0.9614	TestAcc 0.9607	BestValid 0.9614
	Epoch 950:	Loss 0.1831	TrainAcc 0.9626	ValidAcc 0.9612	TestAcc 0.9607	BestValid 0.9614
	Epoch 975:	Loss 0.1816	TrainAcc 0.9630	ValidAcc 0.9616	TestAcc 0.9607	BestValid 0.9616
	Epoch 1000:	Loss 0.1815	TrainAcc 0.9631	ValidAcc 0.9616	TestAcc 0.9608	BestValid 0.9616
	Epoch 1025:	Loss 0.1801	TrainAcc 0.9632	ValidAcc 0.9614	TestAcc 0.9609	BestValid 0.9616
	Epoch 1050:	Loss 0.1771	TrainAcc 0.9637	ValidAcc 0.9615	TestAcc 0.9611	BestValid 0.9616
	Epoch 1075:	Loss 0.1780	TrainAcc 0.9636	ValidAcc 0.9616	TestAcc 0.9611	BestValid 0.9616
	Epoch 1100:	Loss 0.1769	TrainAcc 0.9638	ValidAcc 0.9616	TestAcc 0.9612	BestValid 0.9616
	Epoch 1125:	Loss 0.1758	TrainAcc 0.9637	ValidAcc 0.9619	TestAcc 0.9609	BestValid 0.9619
	Epoch 1150:	Loss 0.1750	TrainAcc 0.9644	ValidAcc 0.9617	TestAcc 0.9613	BestValid 0.9619
	Epoch 1175:	Loss 0.1733	TrainAcc 0.9643	ValidAcc 0.9621	TestAcc 0.9615	BestValid 0.9621
	Epoch 1200:	Loss 0.1727	TrainAcc 0.9647	ValidAcc 0.9620	TestAcc 0.9616	BestValid 0.9621
	Epoch 1225:	Loss 0.1711	TrainAcc 0.9651	ValidAcc 0.9622	TestAcc 0.9616	BestValid 0.9622
	Epoch 1250:	Loss 0.1709	TrainAcc 0.9652	ValidAcc 0.9623	TestAcc 0.9621	BestValid 0.9623
	Epoch 1275:	Loss 0.1706	TrainAcc 0.9655	ValidAcc 0.9623	TestAcc 0.9617	BestValid 0.9623
	Epoch 1300:	Loss 0.1707	TrainAcc 0.9655	ValidAcc 0.9621	TestAcc 0.9619	BestValid 0.9623
	Epoch 1325:	Loss 0.1684	TrainAcc 0.9654	ValidAcc 0.9622	TestAcc 0.9619	BestValid 0.9623
	Epoch 1350:	Loss 0.1680	TrainAcc 0.9658	ValidAcc 0.9621	TestAcc 0.9621	BestValid 0.9623
	Epoch 1375:	Loss 0.1678	TrainAcc 0.9662	ValidAcc 0.9623	TestAcc 0.9620	BestValid 0.9623
	Epoch 1400:	Loss 0.1654	TrainAcc 0.9665	ValidAcc 0.9627	TestAcc 0.9624	BestValid 0.9627
	Epoch 1425:	Loss 0.1655	TrainAcc 0.9667	ValidAcc 0.9625	TestAcc 0.9622	BestValid 0.9627
	Epoch 1450:	Loss 0.1650	TrainAcc 0.9666	ValidAcc 0.9627	TestAcc 0.9624	BestValid 0.9627
	Epoch 1475:	Loss 0.1641	TrainAcc 0.9669	ValidAcc 0.9630	TestAcc 0.9626	BestValid 0.9630
	Epoch 1500:	Loss 0.1645	TrainAcc 0.9668	ValidAcc 0.9626	TestAcc 0.9624	BestValid 0.9630
	Epoch 1525:	Loss 0.1635	TrainAcc 0.9669	ValidAcc 0.9632	TestAcc 0.9628	BestValid 0.9632
	Epoch 1550:	Loss 0.1630	TrainAcc 0.9673	ValidAcc 0.9630	TestAcc 0.9628	BestValid 0.9632
	Epoch 1575:	Loss 0.1615	TrainAcc 0.9673	ValidAcc 0.9626	TestAcc 0.9626	BestValid 0.9632
	Epoch 1600:	Loss 0.1601	TrainAcc 0.9677	ValidAcc 0.9631	TestAcc 0.9626	BestValid 0.9632
	Epoch 1625:	Loss 0.1597	TrainAcc 0.9679	ValidAcc 0.9634	TestAcc 0.9631	BestValid 0.9634
	Epoch 1650:	Loss 0.1585	TrainAcc 0.9678	ValidAcc 0.9632	TestAcc 0.9630	BestValid 0.9634
	Epoch 1675:	Loss 0.1589	TrainAcc 0.9682	ValidAcc 0.9634	TestAcc 0.9631	BestValid 0.9634
	Epoch 1700:	Loss 0.1587	TrainAcc 0.9681	ValidAcc 0.9632	TestAcc 0.9630	BestValid 0.9634
	Epoch 1725:	Loss 0.1584	TrainAcc 0.9682	ValidAcc 0.9634	TestAcc 0.9632	BestValid 0.9634
	Epoch 1750:	Loss 0.1563	TrainAcc 0.9686	ValidAcc 0.9635	TestAcc 0.9634	BestValid 0.9635
	Epoch 1775:	Loss 0.1562	TrainAcc 0.9686	ValidAcc 0.9635	TestAcc 0.9634	BestValid 0.9635
	Epoch 1800:	Loss 0.1569	TrainAcc 0.9687	ValidAcc 0.9634	TestAcc 0.9635	BestValid 0.9635
	Epoch 1825:	Loss 0.1552	TrainAcc 0.9687	ValidAcc 0.9636	TestAcc 0.9633	BestValid 0.9636
	Epoch 1850:	Loss 0.1536	TrainAcc 0.9687	ValidAcc 0.9635	TestAcc 0.9631	BestValid 0.9636
	Epoch 1875:	Loss 0.1537	TrainAcc 0.9688	ValidAcc 0.9636	TestAcc 0.9633	BestValid 0.9636
	Epoch 1900:	Loss 0.1540	TrainAcc 0.9692	ValidAcc 0.9642	TestAcc 0.9638	BestValid 0.9642
	Epoch 1925:	Loss 0.1531	TrainAcc 0.9693	ValidAcc 0.9640	TestAcc 0.9636	BestValid 0.9642
	Epoch 1950:	Loss 0.1506	TrainAcc 0.9695	ValidAcc 0.9640	TestAcc 0.9637	BestValid 0.9642
	Epoch 1975:	Loss 0.1520	TrainAcc 0.9695	ValidAcc 0.9638	TestAcc 0.9636	BestValid 0.9642
	Epoch 2000:	Loss 0.1492	TrainAcc 0.9696	ValidAcc 0.9642	TestAcc 0.9636	BestValid 0.9642
	Epoch 2025:	Loss 0.1513	TrainAcc 0.9693	ValidAcc 0.9643	TestAcc 0.9637	BestValid 0.9643
	Epoch 2050:	Loss 0.1506	TrainAcc 0.9700	ValidAcc 0.9640	TestAcc 0.9638	BestValid 0.9643
	Epoch 2075:	Loss 0.1505	TrainAcc 0.9698	ValidAcc 0.9642	TestAcc 0.9639	BestValid 0.9643
	Epoch 2100:	Loss 0.1502	TrainAcc 0.9700	ValidAcc 0.9642	TestAcc 0.9638	BestValid 0.9643
	Epoch 2125:	Loss 0.1491	TrainAcc 0.9702	ValidAcc 0.9642	TestAcc 0.9640	BestValid 0.9643
	Epoch 2150:	Loss 0.1486	TrainAcc 0.9702	ValidAcc 0.9644	TestAcc 0.9640	BestValid 0.9644
	Epoch 2175:	Loss 0.1484	TrainAcc 0.9700	ValidAcc 0.9643	TestAcc 0.9636	BestValid 0.9644
	Epoch 2200:	Loss 0.1487	TrainAcc 0.9706	ValidAcc 0.9645	TestAcc 0.9644	BestValid 0.9645
	Epoch 2225:	Loss 0.1468	TrainAcc 0.9712	ValidAcc 0.9646	TestAcc 0.9647	BestValid 0.9646
	Epoch 2250:	Loss 0.1473	TrainAcc 0.9709	ValidAcc 0.9648	TestAcc 0.9642	BestValid 0.9648
	Epoch 2275:	Loss 0.1463	TrainAcc 0.9712	ValidAcc 0.9649	TestAcc 0.9643	BestValid 0.9649
	Epoch 2300:	Loss 0.1473	TrainAcc 0.9709	ValidAcc 0.9644	TestAcc 0.9641	BestValid 0.9649
	Epoch 2325:	Loss 0.1459	TrainAcc 0.9709	ValidAcc 0.9645	TestAcc 0.9639	BestValid 0.9649
	Epoch 2350:	Loss 0.1441	TrainAcc 0.9711	ValidAcc 0.9644	TestAcc 0.9641	BestValid 0.9649
	Epoch 2375:	Loss 0.1440	TrainAcc 0.9713	ValidAcc 0.9647	TestAcc 0.9644	BestValid 0.9649
	Epoch 2400:	Loss 0.1444	TrainAcc 0.9714	ValidAcc 0.9650	TestAcc 0.9642	BestValid 0.9650
	Epoch 2425:	Loss 0.1436	TrainAcc 0.9716	ValidAcc 0.9645	TestAcc 0.9645	BestValid 0.9650
	Epoch 2450:	Loss 0.1427	TrainAcc 0.9716	ValidAcc 0.9651	TestAcc 0.9644	BestValid 0.9651
	Epoch 2475:	Loss 0.1425	TrainAcc 0.9717	ValidAcc 0.9648	TestAcc 0.9645	BestValid 0.9651
	Epoch 2500:	Loss 0.1437	TrainAcc 0.9718	ValidAcc 0.9644	TestAcc 0.9641	BestValid 0.9651
	Epoch 2525:	Loss 0.1414	TrainAcc 0.9721	ValidAcc 0.9649	TestAcc 0.9644	BestValid 0.9651
	Epoch 2550:	Loss 0.1424	TrainAcc 0.9722	ValidAcc 0.9652	TestAcc 0.9645	BestValid 0.9652
	Epoch 2575:	Loss 0.1419	TrainAcc 0.9720	ValidAcc 0.9650	TestAcc 0.9645	BestValid 0.9652
	Epoch 2600:	Loss 0.1392	TrainAcc 0.9722	ValidAcc 0.9651	TestAcc 0.9645	BestValid 0.9652
	Epoch 2625:	Loss 0.1403	TrainAcc 0.9722	ValidAcc 0.9653	TestAcc 0.9647	BestValid 0.9653
	Epoch 2650:	Loss 0.1401	TrainAcc 0.9727	ValidAcc 0.9648	TestAcc 0.9648	BestValid 0.9653
	Epoch 2675:	Loss 0.1399	TrainAcc 0.9724	ValidAcc 0.9653	TestAcc 0.9647	BestValid 0.9653
	Epoch 2700:	Loss 0.1386	TrainAcc 0.9729	ValidAcc 0.9653	TestAcc 0.9648	BestValid 0.9653
	Epoch 2725:	Loss 0.1379	TrainAcc 0.9729	ValidAcc 0.9657	TestAcc 0.9648	BestValid 0.9657
	Epoch 2750:	Loss 0.1389	TrainAcc 0.9727	ValidAcc 0.9652	TestAcc 0.9646	BestValid 0.9657
	Epoch 2775:	Loss 0.1387	TrainAcc 0.9732	ValidAcc 0.9653	TestAcc 0.9648	BestValid 0.9657
	Epoch 2800:	Loss 0.1364	TrainAcc 0.9730	ValidAcc 0.9654	TestAcc 0.9645	BestValid 0.9657
	Epoch 2825:	Loss 0.1372	TrainAcc 0.9730	ValidAcc 0.9653	TestAcc 0.9646	BestValid 0.9657
	Epoch 2850:	Loss 0.1374	TrainAcc 0.9734	ValidAcc 0.9656	TestAcc 0.9647	BestValid 0.9657
	Epoch 2875:	Loss 0.1365	TrainAcc 0.9735	ValidAcc 0.9655	TestAcc 0.9648	BestValid 0.9657
	Epoch 2900:	Loss 0.1358	TrainAcc 0.9735	ValidAcc 0.9652	TestAcc 0.9646	BestValid 0.9657
	Epoch 2925:	Loss 0.1345	TrainAcc 0.9735	ValidAcc 0.9651	TestAcc 0.9644	BestValid 0.9657
	Epoch 2950:	Loss 0.1366	TrainAcc 0.9736	ValidAcc 0.9655	TestAcc 0.9648	BestValid 0.9657
	Epoch 2975:	Loss 0.1339	TrainAcc 0.9736	ValidAcc 0.9653	TestAcc 0.9647	BestValid 0.9657
	Epoch 3000:	Loss 0.1356	TrainAcc 0.9739	ValidAcc 0.9656	TestAcc 0.9647	BestValid 0.9657
	Epoch 3025:	Loss 0.1350	TrainAcc 0.9740	ValidAcc 0.9660	TestAcc 0.9650	BestValid 0.9660
	Epoch 3050:	Loss 0.1339	TrainAcc 0.9743	ValidAcc 0.9658	TestAcc 0.9650	BestValid 0.9660
	Epoch 3075:	Loss 0.1344	TrainAcc 0.9740	ValidAcc 0.9659	TestAcc 0.9650	BestValid 0.9660
	Epoch 3100:	Loss 0.1345	TrainAcc 0.9744	ValidAcc 0.9658	TestAcc 0.9649	BestValid 0.9660
	Epoch 3125:	Loss 0.1323	TrainAcc 0.9741	ValidAcc 0.9664	TestAcc 0.9650	BestValid 0.9664
	Epoch 3150:	Loss 0.1337	TrainAcc 0.9743	ValidAcc 0.9658	TestAcc 0.9651	BestValid 0.9664
	Epoch 3175:	Loss 0.1321	TrainAcc 0.9744	ValidAcc 0.9654	TestAcc 0.9650	BestValid 0.9664
	Epoch 3200:	Loss 0.1325	TrainAcc 0.9745	ValidAcc 0.9655	TestAcc 0.9651	BestValid 0.9664
	Epoch 3225:	Loss 0.1315	TrainAcc 0.9748	ValidAcc 0.9660	TestAcc 0.9652	BestValid 0.9664
	Epoch 3250:	Loss 0.1319	TrainAcc 0.9750	ValidAcc 0.9663	TestAcc 0.9653	BestValid 0.9664
	Epoch 3275:	Loss 0.1304	TrainAcc 0.9751	ValidAcc 0.9659	TestAcc 0.9652	BestValid 0.9664
	Epoch 3300:	Loss 0.1301	TrainAcc 0.9754	ValidAcc 0.9661	TestAcc 0.9653	BestValid 0.9664
	Epoch 3325:	Loss 0.1308	TrainAcc 0.9753	ValidAcc 0.9661	TestAcc 0.9650	BestValid 0.9664
	Epoch 3350:	Loss 0.1296	TrainAcc 0.9751	ValidAcc 0.9661	TestAcc 0.9652	BestValid 0.9664
	Epoch 3375:	Loss 0.1296	TrainAcc 0.9753	ValidAcc 0.9662	TestAcc 0.9653	BestValid 0.9664
	Epoch 3400:	Loss 0.1300	TrainAcc 0.9756	ValidAcc 0.9664	TestAcc 0.9655	BestValid 0.9664
	Epoch 3425:	Loss 0.1294	TrainAcc 0.9751	ValidAcc 0.9658	TestAcc 0.9652	BestValid 0.9664
	Epoch 3450:	Loss 0.1301	TrainAcc 0.9757	ValidAcc 0.9661	TestAcc 0.9654	BestValid 0.9664
	Epoch 3475:	Loss 0.1274	TrainAcc 0.9757	ValidAcc 0.9661	TestAcc 0.9654	BestValid 0.9664
	Epoch 3500:	Loss 0.1288	TrainAcc 0.9754	ValidAcc 0.9658	TestAcc 0.9652	BestValid 0.9664
	Epoch 3525:	Loss 0.1289	TrainAcc 0.9757	ValidAcc 0.9662	TestAcc 0.9653	BestValid 0.9664
	Epoch 3550:	Loss 0.1276	TrainAcc 0.9759	ValidAcc 0.9667	TestAcc 0.9655	BestValid 0.9667
	Epoch 3575:	Loss 0.1262	TrainAcc 0.9759	ValidAcc 0.9663	TestAcc 0.9655	BestValid 0.9667
	Epoch 3600:	Loss 0.1283	TrainAcc 0.9761	ValidAcc 0.9658	TestAcc 0.9654	BestValid 0.9667
	Epoch 3625:	Loss 0.1261	TrainAcc 0.9762	ValidAcc 0.9665	TestAcc 0.9653	BestValid 0.9667
	Epoch 3650:	Loss 0.1276	TrainAcc 0.9763	ValidAcc 0.9659	TestAcc 0.9656	BestValid 0.9667
	Epoch 3675:	Loss 0.1255	TrainAcc 0.9759	ValidAcc 0.9658	TestAcc 0.9655	BestValid 0.9667
	Epoch 3700:	Loss 0.1265	TrainAcc 0.9763	ValidAcc 0.9668	TestAcc 0.9652	BestValid 0.9668
	Epoch 3725:	Loss 0.1257	TrainAcc 0.9761	ValidAcc 0.9661	TestAcc 0.9655	BestValid 0.9668
	Epoch 3750:	Loss 0.1265	TrainAcc 0.9766	ValidAcc 0.9666	TestAcc 0.9656	BestValid 0.9668
	Epoch 3775:	Loss 0.1260	TrainAcc 0.9767	ValidAcc 0.9665	TestAcc 0.9654	BestValid 0.9668
	Epoch 3800:	Loss 0.1255	TrainAcc 0.9769	ValidAcc 0.9667	TestAcc 0.9656	BestValid 0.9668
	Epoch 3825:	Loss 0.1240	TrainAcc 0.9771	ValidAcc 0.9667	TestAcc 0.9658	BestValid 0.9668
	Epoch 3850:	Loss 0.1236	TrainAcc 0.9767	ValidAcc 0.9663	TestAcc 0.9656	BestValid 0.9668
	Epoch 3875:	Loss 0.1243	TrainAcc 0.9770	ValidAcc 0.9664	TestAcc 0.9660	BestValid 0.9668
	Epoch 3900:	Loss 0.1241	TrainAcc 0.9770	ValidAcc 0.9663	TestAcc 0.9656	BestValid 0.9668
	Epoch 3925:	Loss 0.1235	TrainAcc 0.9768	ValidAcc 0.9666	TestAcc 0.9657	BestValid 0.9668
	Epoch 3950:	Loss 0.1231	TrainAcc 0.9773	ValidAcc 0.9669	TestAcc 0.9657	BestValid 0.9669
	Epoch 3975:	Loss 0.1230	TrainAcc 0.9774	ValidAcc 0.9668	TestAcc 0.9660	BestValid 0.9669
	Epoch 4000:	Loss 0.1234	TrainAcc 0.9769	ValidAcc 0.9666	TestAcc 0.9654	BestValid 0.9669
	Epoch 4025:	Loss 0.1219	TrainAcc 0.9770	ValidAcc 0.9671	TestAcc 0.9660	BestValid 0.9671
	Epoch 4050:	Loss 0.1221	TrainAcc 0.9772	ValidAcc 0.9661	TestAcc 0.9655	BestValid 0.9671
	Epoch 4075:	Loss 0.1207	TrainAcc 0.9774	ValidAcc 0.9669	TestAcc 0.9656	BestValid 0.9671
	Epoch 4100:	Loss 0.1215	TrainAcc 0.9773	ValidAcc 0.9665	TestAcc 0.9657	BestValid 0.9671
	Epoch 4125:	Loss 0.1227	TrainAcc 0.9785	ValidAcc 0.9672	TestAcc 0.9661	BestValid 0.9672
	Epoch 4150:	Loss 0.1204	TrainAcc 0.9775	ValidAcc 0.9668	TestAcc 0.9657	BestValid 0.9672
	Epoch 4175:	Loss 0.1220	TrainAcc 0.9777	ValidAcc 0.9671	TestAcc 0.9661	BestValid 0.9672
	Epoch 4200:	Loss 0.1204	TrainAcc 0.9775	ValidAcc 0.9671	TestAcc 0.9658	BestValid 0.9672
	Epoch 4225:	Loss 0.1191	TrainAcc 0.9778	ValidAcc 0.9669	TestAcc 0.9657	BestValid 0.9672
	Epoch 4250:	Loss 0.1203	TrainAcc 0.9778	ValidAcc 0.9666	TestAcc 0.9657	BestValid 0.9672
	Epoch 4275:	Loss 0.1199	TrainAcc 0.9782	ValidAcc 0.9668	TestAcc 0.9656	BestValid 0.9672
	Epoch 4300:	Loss 0.1209	TrainAcc 0.9781	ValidAcc 0.9670	TestAcc 0.9660	BestValid 0.9672
	Epoch 4325:	Loss 0.1186	TrainAcc 0.9782	ValidAcc 0.9670	TestAcc 0.9659	BestValid 0.9672
	Epoch 4350:	Loss 0.1197	TrainAcc 0.9781	ValidAcc 0.9665	TestAcc 0.9659	BestValid 0.9672
	Epoch 4375:	Loss 0.1207	TrainAcc 0.9781	ValidAcc 0.9666	TestAcc 0.9657	BestValid 0.9672
	Epoch 4400:	Loss 0.1202	TrainAcc 0.9787	ValidAcc 0.9668	TestAcc 0.9659	BestValid 0.9672
	Epoch 4425:	Loss 0.1192	TrainAcc 0.9788	ValidAcc 0.9668	TestAcc 0.9662	BestValid 0.9672
	Epoch 4450:	Loss 0.1186	TrainAcc 0.9783	ValidAcc 0.9666	TestAcc 0.9656	BestValid 0.9672
	Epoch 4475:	Loss 0.1164	TrainAcc 0.9789	ValidAcc 0.9668	TestAcc 0.9661	BestValid 0.9672
	Epoch 4500:	Loss 0.1173	TrainAcc 0.9786	ValidAcc 0.9668	TestAcc 0.9659	BestValid 0.9672
	Epoch 4525:	Loss 0.1188	TrainAcc 0.9789	ValidAcc 0.9672	TestAcc 0.9661	BestValid 0.9672
	Epoch 4550:	Loss 0.1187	TrainAcc 0.9787	ValidAcc 0.9668	TestAcc 0.9659	BestValid 0.9672
	Epoch 4575:	Loss 0.1177	TrainAcc 0.9786	ValidAcc 0.9669	TestAcc 0.9659	BestValid 0.9672
	Epoch 4600:	Loss 0.1188	TrainAcc 0.9792	ValidAcc 0.9665	TestAcc 0.9660	BestValid 0.9672
	Epoch 4625:	Loss 0.1167	TrainAcc 0.9790	ValidAcc 0.9663	TestAcc 0.9657	BestValid 0.9672
	Epoch 4650:	Loss 0.1181	TrainAcc 0.9790	ValidAcc 0.9666	TestAcc 0.9659	BestValid 0.9672
	Epoch 4675:	Loss 0.1166	TrainAcc 0.9791	ValidAcc 0.9668	TestAcc 0.9658	BestValid 0.9672
	Epoch 4700:	Loss 0.1171	TrainAcc 0.9791	ValidAcc 0.9670	TestAcc 0.9663	BestValid 0.9672
	Epoch 4725:	Loss 0.1164	TrainAcc 0.9792	ValidAcc 0.9671	TestAcc 0.9661	BestValid 0.9672
	Epoch 4750:	Loss 0.1163	TrainAcc 0.9794	ValidAcc 0.9667	TestAcc 0.9662	BestValid 0.9672
	Epoch 4775:	Loss 0.1161	TrainAcc 0.9794	ValidAcc 0.9671	TestAcc 0.9663	BestValid 0.9672
	Epoch 4800:	Loss 0.1162	TrainAcc 0.9797	ValidAcc 0.9670	TestAcc 0.9663	BestValid 0.9672
	Epoch 4825:	Loss 0.1172	TrainAcc 0.9790	ValidAcc 0.9668	TestAcc 0.9661	BestValid 0.9672
	Epoch 4850:	Loss 0.1149	TrainAcc 0.9796	ValidAcc 0.9672	TestAcc 0.9659	BestValid 0.9672
	Epoch 4875:	Loss 0.1139	TrainAcc 0.9799	ValidAcc 0.9669	TestAcc 0.9664	BestValid 0.9672
	Epoch 4900:	Loss 0.1164	TrainAcc 0.9796	ValidAcc 0.9669	TestAcc 0.9661	BestValid 0.9672
	Epoch 4925:	Loss 0.1144	TrainAcc 0.9797	ValidAcc 0.9669	TestAcc 0.9661	BestValid 0.9672
	Epoch 4950:	Loss 0.1141	TrainAcc 0.9799	ValidAcc 0.9668	TestAcc 0.9661	BestValid 0.9672
	Epoch 4975:	Loss 0.1160	TrainAcc 0.9798	ValidAcc 0.9670	TestAcc 0.9662	BestValid 0.9672
	Epoch 5000:	Loss 0.1134	TrainAcc 0.9802	ValidAcc 0.9673	TestAcc 0.9660	BestValid 0.9673
Node 4, Pre/Post-Pipelining: 1.108 / 1.147 ms, Bubble: 102.779 ms, Compute: 389.356 ms, Comm: 26.577 ms, Imbalance: 30.567 ms
Node 6, Pre/Post-Pipelining: 1.108 / 1.093 ms, Bubble: 102.820 ms, Compute: 391.993 ms, Comm: 29.158 ms, Imbalance: 25.160 ms
Node 5, Pre/Post-Pipelining: 1.110 / 1.117 ms, Bubble: 103.103 ms, Compute: 382.254 ms, Comm: 30.981 ms, Imbalance: 32.984 ms
Node 3, Pre/Post-Pipelining: 1.111 / 1.101 ms, Bubble: 102.467 ms, Compute: 392.936 ms, Comm: 26.658 ms, Imbalance: 26.875 ms
Node 1, Pre/Post-Pipelining: 1.110 / 1.190 ms, Bubble: 102.601 ms, Compute: 391.908 ms, Comm: 28.082 ms, Imbalance: 26.192 ms
Node 7, Pre/Post-Pipelining: 1.106 / 16.529 ms, Bubble: 87.633 ms, Compute: 409.098 ms, Comm: 17.149 ms, Imbalance: 19.580 ms
Node 2, Pre/Post-Pipelining: 1.111 / 1.118 ms, Bubble: 102.943 ms, Compute: 389.507 ms, Comm: 29.837 ms, Imbalance: 26.819 ms
Node 0, Pre/Post-Pipelining: 1.107 / 1.283 ms, Bubble: 102.489 ms, Compute: 411.365 ms, Comm: 17.199 ms, Imbalance: 17.248 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.107 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.283 ms
Cluster-Wide Average, Bubble: 102.489 ms
Cluster-Wide Average, Compute: 411.365 ms
Cluster-Wide Average, Communication: 17.199 ms
Cluster-Wide Average, Imbalance: 17.248 ms
Node 0, GPU memory consumption: 6.616 GB
Node 4, GPU memory consumption: 5.237 GB
Node 2, GPU memory consumption: 5.260 GB
Node 5, GPU memory consumption: 5.260 GB
Node 3, GPU memory consumption: 5.237 GB
Node 7, GPU memory consumption: 5.042 GB
Node 1, GPU memory consumption: 5.260 GB
Node 6, GPU memory consumption: 5.260 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 1.094155 s---------------
------------------------node id 4,  per-epoch time: 1.094155 s---------------
------------------------node id 1,  per-epoch time: 1.094154 s---------------
------------------------node id 5,  per-epoch time: 1.094155 s---------------
------------------------node id 2,  per-epoch time: 1.094155 s---------------
------------------------node id 6,  per-epoch time: 1.094155 s---------------
------------------------node id 3,  per-epoch time: 1.094154 s---------------
------------------------node id 7,  per-epoch time: 1.094155 s---------------
************ Profiling Results ************
	Bubble: 699.635962 (ms) (63.97 percentage)
	Compute: 389.485579 (ms) (35.61 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.594460 (ms) (0.42 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 406.021 Gbps
Highest valid_acc: 0.9673
Target test_acc: 0.9660
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
