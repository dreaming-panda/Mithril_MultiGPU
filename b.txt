gnerv1
Mon Aug  7 03:06:08 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   28C    P8    19W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   27C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 11%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 97%] Built target estimate_comm_volume
[ 97%] Built target OSDI2023_MULTI_NODES_graphsage
[ 97%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target OSDI2023_MULTI_NODES_gcnii
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
        It takes 0.020 seconds.
        It takes 0.018 seconds.
        It takes 0.021 seconds.
        It takes 0.018 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.095 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.105 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 500
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.110 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.012 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
89250, 989006, 989006
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.374 Gbps (per GPU), 482.992 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.084 Gbps (per GPU), 480.669 Gbps (aggregated)
The layer-level communication performance: 60.087 Gbps (per GPU), 480.700 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.800 Gbps (per GPU), 478.397 Gbps (aggregated)
The layer-level communication performance: 59.823 Gbps (per GPU), 478.585 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.580 Gbps (per GPU), 476.641 Gbps (aggregated)
The layer-level communication performance: 59.534 Gbps (per GPU), 476.269 Gbps (aggregated)
The layer-level communication performance: 59.498 Gbps (per GPU), 475.988 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.747 Gbps (per GPU), 1253.973 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.764 Gbps (per GPU), 1254.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.738 Gbps (per GPU), 1253.903 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.758 Gbps (per GPU), 1254.067 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.752 Gbps (per GPU), 1254.020 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.755 Gbps (per GPU), 1254.043 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.738 Gbps (per GPU), 1253.900 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.773 Gbps (per GPU), 1254.181 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.996 Gbps (per GPU), 807.969 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.995 Gbps (per GPU), 807.963 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.999 Gbps (per GPU), 807.989 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.996 Gbps (per GPU), 807.969 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.998 Gbps (per GPU), 807.983 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.939 Gbps (per GPU), 807.509 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.000 Gbps (per GPU), 808.002 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.893 Gbps (per GPU), 807.147 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.139 Gbps (per GPU), 305.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.140 Gbps (per GPU), 305.120 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.134 Gbps (per GPU), 305.074 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.136 Gbps (per GPU), 305.089 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.134 Gbps (per GPU), 305.072 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.134 Gbps (per GPU), 305.068 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.128 Gbps (per GPU), 305.023 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.128 Gbps (per GPU), 305.028 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
 chk_1  0.40ms  0.34ms  0.50ms  1.46  2.82K  0.03M
 chk_2  0.40ms  0.34ms  0.50ms  1.47  2.80K  0.03M
 chk_3  0.40ms  0.34ms  0.50ms  1.49  2.80K  0.03M
 chk_4  0.40ms  0.35ms  0.50ms  1.46  2.82K  0.03M
 chk_5  0.40ms  0.35ms  0.51ms  1.45  2.75K  0.03M
 chk_6  0.40ms  0.34ms  0.50ms  1.45  2.71K  0.03M
 chk_7  0.40ms  0.34ms  0.50ms  1.46  2.76K  0.03M
 chk_8  0.40ms  0.34ms  0.50ms  1.46  2.79K  0.03M
 chk_9  0.40ms  0.34ms  0.50ms  1.47  2.81K  0.03M
chk_10  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_11  0.40ms  0.35ms  0.51ms  1.45  2.74K  0.03M
chk_12  0.40ms  0.35ms  0.51ms  1.45  2.76K  0.03M
chk_13  0.40ms  0.35ms  0.51ms  1.45  2.75K  0.03M
chk_14  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_15  0.40ms  0.34ms  0.50ms  1.46  2.77K  0.03M
chk_16  0.40ms  0.34ms  0.50ms  1.46  2.78K  0.03M
chk_17  0.40ms  0.35ms  0.51ms  1.46  2.79K  0.03M
chk_18  0.40ms  0.35ms  0.51ms  1.46  2.82K  0.03M
chk_19  0.40ms  0.34ms  0.50ms  1.47  2.81K  0.03M
chk_20  0.40ms  0.35ms  0.51ms  1.46  2.77K  0.03M
chk_21  0.41ms  0.35ms  0.51ms  1.46  2.84K  0.02M
chk_22  0.40ms  0.34ms  0.50ms  1.47  2.78K  0.03M
chk_23  0.40ms  0.34ms  0.50ms  1.46  2.80K  0.03M
chk_24  0.40ms  0.34ms  0.50ms  1.46  2.80K  0.03M
chk_25  0.40ms  0.34ms  0.50ms  1.46  2.81K  0.03M
chk_26  0.40ms  0.34ms  0.62ms  1.80  2.81K  0.03M
chk_27  0.40ms  0.35ms  0.66ms  1.88  2.79K  0.03M
chk_28  0.40ms  0.35ms  0.51ms  1.47  2.77K  0.03M
chk_29  0.40ms  0.34ms  0.50ms  1.45  2.77K  0.03M
chk_30  0.40ms  0.35ms  0.50ms  1.45  2.80K  0.03M
chk_31  0.40ms  0.35ms  0.51ms  1.45  2.78K  0.03M
   Avg  0.40  0.34  0.51
   Max  0.41  0.35  0.66
   Min  0.40  0.34  0.50
 Ratio  1.02  1.04  1.32
   Var  0.00  0.00  0.00
Profiling takes 0.597 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 55.123 ms
Partition 0 [0, 4) has cost: 45.916 ms
Partition 1 [4, 8) has cost: 44.098 ms
Partition 2 [8, 12) has cost: 44.098 ms
Partition 3 [12, 16) has cost: 44.098 ms
Partition 4 [16, 20) has cost: 44.098 ms
Partition 5 [20, 24) has cost: 44.098 ms
Partition 6 [24, 29) has cost: 55.123 ms
Partition 7 [29, 33) has cost: 49.437 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 28.091 ms
GPU 0, Compute+Comm Time: 20.139 ms, Bubble Time: 5.108 ms, Imbalance Overhead: 2.844 ms
GPU 1, Compute+Comm Time: 19.881 ms, Bubble Time: 5.002 ms, Imbalance Overhead: 3.208 ms
GPU 2, Compute+Comm Time: 19.881 ms, Bubble Time: 4.883 ms, Imbalance Overhead: 3.326 ms
GPU 3, Compute+Comm Time: 19.881 ms, Bubble Time: 4.768 ms, Imbalance Overhead: 3.441 ms
GPU 4, Compute+Comm Time: 19.881 ms, Bubble Time: 4.655 ms, Imbalance Overhead: 3.554 ms
GPU 5, Compute+Comm Time: 19.881 ms, Bubble Time: 4.537 ms, Imbalance Overhead: 3.673 ms
GPU 6, Compute+Comm Time: 23.669 ms, Bubble Time: 4.422 ms, Imbalance Overhead: 0.000 ms
GPU 7, Compute+Comm Time: 20.854 ms, Bubble Time: 4.506 ms, Imbalance Overhead: 2.730 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.717 ms
GPU 0, Compute+Comm Time: 38.044 ms, Bubble Time: 7.860 ms, Imbalance Overhead: 2.813 ms
GPU 1, Compute+Comm Time: 40.915 ms, Bubble Time: 7.782 ms, Imbalance Overhead: 0.020 ms
GPU 2, Compute+Comm Time: 33.678 ms, Bubble Time: 7.963 ms, Imbalance Overhead: 7.076 ms
GPU 3, Compute+Comm Time: 33.678 ms, Bubble Time: 8.140 ms, Imbalance Overhead: 6.898 ms
GPU 4, Compute+Comm Time: 33.678 ms, Bubble Time: 8.324 ms, Imbalance Overhead: 6.715 ms
GPU 5, Compute+Comm Time: 33.678 ms, Bubble Time: 8.521 ms, Imbalance Overhead: 6.518 ms
GPU 6, Compute+Comm Time: 33.678 ms, Bubble Time: 8.718 ms, Imbalance Overhead: 6.321 ms
GPU 7, Compute+Comm Time: 35.237 ms, Bubble Time: 8.900 ms, Imbalance Overhead: 4.579 ms
The estimated cost of the whole pipeline: 80.648 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 99.221 ms
Partition 0 [0, 8) has cost: 90.014 ms
Partition 1 [8, 17) has cost: 99.221 ms
Partition 2 [17, 25) has cost: 88.196 ms
Partition 3 [25, 33) has cost: 93.535 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 30.449 ms
GPU 0, Compute+Comm Time: 23.286 ms, Bubble Time: 4.645 ms, Imbalance Overhead: 2.518 ms
GPU 1, Compute+Comm Time: 25.977 ms, Bubble Time: 4.471 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 23.616 ms, Bubble Time: 4.572 ms, Imbalance Overhead: 2.260 ms
GPU 3, Compute+Comm Time: 24.127 ms, Bubble Time: 4.700 ms, Imbalance Overhead: 1.622 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 48.823 ms
GPU 0, Compute+Comm Time: 39.743 ms, Bubble Time: 7.545 ms, Imbalance Overhead: 1.534 ms
GPU 1, Compute+Comm Time: 37.443 ms, Bubble Time: 7.393 ms, Imbalance Overhead: 3.986 ms
GPU 2, Compute+Comm Time: 41.533 ms, Bubble Time: 7.290 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 37.767 ms, Bubble Time: 7.547 ms, Imbalance Overhead: 3.509 ms
    The estimated cost with 2 DP ways is 83.235 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 189.234 ms
Partition 0 [0, 17) has cost: 189.234 ms
Partition 1 [17, 33) has cost: 181.731 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.880 ms
GPU 0, Compute+Comm Time: 38.221 ms, Bubble Time: 4.659 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 37.477 ms, Bubble Time: 4.773 ms, Imbalance Overhead: 0.631 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 59.783 ms
GPU 0, Compute+Comm Time: 52.329 ms, Bubble Time: 6.623 ms, Imbalance Overhead: 0.831 ms
GPU 1, Compute+Comm Time: 53.289 ms, Bubble Time: 6.494 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 107.796 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 370.965 ms
Partition 0 [0, 33) has cost: 370.965 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 122.141 ms
GPU 0, Compute+Comm Time: 122.141 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 137.258 ms
GPU 0, Compute+Comm Time: 137.258 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 272.370 ms

*** Node 5, starting model training...
Num Stages: 8 / 8
*** Node 6, starting model training...
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
Num Stages: 8 / 8
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 1.6397
	Epoch 50:	Loss 1.6102
	Epoch 75:	Loss 1.5901
	Epoch 100:	Loss 1.5800
	Epoch 125:	Loss 1.5627
	Epoch 150:	Loss 1.5508
	Epoch 175:	Loss 1.5407
	Epoch 200:	Loss 1.5311
	Epoch 225:	Loss 1.5245
	Epoch 250:	Loss 1.5138
	Epoch 275:	Loss 1.5073
	Epoch 300:	Loss 1.5000
	Epoch 325:	Loss 1.4957
	Epoch 350:	Loss 1.4856
	Epoch 375:	Loss 1.4848
	Epoch 400:	Loss 1.4749
	Epoch 425:	Loss 1.4725
	Epoch 450:	Loss 1.4696
	Epoch 475:	Loss 1.4663
	Epoch 500:	Loss 1.4704
Node 1, Pre/Post-Pipelining: 0.449 / 0.484 ms, Bubble: 16.876 ms, Compute: 46.480 ms, Comm: 13.823 ms, Imbalance: 11.822 ms
Node 3, Pre/Post-Pipelining: 0.449 / 0.471 ms, Bubble: 16.896 ms, Compute: 45.264 ms, Comm: 16.246 ms, Imbalance: 10.555 ms
Node 2, Pre/Post-Pipelining: 0.446 / 0.496 ms, Bubble: 17.009 ms, Compute: 47.990 ms, Comm: 15.594 ms, Imbalance: 8.392 ms
Node 5, Pre/Post-Pipelining: 0.449 / 0.508 ms, Bubble: 17.395 ms, Compute: 47.177 ms, Comm: 15.848 ms, Imbalance: 8.589 ms
Node 0, Pre/Post-Pipelining: 0.448 / 0.553 ms, Bubble: 16.528 ms, Compute: 58.347 ms, Comm: 10.955 ms, Imbalance: 2.824 ms
Node 6, Pre/Post-Pipelining: 0.447 / 0.499 ms, Bubble: 17.655 ms, Compute: 46.438 ms, Comm: 13.751 ms, Imbalance: 11.187 ms
Node 7, Pre/Post-Pipelining: 0.452 / 3.699 ms, Bubble: 14.810 ms, Compute: 53.042 ms, Comm: 10.681 ms, Imbalance: 7.163 ms
Node 4, Pre/Post-Pipelining: 0.449 / 0.516 ms, Bubble: 17.039 ms, Compute: 47.347 ms, Comm: 16.616 ms, Imbalance: 8.096 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.448 ms
Cluster-Wide Average, Post-Pipelining Overhead: 0.553 ms
Cluster-Wide Average, Bubble: 16.528 ms
Cluster-Wide Average, Compute: 58.347 ms
Cluster-Wide Average, Communication: 10.955 ms
Cluster-Wide Average, Imbalance: 2.824 ms
Node 4, GPU memory consumption: 2.821 GB
Node 0, GPU memory consumption: 4.174 GB
Node 7, GPU memory consumption: 2.837 GB
Node 3, GPU memory consumption: 2.821 GB
Node 5, GPU memory consumption: 2.844 GB
Node 1, GPU memory consumption: 2.844 GB
Node 6, GPU memory consumption: 2.844 GB
Node 2, GPU memory consumption: 2.844 GB
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 4,  per-epoch time: 0.090334 s---------------
------------------------node id 0,  per-epoch time: 0.090334 s---------------
------------------------node id 5,  per-epoch time: 0.090334 s---------------
------------------------node id 1,  per-epoch time: 0.090334 s---------------
------------------------node id 6,  per-epoch time: 0.090333 s---------------
------------------------node id 2,  per-epoch time: 0.090334 s---------------
------------------------node id 7,  per-epoch time: 0.090333 s---------------
------------------------node id 3,  per-epoch time: 0.090334 s---------------
************ Profiling Results ************
	Bubble: 42.729452 (ms) (46.71 percentage)
	Compute: 46.723972 (ms) (51.08 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.022247 (ms) (2.21 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.931 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 0.932 GB
	Aggregated layer-level communication throughput: 563.580 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0607
Epoch to reach the target acc: 0
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
