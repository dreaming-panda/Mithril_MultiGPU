gnerv1
Thu Aug 10 20:29:24 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   29C    P8    19W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   28C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 30%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[100%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target estimate_comm_volume
[100%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_MULTI_NODES_gcnii
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.031 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.021 seconds.
        It takes 0.022 seconds.
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Feature Vector...
        It takes 0.023 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.102 seconds.
Building the Label Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.006 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.107 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.112 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.712 Gbps (per GPU), 453.694 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.457 Gbps (per GPU), 451.655 Gbps (aggregated)
The layer-level communication performance: 56.450 Gbps (per GPU), 451.604 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.232 Gbps (per GPU), 449.859 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.996 Gbps (per GPU), 447.965 Gbps (aggregated)
The layer-level communication performance: 55.950 Gbps (per GPU), 447.602 Gbps (aggregated)
The layer-level communication performance: 55.920 Gbps (per GPU), 447.359 Gbps (aggregated)
The layer-level communication performance: 56.203 Gbps (per GPU), 449.620 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.082 Gbps (per GPU), 1264.657 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.082 Gbps (per GPU), 1264.654 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.085 Gbps (per GPU), 1264.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.082 Gbps (per GPU), 1264.654 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.085 Gbps (per GPU), 1264.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.091 Gbps (per GPU), 1264.728 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.097 Gbps (per GPU), 1264.773 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.091 Gbps (per GPU), 1264.726 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.171 Gbps (per GPU), 817.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.166 Gbps (per GPU), 817.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.171 Gbps (per GPU), 817.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.170 Gbps (per GPU), 817.357 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.170 Gbps (per GPU), 817.357 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.169 Gbps (per GPU), 817.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.171 Gbps (per GPU), 817.371 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.166 Gbps (per GPU), 817.324 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.074 Gbps (per GPU), 272.591 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.074 Gbps (per GPU), 272.593 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.075 Gbps (per GPU), 272.603 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.072 Gbps (per GPU), 272.572 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.071 Gbps (per GPU), 272.572 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.073 Gbps (per GPU), 272.586 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.070 Gbps (per GPU), 272.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.070 Gbps (per GPU), 272.559 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.35ms  0.51ms  1.45  2.81K  0.03M
 chk_1  0.41ms  0.35ms  0.51ms  1.45  2.82K  0.03M
 chk_2  0.41ms  0.35ms  0.51ms  1.46  2.80K  0.03M
 chk_3  0.41ms  0.35ms  0.51ms  1.45  2.80K  0.03M
 chk_4  0.41ms  0.35ms  0.51ms  1.45  2.82K  0.03M
 chk_5  0.40ms  0.36ms  0.52ms  1.44  2.75K  0.03M
 chk_6  0.40ms  0.35ms  0.51ms  1.45  2.71K  0.03M
 chk_7  0.40ms  0.35ms  0.51ms  1.45  2.76K  0.03M
 chk_8  0.41ms  0.35ms  0.51ms  1.45  2.79K  0.03M
 chk_9  0.41ms  0.35ms  0.51ms  1.45  2.81K  0.03M
chk_10  0.41ms  0.35ms  0.51ms  1.44  2.81K  0.03M
chk_11  0.40ms  0.36ms  0.52ms  1.45  2.74K  0.03M
chk_12  0.42ms  0.35ms  0.52ms  1.45  2.76K  0.03M
chk_13  0.40ms  0.35ms  0.52ms  1.46  2.75K  0.03M
chk_14  0.41ms  0.35ms  0.51ms  1.46  2.81K  0.03M
chk_15  0.40ms  0.35ms  0.51ms  1.46  2.77K  0.03M
chk_16  0.40ms  0.35ms  0.51ms  1.46  2.78K  0.03M
chk_17  0.41ms  0.36ms  0.52ms  1.45  2.79K  0.03M
chk_18  0.41ms  0.36ms  0.52ms  1.45  2.82K  0.03M
chk_19  0.41ms  0.35ms  0.51ms  1.47  2.81K  0.03M
chk_20  0.40ms  0.36ms  0.52ms  1.45  2.77K  0.03M
chk_21  0.41ms  0.35ms  0.52ms  1.45  2.84K  0.02M
chk_22  0.40ms  0.35ms  0.51ms  1.45  2.78K  0.03M
chk_23  0.41ms  0.35ms  0.51ms  1.46  2.80K  0.03M
chk_24  0.41ms  0.35ms  0.51ms  1.46  2.80K  0.03M
chk_25  0.41ms  0.35ms  0.51ms  1.46  2.81K  0.03M
chk_26  0.48ms  0.35ms  0.51ms  1.45  2.81K  0.03M
chk_27  0.45ms  0.36ms  0.52ms  1.45  2.79K  0.03M
chk_28  0.41ms  0.36ms  0.52ms  1.45  2.77K  0.03M
chk_29  0.40ms  0.35ms  0.51ms  1.45  2.77K  0.03M
chk_30  0.41ms  0.35ms  0.52ms  1.46  2.80K  0.03M
chk_31  0.41ms  0.35ms  0.52ms  1.45  2.78K  0.03M
   Avg  0.41  0.35  0.51
   Max  0.48  0.36  0.52
   Min  0.40  0.35  0.51
 Ratio  1.21  1.04  1.02
   Var  0.00  0.00  0.00
Profiling takes 0.611 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 56.497 ms
Partition 0 [0, 4) has cost: 47.011 ms
Partition 1 [4, 8) has cost: 45.197 ms
Partition 2 [8, 12) has cost: 45.197 ms
Partition 3 [12, 16) has cost: 45.197 ms
Partition 4 [16, 20) has cost: 45.197 ms
Partition 5 [20, 25) has cost: 56.497 ms
Partition 6 [25, 29) has cost: 45.197 ms
Partition 7 [29, 33) has cost: 50.318 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 29.136 ms
GPU 0, Compute+Comm Time: 20.883 ms, Bubble Time: 5.199 ms, Imbalance Overhead: 3.053 ms
GPU 1, Compute+Comm Time: 20.622 ms, Bubble Time: 5.079 ms, Imbalance Overhead: 3.434 ms
GPU 2, Compute+Comm Time: 20.622 ms, Bubble Time: 4.959 ms, Imbalance Overhead: 3.555 ms
GPU 3, Compute+Comm Time: 20.622 ms, Bubble Time: 4.847 ms, Imbalance Overhead: 3.667 ms
GPU 4, Compute+Comm Time: 20.622 ms, Bubble Time: 4.732 ms, Imbalance Overhead: 3.782 ms
GPU 5, Compute+Comm Time: 24.519 ms, Bubble Time: 4.617 ms, Imbalance Overhead: 0.000 ms
GPU 6, Compute+Comm Time: 20.622 ms, Bubble Time: 4.702 ms, Imbalance Overhead: 3.811 ms
GPU 7, Compute+Comm Time: 21.573 ms, Bubble Time: 4.801 ms, Imbalance Overhead: 2.762 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 50.141 ms
GPU 0, Compute+Comm Time: 38.817 ms, Bubble Time: 8.272 ms, Imbalance Overhead: 3.052 ms
GPU 1, Compute+Comm Time: 34.647 ms, Bubble Time: 8.178 ms, Imbalance Overhead: 7.316 ms
GPU 2, Compute+Comm Time: 42.050 ms, Bubble Time: 8.092 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 34.647 ms, Bubble Time: 8.279 ms, Imbalance Overhead: 7.215 ms
GPU 4, Compute+Comm Time: 34.647 ms, Bubble Time: 8.468 ms, Imbalance Overhead: 7.027 ms
GPU 5, Compute+Comm Time: 34.647 ms, Bubble Time: 8.638 ms, Imbalance Overhead: 6.857 ms
GPU 6, Compute+Comm Time: 34.647 ms, Bubble Time: 8.822 ms, Imbalance Overhead: 6.672 ms
GPU 7, Compute+Comm Time: 36.199 ms, Bubble Time: 9.022 ms, Imbalance Overhead: 4.920 ms
The estimated cost of the whole pipeline: 83.241 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 101.694 ms
Partition 0 [0, 8) has cost: 92.208 ms
Partition 1 [8, 16) has cost: 90.395 ms
Partition 2 [16, 25) has cost: 101.694 ms
Partition 3 [25, 33) has cost: 95.515 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 31.290 ms
GPU 0, Compute+Comm Time: 24.028 ms, Bubble Time: 4.884 ms, Imbalance Overhead: 2.378 ms
GPU 1, Compute+Comm Time: 24.327 ms, Bubble Time: 4.703 ms, Imbalance Overhead: 2.260 ms
GPU 2, Compute+Comm Time: 26.738 ms, Bubble Time: 4.552 ms, Imbalance Overhead: 0.000 ms
GPU 3, Compute+Comm Time: 24.801 ms, Bubble Time: 4.664 ms, Imbalance Overhead: 1.824 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 49.938 ms
GPU 0, Compute+Comm Time: 40.485 ms, Bubble Time: 7.477 ms, Imbalance Overhead: 1.976 ms
GPU 1, Compute+Comm Time: 42.566 ms, Bubble Time: 7.372 ms, Imbalance Overhead: 0.000 ms
GPU 2, Compute+Comm Time: 38.396 ms, Bubble Time: 7.591 ms, Imbalance Overhead: 3.950 ms
GPU 3, Compute+Comm Time: 38.758 ms, Bubble Time: 7.856 ms, Imbalance Overhead: 3.324 ms
    The estimated cost with 2 DP ways is 85.289 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 193.902 ms
Partition 0 [0, 17) has cost: 193.902 ms
Partition 1 [17, 33) has cost: 185.910 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 43.519 ms
GPU 0, Compute+Comm Time: 38.801 ms, Bubble Time: 4.718 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 37.951 ms, Bubble Time: 4.822 ms, Imbalance Overhead: 0.747 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 60.738 ms
GPU 0, Compute+Comm Time: 52.888 ms, Bubble Time: 6.719 ms, Imbalance Overhead: 1.131 ms
GPU 1, Compute+Comm Time: 54.151 ms, Bubble Time: 6.587 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 109.470 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 379.812 ms
Partition 0 [0, 33) has cost: 379.812 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 135.256 ms
GPU 0, Compute+Comm Time: 135.256 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 150.434 ms
GPU 0, Compute+Comm Time: 150.434 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 299.975 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9379	TrainAcc 0.4153	ValidAcc 0.4187	TestAcc 0.4183	BestValid 0.4187
	Epoch 50:	Loss 1.6079	TrainAcc 0.4608	ValidAcc 0.4600	TestAcc 0.4601	BestValid 0.4600
	Epoch 100:	Loss 1.5754	TrainAcc 0.4578	ValidAcc 0.4587	TestAcc 0.4591	BestValid 0.4600
	Epoch 150:	Loss 1.5458	TrainAcc 0.4562	ValidAcc 0.4548	TestAcc 0.4558	BestValid 0.4600
	Epoch 200:	Loss 1.5258	TrainAcc 0.4773	ValidAcc 0.4766	TestAcc 0.4742	BestValid 0.4766
	Epoch 250:	Loss 1.5084	TrainAcc 0.4899	ValidAcc 0.4820	TestAcc 0.4849	BestValid 0.4820
	Epoch 300:	Loss 1.4905	TrainAcc 0.5014	ValidAcc 0.4961	TestAcc 0.4964	BestValid 0.4961
	Epoch 350:	Loss 1.4805	TrainAcc 0.4926	ValidAcc 0.4847	TestAcc 0.4853	BestValid 0.4961
	Epoch 400:	Loss 1.4715	TrainAcc 0.5009	ValidAcc 0.4914	TestAcc 0.4940	BestValid 0.4961
	Epoch 450:	Loss 1.4690	TrainAcc 0.4865	ValidAcc 0.4785	TestAcc 0.4774	BestValid 0.4961
	Epoch 500:	Loss 1.4649	TrainAcc 0.4987	ValidAcc 0.4903	TestAcc 0.4903	BestValid 0.4961
	Epoch 550:	Loss 1.4576	TrainAcc 0.5055	ValidAcc 0.4946	TestAcc 0.4968	BestValid 0.4961
	Epoch 600:	Loss 1.4504	TrainAcc 0.5129	ValidAcc 0.5030	TestAcc 0.5041	BestValid 0.5030
	Epoch 650:	Loss 1.4497	TrainAcc 0.5070	ValidAcc 0.4967	TestAcc 0.4992	BestValid 0.5030
	Epoch 700:	Loss 1.4476	TrainAcc 0.5119	ValidAcc 0.5016	TestAcc 0.5018	BestValid 0.5030
	Epoch 750:	Loss 1.4414	TrainAcc 0.5167	ValidAcc 0.5058	TestAcc 0.5082	BestValid 0.5058
	Epoch 800:	Loss 1.4455	TrainAcc 0.5158	ValidAcc 0.5041	TestAcc 0.5046	BestValid 0.5058
	Epoch 850:	Loss 1.4367	TrainAcc 0.5181	ValidAcc 0.5055	TestAcc 0.5069	BestValid 0.5058
	Epoch 900:	Loss 1.4368	TrainAcc 0.5190	ValidAcc 0.5075	TestAcc 0.5096	BestValid 0.5075
	Epoch 950:	Loss 1.4350	TrainAcc 0.5203	ValidAcc 0.5080	TestAcc 0.5050	BestValid 0.5080
	Epoch 1000:	Loss 1.4328	TrainAcc 0.5235	ValidAcc 0.5092	TestAcc 0.5113	BestValid 0.5092
	Epoch 1050:	Loss 1.4287	TrainAcc 0.5236	ValidAcc 0.5099	TestAcc 0.5092	BestValid 0.5099
	Epoch 1100:	Loss 1.4263	TrainAcc 0.5218	ValidAcc 0.5067	TestAcc 0.5088	BestValid 0.5099
	Epoch 1150:	Loss 1.4268	TrainAcc 0.5213	ValidAcc 0.5074	TestAcc 0.5088	BestValid 0.5099
	Epoch 1200:	Loss 1.4219	TrainAcc 0.5261	ValidAcc 0.5102	TestAcc 0.5129	BestValid 0.5102
	Epoch 1250:	Loss 1.4269	TrainAcc 0.5256	ValidAcc 0.5125	TestAcc 0.5132	BestValid 0.5125
	Epoch 1300:	Loss 1.4215	TrainAcc 0.5266	ValidAcc 0.5121	TestAcc 0.5124	BestValid 0.5125
	Epoch 1350:	Loss 1.4152	TrainAcc 0.5275	ValidAcc 0.5124	TestAcc 0.5142	BestValid 0.5125
	Epoch 1400:	Loss 1.4182	TrainAcc 0.5285	ValidAcc 0.5125	TestAcc 0.5132	BestValid 0.5125
	Epoch 1450:	Loss 1.4154	TrainAcc 0.5285	ValidAcc 0.5125	TestAcc 0.5143	BestValid 0.5125
	Epoch 1500:	Loss 1.4154	TrainAcc 0.5191	ValidAcc 0.5045	TestAcc 0.5030	BestValid 0.5125
	Epoch 1550:	Loss 1.4118	TrainAcc 0.5269	ValidAcc 0.5104	TestAcc 0.5127	BestValid 0.5125
	Epoch 1600:	Loss 1.4087	TrainAcc 0.5228	ValidAcc 0.5065	TestAcc 0.5072	BestValid 0.5125
	Epoch 1650:	Loss 1.4049	TrainAcc 0.5268	ValidAcc 0.5092	TestAcc 0.5124	BestValid 0.5125
	Epoch 1700:	Loss 1.4088	TrainAcc 0.5326	ValidAcc 0.5149	TestAcc 0.5158	BestValid 0.5149
	Epoch 1750:	Loss 1.4094	TrainAcc 0.5272	ValidAcc 0.5095	TestAcc 0.5109	BestValid 0.5149
	Epoch 1800:	Loss 1.4048	TrainAcc 0.5240	ValidAcc 0.5036	TestAcc 0.5070	BestValid 0.5149
	Epoch 1850:	Loss 1.4054	TrainAcc 0.5351	ValidAcc 0.5154	TestAcc 0.5155	BestValid 0.5154
	Epoch 1900:	Loss 1.4005	TrainAcc 0.5277	ValidAcc 0.5084	TestAcc 0.5095	BestValid 0.5154
	Epoch 1950:	Loss 1.3981	TrainAcc 0.5321	ValidAcc 0.5123	TestAcc 0.5147	BestValid 0.5154
	Epoch 2000:	Loss 1.3970	TrainAcc 0.5277	ValidAcc 0.5092	TestAcc 0.5097	BestValid 0.5154
	Epoch 2050:	Loss 1.3960	TrainAcc 0.5319	ValidAcc 0.5104	TestAcc 0.5115	BestValid 0.5154
	Epoch 2100:	Loss 1.3979	TrainAcc 0.5372	ValidAcc 0.5166	TestAcc 0.5167	BestValid 0.5166
	Epoch 2150:	Loss 1.3987	TrainAcc 0.5398	ValidAcc 0.5187	TestAcc 0.5168	BestValid 0.5187
	Epoch 2200:	Loss 1.3907	TrainAcc 0.5398	ValidAcc 0.5182	TestAcc 0.5171	BestValid 0.5187
	Epoch 2250:	Loss 1.3916	TrainAcc 0.5311	ValidAcc 0.5083	TestAcc 0.5088	BestValid 0.5187
	Epoch 2300:	Loss 1.3922	TrainAcc 0.5242	ValidAcc 0.5056	TestAcc 0.5028	BestValid 0.5187
	Epoch 2350:	Loss 1.3879	TrainAcc 0.5312	ValidAcc 0.5087	TestAcc 0.5091	BestValid 0.5187
	Epoch 2400:	Loss 1.3872	TrainAcc 0.5317	ValidAcc 0.5080	TestAcc 0.5079	BestValid 0.5187
	Epoch 2450:	Loss 1.3849	TrainAcc 0.5373	ValidAcc 0.5119	TestAcc 0.5130	BestValid 0.5187
	Epoch 2500:	Loss 1.3849	TrainAcc 0.5403	ValidAcc 0.5171	TestAcc 0.5162	BestValid 0.5187
	Epoch 2550:	Loss 1.3826	TrainAcc 0.5449	ValidAcc 0.5195	TestAcc 0.5196	BestValid 0.5195
	Epoch 2600:	Loss 1.3826	TrainAcc 0.5380	ValidAcc 0.5127	TestAcc 0.5139	BestValid 0.5195
	Epoch 2650:	Loss 1.3827	TrainAcc 0.5367	ValidAcc 0.5103	TestAcc 0.5113	BestValid 0.5195
	Epoch 2700:	Loss 1.3793	TrainAcc 0.5438	ValidAcc 0.5180	TestAcc 0.5155	BestValid 0.5195
	Epoch 2750:	Loss 1.3784	TrainAcc 0.5368	ValidAcc 0.5100	TestAcc 0.5092	BestValid 0.5195
	Epoch 2800:	Loss 1.3751	TrainAcc 0.5444	ValidAcc 0.5181	TestAcc 0.5171	BestValid 0.5195
	Epoch 2850:	Loss 1.3798	TrainAcc 0.5457	ValidAcc 0.5186	TestAcc 0.5172	BestValid 0.5195
	Epoch 2900:	Loss 1.3751	TrainAcc 0.5388	ValidAcc 0.5100	TestAcc 0.5094	BestValid 0.5195
	Epoch 2950:	Loss 1.3733	TrainAcc 0.5429	ValidAcc 0.5124	TestAcc 0.5119	BestValid 0.5195
	Epoch 3000:	Loss 1.3747	TrainAcc 0.5366	ValidAcc 0.5087	TestAcc 0.5063	BestValid 0.5195
	Epoch 3050:	Loss 1.3717	TrainAcc 0.5500	ValidAcc 0.5184	TestAcc 0.5192	BestValid 0.5195
	Epoch 3100:	Loss 1.3693	TrainAcc 0.5389	ValidAcc 0.5105	TestAcc 0.5089	BestValid 0.5195
	Epoch 3150:	Loss 1.3683	TrainAcc 0.5372	ValidAcc 0.5075	TestAcc 0.5048	BestValid 0.5195
	Epoch 3200:	Loss 1.3695	TrainAcc 0.5429	ValidAcc 0.5130	TestAcc 0.5116	BestValid 0.5195
	Epoch 3250:	Loss 1.3670	TrainAcc 0.5461	ValidAcc 0.5140	TestAcc 0.5132	BestValid 0.5195
	Epoch 3300:	Loss 1.3661	TrainAcc 0.5522	ValidAcc 0.5199	TestAcc 0.5178	BestValid 0.5199
	Epoch 3350:	Loss 1.3628	TrainAcc 0.5482	ValidAcc 0.5158	TestAcc 0.5136	BestValid 0.5199
	Epoch 3400:	Loss 1.3617	TrainAcc 0.5436	ValidAcc 0.5109	TestAcc 0.5098	BestValid 0.5199
	Epoch 3450:	Loss 1.3635	TrainAcc 0.5510	ValidAcc 0.5179	TestAcc 0.5153	BestValid 0.5199
	Epoch 3500:	Loss 1.3591	TrainAcc 0.5492	ValidAcc 0.5137	TestAcc 0.5132	BestValid 0.5199
	Epoch 3550:	Loss 1.3599	TrainAcc 0.5454	ValidAcc 0.5120	TestAcc 0.5104	BestValid 0.5199
	Epoch 3600:	Loss 1.3568	TrainAcc 0.5488	ValidAcc 0.5140	TestAcc 0.5124	BestValid 0.5199
	Epoch 3650:	Loss 1.3535	TrainAcc 0.5600	ValidAcc 0.5224	TestAcc 0.5210	BestValid 0.5224
	Epoch 3700:	Loss 1.3574	TrainAcc 0.5483	ValidAcc 0.5132	TestAcc 0.5109	BestValid 0.5224
	Epoch 3750:	Loss 1.3537	TrainAcc 0.5509	ValidAcc 0.5155	TestAcc 0.5131	BestValid 0.5224
	Epoch 3800:	Loss 1.3537	TrainAcc 0.5498	ValidAcc 0.5142	TestAcc 0.5114	BestValid 0.5224
	Epoch 3850:	Loss 1.3526	TrainAcc 0.5499	ValidAcc 0.5138	TestAcc 0.5119	BestValid 0.5224
	Epoch 3900:	Loss 1.3508	TrainAcc 0.5489	ValidAcc 0.5112	TestAcc 0.5107	BestValid 0.5224
	Epoch 3950:	Loss 1.3503	TrainAcc 0.5533	ValidAcc 0.5166	TestAcc 0.5135	BestValid 0.5224
	Epoch 4000:	Loss 1.3482	TrainAcc 0.5591	ValidAcc 0.5213	TestAcc 0.5179	BestValid 0.5224
	Epoch 4050:	Loss 1.3489	TrainAcc 0.5510	ValidAcc 0.5131	TestAcc 0.5115	BestValid 0.5224
	Epoch 4100:	Loss 1.3482	TrainAcc 0.5611	ValidAcc 0.5196	TestAcc 0.5181	BestValid 0.5224
	Epoch 4150:	Loss 1.3455	TrainAcc 0.5547	ValidAcc 0.5160	TestAcc 0.5138	BestValid 0.5224
	Epoch 4200:	Loss 1.3459	TrainAcc 0.5544	ValidAcc 0.5151	TestAcc 0.5124	BestValid 0.5224
	Epoch 4250:	Loss 1.3474	TrainAcc 0.5521	ValidAcc 0.5121	TestAcc 0.5102	BestValid 0.5224
	Epoch 4300:	Loss 1.3445	TrainAcc 0.5509	ValidAcc 0.5115	TestAcc 0.5102	BestValid 0.5224
	Epoch 4350:	Loss 1.3451	TrainAcc 0.5586	ValidAcc 0.5159	TestAcc 0.5130	BestValid 0.5224
	Epoch 4400:	Loss 1.3445	TrainAcc 0.5619	ValidAcc 0.5190	TestAcc 0.5171	BestValid 0.5224
	Epoch 4450:	Loss 1.3441	TrainAcc 0.5592	ValidAcc 0.5175	TestAcc 0.5153	BestValid 0.5224
	Epoch 4500:	Loss 1.3421	TrainAcc 0.5682	ValidAcc 0.5221	TestAcc 0.5215	BestValid 0.5224
	Epoch 4550:	Loss 1.3403	TrainAcc 0.5639	ValidAcc 0.5192	TestAcc 0.5177	BestValid 0.5224
	Epoch 4600:	Loss 1.3380	TrainAcc 0.5644	ValidAcc 0.5200	TestAcc 0.5186	BestValid 0.5224
	Epoch 4650:	Loss 1.3380	TrainAcc 0.5676	ValidAcc 0.5226	TestAcc 0.5216	BestValid 0.5226
	Epoch 4700:	Loss 1.3389	TrainAcc 0.5705	ValidAcc 0.5225	TestAcc 0.5220	BestValid 0.5226
	Epoch 4750:	Loss 1.3348	TrainAcc 0.5587	ValidAcc 0.5142	TestAcc 0.5128	BestValid 0.5226
	Epoch 4800:	Loss 1.3369	TrainAcc 0.5606	ValidAcc 0.5141	TestAcc 0.5130	BestValid 0.5226
	Epoch 4850:	Loss 1.3360	TrainAcc 0.5657	ValidAcc 0.5209	TestAcc 0.5167	BestValid 0.5226
	Epoch 4900:	Loss 1.3376	TrainAcc 0.5469	ValidAcc 0.5042	TestAcc 0.5014	BestValid 0.5226
	Epoch 4950:	Loss 1.3365	TrainAcc 0.5591	ValidAcc 0.5135	TestAcc 0.5121	BestValid 0.5226
	Epoch 5000:	Loss 1.3347	TrainAcc 0.5688	ValidAcc 0.5220	TestAcc 0.5202	BestValid 0.5226
****** Epoch Time (Excluding Evaluation Cost): 0.094 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 17.404 ms (Max: 18.491, Min: 15.705, Sum: 139.234)
Cluster-Wide Average, Compute: 51.203 ms (Max: 62.070, Min: 47.677, Sum: 409.625)
Cluster-Wide Average, Communication-Layer: 14.290 ms (Max: 16.753, Min: 10.805, Sum: 114.317)
Cluster-Wide Average, Bubble-Imbalance: 9.347 ms (Max: 13.816, Min: 2.318, Sum: 74.773)
Cluster-Wide Average, Communication-Graph: 0.461 ms (Max: 0.504, Min: 0.402, Sum: 3.687)
Cluster-Wide Average, Optimization: 0.094 ms (Max: 0.112, Min: 0.086, Sum: 0.751)
Cluster-Wide Average, Others: 1.182 ms (Max: 3.955, Min: 0.769, Sum: 9.456)
****** Breakdown Sum: 93.980 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.004 GB (Max: 4.174, Min: 2.821, Sum: 24.030)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 68.993 Gbps (Max: 82.703, Min: 51.844, Sum: 551.942)
Layer-level communication (cluster-wide, per-epoch): 0.931 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.931 GB
****** Accuracy Results ******
Highest valid_acc: 0.5226
Target test_acc: 0.5216
Epoch to reach the target acc: 4649
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
