gnerv1
Mon Aug  7 01:36:23 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   27C    P8    19W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   27C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 27%] Built target context
[ 27%] Built target core
[ 77%] Built target cudahelp
[ 88%] Built target OSDI2023_MULTI_NODES_graphsage
[ 88%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target estimate_comm_volume
Running experiments...
gnerv3
gnerv3
gnerv3
gnerv3
gnerv2
gnerv2
gnerv2
gnerv2
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
        It takes 0.010 seconds.
        It takes 0.011 seconds.
Building the Feature Vector...
        It takes 0.010 seconds.
        It takes 0.009 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.013 seconds.
        It takes 0.013 seconds.
Building the Feature Vector...
        It takes 0.013 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/amazon_computers/32_parts
The number of GCNII layers: 32
The number of hidden units: 1024
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 10
Number of feature dimensions: 767
Number of vertices: 13752
Number of GPUs: 8
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.024 seconds.
Building the Label Vector...
        It takes 0.026 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.001 seconds.
        It takes 0.028 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.028 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.001 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
13752, 505474, 505474
Number of vertices per chunk: 430
13752, 505474, 505474
Number of vertices per chunk: 430
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
13752, 505474, 505474
Number of vertices per chunk: 430
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
13752, 505474, 505474
Number of vertices per chunk: 430
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 200, valid nodes 500, test nodes 1000
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 421) 1-[421, 915) 2-[915, 1336) 3-[1336, 1728) 4-[1728, 2108) 5-[2108, 2487) 6-[2487, 2901) 7-[2901, 3403) 8-[3403, 3776) ... 31-[13352, 13752)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
13752, 505474, 505474
Number of vertices per chunk: 430
13752, 505474, 505474
Number of vertices per chunk: 430
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
13752, 505474, 505474
Number of vertices per chunk: 430
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
13752, 505474, 505474
Number of vertices per chunk: 430
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.629 Gbps (per GPU), 477.033 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.326 Gbps (per GPU), 474.611 Gbps (aggregated)
The layer-level communication performance: 59.323 Gbps (per GPU), 474.587 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.069 Gbps (per GPU), 472.548 Gbps (aggregated)
The layer-level communication performance: 59.039 Gbps (per GPU), 472.308 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.818 Gbps (per GPU), 470.545 Gbps (aggregated)
The layer-level communication performance: 58.767 Gbps (per GPU), 470.139 Gbps (aggregated)
The layer-level communication performance: 58.737 Gbps (per GPU), 469.900 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.333 Gbps (per GPU), 1242.665 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.322 Gbps (per GPU), 1242.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.347 Gbps (per GPU), 1242.780 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.232 Gbps (per GPU), 1241.860 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.319 Gbps (per GPU), 1242.550 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.328 Gbps (per GPU), 1242.622 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.238 Gbps (per GPU), 1241.906 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.324 Gbps (per GPU), 1242.596 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.943 Gbps (per GPU), 799.543 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.941 Gbps (per GPU), 799.531 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.940 Gbps (per GPU), 799.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.942 Gbps (per GPU), 799.537 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.943 Gbps (per GPU), 799.543 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.944 Gbps (per GPU), 799.550 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.944 Gbps (per GPU), 799.550 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.946 Gbps (per GPU), 799.569 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.501 Gbps (per GPU), 308.011 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.498 Gbps (per GPU), 307.987 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.497 Gbps (per GPU), 307.975 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.493 Gbps (per GPU), 307.948 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.497 Gbps (per GPU), 307.974 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.491 Gbps (per GPU), 307.930 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.494 Gbps (per GPU), 307.949 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.490 Gbps (per GPU), 307.920 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.43ms  0.67ms  0.81ms  1.87  0.42K  0.02M
 chk_1  0.44ms  0.66ms  0.80ms  1.79  0.49K  0.01M
 chk_2  0.43ms  0.66ms  0.80ms  1.87  0.42K  0.02M
 chk_3  0.42ms  0.65ms  0.79ms  1.88  0.39K  0.02M
 chk_4  0.44ms  0.66ms  0.80ms  1.82  0.38K  0.02M
 chk_5  0.44ms  0.66ms  0.79ms  1.81  0.38K  0.02M
 chk_6  0.42ms  0.66ms  0.80ms  1.88  0.41K  0.02M
 chk_7  0.45ms  0.66ms  0.81ms  1.81  0.50K  0.01M
 chk_8  0.44ms  0.70ms  0.83ms  1.89  0.37K  0.02M
 chk_9  0.47ms  0.66ms  0.79ms  1.69  0.43K  0.01M
chk_10  0.52ms  0.66ms  0.80ms  1.54  0.47K  0.01M
chk_11  0.51ms  0.65ms  0.79ms  1.57  0.43K  0.01M
chk_12  0.42ms  0.66ms  0.79ms  1.86  0.35K  0.02M
chk_13  0.43ms  0.66ms  0.80ms  1.85  0.44K  0.02M
chk_14  0.45ms  0.67ms  0.80ms  1.78  0.48K  0.01M
chk_15  0.45ms  0.69ms  0.83ms  1.83  0.43K  0.01M
chk_16  0.44ms  0.67ms  0.81ms  1.85  0.41K  0.02M
chk_17  0.44ms  0.67ms  0.80ms  1.81  0.38K  0.02M
chk_18  0.45ms  0.68ms  0.82ms  1.82  0.51K  0.01M
chk_19  0.43ms  0.66ms  0.80ms  1.86  0.42K  0.02M
chk_20  0.44ms  0.66ms  0.80ms  1.80  0.48K  0.01M
chk_21  0.44ms  0.67ms  0.81ms  1.84  0.45K  0.01M
chk_22  0.43ms  0.68ms  0.82ms  1.91  0.41K  0.02M
chk_23  0.43ms  0.68ms  0.82ms  1.93  0.40K  0.02M
chk_24  0.42ms  0.66ms  0.79ms  1.87  0.35K  0.02M
chk_25  0.43ms  0.69ms  0.83ms  1.93  0.41K  0.02M
chk_26  0.42ms  0.66ms  0.80ms  1.89  0.40K  0.02M
chk_27  0.43ms  0.66ms  0.79ms  1.85  0.42K  0.02M
chk_28  0.45ms  0.66ms  0.80ms  1.80  0.49K  0.01M
chk_29  0.45ms  0.67ms  0.81ms  1.81  0.50K  0.01M
chk_30  0.44ms  0.66ms  0.80ms  1.80  0.50K  0.01M
chk_31  0.42ms  0.66ms  0.79ms  1.88  0.40K  0.02M
   Avg  0.44  0.67  0.80
   Max  0.52  0.70  0.83
   Min  0.42  0.65  0.79
 Ratio  1.23  1.07  1.06
   Var  0.00  0.00  0.00
Profiling takes 0.838 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 99.435 ms
Partition 0 [0, 5) has cost: 99.435 ms
Partition 1 [5, 9) has cost: 85.306 ms
Partition 2 [9, 13) has cost: 85.306 ms
Partition 3 [13, 17) has cost: 85.306 ms
Partition 4 [17, 21) has cost: 85.306 ms
Partition 5 [21, 25) has cost: 85.306 ms
Partition 6 [25, 29) has cost: 85.306 ms
Partition 7 [29, 33) has cost: 89.708 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.443 ms
GPU 0, Compute+Comm Time: 38.651 ms, Bubble Time: 7.792 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 34.211 ms, Bubble Time: 7.869 ms, Imbalance Overhead: 4.363 ms
GPU 2, Compute+Comm Time: 34.211 ms, Bubble Time: 7.996 ms, Imbalance Overhead: 4.235 ms
GPU 3, Compute+Comm Time: 34.211 ms, Bubble Time: 8.094 ms, Imbalance Overhead: 4.137 ms
GPU 4, Compute+Comm Time: 34.211 ms, Bubble Time: 8.144 ms, Imbalance Overhead: 4.088 ms
GPU 5, Compute+Comm Time: 34.211 ms, Bubble Time: 8.190 ms, Imbalance Overhead: 4.042 ms
GPU 6, Compute+Comm Time: 34.211 ms, Bubble Time: 8.244 ms, Imbalance Overhead: 3.988 ms
GPU 7, Compute+Comm Time: 35.280 ms, Bubble Time: 8.353 ms, Imbalance Overhead: 2.810 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 91.154 ms
GPU 0, Compute+Comm Time: 69.542 ms, Bubble Time: 16.413 ms, Imbalance Overhead: 5.199 ms
GPU 1, Compute+Comm Time: 66.210 ms, Bubble Time: 16.210 ms, Imbalance Overhead: 8.734 ms
GPU 2, Compute+Comm Time: 66.210 ms, Bubble Time: 16.080 ms, Imbalance Overhead: 8.864 ms
GPU 3, Compute+Comm Time: 66.210 ms, Bubble Time: 15.974 ms, Imbalance Overhead: 8.970 ms
GPU 4, Compute+Comm Time: 66.210 ms, Bubble Time: 15.881 ms, Imbalance Overhead: 9.063 ms
GPU 5, Compute+Comm Time: 66.210 ms, Bubble Time: 15.667 ms, Imbalance Overhead: 9.277 ms
GPU 6, Compute+Comm Time: 66.210 ms, Bubble Time: 15.390 ms, Imbalance Overhead: 9.554 ms
GPU 7, Compute+Comm Time: 75.899 ms, Bubble Time: 15.255 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 144.476 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 184.741 ms
Partition 0 [0, 9) has cost: 184.741 ms
Partition 1 [9, 17) has cost: 170.613 ms
Partition 2 [17, 25) has cost: 170.613 ms
Partition 3 [25, 33) has cost: 175.014 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 50.825 ms
GPU 0, Compute+Comm Time: 42.935 ms, Bubble Time: 7.867 ms, Imbalance Overhead: 0.023 ms
GPU 1, Compute+Comm Time: 40.667 ms, Bubble Time: 7.883 ms, Imbalance Overhead: 2.275 ms
GPU 2, Compute+Comm Time: 40.667 ms, Bubble Time: 7.947 ms, Imbalance Overhead: 2.211 ms
GPU 3, Compute+Comm Time: 41.200 ms, Bubble Time: 8.087 ms, Imbalance Overhead: 1.538 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.173 ms
GPU 0, Compute+Comm Time: 74.635 ms, Bubble Time: 14.666 ms, Imbalance Overhead: 2.873 ms
GPU 1, Compute+Comm Time: 72.964 ms, Bubble Time: 14.400 ms, Imbalance Overhead: 4.810 ms
GPU 2, Compute+Comm Time: 72.964 ms, Bubble Time: 14.267 ms, Imbalance Overhead: 4.943 ms
GPU 3, Compute+Comm Time: 77.947 ms, Bubble Time: 14.226 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 150.148 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 355.354 ms
Partition 0 [0, 17) has cost: 355.354 ms
Partition 1 [17, 33) has cost: 345.627 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 75.105 ms
GPU 0, Compute+Comm Time: 66.109 ms, Bubble Time: 8.426 ms, Imbalance Overhead: 0.570 ms
GPU 1, Compute+Comm Time: 65.206 ms, Bubble Time: 7.839 ms, Imbalance Overhead: 2.060 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 113.426 ms
GPU 0, Compute+Comm Time: 98.360 ms, Bubble Time: 12.094 ms, Imbalance Overhead: 2.973 ms
GPU 1, Compute+Comm Time: 100.095 ms, Bubble Time: 12.630 ms, Imbalance Overhead: 0.702 ms
    The estimated cost with 4 DP ways is 197.958 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 700.981 ms
Partition 0 [0, 33) has cost: 700.981 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 218.790 ms
GPU 0, Compute+Comm Time: 218.790 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 252.693 ms
GPU 0, Compute+Comm Time: 252.693 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 495.057 ms

*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 3, starting model training...
Num Stages: 8 / 8
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 13752
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ************ Start Scheduling the Tasks in a Pipelined Fashion ******

*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.0546
	Epoch 50:	Loss 1.6083
	Epoch 75:	Loss 1.2523
Node 1, Pre/Post-Pipelining: 0.892 / 2.842 ms, Bubble: 24.450 ms, Compute: 81.191 ms, Comm: 19.771 ms, Imbalance: 14.623 ms
	Epoch 100:	Loss 0.9978
Node 2, Pre/Post-Pipelining: 0.871 / 3.010 ms, Bubble: 24.389 ms, Compute: 81.142 ms, Comm: 22.994 ms, Imbalance: 11.477 ms
Node 3, Pre/Post-Pipelining: 0.871 / 3.050 ms, Bubble: 23.696 ms, Compute: 79.346 ms, Comm: 25.137 ms, Imbalance: 11.615 ms
Node 4, Pre/Post-Pipelining: 0.892 / 3.001 ms, Bubble: 23.990 ms, Compute: 81.492 ms, Comm: 25.313 ms, Imbalance: 9.327 ms
Node 5, Pre/Post-Pipelining: 0.891 / 3.014 ms, Bubble: 24.443 ms, Compute: 81.148 ms, Comm: 23.160 ms, Imbalance: 11.253 ms
Node 6, Pre/Post-Pipelining: 0.858 / 3.147 ms, Bubble: 24.533 ms, Compute: 81.677 ms, Comm: 19.631 ms, Imbalance: 14.012 ms
Node 7, Pre/Post-Pipelining: 0.856 / 4.446 ms, Bubble: 23.935 ms, Compute: 85.354 ms, Comm: 14.698 ms, Imbalance: 14.554 ms
Node 0, Pre/Post-Pipelining: 0.756 / 3.665 ms, Bubble: 23.901 ms, Compute: 95.006 ms, Comm: 15.284 ms, Imbalance: 4.890 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.756 ms
Cluster-Wide Average, Post-Pipelining Overhead: 3.665 ms
Cluster-Wide Average, Bubble: 23.901 ms
Cluster-Wide Average, Compute: 95.006 ms
Cluster-Wide Average, Communication: 15.284 ms
Cluster-Wide Average, Imbalance: 4.890 ms
Node 4, GPU memory consumption: 3.657 GB
Node 0, GPU memory consumption: 4.880 GB
Node 5, GPU memory consumption: 3.680 GB
Node 1, GPU memory consumption: 3.680 GB
Node 6, GPU memory consumption: 3.680 GB
Node 3, GPU memory consumption: 3.657 GB
Node 7, GPU memory consumption: 3.659 GB
Node 2, GPU memory consumption: 3.680 GB
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 4,  per-epoch time: 0.144150 s---------------
------------------------node id 5,  per-epoch time: 0.144149 s---------------
------------------------node id 0,  per-epoch time: 0.144147 s---------------
------------------------node id 6,  per-epoch time: 0.144147 s---------------
------------------------node id 1,  per-epoch time: 0.144147 s---------------
------------------------node id 7,  per-epoch time: 0.144147 s---------------
------------------------node id 2,  per-epoch time: 0.144145 s---------------
------------------------node id 3,  per-epoch time: 0.144147 s---------------
************ Profiling Results ************
	Bubble: 65.132852 (ms) (43.49 percentage)
	Compute: 79.760157 (ms) (53.26 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.870863 (ms) (3.25 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.469 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.128 GB
	Total communication (cluster-wide, per-epoch): 1.597 GB
	Aggregated layer-level communication throughput: 608.115 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0260
Epoch to reach the target acc: 0
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
