gnerv1
Sun Aug  6 18:34:36 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   27C    P8    19W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   27C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 25%] Built target context
[ 25%] Built target core
[ 77%] Built target cudahelp
[ 88%] Built target OSDI2023_MULTI_NODES_gcn
[ 88%] Built target OSDI2023_MULTI_NODES_graphsage
[ 88%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target estimate_comm_volume
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...Building the CSR structure...

Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.013 seconds.
        It takes 0.011 seconds.
        It takes 0.011 seconds.
Building the Feature Vector...
        It takes 0.010 seconds.
        It takes 0.014 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.009 seconds.
        It takes 0.012 seconds.
Building the Feature Vector...
        It takes 0.011 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.084 seconds.
        It takes 0.084 seconds.
        It takes 0.079 seconds.
        It takes 0.075 seconds.
Building the Label Vector...Building the Label Vector...
Building the Label Vector...

Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.002 seconds.
        It takes 0.002 seconds.
        It takes 0.002 seconds.
        It takes 0.091 seconds.
        It takes 0.084 seconds.
        It takes 0.087 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.085 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.002 seconds.
        It takes 0.002 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/amazon_computers/32_parts
The number of GCNII layers: 32
The number of hidden units: 1024
The number of training epoches: 200
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 10
Number of feature dimensions: 767
Number of vertices: 13752
Number of GPUs: 8
        It takes 0.002 seconds.
        It takes 0.001 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
13752, 505474, 505474
Number of vertices per chunk: 430
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
13752, 505474, 505474
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Number of vertices per chunk: 430
13752, 505474, 505474
Number of vertices per chunk: 430
13752, 505474, 505474
Number of vertices per chunk: 430
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
13752, 505474, 505474
Number of vertices per chunk: 430
train nodes 200, valid nodes 500, test nodes 1000
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 421) 1-[421, 915) 2-[915, 1336) 3-[1336, 1728) 4-[1728, 2108) 5-[2108, 2487) 6-[2487, 2901) 7-[2901, 3403) 8-[3403, 3776) ... 31-[13352, 13752)
13752, 505474, 505474
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 430
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
13752, 505474, 505474
Number of vertices per chunk: 430
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
13752, 505474, 505474
Number of vertices per chunk: 430
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.047 Gbps (per GPU), 448.375 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.794 Gbps (per GPU), 446.354 Gbps (aggregated)
The layer-level communication performance: 55.788 Gbps (per GPU), 446.300 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.573 Gbps (per GPU), 444.587 Gbps (aggregated)
The layer-level communication performance: 55.548 Gbps (per GPU), 444.382 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.354 Gbps (per GPU), 442.833 Gbps (aggregated)
The layer-level communication performance: 55.312 Gbps (per GPU), 442.500 Gbps (aggregated)
The layer-level communication performance: 55.286 Gbps (per GPU), 442.290 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.442 Gbps (per GPU), 1243.540 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.463 Gbps (per GPU), 1243.701 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.445 Gbps (per GPU), 1243.563 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.454 Gbps (per GPU), 1243.632 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.445 Gbps (per GPU), 1243.563 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.448 Gbps (per GPU), 1243.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.448 Gbps (per GPU), 1243.586 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.460 Gbps (per GPU), 1243.678 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.787 Gbps (per GPU), 798.294 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.784 Gbps (per GPU), 798.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.789 Gbps (per GPU), 798.313 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.769 Gbps (per GPU), 798.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.781 Gbps (per GPU), 798.250 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.776 Gbps (per GPU), 798.205 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.788 Gbps (per GPU), 798.306 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.773 Gbps (per GPU), 798.187 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 38.165 Gbps (per GPU), 305.318 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.162 Gbps (per GPU), 305.299 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.161 Gbps (per GPU), 305.290 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.161 Gbps (per GPU), 305.285 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.159 Gbps (per GPU), 305.275 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.158 Gbps (per GPU), 305.264 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.158 Gbps (per GPU), 305.266 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 38.154 Gbps (per GPU), 305.232 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.43ms  0.66ms  0.79ms  1.86  0.42K  0.02M
 chk_1  0.44ms  0.66ms  0.79ms  1.81  0.49K  0.01M
 chk_2  0.42ms  0.66ms  0.79ms  1.87  0.42K  0.02M
 chk_3  0.42ms  0.65ms  0.78ms  1.85  0.39K  0.02M
 chk_4  0.43ms  0.66ms  0.79ms  1.81  0.38K  0.02M
 chk_5  0.43ms  0.65ms  0.78ms  1.80  0.38K  0.02M
 chk_6  0.42ms  0.65ms  0.78ms  1.87  0.41K  0.02M
 chk_7  0.44ms  0.66ms  0.79ms  1.78  0.50K  0.01M
 chk_8  0.43ms  0.70ms  0.82ms  1.89  0.37K  0.02M
 chk_9  0.43ms  0.65ms  0.78ms  1.84  0.43K  0.01M
chk_10  0.44ms  0.65ms  0.79ms  1.81  0.47K  0.01M
chk_11  0.43ms  0.64ms  0.78ms  1.83  0.43K  0.01M
chk_12  0.42ms  0.65ms  0.78ms  1.87  0.35K  0.02M
chk_13  0.43ms  0.65ms  0.79ms  1.84  0.44K  0.02M
chk_14  0.44ms  0.66ms  0.79ms  1.81  0.48K  0.01M
chk_15  0.43ms  0.69ms  0.82ms  1.93  0.43K  0.01M
chk_16  0.42ms  0.66ms  0.80ms  1.89  0.41K  0.02M
chk_17  0.43ms  0.66ms  0.79ms  1.81  0.38K  0.02M
chk_18  0.44ms  0.67ms  0.80ms  1.82  0.51K  0.01M
chk_19  0.42ms  0.65ms  0.79ms  1.86  0.42K  0.02M
chk_20  0.44ms  0.65ms  0.79ms  1.80  0.48K  0.01M
chk_21  0.43ms  0.66ms  0.79ms  1.83  0.45K  0.01M
chk_22  0.42ms  0.67ms  0.81ms  1.92  0.41K  0.02M
chk_23  0.42ms  0.67ms  0.80ms  1.90  0.40K  0.02M
chk_24  0.42ms  0.65ms  0.79ms  1.89  0.35K  0.02M
chk_25  0.42ms  0.68ms  0.82ms  1.94  0.41K  0.02M
chk_26  0.42ms  0.65ms  0.79ms  1.89  0.40K  0.02M
chk_27  0.42ms  0.65ms  0.78ms  1.85  0.42K  0.02M
chk_28  0.44ms  0.65ms  0.79ms  1.80  0.49K  0.01M
chk_29  0.44ms  0.66ms  0.80ms  1.81  0.50K  0.01M
chk_30  0.44ms  0.67ms  0.79ms  1.79  0.50K  0.01M
chk_31  0.42ms  0.67ms  0.78ms  1.88  0.40K  0.02M
   Avg  0.43  0.66  0.79
   Max  0.44  0.70  0.82
   Min  0.42  0.64  0.78
 Ratio  1.06  1.09  1.06
   Var  0.00  0.00  0.00
Profiling takes 0.817 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 98.144 ms
Partition 0 [0, 5) has cost: 98.144 ms
Partition 1 [5, 9) has cost: 84.418 ms
Partition 2 [9, 13) has cost: 84.418 ms
Partition 3 [13, 17) has cost: 84.418 ms
Partition 4 [17, 21) has cost: 84.418 ms
Partition 5 [21, 25) has cost: 84.418 ms
Partition 6 [25, 29) has cost: 84.418 ms
Partition 7 [29, 33) has cost: 88.669 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 46.547 ms
GPU 0, Compute+Comm Time: 38.684 ms, Bubble Time: 7.862 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 34.377 ms, Bubble Time: 7.938 ms, Imbalance Overhead: 4.232 ms
GPU 2, Compute+Comm Time: 34.377 ms, Bubble Time: 8.069 ms, Imbalance Overhead: 4.101 ms
GPU 3, Compute+Comm Time: 34.377 ms, Bubble Time: 8.164 ms, Imbalance Overhead: 4.005 ms
GPU 4, Compute+Comm Time: 34.377 ms, Bubble Time: 8.215 ms, Imbalance Overhead: 3.954 ms
GPU 5, Compute+Comm Time: 34.377 ms, Bubble Time: 8.253 ms, Imbalance Overhead: 3.916 ms
GPU 6, Compute+Comm Time: 34.377 ms, Bubble Time: 8.292 ms, Imbalance Overhead: 3.878 ms
GPU 7, Compute+Comm Time: 35.401 ms, Bubble Time: 8.359 ms, Imbalance Overhead: 2.786 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 90.878 ms
GPU 0, Compute+Comm Time: 69.348 ms, Bubble Time: 16.348 ms, Imbalance Overhead: 5.182 ms
GPU 1, Compute+Comm Time: 66.120 ms, Bubble Time: 16.157 ms, Imbalance Overhead: 8.601 ms
GPU 2, Compute+Comm Time: 66.120 ms, Bubble Time: 16.088 ms, Imbalance Overhead: 8.669 ms
GPU 3, Compute+Comm Time: 66.120 ms, Bubble Time: 15.980 ms, Imbalance Overhead: 8.778 ms
GPU 4, Compute+Comm Time: 66.120 ms, Bubble Time: 15.891 ms, Imbalance Overhead: 8.867 ms
GPU 5, Compute+Comm Time: 66.120 ms, Bubble Time: 15.714 ms, Imbalance Overhead: 9.044 ms
GPU 6, Compute+Comm Time: 66.120 ms, Bubble Time: 15.472 ms, Imbalance Overhead: 9.286 ms
GPU 7, Compute+Comm Time: 75.540 ms, Bubble Time: 15.338 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 144.296 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 182.562 ms
Partition 0 [0, 9) has cost: 182.562 ms
Partition 1 [9, 17) has cost: 168.835 ms
Partition 2 [17, 25) has cost: 168.835 ms
Partition 3 [25, 33) has cost: 173.087 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 50.921 ms
GPU 0, Compute+Comm Time: 42.950 ms, Bubble Time: 7.910 ms, Imbalance Overhead: 0.061 ms
GPU 1, Compute+Comm Time: 40.782 ms, Bubble Time: 7.912 ms, Imbalance Overhead: 2.227 ms
GPU 2, Compute+Comm Time: 40.782 ms, Bubble Time: 7.966 ms, Imbalance Overhead: 2.173 ms
GPU 3, Compute+Comm Time: 41.303 ms, Bubble Time: 8.096 ms, Imbalance Overhead: 1.522 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.023 ms
GPU 0, Compute+Comm Time: 74.572 ms, Bubble Time: 14.627 ms, Imbalance Overhead: 2.823 ms
GPU 1, Compute+Comm Time: 72.957 ms, Bubble Time: 14.365 ms, Imbalance Overhead: 4.700 ms
GPU 2, Compute+Comm Time: 72.957 ms, Bubble Time: 14.333 ms, Imbalance Overhead: 4.732 ms
GPU 3, Compute+Comm Time: 77.728 ms, Bubble Time: 14.295 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 2 DP ways is 150.091 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 351.397 ms
Partition 0 [0, 17) has cost: 351.397 ms
Partition 1 [17, 33) has cost: 341.922 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 75.323 ms
GPU 0, Compute+Comm Time: 66.230 ms, Bubble Time: 8.486 ms, Imbalance Overhead: 0.607 ms
GPU 1, Compute+Comm Time: 65.398 ms, Bubble Time: 7.825 ms, Imbalance Overhead: 2.100 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 113.453 ms
GPU 0, Compute+Comm Time: 98.471 ms, Bubble Time: 12.240 ms, Imbalance Overhead: 2.742 ms
GPU 1, Compute+Comm Time: 100.078 ms, Bubble Time: 12.620 ms, Imbalance Overhead: 0.754 ms
    The estimated cost with 4 DP ways is 198.214 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 693.319 ms
Partition 0 [0, 33) has cost: 693.319 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 220.137 ms
GPU 0, Compute+Comm Time: 220.137 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 254.095 ms
GPU 0, Compute+Comm Time: 254.095 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 497.943 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 13752
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.0542	TrainAcc 0.4200	ValidAcc 0.1740	TestAcc 0.1540	BestValid 0.1740
	Epoch 50:	Loss 1.5998	TrainAcc 0.8100	ValidAcc 0.4780	TestAcc 0.4880	BestValid 0.4780
	Epoch 75:	Loss 1.2649	TrainAcc 0.8400	ValidAcc 0.7320	TestAcc 0.6910	BestValid 0.7320
	Epoch 100:	Loss 1.0955	TrainAcc 0.8750	ValidAcc 0.7260	TestAcc 0.6570	BestValid 0.7320
	Epoch 125:	Loss 1.3013	TrainAcc 0.9300	ValidAcc 0.7740	TestAcc 0.7390	BestValid 0.7740
	Epoch 150:	Loss 0.9385	TrainAcc 0.9650	ValidAcc 0.8020	TestAcc 0.7600	BestValid 0.8020
	Epoch 175:	Loss 0.8150	TrainAcc 0.9450	ValidAcc 0.8160	TestAcc 0.8010	BestValid 0.8160
	Epoch 200:	Loss 0.8284	TrainAcc 0.9150	ValidAcc 0.8360	TestAcc 0.8250	BestValid 0.8360
Node 2, Pre/Post-Pipelining: 0.802 / 3.206 ms, Bubble: 24.082 ms, Compute: 82.220 ms, Comm: 23.080 ms, Imbalance: 10.809 ms
Node 4, Pre/Post-Pipelining: 0.882 / 3.060 ms, Bubble: 23.741 ms, Compute: 81.882 ms, Comm: 25.669 ms, Imbalance: 9.048 ms
Node 7, Pre/Post-Pipelining: 0.878 / 4.322 ms, Bubble: 23.984 ms, Compute: 84.614 ms, Comm: 14.723 ms, Imbalance: 15.625 ms
Node 6, Pre/Post-Pipelining: 0.890 / 3.073 ms, Bubble: 24.478 ms, Compute: 82.487 ms, Comm: 19.732 ms, Imbalance: 13.503 ms
Node 0, Pre/Post-Pipelining: 0.802 / 3.328 ms, Bubble: 23.918 ms, Compute: 95.003 ms, Comm: 15.297 ms, Imbalance: 5.372 ms
Node 1, Pre/Post-Pipelining: 0.783 / 3.271 ms, Bubble: 23.908 ms, Compute: 81.707 ms, Comm: 19.847 ms, Imbalance: 14.527 ms
Node 3, Pre/Post-Pipelining: 0.890 / 2.996 ms, Bubble: 23.528 ms, Compute: 79.650 ms, Comm: 25.314 ms, Imbalance: 11.608 ms
Node 5, Pre/Post-Pipelining: 0.887 / 3.043 ms, Bubble: 24.232 ms, Compute: 81.257 ms, Comm: 23.331 ms, Imbalance: 11.423 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.802 ms
Cluster-Wide Average, Post-Pipelining Overhead: 3.328 ms
Cluster-Wide Average, Bubble: 23.918 ms
Cluster-Wide Average, Compute: 95.003 ms
Cluster-Wide Average, Communication: 15.297 ms
Cluster-Wide Average, Imbalance: 5.372 ms
Node 0, GPU memory consumption: 4.880 GB
Node 4, GPU memory consumption: 3.657 GB
Node 2, GPU memory consumption: 3.680 GB
Node 6, GPU memory consumption: 3.680 GB
Node 3, GPU memory consumption: 3.657 GB
Node 7, GPU memory consumption: 3.659 GB
Node 1, GPU memory consumption: 3.680 GB
Node 5, GPU memory consumption: 3.680 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.430261 s---------------
------------------------node id 4,  per-epoch time: 0.430260 s---------------
------------------------node id 1,  per-epoch time: 0.430262 s---------------
------------------------node id 5,  per-epoch time: 0.430263 s---------------
------------------------node id 2,  per-epoch time: 0.430261 s---------------
------------------------node id 6,  per-epoch time: 0.430262 s---------------
------------------------node id 3,  per-epoch time: 0.430263 s---------------
------------------------node id 7,  per-epoch time: 0.430261 s---------------
************ Profiling Results ************
	Bubble: 341.998280 (ms) (80.30 percentage)
	Compute: 79.031776 (ms) (18.56 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.884337 (ms) (1.15 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.469 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.128 GB
	Total communication (cluster-wide, per-epoch): 1.597 GB
	Aggregated layer-level communication throughput: 604.462 Gbps
Highest valid_acc: 0.8360
Target test_acc: 0.8250
Epoch to reach the target acc: 199
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
