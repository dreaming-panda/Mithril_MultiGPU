gnerv1
Fri Aug  4 02:27:22 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   27C    P8    19W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   27C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 33%] Built target core
[ 33%] Built target context
[ 77%] Built target cudahelp
[ 88%] Built target OSDI2023_MULTI_NODES_graphsage
[ 97%] Built target estimate_comm_volume
[ 97%] Built target OSDI2023_MULTI_NODES_gcnii
[ 97%] Built target OSDI2023_MULTI_NODES_gcn
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.905 seconds.
Building the CSC structure...
        It takes 1.981 seconds.
Building the CSC structure...
        It takes 2.045 seconds.
Building the CSC structure...
        It takes 2.059 seconds.
Building the CSC structure...
        It takes 2.075 seconds.
Building the CSC structure...
        It takes 2.208 seconds.
Building the CSC structure...
        It takes 2.374 seconds.
Building the CSC structure...
        It takes 2.391 seconds.
Building the CSC structure...
        It takes 1.842 seconds.
        It takes 1.839 seconds.
        It takes 1.832 seconds.
        It takes 1.841 seconds.
        It takes 1.896 seconds.
        It takes 2.273 seconds.
        It takes 2.329 seconds.
Building the Feature Vector...
        It takes 2.367 seconds.
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.264 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
        It takes 0.277 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.040 seconds.
        It takes 0.290 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.041 seconds.
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.016 Gbps (per GPU), 472.127 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.732 Gbps (per GPU), 469.855 Gbps (aggregated)
The layer-level communication performance: 58.730 Gbps (per GPU), 469.844 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.507 Gbps (per GPU), 468.056 Gbps (aggregated)
The layer-level communication performance: 58.480 Gbps (per GPU), 467.837 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.276 Gbps (per GPU), 466.204 Gbps (aggregated)
The layer-level communication performance: 58.230 Gbps (per GPU), 465.838 Gbps (aggregated)
The layer-level communication performance: 58.201 Gbps (per GPU), 465.608 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 157.595 Gbps (per GPU), 1260.758 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.612 Gbps (per GPU), 1260.900 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.630 Gbps (per GPU), 1261.042 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.610 Gbps (per GPU), 1260.879 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.603 Gbps (per GPU), 1260.826 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.612 Gbps (per GPU), 1260.900 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.547 Gbps (per GPU), 1260.379 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 157.612 Gbps (per GPU), 1260.900 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.150 Gbps (per GPU), 801.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.151 Gbps (per GPU), 801.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.150 Gbps (per GPU), 801.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.151 Gbps (per GPU), 801.204 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.150 Gbps (per GPU), 801.199 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.151 Gbps (per GPU), 801.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.150 Gbps (per GPU), 801.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.152 Gbps (per GPU), 801.217 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.042 Gbps (per GPU), 264.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.042 Gbps (per GPU), 264.337 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.039 Gbps (per GPU), 264.315 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.041 Gbps (per GPU), 264.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.041 Gbps (per GPU), 264.329 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.042 Gbps (per GPU), 264.338 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.040 Gbps (per GPU), 264.318 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.042 Gbps (per GPU), 264.335 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.38ms  2.44ms  2.18ms  1.12  8.38K  3.53M
 chk_1  2.70ms  2.65ms  2.49ms  1.08  6.74K  3.60M
 chk_2  2.63ms  2.54ms  2.36ms  1.11  7.27K  3.53M
 chk_3  2.63ms  2.55ms  2.37ms  1.11  7.92K  3.61M
 chk_4  2.54ms  2.50ms  2.36ms  1.08  5.33K  3.68M
 chk_5  2.54ms  2.45ms  2.21ms  1.15 10.07K  3.45M
 chk_6  2.72ms  2.63ms  2.40ms  1.13  9.41K  3.48M
 chk_7  2.54ms  2.48ms  2.31ms  1.10  8.12K  3.60M
 chk_8  2.66ms  2.61ms  2.48ms  1.07  6.09K  3.64M
 chk_9  2.46ms  2.34ms  2.10ms  1.17 11.10K  3.38M
chk_10  2.70ms  2.64ms  2.51ms  1.07  5.67K  3.63M
chk_11  2.55ms  2.49ms  2.32ms  1.10  8.16K  3.54M
chk_12  2.78ms  2.71ms  2.54ms  1.10  7.24K  3.55M
chk_13  2.59ms  2.53ms  2.41ms  1.07  5.41K  3.68M
chk_14  2.85ms  2.79ms  2.61ms  1.09  7.14K  3.53M
chk_15  2.68ms  2.60ms  2.37ms  1.13  9.25K  3.49M
chk_16  2.47ms  2.47ms  2.34ms  1.06  4.78K  3.77M
chk_17  2.67ms  2.59ms  2.42ms  1.10  6.85K  3.60M
chk_18  2.46ms  2.40ms  2.23ms  1.10  7.47K  3.57M
chk_19  2.52ms  2.49ms  2.35ms  1.07  4.88K  3.75M
chk_20  2.52ms  2.47ms  2.30ms  1.09  7.00K  3.63M
chk_21  2.50ms  2.46ms  2.34ms  1.07  5.41K  3.68M
chk_22  2.73ms  2.60ms  2.36ms  1.16 11.07K  3.39M
chk_23  2.64ms  2.55ms  2.39ms  1.10  7.23K  3.64M
chk_24  2.66ms  2.55ms  2.33ms  1.14 10.13K  3.43M
chk_25  2.49ms  2.42ms  2.56ms  1.06  6.40K  3.57M
chk_26  2.69ms  2.63ms  2.47ms  1.09  5.78K  3.55M
chk_27  2.55ms  2.47ms  2.23ms  1.14  9.34K  3.48M
chk_28  2.86ms  2.81ms  2.64ms  1.08  6.37K  3.57M
chk_29  2.71ms  2.62ms  2.48ms  1.09  5.16K  3.78M
chk_30  2.57ms  2.56ms  2.36ms  1.09  5.44K  3.67M
chk_31  2.72ms  2.68ms  2.48ms  1.09  6.33K  3.63M
   Avg  2.61  2.55  2.38
   Max  2.86  2.81  2.64
   Min  2.38  2.34  2.10
 Ratio  1.20  1.20  1.25
   Var  0.01  0.01  0.01
Profiling takes 2.806 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 328.798 ms
Partition 0 [0, 4) has cost: 328.798 ms
Partition 1 [4, 8) has cost: 326.835 ms
Partition 2 [8, 12) has cost: 326.835 ms
Partition 3 [12, 16) has cost: 326.835 ms
Partition 4 [16, 20) has cost: 326.835 ms
Partition 5 [20, 24) has cost: 326.835 ms
Partition 6 [24, 28) has cost: 326.835 ms
Partition 7 [28, 32) has cost: 321.423 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 156.345 ms
GPU 0, Compute+Comm Time: 122.371 ms, Bubble Time: 27.881 ms, Imbalance Overhead: 6.093 ms
GPU 1, Compute+Comm Time: 121.020 ms, Bubble Time: 27.517 ms, Imbalance Overhead: 7.808 ms
GPU 2, Compute+Comm Time: 121.020 ms, Bubble Time: 27.340 ms, Imbalance Overhead: 7.985 ms
GPU 3, Compute+Comm Time: 121.020 ms, Bubble Time: 27.122 ms, Imbalance Overhead: 8.203 ms
GPU 4, Compute+Comm Time: 121.020 ms, Bubble Time: 26.964 ms, Imbalance Overhead: 8.361 ms
GPU 5, Compute+Comm Time: 121.020 ms, Bubble Time: 26.918 ms, Imbalance Overhead: 8.407 ms
GPU 6, Compute+Comm Time: 121.020 ms, Bubble Time: 26.873 ms, Imbalance Overhead: 8.452 ms
GPU 7, Compute+Comm Time: 119.080 ms, Bubble Time: 27.075 ms, Imbalance Overhead: 10.190 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 298.063 ms
GPU 0, Compute+Comm Time: 227.607 ms, Bubble Time: 51.831 ms, Imbalance Overhead: 18.625 ms
GPU 1, Compute+Comm Time: 231.079 ms, Bubble Time: 51.636 ms, Imbalance Overhead: 15.348 ms
GPU 2, Compute+Comm Time: 231.079 ms, Bubble Time: 51.692 ms, Imbalance Overhead: 15.292 ms
GPU 3, Compute+Comm Time: 231.079 ms, Bubble Time: 51.748 ms, Imbalance Overhead: 15.236 ms
GPU 4, Compute+Comm Time: 231.079 ms, Bubble Time: 52.019 ms, Imbalance Overhead: 14.965 ms
GPU 5, Compute+Comm Time: 231.079 ms, Bubble Time: 52.403 ms, Imbalance Overhead: 14.581 ms
GPU 6, Compute+Comm Time: 231.079 ms, Bubble Time: 52.771 ms, Imbalance Overhead: 14.213 ms
GPU 7, Compute+Comm Time: 231.691 ms, Bubble Time: 53.714 ms, Imbalance Overhead: 12.658 ms
The estimated cost of the whole pipeline: 477.129 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 655.634 ms
Partition 0 [0, 8) has cost: 655.634 ms
Partition 1 [8, 16) has cost: 653.670 ms
Partition 2 [16, 24) has cost: 653.670 ms
Partition 3 [24, 32) has cost: 648.258 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 163.634 ms
GPU 0, Compute+Comm Time: 133.863 ms, Bubble Time: 26.078 ms, Imbalance Overhead: 3.694 ms
GPU 1, Compute+Comm Time: 133.172 ms, Bubble Time: 25.333 ms, Imbalance Overhead: 5.129 ms
GPU 2, Compute+Comm Time: 133.172 ms, Bubble Time: 24.936 ms, Imbalance Overhead: 5.526 ms
GPU 3, Compute+Comm Time: 132.301 ms, Bubble Time: 24.567 ms, Imbalance Overhead: 6.767 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 300.193 ms
GPU 0, Compute+Comm Time: 243.376 ms, Bubble Time: 45.556 ms, Imbalance Overhead: 11.262 ms
GPU 1, Compute+Comm Time: 244.788 ms, Bubble Time: 45.883 ms, Imbalance Overhead: 9.523 ms
GPU 2, Compute+Comm Time: 244.788 ms, Bubble Time: 46.439 ms, Imbalance Overhead: 8.967 ms
GPU 3, Compute+Comm Time: 245.114 ms, Bubble Time: 48.068 ms, Imbalance Overhead: 7.012 ms
    The estimated cost with 2 DP ways is 487.019 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1309.304 ms
Partition 0 [0, 16) has cost: 1309.304 ms
Partition 1 [16, 32) has cost: 1301.929 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 212.373 ms
GPU 0, Compute+Comm Time: 184.292 ms, Bubble Time: 22.938 ms, Imbalance Overhead: 5.143 ms
GPU 1, Compute+Comm Time: 183.527 ms, Bubble Time: 23.293 ms, Imbalance Overhead: 5.552 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 342.421 ms
GPU 0, Compute+Comm Time: 298.414 ms, Bubble Time: 37.279 ms, Imbalance Overhead: 6.727 ms
GPU 1, Compute+Comm Time: 299.278 ms, Bubble Time: 37.346 ms, Imbalance Overhead: 5.797 ms
    The estimated cost with 4 DP ways is 582.534 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2611.233 ms
Partition 0 [0, 32) has cost: 2611.233 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 567.597 ms
GPU 0, Compute+Comm Time: 567.597 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 685.430 ms
GPU 0, Compute+Comm Time: 685.430 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1315.678 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 24)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [96, 120)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [24, 48)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [120, 144)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [48, 72)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [144, 168)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [72, 96)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [168, 190)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[96, 120)...
+++++++++ Node 0 initializing the weights for op[0, 24)...
+++++++++ Node 5 initializing the weights for op[120, 144)...
+++++++++ Node 1 initializing the weights for op[24, 48)...
+++++++++ Node 6 initializing the weights for op[144, 168)...
+++++++++ Node 2 initializing the weights for op[48, 72)...
+++++++++ Node 7 initializing the weights for op[168, 190)...
+++++++++ Node 3 initializing the weights for op[72, 96)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.6080	TrainAcc 0.4687	ValidAcc 0.4832	TestAcc 0.4833	BestValid 0.4832
	Epoch 50:	Loss 1.6590	TrainAcc 0.7111	ValidAcc 0.7342	TestAcc 0.7295	BestValid 0.7342
	Epoch 75:	Loss 1.0309	TrainAcc 0.8135	ValidAcc 0.8300	TestAcc 0.8249	BestValid 0.8300
	Epoch 100:	Loss 0.7319	TrainAcc 0.8512	ValidAcc 0.8636	TestAcc 0.8620	BestValid 0.8636
	Epoch 125:	Loss 0.5745	TrainAcc 0.8950	ValidAcc 0.9064	TestAcc 0.9042	BestValid 0.9064
	Epoch 150:	Loss 0.5233	TrainAcc 0.9047	ValidAcc 0.9145	TestAcc 0.9125	BestValid 0.9145
	Epoch 175:	Loss 0.4879	TrainAcc 0.9082	ValidAcc 0.9175	TestAcc 0.9164	BestValid 0.9175
	Epoch 200:	Loss 0.4647	TrainAcc 0.9099	ValidAcc 0.9194	TestAcc 0.9179	BestValid 0.9194
	Epoch 225:	Loss 0.4391	TrainAcc 0.9135	ValidAcc 0.9235	TestAcc 0.9214	BestValid 0.9235
	Epoch 250:	Loss 0.4263	TrainAcc 0.9158	ValidAcc 0.9229	TestAcc 0.9230	BestValid 0.9235
	Epoch 275:	Loss 0.4129	TrainAcc 0.9179	ValidAcc 0.9243	TestAcc 0.9249	BestValid 0.9243
	Epoch 300:	Loss 0.4039	TrainAcc 0.9194	ValidAcc 0.9258	TestAcc 0.9257	BestValid 0.9258
	Epoch 325:	Loss 0.3948	TrainAcc 0.9210	ValidAcc 0.9272	TestAcc 0.9274	BestValid 0.9272
	Epoch 350:	Loss 0.3856	TrainAcc 0.9230	ValidAcc 0.9283	TestAcc 0.9292	BestValid 0.9283
	Epoch 375:	Loss 0.3799	TrainAcc 0.9237	ValidAcc 0.9285	TestAcc 0.9297	BestValid 0.9285
	Epoch 400:	Loss 0.3733	TrainAcc 0.9247	ValidAcc 0.9294	TestAcc 0.9301	BestValid 0.9294
	Epoch 425:	Loss 0.3661	TrainAcc 0.9259	ValidAcc 0.9308	TestAcc 0.9312	BestValid 0.9308
	Epoch 450:	Loss 0.3616	TrainAcc 0.9269	ValidAcc 0.9315	TestAcc 0.9322	BestValid 0.9315
	Epoch 475:	Loss 0.3574	TrainAcc 0.9280	ValidAcc 0.9325	TestAcc 0.9327	BestValid 0.9325
	Epoch 500:	Loss 0.3530	TrainAcc 0.9286	ValidAcc 0.9332	TestAcc 0.9336	BestValid 0.9332
	Epoch 525:	Loss 0.3504	TrainAcc 0.9294	ValidAcc 0.9337	TestAcc 0.9344	BestValid 0.9337
	Epoch 550:	Loss 0.3445	TrainAcc 0.9295	ValidAcc 0.9334	TestAcc 0.9338	BestValid 0.9337
	Epoch 575:	Loss 0.3416	TrainAcc 0.9307	ValidAcc 0.9342	TestAcc 0.9350	BestValid 0.9342
	Epoch 600:	Loss 0.3389	TrainAcc 0.9310	ValidAcc 0.9347	TestAcc 0.9359	BestValid 0.9347
	Epoch 625:	Loss 0.3363	TrainAcc 0.9315	ValidAcc 0.9360	TestAcc 0.9365	BestValid 0.9360
	Epoch 650:	Loss 0.3337	TrainAcc 0.9321	ValidAcc 0.9360	TestAcc 0.9365	BestValid 0.9360
	Epoch 675:	Loss 0.3303	TrainAcc 0.9326	ValidAcc 0.9365	TestAcc 0.9372	BestValid 0.9365
	Epoch 700:	Loss 0.3289	TrainAcc 0.9331	ValidAcc 0.9365	TestAcc 0.9373	BestValid 0.9365
	Epoch 725:	Loss 0.3256	TrainAcc 0.9335	ValidAcc 0.9366	TestAcc 0.9375	BestValid 0.9366
	Epoch 750:	Loss 0.3232	TrainAcc 0.9338	ValidAcc 0.9375	TestAcc 0.9382	BestValid 0.9375
	Epoch 775:	Loss 0.3216	TrainAcc 0.9346	ValidAcc 0.9378	TestAcc 0.9386	BestValid 0.9378
	Epoch 800:	Loss 0.3187	TrainAcc 0.9348	ValidAcc 0.9378	TestAcc 0.9387	BestValid 0.9378
	Epoch 825:	Loss 0.3168	TrainAcc 0.9353	ValidAcc 0.9380	TestAcc 0.9389	BestValid 0.9380
	Epoch 850:	Loss 0.3158	TrainAcc 0.9354	ValidAcc 0.9377	TestAcc 0.9392	BestValid 0.9380
	Epoch 875:	Loss 0.3136	TrainAcc 0.9356	ValidAcc 0.9383	TestAcc 0.9399	BestValid 0.9383
	Epoch 900:	Loss 0.3115	TrainAcc 0.9357	ValidAcc 0.9384	TestAcc 0.9397	BestValid 0.9384
	Epoch 925:	Loss 0.3097	TrainAcc 0.9357	ValidAcc 0.9387	TestAcc 0.9401	BestValid 0.9387
	Epoch 950:	Loss 0.3087	TrainAcc 0.9364	ValidAcc 0.9382	TestAcc 0.9401	BestValid 0.9387
	Epoch 975:	Loss 0.3075	TrainAcc 0.9373	ValidAcc 0.9394	TestAcc 0.9407	BestValid 0.9394
	Epoch 1000:	Loss 0.3061	TrainAcc 0.9366	ValidAcc 0.9389	TestAcc 0.9404	BestValid 0.9394
	Epoch 1025:	Loss 0.3036	TrainAcc 0.9376	ValidAcc 0.9389	TestAcc 0.9406	BestValid 0.9394
	Epoch 1050:	Loss 0.3030	TrainAcc 0.9377	ValidAcc 0.9388	TestAcc 0.9408	BestValid 0.9394
	Epoch 1075:	Loss 0.3015	TrainAcc 0.9379	ValidAcc 0.9395	TestAcc 0.9410	BestValid 0.9395
	Epoch 1100:	Loss 0.2992	TrainAcc 0.9381	ValidAcc 0.9394	TestAcc 0.9417	BestValid 0.9395
	Epoch 1125:	Loss 0.2993	TrainAcc 0.9382	ValidAcc 0.9402	TestAcc 0.9417	BestValid 0.9402
	Epoch 1150:	Loss 0.2981	TrainAcc 0.9386	ValidAcc 0.9400	TestAcc 0.9418	BestValid 0.9402
	Epoch 1175:	Loss 0.2957	TrainAcc 0.9384	ValidAcc 0.9400	TestAcc 0.9415	BestValid 0.9402
	Epoch 1200:	Loss 0.2950	TrainAcc 0.9392	ValidAcc 0.9404	TestAcc 0.9423	BestValid 0.9404
	Epoch 1225:	Loss 0.2928	TrainAcc 0.9392	ValidAcc 0.9406	TestAcc 0.9421	BestValid 0.9406
	Epoch 1250:	Loss 0.2918	TrainAcc 0.9396	ValidAcc 0.9406	TestAcc 0.9423	BestValid 0.9406
	Epoch 1275:	Loss 0.2908	TrainAcc 0.9396	ValidAcc 0.9415	TestAcc 0.9425	BestValid 0.9415
	Epoch 1300:	Loss 0.2898	TrainAcc 0.9396	ValidAcc 0.9405	TestAcc 0.9419	BestValid 0.9415
	Epoch 1325:	Loss 0.2892	TrainAcc 0.9400	ValidAcc 0.9402	TestAcc 0.9422	BestValid 0.9415
	Epoch 1350:	Loss 0.2875	TrainAcc 0.9402	ValidAcc 0.9407	TestAcc 0.9427	BestValid 0.9415
	Epoch 1375:	Loss 0.2870	TrainAcc 0.9407	ValidAcc 0.9409	TestAcc 0.9427	BestValid 0.9415
	Epoch 1400:	Loss 0.2861	TrainAcc 0.9407	ValidAcc 0.9410	TestAcc 0.9432	BestValid 0.9415
	Epoch 1425:	Loss 0.2851	TrainAcc 0.9401	ValidAcc 0.9411	TestAcc 0.9425	BestValid 0.9415
	Epoch 1450:	Loss 0.2842	TrainAcc 0.9410	ValidAcc 0.9413	TestAcc 0.9429	BestValid 0.9415
	Epoch 1475:	Loss 0.2824	TrainAcc 0.9414	ValidAcc 0.9414	TestAcc 0.9429	BestValid 0.9415
	Epoch 1500:	Loss 0.2810	TrainAcc 0.9416	ValidAcc 0.9420	TestAcc 0.9435	BestValid 0.9420
	Epoch 1525:	Loss 0.2806	TrainAcc 0.9416	ValidAcc 0.9418	TestAcc 0.9431	BestValid 0.9420
	Epoch 1550:	Loss 0.2800	TrainAcc 0.9415	ValidAcc 0.9415	TestAcc 0.9430	BestValid 0.9420
	Epoch 1575:	Loss 0.2793	TrainAcc 0.9416	ValidAcc 0.9413	TestAcc 0.9432	BestValid 0.9420
	Epoch 1600:	Loss 0.2777	TrainAcc 0.9418	ValidAcc 0.9415	TestAcc 0.9434	BestValid 0.9420
	Epoch 1625:	Loss 0.2781	TrainAcc 0.9417	ValidAcc 0.9421	TestAcc 0.9438	BestValid 0.9421
	Epoch 1650:	Loss 0.2773	TrainAcc 0.9423	ValidAcc 0.9424	TestAcc 0.9439	BestValid 0.9424
	Epoch 1675:	Loss 0.2754	TrainAcc 0.9425	ValidAcc 0.9415	TestAcc 0.9439	BestValid 0.9424
	Epoch 1700:	Loss 0.2757	TrainAcc 0.9429	ValidAcc 0.9427	TestAcc 0.9437	BestValid 0.9427
	Epoch 1725:	Loss 0.2742	TrainAcc 0.9426	ValidAcc 0.9423	TestAcc 0.9441	BestValid 0.9427
	Epoch 1750:	Loss 0.2730	TrainAcc 0.9428	ValidAcc 0.9430	TestAcc 0.9443	BestValid 0.9430
	Epoch 1775:	Loss 0.2729	TrainAcc 0.9432	ValidAcc 0.9427	TestAcc 0.9442	BestValid 0.9430
	Epoch 1800:	Loss 0.2721	TrainAcc 0.9431	ValidAcc 0.9428	TestAcc 0.9444	BestValid 0.9430
	Epoch 1825:	Loss 0.2713	TrainAcc 0.9425	ValidAcc 0.9422	TestAcc 0.9441	BestValid 0.9430
	Epoch 1850:	Loss 0.2701	TrainAcc 0.9434	ValidAcc 0.9435	TestAcc 0.9445	BestValid 0.9435
	Epoch 1875:	Loss 0.2699	TrainAcc 0.9435	ValidAcc 0.9434	TestAcc 0.9442	BestValid 0.9435
	Epoch 1900:	Loss 0.2693	TrainAcc 0.9438	ValidAcc 0.9434	TestAcc 0.9447	BestValid 0.9435
	Epoch 1925:	Loss 0.2681	TrainAcc 0.9426	ValidAcc 0.9419	TestAcc 0.9437	BestValid 0.9435
	Epoch 1950:	Loss 0.2675	TrainAcc 0.9433	ValidAcc 0.9436	TestAcc 0.9443	BestValid 0.9436
	Epoch 1975:	Loss 0.2672	TrainAcc 0.9440	ValidAcc 0.9431	TestAcc 0.9449	BestValid 0.9436
	Epoch 2000:	Loss 0.2657	TrainAcc 0.9445	ValidAcc 0.9433	TestAcc 0.9449	BestValid 0.9436
	Epoch 2025:	Loss 0.2654	TrainAcc 0.9441	ValidAcc 0.9435	TestAcc 0.9449	BestValid 0.9436
	Epoch 2050:	Loss 0.2650	TrainAcc 0.9446	ValidAcc 0.9437	TestAcc 0.9451	BestValid 0.9437
	Epoch 2075:	Loss 0.2641	TrainAcc 0.9449	ValidAcc 0.9443	TestAcc 0.9452	BestValid 0.9443
	Epoch 2100:	Loss 0.2627	TrainAcc 0.9451	ValidAcc 0.9442	TestAcc 0.9455	BestValid 0.9443
	Epoch 2125:	Loss 0.2630	TrainAcc 0.9440	ValidAcc 0.9431	TestAcc 0.9449	BestValid 0.9443
	Epoch 2150:	Loss 0.2626	TrainAcc 0.9448	ValidAcc 0.9435	TestAcc 0.9454	BestValid 0.9443
	Epoch 2175:	Loss 0.2626	TrainAcc 0.9449	ValidAcc 0.9439	TestAcc 0.9449	BestValid 0.9443
	Epoch 2200:	Loss 0.2604	TrainAcc 0.9448	ValidAcc 0.9435	TestAcc 0.9451	BestValid 0.9443
	Epoch 2225:	Loss 0.2595	TrainAcc 0.9449	ValidAcc 0.9432	TestAcc 0.9445	BestValid 0.9443
	Epoch 2250:	Loss 0.2598	TrainAcc 0.9454	ValidAcc 0.9444	TestAcc 0.9457	BestValid 0.9444
	Epoch 2275:	Loss 0.2598	TrainAcc 0.9459	ValidAcc 0.9441	TestAcc 0.9454	BestValid 0.9444
	Epoch 2300:	Loss 0.2580	TrainAcc 0.9450	ValidAcc 0.9444	TestAcc 0.9455	BestValid 0.9444
	Epoch 2325:	Loss 0.2574	TrainAcc 0.9456	ValidAcc 0.9445	TestAcc 0.9455	BestValid 0.9445
	Epoch 2350:	Loss 0.2569	TrainAcc 0.9463	ValidAcc 0.9446	TestAcc 0.9459	BestValid 0.9446
	Epoch 2375:	Loss 0.2570	TrainAcc 0.9458	ValidAcc 0.9445	TestAcc 0.9457	BestValid 0.9446
	Epoch 2400:	Loss 0.2566	TrainAcc 0.9457	ValidAcc 0.9439	TestAcc 0.9452	BestValid 0.9446
	Epoch 2425:	Loss 0.2561	TrainAcc 0.9459	ValidAcc 0.9445	TestAcc 0.9459	BestValid 0.9446
	Epoch 2450:	Loss 0.2552	TrainAcc 0.9461	ValidAcc 0.9454	TestAcc 0.9461	BestValid 0.9454
	Epoch 2475:	Loss 0.2545	TrainAcc 0.9452	ValidAcc 0.9435	TestAcc 0.9443	BestValid 0.9454
	Epoch 2500:	Loss 0.2547	TrainAcc 0.9464	ValidAcc 0.9447	TestAcc 0.9462	BestValid 0.9454
	Epoch 2525:	Loss 0.2541	TrainAcc 0.9460	ValidAcc 0.9447	TestAcc 0.9451	BestValid 0.9454
	Epoch 2550:	Loss 0.2526	TrainAcc 0.9463	ValidAcc 0.9452	TestAcc 0.9460	BestValid 0.9454
	Epoch 2575:	Loss 0.2529	TrainAcc 0.9463	ValidAcc 0.9447	TestAcc 0.9460	BestValid 0.9454
	Epoch 2600:	Loss 0.2519	TrainAcc 0.9467	ValidAcc 0.9452	TestAcc 0.9460	BestValid 0.9454
	Epoch 2625:	Loss 0.2523	TrainAcc 0.9465	ValidAcc 0.9446	TestAcc 0.9460	BestValid 0.9454
	Epoch 2650:	Loss 0.2511	TrainAcc 0.9467	ValidAcc 0.9456	TestAcc 0.9459	BestValid 0.9456
	Epoch 2675:	Loss 0.2503	TrainAcc 0.9469	ValidAcc 0.9449	TestAcc 0.9462	BestValid 0.9456
	Epoch 2700:	Loss 0.2510	TrainAcc 0.9471	ValidAcc 0.9447	TestAcc 0.9464	BestValid 0.9456
	Epoch 2725:	Loss 0.2499	TrainAcc 0.9470	ValidAcc 0.9443	TestAcc 0.9458	BestValid 0.9456
	Epoch 2750:	Loss 0.2491	TrainAcc 0.9472	ValidAcc 0.9453	TestAcc 0.9463	BestValid 0.9456
	Epoch 2775:	Loss 0.2493	TrainAcc 0.9477	ValidAcc 0.9457	TestAcc 0.9461	BestValid 0.9457
	Epoch 2800:	Loss 0.2479	TrainAcc 0.9475	ValidAcc 0.9455	TestAcc 0.9460	BestValid 0.9457
	Epoch 2825:	Loss 0.2480	TrainAcc 0.9475	ValidAcc 0.9453	TestAcc 0.9463	BestValid 0.9457
	Epoch 2850:	Loss 0.2480	TrainAcc 0.9478	ValidAcc 0.9454	TestAcc 0.9464	BestValid 0.9457
	Epoch 2875:	Loss 0.2471	TrainAcc 0.9467	ValidAcc 0.9450	TestAcc 0.9458	BestValid 0.9457
	Epoch 2900:	Loss 0.2465	TrainAcc 0.9475	ValidAcc 0.9456	TestAcc 0.9462	BestValid 0.9457
	Epoch 2925:	Loss 0.2466	TrainAcc 0.9471	ValidAcc 0.9454	TestAcc 0.9463	BestValid 0.9457
	Epoch 2950:	Loss 0.2458	TrainAcc 0.9484	ValidAcc 0.9460	TestAcc 0.9468	BestValid 0.9460
	Epoch 2975:	Loss 0.2459	TrainAcc 0.9481	ValidAcc 0.9457	TestAcc 0.9467	BestValid 0.9460
	Epoch 3000:	Loss 0.2454	TrainAcc 0.9472	ValidAcc 0.9455	TestAcc 0.9462	BestValid 0.9460
	Epoch 3025:	Loss 0.2448	TrainAcc 0.9457	ValidAcc 0.9444	TestAcc 0.9449	BestValid 0.9460
	Epoch 3050:	Loss 0.2451	TrainAcc 0.9477	ValidAcc 0.9453	TestAcc 0.9463	BestValid 0.9460
	Epoch 3075:	Loss 0.2440	TrainAcc 0.9485	ValidAcc 0.9464	TestAcc 0.9468	BestValid 0.9464
	Epoch 3100:	Loss 0.2438	TrainAcc 0.9475	ValidAcc 0.9460	TestAcc 0.9466	BestValid 0.9464
	Epoch 3125:	Loss 0.2426	TrainAcc 0.9471	ValidAcc 0.9455	TestAcc 0.9461	BestValid 0.9464
	Epoch 3150:	Loss 0.2427	TrainAcc 0.9485	ValidAcc 0.9463	TestAcc 0.9468	BestValid 0.9464
	Epoch 3175:	Loss 0.2420	TrainAcc 0.9486	ValidAcc 0.9456	TestAcc 0.9469	BestValid 0.9464
	Epoch 3200:	Loss 0.2422	TrainAcc 0.9486	ValidAcc 0.9458	TestAcc 0.9470	BestValid 0.9464
	Epoch 3225:	Loss 0.2422	TrainAcc 0.9480	ValidAcc 0.9457	TestAcc 0.9462	BestValid 0.9464
	Epoch 3250:	Loss 0.2412	TrainAcc 0.9489	ValidAcc 0.9457	TestAcc 0.9473	BestValid 0.9464
	Epoch 3275:	Loss 0.2413	TrainAcc 0.9489	ValidAcc 0.9460	TestAcc 0.9469	BestValid 0.9464
	Epoch 3300:	Loss 0.2406	TrainAcc 0.9478	ValidAcc 0.9454	TestAcc 0.9468	BestValid 0.9464
	Epoch 3325:	Loss 0.2396	TrainAcc 0.9486	ValidAcc 0.9458	TestAcc 0.9474	BestValid 0.9464
	Epoch 3350:	Loss 0.2399	TrainAcc 0.9489	ValidAcc 0.9463	TestAcc 0.9469	BestValid 0.9464
	Epoch 3375:	Loss 0.2390	TrainAcc 0.9487	ValidAcc 0.9465	TestAcc 0.9471	BestValid 0.9465
	Epoch 3400:	Loss 0.2383	TrainAcc 0.9492	ValidAcc 0.9463	TestAcc 0.9473	BestValid 0.9465
	Epoch 3425:	Loss 0.2385	TrainAcc 0.9493	ValidAcc 0.9463	TestAcc 0.9473	BestValid 0.9465
	Epoch 3450:	Loss 0.2377	TrainAcc 0.9486	ValidAcc 0.9465	TestAcc 0.9466	BestValid 0.9465
	Epoch 3475:	Loss 0.2381	TrainAcc 0.9492	ValidAcc 0.9465	TestAcc 0.9473	BestValid 0.9465
	Epoch 3500:	Loss 0.2376	TrainAcc 0.9496	ValidAcc 0.9466	TestAcc 0.9473	BestValid 0.9466
	Epoch 3525:	Loss 0.2378	TrainAcc 0.9480	ValidAcc 0.9447	TestAcc 0.9461	BestValid 0.9466
	Epoch 3550:	Loss 0.2361	TrainAcc 0.9495	ValidAcc 0.9462	TestAcc 0.9475	BestValid 0.9466
	Epoch 3575:	Loss 0.2370	TrainAcc 0.9492	ValidAcc 0.9467	TestAcc 0.9472	BestValid 0.9467
	Epoch 3600:	Loss 0.2355	TrainAcc 0.9498	ValidAcc 0.9465	TestAcc 0.9476	BestValid 0.9467
	Epoch 3625:	Loss 0.2354	TrainAcc 0.9491	ValidAcc 0.9463	TestAcc 0.9468	BestValid 0.9467
	Epoch 3650:	Loss 0.2355	TrainAcc 0.9499	ValidAcc 0.9471	TestAcc 0.9475	BestValid 0.9471
	Epoch 3675:	Loss 0.2350	TrainAcc 0.9500	ValidAcc 0.9468	TestAcc 0.9476	BestValid 0.9471
	Epoch 3700:	Loss 0.2338	TrainAcc 0.9498	ValidAcc 0.9466	TestAcc 0.9478	BestValid 0.9471
	Epoch 3725:	Loss 0.2338	TrainAcc 0.9497	ValidAcc 0.9466	TestAcc 0.9472	BestValid 0.9471
	Epoch 3750:	Loss 0.2342	TrainAcc 0.9500	ValidAcc 0.9474	TestAcc 0.9477	BestValid 0.9474
	Epoch 3775:	Loss 0.2337	TrainAcc 0.9500	ValidAcc 0.9467	TestAcc 0.9480	BestValid 0.9474
	Epoch 3800:	Loss 0.2336	TrainAcc 0.9489	ValidAcc 0.9465	TestAcc 0.9467	BestValid 0.9474
	Epoch 3825:	Loss 0.2336	TrainAcc 0.9501	ValidAcc 0.9471	TestAcc 0.9477	BestValid 0.9474
	Epoch 3850:	Loss 0.2332	TrainAcc 0.9501	ValidAcc 0.9472	TestAcc 0.9478	BestValid 0.9474
	Epoch 3875:	Loss 0.2327	TrainAcc 0.9499	ValidAcc 0.9468	TestAcc 0.9475	BestValid 0.9474
	Epoch 3900:	Loss 0.2320	TrainAcc 0.9496	ValidAcc 0.9470	TestAcc 0.9473	BestValid 0.9474
	Epoch 3925:	Loss 0.2310	TrainAcc 0.9501	ValidAcc 0.9471	TestAcc 0.9481	BestValid 0.9474
	Epoch 3950:	Loss 0.2318	TrainAcc 0.9500	ValidAcc 0.9474	TestAcc 0.9480	BestValid 0.9474
	Epoch 3975:	Loss 0.2304	TrainAcc 0.9497	ValidAcc 0.9472	TestAcc 0.9470	BestValid 0.9474
	Epoch 4000:	Loss 0.2309	TrainAcc 0.9507	ValidAcc 0.9475	TestAcc 0.9484	BestValid 0.9475
	Epoch 4025:	Loss 0.2308	TrainAcc 0.9498	ValidAcc 0.9470	TestAcc 0.9478	BestValid 0.9475
	Epoch 4050:	Loss 0.2301	TrainAcc 0.9506	ValidAcc 0.9470	TestAcc 0.9482	BestValid 0.9475
	Epoch 4075:	Loss 0.2305	TrainAcc 0.9499	ValidAcc 0.9473	TestAcc 0.9474	BestValid 0.9475
	Epoch 4100:	Loss 0.2291	TrainAcc 0.9503	ValidAcc 0.9472	TestAcc 0.9480	BestValid 0.9475
	Epoch 4125:	Loss 0.2287	TrainAcc 0.9508	ValidAcc 0.9476	TestAcc 0.9482	BestValid 0.9476
	Epoch 4150:	Loss 0.2295	TrainAcc 0.9509	ValidAcc 0.9476	TestAcc 0.9482	BestValid 0.9476
	Epoch 4175:	Loss 0.2297	TrainAcc 0.9510	ValidAcc 0.9477	TestAcc 0.9481	BestValid 0.9477
	Epoch 4200:	Loss 0.2284	TrainAcc 0.9509	ValidAcc 0.9475	TestAcc 0.9484	BestValid 0.9477
	Epoch 4225:	Loss 0.2283	TrainAcc 0.9510	ValidAcc 0.9482	TestAcc 0.9488	BestValid 0.9482
	Epoch 4250:	Loss 0.2277	TrainAcc 0.9513	ValidAcc 0.9476	TestAcc 0.9486	BestValid 0.9482
	Epoch 4275:	Loss 0.2270	TrainAcc 0.9511	ValidAcc 0.9470	TestAcc 0.9483	BestValid 0.9482
	Epoch 4300:	Loss 0.2277	TrainAcc 0.9510	ValidAcc 0.9478	TestAcc 0.9483	BestValid 0.9482
	Epoch 4325:	Loss 0.2273	TrainAcc 0.9510	ValidAcc 0.9468	TestAcc 0.9480	BestValid 0.9482
	Epoch 4350:	Loss 0.2276	TrainAcc 0.9508	ValidAcc 0.9468	TestAcc 0.9480	BestValid 0.9482
	Epoch 4375:	Loss 0.2268	TrainAcc 0.9512	ValidAcc 0.9477	TestAcc 0.9486	BestValid 0.9482
	Epoch 4400:	Loss 0.2266	TrainAcc 0.9508	ValidAcc 0.9473	TestAcc 0.9483	BestValid 0.9482
	Epoch 4425:	Loss 0.2262	TrainAcc 0.9514	ValidAcc 0.9481	TestAcc 0.9484	BestValid 0.9482
	Epoch 4450:	Loss 0.2259	TrainAcc 0.9516	ValidAcc 0.9481	TestAcc 0.9489	BestValid 0.9482
	Epoch 4475:	Loss 0.2256	TrainAcc 0.9512	ValidAcc 0.9478	TestAcc 0.9482	BestValid 0.9482
	Epoch 4500:	Loss 0.2245	TrainAcc 0.9508	ValidAcc 0.9475	TestAcc 0.9479	BestValid 0.9482
	Epoch 4525:	Loss 0.2258	TrainAcc 0.9509	ValidAcc 0.9477	TestAcc 0.9481	BestValid 0.9482
	Epoch 4550:	Loss 0.2254	TrainAcc 0.9514	ValidAcc 0.9476	TestAcc 0.9484	BestValid 0.9482
	Epoch 4575:	Loss 0.2244	TrainAcc 0.9514	ValidAcc 0.9481	TestAcc 0.9485	BestValid 0.9482
	Epoch 4600:	Loss 0.2235	TrainAcc 0.9513	ValidAcc 0.9476	TestAcc 0.9487	BestValid 0.9482
	Epoch 4625:	Loss 0.2243	TrainAcc 0.9505	ValidAcc 0.9465	TestAcc 0.9485	BestValid 0.9482
	Epoch 4650:	Loss 0.2236	TrainAcc 0.9512	ValidAcc 0.9477	TestAcc 0.9481	BestValid 0.9482
	Epoch 4675:	Loss 0.2243	TrainAcc 0.9518	ValidAcc 0.9483	TestAcc 0.9491	BestValid 0.9483
	Epoch 4700:	Loss 0.2230	TrainAcc 0.9514	ValidAcc 0.9478	TestAcc 0.9483	BestValid 0.9483
	Epoch 4725:	Loss 0.2226	TrainAcc 0.9517	ValidAcc 0.9475	TestAcc 0.9483	BestValid 0.9483
	Epoch 4750:	Loss 0.2231	TrainAcc 0.9518	ValidAcc 0.9480	TestAcc 0.9486	BestValid 0.9483
	Epoch 4775:	Loss 0.2220	TrainAcc 0.9518	ValidAcc 0.9485	TestAcc 0.9483	BestValid 0.9485
	Epoch 4800:	Loss 0.2218	TrainAcc 0.9520	ValidAcc 0.9483	TestAcc 0.9491	BestValid 0.9485
	Epoch 4825:	Loss 0.2222	TrainAcc 0.9519	ValidAcc 0.9483	TestAcc 0.9485	BestValid 0.9485
	Epoch 4850:	Loss 0.2216	TrainAcc 0.9522	ValidAcc 0.9490	TestAcc 0.9489	BestValid 0.9490
	Epoch 4875:	Loss 0.2204	TrainAcc 0.9523	ValidAcc 0.9480	TestAcc 0.9489	BestValid 0.9490
	Epoch 4900:	Loss 0.2215	TrainAcc 0.9521	ValidAcc 0.9477	TestAcc 0.9487	BestValid 0.9490
	Epoch 4925:	Loss 0.2210	TrainAcc 0.9508	ValidAcc 0.9465	TestAcc 0.9473	BestValid 0.9490
	Epoch 4950:	Loss 0.2213	TrainAcc 0.9522	ValidAcc 0.9483	TestAcc 0.9491	BestValid 0.9490
	Epoch 4975:	Loss 0.2203	TrainAcc 0.9524	ValidAcc 0.9492	TestAcc 0.9491	BestValid 0.9492
	Epoch 5000:	Loss 0.2201	TrainAcc 0.9522	ValidAcc 0.9483	TestAcc 0.9491	BestValid 0.9492
Node 7, Pre/Post-Pipelining: 1.092 / 16.609 ms, Bubble: 57.947 ms, Compute: 265.534 ms, Comm: 16.827 ms, Imbalance: 16.029 ms
Node 6, Pre/Post-Pipelining: 1.091 / 0.852 ms, Bubble: 73.752 ms, Compute: 251.619 ms, Comm: 28.987 ms, Imbalance: 17.928 ms
Node 4, Pre/Post-Pipelining: 1.090 / 0.885 ms, Bubble: 74.416 ms, Compute: 249.792 ms, Comm: 26.077 ms, Imbalance: 22.053 ms
Node 5, Pre/Post-Pipelining: 1.089 / 0.870 ms, Bubble: 74.181 ms, Compute: 246.161 ms, Comm: 30.572 ms, Imbalance: 21.373 ms
Node 0, Pre/Post-Pipelining: 1.089 / 0.943 ms, Bubble: 76.660 ms, Compute: 251.978 ms, Comm: 17.182 ms, Imbalance: 25.882 ms
Node 1, Pre/Post-Pipelining: 1.094 / 0.877 ms, Bubble: 75.719 ms, Compute: 250.223 ms, Comm: 27.800 ms, Imbalance: 18.165 ms
Node 2, Pre/Post-Pipelining: 1.091 / 0.893 ms, Bubble: 75.369 ms, Compute: 250.925 ms, Comm: 29.677 ms, Imbalance: 16.196 ms
Node 3, Pre/Post-Pipelining: 1.091 / 0.885 ms, Bubble: 74.525 ms, Compute: 252.313 ms, Comm: 26.600 ms, Imbalance: 18.612 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.089 ms
Cluster-Wide Average, Post-Pipelining Overhead: 0.943 ms
Cluster-Wide Average, Bubble: 76.660 ms
Cluster-Wide Average, Compute: 251.978 ms
Cluster-Wide Average, Communication: 17.182 ms
Cluster-Wide Average, Imbalance: 25.882 ms
Node 0, GPU memory consumption: 6.739 GB
Node 7, GPU memory consumption: 5.569 GB
Node 1, GPU memory consumption: 5.819 GB
Node 4, GPU memory consumption: 5.796 GB
Node 2, GPU memory consumption: 5.819 GB
Node 6, GPU memory consumption: 5.819 GB
Node 3, GPU memory consumption: 5.796 GB
Node 5, GPU memory consumption: 5.819 GB
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.760644 s---------------
------------------------node id 4,  per-epoch time: 0.760643 s---------------
------------------------node id 1,  per-epoch time: 0.760644 s---------------
------------------------node id 5,  per-epoch time: 0.760643 s---------------
------------------------node id 2,  per-epoch time: 0.760644 s---------------
------------------------node id 6,  per-epoch time: 0.760643 s---------------
------------------------node id 3,  per-epoch time: 0.760644 s---------------
------------------------node id 7,  per-epoch time: 0.760643 s---------------
************ Profiling Results ************
	Bubble: 508.773179 (ms) (66.91 percentage)
	Compute: 248.089344 (ms) (32.63 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.505259 (ms) (0.46 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 1.216 GB
	Aggregated layer-level communication throughput: 409.847 Gbps
Highest valid_acc: 0.9492
Target test_acc: 0.9491
Epoch to reach the target acc: 4974
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
