Initialized node 0 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.021 seconds.
Building the CSC structure...
        It takes 2.360 seconds.
Building the CSC structure...
        It takes 2.384 seconds.
Building the CSC structure...
        It takes 2.430 seconds.
Building the CSC structure...
        It takes 2.476 seconds.
Building the CSC structure...
        It takes 2.618 seconds.
Building the CSC structure...
        It takes 2.649 seconds.
Building the CSC structure...
        It takes 2.667 seconds.
Building the CSC structure...
        It takes 1.845 seconds.
        It takes 2.307 seconds.
        It takes 2.323 seconds.
        It takes 2.398 seconds.
        It takes 2.366 seconds.
        It takes 2.278 seconds.
        It takes 2.382 seconds.
        It takes 2.389 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.255 seconds.
Building the Label Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.030 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.031 seconds.
        It takes 0.289 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.039 seconds.
        It takes 0.277 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 62.706 Gbps (per GPU), 501.652 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.666 Gbps (per GPU), 501.331 Gbps (aggregated)
The layer-level communication performance: 62.664 Gbps (per GPU), 501.310 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.627 Gbps (per GPU), 501.018 Gbps (aggregated)
The layer-level communication performance: 62.624 Gbps (per GPU), 500.988 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.599 Gbps (per GPU), 500.790 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.591 Gbps (per GPU), 500.731 Gbps (aggregated)
The layer-level communication performance: 62.587 Gbps (per GPU), 500.694 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.292 Gbps (per GPU), 1330.337 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.291 Gbps (per GPU), 1330.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.297 Gbps (per GPU), 1330.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.292 Gbps (per GPU), 1330.334 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.280 Gbps (per GPU), 1330.238 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.292 Gbps (per GPU), 1330.334 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.283 Gbps (per GPU), 1330.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.290 Gbps (per GPU), 1330.317 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.411 Gbps (per GPU), 915.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.412 Gbps (per GPU), 915.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.411 Gbps (per GPU), 915.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.410 Gbps (per GPU), 915.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.411 Gbps (per GPU), 915.286 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.411 Gbps (per GPU), 915.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.410 Gbps (per GPU), 915.282 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.411 Gbps (per GPU), 915.287 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.381 Gbps (per GPU), 363.047 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.381 Gbps (per GPU), 363.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.381 Gbps (per GPU), 363.045 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.381 Gbps (per GPU), 363.045 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.381 Gbps (per GPU), 363.046 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.381 Gbps (per GPU), 363.045 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.381 Gbps (per GPU), 363.045 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.379 Gbps (per GPU), 363.030 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.46ms  2.75ms  3.06  8.38K  3.53M
 chk_1  0.75ms  2.77ms  2.96ms  3.94  6.74K  3.60M
 chk_2  0.80ms  2.64ms  2.86ms  3.58  7.27K  3.53M
 chk_3  0.80ms  2.68ms  2.89ms  3.61  7.92K  3.61M
 chk_4  0.62ms  2.66ms  2.82ms  4.51  5.33K  3.68M
 chk_5  1.00ms  2.65ms  2.84ms  2.83 10.07K  3.45M
 chk_6  0.96ms  2.79ms  2.99ms  3.11  9.41K  3.48M
 chk_7  0.82ms  2.65ms  2.82ms  3.44  8.12K  3.60M
 chk_8  0.68ms  2.75ms  2.92ms  4.30  6.09K  3.64M
 chk_9  1.10ms  2.57ms  2.76ms  2.51 11.10K  3.38M
chk_10  0.65ms  2.78ms  2.96ms  4.53  5.67K  3.63M
chk_11  0.82ms  2.66ms  2.83ms  3.44  8.16K  3.54M
chk_12  0.80ms  2.84ms  3.02ms  3.79  7.24K  3.55M
chk_13  0.64ms  2.69ms  2.85ms  4.49  5.41K  3.68M
chk_14  0.78ms  2.91ms  3.09ms  3.96  7.14K  3.53M
chk_15  0.95ms  2.76ms  2.99ms  3.14  9.25K  3.49M
chk_16  0.60ms  2.64ms  2.79ms  4.67  4.78K  3.77M
chk_17  0.76ms  2.75ms  2.91ms  3.81  6.85K  3.60M
chk_18  0.81ms  2.60ms  2.73ms  3.38  7.47K  3.57M
chk_19  0.60ms  2.65ms  2.78ms  4.60  4.88K  3.75M
chk_20  0.77ms  2.65ms  2.79ms  3.63  7.00K  3.63M
chk_21  0.63ms  2.62ms  2.78ms  4.39  5.41K  3.68M
chk_22  1.10ms  2.83ms  3.02ms  2.75 11.07K  3.39M
chk_23  0.79ms  2.74ms  2.88ms  3.64  7.23K  3.64M
chk_24  1.01ms  2.78ms  2.94ms  2.91 10.13K  3.43M
chk_25  0.73ms  2.61ms  2.74ms  3.77  6.40K  3.57M
chk_26  0.66ms  2.79ms  2.92ms  4.45  5.78K  3.55M
chk_27  0.96ms  2.70ms  2.85ms  2.97  9.34K  3.48M
chk_28  0.72ms  2.95ms  3.10ms  4.28  6.37K  3.57M
chk_29  0.63ms  2.75ms  2.90ms  4.62  5.16K  3.78M
chk_30  0.64ms  2.67ms  2.80ms  4.39  5.44K  3.67M
chk_31  0.72ms  2.78ms  2.95ms  4.09  6.33K  3.63M
   Avg  0.79  2.71  2.88
   Max  1.10  2.95  3.10
   Min  0.60  2.46  2.73
 Ratio  1.84  1.20  1.14
   Var  0.02  0.01  0.01
Profiling takes 2.447 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 372.325 ms
Partition 0 [0, 5) has cost: 372.325 ms
Partition 1 [5, 9) has cost: 347.129 ms
Partition 2 [9, 13) has cost: 347.129 ms
Partition 3 [13, 17) has cost: 347.129 ms
Partition 4 [17, 21) has cost: 347.129 ms
Partition 5 [21, 25) has cost: 347.129 ms
Partition 6 [25, 29) has cost: 347.129 ms
Partition 7 [29, 33) has cost: 352.552 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.441 ms
GPU 0, Compute+Comm Time: 134.030 ms, Bubble Time: 29.125 ms, Imbalance Overhead: 2.286 ms
GPU 1, Compute+Comm Time: 126.989 ms, Bubble Time: 28.785 ms, Imbalance Overhead: 9.667 ms
GPU 2, Compute+Comm Time: 126.989 ms, Bubble Time: 28.729 ms, Imbalance Overhead: 9.722 ms
GPU 3, Compute+Comm Time: 126.989 ms, Bubble Time: 28.576 ms, Imbalance Overhead: 9.875 ms
GPU 4, Compute+Comm Time: 126.989 ms, Bubble Time: 28.450 ms, Imbalance Overhead: 10.002 ms
GPU 5, Compute+Comm Time: 126.989 ms, Bubble Time: 28.423 ms, Imbalance Overhead: 10.029 ms
GPU 6, Compute+Comm Time: 126.989 ms, Bubble Time: 28.673 ms, Imbalance Overhead: 9.779 ms
GPU 7, Compute+Comm Time: 128.145 ms, Bubble Time: 29.048 ms, Imbalance Overhead: 8.248 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 321.578 ms
GPU 0, Compute+Comm Time: 248.184 ms, Bubble Time: 57.035 ms, Imbalance Overhead: 16.359 ms
GPU 1, Compute+Comm Time: 243.918 ms, Bubble Time: 56.263 ms, Imbalance Overhead: 21.397 ms
GPU 2, Compute+Comm Time: 243.918 ms, Bubble Time: 55.667 ms, Imbalance Overhead: 21.994 ms
GPU 3, Compute+Comm Time: 243.918 ms, Bubble Time: 55.617 ms, Imbalance Overhead: 22.043 ms
GPU 4, Compute+Comm Time: 243.918 ms, Bubble Time: 55.777 ms, Imbalance Overhead: 21.883 ms
GPU 5, Compute+Comm Time: 243.918 ms, Bubble Time: 55.982 ms, Imbalance Overhead: 21.678 ms
GPU 6, Compute+Comm Time: 243.918 ms, Bubble Time: 55.865 ms, Imbalance Overhead: 21.795 ms
GPU 7, Compute+Comm Time: 262.073 ms, Bubble Time: 56.397 ms, Imbalance Overhead: 3.108 ms
The estimated cost of the whole pipeline: 511.369 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 719.455 ms
Partition 0 [0, 9) has cost: 719.455 ms
Partition 1 [9, 17) has cost: 694.259 ms
Partition 2 [17, 25) has cost: 694.259 ms
Partition 3 [25, 33) has cost: 699.681 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.300 ms
GPU 0, Compute+Comm Time: 143.907 ms, Bubble Time: 26.404 ms, Imbalance Overhead: 2.988 ms
GPU 1, Compute+Comm Time: 139.999 ms, Bubble Time: 26.421 ms, Imbalance Overhead: 6.881 ms
GPU 2, Compute+Comm Time: 139.999 ms, Bubble Time: 26.155 ms, Imbalance Overhead: 7.147 ms
GPU 3, Compute+Comm Time: 140.583 ms, Bubble Time: 26.548 ms, Imbalance Overhead: 6.169 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 323.685 ms
GPU 0, Compute+Comm Time: 261.856 ms, Bubble Time: 49.820 ms, Imbalance Overhead: 12.009 ms
GPU 1, Compute+Comm Time: 259.723 ms, Bubble Time: 49.400 ms, Imbalance Overhead: 14.563 ms
GPU 2, Compute+Comm Time: 259.723 ms, Bubble Time: 50.040 ms, Imbalance Overhead: 13.923 ms
GPU 3, Compute+Comm Time: 270.009 ms, Bubble Time: 49.938 ms, Imbalance Overhead: 3.739 ms
    The estimated cost with 2 DP ways is 521.835 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1413.714 ms
Partition 0 [0, 17) has cost: 1413.714 ms
Partition 1 [17, 33) has cost: 1393.940 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 209.406 ms
GPU 0, Compute+Comm Time: 183.191 ms, Bubble Time: 21.195 ms, Imbalance Overhead: 5.020 ms
GPU 1, Compute+Comm Time: 181.415 ms, Bubble Time: 21.976 ms, Imbalance Overhead: 6.014 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 349.944 ms
GPU 0, Compute+Comm Time: 304.310 ms, Bubble Time: 37.630 ms, Imbalance Overhead: 8.003 ms
GPU 1, Compute+Comm Time: 308.748 ms, Bubble Time: 37.158 ms, Imbalance Overhead: 4.038 ms
    The estimated cost with 4 DP ways is 587.317 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2807.654 ms
Partition 0 [0, 33) has cost: 2807.654 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 457.604 ms
GPU 0, Compute+Comm Time: 457.604 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 584.315 ms
GPU 0, Compute+Comm Time: 584.315 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1094.015 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5507	TrainAcc 0.4287	ValidAcc 0.4462	TestAcc 0.4464	BestValid 0.4462
	Epoch 50:	Loss 1.9078	TrainAcc 0.6977	ValidAcc 0.7150	TestAcc 0.7116	BestValid 0.7150
	Epoch 75:	Loss 1.5316	TrainAcc 0.7544	ValidAcc 0.7680	TestAcc 0.7634	BestValid 0.7680
	Epoch 100:	Loss 1.3181	TrainAcc 0.7855	ValidAcc 0.7982	TestAcc 0.7928	BestValid 0.7982
	Epoch 125:	Loss 1.1438	TrainAcc 0.8159	ValidAcc 0.8264	TestAcc 0.8206	BestValid 0.8264
	Epoch 150:	Loss 1.0416	TrainAcc 0.8352	ValidAcc 0.8440	TestAcc 0.8401	BestValid 0.8440
	Epoch 175:	Loss 0.9755	TrainAcc 0.8504	ValidAcc 0.8577	TestAcc 0.8540	BestValid 0.8577
	Epoch 200:	Loss 0.9206	TrainAcc 0.8597	ValidAcc 0.8666	TestAcc 0.8624	BestValid 0.8666
	Epoch 225:	Loss 0.8708	TrainAcc 0.8669	ValidAcc 0.8737	TestAcc 0.8695	BestValid 0.8737
	Epoch 250:	Loss 0.8379	TrainAcc 0.8717	ValidAcc 0.8779	TestAcc 0.8740	BestValid 0.8779
	Epoch 275:	Loss 0.8182	TrainAcc 0.8786	ValidAcc 0.8857	TestAcc 0.8809	BestValid 0.8857
	Epoch 300:	Loss 0.7840	TrainAcc 0.8834	ValidAcc 0.8897	TestAcc 0.8856	BestValid 0.8897
	Epoch 325:	Loss 0.7583	TrainAcc 0.8871	ValidAcc 0.8933	TestAcc 0.8893	BestValid 0.8933
	Epoch 350:	Loss 0.7398	TrainAcc 0.8903	ValidAcc 0.8960	TestAcc 0.8916	BestValid 0.8960
	Epoch 375:	Loss 0.7251	TrainAcc 0.8935	ValidAcc 0.8987	TestAcc 0.8945	BestValid 0.8987
	Epoch 400:	Loss 0.7058	TrainAcc 0.8956	ValidAcc 0.9003	TestAcc 0.8960	BestValid 0.9003
	Epoch 425:	Loss 0.6920	TrainAcc 0.8976	ValidAcc 0.9025	TestAcc 0.8976	BestValid 0.9025
	Epoch 450:	Loss 0.6848	TrainAcc 0.9010	ValidAcc 0.9055	TestAcc 0.9006	BestValid 0.9055
	Epoch 475:	Loss 0.6713	TrainAcc 0.9027	ValidAcc 0.9062	TestAcc 0.9021	BestValid 0.9062
	Epoch 500:	Loss 0.6672	TrainAcc 0.9044	ValidAcc 0.9080	TestAcc 0.9028	BestValid 0.9080
	Epoch 525:	Loss 0.6542	TrainAcc 0.9066	ValidAcc 0.9091	TestAcc 0.9047	BestValid 0.9091
	Epoch 550:	Loss 0.6441	TrainAcc 0.9074	ValidAcc 0.9098	TestAcc 0.9056	BestValid 0.9098
	Epoch 575:	Loss 0.6352	TrainAcc 0.9084	ValidAcc 0.9113	TestAcc 0.9069	BestValid 0.9113
	Epoch 600:	Loss 0.6285	TrainAcc 0.9100	ValidAcc 0.9122	TestAcc 0.9080	BestValid 0.9122
	Epoch 625:	Loss 0.6229	TrainAcc 0.9124	ValidAcc 0.9146	TestAcc 0.9101	BestValid 0.9146
	Epoch 650:	Loss 0.6152	TrainAcc 0.9137	ValidAcc 0.9160	TestAcc 0.9110	BestValid 0.9160
	Epoch 675:	Loss 0.6119	TrainAcc 0.9135	ValidAcc 0.9152	TestAcc 0.9108	BestValid 0.9160
	Epoch 700:	Loss 0.6049	TrainAcc 0.9150	ValidAcc 0.9167	TestAcc 0.9122	BestValid 0.9167
	Epoch 725:	Loss 0.6002	TrainAcc 0.9160	ValidAcc 0.9182	TestAcc 0.9132	BestValid 0.9182
	Epoch 750:	Loss 0.5978	TrainAcc 0.9171	ValidAcc 0.9193	TestAcc 0.9141	BestValid 0.9193
	Epoch 775:	Loss 0.5895	TrainAcc 0.9176	ValidAcc 0.9195	TestAcc 0.9143	BestValid 0.9195
	Epoch 800:	Loss 0.5822	TrainAcc 0.9185	ValidAcc 0.9207	TestAcc 0.9156	BestValid 0.9207
	Epoch 825:	Loss 0.5841	TrainAcc 0.9197	ValidAcc 0.9218	TestAcc 0.9165	BestValid 0.9218
	Epoch 850:	Loss 0.5724	TrainAcc 0.9202	ValidAcc 0.9219	TestAcc 0.9170	BestValid 0.9219
	Epoch 875:	Loss 0.5754	TrainAcc 0.9206	ValidAcc 0.9225	TestAcc 0.9177	BestValid 0.9225
	Epoch 900:	Loss 0.5715	TrainAcc 0.9216	ValidAcc 0.9235	TestAcc 0.9184	BestValid 0.9235
	Epoch 925:	Loss 0.5649	TrainAcc 0.9217	ValidAcc 0.9234	TestAcc 0.9186	BestValid 0.9235
	Epoch 950:	Loss 0.5619	TrainAcc 0.9227	ValidAcc 0.9243	TestAcc 0.9195	BestValid 0.9243
	Epoch 975:	Loss 0.5579	TrainAcc 0.9228	ValidAcc 0.9245	TestAcc 0.9196	BestValid 0.9245
	Epoch 1000:	Loss 0.5522	TrainAcc 0.9241	ValidAcc 0.9255	TestAcc 0.9206	BestValid 0.9255
	Epoch 1025:	Loss 0.5501	TrainAcc 0.9237	ValidAcc 0.9253	TestAcc 0.9202	BestValid 0.9255
	Epoch 1050:	Loss 0.5522	TrainAcc 0.9244	ValidAcc 0.9252	TestAcc 0.9208	BestValid 0.9255
	Epoch 1075:	Loss 0.5444	TrainAcc 0.9255	ValidAcc 0.9265	TestAcc 0.9220	BestValid 0.9265
	Epoch 1100:	Loss 0.5458	TrainAcc 0.9261	ValidAcc 0.9268	TestAcc 0.9222	BestValid 0.9268
	Epoch 1125:	Loss 0.5413	TrainAcc 0.9260	ValidAcc 0.9265	TestAcc 0.9223	BestValid 0.9268
	Epoch 1150:	Loss 0.5388	TrainAcc 0.9267	ValidAcc 0.9272	TestAcc 0.9228	BestValid 0.9272
	Epoch 1175:	Loss 0.5379	TrainAcc 0.9275	ValidAcc 0.9278	TestAcc 0.9236	BestValid 0.9278
	Epoch 1200:	Loss 0.5327	TrainAcc 0.9276	ValidAcc 0.9282	TestAcc 0.9235	BestValid 0.9282
	Epoch 1225:	Loss 0.5331	TrainAcc 0.9278	ValidAcc 0.9284	TestAcc 0.9239	BestValid 0.9284
	Epoch 1250:	Loss 0.5287	TrainAcc 0.9284	ValidAcc 0.9287	TestAcc 0.9243	BestValid 0.9287
	Epoch 1275:	Loss 0.5263	TrainAcc 0.9286	ValidAcc 0.9287	TestAcc 0.9245	BestValid 0.9287
	Epoch 1300:	Loss 0.5201	TrainAcc 0.9292	ValidAcc 0.9290	TestAcc 0.9250	BestValid 0.9290
	Epoch 1325:	Loss 0.5196	TrainAcc 0.9296	ValidAcc 0.9298	TestAcc 0.9253	BestValid 0.9298
	Epoch 1350:	Loss 0.5206	TrainAcc 0.9300	ValidAcc 0.9300	TestAcc 0.9258	BestValid 0.9300
	Epoch 1375:	Loss 0.5206	TrainAcc 0.9300	ValidAcc 0.9298	TestAcc 0.9255	BestValid 0.9300
	Epoch 1400:	Loss 0.5188	TrainAcc 0.9307	ValidAcc 0.9302	TestAcc 0.9264	BestValid 0.9302
	Epoch 1425:	Loss 0.5143	TrainAcc 0.9308	ValidAcc 0.9306	TestAcc 0.9263	BestValid 0.9306
	Epoch 1450:	Loss 0.5150	TrainAcc 0.9314	ValidAcc 0.9306	TestAcc 0.9268	BestValid 0.9306
	Epoch 1475:	Loss 0.5118	TrainAcc 0.9317	ValidAcc 0.9306	TestAcc 0.9268	BestValid 0.9306
	Epoch 1500:	Loss 0.5137	TrainAcc 0.9309	ValidAcc 0.9300	TestAcc 0.9261	BestValid 0.9306
	Epoch 1525:	Loss 0.5033	TrainAcc 0.9323	ValidAcc 0.9316	TestAcc 0.9275	BestValid 0.9316
	Epoch 1550:	Loss 0.5093	TrainAcc 0.9327	ValidAcc 0.9318	TestAcc 0.9279	BestValid 0.9318
	Epoch 1575:	Loss 0.5053	TrainAcc 0.9331	ValidAcc 0.9309	TestAcc 0.9280	BestValid 0.9318
	Epoch 1600:	Loss 0.5028	TrainAcc 0.9331	ValidAcc 0.9315	TestAcc 0.9278	BestValid 0.9318
	Epoch 1625:	Loss 0.5036	TrainAcc 0.9327	ValidAcc 0.9310	TestAcc 0.9278	BestValid 0.9318
	Epoch 1650:	Loss 0.4982	TrainAcc 0.9330	ValidAcc 0.9313	TestAcc 0.9280	BestValid 0.9318
	Epoch 1675:	Loss 0.4999	TrainAcc 0.9339	ValidAcc 0.9323	TestAcc 0.9286	BestValid 0.9323
	Epoch 1700:	Loss 0.4942	TrainAcc 0.9342	ValidAcc 0.9324	TestAcc 0.9292	BestValid 0.9324
	Epoch 1725:	Loss 0.4944	TrainAcc 0.9345	ValidAcc 0.9327	TestAcc 0.9293	BestValid 0.9327
	Epoch 1750:	Loss 0.4938	TrainAcc 0.9343	ValidAcc 0.9321	TestAcc 0.9292	BestValid 0.9327
	Epoch 1775:	Loss 0.4912	TrainAcc 0.9346	ValidAcc 0.9325	TestAcc 0.9295	BestValid 0.9327
	Epoch 1800:	Loss 0.4901	TrainAcc 0.9354	ValidAcc 0.9337	TestAcc 0.9300	BestValid 0.9337
	Epoch 1825:	Loss 0.4924	TrainAcc 0.9357	ValidAcc 0.9338	TestAcc 0.9305	BestValid 0.9338
	Epoch 1850:	Loss 0.4851	TrainAcc 0.9354	ValidAcc 0.9334	TestAcc 0.9301	BestValid 0.9338
	Epoch 1875:	Loss 0.4915	TrainAcc 0.9353	ValidAcc 0.9329	TestAcc 0.9301	BestValid 0.9338
	Epoch 1900:	Loss 0.4843	TrainAcc 0.9359	ValidAcc 0.9336	TestAcc 0.9302	BestValid 0.9338
	Epoch 1925:	Loss 0.4848	TrainAcc 0.9359	ValidAcc 0.9337	TestAcc 0.9308	BestValid 0.9338
	Epoch 1950:	Loss 0.4823	TrainAcc 0.9360	ValidAcc 0.9339	TestAcc 0.9306	BestValid 0.9339
	Epoch 1975:	Loss 0.4794	TrainAcc 0.9364	ValidAcc 0.9338	TestAcc 0.9308	BestValid 0.9339
	Epoch 2000:	Loss 0.4805	TrainAcc 0.9358	ValidAcc 0.9339	TestAcc 0.9304	BestValid 0.9339
	Epoch 2025:	Loss 0.4828	TrainAcc 0.9365	ValidAcc 0.9339	TestAcc 0.9310	BestValid 0.9339
	Epoch 2050:	Loss 0.4835	TrainAcc 0.9368	ValidAcc 0.9339	TestAcc 0.9309	BestValid 0.9339
	Epoch 2075:	Loss 0.4803	TrainAcc 0.9370	ValidAcc 0.9347	TestAcc 0.9311	BestValid 0.9347
	Epoch 2100:	Loss 0.4744	TrainAcc 0.9367	ValidAcc 0.9344	TestAcc 0.9309	BestValid 0.9347
	Epoch 2125:	Loss 0.4792	TrainAcc 0.9368	ValidAcc 0.9342	TestAcc 0.9313	BestValid 0.9347
	Epoch 2150:	Loss 0.4750	TrainAcc 0.9371	ValidAcc 0.9348	TestAcc 0.9313	BestValid 0.9348
	Epoch 2175:	Loss 0.4763	TrainAcc 0.9382	ValidAcc 0.9354	TestAcc 0.9318	BestValid 0.9354
	Epoch 2200:	Loss 0.4794	TrainAcc 0.9378	ValidAcc 0.9355	TestAcc 0.9318	BestValid 0.9355
	Epoch 2225:	Loss 0.4675	TrainAcc 0.9386	ValidAcc 0.9360	TestAcc 0.9326	BestValid 0.9360
	Epoch 2250:	Loss 0.4726	TrainAcc 0.9382	ValidAcc 0.9354	TestAcc 0.9319	BestValid 0.9360
	Epoch 2275:	Loss 0.4701	TrainAcc 0.9389	ValidAcc 0.9361	TestAcc 0.9327	BestValid 0.9361
	Epoch 2300:	Loss 0.4709	TrainAcc 0.9380	ValidAcc 0.9351	TestAcc 0.9318	BestValid 0.9361
	Epoch 2325:	Loss 0.4749	TrainAcc 0.9388	ValidAcc 0.9360	TestAcc 0.9325	BestValid 0.9361
	Epoch 2350:	Loss 0.4664	TrainAcc 0.9383	ValidAcc 0.9354	TestAcc 0.9320	BestValid 0.9361
	Epoch 2375:	Loss 0.4661	TrainAcc 0.9389	ValidAcc 0.9359	TestAcc 0.9326	BestValid 0.9361
	Epoch 2400:	Loss 0.4647	TrainAcc 0.9396	ValidAcc 0.9364	TestAcc 0.9327	BestValid 0.9364
	Epoch 2425:	Loss 0.4638	TrainAcc 0.9390	ValidAcc 0.9361	TestAcc 0.9328	BestValid 0.9364
	Epoch 2450:	Loss 0.4669	TrainAcc 0.9389	ValidAcc 0.9362	TestAcc 0.9326	BestValid 0.9364
	Epoch 2475:	Loss 0.4615	TrainAcc 0.9402	ValidAcc 0.9371	TestAcc 0.9339	BestValid 0.9371
	Epoch 2500:	Loss 0.4635	TrainAcc 0.9391	ValidAcc 0.9360	TestAcc 0.9328	BestValid 0.9371
	Epoch 2525:	Loss 0.4656	TrainAcc 0.9389	ValidAcc 0.9361	TestAcc 0.9327	BestValid 0.9371
	Epoch 2550:	Loss 0.4646	TrainAcc 0.9396	ValidAcc 0.9362	TestAcc 0.9330	BestValid 0.9371
	Epoch 2575:	Loss 0.4578	TrainAcc 0.9403	ValidAcc 0.9367	TestAcc 0.9334	BestValid 0.9371
	Epoch 2600:	Loss 0.4603	TrainAcc 0.9394	ValidAcc 0.9358	TestAcc 0.9329	BestValid 0.9371
	Epoch 2625:	Loss 0.4596	TrainAcc 0.9400	ValidAcc 0.9368	TestAcc 0.9335	BestValid 0.9371
	Epoch 2650:	Loss 0.4592	TrainAcc 0.9405	ValidAcc 0.9368	TestAcc 0.9337	BestValid 0.9371
	Epoch 2675:	Loss 0.4567	TrainAcc 0.9396	ValidAcc 0.9366	TestAcc 0.9332	BestValid 0.9371
	Epoch 2700:	Loss 0.4577	TrainAcc 0.9405	ValidAcc 0.9371	TestAcc 0.9340	BestValid 0.9371
	Epoch 2725:	Loss 0.4581	TrainAcc 0.9409	ValidAcc 0.9373	TestAcc 0.9340	BestValid 0.9373
	Epoch 2750:	Loss 0.4597	TrainAcc 0.9410	ValidAcc 0.9374	TestAcc 0.9343	BestValid 0.9374
	Epoch 2775:	Loss 0.4574	TrainAcc 0.9410	ValidAcc 0.9374	TestAcc 0.9342	BestValid 0.9374
	Epoch 2800:	Loss 0.4526	TrainAcc 0.9412	ValidAcc 0.9372	TestAcc 0.9340	BestValid 0.9374
	Epoch 2825:	Loss 0.4509	TrainAcc 0.9408	ValidAcc 0.9375	TestAcc 0.9341	BestValid 0.9375
	Epoch 2850:	Loss 0.4571	TrainAcc 0.9410	ValidAcc 0.9376	TestAcc 0.9343	BestValid 0.9376
	Epoch 2875:	Loss 0.4495	TrainAcc 0.9418	ValidAcc 0.9379	TestAcc 0.9348	BestValid 0.9379
	Epoch 2900:	Loss 0.4540	TrainAcc 0.9415	ValidAcc 0.9380	TestAcc 0.9350	BestValid 0.9380
	Epoch 2925:	Loss 0.4504	TrainAcc 0.9421	ValidAcc 0.9382	TestAcc 0.9349	BestValid 0.9382
	Epoch 2950:	Loss 0.4498	TrainAcc 0.9407	ValidAcc 0.9372	TestAcc 0.9338	BestValid 0.9382
	Epoch 2975:	Loss 0.4595	TrainAcc 0.9424	ValidAcc 0.9384	TestAcc 0.9352	BestValid 0.9384
	Epoch 3000:	Loss 0.4492	TrainAcc 0.9422	ValidAcc 0.9386	TestAcc 0.9352	BestValid 0.9386
	Epoch 3025:	Loss 0.4529	TrainAcc 0.9426	ValidAcc 0.9384	TestAcc 0.9356	BestValid 0.9386
	Epoch 3050:	Loss 0.4474	TrainAcc 0.9418	ValidAcc 0.9379	TestAcc 0.9345	BestValid 0.9386
	Epoch 3075:	Loss 0.4488	TrainAcc 0.9427	ValidAcc 0.9386	TestAcc 0.9356	BestValid 0.9386
	Epoch 3100:	Loss 0.4453	TrainAcc 0.9420	ValidAcc 0.9371	TestAcc 0.9345	BestValid 0.9386
	Epoch 3125:	Loss 0.4452	TrainAcc 0.9421	ValidAcc 0.9382	TestAcc 0.9352	BestValid 0.9386
	Epoch 3150:	Loss 0.4479	TrainAcc 0.9426	ValidAcc 0.9381	TestAcc 0.9355	BestValid 0.9386
	Epoch 3175:	Loss 0.4479	TrainAcc 0.9418	ValidAcc 0.9372	TestAcc 0.9350	BestValid 0.9386
	Epoch 3200:	Loss 0.4415	TrainAcc 0.9429	ValidAcc 0.9388	TestAcc 0.9359	BestValid 0.9388
	Epoch 3225:	Loss 0.4422	TrainAcc 0.9435	ValidAcc 0.9392	TestAcc 0.9361	BestValid 0.9392
	Epoch 3250:	Loss 0.4441	TrainAcc 0.9439	ValidAcc 0.9392	TestAcc 0.9365	BestValid 0.9392
	Epoch 3275:	Loss 0.4459	TrainAcc 0.9424	ValidAcc 0.9386	TestAcc 0.9356	BestValid 0.9392
	Epoch 3300:	Loss 0.4438	TrainAcc 0.9430	ValidAcc 0.9388	TestAcc 0.9359	BestValid 0.9392
	Epoch 3325:	Loss 0.4458	TrainAcc 0.9430	ValidAcc 0.9383	TestAcc 0.9357	BestValid 0.9392
	Epoch 3350:	Loss 0.4444	TrainAcc 0.9435	ValidAcc 0.9392	TestAcc 0.9362	BestValid 0.9392
	Epoch 3375:	Loss 0.4398	TrainAcc 0.9427	ValidAcc 0.9384	TestAcc 0.9352	BestValid 0.9392
	Epoch 3400:	Loss 0.4402	TrainAcc 0.9431	ValidAcc 0.9385	TestAcc 0.9359	BestValid 0.9392
	Epoch 3425:	Loss 0.4436	TrainAcc 0.9436	ValidAcc 0.9391	TestAcc 0.9362	BestValid 0.9392
	Epoch 3450:	Loss 0.4399	TrainAcc 0.9439	ValidAcc 0.9391	TestAcc 0.9363	BestValid 0.9392
	Epoch 3475:	Loss 0.4412	TrainAcc 0.9438	ValidAcc 0.9398	TestAcc 0.9364	BestValid 0.9398
	Epoch 3500:	Loss 0.4400	TrainAcc 0.9428	ValidAcc 0.9382	TestAcc 0.9357	BestValid 0.9398
	Epoch 3525:	Loss 0.4403	TrainAcc 0.9435	ValidAcc 0.9390	TestAcc 0.9362	BestValid 0.9398
	Epoch 3550:	Loss 0.4365	TrainAcc 0.9436	ValidAcc 0.9389	TestAcc 0.9360	BestValid 0.9398
	Epoch 3575:	Loss 0.4344	TrainAcc 0.9440	ValidAcc 0.9396	TestAcc 0.9366	BestValid 0.9398
	Epoch 3600:	Loss 0.4341	TrainAcc 0.9440	ValidAcc 0.9395	TestAcc 0.9364	BestValid 0.9398
	Epoch 3625:	Loss 0.4342	TrainAcc 0.9447	ValidAcc 0.9400	TestAcc 0.9373	BestValid 0.9400
	Epoch 3650:	Loss 0.4362	TrainAcc 0.9445	ValidAcc 0.9395	TestAcc 0.9372	BestValid 0.9400
	Epoch 3675:	Loss 0.4353	TrainAcc 0.9432	ValidAcc 0.9382	TestAcc 0.9354	BestValid 0.9400
	Epoch 3700:	Loss 0.4368	TrainAcc 0.9436	ValidAcc 0.9387	TestAcc 0.9360	BestValid 0.9400
	Epoch 3725:	Loss 0.4353	TrainAcc 0.9447	ValidAcc 0.9402	TestAcc 0.9372	BestValid 0.9402
	Epoch 3750:	Loss 0.4317	TrainAcc 0.9431	ValidAcc 0.9390	TestAcc 0.9358	BestValid 0.9402
	Epoch 3775:	Loss 0.4350	TrainAcc 0.9456	ValidAcc 0.9410	TestAcc 0.9378	BestValid 0.9410
	Epoch 3800:	Loss 0.4386	TrainAcc 0.9453	ValidAcc 0.9410	TestAcc 0.9373	BestValid 0.9410
	Epoch 3825:	Loss 0.4318	TrainAcc 0.9442	ValidAcc 0.9397	TestAcc 0.9364	BestValid 0.9410
	Epoch 3850:	Loss 0.4342	TrainAcc 0.9442	ValidAcc 0.9399	TestAcc 0.9369	BestValid 0.9410
	Epoch 3875:	Loss 0.4320	TrainAcc 0.9449	ValidAcc 0.9405	TestAcc 0.9375	BestValid 0.9410
	Epoch 3900:	Loss 0.4319	TrainAcc 0.9449	ValidAcc 0.9405	TestAcc 0.9373	BestValid 0.9410
	Epoch 3925:	Loss 0.4328	TrainAcc 0.9443	ValidAcc 0.9400	TestAcc 0.9370	BestValid 0.9410
	Epoch 3950:	Loss 0.4346	TrainAcc 0.9440	ValidAcc 0.9397	TestAcc 0.9367	BestValid 0.9410
	Epoch 3975:	Loss 0.4324	TrainAcc 0.9439	ValidAcc 0.9397	TestAcc 0.9367	BestValid 0.9410
	Epoch 4000:	Loss 0.4278	TrainAcc 0.9451	ValidAcc 0.9410	TestAcc 0.9372	BestValid 0.9410
	Epoch 4025:	Loss 0.4259	TrainAcc 0.9454	ValidAcc 0.9407	TestAcc 0.9373	BestValid 0.9410
	Epoch 4050:	Loss 0.4296	TrainAcc 0.9457	ValidAcc 0.9412	TestAcc 0.9379	BestValid 0.9412
	Epoch 4075:	Loss 0.4268	TrainAcc 0.9463	ValidAcc 0.9419	TestAcc 0.9385	BestValid 0.9419
	Epoch 4100:	Loss 0.4278	TrainAcc 0.9450	ValidAcc 0.9409	TestAcc 0.9376	BestValid 0.9419
	Epoch 4125:	Loss 0.4279	TrainAcc 0.9450	ValidAcc 0.9407	TestAcc 0.9373	BestValid 0.9419
	Epoch 4150:	Loss 0.4242	TrainAcc 0.9456	ValidAcc 0.9412	TestAcc 0.9379	BestValid 0.9419
	Epoch 4175:	Loss 0.4236	TrainAcc 0.9456	ValidAcc 0.9411	TestAcc 0.9378	BestValid 0.9419
	Epoch 4200:	Loss 0.4266	TrainAcc 0.9458	ValidAcc 0.9410	TestAcc 0.9379	BestValid 0.9419
	Epoch 4225:	Loss 0.4265	TrainAcc 0.9461	ValidAcc 0.9413	TestAcc 0.9382	BestValid 0.9419
	Epoch 4250:	Loss 0.4243	TrainAcc 0.9460	ValidAcc 0.9415	TestAcc 0.9383	BestValid 0.9419
	Epoch 4275:	Loss 0.4255	TrainAcc 0.9461	ValidAcc 0.9419	TestAcc 0.9384	BestValid 0.9419
	Epoch 4300:	Loss 0.4236	TrainAcc 0.9460	ValidAcc 0.9416	TestAcc 0.9381	BestValid 0.9419
	Epoch 4325:	Loss 0.4264	TrainAcc 0.9462	ValidAcc 0.9419	TestAcc 0.9386	BestValid 0.9419
	Epoch 4350:	Loss 0.4239	TrainAcc 0.9452	ValidAcc 0.9407	TestAcc 0.9378	BestValid 0.9419
	Epoch 4375:	Loss 0.4231	TrainAcc 0.9464	ValidAcc 0.9418	TestAcc 0.9382	BestValid 0.9419
	Epoch 4400:	Loss 0.4213	TrainAcc 0.9466	ValidAcc 0.9419	TestAcc 0.9385	BestValid 0.9419
	Epoch 4425:	Loss 0.4228	TrainAcc 0.9453	ValidAcc 0.9406	TestAcc 0.9378	BestValid 0.9419
	Epoch 4450:	Loss 0.4187	TrainAcc 0.9466	ValidAcc 0.9416	TestAcc 0.9388	BestValid 0.9419
	Epoch 4475:	Loss 0.4190	TrainAcc 0.9461	ValidAcc 0.9414	TestAcc 0.9383	BestValid 0.9419
	Epoch 4500:	Loss 0.4210	TrainAcc 0.9456	ValidAcc 0.9411	TestAcc 0.9383	BestValid 0.9419
	Epoch 4525:	Loss 0.4239	TrainAcc 0.9466	ValidAcc 0.9418	TestAcc 0.9391	BestValid 0.9419
	Epoch 4550:	Loss 0.4180	TrainAcc 0.9461	ValidAcc 0.9413	TestAcc 0.9383	BestValid 0.9419
	Epoch 4575:	Loss 0.4177	TrainAcc 0.9469	ValidAcc 0.9424	TestAcc 0.9391	BestValid 0.9424
	Epoch 4600:	Loss 0.4183	TrainAcc 0.9470	ValidAcc 0.9422	TestAcc 0.9394	BestValid 0.9424
	Epoch 4625:	Loss 0.4217	TrainAcc 0.9464	ValidAcc 0.9415	TestAcc 0.9390	BestValid 0.9424
	Epoch 4650:	Loss 0.4174	TrainAcc 0.9463	ValidAcc 0.9414	TestAcc 0.9386	BestValid 0.9424
	Epoch 4675:	Loss 0.4184	TrainAcc 0.9468	ValidAcc 0.9418	TestAcc 0.9388	BestValid 0.9424
	Epoch 4700:	Loss 0.4181	TrainAcc 0.9471	ValidAcc 0.9426	TestAcc 0.9392	BestValid 0.9426
	Epoch 4725:	Loss 0.4129	TrainAcc 0.9469	ValidAcc 0.9420	TestAcc 0.9393	BestValid 0.9426
	Epoch 4750:	Loss 0.4213	TrainAcc 0.9460	ValidAcc 0.9412	TestAcc 0.9386	BestValid 0.9426
	Epoch 4775:	Loss 0.4131	TrainAcc 0.9473	ValidAcc 0.9422	TestAcc 0.9397	BestValid 0.9426
	Epoch 4800:	Loss 0.4210	TrainAcc 0.9464	ValidAcc 0.9416	TestAcc 0.9388	BestValid 0.9426
	Epoch 4825:	Loss 0.4176	TrainAcc 0.9477	ValidAcc 0.9431	TestAcc 0.9394	BestValid 0.9431
	Epoch 4850:	Loss 0.4172	TrainAcc 0.9471	ValidAcc 0.9422	TestAcc 0.9391	BestValid 0.9431
	Epoch 4875:	Loss 0.4161	TrainAcc 0.9469	ValidAcc 0.9419	TestAcc 0.9393	BestValid 0.9431
	Epoch 4900:	Loss 0.4154	TrainAcc 0.9465	ValidAcc 0.9414	TestAcc 0.9387	BestValid 0.9431
	Epoch 4925:	Loss 0.4165	TrainAcc 0.9472	ValidAcc 0.9425	TestAcc 0.9392	BestValid 0.9431
	Epoch 4950:	Loss 0.4144	TrainAcc 0.9466	ValidAcc 0.9420	TestAcc 0.9391	BestValid 0.9431
	Epoch 4975:	Loss 0.4174	TrainAcc 0.9466	ValidAcc 0.9419	TestAcc 0.9388	BestValid 0.9431
	Epoch 5000:	Loss 0.4152	TrainAcc 0.9465	ValidAcc 0.9415	TestAcc 0.9388	BestValid 0.9431
Node 1, Pre/Post-Pipelining: 1.095 / 12.259 ms, Bubble: 80.989 ms, Compute: 254.728 ms, Comm: 42.328 ms, Imbalance: 33.541 ms
Node 2, Pre/Post-Pipelining: 1.094 / 12.277 ms, Bubble: 81.561 ms, Compute: 249.875 ms, Comm: 47.039 ms, Imbalance: 33.670 ms
Node 3, Pre/Post-Pipelining: 1.093 / 12.293 ms, Bubble: 80.469 ms, Compute: 257.028 ms, Comm: 47.093 ms, Imbalance: 27.208 ms
Node 0, Pre/Post-Pipelining: 1.092 / 12.326 ms, Bubble: 80.922 ms, Compute: 285.089 ms, Comm: 32.484 ms, Imbalance: 12.351 ms
Node 4, Pre/Post-Pipelining: 1.094 / 12.272 ms, Bubble: 81.153 ms, Compute: 251.147 ms, Comm: 46.572 ms, Imbalance: 33.539 ms
Node 5, Pre/Post-Pipelining: 1.093 / 12.275 ms, Bubble: 81.412 ms, Compute: 256.234 ms, Comm: 45.011 ms, Imbalance: 29.396 ms
Node 7, Pre/Post-Pipelining: 1.099 / 27.514 ms, Bubble: 66.735 ms, Compute: 280.715 ms, Comm: 31.986 ms, Imbalance: 16.958 ms
Node 6, Pre/Post-Pipelining: 1.096 / 12.237 ms, Bubble: 81.782 ms, Compute: 255.083 ms, Comm: 40.593 ms, Imbalance: 34.776 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.092 ms
Cluster-Wide Average, Post-Pipelining Overhead: 12.326 ms
Cluster-Wide Average, Bubble: 80.922 ms
Cluster-Wide Average, Compute: 285.089 ms
Cluster-Wide Average, Communication: 32.484 ms
Cluster-Wide Average, Imbalance: 12.351 ms
Node 0, GPU memory consumption: 8.059 GB
Node 3, GPU memory consumption: 6.018 GB
Node 1, GPU memory consumption: 6.042 GB
Node 2, GPU memory consumption: 6.042 GB
Node 4, GPU memory consumption: 6.018 GB
Node 6, GPU memory consumption: 6.042 GB
Node 5, GPU memory consumption: 6.042 GB
Node 7, GPU memory consumption: 6.171 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.914760 s---------------
------------------------node id 1,  per-epoch time: 0.914760 s---------------
------------------------node id 2,  per-epoch time: 0.914760 s---------------
------------------------node id 3,  per-epoch time: 0.914760 s---------------
------------------------node id 4,  per-epoch time: 0.914760 s---------------
------------------------node id 5,  per-epoch time: 0.914760 s---------------
------------------------node id 6,  per-epoch time: 0.914760 s---------------
------------------------node id 7,  per-epoch time: 0.914760 s---------------
************ Profiling Results ************
	Bubble: 653.833761 (ms) (71.51 percentage)
	Compute: 257.062517 (ms) (28.11 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.486319 (ms) (0.38 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 2.441 GB
	Aggregated layer-level communication throughput: 501.312 Gbps
Highest valid_acc: 0.9431
Target test_acc: 0.9394
Epoch to reach the target acc: 4824
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
