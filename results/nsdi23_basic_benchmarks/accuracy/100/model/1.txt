Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.857 seconds.
Building the CSC structure...
        It takes 1.910 seconds.
Building the CSC structure...
        It takes 2.041 seconds.
Building the CSC structure...
        It takes 2.068 seconds.
Building the CSC structure...
        It takes 2.341 seconds.
Building the CSC structure...
        It takes 2.451 seconds.
Building the CSC structure...
        It takes 2.603 seconds.
Building the CSC structure...
        It takes 2.660 seconds.
Building the CSC structure...
        It takes 1.799 seconds.
        It takes 1.840 seconds.
        It takes 1.857 seconds.
        It takes 1.941 seconds.
        It takes 2.289 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.384 seconds.
        It takes 2.290 seconds.
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 2.368 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.040 seconds.
        It takes 0.251 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
Building the Feature Vector...
        It takes 0.262 seconds.
Building the Label Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.037 seconds.
Building the Feature Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.292 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
        It takes 0.276 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.037 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 62.937 Gbps (per GPU), 503.492 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.892 Gbps (per GPU), 503.133 Gbps (aggregated)
The layer-level communication performance: 62.891 Gbps (per GPU), 503.126 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.854 Gbps (per GPU), 502.835 Gbps (aggregated)
The layer-level communication performance: 62.849 Gbps (per GPU), 502.794 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.820 Gbps (per GPU), 502.558 Gbps (aggregated)
The layer-level communication performance: 62.810 Gbps (per GPU), 502.479 Gbps (aggregated)
The layer-level communication performance: 62.813 Gbps (per GPU), 502.501 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.174 Gbps (per GPU), 1313.394 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.203 Gbps (per GPU), 1313.626 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.173 Gbps (per GPU), 1313.381 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.199 Gbps (per GPU), 1313.590 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.169 Gbps (per GPU), 1313.349 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.179 Gbps (per GPU), 1313.430 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.175 Gbps (per GPU), 1313.397 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.178 Gbps (per GPU), 1313.423 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.184 Gbps (per GPU), 913.471 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.185 Gbps (per GPU), 913.477 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.181 Gbps (per GPU), 913.452 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.183 Gbps (per GPU), 913.460 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.185 Gbps (per GPU), 913.477 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.184 Gbps (per GPU), 913.475 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.184 Gbps (per GPU), 913.468 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.184 Gbps (per GPU), 913.471 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 46.230 Gbps (per GPU), 369.838 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.230 Gbps (per GPU), 369.840 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.230 Gbps (per GPU), 369.837 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.229 Gbps (per GPU), 369.835 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.230 Gbps (per GPU), 369.840 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.230 Gbps (per GPU), 369.837 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.229 Gbps (per GPU), 369.831 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.229 Gbps (per GPU), 369.832 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.39ms  2.72ms  3.07  8.38K  3.53M
 chk_1  0.75ms  2.59ms  2.92ms  3.88  6.74K  3.60M
 chk_2  0.79ms  2.59ms  2.79ms  3.51  7.27K  3.53M
 chk_3  0.80ms  2.67ms  2.84ms  3.53  7.92K  3.61M
 chk_4  0.63ms  2.59ms  2.73ms  4.37  5.33K  3.68M
 chk_5  1.00ms  2.58ms  2.77ms  2.78 10.07K  3.45M
 chk_6  0.96ms  2.75ms  2.92ms  3.06  9.41K  3.48M
 chk_7  0.82ms  2.59ms  2.75ms  3.38  8.12K  3.60M
 chk_8  0.68ms  2.70ms  2.85ms  4.21  6.09K  3.64M
 chk_9  1.09ms  2.50ms  2.70ms  2.47 11.10K  3.38M
chk_10  0.65ms  2.73ms  2.88ms  4.45  5.67K  3.63M
chk_11  0.82ms  2.60ms  2.75ms  3.36  8.16K  3.54M
chk_12  0.80ms  2.77ms  3.16ms  3.94  7.24K  3.55M
chk_13  0.64ms  2.61ms  2.78ms  4.37  5.41K  3.68M
chk_14  0.78ms  2.86ms  3.03ms  3.89  7.14K  3.53M
chk_15  0.95ms  2.70ms  2.90ms  3.07  9.25K  3.49M
chk_16  0.60ms  2.55ms  2.69ms  4.53  4.78K  3.77M
chk_17  0.76ms  2.68ms  2.83ms  3.72  6.85K  3.60M
chk_18  0.81ms  2.52ms  2.64ms  3.27  7.47K  3.57M
chk_19  0.61ms  2.57ms  2.68ms  4.43  4.88K  3.75M
chk_20  0.77ms  2.60ms  2.73ms  3.56  7.00K  3.63M
chk_21  0.63ms  2.59ms  2.71ms  4.29  5.41K  3.68M
chk_22  1.09ms  2.80ms  2.97ms  2.71 11.07K  3.39M
chk_23  0.79ms  2.66ms  2.85ms  3.60  7.23K  3.64M
chk_24  1.01ms  2.74ms  2.94ms  2.92 10.13K  3.43M
chk_25  0.72ms  2.55ms  2.71ms  3.74  6.40K  3.57M
chk_26  0.66ms  2.74ms  2.89ms  4.39  5.78K  3.55M
chk_27  0.95ms  2.61ms  2.82ms  2.96  9.34K  3.48M
chk_28  0.72ms  2.90ms  3.09ms  4.28  6.37K  3.57M
chk_29  0.63ms  2.70ms  2.87ms  4.57  5.16K  3.78M
chk_30  0.63ms  2.57ms  2.77ms  4.37  5.44K  3.67M
chk_31  0.72ms  2.74ms  2.90ms  4.03  6.33K  3.63M
   Avg  0.79  2.65  2.83
   Max  1.09  2.90  3.16
   Min  0.60  2.39  2.64
 Ratio  1.84  1.21  1.20
   Var  0.02  0.01  0.01
Profiling takes 2.410 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 364.068 ms
Partition 0 [0, 5) has cost: 364.068 ms
Partition 1 [5, 9) has cost: 338.941 ms
Partition 2 [9, 13) has cost: 338.941 ms
Partition 3 [13, 17) has cost: 338.941 ms
Partition 4 [17, 21) has cost: 338.941 ms
Partition 5 [21, 25) has cost: 338.941 ms
Partition 6 [25, 29) has cost: 338.941 ms
Partition 7 [29, 33) has cost: 344.794 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 162.982 ms
GPU 0, Compute+Comm Time: 131.413 ms, Bubble Time: 28.686 ms, Imbalance Overhead: 2.882 ms
GPU 1, Compute+Comm Time: 124.388 ms, Bubble Time: 28.362 ms, Imbalance Overhead: 10.232 ms
GPU 2, Compute+Comm Time: 124.388 ms, Bubble Time: 28.171 ms, Imbalance Overhead: 10.423 ms
GPU 3, Compute+Comm Time: 124.388 ms, Bubble Time: 28.047 ms, Imbalance Overhead: 10.546 ms
GPU 4, Compute+Comm Time: 124.388 ms, Bubble Time: 27.971 ms, Imbalance Overhead: 10.623 ms
GPU 5, Compute+Comm Time: 124.388 ms, Bubble Time: 27.941 ms, Imbalance Overhead: 10.652 ms
GPU 6, Compute+Comm Time: 124.388 ms, Bubble Time: 28.164 ms, Imbalance Overhead: 10.430 ms
GPU 7, Compute+Comm Time: 125.609 ms, Bubble Time: 28.529 ms, Imbalance Overhead: 8.845 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 315.594 ms
GPU 0, Compute+Comm Time: 242.875 ms, Bubble Time: 55.566 ms, Imbalance Overhead: 17.154 ms
GPU 1, Compute+Comm Time: 238.242 ms, Bubble Time: 54.796 ms, Imbalance Overhead: 22.556 ms
GPU 2, Compute+Comm Time: 238.242 ms, Bubble Time: 54.256 ms, Imbalance Overhead: 23.096 ms
GPU 3, Compute+Comm Time: 238.242 ms, Bubble Time: 54.274 ms, Imbalance Overhead: 23.078 ms
GPU 4, Compute+Comm Time: 238.242 ms, Bubble Time: 54.364 ms, Imbalance Overhead: 22.988 ms
GPU 5, Compute+Comm Time: 238.242 ms, Bubble Time: 54.612 ms, Imbalance Overhead: 22.740 ms
GPU 6, Compute+Comm Time: 238.242 ms, Bubble Time: 54.858 ms, Imbalance Overhead: 22.494 ms
GPU 7, Compute+Comm Time: 256.345 ms, Bubble Time: 55.471 ms, Imbalance Overhead: 3.778 ms
The estimated cost of the whole pipeline: 502.505 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 703.009 ms
Partition 0 [0, 9) has cost: 703.009 ms
Partition 1 [9, 17) has cost: 677.881 ms
Partition 2 [17, 25) has cost: 677.881 ms
Partition 3 [25, 33) has cost: 683.734 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 171.132 ms
GPU 0, Compute+Comm Time: 141.630 ms, Bubble Time: 26.086 ms, Imbalance Overhead: 3.416 ms
GPU 1, Compute+Comm Time: 137.737 ms, Bubble Time: 25.784 ms, Imbalance Overhead: 7.611 ms
GPU 2, Compute+Comm Time: 137.737 ms, Bubble Time: 25.615 ms, Imbalance Overhead: 7.780 ms
GPU 3, Compute+Comm Time: 138.326 ms, Bubble Time: 25.940 ms, Imbalance Overhead: 6.867 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 318.027 ms
GPU 0, Compute+Comm Time: 257.024 ms, Bubble Time: 48.371 ms, Imbalance Overhead: 12.632 ms
GPU 1, Compute+Comm Time: 254.643 ms, Bubble Time: 48.055 ms, Imbalance Overhead: 15.330 ms
GPU 2, Compute+Comm Time: 254.643 ms, Bubble Time: 48.524 ms, Imbalance Overhead: 14.860 ms
GPU 3, Compute+Comm Time: 264.895 ms, Bubble Time: 49.152 ms, Imbalance Overhead: 3.981 ms
    The estimated cost with 2 DP ways is 513.617 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1380.890 ms
Partition 0 [0, 17) has cost: 1380.890 ms
Partition 1 [17, 33) has cost: 1361.615 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 207.190 ms
GPU 0, Compute+Comm Time: 181.016 ms, Bubble Time: 20.944 ms, Imbalance Overhead: 5.230 ms
GPU 1, Compute+Comm Time: 179.256 ms, Bubble Time: 21.541 ms, Imbalance Overhead: 6.393 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 345.127 ms
GPU 0, Compute+Comm Time: 299.634 ms, Bubble Time: 36.374 ms, Imbalance Overhead: 9.120 ms
GPU 1, Compute+Comm Time: 303.838 ms, Bubble Time: 36.605 ms, Imbalance Overhead: 4.684 ms
    The estimated cost with 4 DP ways is 579.933 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2742.505 ms
Partition 0 [0, 33) has cost: 2742.505 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 449.669 ms
GPU 0, Compute+Comm Time: 449.669 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 574.503 ms
GPU 0, Compute+Comm Time: 574.503 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1075.380 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5719	TrainAcc 0.4338	ValidAcc 0.4523	TestAcc 0.4523	BestValid 0.4523
	Epoch 50:	Loss 1.9244	TrainAcc 0.6915	ValidAcc 0.7085	TestAcc 0.7027	BestValid 0.7085
	Epoch 75:	Loss 1.5380	TrainAcc 0.7544	ValidAcc 0.7694	TestAcc 0.7634	BestValid 0.7694
	Epoch 100:	Loss 1.3130	TrainAcc 0.7865	ValidAcc 0.7996	TestAcc 0.7933	BestValid 0.7996
	Epoch 125:	Loss 1.1247	TrainAcc 0.8161	ValidAcc 0.8271	TestAcc 0.8211	BestValid 0.8271
	Epoch 150:	Loss 1.0324	TrainAcc 0.8376	ValidAcc 0.8465	TestAcc 0.8409	BestValid 0.8465
	Epoch 175:	Loss 0.9689	TrainAcc 0.8500	ValidAcc 0.8584	TestAcc 0.8529	BestValid 0.8584
	Epoch 200:	Loss 0.9107	TrainAcc 0.8584	ValidAcc 0.8661	TestAcc 0.8605	BestValid 0.8661
	Epoch 225:	Loss 0.8581	TrainAcc 0.8688	ValidAcc 0.8762	TestAcc 0.8705	BestValid 0.8762
	Epoch 250:	Loss 0.8284	TrainAcc 0.8769	ValidAcc 0.8838	TestAcc 0.8784	BestValid 0.8838
	Epoch 275:	Loss 0.8017	TrainAcc 0.8806	ValidAcc 0.8865	TestAcc 0.8816	BestValid 0.8865
	Epoch 300:	Loss 0.7770	TrainAcc 0.8847	ValidAcc 0.8898	TestAcc 0.8856	BestValid 0.8898
	Epoch 325:	Loss 0.7547	TrainAcc 0.8891	ValidAcc 0.8940	TestAcc 0.8899	BestValid 0.8940
	Epoch 350:	Loss 0.7353	TrainAcc 0.8910	ValidAcc 0.8953	TestAcc 0.8910	BestValid 0.8953
	Epoch 375:	Loss 0.7140	TrainAcc 0.8950	ValidAcc 0.8982	TestAcc 0.8950	BestValid 0.8982
	Epoch 400:	Loss 0.7021	TrainAcc 0.8971	ValidAcc 0.9005	TestAcc 0.8968	BestValid 0.9005
	Epoch 425:	Loss 0.6892	TrainAcc 0.8984	ValidAcc 0.9017	TestAcc 0.8980	BestValid 0.9017
	Epoch 450:	Loss 0.6793	TrainAcc 0.9013	ValidAcc 0.9047	TestAcc 0.9001	BestValid 0.9047
	Epoch 475:	Loss 0.6670	TrainAcc 0.9037	ValidAcc 0.9067	TestAcc 0.9022	BestValid 0.9067
	Epoch 500:	Loss 0.6596	TrainAcc 0.9059	ValidAcc 0.9085	TestAcc 0.9036	BestValid 0.9085
	Epoch 525:	Loss 0.6494	TrainAcc 0.9068	ValidAcc 0.9095	TestAcc 0.9045	BestValid 0.9095
	Epoch 550:	Loss 0.6435	TrainAcc 0.9086	ValidAcc 0.9106	TestAcc 0.9065	BestValid 0.9106
	Epoch 575:	Loss 0.6337	TrainAcc 0.9104	ValidAcc 0.9124	TestAcc 0.9081	BestValid 0.9124
	Epoch 600:	Loss 0.6235	TrainAcc 0.9118	ValidAcc 0.9139	TestAcc 0.9090	BestValid 0.9139
	Epoch 625:	Loss 0.6123	TrainAcc 0.9121	ValidAcc 0.9137	TestAcc 0.9097	BestValid 0.9139
	Epoch 650:	Loss 0.6097	TrainAcc 0.9134	ValidAcc 0.9144	TestAcc 0.9111	BestValid 0.9144
	Epoch 675:	Loss 0.6050	TrainAcc 0.9147	ValidAcc 0.9159	TestAcc 0.9123	BestValid 0.9159
	Epoch 700:	Loss 0.5990	TrainAcc 0.9162	ValidAcc 0.9166	TestAcc 0.9139	BestValid 0.9166
	Epoch 725:	Loss 0.5928	TrainAcc 0.9173	ValidAcc 0.9182	TestAcc 0.9146	BestValid 0.9182
	Epoch 750:	Loss 0.5902	TrainAcc 0.9181	ValidAcc 0.9185	TestAcc 0.9153	BestValid 0.9185
	Epoch 775:	Loss 0.5863	TrainAcc 0.9185	ValidAcc 0.9193	TestAcc 0.9161	BestValid 0.9193
	Epoch 800:	Loss 0.5862	TrainAcc 0.9185	ValidAcc 0.9191	TestAcc 0.9159	BestValid 0.9193
	Epoch 825:	Loss 0.5761	TrainAcc 0.9195	ValidAcc 0.9199	TestAcc 0.9167	BestValid 0.9199
	Epoch 850:	Loss 0.5709	TrainAcc 0.9208	ValidAcc 0.9214	TestAcc 0.9177	BestValid 0.9214
	Epoch 875:	Loss 0.5729	TrainAcc 0.9215	ValidAcc 0.9222	TestAcc 0.9186	BestValid 0.9222
	Epoch 900:	Loss 0.5630	TrainAcc 0.9221	ValidAcc 0.9225	TestAcc 0.9189	BestValid 0.9225
	Epoch 925:	Loss 0.5581	TrainAcc 0.9229	ValidAcc 0.9227	TestAcc 0.9200	BestValid 0.9227
	Epoch 950:	Loss 0.5540	TrainAcc 0.9232	ValidAcc 0.9230	TestAcc 0.9200	BestValid 0.9230
	Epoch 975:	Loss 0.5555	TrainAcc 0.9234	ValidAcc 0.9230	TestAcc 0.9200	BestValid 0.9230
	Epoch 1000:	Loss 0.5525	TrainAcc 0.9245	ValidAcc 0.9244	TestAcc 0.9212	BestValid 0.9244
	Epoch 1025:	Loss 0.5506	TrainAcc 0.9251	ValidAcc 0.9248	TestAcc 0.9210	BestValid 0.9248
	Epoch 1050:	Loss 0.5471	TrainAcc 0.9256	ValidAcc 0.9252	TestAcc 0.9219	BestValid 0.9252
	Epoch 1075:	Loss 0.5443	TrainAcc 0.9262	ValidAcc 0.9261	TestAcc 0.9228	BestValid 0.9261
	Epoch 1100:	Loss 0.5416	TrainAcc 0.9269	ValidAcc 0.9261	TestAcc 0.9230	BestValid 0.9261
	Epoch 1125:	Loss 0.5356	TrainAcc 0.9275	ValidAcc 0.9262	TestAcc 0.9233	BestValid 0.9262
	Epoch 1150:	Loss 0.5347	TrainAcc 0.9277	ValidAcc 0.9261	TestAcc 0.9231	BestValid 0.9262
	Epoch 1175:	Loss 0.5352	TrainAcc 0.9282	ValidAcc 0.9269	TestAcc 0.9240	BestValid 0.9269
	Epoch 1200:	Loss 0.5319	TrainAcc 0.9283	ValidAcc 0.9270	TestAcc 0.9238	BestValid 0.9270
	Epoch 1225:	Loss 0.5299	TrainAcc 0.9287	ValidAcc 0.9271	TestAcc 0.9242	BestValid 0.9271
	Epoch 1250:	Loss 0.5277	TrainAcc 0.9293	ValidAcc 0.9277	TestAcc 0.9248	BestValid 0.9277
	Epoch 1275:	Loss 0.5221	TrainAcc 0.9294	ValidAcc 0.9279	TestAcc 0.9247	BestValid 0.9279
	Epoch 1300:	Loss 0.5204	TrainAcc 0.9300	ValidAcc 0.9284	TestAcc 0.9256	BestValid 0.9284
	Epoch 1325:	Loss 0.5202	TrainAcc 0.9302	ValidAcc 0.9294	TestAcc 0.9259	BestValid 0.9294
	Epoch 1350:	Loss 0.5132	TrainAcc 0.9303	ValidAcc 0.9288	TestAcc 0.9257	BestValid 0.9294
	Epoch 1375:	Loss 0.5162	TrainAcc 0.9313	ValidAcc 0.9297	TestAcc 0.9267	BestValid 0.9297
	Epoch 1400:	Loss 0.5082	TrainAcc 0.9315	ValidAcc 0.9301	TestAcc 0.9268	BestValid 0.9301
	Epoch 1425:	Loss 0.5138	TrainAcc 0.9314	ValidAcc 0.9298	TestAcc 0.9264	BestValid 0.9301
	Epoch 1450:	Loss 0.5130	TrainAcc 0.9315	ValidAcc 0.9302	TestAcc 0.9268	BestValid 0.9302
	Epoch 1475:	Loss 0.5092	TrainAcc 0.9326	ValidAcc 0.9312	TestAcc 0.9278	BestValid 0.9312
	Epoch 1500:	Loss 0.5037	TrainAcc 0.9324	ValidAcc 0.9308	TestAcc 0.9277	BestValid 0.9312
	Epoch 1525:	Loss 0.5036	TrainAcc 0.9333	ValidAcc 0.9317	TestAcc 0.9285	BestValid 0.9317
	Epoch 1550:	Loss 0.4982	TrainAcc 0.9335	ValidAcc 0.9317	TestAcc 0.9286	BestValid 0.9317
	Epoch 1575:	Loss 0.4997	TrainAcc 0.9329	ValidAcc 0.9311	TestAcc 0.9277	BestValid 0.9317
	Epoch 1600:	Loss 0.5028	TrainAcc 0.9340	ValidAcc 0.9320	TestAcc 0.9290	BestValid 0.9320
	Epoch 1625:	Loss 0.4948	TrainAcc 0.9343	ValidAcc 0.9319	TestAcc 0.9290	BestValid 0.9320
	Epoch 1650:	Loss 0.4958	TrainAcc 0.9345	ValidAcc 0.9321	TestAcc 0.9291	BestValid 0.9321
	Epoch 1675:	Loss 0.4953	TrainAcc 0.9343	ValidAcc 0.9320	TestAcc 0.9290	BestValid 0.9321
	Epoch 1700:	Loss 0.4925	TrainAcc 0.9344	ValidAcc 0.9319	TestAcc 0.9294	BestValid 0.9321
	Epoch 1725:	Loss 0.4910	TrainAcc 0.9351	ValidAcc 0.9323	TestAcc 0.9300	BestValid 0.9323
	Epoch 1750:	Loss 0.4922	TrainAcc 0.9350	ValidAcc 0.9325	TestAcc 0.9298	BestValid 0.9325
	Epoch 1775:	Loss 0.4948	TrainAcc 0.9351	ValidAcc 0.9328	TestAcc 0.9299	BestValid 0.9328
	Epoch 1800:	Loss 0.4875	TrainAcc 0.9359	ValidAcc 0.9329	TestAcc 0.9305	BestValid 0.9329
	Epoch 1825:	Loss 0.4891	TrainAcc 0.9363	ValidAcc 0.9336	TestAcc 0.9309	BestValid 0.9336
	Epoch 1850:	Loss 0.4822	TrainAcc 0.9364	ValidAcc 0.9334	TestAcc 0.9308	BestValid 0.9336
	Epoch 1875:	Loss 0.4859	TrainAcc 0.9364	ValidAcc 0.9334	TestAcc 0.9310	BestValid 0.9336
	Epoch 1900:	Loss 0.4828	TrainAcc 0.9365	ValidAcc 0.9335	TestAcc 0.9310	BestValid 0.9336
	Epoch 1925:	Loss 0.4795	TrainAcc 0.9369	ValidAcc 0.9343	TestAcc 0.9313	BestValid 0.9343
	Epoch 1950:	Loss 0.4806	TrainAcc 0.9374	ValidAcc 0.9344	TestAcc 0.9317	BestValid 0.9344
	Epoch 1975:	Loss 0.4767	TrainAcc 0.9366	ValidAcc 0.9343	TestAcc 0.9312	BestValid 0.9344
	Epoch 2000:	Loss 0.4784	TrainAcc 0.9372	ValidAcc 0.9340	TestAcc 0.9318	BestValid 0.9344
	Epoch 2025:	Loss 0.4754	TrainAcc 0.9373	ValidAcc 0.9345	TestAcc 0.9316	BestValid 0.9345
	Epoch 2050:	Loss 0.4770	TrainAcc 0.9377	ValidAcc 0.9346	TestAcc 0.9319	BestValid 0.9346
	Epoch 2075:	Loss 0.4781	TrainAcc 0.9384	ValidAcc 0.9348	TestAcc 0.9325	BestValid 0.9348
	Epoch 2100:	Loss 0.4746	TrainAcc 0.9380	ValidAcc 0.9352	TestAcc 0.9321	BestValid 0.9352
	Epoch 2125:	Loss 0.4750	TrainAcc 0.9387	ValidAcc 0.9355	TestAcc 0.9328	BestValid 0.9355
	Epoch 2150:	Loss 0.4721	TrainAcc 0.9386	ValidAcc 0.9351	TestAcc 0.9328	BestValid 0.9355
	Epoch 2175:	Loss 0.4709	TrainAcc 0.9386	ValidAcc 0.9350	TestAcc 0.9328	BestValid 0.9355
	Epoch 2200:	Loss 0.4772	TrainAcc 0.9386	ValidAcc 0.9356	TestAcc 0.9330	BestValid 0.9356
	Epoch 2225:	Loss 0.4679	TrainAcc 0.9390	ValidAcc 0.9357	TestAcc 0.9329	BestValid 0.9357
	Epoch 2250:	Loss 0.4691	TrainAcc 0.9392	ValidAcc 0.9355	TestAcc 0.9332	BestValid 0.9357
	Epoch 2275:	Loss 0.4671	TrainAcc 0.9391	ValidAcc 0.9354	TestAcc 0.9334	BestValid 0.9357
	Epoch 2300:	Loss 0.4668	TrainAcc 0.9396	ValidAcc 0.9360	TestAcc 0.9341	BestValid 0.9360
	Epoch 2325:	Loss 0.4648	TrainAcc 0.9399	ValidAcc 0.9363	TestAcc 0.9336	BestValid 0.9363
	Epoch 2350:	Loss 0.4635	TrainAcc 0.9399	ValidAcc 0.9363	TestAcc 0.9337	BestValid 0.9363
	Epoch 2375:	Loss 0.4631	TrainAcc 0.9403	ValidAcc 0.9360	TestAcc 0.9343	BestValid 0.9363
	Epoch 2400:	Loss 0.4640	TrainAcc 0.9401	ValidAcc 0.9363	TestAcc 0.9340	BestValid 0.9363
	Epoch 2425:	Loss 0.4590	TrainAcc 0.9404	ValidAcc 0.9364	TestAcc 0.9346	BestValid 0.9364
	Epoch 2450:	Loss 0.4604	TrainAcc 0.9403	ValidAcc 0.9363	TestAcc 0.9342	BestValid 0.9364
	Epoch 2475:	Loss 0.4597	TrainAcc 0.9405	ValidAcc 0.9365	TestAcc 0.9344	BestValid 0.9365
	Epoch 2500:	Loss 0.4575	TrainAcc 0.9409	ValidAcc 0.9370	TestAcc 0.9347	BestValid 0.9370
	Epoch 2525:	Loss 0.4605	TrainAcc 0.9409	ValidAcc 0.9369	TestAcc 0.9346	BestValid 0.9370
	Epoch 2550:	Loss 0.4579	TrainAcc 0.9412	ValidAcc 0.9370	TestAcc 0.9350	BestValid 0.9370
	Epoch 2575:	Loss 0.4585	TrainAcc 0.9409	ValidAcc 0.9367	TestAcc 0.9350	BestValid 0.9370
	Epoch 2600:	Loss 0.4542	TrainAcc 0.9415	ValidAcc 0.9371	TestAcc 0.9356	BestValid 0.9371
	Epoch 2625:	Loss 0.4606	TrainAcc 0.9412	ValidAcc 0.9368	TestAcc 0.9350	BestValid 0.9371
	Epoch 2650:	Loss 0.4550	TrainAcc 0.9415	ValidAcc 0.9369	TestAcc 0.9352	BestValid 0.9371
	Epoch 2675:	Loss 0.4518	TrainAcc 0.9417	ValidAcc 0.9371	TestAcc 0.9354	BestValid 0.9371
	Epoch 2700:	Loss 0.4572	TrainAcc 0.9414	ValidAcc 0.9370	TestAcc 0.9352	BestValid 0.9371
	Epoch 2725:	Loss 0.4566	TrainAcc 0.9419	ValidAcc 0.9374	TestAcc 0.9358	BestValid 0.9374
	Epoch 2750:	Loss 0.4532	TrainAcc 0.9415	ValidAcc 0.9372	TestAcc 0.9351	BestValid 0.9374
	Epoch 2775:	Loss 0.4509	TrainAcc 0.9420	ValidAcc 0.9380	TestAcc 0.9357	BestValid 0.9380
	Epoch 2800:	Loss 0.4518	TrainAcc 0.9422	ValidAcc 0.9375	TestAcc 0.9363	BestValid 0.9380
	Epoch 2825:	Loss 0.4473	TrainAcc 0.9420	ValidAcc 0.9374	TestAcc 0.9359	BestValid 0.9380
	Epoch 2850:	Loss 0.4530	TrainAcc 0.9424	ValidAcc 0.9379	TestAcc 0.9359	BestValid 0.9380
	Epoch 2875:	Loss 0.4519	TrainAcc 0.9425	ValidAcc 0.9377	TestAcc 0.9359	BestValid 0.9380
	Epoch 2900:	Loss 0.4538	TrainAcc 0.9428	ValidAcc 0.9381	TestAcc 0.9361	BestValid 0.9381
	Epoch 2925:	Loss 0.4481	TrainAcc 0.9430	ValidAcc 0.9383	TestAcc 0.9364	BestValid 0.9383
	Epoch 2950:	Loss 0.4517	TrainAcc 0.9433	ValidAcc 0.9384	TestAcc 0.9366	BestValid 0.9384
	Epoch 2975:	Loss 0.4441	TrainAcc 0.9433	ValidAcc 0.9384	TestAcc 0.9370	BestValid 0.9384
	Epoch 3000:	Loss 0.4459	TrainAcc 0.9428	ValidAcc 0.9385	TestAcc 0.9360	BestValid 0.9385
	Epoch 3025:	Loss 0.4465	TrainAcc 0.9434	ValidAcc 0.9386	TestAcc 0.9369	BestValid 0.9386
	Epoch 3050:	Loss 0.4476	TrainAcc 0.9425	ValidAcc 0.9379	TestAcc 0.9359	BestValid 0.9386
	Epoch 3075:	Loss 0.4454	TrainAcc 0.9436	ValidAcc 0.9389	TestAcc 0.9372	BestValid 0.9389
	Epoch 3100:	Loss 0.4441	TrainAcc 0.9432	ValidAcc 0.9387	TestAcc 0.9366	BestValid 0.9389
	Epoch 3125:	Loss 0.4393	TrainAcc 0.9434	ValidAcc 0.9389	TestAcc 0.9369	BestValid 0.9389
	Epoch 3150:	Loss 0.4408	TrainAcc 0.9429	ValidAcc 0.9384	TestAcc 0.9366	BestValid 0.9389
	Epoch 3175:	Loss 0.4386	TrainAcc 0.9431	ValidAcc 0.9383	TestAcc 0.9368	BestValid 0.9389
	Epoch 3200:	Loss 0.4447	TrainAcc 0.9441	ValidAcc 0.9395	TestAcc 0.9374	BestValid 0.9395
	Epoch 3225:	Loss 0.4442	TrainAcc 0.9442	ValidAcc 0.9395	TestAcc 0.9373	BestValid 0.9395
	Epoch 3250:	Loss 0.4396	TrainAcc 0.9437	ValidAcc 0.9388	TestAcc 0.9368	BestValid 0.9395
	Epoch 3275:	Loss 0.4370	TrainAcc 0.9443	ValidAcc 0.9393	TestAcc 0.9377	BestValid 0.9395
	Epoch 3300:	Loss 0.4356	TrainAcc 0.9446	ValidAcc 0.9397	TestAcc 0.9378	BestValid 0.9397
	Epoch 3325:	Loss 0.4393	TrainAcc 0.9440	ValidAcc 0.9387	TestAcc 0.9372	BestValid 0.9397
	Epoch 3350:	Loss 0.4412	TrainAcc 0.9443	ValidAcc 0.9393	TestAcc 0.9376	BestValid 0.9397
	Epoch 3375:	Loss 0.4375	TrainAcc 0.9443	ValidAcc 0.9397	TestAcc 0.9370	BestValid 0.9397
	Epoch 3400:	Loss 0.4384	TrainAcc 0.9446	ValidAcc 0.9395	TestAcc 0.9377	BestValid 0.9397
	Epoch 3425:	Loss 0.4337	TrainAcc 0.9448	ValidAcc 0.9400	TestAcc 0.9379	BestValid 0.9400
	Epoch 3450:	Loss 0.4349	TrainAcc 0.9447	ValidAcc 0.9395	TestAcc 0.9379	BestValid 0.9400
	Epoch 3475:	Loss 0.4340	TrainAcc 0.9451	ValidAcc 0.9403	TestAcc 0.9380	BestValid 0.9403
	Epoch 3500:	Loss 0.4352	TrainAcc 0.9442	ValidAcc 0.9392	TestAcc 0.9373	BestValid 0.9403
	Epoch 3525:	Loss 0.4327	TrainAcc 0.9447	ValidAcc 0.9397	TestAcc 0.9379	BestValid 0.9403
	Epoch 3550:	Loss 0.4349	TrainAcc 0.9446	ValidAcc 0.9394	TestAcc 0.9375	BestValid 0.9403
	Epoch 3575:	Loss 0.4344	TrainAcc 0.9454	ValidAcc 0.9406	TestAcc 0.9382	BestValid 0.9406
	Epoch 3600:	Loss 0.4343	TrainAcc 0.9450	ValidAcc 0.9399	TestAcc 0.9381	BestValid 0.9406
	Epoch 3625:	Loss 0.4321	TrainAcc 0.9452	ValidAcc 0.9402	TestAcc 0.9384	BestValid 0.9406
	Epoch 3650:	Loss 0.4360	TrainAcc 0.9447	ValidAcc 0.9400	TestAcc 0.9375	BestValid 0.9406
	Epoch 3675:	Loss 0.4301	TrainAcc 0.9457	ValidAcc 0.9408	TestAcc 0.9387	BestValid 0.9408
	Epoch 3700:	Loss 0.4364	TrainAcc 0.9449	ValidAcc 0.9402	TestAcc 0.9378	BestValid 0.9408
	Epoch 3725:	Loss 0.4290	TrainAcc 0.9460	ValidAcc 0.9410	TestAcc 0.9387	BestValid 0.9410
	Epoch 3750:	Loss 0.4307	TrainAcc 0.9457	ValidAcc 0.9409	TestAcc 0.9386	BestValid 0.9410
	Epoch 3775:	Loss 0.4326	TrainAcc 0.9450	ValidAcc 0.9402	TestAcc 0.9381	BestValid 0.9410
	Epoch 3800:	Loss 0.4302	TrainAcc 0.9455	ValidAcc 0.9403	TestAcc 0.9383	BestValid 0.9410
	Epoch 3825:	Loss 0.4306	TrainAcc 0.9452	ValidAcc 0.9405	TestAcc 0.9382	BestValid 0.9410
	Epoch 3850:	Loss 0.4287	TrainAcc 0.9452	ValidAcc 0.9404	TestAcc 0.9381	BestValid 0.9410
	Epoch 3875:	Loss 0.4280	TrainAcc 0.9460	ValidAcc 0.9411	TestAcc 0.9394	BestValid 0.9411
	Epoch 3900:	Loss 0.4290	TrainAcc 0.9460	ValidAcc 0.9407	TestAcc 0.9389	BestValid 0.9411
	Epoch 3925:	Loss 0.4303	TrainAcc 0.9455	ValidAcc 0.9406	TestAcc 0.9384	BestValid 0.9411
	Epoch 3950:	Loss 0.4242	TrainAcc 0.9462	ValidAcc 0.9416	TestAcc 0.9391	BestValid 0.9416
	Epoch 3975:	Loss 0.4307	TrainAcc 0.9460	ValidAcc 0.9411	TestAcc 0.9390	BestValid 0.9416
	Epoch 4000:	Loss 0.4274	TrainAcc 0.9461	ValidAcc 0.9414	TestAcc 0.9387	BestValid 0.9416
	Epoch 4025:	Loss 0.4258	TrainAcc 0.9460	ValidAcc 0.9415	TestAcc 0.9395	BestValid 0.9416
	Epoch 4050:	Loss 0.4217	TrainAcc 0.9460	ValidAcc 0.9410	TestAcc 0.9389	BestValid 0.9416
	Epoch 4075:	Loss 0.4247	TrainAcc 0.9462	ValidAcc 0.9413	TestAcc 0.9391	BestValid 0.9416
	Epoch 4100:	Loss 0.4267	TrainAcc 0.9468	ValidAcc 0.9418	TestAcc 0.9399	BestValid 0.9418
	Epoch 4125:	Loss 0.4182	TrainAcc 0.9463	ValidAcc 0.9411	TestAcc 0.9392	BestValid 0.9418
	Epoch 4150:	Loss 0.4241	TrainAcc 0.9470	ValidAcc 0.9417	TestAcc 0.9400	BestValid 0.9418
	Epoch 4175:	Loss 0.4249	TrainAcc 0.9468	ValidAcc 0.9416	TestAcc 0.9399	BestValid 0.9418
	Epoch 4200:	Loss 0.4224	TrainAcc 0.9468	ValidAcc 0.9419	TestAcc 0.9398	BestValid 0.9419
	Epoch 4225:	Loss 0.4206	TrainAcc 0.9469	ValidAcc 0.9420	TestAcc 0.9398	BestValid 0.9420
	Epoch 4250:	Loss 0.4215	TrainAcc 0.9476	ValidAcc 0.9423	TestAcc 0.9405	BestValid 0.9423
	Epoch 4275:	Loss 0.4240	TrainAcc 0.9466	ValidAcc 0.9414	TestAcc 0.9395	BestValid 0.9423
	Epoch 4300:	Loss 0.4211	TrainAcc 0.9466	ValidAcc 0.9416	TestAcc 0.9396	BestValid 0.9423
	Epoch 4325:	Loss 0.4196	TrainAcc 0.9467	ValidAcc 0.9414	TestAcc 0.9396	BestValid 0.9423
	Epoch 4350:	Loss 0.4175	TrainAcc 0.9470	ValidAcc 0.9416	TestAcc 0.9403	BestValid 0.9423
	Epoch 4375:	Loss 0.4189	TrainAcc 0.9468	ValidAcc 0.9418	TestAcc 0.9397	BestValid 0.9423
	Epoch 4400:	Loss 0.4193	TrainAcc 0.9467	ValidAcc 0.9417	TestAcc 0.9395	BestValid 0.9423
	Epoch 4425:	Loss 0.4198	TrainAcc 0.9472	ValidAcc 0.9416	TestAcc 0.9402	BestValid 0.9423
	Epoch 4450:	Loss 0.4166	TrainAcc 0.9470	ValidAcc 0.9417	TestAcc 0.9397	BestValid 0.9423
	Epoch 4475:	Loss 0.4122	TrainAcc 0.9468	ValidAcc 0.9415	TestAcc 0.9394	BestValid 0.9423
	Epoch 4500:	Loss 0.4146	TrainAcc 0.9475	ValidAcc 0.9419	TestAcc 0.9404	BestValid 0.9423
	Epoch 4525:	Loss 0.4161	TrainAcc 0.9474	ValidAcc 0.9418	TestAcc 0.9403	BestValid 0.9423
	Epoch 4550:	Loss 0.4194	TrainAcc 0.9480	ValidAcc 0.9424	TestAcc 0.9409	BestValid 0.9424
	Epoch 4575:	Loss 0.4146	TrainAcc 0.9481	ValidAcc 0.9426	TestAcc 0.9409	BestValid 0.9426
	Epoch 4600:	Loss 0.4154	TrainAcc 0.9476	ValidAcc 0.9424	TestAcc 0.9403	BestValid 0.9426
	Epoch 4625:	Loss 0.4069	TrainAcc 0.9475	ValidAcc 0.9422	TestAcc 0.9404	BestValid 0.9426
	Epoch 4650:	Loss 0.4162	TrainAcc 0.9481	ValidAcc 0.9423	TestAcc 0.9404	BestValid 0.9426
	Epoch 4675:	Loss 0.4189	TrainAcc 0.9481	ValidAcc 0.9422	TestAcc 0.9408	BestValid 0.9426
	Epoch 4700:	Loss 0.4148	TrainAcc 0.9474	ValidAcc 0.9422	TestAcc 0.9401	BestValid 0.9426
	Epoch 4725:	Loss 0.4153	TrainAcc 0.9479	ValidAcc 0.9425	TestAcc 0.9409	BestValid 0.9426
	Epoch 4750:	Loss 0.4174	TrainAcc 0.9479	ValidAcc 0.9426	TestAcc 0.9411	BestValid 0.9426
	Epoch 4775:	Loss 0.4173	TrainAcc 0.9479	ValidAcc 0.9428	TestAcc 0.9404	BestValid 0.9428
	Epoch 4800:	Loss 0.4091	TrainAcc 0.9475	ValidAcc 0.9424	TestAcc 0.9402	BestValid 0.9428
	Epoch 4825:	Loss 0.4080	TrainAcc 0.9482	ValidAcc 0.9428	TestAcc 0.9410	BestValid 0.9428
	Epoch 4850:	Loss 0.4085	TrainAcc 0.9482	ValidAcc 0.9432	TestAcc 0.9411	BestValid 0.9432
	Epoch 4875:	Loss 0.4113	TrainAcc 0.9472	ValidAcc 0.9423	TestAcc 0.9398	BestValid 0.9432
	Epoch 4900:	Loss 0.4078	TrainAcc 0.9481	ValidAcc 0.9429	TestAcc 0.9412	BestValid 0.9432
	Epoch 4925:	Loss 0.4100	TrainAcc 0.9485	ValidAcc 0.9434	TestAcc 0.9417	BestValid 0.9434
	Epoch 4950:	Loss 0.4089	TrainAcc 0.9480	ValidAcc 0.9427	TestAcc 0.9404	BestValid 0.9434
	Epoch 4975:	Loss 0.4075	TrainAcc 0.9485	ValidAcc 0.9434	TestAcc 0.9411	BestValid 0.9434
	Epoch 5000:	Loss 0.4122	TrainAcc 0.9487	ValidAcc 0.9437	TestAcc 0.9416	BestValid 0.9437
Node 1, Pre/Post-Pipelining: 1.090 / 12.242 ms, Bubble: 81.065 ms, Compute: 256.426 ms, Comm: 42.212 ms, Imbalance: 32.238 ms
Node 4, Pre/Post-Pipelining: 1.092 / 12.198 ms, Bubble: 81.434 ms, Compute: 250.563 ms, Comm: 46.559 ms, Imbalance: 34.340 ms
Node 2, Pre/Post-Pipelining: 1.092 / 12.179 ms, Bubble: 81.844 ms, Compute: 248.576 ms, Comm: 46.873 ms, Imbalance: 35.361 ms
Node 5, Pre/Post-Pipelining: 1.091 / 12.188 ms, Bubble: 81.682 ms, Compute: 254.370 ms, Comm: 45.149 ms, Imbalance: 31.348 ms
Node 3, Pre/Post-Pipelining: 1.092 / 12.191 ms, Bubble: 80.768 ms, Compute: 255.297 ms, Comm: 46.860 ms, Imbalance: 29.376 ms
Node 7, Pre/Post-Pipelining: 1.097 / 27.465 ms, Bubble: 66.852 ms, Compute: 281.546 ms, Comm: 32.067 ms, Imbalance: 16.370 ms
Node 6, Pre/Post-Pipelining: 1.091 / 12.209 ms, Bubble: 81.848 ms, Compute: 256.369 ms, Comm: 40.671 ms, Imbalance: 33.698 ms
Node 0, Pre/Post-Pipelining: 1.093 / 12.279 ms, Bubble: 81.027 ms, Compute: 286.125 ms, Comm: 32.596 ms, Imbalance: 11.519 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.093 ms
Cluster-Wide Average, Post-Pipelining Overhead: 12.279 ms
Cluster-Wide Average, Bubble: 81.027 ms
Cluster-Wide Average, Compute: 286.125 ms
Cluster-Wide Average, Communication: 32.596 ms
Cluster-Wide Average, Imbalance: 11.519 ms
Node 0, GPU memory consumption: 8.059 GB
Node 2, GPU memory consumption: 6.042 GB
Node 3, GPU memory consumption: 6.018 GB
Node 1, GPU memory consumption: 6.042 GB
Node 4, GPU memory consumption: 6.018 GB
Node 5, GPU memory consumption: 6.042 GB
Node 6, GPU memory consumption: 6.042 GB
Node 7, GPU memory consumption: 6.171 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.904824 s---------------
------------------------node id 1,  per-epoch time: 0.904824 s---------------
------------------------node id 2,  per-epoch time: 0.904824 s---------------
------------------------node id 3,  per-epoch time: 0.904824 s---------------
------------------------node id 4,  per-epoch time: 0.904824 s---------------
------------------------node id 5,  per-epoch time: 0.904824 s---------------
------------------------node id 6,  per-epoch time: 0.904824 s---------------
------------------------node id 7,  per-epoch time: 0.904824 s---------------
************ Profiling Results ************
	Bubble: 643.994268 (ms) (71.20 percentage)
	Compute: 256.991926 (ms) (28.41 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.483979 (ms) (0.39 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 2.441 GB
	Aggregated layer-level communication throughput: 501.490 Gbps
Highest valid_acc: 0.9437
Target test_acc: 0.9416
Epoch to reach the target acc: 4999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
