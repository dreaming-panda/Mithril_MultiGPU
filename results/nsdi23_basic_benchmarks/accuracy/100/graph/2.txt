Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.866 seconds.
Building the CSC structure...
        It takes 2.020 seconds.
Building the CSC structure...
        It takes 2.052 seconds.
Building the CSC structure...
        It takes 2.381 seconds.
Building the CSC structure...
        It takes 2.434 seconds.
Building the CSC structure...
        It takes 2.481 seconds.
Building the CSC structure...
        It takes 2.587 seconds.
Building the CSC structure...
        It takes 2.636 seconds.
Building the CSC structure...
        It takes 1.831 seconds.
        It takes 1.873 seconds.
        It takes 1.858 seconds.
Building the Feature Vector...
        It takes 2.430 seconds.
        It takes 2.385 seconds.
        It takes 2.360 seconds.
        It takes 2.290 seconds.
        It takes 0.252 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 2.377 seconds.
Building the Feature Vector...
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.270 seconds.
Building the Label Vector...
GPU 0, layer [0, 33)
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.283 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.285 seconds.
Building the Label Vector...
GPU 0, layer [0, 33)
        It takes 0.033 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.256 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
        It takes 0.302 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 62.632 Gbps (per GPU), 501.058 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.590 Gbps (per GPU), 500.717 Gbps (aggregated)
The layer-level communication performance: 62.588 Gbps (per GPU), 500.706 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.555 Gbps (per GPU), 500.438 Gbps (aggregated)
The layer-level communication performance: 62.550 Gbps (per GPU), 500.402 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.518 Gbps (per GPU), 500.147 Gbps (aggregated)
The layer-level communication performance: 62.511 Gbps (per GPU), 500.091 Gbps (aggregated)
The layer-level communication performance: 62.507 Gbps (per GPU), 500.059 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.865 Gbps (per GPU), 1326.920 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.856 Gbps (per GPU), 1326.846 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.864 Gbps (per GPU), 1326.914 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.863 Gbps (per GPU), 1326.908 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.861 Gbps (per GPU), 1326.885 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.859 Gbps (per GPU), 1326.875 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.859 Gbps (per GPU), 1326.872 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.861 Gbps (per GPU), 1326.888 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.292 Gbps (per GPU), 914.335 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.293 Gbps (per GPU), 914.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.292 Gbps (per GPU), 914.336 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.292 Gbps (per GPU), 914.332 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.292 Gbps (per GPU), 914.336 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.292 Gbps (per GPU), 914.335 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.291 Gbps (per GPU), 914.330 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.292 Gbps (per GPU), 914.336 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.922 Gbps (per GPU), 367.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.922 Gbps (per GPU), 367.380 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.922 Gbps (per GPU), 367.377 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.922 Gbps (per GPU), 367.375 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.922 Gbps (per GPU), 367.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.922 Gbps (per GPU), 367.377 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.922 Gbps (per GPU), 367.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.922 Gbps (per GPU), 367.378 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.53ms  9.70ms 10.02ms  3.97 29.12K 14.23M
 chk_1  2.55ms  5.25ms  5.61ms  2.20 29.12K  6.56M
 chk_2  2.54ms 17.09ms 17.36ms  6.82 29.12K 24.68M
 chk_3  2.55ms 16.93ms 17.40ms  6.81 29.12K 22.95M
 chk_4  2.55ms  5.08ms  5.45ms  2.14 29.12K  6.33M
 chk_5  2.56ms  9.46ms  9.58ms  3.74 29.12K 12.05M
 chk_6  2.56ms 10.39ms 10.75ms  4.20 29.12K 14.60M
 chk_7  2.56ms  9.60ms 10.08ms  3.94 29.12K 13.21M
   Avg  2.55 10.44 10.78
   Max  2.56 17.09 17.40
   Min  2.53  5.08  5.45
 Ratio  1.01  3.36  3.19
   Var  0.00 18.03 18.08
Profiling takes 2.209 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 354.371 ms
Partition 0 [0, 5) has cost: 354.371 ms
Partition 1 [5, 9) has cost: 333.966 ms
Partition 2 [9, 13) has cost: 333.966 ms
Partition 3 [13, 17) has cost: 333.966 ms
Partition 4 [17, 21) has cost: 333.966 ms
Partition 5 [21, 25) has cost: 333.966 ms
Partition 6 [25, 29) has cost: 333.966 ms
Partition 7 [29, 33) has cost: 336.744 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 306.943 ms
GPU 0, Compute+Comm Time: 126.763 ms, Bubble Time: 132.522 ms, Imbalance Overhead: 47.657 ms
GPU 1, Compute+Comm Time: 121.659 ms, Bubble Time: 122.979 ms, Imbalance Overhead: 62.305 ms
GPU 2, Compute+Comm Time: 121.659 ms, Bubble Time: 112.765 ms, Imbalance Overhead: 72.519 ms
GPU 3, Compute+Comm Time: 121.659 ms, Bubble Time: 113.485 ms, Imbalance Overhead: 71.799 ms
GPU 4, Compute+Comm Time: 121.659 ms, Bubble Time: 122.736 ms, Imbalance Overhead: 62.548 ms
GPU 5, Compute+Comm Time: 121.659 ms, Bubble Time: 131.544 ms, Imbalance Overhead: 53.740 ms
GPU 6, Compute+Comm Time: 121.659 ms, Bubble Time: 140.301 ms, Imbalance Overhead: 44.983 ms
GPU 7, Compute+Comm Time: 122.248 ms, Bubble Time: 150.289 ms, Imbalance Overhead: 34.406 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 601.392 ms
GPU 0, Compute+Comm Time: 238.301 ms, Bubble Time: 296.840 ms, Imbalance Overhead: 66.251 ms
GPU 1, Compute+Comm Time: 236.113 ms, Bubble Time: 277.367 ms, Imbalance Overhead: 87.913 ms
GPU 2, Compute+Comm Time: 236.113 ms, Bubble Time: 259.695 ms, Imbalance Overhead: 105.585 ms
GPU 3, Compute+Comm Time: 236.113 ms, Bubble Time: 241.710 ms, Imbalance Overhead: 123.570 ms
GPU 4, Compute+Comm Time: 236.113 ms, Bubble Time: 222.217 ms, Imbalance Overhead: 143.062 ms
GPU 5, Compute+Comm Time: 236.113 ms, Bubble Time: 220.255 ms, Imbalance Overhead: 145.025 ms
GPU 6, Compute+Comm Time: 236.113 ms, Bubble Time: 239.877 ms, Imbalance Overhead: 125.403 ms
GPU 7, Compute+Comm Time: 251.413 ms, Bubble Time: 257.366 ms, Imbalance Overhead: 92.613 ms
The estimated cost of the whole pipeline: 953.752 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 688.337 ms
Partition 0 [0, 9) has cost: 688.337 ms
Partition 1 [9, 17) has cost: 667.933 ms
Partition 2 [17, 25) has cost: 667.933 ms
Partition 3 [25, 33) has cost: 670.710 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 299.397 ms
GPU 0, Compute+Comm Time: 146.081 ms, Bubble Time: 116.416 ms, Imbalance Overhead: 36.900 ms
GPU 1, Compute+Comm Time: 143.521 ms, Bubble Time: 96.649 ms, Imbalance Overhead: 59.228 ms
GPU 2, Compute+Comm Time: 143.521 ms, Bubble Time: 114.904 ms, Imbalance Overhead: 40.972 ms
GPU 3, Compute+Comm Time: 143.689 ms, Bubble Time: 132.470 ms, Imbalance Overhead: 23.238 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 570.006 ms
GPU 0, Compute+Comm Time: 272.245 ms, Bubble Time: 254.905 ms, Imbalance Overhead: 42.856 ms
GPU 1, Compute+Comm Time: 271.286 ms, Bubble Time: 219.248 ms, Imbalance Overhead: 79.471 ms
GPU 2, Compute+Comm Time: 271.286 ms, Bubble Time: 181.364 ms, Imbalance Overhead: 117.355 ms
GPU 3, Compute+Comm Time: 278.948 ms, Bubble Time: 218.482 ms, Imbalance Overhead: 72.576 ms
    The estimated cost with 2 DP ways is 912.873 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1356.270 ms
Partition 0 [0, 17) has cost: 1356.270 ms
Partition 1 [17, 33) has cost: 1338.643 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 313.811 ms
GPU 0, Compute+Comm Time: 198.242 ms, Bubble Time: 80.915 ms, Imbalance Overhead: 34.654 ms
GPU 1, Compute+Comm Time: 197.075 ms, Bubble Time: 116.736 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 555.475 ms
GPU 0, Compute+Comm Time: 345.248 ms, Bubble Time: 210.227 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 348.519 ms, Bubble Time: 136.686 ms, Imbalance Overhead: 70.270 ms
    The estimated cost with 4 DP ways is 912.750 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2694.913 ms
Partition 0 [0, 33) has cost: 2694.913 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 409.134 ms
GPU 0, Compute+Comm Time: 409.134 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 595.030 ms
GPU 0, Compute+Comm Time: 595.030 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1054.373 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.3223	TrainAcc 0.5247	ValidAcc 0.5497	TestAcc 0.5441	BestValid 0.5497
	Epoch 50:	Loss 1.7024	TrainAcc 0.6863	ValidAcc 0.7067	TestAcc 0.7002	BestValid 0.7067
	Epoch 75:	Loss 1.3715	TrainAcc 0.7432	ValidAcc 0.7580	TestAcc 0.7539	BestValid 0.7580
	Epoch 100:	Loss 1.1726	TrainAcc 0.7893	ValidAcc 0.8031	TestAcc 0.7977	BestValid 0.8031
	Epoch 125:	Loss 1.0379	TrainAcc 0.8207	ValidAcc 0.8313	TestAcc 0.8266	BestValid 0.8313
	Epoch 150:	Loss 0.9396	TrainAcc 0.8409	ValidAcc 0.8517	TestAcc 0.8457	BestValid 0.8517
	Epoch 175:	Loss 0.8656	TrainAcc 0.8565	ValidAcc 0.8660	TestAcc 0.8603	BestValid 0.8660
	Epoch 200:	Loss 0.8128	TrainAcc 0.8672	ValidAcc 0.8757	TestAcc 0.8703	BestValid 0.8757
	Epoch 225:	Loss 0.7695	TrainAcc 0.8755	ValidAcc 0.8834	TestAcc 0.8783	BestValid 0.8834
	Epoch 250:	Loss 0.7341	TrainAcc 0.8830	ValidAcc 0.8900	TestAcc 0.8850	BestValid 0.8900
	Epoch 275:	Loss 0.7043	TrainAcc 0.8863	ValidAcc 0.8934	TestAcc 0.8882	BestValid 0.8934
	Epoch 300:	Loss 0.6805	TrainAcc 0.8909	ValidAcc 0.8973	TestAcc 0.8924	BestValid 0.8973
	Epoch 325:	Loss 0.6650	TrainAcc 0.8958	ValidAcc 0.9016	TestAcc 0.8968	BestValid 0.9016
	Epoch 350:	Loss 0.6417	TrainAcc 0.8971	ValidAcc 0.9028	TestAcc 0.8988	BestValid 0.9028
	Epoch 375:	Loss 0.6262	TrainAcc 0.8988	ValidAcc 0.9050	TestAcc 0.9009	BestValid 0.9050
	Epoch 400:	Loss 0.6113	TrainAcc 0.9032	ValidAcc 0.9097	TestAcc 0.9049	BestValid 0.9097
	Epoch 425:	Loss 0.5995	TrainAcc 0.9040	ValidAcc 0.9103	TestAcc 0.9058	BestValid 0.9103
	Epoch 450:	Loss 0.5884	TrainAcc 0.9052	ValidAcc 0.9115	TestAcc 0.9065	BestValid 0.9115
	Epoch 475:	Loss 0.5819	TrainAcc 0.9066	ValidAcc 0.9128	TestAcc 0.9081	BestValid 0.9128
	Epoch 500:	Loss 0.5697	TrainAcc 0.9066	ValidAcc 0.9130	TestAcc 0.9084	BestValid 0.9130
	Epoch 525:	Loss 0.5603	TrainAcc 0.9057	ValidAcc 0.9114	TestAcc 0.9074	BestValid 0.9130
	Epoch 550:	Loss 0.5573	TrainAcc 0.9081	ValidAcc 0.9136	TestAcc 0.9093	BestValid 0.9136
	Epoch 575:	Loss 0.5478	TrainAcc 0.9118	ValidAcc 0.9169	TestAcc 0.9123	BestValid 0.9169
	Epoch 600:	Loss 0.5404	TrainAcc 0.9109	ValidAcc 0.9155	TestAcc 0.9118	BestValid 0.9169
	Epoch 625:	Loss 0.5403	TrainAcc 0.9150	ValidAcc 0.9206	TestAcc 0.9149	BestValid 0.9206
	Epoch 650:	Loss 0.5283	TrainAcc 0.9109	ValidAcc 0.9159	TestAcc 0.9119	BestValid 0.9206
	Epoch 675:	Loss 0.5254	TrainAcc 0.9124	ValidAcc 0.9172	TestAcc 0.9133	BestValid 0.9206
	Epoch 700:	Loss 0.5220	TrainAcc 0.9146	ValidAcc 0.9192	TestAcc 0.9151	BestValid 0.9206
	Epoch 725:	Loss 0.5169	TrainAcc 0.9141	ValidAcc 0.9184	TestAcc 0.9147	BestValid 0.9206
	Epoch 750:	Loss 0.5115	TrainAcc 0.9171	ValidAcc 0.9217	TestAcc 0.9167	BestValid 0.9217
	Epoch 775:	Loss 0.5127	TrainAcc 0.9162	ValidAcc 0.9201	TestAcc 0.9162	BestValid 0.9217
	Epoch 800:	Loss 0.5036	TrainAcc 0.9175	ValidAcc 0.9218	TestAcc 0.9173	BestValid 0.9218
	Epoch 825:	Loss 0.5081	TrainAcc 0.9190	ValidAcc 0.9227	TestAcc 0.9186	BestValid 0.9227
	Epoch 850:	Loss 0.4974	TrainAcc 0.9149	ValidAcc 0.9183	TestAcc 0.9151	BestValid 0.9227
	Epoch 875:	Loss 0.4945	TrainAcc 0.9171	ValidAcc 0.9203	TestAcc 0.9168	BestValid 0.9227
	Epoch 900:	Loss 0.4962	TrainAcc 0.9156	ValidAcc 0.9185	TestAcc 0.9157	BestValid 0.9227
	Epoch 925:	Loss 0.4875	TrainAcc 0.9193	ValidAcc 0.9223	TestAcc 0.9191	BestValid 0.9227
	Epoch 950:	Loss 0.4842	TrainAcc 0.9211	ValidAcc 0.9237	TestAcc 0.9208	BestValid 0.9237
	Epoch 975:	Loss 0.4885	TrainAcc 0.9187	ValidAcc 0.9214	TestAcc 0.9186	BestValid 0.9237
	Epoch 1000:	Loss 0.4763	TrainAcc 0.9177	ValidAcc 0.9200	TestAcc 0.9178	BestValid 0.9237
	Epoch 1025:	Loss 0.4858	TrainAcc 0.9160	ValidAcc 0.9186	TestAcc 0.9162	BestValid 0.9237
	Epoch 1050:	Loss 0.4800	TrainAcc 0.9158	ValidAcc 0.9176	TestAcc 0.9159	BestValid 0.9237
	Epoch 1075:	Loss 0.4752	TrainAcc 0.9214	ValidAcc 0.9233	TestAcc 0.9212	BestValid 0.9237
	Epoch 1100:	Loss 0.4703	TrainAcc 0.9225	ValidAcc 0.9243	TestAcc 0.9223	BestValid 0.9243
	Epoch 1125:	Loss 0.4673	TrainAcc 0.9257	ValidAcc 0.9278	TestAcc 0.9249	BestValid 0.9278
	Epoch 1150:	Loss 0.4647	TrainAcc 0.9224	ValidAcc 0.9240	TestAcc 0.9221	BestValid 0.9278
	Epoch 1175:	Loss 0.4635	TrainAcc 0.9219	ValidAcc 0.9237	TestAcc 0.9216	BestValid 0.9278
	Epoch 1200:	Loss 0.4593	TrainAcc 0.9233	ValidAcc 0.9246	TestAcc 0.9226	BestValid 0.9278
	Epoch 1225:	Loss 0.4607	TrainAcc 0.9198	ValidAcc 0.9208	TestAcc 0.9197	BestValid 0.9278
	Epoch 1250:	Loss 0.4587	TrainAcc 0.9240	ValidAcc 0.9257	TestAcc 0.9235	BestValid 0.9278
	Epoch 1275:	Loss 0.4629	TrainAcc 0.9250	ValidAcc 0.9264	TestAcc 0.9240	BestValid 0.9278
	Epoch 1300:	Loss 0.4529	TrainAcc 0.9247	ValidAcc 0.9264	TestAcc 0.9238	BestValid 0.9278
	Epoch 1325:	Loss 0.4496	TrainAcc 0.9241	ValidAcc 0.9256	TestAcc 0.9237	BestValid 0.9278
	Epoch 1350:	Loss 0.4484	TrainAcc 0.9259	ValidAcc 0.9274	TestAcc 0.9245	BestValid 0.9278
	Epoch 1375:	Loss 0.4489	TrainAcc 0.9239	ValidAcc 0.9253	TestAcc 0.9229	BestValid 0.9278
	Epoch 1400:	Loss 0.4504	TrainAcc 0.9226	ValidAcc 0.9240	TestAcc 0.9215	BestValid 0.9278
	Epoch 1425:	Loss 0.4505	TrainAcc 0.9243	ValidAcc 0.9256	TestAcc 0.9229	BestValid 0.9278
	Epoch 1450:	Loss 0.4548	TrainAcc 0.9207	ValidAcc 0.9218	TestAcc 0.9197	BestValid 0.9278
	Epoch 1475:	Loss 0.4475	TrainAcc 0.9254	ValidAcc 0.9264	TestAcc 0.9240	BestValid 0.9278
	Epoch 1500:	Loss 0.4376	TrainAcc 0.9252	ValidAcc 0.9263	TestAcc 0.9240	BestValid 0.9278
	Epoch 1525:	Loss 0.4465	TrainAcc 0.9273	ValidAcc 0.9287	TestAcc 0.9257	BestValid 0.9287
	Epoch 1550:	Loss 0.4405	TrainAcc 0.9265	ValidAcc 0.9280	TestAcc 0.9249	BestValid 0.9287
	Epoch 1575:	Loss 0.4392	TrainAcc 0.9291	ValidAcc 0.9300	TestAcc 0.9272	BestValid 0.9300
	Epoch 1600:	Loss 0.4381	TrainAcc 0.9279	ValidAcc 0.9287	TestAcc 0.9259	BestValid 0.9300
	Epoch 1625:	Loss 0.4364	TrainAcc 0.9276	ValidAcc 0.9282	TestAcc 0.9259	BestValid 0.9300
	Epoch 1650:	Loss 0.4434	TrainAcc 0.9231	ValidAcc 0.9241	TestAcc 0.9213	BestValid 0.9300
	Epoch 1675:	Loss 0.4323	TrainAcc 0.9240	ValidAcc 0.9245	TestAcc 0.9225	BestValid 0.9300
	Epoch 1700:	Loss 0.4323	TrainAcc 0.9270	ValidAcc 0.9278	TestAcc 0.9255	BestValid 0.9300
	Epoch 1725:	Loss 0.4298	TrainAcc 0.9274	ValidAcc 0.9286	TestAcc 0.9259	BestValid 0.9300
	Epoch 1750:	Loss 0.4301	TrainAcc 0.9301	ValidAcc 0.9311	TestAcc 0.9282	BestValid 0.9311
	Epoch 1775:	Loss 0.4294	TrainAcc 0.9270	ValidAcc 0.9276	TestAcc 0.9254	BestValid 0.9311
	Epoch 1800:	Loss 0.4307	TrainAcc 0.9286	ValidAcc 0.9295	TestAcc 0.9266	BestValid 0.9311
	Epoch 1825:	Loss 0.4287	TrainAcc 0.9281	ValidAcc 0.9287	TestAcc 0.9264	BestValid 0.9311
	Epoch 1850:	Loss 0.4342	TrainAcc 0.9274	ValidAcc 0.9285	TestAcc 0.9257	BestValid 0.9311
	Epoch 1875:	Loss 0.4223	TrainAcc 0.9265	ValidAcc 0.9272	TestAcc 0.9245	BestValid 0.9311
	Epoch 1900:	Loss 0.4245	TrainAcc 0.9316	ValidAcc 0.9323	TestAcc 0.9294	BestValid 0.9323
	Epoch 1925:	Loss 0.4246	TrainAcc 0.9294	ValidAcc 0.9300	TestAcc 0.9266	BestValid 0.9323
	Epoch 1950:	Loss 0.4222	TrainAcc 0.9305	ValidAcc 0.9308	TestAcc 0.9284	BestValid 0.9323
	Epoch 1975:	Loss 0.4199	TrainAcc 0.9306	ValidAcc 0.9307	TestAcc 0.9285	BestValid 0.9323
	Epoch 2000:	Loss 0.4377	TrainAcc 0.9296	ValidAcc 0.9301	TestAcc 0.9276	BestValid 0.9323
	Epoch 2025:	Loss 0.4236	TrainAcc 0.9293	ValidAcc 0.9303	TestAcc 0.9273	BestValid 0.9323
	Epoch 2050:	Loss 0.4221	TrainAcc 0.9311	ValidAcc 0.9312	TestAcc 0.9287	BestValid 0.9323
	Epoch 2075:	Loss 0.4210	TrainAcc 0.9294	ValidAcc 0.9298	TestAcc 0.9271	BestValid 0.9323
	Epoch 2100:	Loss 0.4325	TrainAcc 0.9265	ValidAcc 0.9266	TestAcc 0.9247	BestValid 0.9323
	Epoch 2125:	Loss 0.4171	TrainAcc 0.9319	ValidAcc 0.9321	TestAcc 0.9294	BestValid 0.9323
	Epoch 2150:	Loss 0.4162	TrainAcc 0.9292	ValidAcc 0.9294	TestAcc 0.9275	BestValid 0.9323
	Epoch 2175:	Loss 0.4146	TrainAcc 0.9296	ValidAcc 0.9299	TestAcc 0.9280	BestValid 0.9323
	Epoch 2200:	Loss 0.4165	TrainAcc 0.9311	ValidAcc 0.9312	TestAcc 0.9287	BestValid 0.9323
	Epoch 2225:	Loss 0.4172	TrainAcc 0.9332	ValidAcc 0.9330	TestAcc 0.9305	BestValid 0.9330
	Epoch 2250:	Loss 0.4129	TrainAcc 0.9314	ValidAcc 0.9311	TestAcc 0.9292	BestValid 0.9330
	Epoch 2275:	Loss 0.4196	TrainAcc 0.9298	ValidAcc 0.9300	TestAcc 0.9278	BestValid 0.9330
	Epoch 2300:	Loss 0.4111	TrainAcc 0.9351	ValidAcc 0.9354	TestAcc 0.9328	BestValid 0.9354
	Epoch 2325:	Loss 0.4064	TrainAcc 0.9305	ValidAcc 0.9308	TestAcc 0.9285	BestValid 0.9354
	Epoch 2350:	Loss 0.4112	TrainAcc 0.9284	ValidAcc 0.9280	TestAcc 0.9261	BestValid 0.9354
	Epoch 2375:	Loss 0.4084	TrainAcc 0.9320	ValidAcc 0.9319	TestAcc 0.9295	BestValid 0.9354
	Epoch 2400:	Loss 0.4075	TrainAcc 0.9356	ValidAcc 0.9355	TestAcc 0.9329	BestValid 0.9355
	Epoch 2425:	Loss 0.4104	TrainAcc 0.9279	ValidAcc 0.9277	TestAcc 0.9252	BestValid 0.9355
	Epoch 2450:	Loss 0.4151	TrainAcc 0.9301	ValidAcc 0.9300	TestAcc 0.9277	BestValid 0.9355
	Epoch 2475:	Loss 0.4067	TrainAcc 0.9340	ValidAcc 0.9336	TestAcc 0.9311	BestValid 0.9355
	Epoch 2500:	Loss 0.4175	TrainAcc 0.9319	ValidAcc 0.9316	TestAcc 0.9290	BestValid 0.9355
	Epoch 2525:	Loss 0.4012	TrainAcc 0.9319	ValidAcc 0.9313	TestAcc 0.9292	BestValid 0.9355
	Epoch 2550:	Loss 0.4051	TrainAcc 0.9328	ValidAcc 0.9322	TestAcc 0.9301	BestValid 0.9355
	Epoch 2575:	Loss 0.4045	TrainAcc 0.9359	ValidAcc 0.9353	TestAcc 0.9331	BestValid 0.9355
	Epoch 2600:	Loss 0.4071	TrainAcc 0.9335	ValidAcc 0.9327	TestAcc 0.9306	BestValid 0.9355
	Epoch 2625:	Loss 0.4047	TrainAcc 0.9301	ValidAcc 0.9297	TestAcc 0.9275	BestValid 0.9355
	Epoch 2650:	Loss 0.4094	TrainAcc 0.9276	ValidAcc 0.9270	TestAcc 0.9248	BestValid 0.9355
	Epoch 2675:	Loss 0.4053	TrainAcc 0.9331	ValidAcc 0.9325	TestAcc 0.9300	BestValid 0.9355
	Epoch 2700:	Loss 0.4085	TrainAcc 0.9305	ValidAcc 0.9300	TestAcc 0.9280	BestValid 0.9355
	Epoch 2725:	Loss 0.3989	TrainAcc 0.9336	ValidAcc 0.9325	TestAcc 0.9306	BestValid 0.9355
	Epoch 2750:	Loss 0.3990	TrainAcc 0.9347	ValidAcc 0.9343	TestAcc 0.9316	BestValid 0.9355
	Epoch 2775:	Loss 0.3981	TrainAcc 0.9348	ValidAcc 0.9345	TestAcc 0.9316	BestValid 0.9355
	Epoch 2800:	Loss 0.4030	TrainAcc 0.9355	ValidAcc 0.9351	TestAcc 0.9324	BestValid 0.9355
	Epoch 2825:	Loss 0.4002	TrainAcc 0.9337	ValidAcc 0.9327	TestAcc 0.9307	BestValid 0.9355
	Epoch 2850:	Loss 0.3979	TrainAcc 0.9341	ValidAcc 0.9337	TestAcc 0.9308	BestValid 0.9355
	Epoch 2875:	Loss 0.3976	TrainAcc 0.9349	ValidAcc 0.9346	TestAcc 0.9318	BestValid 0.9355
	Epoch 2900:	Loss 0.3988	TrainAcc 0.9280	ValidAcc 0.9272	TestAcc 0.9254	BestValid 0.9355
	Epoch 2925:	Loss 0.3949	TrainAcc 0.9345	ValidAcc 0.9339	TestAcc 0.9313	BestValid 0.9355
	Epoch 2950:	Loss 0.3941	TrainAcc 0.9351	ValidAcc 0.9347	TestAcc 0.9319	BestValid 0.9355
	Epoch 2975:	Loss 0.3932	TrainAcc 0.9331	ValidAcc 0.9321	TestAcc 0.9300	BestValid 0.9355
	Epoch 3000:	Loss 0.3949	TrainAcc 0.9300	ValidAcc 0.9286	TestAcc 0.9270	BestValid 0.9355
	Epoch 3025:	Loss 0.3922	TrainAcc 0.9368	ValidAcc 0.9360	TestAcc 0.9333	BestValid 0.9360
	Epoch 3050:	Loss 0.3942	TrainAcc 0.9345	ValidAcc 0.9334	TestAcc 0.9315	BestValid 0.9360
	Epoch 3075:	Loss 0.3914	TrainAcc 0.9338	ValidAcc 0.9330	TestAcc 0.9307	BestValid 0.9360
	Epoch 3100:	Loss 0.3895	TrainAcc 0.9352	ValidAcc 0.9345	TestAcc 0.9319	BestValid 0.9360
	Epoch 3125:	Loss 0.3962	TrainAcc 0.9384	ValidAcc 0.9375	TestAcc 0.9351	BestValid 0.9375
	Epoch 3150:	Loss 0.3900	TrainAcc 0.9368	ValidAcc 0.9358	TestAcc 0.9334	BestValid 0.9375
	Epoch 3175:	Loss 0.3909	TrainAcc 0.9362	ValidAcc 0.9357	TestAcc 0.9328	BestValid 0.9375
	Epoch 3200:	Loss 0.3886	TrainAcc 0.9321	ValidAcc 0.9308	TestAcc 0.9289	BestValid 0.9375
	Epoch 3225:	Loss 0.3881	TrainAcc 0.9336	ValidAcc 0.9328	TestAcc 0.9304	BestValid 0.9375
	Epoch 3250:	Loss 0.3866	TrainAcc 0.9385	ValidAcc 0.9377	TestAcc 0.9348	BestValid 0.9377
	Epoch 3275:	Loss 0.3918	TrainAcc 0.9266	ValidAcc 0.9249	TestAcc 0.9238	BestValid 0.9377
	Epoch 3300:	Loss 0.3864	TrainAcc 0.9374	ValidAcc 0.9365	TestAcc 0.9337	BestValid 0.9377
	Epoch 3325:	Loss 0.3904	TrainAcc 0.9339	ValidAcc 0.9327	TestAcc 0.9303	BestValid 0.9377
	Epoch 3350:	Loss 0.3846	TrainAcc 0.9342	ValidAcc 0.9327	TestAcc 0.9306	BestValid 0.9377
	Epoch 3375:	Loss 0.3862	TrainAcc 0.9350	ValidAcc 0.9340	TestAcc 0.9318	BestValid 0.9377
	Epoch 3400:	Loss 0.3876	TrainAcc 0.9359	ValidAcc 0.9345	TestAcc 0.9325	BestValid 0.9377
	Epoch 3425:	Loss 0.3871	TrainAcc 0.9356	ValidAcc 0.9344	TestAcc 0.9320	BestValid 0.9377
	Epoch 3450:	Loss 0.3831	TrainAcc 0.9317	ValidAcc 0.9302	TestAcc 0.9282	BestValid 0.9377
	Epoch 3475:	Loss 0.3844	TrainAcc 0.9330	ValidAcc 0.9322	TestAcc 0.9302	BestValid 0.9377
	Epoch 3500:	Loss 0.3809	TrainAcc 0.9329	ValidAcc 0.9313	TestAcc 0.9297	BestValid 0.9377
	Epoch 3525:	Loss 0.3861	TrainAcc 0.9380	ValidAcc 0.9366	TestAcc 0.9345	BestValid 0.9377
	Epoch 3550:	Loss 0.3842	TrainAcc 0.9332	ValidAcc 0.9320	TestAcc 0.9299	BestValid 0.9377
	Epoch 3575:	Loss 0.3841	TrainAcc 0.9365	ValidAcc 0.9354	TestAcc 0.9331	BestValid 0.9377
	Epoch 3600:	Loss 0.3836	TrainAcc 0.9316	ValidAcc 0.9302	TestAcc 0.9284	BestValid 0.9377
	Epoch 3625:	Loss 0.3849	TrainAcc 0.9351	ValidAcc 0.9333	TestAcc 0.9314	BestValid 0.9377
	Epoch 3650:	Loss 0.3834	TrainAcc 0.9390	ValidAcc 0.9381	TestAcc 0.9354	BestValid 0.9381
	Epoch 3675:	Loss 0.3825	TrainAcc 0.9343	ValidAcc 0.9330	TestAcc 0.9309	BestValid 0.9381
	Epoch 3700:	Loss 0.3863	TrainAcc 0.9394	ValidAcc 0.9383	TestAcc 0.9356	BestValid 0.9383
	Epoch 3725:	Loss 0.3806	TrainAcc 0.9317	ValidAcc 0.9300	TestAcc 0.9285	BestValid 0.9383
	Epoch 3750:	Loss 0.3796	TrainAcc 0.9387	ValidAcc 0.9372	TestAcc 0.9346	BestValid 0.9383
	Epoch 3775:	Loss 0.3818	TrainAcc 0.9353	ValidAcc 0.9335	TestAcc 0.9315	BestValid 0.9383
	Epoch 3800:	Loss 0.3802	TrainAcc 0.9369	ValidAcc 0.9358	TestAcc 0.9333	BestValid 0.9383
	Epoch 3825:	Loss 0.3793	TrainAcc 0.9346	ValidAcc 0.9331	TestAcc 0.9309	BestValid 0.9383
	Epoch 3850:	Loss 0.3794	TrainAcc 0.9373	ValidAcc 0.9360	TestAcc 0.9335	BestValid 0.9383
	Epoch 3875:	Loss 0.3833	TrainAcc 0.9368	ValidAcc 0.9351	TestAcc 0.9332	BestValid 0.9383
	Epoch 3900:	Loss 0.3766	TrainAcc 0.9361	ValidAcc 0.9342	TestAcc 0.9323	BestValid 0.9383
	Epoch 3925:	Loss 0.3785	TrainAcc 0.9359	ValidAcc 0.9341	TestAcc 0.9322	BestValid 0.9383
	Epoch 3950:	Loss 0.3801	TrainAcc 0.9375	ValidAcc 0.9361	TestAcc 0.9336	BestValid 0.9383
	Epoch 3975:	Loss 0.3768	TrainAcc 0.9365	ValidAcc 0.9347	TestAcc 0.9326	BestValid 0.9383
	Epoch 4000:	Loss 0.3760	TrainAcc 0.9382	ValidAcc 0.9367	TestAcc 0.9344	BestValid 0.9383
	Epoch 4025:	Loss 0.3899	TrainAcc 0.9395	ValidAcc 0.9382	TestAcc 0.9356	BestValid 0.9383
	Epoch 4050:	Loss 0.3779	TrainAcc 0.9361	ValidAcc 0.9343	TestAcc 0.9324	BestValid 0.9383
	Epoch 4075:	Loss 0.3769	TrainAcc 0.9388	ValidAcc 0.9369	TestAcc 0.9347	BestValid 0.9383
	Epoch 4100:	Loss 0.3745	TrainAcc 0.9347	ValidAcc 0.9328	TestAcc 0.9308	BestValid 0.9383
	Epoch 4125:	Loss 0.3874	TrainAcc 0.9264	ValidAcc 0.9248	TestAcc 0.9229	BestValid 0.9383
	Epoch 4150:	Loss 0.3806	TrainAcc 0.9379	ValidAcc 0.9363	TestAcc 0.9337	BestValid 0.9383
	Epoch 4175:	Loss 0.3757	TrainAcc 0.9393	ValidAcc 0.9378	TestAcc 0.9350	BestValid 0.9383
	Epoch 4200:	Loss 0.3725	TrainAcc 0.9402	ValidAcc 0.9384	TestAcc 0.9361	BestValid 0.9384
	Epoch 4225:	Loss 0.3730	TrainAcc 0.9364	ValidAcc 0.9345	TestAcc 0.9327	BestValid 0.9384
	Epoch 4250:	Loss 0.3731	TrainAcc 0.9402	ValidAcc 0.9386	TestAcc 0.9362	BestValid 0.9386
	Epoch 4275:	Loss 0.3737	TrainAcc 0.9391	ValidAcc 0.9372	TestAcc 0.9350	BestValid 0.9386
	Epoch 4300:	Loss 0.3713	TrainAcc 0.9369	ValidAcc 0.9346	TestAcc 0.9329	BestValid 0.9386
	Epoch 4325:	Loss 0.3702	TrainAcc 0.9368	ValidAcc 0.9348	TestAcc 0.9326	BestValid 0.9386
	Epoch 4350:	Loss 0.3741	TrainAcc 0.9383	ValidAcc 0.9366	TestAcc 0.9341	BestValid 0.9386
	Epoch 4375:	Loss 0.3704	TrainAcc 0.9357	ValidAcc 0.9328	TestAcc 0.9318	BestValid 0.9386
	Epoch 4400:	Loss 0.3716	TrainAcc 0.9375	ValidAcc 0.9357	TestAcc 0.9333	BestValid 0.9386
	Epoch 4425:	Loss 0.3691	TrainAcc 0.9390	ValidAcc 0.9372	TestAcc 0.9348	BestValid 0.9386
	Epoch 4450:	Loss 0.3700	TrainAcc 0.9377	ValidAcc 0.9355	TestAcc 0.9336	BestValid 0.9386
	Epoch 4475:	Loss 0.3718	TrainAcc 0.9362	ValidAcc 0.9340	TestAcc 0.9321	BestValid 0.9386
	Epoch 4500:	Loss 0.3711	TrainAcc 0.9393	ValidAcc 0.9372	TestAcc 0.9349	BestValid 0.9386
	Epoch 4525:	Loss 0.3768	TrainAcc 0.9423	ValidAcc 0.9406	TestAcc 0.9381	BestValid 0.9406
	Epoch 4550:	Loss 0.3763	TrainAcc 0.9362	ValidAcc 0.9343	TestAcc 0.9320	BestValid 0.9406
	Epoch 4575:	Loss 0.3740	TrainAcc 0.9321	ValidAcc 0.9299	TestAcc 0.9286	BestValid 0.9406
	Epoch 4600:	Loss 0.3677	TrainAcc 0.9409	ValidAcc 0.9392	TestAcc 0.9365	BestValid 0.9406
	Epoch 4625:	Loss 0.3706	TrainAcc 0.9360	ValidAcc 0.9342	TestAcc 0.9318	BestValid 0.9406
	Epoch 4650:	Loss 0.3719	TrainAcc 0.9360	ValidAcc 0.9342	TestAcc 0.9322	BestValid 0.9406
	Epoch 4675:	Loss 0.3717	TrainAcc 0.9377	ValidAcc 0.9359	TestAcc 0.9334	BestValid 0.9406
	Epoch 4700:	Loss 0.3665	TrainAcc 0.9390	ValidAcc 0.9374	TestAcc 0.9347	BestValid 0.9406
	Epoch 4725:	Loss 0.3685	TrainAcc 0.9388	ValidAcc 0.9370	TestAcc 0.9342	BestValid 0.9406
	Epoch 4750:	Loss 0.3779	TrainAcc 0.9396	ValidAcc 0.9379	TestAcc 0.9350	BestValid 0.9406
	Epoch 4775:	Loss 0.3656	TrainAcc 0.9388	ValidAcc 0.9372	TestAcc 0.9345	BestValid 0.9406
	Epoch 4800:	Loss 0.3756	TrainAcc 0.9346	ValidAcc 0.9321	TestAcc 0.9307	BestValid 0.9406
	Epoch 4825:	Loss 0.3673	TrainAcc 0.9383	ValidAcc 0.9362	TestAcc 0.9338	BestValid 0.9406
	Epoch 4850:	Loss 0.3625	TrainAcc 0.9374	ValidAcc 0.9354	TestAcc 0.9328	BestValid 0.9406
	Epoch 4875:	Loss 0.3670	TrainAcc 0.9391	ValidAcc 0.9372	TestAcc 0.9347	BestValid 0.9406
	Epoch 4900:	Loss 0.3670	TrainAcc 0.9343	ValidAcc 0.9321	TestAcc 0.9303	BestValid 0.9406
	Epoch 4925:	Loss 0.3642	TrainAcc 0.9384	ValidAcc 0.9362	TestAcc 0.9340	BestValid 0.9406
	Epoch 4950:	Loss 0.3687	TrainAcc 0.9409	ValidAcc 0.9387	TestAcc 0.9362	BestValid 0.9406
	Epoch 4975:	Loss 0.3661	TrainAcc 0.9347	ValidAcc 0.9324	TestAcc 0.9307	BestValid 0.9406
	Epoch 5000:	Loss 0.3672	TrainAcc 0.9378	ValidAcc 0.9362	TestAcc 0.9333	BestValid 0.9406
Node 1, Pre/Post-Pipelining: 8.583 / 17.557 ms, Bubble: 0.960 ms, Compute: 881.804 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 2, Pre/Post-Pipelining: 8.592 / 17.614 ms, Bubble: 0.437 ms, Compute: 882.276 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 3, Pre/Post-Pipelining: 8.591 / 17.625 ms, Bubble: 0.059 ms, Compute: 882.637 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 0, Pre/Post-Pipelining: 8.583 / 17.562 ms, Bubble: 1.066 ms, Compute: 881.695 ms, Comm: 0.009 ms, Imbalance: 0.018 ms
Node 4, Pre/Post-Pipelining: 8.590 / 17.591 ms, Bubble: 1.141 ms, Compute: 881.593 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 5, Pre/Post-Pipelining: 8.586 / 17.590 ms, Bubble: 1.061 ms, Compute: 881.679 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 8.588 / 17.570 ms, Bubble: 0.617 ms, Compute: 882.140 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 7, Pre/Post-Pipelining: 8.584 / 17.587 ms, Bubble: 1.059 ms, Compute: 881.683 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.583 ms
Cluster-Wide Average, Post-Pipelining Overhead: 17.562 ms
Cluster-Wide Average, Bubble: 1.066 ms
Cluster-Wide Average, Compute: 881.695 ms
Cluster-Wide Average, Communication: 0.009 ms
Cluster-Wide Average, Imbalance: 0.018 ms
Node 0, GPU memory consumption: 22.329 GB
Node 1, GPU memory consumption: 21.518 GB
Node 2, GPU memory consumption: 21.526 GB
Node 3, GPU memory consumption: 21.505 GB
Node 4, GPU memory consumption: 21.495 GB
Node 6, GPU memory consumption: 21.522 GB
Node 7, GPU memory consumption: 21.499 GB
Node 5, GPU memory consumption: 21.518 GB
Node 0, Graph-Level Communication Throughput: 25.530 Gbps, Time: 625.357 ms
Node 1, Graph-Level Communication Throughput: 23.791 Gbps, Time: 713.790 ms
Node 2, Graph-Level Communication Throughput: 36.250 Gbps, Time: 487.888 ms
Node 4, Graph-Level Communication Throughput: 11.525 Gbps, Time: 730.153 ms
Node 3, Graph-Level Communication Throughput: 49.585 Gbps, Time: 466.734 ms
Node 5, Graph-Level Communication Throughput: 19.105 Gbps, Time: 639.274 ms
Node 6, Graph-Level Communication Throughput: 26.470 Gbps, Time: 618.848 ms
Node 7, Graph-Level Communication Throughput: 21.501 Gbps, Time: 633.222 ms
------------------------node id 0,  per-epoch time: 1.387424 s---------------
------------------------node id 1,  per-epoch time: 1.387424 s---------------
------------------------node id 2,  per-epoch time: 1.387424 s---------------
------------------------node id 3,  per-epoch time: 1.387424 s---------------
------------------------node id 4,  per-epoch time: 1.387424 s---------------
------------------------node id 5,  per-epoch time: 1.387424 s---------------
------------------------node id 6,  per-epoch time: 1.387424 s---------------
------------------------node id 7,  per-epoch time: 1.387424 s---------------
************ Profiling Results ************
	Bubble: 505.285620 (ms) (36.43 percentage)
	Compute: 228.368968 (ms) (16.47 percentage)
	GraphCommComputeOverhead: 19.099599 (ms) (1.38 percentage)
	GraphCommNetwork: 614.407322 (ms) (44.30 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.804395 (ms) (1.43 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 14.493 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.9406
Target test_acc: 0.9381
Epoch to reach the target acc: 4524
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
