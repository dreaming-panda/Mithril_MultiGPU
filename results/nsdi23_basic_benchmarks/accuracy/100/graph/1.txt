Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.871 seconds.
Building the CSC structure...
        It takes 2.031 seconds.
Building the CSC structure...
        It takes 2.245 seconds.
Building the CSC structure...
        It takes 2.309 seconds.
Building the CSC structure...
        It takes 2.406 seconds.
Building the CSC structure...
        It takes 2.491 seconds.
Building the CSC structure...
        It takes 2.614 seconds.
Building the CSC structure...
        It takes 2.658 seconds.
Building the CSC structure...
        It takes 1.806 seconds.
        It takes 1.866 seconds.
        It takes 2.189 seconds.
        It takes 2.213 seconds.
Building the Feature Vector...
        It takes 2.354 seconds.
        It takes 2.356 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 2.320 seconds.
        It takes 0.031 seconds.
        It takes 2.345 seconds.
Building the Feature Vector...
        It takes 0.249 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
Building the Feature Vector...
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 33)
Building the Feature Vector...
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
Building the Feature Vector...
GPU 0, layer [0, 33)
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.256 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 64.010 Gbps (per GPU), 512.078 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.969 Gbps (per GPU), 511.752 Gbps (aggregated)
The layer-level communication performance: 63.966 Gbps (per GPU), 511.725 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.934 Gbps (per GPU), 511.472 Gbps (aggregated)
The layer-level communication performance: 63.928 Gbps (per GPU), 511.426 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.897 Gbps (per GPU), 511.178 Gbps (aggregated)
The layer-level communication performance: 63.891 Gbps (per GPU), 511.129 Gbps (aggregated)
The layer-level communication performance: 63.886 Gbps (per GPU), 511.089 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.887 Gbps (per GPU), 1319.100 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.883 Gbps (per GPU), 1319.067 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.888 Gbps (per GPU), 1319.106 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.890 Gbps (per GPU), 1319.122 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.887 Gbps (per GPU), 1319.100 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.885 Gbps (per GPU), 1319.083 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.887 Gbps (per GPU), 1319.093 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.890 Gbps (per GPU), 1319.119 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.190 Gbps (per GPU), 913.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.190 Gbps (per GPU), 913.520 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.190 Gbps (per GPU), 913.523 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.188 Gbps (per GPU), 913.507 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.190 Gbps (per GPU), 913.520 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.188 Gbps (per GPU), 913.508 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.190 Gbps (per GPU), 913.517 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.190 Gbps (per GPU), 913.522 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 46.256 Gbps (per GPU), 370.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.256 Gbps (per GPU), 370.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.256 Gbps (per GPU), 370.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.256 Gbps (per GPU), 370.045 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.256 Gbps (per GPU), 370.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.256 Gbps (per GPU), 370.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.256 Gbps (per GPU), 370.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.256 Gbps (per GPU), 370.049 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.51ms  9.48ms 10.01ms  3.99 29.12K 14.23M
 chk_1  2.52ms  5.15ms  5.55ms  2.20 29.12K  6.56M
 chk_2  2.53ms 16.69ms 17.16ms  6.79 29.12K 24.68M
 chk_3  2.53ms 16.75ms 17.12ms  6.77 29.12K 22.95M
 chk_4  2.54ms  5.00ms  5.38ms  2.12 29.12K  6.33M
 chk_5  2.54ms  9.31ms  9.36ms  3.69 29.12K 12.05M
 chk_6  2.54ms 10.19ms 10.53ms  4.15 29.12K 14.60M
 chk_7  2.54ms  9.35ms 10.01ms  3.95 29.12K 13.21M
   Avg  2.53 10.24 10.64
   Max  2.54 16.75 17.16
   Min  2.51  5.00  5.38
 Ratio  1.01  3.35  3.19
   Var  0.00 17.45 17.56
Profiling takes 2.179 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 347.929 ms
Partition 0 [0, 5) has cost: 347.929 ms
Partition 1 [5, 9) has cost: 327.690 ms
Partition 2 [9, 13) has cost: 327.690 ms
Partition 3 [13, 17) has cost: 327.690 ms
Partition 4 [17, 21) has cost: 327.690 ms
Partition 5 [21, 25) has cost: 327.690 ms
Partition 6 [25, 29) has cost: 327.690 ms
Partition 7 [29, 33) has cost: 330.879 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 300.776 ms
GPU 0, Compute+Comm Time: 124.257 ms, Bubble Time: 129.782 ms, Imbalance Overhead: 46.737 ms
GPU 1, Compute+Comm Time: 119.189 ms, Bubble Time: 120.361 ms, Imbalance Overhead: 61.226 ms
GPU 2, Compute+Comm Time: 119.189 ms, Bubble Time: 110.289 ms, Imbalance Overhead: 71.298 ms
GPU 3, Compute+Comm Time: 119.189 ms, Bubble Time: 110.731 ms, Imbalance Overhead: 70.857 ms
GPU 4, Compute+Comm Time: 119.189 ms, Bubble Time: 120.072 ms, Imbalance Overhead: 61.515 ms
GPU 5, Compute+Comm Time: 119.189 ms, Bubble Time: 128.783 ms, Imbalance Overhead: 52.804 ms
GPU 6, Compute+Comm Time: 119.189 ms, Bubble Time: 137.420 ms, Imbalance Overhead: 44.167 ms
GPU 7, Compute+Comm Time: 119.981 ms, Bubble Time: 147.331 ms, Imbalance Overhead: 33.464 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 590.351 ms
GPU 0, Compute+Comm Time: 234.191 ms, Bubble Time: 291.197 ms, Imbalance Overhead: 64.963 ms
GPU 1, Compute+Comm Time: 231.794 ms, Bubble Time: 272.186 ms, Imbalance Overhead: 86.372 ms
GPU 2, Compute+Comm Time: 231.794 ms, Bubble Time: 254.939 ms, Imbalance Overhead: 103.619 ms
GPU 3, Compute+Comm Time: 231.794 ms, Bubble Time: 237.429 ms, Imbalance Overhead: 121.128 ms
GPU 4, Compute+Comm Time: 231.794 ms, Bubble Time: 218.022 ms, Imbalance Overhead: 140.536 ms
GPU 5, Compute+Comm Time: 231.794 ms, Bubble Time: 216.532 ms, Imbalance Overhead: 142.026 ms
GPU 6, Compute+Comm Time: 231.794 ms, Bubble Time: 235.774 ms, Imbalance Overhead: 122.784 ms
GPU 7, Compute+Comm Time: 246.965 ms, Bubble Time: 252.913 ms, Imbalance Overhead: 90.473 ms
The estimated cost of the whole pipeline: 935.684 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 675.619 ms
Partition 0 [0, 9) has cost: 675.619 ms
Partition 1 [9, 17) has cost: 655.380 ms
Partition 2 [17, 25) has cost: 655.380 ms
Partition 3 [25, 33) has cost: 658.569 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 293.622 ms
GPU 0, Compute+Comm Time: 143.440 ms, Bubble Time: 114.100 ms, Imbalance Overhead: 36.082 ms
GPU 1, Compute+Comm Time: 140.901 ms, Bubble Time: 94.532 ms, Imbalance Overhead: 58.190 ms
GPU 2, Compute+Comm Time: 140.901 ms, Bubble Time: 112.587 ms, Imbalance Overhead: 40.134 ms
GPU 3, Compute+Comm Time: 141.233 ms, Bubble Time: 129.935 ms, Imbalance Overhead: 22.454 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 559.463 ms
GPU 0, Compute+Comm Time: 267.320 ms, Bubble Time: 249.966 ms, Imbalance Overhead: 42.177 ms
GPU 1, Compute+Comm Time: 266.324 ms, Bubble Time: 215.210 ms, Imbalance Overhead: 77.929 ms
GPU 2, Compute+Comm Time: 266.324 ms, Bubble Time: 178.293 ms, Imbalance Overhead: 114.846 ms
GPU 3, Compute+Comm Time: 273.917 ms, Bubble Time: 214.754 ms, Imbalance Overhead: 70.792 ms
    The estimated cost with 2 DP ways is 895.739 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1330.998 ms
Partition 0 [0, 17) has cost: 1330.998 ms
Partition 1 [17, 33) has cost: 1313.948 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 308.689 ms
GPU 0, Compute+Comm Time: 194.906 ms, Bubble Time: 79.470 ms, Imbalance Overhead: 34.313 ms
GPU 1, Compute+Comm Time: 193.816 ms, Bubble Time: 114.873 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 546.074 ms
GPU 0, Compute+Comm Time: 339.626 ms, Bubble Time: 206.448 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 342.862 ms, Bubble Time: 134.775 ms, Imbalance Overhead: 68.437 ms
    The estimated cost with 4 DP ways is 897.501 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2644.947 ms
Partition 0 [0, 33) has cost: 2644.947 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 404.037 ms
GPU 0, Compute+Comm Time: 404.037 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 586.118 ms
GPU 0, Compute+Comm Time: 586.118 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1039.663 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.3436	TrainAcc 0.5422	ValidAcc 0.5678	TestAcc 0.5641	BestValid 0.5678
	Epoch 50:	Loss 1.7027	TrainAcc 0.6899	ValidAcc 0.7118	TestAcc 0.7060	BestValid 0.7118
	Epoch 75:	Loss 1.3613	TrainAcc 0.7592	ValidAcc 0.7757	TestAcc 0.7696	BestValid 0.7757
	Epoch 100:	Loss 1.1577	TrainAcc 0.8003	ValidAcc 0.8136	TestAcc 0.8083	BestValid 0.8136
	Epoch 125:	Loss 1.0257	TrainAcc 0.8255	ValidAcc 0.8363	TestAcc 0.8314	BestValid 0.8363
	Epoch 150:	Loss 0.9260	TrainAcc 0.8436	ValidAcc 0.8529	TestAcc 0.8487	BestValid 0.8529
	Epoch 175:	Loss 0.8569	TrainAcc 0.8575	ValidAcc 0.8670	TestAcc 0.8615	BestValid 0.8670
	Epoch 200:	Loss 0.8007	TrainAcc 0.8671	ValidAcc 0.8752	TestAcc 0.8702	BestValid 0.8752
	Epoch 225:	Loss 0.7572	TrainAcc 0.8777	ValidAcc 0.8845	TestAcc 0.8796	BestValid 0.8845
	Epoch 250:	Loss 0.7271	TrainAcc 0.8849	ValidAcc 0.8912	TestAcc 0.8860	BestValid 0.8912
	Epoch 275:	Loss 0.6984	TrainAcc 0.8894	ValidAcc 0.8950	TestAcc 0.8902	BestValid 0.8950
	Epoch 300:	Loss 0.6712	TrainAcc 0.8937	ValidAcc 0.8997	TestAcc 0.8956	BestValid 0.8997
	Epoch 325:	Loss 0.6507	TrainAcc 0.8967	ValidAcc 0.9025	TestAcc 0.8983	BestValid 0.9025
	Epoch 350:	Loss 0.6364	TrainAcc 0.9001	ValidAcc 0.9059	TestAcc 0.9011	BestValid 0.9059
	Epoch 375:	Loss 0.6198	TrainAcc 0.9019	ValidAcc 0.9076	TestAcc 0.9029	BestValid 0.9076
	Epoch 400:	Loss 0.6089	TrainAcc 0.9036	ValidAcc 0.9094	TestAcc 0.9038	BestValid 0.9094
	Epoch 425:	Loss 0.5995	TrainAcc 0.9059	ValidAcc 0.9117	TestAcc 0.9066	BestValid 0.9117
	Epoch 450:	Loss 0.5826	TrainAcc 0.9081	ValidAcc 0.9141	TestAcc 0.9093	BestValid 0.9141
	Epoch 475:	Loss 0.5747	TrainAcc 0.9089	ValidAcc 0.9149	TestAcc 0.9095	BestValid 0.9149
	Epoch 500:	Loss 0.5662	TrainAcc 0.9105	ValidAcc 0.9159	TestAcc 0.9107	BestValid 0.9159
	Epoch 525:	Loss 0.5593	TrainAcc 0.9103	ValidAcc 0.9155	TestAcc 0.9104	BestValid 0.9159
	Epoch 550:	Loss 0.5485	TrainAcc 0.9132	ValidAcc 0.9192	TestAcc 0.9133	BestValid 0.9192
	Epoch 575:	Loss 0.5412	TrainAcc 0.9132	ValidAcc 0.9185	TestAcc 0.9128	BestValid 0.9192
	Epoch 600:	Loss 0.5381	TrainAcc 0.9157	ValidAcc 0.9211	TestAcc 0.9158	BestValid 0.9211
	Epoch 625:	Loss 0.5314	TrainAcc 0.9164	ValidAcc 0.9217	TestAcc 0.9164	BestValid 0.9217
	Epoch 650:	Loss 0.5286	TrainAcc 0.9172	ValidAcc 0.9222	TestAcc 0.9169	BestValid 0.9222
	Epoch 675:	Loss 0.5180	TrainAcc 0.9167	ValidAcc 0.9214	TestAcc 0.9160	BestValid 0.9222
	Epoch 700:	Loss 0.5166	TrainAcc 0.9183	ValidAcc 0.9231	TestAcc 0.9174	BestValid 0.9231
	Epoch 725:	Loss 0.5141	TrainAcc 0.9198	ValidAcc 0.9248	TestAcc 0.9190	BestValid 0.9248
	Epoch 750:	Loss 0.5062	TrainAcc 0.9171	ValidAcc 0.9222	TestAcc 0.9163	BestValid 0.9248
	Epoch 775:	Loss 0.5048	TrainAcc 0.9207	ValidAcc 0.9259	TestAcc 0.9200	BestValid 0.9259
	Epoch 800:	Loss 0.5014	TrainAcc 0.9204	ValidAcc 0.9253	TestAcc 0.9191	BestValid 0.9259
	Epoch 825:	Loss 0.4989	TrainAcc 0.9213	ValidAcc 0.9259	TestAcc 0.9199	BestValid 0.9259
	Epoch 850:	Loss 0.4926	TrainAcc 0.9230	ValidAcc 0.9281	TestAcc 0.9223	BestValid 0.9281
	Epoch 875:	Loss 0.4920	TrainAcc 0.9226	ValidAcc 0.9274	TestAcc 0.9217	BestValid 0.9281
	Epoch 900:	Loss 0.4885	TrainAcc 0.9236	ValidAcc 0.9283	TestAcc 0.9231	BestValid 0.9283
	Epoch 925:	Loss 0.4858	TrainAcc 0.9213	ValidAcc 0.9253	TestAcc 0.9206	BestValid 0.9283
	Epoch 950:	Loss 0.4826	TrainAcc 0.9234	ValidAcc 0.9272	TestAcc 0.9223	BestValid 0.9283
	Epoch 975:	Loss 0.4761	TrainAcc 0.9241	ValidAcc 0.9274	TestAcc 0.9232	BestValid 0.9283
	Epoch 1000:	Loss 0.4761	TrainAcc 0.9245	ValidAcc 0.9281	TestAcc 0.9236	BestValid 0.9283
	Epoch 1025:	Loss 0.4794	TrainAcc 0.9241	ValidAcc 0.9274	TestAcc 0.9230	BestValid 0.9283
	Epoch 1050:	Loss 0.4734	TrainAcc 0.9209	ValidAcc 0.9246	TestAcc 0.9198	BestValid 0.9283
	Epoch 1075:	Loss 0.4712	TrainAcc 0.9216	ValidAcc 0.9252	TestAcc 0.9203	BestValid 0.9283
	Epoch 1100:	Loss 0.4621	TrainAcc 0.9243	ValidAcc 0.9276	TestAcc 0.9227	BestValid 0.9283
	Epoch 1125:	Loss 0.4608	TrainAcc 0.9254	ValidAcc 0.9285	TestAcc 0.9239	BestValid 0.9285
	Epoch 1150:	Loss 0.4585	TrainAcc 0.9237	ValidAcc 0.9269	TestAcc 0.9224	BestValid 0.9285
	Epoch 1175:	Loss 0.4602	TrainAcc 0.9285	ValidAcc 0.9313	TestAcc 0.9279	BestValid 0.9313
	Epoch 1200:	Loss 0.4630	TrainAcc 0.9278	ValidAcc 0.9304	TestAcc 0.9266	BestValid 0.9313
	Epoch 1225:	Loss 0.4716	TrainAcc 0.9267	ValidAcc 0.9287	TestAcc 0.9250	BestValid 0.9313
	Epoch 1250:	Loss 0.4534	TrainAcc 0.9286	ValidAcc 0.9313	TestAcc 0.9275	BestValid 0.9313
	Epoch 1275:	Loss 0.4504	TrainAcc 0.9275	ValidAcc 0.9298	TestAcc 0.9260	BestValid 0.9313
	Epoch 1300:	Loss 0.4504	TrainAcc 0.9288	ValidAcc 0.9309	TestAcc 0.9276	BestValid 0.9313
	Epoch 1325:	Loss 0.4489	TrainAcc 0.9270	ValidAcc 0.9290	TestAcc 0.9254	BestValid 0.9313
	Epoch 1350:	Loss 0.4473	TrainAcc 0.9285	ValidAcc 0.9306	TestAcc 0.9271	BestValid 0.9313
	Epoch 1375:	Loss 0.4439	TrainAcc 0.9285	ValidAcc 0.9302	TestAcc 0.9271	BestValid 0.9313
	Epoch 1400:	Loss 0.4430	TrainAcc 0.9300	ValidAcc 0.9318	TestAcc 0.9286	BestValid 0.9318
	Epoch 1425:	Loss 0.4403	TrainAcc 0.9309	ValidAcc 0.9329	TestAcc 0.9296	BestValid 0.9329
	Epoch 1450:	Loss 0.4392	TrainAcc 0.9316	ValidAcc 0.9337	TestAcc 0.9306	BestValid 0.9337
	Epoch 1475:	Loss 0.4390	TrainAcc 0.9270	ValidAcc 0.9288	TestAcc 0.9252	BestValid 0.9337
	Epoch 1500:	Loss 0.4368	TrainAcc 0.9287	ValidAcc 0.9305	TestAcc 0.9271	BestValid 0.9337
	Epoch 1525:	Loss 0.4352	TrainAcc 0.9302	ValidAcc 0.9322	TestAcc 0.9283	BestValid 0.9337
	Epoch 1550:	Loss 0.4335	TrainAcc 0.9310	ValidAcc 0.9326	TestAcc 0.9297	BestValid 0.9337
	Epoch 1575:	Loss 0.4324	TrainAcc 0.9322	ValidAcc 0.9339	TestAcc 0.9311	BestValid 0.9339
	Epoch 1600:	Loss 0.4335	TrainAcc 0.9308	ValidAcc 0.9322	TestAcc 0.9290	BestValid 0.9339
	Epoch 1625:	Loss 0.4284	TrainAcc 0.9291	ValidAcc 0.9314	TestAcc 0.9273	BestValid 0.9339
	Epoch 1650:	Loss 0.4297	TrainAcc 0.9313	ValidAcc 0.9329	TestAcc 0.9298	BestValid 0.9339
	Epoch 1675:	Loss 0.4285	TrainAcc 0.9320	ValidAcc 0.9332	TestAcc 0.9305	BestValid 0.9339
	Epoch 1700:	Loss 0.4256	TrainAcc 0.9321	ValidAcc 0.9335	TestAcc 0.9307	BestValid 0.9339
	Epoch 1725:	Loss 0.4291	TrainAcc 0.9317	ValidAcc 0.9331	TestAcc 0.9304	BestValid 0.9339
	Epoch 1750:	Loss 0.4313	TrainAcc 0.9339	ValidAcc 0.9350	TestAcc 0.9324	BestValid 0.9350
	Epoch 1775:	Loss 0.4258	TrainAcc 0.9339	ValidAcc 0.9347	TestAcc 0.9324	BestValid 0.9350
	Epoch 1800:	Loss 0.4226	TrainAcc 0.9318	ValidAcc 0.9336	TestAcc 0.9300	BestValid 0.9350
	Epoch 1825:	Loss 0.4239	TrainAcc 0.9303	ValidAcc 0.9315	TestAcc 0.9285	BestValid 0.9350
	Epoch 1850:	Loss 0.4301	TrainAcc 0.9331	ValidAcc 0.9341	TestAcc 0.9315	BestValid 0.9350
	Epoch 1875:	Loss 0.4165	TrainAcc 0.9269	ValidAcc 0.9280	TestAcc 0.9247	BestValid 0.9350
	Epoch 1900:	Loss 0.4173	TrainAcc 0.9332	ValidAcc 0.9345	TestAcc 0.9316	BestValid 0.9350
	Epoch 1925:	Loss 0.4191	TrainAcc 0.9335	ValidAcc 0.9349	TestAcc 0.9316	BestValid 0.9350
	Epoch 1950:	Loss 0.4153	TrainAcc 0.9305	ValidAcc 0.9319	TestAcc 0.9280	BestValid 0.9350
	Epoch 1975:	Loss 0.4151	TrainAcc 0.9325	ValidAcc 0.9334	TestAcc 0.9302	BestValid 0.9350
	Epoch 2000:	Loss 0.4151	TrainAcc 0.9310	ValidAcc 0.9327	TestAcc 0.9288	BestValid 0.9350
	Epoch 2025:	Loss 0.4149	TrainAcc 0.9307	ValidAcc 0.9321	TestAcc 0.9282	BestValid 0.9350
	Epoch 2050:	Loss 0.4112	TrainAcc 0.9348	ValidAcc 0.9363	TestAcc 0.9329	BestValid 0.9363
	Epoch 2075:	Loss 0.4128	TrainAcc 0.9325	ValidAcc 0.9342	TestAcc 0.9302	BestValid 0.9363
	Epoch 2100:	Loss 0.4102	TrainAcc 0.9345	ValidAcc 0.9358	TestAcc 0.9322	BestValid 0.9363
	Epoch 2125:	Loss 0.4104	TrainAcc 0.9351	ValidAcc 0.9365	TestAcc 0.9329	BestValid 0.9365
	Epoch 2150:	Loss 0.4095	TrainAcc 0.9335	ValidAcc 0.9347	TestAcc 0.9311	BestValid 0.9365
	Epoch 2175:	Loss 0.4120	TrainAcc 0.9325	ValidAcc 0.9342	TestAcc 0.9302	BestValid 0.9365
	Epoch 2200:	Loss 0.4096	TrainAcc 0.9352	ValidAcc 0.9363	TestAcc 0.9330	BestValid 0.9365
	Epoch 2225:	Loss 0.4067	TrainAcc 0.9334	ValidAcc 0.9349	TestAcc 0.9308	BestValid 0.9365
	Epoch 2250:	Loss 0.4103	TrainAcc 0.9360	ValidAcc 0.9369	TestAcc 0.9340	BestValid 0.9369
	Epoch 2275:	Loss 0.4035	TrainAcc 0.9325	ValidAcc 0.9337	TestAcc 0.9298	BestValid 0.9369
	Epoch 2300:	Loss 0.4071	TrainAcc 0.9334	ValidAcc 0.9354	TestAcc 0.9308	BestValid 0.9369
	Epoch 2325:	Loss 0.4032	TrainAcc 0.9357	ValidAcc 0.9369	TestAcc 0.9335	BestValid 0.9369
	Epoch 2350:	Loss 0.4042	TrainAcc 0.9350	ValidAcc 0.9362	TestAcc 0.9328	BestValid 0.9369
	Epoch 2375:	Loss 0.4043	TrainAcc 0.9339	ValidAcc 0.9354	TestAcc 0.9312	BestValid 0.9369
	Epoch 2400:	Loss 0.4014	TrainAcc 0.9330	ValidAcc 0.9345	TestAcc 0.9303	BestValid 0.9369
	Epoch 2425:	Loss 0.3996	TrainAcc 0.9343	ValidAcc 0.9354	TestAcc 0.9314	BestValid 0.9369
	Epoch 2450:	Loss 0.4031	TrainAcc 0.9364	ValidAcc 0.9377	TestAcc 0.9342	BestValid 0.9377
	Epoch 2475:	Loss 0.3995	TrainAcc 0.9346	ValidAcc 0.9360	TestAcc 0.9321	BestValid 0.9377
	Epoch 2500:	Loss 0.3993	TrainAcc 0.9356	ValidAcc 0.9366	TestAcc 0.9331	BestValid 0.9377
	Epoch 2525:	Loss 0.3979	TrainAcc 0.9349	ValidAcc 0.9360	TestAcc 0.9324	BestValid 0.9377
	Epoch 2550:	Loss 0.4080	TrainAcc 0.9347	ValidAcc 0.9356	TestAcc 0.9316	BestValid 0.9377
	Epoch 2575:	Loss 0.3976	TrainAcc 0.9385	ValidAcc 0.9389	TestAcc 0.9359	BestValid 0.9389
	Epoch 2600:	Loss 0.3948	TrainAcc 0.9364	ValidAcc 0.9373	TestAcc 0.9336	BestValid 0.9389
	Epoch 2625:	Loss 0.3951	TrainAcc 0.9385	ValidAcc 0.9391	TestAcc 0.9357	BestValid 0.9391
	Epoch 2650:	Loss 0.3955	TrainAcc 0.9354	ValidAcc 0.9366	TestAcc 0.9323	BestValid 0.9391
	Epoch 2675:	Loss 0.3944	TrainAcc 0.9370	ValidAcc 0.9374	TestAcc 0.9342	BestValid 0.9391
	Epoch 2700:	Loss 0.3947	TrainAcc 0.9365	ValidAcc 0.9369	TestAcc 0.9337	BestValid 0.9391
	Epoch 2725:	Loss 0.3941	TrainAcc 0.9374	ValidAcc 0.9378	TestAcc 0.9345	BestValid 0.9391
	Epoch 2750:	Loss 0.3917	TrainAcc 0.9367	ValidAcc 0.9371	TestAcc 0.9339	BestValid 0.9391
	Epoch 2775:	Loss 0.4031	TrainAcc 0.9309	ValidAcc 0.9309	TestAcc 0.9276	BestValid 0.9391
	Epoch 2800:	Loss 0.3928	TrainAcc 0.9374	ValidAcc 0.9384	TestAcc 0.9347	BestValid 0.9391
	Epoch 2825:	Loss 0.3957	TrainAcc 0.9370	ValidAcc 0.9379	TestAcc 0.9339	BestValid 0.9391
	Epoch 2850:	Loss 0.3885	TrainAcc 0.9383	ValidAcc 0.9392	TestAcc 0.9354	BestValid 0.9392
	Epoch 2875:	Loss 0.3929	TrainAcc 0.9386	ValidAcc 0.9388	TestAcc 0.9356	BestValid 0.9392
	Epoch 2900:	Loss 0.3879	TrainAcc 0.9388	ValidAcc 0.9392	TestAcc 0.9357	BestValid 0.9392
	Epoch 2925:	Loss 0.3920	TrainAcc 0.9380	ValidAcc 0.9388	TestAcc 0.9349	BestValid 0.9392
	Epoch 2950:	Loss 0.3934	TrainAcc 0.9399	ValidAcc 0.9400	TestAcc 0.9367	BestValid 0.9400
	Epoch 2975:	Loss 0.3876	TrainAcc 0.9390	ValidAcc 0.9393	TestAcc 0.9359	BestValid 0.9400
	Epoch 3000:	Loss 0.3932	TrainAcc 0.9367	ValidAcc 0.9374	TestAcc 0.9333	BestValid 0.9400
	Epoch 3025:	Loss 0.3897	TrainAcc 0.9406	ValidAcc 0.9408	TestAcc 0.9377	BestValid 0.9408
	Epoch 3050:	Loss 0.3840	TrainAcc 0.9369	ValidAcc 0.9375	TestAcc 0.9333	BestValid 0.9408
	Epoch 3075:	Loss 0.3838	TrainAcc 0.9390	ValidAcc 0.9394	TestAcc 0.9358	BestValid 0.9408
	Epoch 3100:	Loss 0.3850	TrainAcc 0.9354	ValidAcc 0.9362	TestAcc 0.9319	BestValid 0.9408
	Epoch 3125:	Loss 0.3901	TrainAcc 0.9392	ValidAcc 0.9394	TestAcc 0.9357	BestValid 0.9408
	Epoch 3150:	Loss 0.3845	TrainAcc 0.9369	ValidAcc 0.9378	TestAcc 0.9329	BestValid 0.9408
	Epoch 3175:	Loss 0.3827	TrainAcc 0.9395	ValidAcc 0.9394	TestAcc 0.9362	BestValid 0.9408
	Epoch 3200:	Loss 0.3845	TrainAcc 0.9397	ValidAcc 0.9397	TestAcc 0.9364	BestValid 0.9408
	Epoch 3225:	Loss 0.3795	TrainAcc 0.9367	ValidAcc 0.9371	TestAcc 0.9328	BestValid 0.9408
	Epoch 3250:	Loss 0.3789	TrainAcc 0.9395	ValidAcc 0.9394	TestAcc 0.9359	BestValid 0.9408
	Epoch 3275:	Loss 0.3877	TrainAcc 0.9370	ValidAcc 0.9375	TestAcc 0.9332	BestValid 0.9408
	Epoch 3300:	Loss 0.3857	TrainAcc 0.9394	ValidAcc 0.9401	TestAcc 0.9359	BestValid 0.9408
	Epoch 3325:	Loss 0.3807	TrainAcc 0.9383	ValidAcc 0.9388	TestAcc 0.9344	BestValid 0.9408
	Epoch 3350:	Loss 0.3802	TrainAcc 0.9387	ValidAcc 0.9391	TestAcc 0.9351	BestValid 0.9408
	Epoch 3375:	Loss 0.3822	TrainAcc 0.9376	ValidAcc 0.9381	TestAcc 0.9340	BestValid 0.9408
	Epoch 3400:	Loss 0.3799	TrainAcc 0.9396	ValidAcc 0.9396	TestAcc 0.9361	BestValid 0.9408
	Epoch 3425:	Loss 0.3837	TrainAcc 0.9364	ValidAcc 0.9372	TestAcc 0.9329	BestValid 0.9408
	Epoch 3450:	Loss 0.3800	TrainAcc 0.9407	ValidAcc 0.9402	TestAcc 0.9370	BestValid 0.9408
	Epoch 3475:	Loss 0.3833	TrainAcc 0.9406	ValidAcc 0.9404	TestAcc 0.9367	BestValid 0.9408
	Epoch 3500:	Loss 0.3795	TrainAcc 0.9389	ValidAcc 0.9394	TestAcc 0.9354	BestValid 0.9408
	Epoch 3525:	Loss 0.3920	TrainAcc 0.9379	ValidAcc 0.9383	TestAcc 0.9342	BestValid 0.9408
	Epoch 3550:	Loss 0.3826	TrainAcc 0.9404	ValidAcc 0.9404	TestAcc 0.9366	BestValid 0.9408
	Epoch 3575:	Loss 0.3766	TrainAcc 0.9393	ValidAcc 0.9394	TestAcc 0.9358	BestValid 0.9408
	Epoch 3600:	Loss 0.4053	TrainAcc 0.9395	ValidAcc 0.9394	TestAcc 0.9356	BestValid 0.9408
	Epoch 3625:	Loss 0.3755	TrainAcc 0.9383	ValidAcc 0.9383	TestAcc 0.9340	BestValid 0.9408
	Epoch 3650:	Loss 0.3792	TrainAcc 0.9392	ValidAcc 0.9391	TestAcc 0.9353	BestValid 0.9408
	Epoch 3675:	Loss 0.3758	TrainAcc 0.9393	ValidAcc 0.9393	TestAcc 0.9351	BestValid 0.9408
	Epoch 3700:	Loss 0.3860	TrainAcc 0.9387	ValidAcc 0.9388	TestAcc 0.9347	BestValid 0.9408
	Epoch 3725:	Loss 0.3748	TrainAcc 0.9401	ValidAcc 0.9398	TestAcc 0.9359	BestValid 0.9408
	Epoch 3750:	Loss 0.3747	TrainAcc 0.9395	ValidAcc 0.9394	TestAcc 0.9356	BestValid 0.9408
	Epoch 3775:	Loss 0.3746	TrainAcc 0.9394	ValidAcc 0.9391	TestAcc 0.9354	BestValid 0.9408
	Epoch 3800:	Loss 0.3759	TrainAcc 0.9411	ValidAcc 0.9410	TestAcc 0.9373	BestValid 0.9410
	Epoch 3825:	Loss 0.3753	TrainAcc 0.9381	ValidAcc 0.9380	TestAcc 0.9337	BestValid 0.9410
	Epoch 3850:	Loss 0.3739	TrainAcc 0.9383	ValidAcc 0.9384	TestAcc 0.9342	BestValid 0.9410
	Epoch 3875:	Loss 0.3760	TrainAcc 0.9404	ValidAcc 0.9401	TestAcc 0.9364	BestValid 0.9410
	Epoch 3900:	Loss 0.3727	TrainAcc 0.9406	ValidAcc 0.9405	TestAcc 0.9367	BestValid 0.9410
	Epoch 3925:	Loss 0.3718	TrainAcc 0.9406	ValidAcc 0.9403	TestAcc 0.9365	BestValid 0.9410
	Epoch 3950:	Loss 0.3713	TrainAcc 0.9384	ValidAcc 0.9385	TestAcc 0.9344	BestValid 0.9410
	Epoch 3975:	Loss 0.3728	TrainAcc 0.9423	ValidAcc 0.9417	TestAcc 0.9383	BestValid 0.9417
	Epoch 4000:	Loss 0.3711	TrainAcc 0.9404	ValidAcc 0.9400	TestAcc 0.9363	BestValid 0.9417
	Epoch 4025:	Loss 0.3738	TrainAcc 0.9414	ValidAcc 0.9407	TestAcc 0.9371	BestValid 0.9417
	Epoch 4050:	Loss 0.3724	TrainAcc 0.9419	ValidAcc 0.9415	TestAcc 0.9376	BestValid 0.9417
	Epoch 4075:	Loss 0.3739	TrainAcc 0.9403	ValidAcc 0.9398	TestAcc 0.9362	BestValid 0.9417
	Epoch 4100:	Loss 0.3741	TrainAcc 0.9428	ValidAcc 0.9423	TestAcc 0.9385	BestValid 0.9423
	Epoch 4125:	Loss 0.3716	TrainAcc 0.9387	ValidAcc 0.9391	TestAcc 0.9343	BestValid 0.9423
	Epoch 4150:	Loss 0.3695	TrainAcc 0.9422	ValidAcc 0.9416	TestAcc 0.9378	BestValid 0.9423
	Epoch 4175:	Loss 0.3728	TrainAcc 0.9390	ValidAcc 0.9388	TestAcc 0.9346	BestValid 0.9423
	Epoch 4200:	Loss 0.3672	TrainAcc 0.9408	ValidAcc 0.9402	TestAcc 0.9363	BestValid 0.9423
	Epoch 4225:	Loss 0.3690	TrainAcc 0.9405	ValidAcc 0.9405	TestAcc 0.9361	BestValid 0.9423
	Epoch 4250:	Loss 0.3674	TrainAcc 0.9416	ValidAcc 0.9410	TestAcc 0.9374	BestValid 0.9423
	Epoch 4275:	Loss 0.3768	TrainAcc 0.9391	ValidAcc 0.9395	TestAcc 0.9347	BestValid 0.9423
	Epoch 4300:	Loss 0.3692	TrainAcc 0.9422	ValidAcc 0.9417	TestAcc 0.9380	BestValid 0.9423
	Epoch 4325:	Loss 0.3718	TrainAcc 0.9389	ValidAcc 0.9384	TestAcc 0.9343	BestValid 0.9423
	Epoch 4350:	Loss 0.3672	TrainAcc 0.9425	ValidAcc 0.9418	TestAcc 0.9383	BestValid 0.9423
	Epoch 4375:	Loss 0.3701	TrainAcc 0.9379	ValidAcc 0.9376	TestAcc 0.9331	BestValid 0.9423
	Epoch 4400:	Loss 0.3678	TrainAcc 0.9395	ValidAcc 0.9394	TestAcc 0.9346	BestValid 0.9423
	Epoch 4425:	Loss 0.3649	TrainAcc 0.9441	ValidAcc 0.9434	TestAcc 0.9398	BestValid 0.9434
	Epoch 4450:	Loss 0.3678	TrainAcc 0.9413	ValidAcc 0.9410	TestAcc 0.9368	BestValid 0.9434
	Epoch 4475:	Loss 0.3727	TrainAcc 0.9410	ValidAcc 0.9405	TestAcc 0.9359	BestValid 0.9434
	Epoch 4500:	Loss 0.3619	TrainAcc 0.9426	ValidAcc 0.9419	TestAcc 0.9382	BestValid 0.9434
	Epoch 4525:	Loss 0.3655	TrainAcc 0.9420	ValidAcc 0.9411	TestAcc 0.9374	BestValid 0.9434
	Epoch 4550:	Loss 0.3694	TrainAcc 0.9405	ValidAcc 0.9403	TestAcc 0.9361	BestValid 0.9434
	Epoch 4575:	Loss 0.3644	TrainAcc 0.9423	ValidAcc 0.9414	TestAcc 0.9379	BestValid 0.9434
	Epoch 4600:	Loss 0.3681	TrainAcc 0.9426	ValidAcc 0.9415	TestAcc 0.9380	BestValid 0.9434
	Epoch 4625:	Loss 0.3653	TrainAcc 0.9430	ValidAcc 0.9419	TestAcc 0.9383	BestValid 0.9434
	Epoch 4650:	Loss 0.3646	TrainAcc 0.9410	ValidAcc 0.9406	TestAcc 0.9363	BestValid 0.9434
	Epoch 4675:	Loss 0.3612	TrainAcc 0.9406	ValidAcc 0.9402	TestAcc 0.9359	BestValid 0.9434
	Epoch 4700:	Loss 0.3633	TrainAcc 0.9449	ValidAcc 0.9440	TestAcc 0.9406	BestValid 0.9440
	Epoch 4725:	Loss 0.3635	TrainAcc 0.9410	ValidAcc 0.9407	TestAcc 0.9366	BestValid 0.9440
	Epoch 4750:	Loss 0.3644	TrainAcc 0.9418	ValidAcc 0.9413	TestAcc 0.9372	BestValid 0.9440
	Epoch 4775:	Loss 0.3633	TrainAcc 0.9437	ValidAcc 0.9428	TestAcc 0.9393	BestValid 0.9440
	Epoch 4800:	Loss 0.3592	TrainAcc 0.9439	ValidAcc 0.9428	TestAcc 0.9390	BestValid 0.9440
	Epoch 4825:	Loss 0.3612	TrainAcc 0.9444	ValidAcc 0.9431	TestAcc 0.9397	BestValid 0.9440
	Epoch 4850:	Loss 0.3624	TrainAcc 0.9411	ValidAcc 0.9409	TestAcc 0.9365	BestValid 0.9440
	Epoch 4875:	Loss 0.3665	TrainAcc 0.9391	ValidAcc 0.9391	TestAcc 0.9344	BestValid 0.9440
	Epoch 4900:	Loss 0.3584	TrainAcc 0.9421	ValidAcc 0.9416	TestAcc 0.9374	BestValid 0.9440
	Epoch 4925:	Loss 0.3642	TrainAcc 0.9420	ValidAcc 0.9414	TestAcc 0.9370	BestValid 0.9440
	Epoch 4950:	Loss 0.3712	TrainAcc 0.9439	ValidAcc 0.9428	TestAcc 0.9391	BestValid 0.9440
	Epoch 4975:	Loss 0.3623	TrainAcc 0.9419	ValidAcc 0.9412	TestAcc 0.9369	BestValid 0.9440
	Epoch 5000:	Loss 0.3628	TrainAcc 0.9402	ValidAcc 0.9393	TestAcc 0.9348	BestValid 0.9440
Node 1, Pre/Post-Pipelining: 8.587 / 17.352 ms, Bubble: 0.956 ms, Compute: 881.861 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 8.591 / 17.361 ms, Bubble: 0.433 ms, Compute: 882.375 ms, Comm: 0.009 ms, Imbalance: 0.018 ms
Node 0, Pre/Post-Pipelining: 8.589 / 17.392 ms, Bubble: 1.030 ms, Compute: 881.749 ms, Comm: 0.011 ms, Imbalance: 0.023 ms
Node 3, Pre/Post-Pipelining: 8.593 / 17.391 ms, Bubble: 0.055 ms, Compute: 882.717 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 4, Pre/Post-Pipelining: 8.585 / 17.290 ms, Bubble: 1.175 ms, Compute: 881.709 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 5, Pre/Post-Pipelining: 8.584 / 17.328 ms, Bubble: 1.055 ms, Compute: 881.790 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 8.589 / 17.331 ms, Bubble: 0.609 ms, Compute: 882.229 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 7, Pre/Post-Pipelining: 8.580 / 17.290 ms, Bubble: 1.095 ms, Compute: 881.794 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.589 ms
Cluster-Wide Average, Post-Pipelining Overhead: 17.392 ms
Cluster-Wide Average, Bubble: 1.030 ms
Cluster-Wide Average, Compute: 881.749 ms
Cluster-Wide Average, Communication: 0.011 ms
Cluster-Wide Average, Imbalance: 0.023 ms
Node 0, GPU memory consumption: 22.329 GB
Node 1, GPU memory consumption: 21.518 GB
Node 2, GPU memory consumption: 21.526 GB
Node 3, GPU memory consumption: 21.505 GB
Node 5, GPU memory consumption: 21.518 GB
Node 6, GPU memory consumption: 21.522 GB
Node 4, GPU memory consumption: 21.495 GB
Node 7, GPU memory consumption: 21.499 GB
Node 0, Graph-Level Communication Throughput: 25.551 Gbps, Time: 624.824 ms
Node 1, Graph-Level Communication Throughput: 23.804 Gbps, Time: 713.385 ms
Node 2, Graph-Level Communication Throughput: 36.197 Gbps, Time: 488.611 ms
Node 3, Graph-Level Communication Throughput: 49.582 Gbps, Time: 466.760 ms
Node 4, Graph-Level Communication Throughput: 11.516 Gbps, Time: 730.717 ms
Node 5, Graph-Level Communication Throughput: 19.102 Gbps, Time: 639.363 ms
Node 6, Graph-Level Communication Throughput: 26.465 Gbps, Time: 618.975 ms
Node 7, Graph-Level Communication Throughput: 21.467 Gbps, Time: 634.226 ms
------------------------node id 0,  per-epoch time: 1.405144 s---------------
------------------------node id 1,  per-epoch time: 1.405144 s---------------
------------------------node id 2,  per-epoch time: 1.405144 s---------------
------------------------node id 3,  per-epoch time: 1.405144 s---------------
------------------------node id 4,  per-epoch time: 1.405144 s---------------
------------------------node id 5,  per-epoch time: 1.405144 s---------------
------------------------node id 6,  per-epoch time: 1.405144 s---------------
------------------------node id 7,  per-epoch time: 1.405144 s---------------
************ Profiling Results ************
	Bubble: 522.901530 (ms) (37.23 percentage)
	Compute: 228.247156 (ms) (16.25 percentage)
	GraphCommComputeOverhead: 19.100534 (ms) (1.36 percentage)
	GraphCommNetwork: 614.606204 (ms) (43.75 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.815526 (ms) (1.41 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 14.493 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.9440
Target test_acc: 0.9406
Epoch to reach the target acc: 4699
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
