Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.922 seconds.
Building the CSC structure...
        It takes 2.105 seconds.
Building the CSC structure...
        It takes 2.387 seconds.
Building the CSC structure...
        It takes 2.392 seconds.
Building the CSC structure...
        It takes 2.438 seconds.
Building the CSC structure...
        It takes 2.447 seconds.
Building the CSC structure...
        It takes 2.650 seconds.
Building the CSC structure...
        It takes 2.665 seconds.
Building the CSC structure...
        It takes 1.837 seconds.
        It takes 1.878 seconds.
        It takes 2.268 seconds.
        It takes 2.334 seconds.
Building the Feature Vector...
        It takes 2.390 seconds.
        It takes 2.375 seconds.
        It takes 2.268 seconds.
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 2.380 seconds.
        It takes 0.039 seconds.
Building the Feature Vector...
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.254 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.779 Gbps (per GPU), 510.230 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.735 Gbps (per GPU), 509.876 Gbps (aggregated)
The layer-level communication performance: 63.733 Gbps (per GPU), 509.863 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.698 Gbps (per GPU), 509.588 Gbps (aggregated)
The layer-level communication performance: 63.695 Gbps (per GPU), 509.560 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.665 Gbps (per GPU), 509.318 Gbps (aggregated)
The layer-level communication performance: 63.658 Gbps (per GPU), 509.262 Gbps (aggregated)
The layer-level communication performance: 63.654 Gbps (per GPU), 509.231 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.024 Gbps (per GPU), 1320.189 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.016 Gbps (per GPU), 1320.131 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.023 Gbps (per GPU), 1320.186 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.015 Gbps (per GPU), 1320.118 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.020 Gbps (per GPU), 1320.164 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.018 Gbps (per GPU), 1320.147 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.020 Gbps (per GPU), 1320.160 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.022 Gbps (per GPU), 1320.176 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.202 Gbps (per GPU), 905.615 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.206 Gbps (per GPU), 905.649 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.205 Gbps (per GPU), 905.643 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.205 Gbps (per GPU), 905.643 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.205 Gbps (per GPU), 905.638 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.204 Gbps (per GPU), 905.628 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.204 Gbps (per GPU), 905.631 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.204 Gbps (per GPU), 905.631 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.225 Gbps (per GPU), 361.798 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.225 Gbps (per GPU), 361.798 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.224 Gbps (per GPU), 361.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.224 Gbps (per GPU), 361.791 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.225 Gbps (per GPU), 361.797 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.225 Gbps (per GPU), 361.796 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.225 Gbps (per GPU), 361.796 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.225 Gbps (per GPU), 361.797 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.48ms  2.80ms  3.11  8.38K  3.53M
 chk_1  0.76ms  2.87ms  3.02ms  3.94  6.74K  3.60M
 chk_2  0.81ms  2.73ms  2.91ms  3.59  7.27K  3.53M
 chk_3  0.82ms  2.79ms  2.95ms  3.62  7.92K  3.61M
 chk_4  0.64ms  2.71ms  2.85ms  4.46  5.33K  3.68M
 chk_5  1.02ms  2.70ms  2.88ms  2.82 10.07K  3.45M
 chk_6  0.97ms  2.84ms  3.03ms  3.11  9.41K  3.48M
 chk_7  0.83ms  2.69ms  2.86ms  3.44  8.12K  3.60M
 chk_8  0.69ms  2.81ms  2.95ms  4.26  6.09K  3.64M
 chk_9  1.11ms  2.61ms  2.82ms  2.53 11.10K  3.38M
chk_10  0.66ms  2.84ms  2.97ms  4.48  5.67K  3.63M
chk_11  0.84ms  2.70ms  2.89ms  3.46  8.16K  3.54M
chk_12  0.81ms  2.89ms  3.07ms  3.81  7.24K  3.55M
chk_13  0.65ms  2.75ms  2.90ms  4.46  5.41K  3.68M
chk_14  0.79ms  2.94ms  3.13ms  3.96  7.14K  3.53M
chk_15  0.97ms  2.84ms  3.03ms  3.13  9.25K  3.49M
chk_16  0.61ms  2.68ms  2.82ms  4.64  4.78K  3.77M
chk_17  0.78ms  2.78ms  2.95ms  3.75  6.85K  3.60M
chk_18  0.82ms  2.60ms  2.77ms  3.37  7.47K  3.57M
chk_19  0.62ms  2.66ms  2.81ms  4.56  4.88K  3.75M
chk_20  0.78ms  2.69ms  2.85ms  3.64  7.00K  3.63M
chk_21  0.65ms  2.67ms  2.82ms  4.36  5.41K  3.68M
chk_22  1.11ms  2.86ms  3.07ms  2.76 11.07K  3.39M
chk_23  0.81ms  2.79ms  2.94ms  3.65  7.23K  3.64M
chk_24  1.03ms  2.82ms  3.01ms  2.93 10.13K  3.43M
chk_25  0.74ms  2.65ms  2.82ms  3.80  6.40K  3.57M
chk_26  0.67ms  2.83ms  3.03ms  4.51  5.78K  3.55M
chk_27  0.97ms  2.73ms  2.96ms  3.04  9.34K  3.48M
chk_28  0.74ms  3.00ms  3.18ms  4.31  6.37K  3.57M
chk_29  0.64ms  2.83ms  2.99ms  4.66  5.16K  3.78M
chk_30  0.65ms  2.70ms  2.87ms  4.42  5.44K  3.67M
chk_31  0.74ms  2.84ms  3.01ms  4.09  6.33K  3.63M
   Avg  0.80  2.76  2.94
   Max  1.11  3.00  3.18
   Min  0.61  2.48  2.77
 Ratio  1.84  1.21  1.15
   Var  0.02  0.01  0.01
Profiling takes 2.492 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 378.944 ms
Partition 0 [0, 5) has cost: 378.944 ms
Partition 1 [5, 9) has cost: 353.315 ms
Partition 2 [9, 13) has cost: 353.315 ms
Partition 3 [13, 17) has cost: 353.315 ms
Partition 4 [17, 21) has cost: 353.315 ms
Partition 5 [21, 25) has cost: 353.315 ms
Partition 6 [25, 29) has cost: 353.315 ms
Partition 7 [29, 33) has cost: 358.920 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 167.835 ms
GPU 0, Compute+Comm Time: 136.092 ms, Bubble Time: 29.556 ms, Imbalance Overhead: 2.187 ms
GPU 1, Compute+Comm Time: 128.899 ms, Bubble Time: 29.218 ms, Imbalance Overhead: 9.718 ms
GPU 2, Compute+Comm Time: 128.899 ms, Bubble Time: 29.277 ms, Imbalance Overhead: 9.659 ms
GPU 3, Compute+Comm Time: 128.899 ms, Bubble Time: 29.185 ms, Imbalance Overhead: 9.752 ms
GPU 4, Compute+Comm Time: 128.899 ms, Bubble Time: 29.136 ms, Imbalance Overhead: 9.800 ms
GPU 5, Compute+Comm Time: 128.899 ms, Bubble Time: 29.191 ms, Imbalance Overhead: 9.745 ms
GPU 6, Compute+Comm Time: 128.899 ms, Bubble Time: 29.443 ms, Imbalance Overhead: 9.494 ms
GPU 7, Compute+Comm Time: 130.113 ms, Bubble Time: 29.803 ms, Imbalance Overhead: 7.918 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 326.505 ms
GPU 0, Compute+Comm Time: 252.184 ms, Bubble Time: 58.263 ms, Imbalance Overhead: 16.059 ms
GPU 1, Compute+Comm Time: 247.793 ms, Bubble Time: 57.501 ms, Imbalance Overhead: 21.211 ms
GPU 2, Compute+Comm Time: 247.793 ms, Bubble Time: 56.911 ms, Imbalance Overhead: 21.801 ms
GPU 3, Compute+Comm Time: 247.793 ms, Bubble Time: 56.820 ms, Imbalance Overhead: 21.892 ms
GPU 4, Compute+Comm Time: 247.793 ms, Bubble Time: 56.795 ms, Imbalance Overhead: 21.916 ms
GPU 5, Compute+Comm Time: 247.793 ms, Bubble Time: 56.886 ms, Imbalance Overhead: 21.826 ms
GPU 6, Compute+Comm Time: 247.793 ms, Bubble Time: 56.660 ms, Imbalance Overhead: 22.052 ms
GPU 7, Compute+Comm Time: 266.229 ms, Bubble Time: 57.306 ms, Imbalance Overhead: 2.970 ms
The estimated cost of the whole pipeline: 519.057 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 732.259 ms
Partition 0 [0, 9) has cost: 732.259 ms
Partition 1 [9, 17) has cost: 706.630 ms
Partition 2 [17, 25) has cost: 706.630 ms
Partition 3 [25, 33) has cost: 712.235 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 175.786 ms
GPU 0, Compute+Comm Time: 146.120 ms, Bubble Time: 26.804 ms, Imbalance Overhead: 2.862 ms
GPU 1, Compute+Comm Time: 142.133 ms, Bubble Time: 27.013 ms, Imbalance Overhead: 6.641 ms
GPU 2, Compute+Comm Time: 142.133 ms, Bubble Time: 26.950 ms, Imbalance Overhead: 6.703 ms
GPU 3, Compute+Comm Time: 142.711 ms, Bubble Time: 27.346 ms, Imbalance Overhead: 5.730 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 328.687 ms
GPU 0, Compute+Comm Time: 265.973 ms, Bubble Time: 51.147 ms, Imbalance Overhead: 11.567 ms
GPU 1, Compute+Comm Time: 263.822 ms, Bubble Time: 50.745 ms, Imbalance Overhead: 14.121 ms
GPU 2, Compute+Comm Time: 263.822 ms, Bubble Time: 51.008 ms, Imbalance Overhead: 13.857 ms
GPU 3, Compute+Comm Time: 274.255 ms, Bubble Time: 50.700 ms, Imbalance Overhead: 3.732 ms
    The estimated cost with 2 DP ways is 529.697 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1438.888 ms
Partition 0 [0, 17) has cost: 1438.888 ms
Partition 1 [17, 33) has cost: 1418.864 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 212.466 ms
GPU 0, Compute+Comm Time: 185.824 ms, Bubble Time: 21.468 ms, Imbalance Overhead: 5.173 ms
GPU 1, Compute+Comm Time: 184.023 ms, Bubble Time: 22.680 ms, Imbalance Overhead: 5.763 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 354.633 ms
GPU 0, Compute+Comm Time: 308.471 ms, Bubble Time: 38.545 ms, Imbalance Overhead: 7.617 ms
GPU 1, Compute+Comm Time: 312.965 ms, Bubble Time: 37.673 ms, Imbalance Overhead: 3.994 ms
    The estimated cost with 4 DP ways is 595.453 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2857.753 ms
Partition 0 [0, 33) has cost: 2857.753 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 460.859 ms
GPU 0, Compute+Comm Time: 460.859 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 589.348 ms
GPU 0, Compute+Comm Time: 589.348 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1102.718 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5557	TrainAcc 0.4334	ValidAcc 0.4520	TestAcc 0.4517	BestValid 0.4520
	Epoch 50:	Loss 1.8796	TrainAcc 0.6993	ValidAcc 0.7152	TestAcc 0.7096	BestValid 0.7152
	Epoch 75:	Loss 1.4990	TrainAcc 0.7650	ValidAcc 0.7782	TestAcc 0.7718	BestValid 0.7782
	Epoch 100:	Loss 1.2745	TrainAcc 0.7915	ValidAcc 0.8037	TestAcc 0.7969	BestValid 0.8037
	Epoch 125:	Loss 1.0880	TrainAcc 0.8257	ValidAcc 0.8372	TestAcc 0.8291	BestValid 0.8372
	Epoch 150:	Loss 0.9941	TrainAcc 0.8475	ValidAcc 0.8560	TestAcc 0.8500	BestValid 0.8560
	Epoch 175:	Loss 0.9264	TrainAcc 0.8600	ValidAcc 0.8694	TestAcc 0.8613	BestValid 0.8694
	Epoch 200:	Loss 0.8769	TrainAcc 0.8681	ValidAcc 0.8763	TestAcc 0.8686	BestValid 0.8763
	Epoch 225:	Loss 0.8248	TrainAcc 0.8749	ValidAcc 0.8820	TestAcc 0.8758	BestValid 0.8820
	Epoch 250:	Loss 0.7941	TrainAcc 0.8812	ValidAcc 0.8867	TestAcc 0.8821	BestValid 0.8867
	Epoch 275:	Loss 0.7657	TrainAcc 0.8861	ValidAcc 0.8913	TestAcc 0.8868	BestValid 0.8913
	Epoch 300:	Loss 0.7452	TrainAcc 0.8913	ValidAcc 0.8961	TestAcc 0.8914	BestValid 0.8961
	Epoch 325:	Loss 0.7193	TrainAcc 0.8940	ValidAcc 0.8987	TestAcc 0.8936	BestValid 0.8987
	Epoch 350:	Loss 0.7027	TrainAcc 0.8951	ValidAcc 0.9000	TestAcc 0.8952	BestValid 0.9000
	Epoch 375:	Loss 0.6839	TrainAcc 0.8987	ValidAcc 0.9033	TestAcc 0.8982	BestValid 0.9033
	Epoch 400:	Loss 0.6759	TrainAcc 0.9011	ValidAcc 0.9045	TestAcc 0.9002	BestValid 0.9045
	Epoch 425:	Loss 0.6587	TrainAcc 0.9033	ValidAcc 0.9064	TestAcc 0.9018	BestValid 0.9064
	Epoch 450:	Loss 0.6451	TrainAcc 0.9044	ValidAcc 0.9076	TestAcc 0.9028	BestValid 0.9076
	Epoch 475:	Loss 0.6344	TrainAcc 0.9069	ValidAcc 0.9098	TestAcc 0.9051	BestValid 0.9098
	Epoch 500:	Loss 0.6274	TrainAcc 0.9085	ValidAcc 0.9119	TestAcc 0.9066	BestValid 0.9119
	Epoch 525:	Loss 0.6169	TrainAcc 0.9089	ValidAcc 0.9118	TestAcc 0.9074	BestValid 0.9119
	Epoch 550:	Loss 0.6092	TrainAcc 0.9110	ValidAcc 0.9137	TestAcc 0.9095	BestValid 0.9137
	Epoch 575:	Loss 0.5996	TrainAcc 0.9126	ValidAcc 0.9150	TestAcc 0.9112	BestValid 0.9150
	Epoch 600:	Loss 0.5950	TrainAcc 0.9139	ValidAcc 0.9149	TestAcc 0.9116	BestValid 0.9150
	Epoch 625:	Loss 0.5878	TrainAcc 0.9159	ValidAcc 0.9175	TestAcc 0.9135	BestValid 0.9175
	Epoch 650:	Loss 0.5826	TrainAcc 0.9165	ValidAcc 0.9178	TestAcc 0.9142	BestValid 0.9178
	Epoch 675:	Loss 0.5769	TrainAcc 0.9162	ValidAcc 0.9171	TestAcc 0.9145	BestValid 0.9178
	Epoch 700:	Loss 0.5697	TrainAcc 0.9180	ValidAcc 0.9195	TestAcc 0.9159	BestValid 0.9195
	Epoch 725:	Loss 0.5629	TrainAcc 0.9188	ValidAcc 0.9199	TestAcc 0.9168	BestValid 0.9199
	Epoch 750:	Loss 0.5563	TrainAcc 0.9197	ValidAcc 0.9209	TestAcc 0.9172	BestValid 0.9209
	Epoch 775:	Loss 0.5545	TrainAcc 0.9203	ValidAcc 0.9215	TestAcc 0.9183	BestValid 0.9215
	Epoch 800:	Loss 0.5485	TrainAcc 0.9214	ValidAcc 0.9217	TestAcc 0.9188	BestValid 0.9217
	Epoch 825:	Loss 0.5453	TrainAcc 0.9218	ValidAcc 0.9221	TestAcc 0.9191	BestValid 0.9221
	Epoch 850:	Loss 0.5401	TrainAcc 0.9230	ValidAcc 0.9232	TestAcc 0.9201	BestValid 0.9232
	Epoch 875:	Loss 0.5360	TrainAcc 0.9236	ValidAcc 0.9236	TestAcc 0.9207	BestValid 0.9236
	Epoch 900:	Loss 0.5345	TrainAcc 0.9241	ValidAcc 0.9238	TestAcc 0.9210	BestValid 0.9238
	Epoch 925:	Loss 0.5300	TrainAcc 0.9251	ValidAcc 0.9245	TestAcc 0.9219	BestValid 0.9245
	Epoch 950:	Loss 0.5270	TrainAcc 0.9251	ValidAcc 0.9241	TestAcc 0.9217	BestValid 0.9245
	Epoch 975:	Loss 0.5275	TrainAcc 0.9256	ValidAcc 0.9243	TestAcc 0.9220	BestValid 0.9245
	Epoch 1000:	Loss 0.5186	TrainAcc 0.9272	ValidAcc 0.9266	TestAcc 0.9239	BestValid 0.9266
	Epoch 1025:	Loss 0.5155	TrainAcc 0.9271	ValidAcc 0.9256	TestAcc 0.9234	BestValid 0.9266
	Epoch 1050:	Loss 0.5119	TrainAcc 0.9274	ValidAcc 0.9261	TestAcc 0.9236	BestValid 0.9266
	Epoch 1075:	Loss 0.5115	TrainAcc 0.9281	ValidAcc 0.9267	TestAcc 0.9244	BestValid 0.9267
	Epoch 1100:	Loss 0.5095	TrainAcc 0.9288	ValidAcc 0.9267	TestAcc 0.9244	BestValid 0.9267
	Epoch 1125:	Loss 0.5070	TrainAcc 0.9294	ValidAcc 0.9277	TestAcc 0.9250	BestValid 0.9277
	Epoch 1150:	Loss 0.5014	TrainAcc 0.9294	ValidAcc 0.9281	TestAcc 0.9249	BestValid 0.9281
	Epoch 1175:	Loss 0.5014	TrainAcc 0.9307	ValidAcc 0.9291	TestAcc 0.9264	BestValid 0.9291
	Epoch 1200:	Loss 0.5003	TrainAcc 0.9305	ValidAcc 0.9285	TestAcc 0.9260	BestValid 0.9291
	Epoch 1225:	Loss 0.4979	TrainAcc 0.9306	ValidAcc 0.9283	TestAcc 0.9257	BestValid 0.9291
	Epoch 1250:	Loss 0.4936	TrainAcc 0.9309	ValidAcc 0.9289	TestAcc 0.9259	BestValid 0.9291
	Epoch 1275:	Loss 0.4905	TrainAcc 0.9315	ValidAcc 0.9296	TestAcc 0.9269	BestValid 0.9296
	Epoch 1300:	Loss 0.4875	TrainAcc 0.9319	ValidAcc 0.9299	TestAcc 0.9268	BestValid 0.9299
	Epoch 1325:	Loss 0.4877	TrainAcc 0.9321	ValidAcc 0.9301	TestAcc 0.9274	BestValid 0.9301
	Epoch 1350:	Loss 0.4844	TrainAcc 0.9321	ValidAcc 0.9296	TestAcc 0.9273	BestValid 0.9301
	Epoch 1375:	Loss 0.4845	TrainAcc 0.9332	ValidAcc 0.9315	TestAcc 0.9287	BestValid 0.9315
	Epoch 1400:	Loss 0.4781	TrainAcc 0.9332	ValidAcc 0.9310	TestAcc 0.9282	BestValid 0.9315
	Epoch 1425:	Loss 0.4817	TrainAcc 0.9338	ValidAcc 0.9313	TestAcc 0.9286	BestValid 0.9315
	Epoch 1450:	Loss 0.4786	TrainAcc 0.9340	ValidAcc 0.9315	TestAcc 0.9287	BestValid 0.9315
	Epoch 1475:	Loss 0.4798	TrainAcc 0.9341	ValidAcc 0.9305	TestAcc 0.9287	BestValid 0.9315
	Epoch 1500:	Loss 0.4783	TrainAcc 0.9347	ValidAcc 0.9316	TestAcc 0.9295	BestValid 0.9316
	Epoch 1525:	Loss 0.4740	TrainAcc 0.9353	ValidAcc 0.9323	TestAcc 0.9302	BestValid 0.9323
	Epoch 1550:	Loss 0.4723	TrainAcc 0.9347	ValidAcc 0.9317	TestAcc 0.9295	BestValid 0.9323
	Epoch 1575:	Loss 0.4723	TrainAcc 0.9350	ValidAcc 0.9321	TestAcc 0.9296	BestValid 0.9323
	Epoch 1600:	Loss 0.4721	TrainAcc 0.9356	ValidAcc 0.9323	TestAcc 0.9300	BestValid 0.9323
	Epoch 1625:	Loss 0.4705	TrainAcc 0.9368	ValidAcc 0.9337	TestAcc 0.9316	BestValid 0.9337
	Epoch 1650:	Loss 0.4642	TrainAcc 0.9364	ValidAcc 0.9332	TestAcc 0.9310	BestValid 0.9337
	Epoch 1675:	Loss 0.4659	TrainAcc 0.9362	ValidAcc 0.9328	TestAcc 0.9304	BestValid 0.9337
	Epoch 1700:	Loss 0.4656	TrainAcc 0.9372	ValidAcc 0.9340	TestAcc 0.9319	BestValid 0.9340
	Epoch 1725:	Loss 0.4631	TrainAcc 0.9371	ValidAcc 0.9342	TestAcc 0.9318	BestValid 0.9342
	Epoch 1750:	Loss 0.4617	TrainAcc 0.9374	ValidAcc 0.9341	TestAcc 0.9320	BestValid 0.9342
	Epoch 1775:	Loss 0.4617	TrainAcc 0.9376	ValidAcc 0.9345	TestAcc 0.9321	BestValid 0.9345
	Epoch 1800:	Loss 0.4589	TrainAcc 0.9372	ValidAcc 0.9339	TestAcc 0.9317	BestValid 0.9345
	Epoch 1825:	Loss 0.4576	TrainAcc 0.9385	ValidAcc 0.9347	TestAcc 0.9328	BestValid 0.9347
	Epoch 1850:	Loss 0.4576	TrainAcc 0.9388	ValidAcc 0.9347	TestAcc 0.9331	BestValid 0.9347
	Epoch 1875:	Loss 0.4580	TrainAcc 0.9370	ValidAcc 0.9334	TestAcc 0.9312	BestValid 0.9347
	Epoch 1900:	Loss 0.4512	TrainAcc 0.9389	ValidAcc 0.9350	TestAcc 0.9332	BestValid 0.9350
	Epoch 1925:	Loss 0.4541	TrainAcc 0.9388	ValidAcc 0.9350	TestAcc 0.9328	BestValid 0.9350
	Epoch 1950:	Loss 0.4513	TrainAcc 0.9395	ValidAcc 0.9353	TestAcc 0.9336	BestValid 0.9353
	Epoch 1975:	Loss 0.4523	TrainAcc 0.9383	ValidAcc 0.9347	TestAcc 0.9320	BestValid 0.9353
	Epoch 2000:	Loss 0.4475	TrainAcc 0.9395	ValidAcc 0.9353	TestAcc 0.9332	BestValid 0.9353
	Epoch 2025:	Loss 0.4471	TrainAcc 0.9396	ValidAcc 0.9353	TestAcc 0.9338	BestValid 0.9353
	Epoch 2050:	Loss 0.4460	TrainAcc 0.9388	ValidAcc 0.9350	TestAcc 0.9322	BestValid 0.9353
	Epoch 2075:	Loss 0.4468	TrainAcc 0.9403	ValidAcc 0.9364	TestAcc 0.9343	BestValid 0.9364
	Epoch 2100:	Loss 0.4441	TrainAcc 0.9409	ValidAcc 0.9363	TestAcc 0.9345	BestValid 0.9364
	Epoch 2125:	Loss 0.4444	TrainAcc 0.9399	ValidAcc 0.9365	TestAcc 0.9332	BestValid 0.9365
	Epoch 2150:	Loss 0.4444	TrainAcc 0.9414	ValidAcc 0.9368	TestAcc 0.9349	BestValid 0.9368
	Epoch 2175:	Loss 0.4410	TrainAcc 0.9408	ValidAcc 0.9369	TestAcc 0.9342	BestValid 0.9369
	Epoch 2200:	Loss 0.4421	TrainAcc 0.9404	ValidAcc 0.9363	TestAcc 0.9336	BestValid 0.9369
	Epoch 2225:	Loss 0.4435	TrainAcc 0.9413	ValidAcc 0.9369	TestAcc 0.9345	BestValid 0.9369
	Epoch 2250:	Loss 0.4418	TrainAcc 0.9409	ValidAcc 0.9376	TestAcc 0.9343	BestValid 0.9376
	Epoch 2275:	Loss 0.4389	TrainAcc 0.9408	ValidAcc 0.9366	TestAcc 0.9339	BestValid 0.9376
	Epoch 2300:	Loss 0.4387	TrainAcc 0.9406	ValidAcc 0.9367	TestAcc 0.9337	BestValid 0.9376
	Epoch 2325:	Loss 0.4354	TrainAcc 0.9413	ValidAcc 0.9376	TestAcc 0.9347	BestValid 0.9376
	Epoch 2350:	Loss 0.4401	TrainAcc 0.9423	ValidAcc 0.9377	TestAcc 0.9357	BestValid 0.9377
	Epoch 2375:	Loss 0.4355	TrainAcc 0.9409	ValidAcc 0.9366	TestAcc 0.9338	BestValid 0.9377
	Epoch 2400:	Loss 0.4326	TrainAcc 0.9419	ValidAcc 0.9378	TestAcc 0.9350	BestValid 0.9378
	Epoch 2425:	Loss 0.4338	TrainAcc 0.9416	ValidAcc 0.9376	TestAcc 0.9351	BestValid 0.9378
	Epoch 2450:	Loss 0.4282	TrainAcc 0.9410	ValidAcc 0.9368	TestAcc 0.9345	BestValid 0.9378
	Epoch 2475:	Loss 0.4325	TrainAcc 0.9427	ValidAcc 0.9381	TestAcc 0.9356	BestValid 0.9381
	Epoch 2500:	Loss 0.4289	TrainAcc 0.9430	ValidAcc 0.9383	TestAcc 0.9363	BestValid 0.9383
	Epoch 2525:	Loss 0.4322	TrainAcc 0.9426	ValidAcc 0.9378	TestAcc 0.9359	BestValid 0.9383
	Epoch 2550:	Loss 0.4289	TrainAcc 0.9436	ValidAcc 0.9386	TestAcc 0.9368	BestValid 0.9386
	Epoch 2575:	Loss 0.4303	TrainAcc 0.9430	ValidAcc 0.9384	TestAcc 0.9364	BestValid 0.9386
	Epoch 2600:	Loss 0.4300	TrainAcc 0.9428	ValidAcc 0.9384	TestAcc 0.9356	BestValid 0.9386
	Epoch 2625:	Loss 0.4263	TrainAcc 0.9429	ValidAcc 0.9381	TestAcc 0.9359	BestValid 0.9386
	Epoch 2650:	Loss 0.4279	TrainAcc 0.9432	ValidAcc 0.9384	TestAcc 0.9360	BestValid 0.9386
	Epoch 2675:	Loss 0.4254	TrainAcc 0.9445	ValidAcc 0.9399	TestAcc 0.9373	BestValid 0.9399
	Epoch 2700:	Loss 0.4312	TrainAcc 0.9443	ValidAcc 0.9392	TestAcc 0.9372	BestValid 0.9399
	Epoch 2725:	Loss 0.4271	TrainAcc 0.9440	ValidAcc 0.9387	TestAcc 0.9368	BestValid 0.9399
	Epoch 2750:	Loss 0.4261	TrainAcc 0.9424	ValidAcc 0.9382	TestAcc 0.9354	BestValid 0.9399
	Epoch 2775:	Loss 0.4230	TrainAcc 0.9434	ValidAcc 0.9388	TestAcc 0.9363	BestValid 0.9399
	Epoch 2800:	Loss 0.4239	TrainAcc 0.9444	ValidAcc 0.9395	TestAcc 0.9369	BestValid 0.9399
	Epoch 2825:	Loss 0.4253	TrainAcc 0.9440	ValidAcc 0.9388	TestAcc 0.9367	BestValid 0.9399
	Epoch 2850:	Loss 0.4224	TrainAcc 0.9444	ValidAcc 0.9394	TestAcc 0.9373	BestValid 0.9399
	Epoch 2875:	Loss 0.4185	TrainAcc 0.9450	ValidAcc 0.9401	TestAcc 0.9378	BestValid 0.9401
	Epoch 2900:	Loss 0.4208	TrainAcc 0.9436	ValidAcc 0.9392	TestAcc 0.9365	BestValid 0.9401
	Epoch 2925:	Loss 0.4206	TrainAcc 0.9445	ValidAcc 0.9397	TestAcc 0.9374	BestValid 0.9401
	Epoch 2950:	Loss 0.4195	TrainAcc 0.9453	ValidAcc 0.9405	TestAcc 0.9375	BestValid 0.9405
	Epoch 2975:	Loss 0.4196	TrainAcc 0.9448	ValidAcc 0.9398	TestAcc 0.9375	BestValid 0.9405
	Epoch 3000:	Loss 0.4195	TrainAcc 0.9445	ValidAcc 0.9397	TestAcc 0.9367	BestValid 0.9405
	Epoch 3025:	Loss 0.4159	TrainAcc 0.9453	ValidAcc 0.9400	TestAcc 0.9382	BestValid 0.9405
	Epoch 3050:	Loss 0.4179	TrainAcc 0.9455	ValidAcc 0.9404	TestAcc 0.9383	BestValid 0.9405
	Epoch 3075:	Loss 0.4139	TrainAcc 0.9456	ValidAcc 0.9405	TestAcc 0.9382	BestValid 0.9405
	Epoch 3100:	Loss 0.4173	TrainAcc 0.9456	ValidAcc 0.9404	TestAcc 0.9381	BestValid 0.9405
	Epoch 3125:	Loss 0.4141	TrainAcc 0.9455	ValidAcc 0.9402	TestAcc 0.9378	BestValid 0.9405
	Epoch 3150:	Loss 0.4165	TrainAcc 0.9464	ValidAcc 0.9408	TestAcc 0.9387	BestValid 0.9408
	Epoch 3175:	Loss 0.4121	TrainAcc 0.9457	ValidAcc 0.9403	TestAcc 0.9383	BestValid 0.9408
	Epoch 3200:	Loss 0.4135	TrainAcc 0.9448	ValidAcc 0.9399	TestAcc 0.9375	BestValid 0.9408
	Epoch 3225:	Loss 0.4128	TrainAcc 0.9454	ValidAcc 0.9403	TestAcc 0.9380	BestValid 0.9408
	Epoch 3250:	Loss 0.4126	TrainAcc 0.9464	ValidAcc 0.9409	TestAcc 0.9385	BestValid 0.9409
	Epoch 3275:	Loss 0.4136	TrainAcc 0.9455	ValidAcc 0.9406	TestAcc 0.9378	BestValid 0.9409
	Epoch 3300:	Loss 0.4110	TrainAcc 0.9455	ValidAcc 0.9403	TestAcc 0.9381	BestValid 0.9409
	Epoch 3325:	Loss 0.4064	TrainAcc 0.9459	ValidAcc 0.9409	TestAcc 0.9385	BestValid 0.9409
	Epoch 3350:	Loss 0.4093	TrainAcc 0.9466	ValidAcc 0.9412	TestAcc 0.9386	BestValid 0.9412
	Epoch 3375:	Loss 0.4089	TrainAcc 0.9462	ValidAcc 0.9408	TestAcc 0.9386	BestValid 0.9412
	Epoch 3400:	Loss 0.4096	TrainAcc 0.9462	ValidAcc 0.9410	TestAcc 0.9381	BestValid 0.9412
	Epoch 3425:	Loss 0.4096	TrainAcc 0.9467	ValidAcc 0.9413	TestAcc 0.9386	BestValid 0.9413
	Epoch 3450:	Loss 0.4156	TrainAcc 0.9460	ValidAcc 0.9408	TestAcc 0.9382	BestValid 0.9413
	Epoch 3475:	Loss 0.4082	TrainAcc 0.9465	ValidAcc 0.9415	TestAcc 0.9391	BestValid 0.9415
	Epoch 3500:	Loss 0.4106	TrainAcc 0.9450	ValidAcc 0.9399	TestAcc 0.9374	BestValid 0.9415
	Epoch 3525:	Loss 0.4053	TrainAcc 0.9471	ValidAcc 0.9417	TestAcc 0.9395	BestValid 0.9417
	Epoch 3550:	Loss 0.4078	TrainAcc 0.9455	ValidAcc 0.9413	TestAcc 0.9379	BestValid 0.9417
	Epoch 3575:	Loss 0.4053	TrainAcc 0.9466	ValidAcc 0.9413	TestAcc 0.9387	BestValid 0.9417
	Epoch 3600:	Loss 0.4053	TrainAcc 0.9460	ValidAcc 0.9409	TestAcc 0.9380	BestValid 0.9417
	Epoch 3625:	Loss 0.4027	TrainAcc 0.9469	ValidAcc 0.9413	TestAcc 0.9390	BestValid 0.9417
	Epoch 3650:	Loss 0.4044	TrainAcc 0.9474	ValidAcc 0.9422	TestAcc 0.9393	BestValid 0.9422
	Epoch 3675:	Loss 0.4007	TrainAcc 0.9470	ValidAcc 0.9418	TestAcc 0.9391	BestValid 0.9422
	Epoch 3700:	Loss 0.3998	TrainAcc 0.9467	ValidAcc 0.9413	TestAcc 0.9388	BestValid 0.9422
	Epoch 3725:	Loss 0.4030	TrainAcc 0.9477	ValidAcc 0.9421	TestAcc 0.9392	BestValid 0.9422
	Epoch 3750:	Loss 0.4010	TrainAcc 0.9471	ValidAcc 0.9413	TestAcc 0.9389	BestValid 0.9422
	Epoch 3775:	Loss 0.3997	TrainAcc 0.9479	ValidAcc 0.9421	TestAcc 0.9399	BestValid 0.9422
	Epoch 3800:	Loss 0.4036	TrainAcc 0.9469	ValidAcc 0.9418	TestAcc 0.9387	BestValid 0.9422
	Epoch 3825:	Loss 0.4007	TrainAcc 0.9467	ValidAcc 0.9409	TestAcc 0.9384	BestValid 0.9422
	Epoch 3850:	Loss 0.4014	TrainAcc 0.9479	ValidAcc 0.9423	TestAcc 0.9398	BestValid 0.9423
	Epoch 3875:	Loss 0.3983	TrainAcc 0.9482	ValidAcc 0.9423	TestAcc 0.9400	BestValid 0.9423
	Epoch 3900:	Loss 0.3969	TrainAcc 0.9483	ValidAcc 0.9425	TestAcc 0.9401	BestValid 0.9425
	Epoch 3925:	Loss 0.3959	TrainAcc 0.9476	ValidAcc 0.9419	TestAcc 0.9394	BestValid 0.9425
	Epoch 3950:	Loss 0.3969	TrainAcc 0.9490	ValidAcc 0.9428	TestAcc 0.9409	BestValid 0.9428
	Epoch 3975:	Loss 0.3986	TrainAcc 0.9490	ValidAcc 0.9429	TestAcc 0.9407	BestValid 0.9429
	Epoch 4000:	Loss 0.3942	TrainAcc 0.9488	ValidAcc 0.9424	TestAcc 0.9405	BestValid 0.9429
	Epoch 4025:	Loss 0.3949	TrainAcc 0.9487	ValidAcc 0.9428	TestAcc 0.9407	BestValid 0.9429
	Epoch 4050:	Loss 0.3957	TrainAcc 0.9489	ValidAcc 0.9432	TestAcc 0.9404	BestValid 0.9432
	Epoch 4075:	Loss 0.3966	TrainAcc 0.9493	ValidAcc 0.9433	TestAcc 0.9413	BestValid 0.9433
	Epoch 4100:	Loss 0.3985	TrainAcc 0.9477	ValidAcc 0.9426	TestAcc 0.9397	BestValid 0.9433
	Epoch 4125:	Loss 0.3950	TrainAcc 0.9480	ValidAcc 0.9428	TestAcc 0.9398	BestValid 0.9433
	Epoch 4150:	Loss 0.3954	TrainAcc 0.9482	ValidAcc 0.9423	TestAcc 0.9398	BestValid 0.9433
	Epoch 4175:	Loss 0.3944	TrainAcc 0.9489	ValidAcc 0.9428	TestAcc 0.9406	BestValid 0.9433
	Epoch 4200:	Loss 0.3903	TrainAcc 0.9486	ValidAcc 0.9426	TestAcc 0.9404	BestValid 0.9433
	Epoch 4225:	Loss 0.3964	TrainAcc 0.9484	ValidAcc 0.9431	TestAcc 0.9405	BestValid 0.9433
	Epoch 4250:	Loss 0.3888	TrainAcc 0.9491	ValidAcc 0.9435	TestAcc 0.9410	BestValid 0.9435
	Epoch 4275:	Loss 0.3943	TrainAcc 0.9488	ValidAcc 0.9431	TestAcc 0.9407	BestValid 0.9435
	Epoch 4300:	Loss 0.3929	TrainAcc 0.9483	ValidAcc 0.9424	TestAcc 0.9402	BestValid 0.9435
	Epoch 4325:	Loss 0.3879	TrainAcc 0.9485	ValidAcc 0.9426	TestAcc 0.9402	BestValid 0.9435
	Epoch 4350:	Loss 0.3896	TrainAcc 0.9493	ValidAcc 0.9437	TestAcc 0.9411	BestValid 0.9437
	Epoch 4375:	Loss 0.3910	TrainAcc 0.9496	ValidAcc 0.9439	TestAcc 0.9412	BestValid 0.9439
	Epoch 4400:	Loss 0.3913	TrainAcc 0.9491	ValidAcc 0.9432	TestAcc 0.9413	BestValid 0.9439
	Epoch 4425:	Loss 0.3902	TrainAcc 0.9484	ValidAcc 0.9427	TestAcc 0.9399	BestValid 0.9439
	Epoch 4450:	Loss 0.3917	TrainAcc 0.9503	ValidAcc 0.9444	TestAcc 0.9424	BestValid 0.9444
	Epoch 4475:	Loss 0.3908	TrainAcc 0.9496	ValidAcc 0.9436	TestAcc 0.9414	BestValid 0.9444
	Epoch 4500:	Loss 0.3889	TrainAcc 0.9495	ValidAcc 0.9431	TestAcc 0.9408	BestValid 0.9444
	Epoch 4525:	Loss 0.3890	TrainAcc 0.9497	ValidAcc 0.9445	TestAcc 0.9415	BestValid 0.9445
	Epoch 4550:	Loss 0.3893	TrainAcc 0.9499	ValidAcc 0.9444	TestAcc 0.9421	BestValid 0.9445
	Epoch 4575:	Loss 0.3850	TrainAcc 0.9493	ValidAcc 0.9434	TestAcc 0.9410	BestValid 0.9445
	Epoch 4600:	Loss 0.3846	TrainAcc 0.9504	ValidAcc 0.9438	TestAcc 0.9425	BestValid 0.9445
	Epoch 4625:	Loss 0.3867	TrainAcc 0.9493	ValidAcc 0.9438	TestAcc 0.9413	BestValid 0.9445
	Epoch 4650:	Loss 0.3831	TrainAcc 0.9505	ValidAcc 0.9440	TestAcc 0.9426	BestValid 0.9445
	Epoch 4675:	Loss 0.3867	TrainAcc 0.9500	ValidAcc 0.9438	TestAcc 0.9419	BestValid 0.9445
	Epoch 4700:	Loss 0.3850	TrainAcc 0.9495	ValidAcc 0.9437	TestAcc 0.9420	BestValid 0.9445
	Epoch 4725:	Loss 0.3825	TrainAcc 0.9498	ValidAcc 0.9443	TestAcc 0.9416	BestValid 0.9445
	Epoch 4750:	Loss 0.3817	TrainAcc 0.9506	ValidAcc 0.9445	TestAcc 0.9427	BestValid 0.9445
	Epoch 4775:	Loss 0.3823	TrainAcc 0.9499	ValidAcc 0.9438	TestAcc 0.9419	BestValid 0.9445
	Epoch 4800:	Loss 0.3845	TrainAcc 0.9501	ValidAcc 0.9434	TestAcc 0.9426	BestValid 0.9445
	Epoch 4825:	Loss 0.3822	TrainAcc 0.9505	ValidAcc 0.9443	TestAcc 0.9431	BestValid 0.9445
	Epoch 4850:	Loss 0.3772	TrainAcc 0.9502	ValidAcc 0.9446	TestAcc 0.9427	BestValid 0.9446
	Epoch 4875:	Loss 0.3829	TrainAcc 0.9499	ValidAcc 0.9443	TestAcc 0.9422	BestValid 0.9446
	Epoch 4900:	Loss 0.3791	TrainAcc 0.9502	ValidAcc 0.9440	TestAcc 0.9428	BestValid 0.9446
	Epoch 4925:	Loss 0.3821	TrainAcc 0.9498	ValidAcc 0.9437	TestAcc 0.9417	BestValid 0.9446
	Epoch 4950:	Loss 0.3791	TrainAcc 0.9499	ValidAcc 0.9439	TestAcc 0.9422	BestValid 0.9446
	Epoch 4975:	Loss 0.3805	TrainAcc 0.9515	ValidAcc 0.9455	TestAcc 0.9437	BestValid 0.9455
	Epoch 5000:	Loss 0.3809	TrainAcc 0.9498	ValidAcc 0.9434	TestAcc 0.9422	BestValid 0.9455
Node 0, Pre/Post-Pipelining: 2.161 / 30.136 ms, Bubble: 67.934 ms, Compute: 337.858 ms, Comm: 30.353 ms, Imbalance: 7.646 ms
Node 1, Pre/Post-Pipelining: 2.160 / 30.101 ms, Bubble: 68.011 ms, Compute: 337.373 ms, Comm: 31.136 ms, Imbalance: 7.300 ms
Node 3, Pre/Post-Pipelining: 2.154 / 30.086 ms, Bubble: 67.377 ms, Compute: 318.213 ms, Comm: 36.227 ms, Imbalance: 22.935 ms
Node 2, Pre/Post-Pipelining: 2.157 / 30.111 ms, Bubble: 67.356 ms, Compute: 318.503 ms, Comm: 35.332 ms, Imbalance: 23.531 ms
Node 4, Pre/Post-Pipelining: 2.156 / 30.112 ms, Bubble: 69.160 ms, Compute: 316.787 ms, Comm: 33.549 ms, Imbalance: 26.479 ms
Node 5, Pre/Post-Pipelining: 2.154 / 30.058 ms, Bubble: 69.226 ms, Compute: 317.060 ms, Comm: 33.696 ms, Imbalance: 26.082 ms
Node 6, Pre/Post-Pipelining: 2.159 / 37.994 ms, Bubble: 62.836 ms, Compute: 331.659 ms, Comm: 28.100 ms, Imbalance: 14.799 ms
Node 7, Pre/Post-Pipelining: 2.158 / 37.843 ms, Bubble: 62.976 ms, Compute: 332.850 ms, Comm: 27.128 ms, Imbalance: 14.525 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 2.161 ms
Cluster-Wide Average, Post-Pipelining Overhead: 30.136 ms
Cluster-Wide Average, Bubble: 67.934 ms
Cluster-Wide Average, Compute: 337.858 ms
Cluster-Wide Average, Communication: 30.353 ms
Cluster-Wide Average, Imbalance: 7.646 ms
Node 0, GPU memory consumption: 10.143 GB
Node 1, GPU memory consumption: 8.962 GB
Node 3, GPU memory consumption: 7.893 GB
Node 2, GPU memory consumption: 7.915 GB
Node 6, GPU memory consumption: 8.067 GB
Node 5, GPU memory consumption: 7.917 GB
Node 7, GPU memory consumption: 8.046 GB
Node 4, GPU memory consumption: 7.891 GB
Node 0, Graph-Level Communication Throughput: 102.471 Gbps, Time: 54.683 ms
Node 1, Graph-Level Communication Throughput: 99.132 Gbps, Time: 58.350 ms
Node 2, Graph-Level Communication Throughput: 103.126 Gbps, Time: 54.336 ms
Node 3, Graph-Level Communication Throughput: 105.774 Gbps, Time: 54.685 ms
Node 4, Graph-Level Communication Throughput: 106.083 Gbps, Time: 52.821 ms
Node 5, Graph-Level Communication Throughput: 102.658 Gbps, Time: 56.346 ms
Node 6, Graph-Level Communication Throughput: 107.636 Gbps, Time: 52.059 ms
Node 7, Graph-Level Communication Throughput: 101.629 Gbps, Time: 56.916 ms
------------------------node id 0,  per-epoch time: 0.968365 s---------------
------------------------node id 1,  per-epoch time: 0.968365 s---------------
------------------------node id 4,  per-epoch time: 0.968365 s---------------
------------------------node id 2,  per-epoch time: 0.968365 s---------------
------------------------node id 5,  per-epoch time: 0.968365 s---------------
------------------------node id 3,  per-epoch time: 0.968365 s---------------
------------------------node id 6,  per-epoch time: 0.968365 s---------------
------------------------node id 7,  per-epoch time: 0.968365 s---------------
************ Profiling Results ************
	Bubble: 642.843362 (ms) (66.41 percentage)
	Compute: 254.028498 (ms) (26.24 percentage)
	GraphCommComputeOverhead: 9.739851 (ms) (1.01 percentage)
	GraphCommNetwork: 55.029042 (ms) (5.69 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.306890 (ms) (0.65 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.041 GB
	Graph-level communication (cluster-wide, per-epoch): 5.303 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 6.356 GB
	Aggregated layer-level communication throughput: 280.082 Gbps
Highest valid_acc: 0.9455
Target test_acc: 0.9437
Epoch to reach the target acc: 4974
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
