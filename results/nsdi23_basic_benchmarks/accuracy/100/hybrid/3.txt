Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.349 seconds.
Building the CSC structure...
        It takes 2.381 seconds.
Building the CSC structure...
        It takes 2.406 seconds.
Building the CSC structure...
        It takes 2.414 seconds.
Building the CSC structure...
        It takes 2.444 seconds.
Building the CSC structure...
        It takes 2.512 seconds.
Building the CSC structure...
        It takes 2.565 seconds.
Building the CSC structure...
        It takes 2.654 seconds.
Building the CSC structure...
        It takes 2.303 seconds.
        It takes 2.309 seconds.
        It takes 2.327 seconds.
        It takes 2.397 seconds.
        It takes 2.368 seconds.
        It takes 2.402 seconds.
        It takes 2.393 seconds.
        It takes 2.342 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.296 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.031 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 64.029 Gbps (per GPU), 512.233 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.992 Gbps (per GPU), 511.935 Gbps (aggregated)
The layer-level communication performance: 63.990 Gbps (per GPU), 511.919 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.957 Gbps (per GPU), 511.657 Gbps (aggregated)
The layer-level communication performance: 63.952 Gbps (per GPU), 511.613 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.923 Gbps (per GPU), 511.381 Gbps (aggregated)
The layer-level communication performance: 63.916 Gbps (per GPU), 511.325 Gbps (aggregated)
The layer-level communication performance: 63.911 Gbps (per GPU), 511.287 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.179 Gbps (per GPU), 1329.435 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.176 Gbps (per GPU), 1329.405 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.181 Gbps (per GPU), 1329.451 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.174 Gbps (per GPU), 1329.392 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.176 Gbps (per GPU), 1329.412 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.174 Gbps (per GPU), 1329.395 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.176 Gbps (per GPU), 1329.408 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.174 Gbps (per GPU), 1329.392 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.530 Gbps (per GPU), 916.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.530 Gbps (per GPU), 916.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.530 Gbps (per GPU), 916.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.528 Gbps (per GPU), 916.220 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.530 Gbps (per GPU), 916.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.530 Gbps (per GPU), 916.237 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.529 Gbps (per GPU), 916.234 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.530 Gbps (per GPU), 916.237 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.674 Gbps (per GPU), 365.393 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.675 Gbps (per GPU), 365.398 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.674 Gbps (per GPU), 365.394 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.674 Gbps (per GPU), 365.394 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.674 Gbps (per GPU), 365.393 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.674 Gbps (per GPU), 365.393 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.674 Gbps (per GPU), 365.394 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.674 Gbps (per GPU), 365.393 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.47ms  2.77ms  3.11  8.38K  3.53M
 chk_1  0.75ms  2.80ms  2.97ms  3.96  6.74K  3.60M
 chk_2  0.80ms  2.72ms  2.87ms  3.60  7.27K  3.53M
 chk_3  0.80ms  2.75ms  2.89ms  3.61  7.92K  3.61M
 chk_4  0.63ms  2.67ms  2.82ms  4.49  5.33K  3.68M
 chk_5  1.01ms  2.65ms  2.83ms  2.81 10.07K  3.45M
 chk_6  0.96ms  2.81ms  2.99ms  3.11  9.41K  3.48M
 chk_7  0.82ms  2.66ms  2.84ms  3.44  8.12K  3.60M
 chk_8  0.70ms  2.78ms  2.92ms  4.14  6.09K  3.64M
 chk_9  1.11ms  2.58ms  2.78ms  2.51 11.10K  3.38M
chk_10  0.65ms  2.79ms  2.96ms  4.53  5.67K  3.63M
chk_11  0.82ms  2.67ms  2.83ms  3.43  8.16K  3.54M
chk_12  0.80ms  2.85ms  3.02ms  3.78  7.24K  3.55M
chk_13  0.64ms  2.71ms  2.86ms  4.46  5.41K  3.68M
chk_14  0.78ms  2.93ms  3.11ms  3.96  7.14K  3.53M
chk_15  0.95ms  2.79ms  2.97ms  3.11  9.25K  3.49M
chk_16  0.60ms  2.64ms  2.77ms  4.63  4.78K  3.77M
chk_17  0.76ms  2.73ms  2.90ms  3.80  6.85K  3.60M
chk_18  0.83ms  2.56ms  2.73ms  3.29  7.47K  3.57M
chk_19  0.61ms  2.62ms  2.78ms  4.58  4.88K  3.75M
chk_20  0.77ms  2.63ms  2.80ms  3.64  7.00K  3.63M
chk_21  0.63ms  2.60ms  2.76ms  4.36  5.41K  3.68M
chk_22  1.10ms  2.83ms  3.03ms  2.75 11.07K  3.39M
chk_23  0.79ms  2.73ms  2.88ms  3.63  7.23K  3.64M
chk_24  1.01ms  2.79ms  2.95ms  2.92 10.13K  3.43M
chk_25  0.73ms  2.60ms  2.74ms  3.77  6.40K  3.57M
chk_26  0.66ms  2.78ms  2.97ms  4.50  5.78K  3.55M
chk_27  0.96ms  2.67ms  2.92ms  3.05  9.34K  3.48M
chk_28  0.73ms  2.94ms  3.24ms  4.46  6.37K  3.57M
chk_29  0.63ms  2.77ms  2.92ms  4.65  5.16K  3.78M
chk_30  0.64ms  2.68ms  2.82ms  4.43  5.44K  3.67M
chk_31  0.72ms  2.81ms  2.96ms  4.10  6.33K  3.63M
   Avg  0.79  2.72  2.89
   Max  1.11  2.94  3.24
   Min  0.60  2.47  2.73
 Ratio  1.85  1.19  1.18
   Var  0.02  0.01  0.01
Profiling takes 2.451 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 373.287 ms
Partition 0 [0, 5) has cost: 373.287 ms
Partition 1 [5, 9) has cost: 347.994 ms
Partition 2 [9, 13) has cost: 347.994 ms
Partition 3 [13, 17) has cost: 347.994 ms
Partition 4 [17, 21) has cost: 347.994 ms
Partition 5 [21, 25) has cost: 347.994 ms
Partition 6 [25, 29) has cost: 347.994 ms
Partition 7 [29, 33) has cost: 353.601 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.749 ms
GPU 0, Compute+Comm Time: 134.103 ms, Bubble Time: 29.178 ms, Imbalance Overhead: 2.469 ms
GPU 1, Compute+Comm Time: 127.040 ms, Bubble Time: 28.859 ms, Imbalance Overhead: 9.850 ms
GPU 2, Compute+Comm Time: 127.040 ms, Bubble Time: 28.877 ms, Imbalance Overhead: 9.833 ms
GPU 3, Compute+Comm Time: 127.040 ms, Bubble Time: 28.855 ms, Imbalance Overhead: 9.855 ms
GPU 4, Compute+Comm Time: 127.040 ms, Bubble Time: 28.742 ms, Imbalance Overhead: 9.968 ms
GPU 5, Compute+Comm Time: 127.040 ms, Bubble Time: 28.732 ms, Imbalance Overhead: 9.978 ms
GPU 6, Compute+Comm Time: 127.040 ms, Bubble Time: 28.953 ms, Imbalance Overhead: 9.757 ms
GPU 7, Compute+Comm Time: 128.353 ms, Bubble Time: 29.323 ms, Imbalance Overhead: 8.073 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 322.503 ms
GPU 0, Compute+Comm Time: 248.534 ms, Bubble Time: 57.533 ms, Imbalance Overhead: 16.437 ms
GPU 1, Compute+Comm Time: 244.241 ms, Bubble Time: 56.760 ms, Imbalance Overhead: 21.503 ms
GPU 2, Compute+Comm Time: 244.241 ms, Bubble Time: 56.224 ms, Imbalance Overhead: 22.038 ms
GPU 3, Compute+Comm Time: 244.241 ms, Bubble Time: 56.196 ms, Imbalance Overhead: 22.067 ms
GPU 4, Compute+Comm Time: 244.241 ms, Bubble Time: 56.183 ms, Imbalance Overhead: 22.080 ms
GPU 5, Compute+Comm Time: 244.241 ms, Bubble Time: 56.134 ms, Imbalance Overhead: 22.129 ms
GPU 6, Compute+Comm Time: 244.241 ms, Bubble Time: 55.913 ms, Imbalance Overhead: 22.350 ms
GPU 7, Compute+Comm Time: 262.470 ms, Bubble Time: 56.371 ms, Imbalance Overhead: 3.662 ms
The estimated cost of the whole pipeline: 512.665 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 721.282 ms
Partition 0 [0, 9) has cost: 721.282 ms
Partition 1 [9, 17) has cost: 695.989 ms
Partition 2 [17, 25) has cost: 695.989 ms
Partition 3 [25, 33) has cost: 701.595 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.964 ms
GPU 0, Compute+Comm Time: 144.139 ms, Bubble Time: 26.485 ms, Imbalance Overhead: 3.339 ms
GPU 1, Compute+Comm Time: 140.227 ms, Bubble Time: 26.591 ms, Imbalance Overhead: 7.145 ms
GPU 2, Compute+Comm Time: 140.227 ms, Bubble Time: 26.457 ms, Imbalance Overhead: 7.280 ms
GPU 3, Compute+Comm Time: 140.921 ms, Bubble Time: 26.842 ms, Imbalance Overhead: 6.201 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 324.544 ms
GPU 0, Compute+Comm Time: 262.237 ms, Bubble Time: 50.217 ms, Imbalance Overhead: 12.090 ms
GPU 1, Compute+Comm Time: 260.104 ms, Bubble Time: 49.901 ms, Imbalance Overhead: 14.539 ms
GPU 2, Compute+Comm Time: 260.104 ms, Bubble Time: 50.162 ms, Imbalance Overhead: 14.278 ms
GPU 3, Compute+Comm Time: 270.427 ms, Bubble Time: 49.859 ms, Imbalance Overhead: 4.258 ms
    The estimated cost with 2 DP ways is 523.433 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1417.270 ms
Partition 0 [0, 17) has cost: 1417.270 ms
Partition 1 [17, 33) has cost: 1397.584 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 209.547 ms
GPU 0, Compute+Comm Time: 183.074 ms, Bubble Time: 21.172 ms, Imbalance Overhead: 5.302 ms
GPU 1, Compute+Comm Time: 181.436 ms, Bubble Time: 22.116 ms, Imbalance Overhead: 5.996 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 350.623 ms
GPU 0, Compute+Comm Time: 304.473 ms, Bubble Time: 37.807 ms, Imbalance Overhead: 8.344 ms
GPU 1, Compute+Comm Time: 308.921 ms, Bubble Time: 36.968 ms, Imbalance Overhead: 4.735 ms
    The estimated cost with 4 DP ways is 588.179 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2814.854 ms
Partition 0 [0, 33) has cost: 2814.854 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 455.606 ms
GPU 0, Compute+Comm Time: 455.606 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 582.930 ms
GPU 0, Compute+Comm Time: 582.930 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1090.463 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 4, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5185	TrainAcc 0.4250	ValidAcc 0.4428	TestAcc 0.4430	BestValid 0.4428
	Epoch 50:	Loss 1.8661	TrainAcc 0.7043	ValidAcc 0.7198	TestAcc 0.7175	BestValid 0.7198
	Epoch 75:	Loss 1.4976	TrainAcc 0.7601	ValidAcc 0.7727	TestAcc 0.7684	BestValid 0.7727
	Epoch 100:	Loss 1.2765	TrainAcc 0.7885	ValidAcc 0.7990	TestAcc 0.7951	BestValid 0.7990
	Epoch 125:	Loss 1.1045	TrainAcc 0.8177	ValidAcc 0.8272	TestAcc 0.8226	BestValid 0.8272
	Epoch 150:	Loss 1.0120	TrainAcc 0.8456	ValidAcc 0.8533	TestAcc 0.8489	BestValid 0.8533
	Epoch 175:	Loss 0.9380	TrainAcc 0.8587	ValidAcc 0.8662	TestAcc 0.8606	BestValid 0.8662
	Epoch 200:	Loss 0.8849	TrainAcc 0.8670	ValidAcc 0.8738	TestAcc 0.8682	BestValid 0.8738
	Epoch 225:	Loss 0.8371	TrainAcc 0.8749	ValidAcc 0.8823	TestAcc 0.8757	BestValid 0.8823
	Epoch 250:	Loss 0.8082	TrainAcc 0.8790	ValidAcc 0.8857	TestAcc 0.8801	BestValid 0.8857
	Epoch 275:	Loss 0.7765	TrainAcc 0.8853	ValidAcc 0.8908	TestAcc 0.8874	BestValid 0.8908
	Epoch 300:	Loss 0.7553	TrainAcc 0.8885	ValidAcc 0.8940	TestAcc 0.8901	BestValid 0.8940
	Epoch 325:	Loss 0.7272	TrainAcc 0.8922	ValidAcc 0.8976	TestAcc 0.8931	BestValid 0.8976
	Epoch 350:	Loss 0.7092	TrainAcc 0.8942	ValidAcc 0.8996	TestAcc 0.8951	BestValid 0.8996
	Epoch 375:	Loss 0.6906	TrainAcc 0.8968	ValidAcc 0.9020	TestAcc 0.8969	BestValid 0.9020
	Epoch 400:	Loss 0.6764	TrainAcc 0.9002	ValidAcc 0.9053	TestAcc 0.8999	BestValid 0.9053
	Epoch 425:	Loss 0.6606	TrainAcc 0.9024	ValidAcc 0.9072	TestAcc 0.9018	BestValid 0.9072
	Epoch 450:	Loss 0.6498	TrainAcc 0.9023	ValidAcc 0.9068	TestAcc 0.9011	BestValid 0.9072
	Epoch 475:	Loss 0.6413	TrainAcc 0.9064	ValidAcc 0.9100	TestAcc 0.9049	BestValid 0.9100
	Epoch 500:	Loss 0.6286	TrainAcc 0.9075	ValidAcc 0.9108	TestAcc 0.9058	BestValid 0.9108
	Epoch 525:	Loss 0.6227	TrainAcc 0.9097	ValidAcc 0.9131	TestAcc 0.9079	BestValid 0.9131
	Epoch 550:	Loss 0.6151	TrainAcc 0.9110	ValidAcc 0.9142	TestAcc 0.9092	BestValid 0.9142
	Epoch 575:	Loss 0.6057	TrainAcc 0.9126	ValidAcc 0.9156	TestAcc 0.9102	BestValid 0.9156
	Epoch 600:	Loss 0.5957	TrainAcc 0.9142	ValidAcc 0.9173	TestAcc 0.9121	BestValid 0.9173
	Epoch 625:	Loss 0.5917	TrainAcc 0.9146	ValidAcc 0.9175	TestAcc 0.9124	BestValid 0.9175
	Epoch 650:	Loss 0.5868	TrainAcc 0.9158	ValidAcc 0.9189	TestAcc 0.9133	BestValid 0.9189
	Epoch 675:	Loss 0.5787	TrainAcc 0.9163	ValidAcc 0.9193	TestAcc 0.9137	BestValid 0.9193
	Epoch 700:	Loss 0.5713	TrainAcc 0.9181	ValidAcc 0.9204	TestAcc 0.9151	BestValid 0.9204
	Epoch 725:	Loss 0.5668	TrainAcc 0.9188	ValidAcc 0.9209	TestAcc 0.9158	BestValid 0.9209
	Epoch 750:	Loss 0.5633	TrainAcc 0.9200	ValidAcc 0.9221	TestAcc 0.9171	BestValid 0.9221
	Epoch 775:	Loss 0.5584	TrainAcc 0.9197	ValidAcc 0.9222	TestAcc 0.9168	BestValid 0.9222
	Epoch 800:	Loss 0.5545	TrainAcc 0.9201	ValidAcc 0.9217	TestAcc 0.9170	BestValid 0.9222
	Epoch 825:	Loss 0.5472	TrainAcc 0.9216	ValidAcc 0.9234	TestAcc 0.9181	BestValid 0.9234
	Epoch 850:	Loss 0.5439	TrainAcc 0.9221	ValidAcc 0.9241	TestAcc 0.9188	BestValid 0.9241
	Epoch 875:	Loss 0.5415	TrainAcc 0.9233	ValidAcc 0.9253	TestAcc 0.9199	BestValid 0.9253
	Epoch 900:	Loss 0.5345	TrainAcc 0.9240	ValidAcc 0.9256	TestAcc 0.9204	BestValid 0.9256
	Epoch 925:	Loss 0.5310	TrainAcc 0.9240	ValidAcc 0.9252	TestAcc 0.9203	BestValid 0.9256
	Epoch 950:	Loss 0.5331	TrainAcc 0.9255	ValidAcc 0.9259	TestAcc 0.9215	BestValid 0.9259
	Epoch 975:	Loss 0.5274	TrainAcc 0.9258	ValidAcc 0.9270	TestAcc 0.9220	BestValid 0.9270
	Epoch 1000:	Loss 0.5239	TrainAcc 0.9267	ValidAcc 0.9274	TestAcc 0.9227	BestValid 0.9274
	Epoch 1025:	Loss 0.5216	TrainAcc 0.9269	ValidAcc 0.9271	TestAcc 0.9227	BestValid 0.9274
	Epoch 1050:	Loss 0.5204	TrainAcc 0.9274	ValidAcc 0.9273	TestAcc 0.9232	BestValid 0.9274
	Epoch 1075:	Loss 0.5117	TrainAcc 0.9273	ValidAcc 0.9270	TestAcc 0.9230	BestValid 0.9274
	Epoch 1100:	Loss 0.5100	TrainAcc 0.9281	ValidAcc 0.9279	TestAcc 0.9241	BestValid 0.9279
	Epoch 1125:	Loss 0.5113	TrainAcc 0.9279	ValidAcc 0.9270	TestAcc 0.9240	BestValid 0.9279
	Epoch 1150:	Loss 0.5052	TrainAcc 0.9291	ValidAcc 0.9287	TestAcc 0.9250	BestValid 0.9287
	Epoch 1175:	Loss 0.5067	TrainAcc 0.9288	ValidAcc 0.9280	TestAcc 0.9244	BestValid 0.9287
	Epoch 1200:	Loss 0.4999	TrainAcc 0.9307	ValidAcc 0.9296	TestAcc 0.9262	BestValid 0.9296
	Epoch 1225:	Loss 0.5006	TrainAcc 0.9294	ValidAcc 0.9284	TestAcc 0.9252	BestValid 0.9296
	Epoch 1250:	Loss 0.4990	TrainAcc 0.9295	ValidAcc 0.9282	TestAcc 0.9254	BestValid 0.9296
	Epoch 1275:	Loss 0.4942	TrainAcc 0.9314	ValidAcc 0.9304	TestAcc 0.9269	BestValid 0.9304
	Epoch 1300:	Loss 0.4936	TrainAcc 0.9312	ValidAcc 0.9293	TestAcc 0.9266	BestValid 0.9304
	Epoch 1325:	Loss 0.4912	TrainAcc 0.9317	ValidAcc 0.9302	TestAcc 0.9272	BestValid 0.9304
	Epoch 1350:	Loss 0.4891	TrainAcc 0.9322	ValidAcc 0.9306	TestAcc 0.9275	BestValid 0.9306
	Epoch 1375:	Loss 0.4901	TrainAcc 0.9325	ValidAcc 0.9310	TestAcc 0.9275	BestValid 0.9310
	Epoch 1400:	Loss 0.4854	TrainAcc 0.9323	ValidAcc 0.9298	TestAcc 0.9273	BestValid 0.9310
	Epoch 1425:	Loss 0.4820	TrainAcc 0.9328	ValidAcc 0.9306	TestAcc 0.9278	BestValid 0.9310
	Epoch 1450:	Loss 0.4818	TrainAcc 0.9327	ValidAcc 0.9312	TestAcc 0.9277	BestValid 0.9312
	Epoch 1475:	Loss 0.4810	TrainAcc 0.9335	ValidAcc 0.9313	TestAcc 0.9285	BestValid 0.9313
	Epoch 1500:	Loss 0.4811	TrainAcc 0.9343	ValidAcc 0.9324	TestAcc 0.9291	BestValid 0.9324
	Epoch 1525:	Loss 0.4765	TrainAcc 0.9344	ValidAcc 0.9320	TestAcc 0.9292	BestValid 0.9324
	Epoch 1550:	Loss 0.4766	TrainAcc 0.9352	ValidAcc 0.9329	TestAcc 0.9297	BestValid 0.9329
	Epoch 1575:	Loss 0.4738	TrainAcc 0.9342	ValidAcc 0.9319	TestAcc 0.9289	BestValid 0.9329
	Epoch 1600:	Loss 0.4713	TrainAcc 0.9360	ValidAcc 0.9336	TestAcc 0.9309	BestValid 0.9336
	Epoch 1625:	Loss 0.4699	TrainAcc 0.9347	ValidAcc 0.9321	TestAcc 0.9291	BestValid 0.9336
	Epoch 1650:	Loss 0.4706	TrainAcc 0.9352	ValidAcc 0.9326	TestAcc 0.9296	BestValid 0.9336
	Epoch 1675:	Loss 0.4647	TrainAcc 0.9343	ValidAcc 0.9318	TestAcc 0.9294	BestValid 0.9336
	Epoch 1700:	Loss 0.4673	TrainAcc 0.9362	ValidAcc 0.9331	TestAcc 0.9306	BestValid 0.9336
	Epoch 1725:	Loss 0.4643	TrainAcc 0.9354	ValidAcc 0.9329	TestAcc 0.9301	BestValid 0.9336
	Epoch 1750:	Loss 0.4629	TrainAcc 0.9368	ValidAcc 0.9345	TestAcc 0.9312	BestValid 0.9345
	Epoch 1775:	Loss 0.4603	TrainAcc 0.9365	ValidAcc 0.9337	TestAcc 0.9310	BestValid 0.9345
	Epoch 1800:	Loss 0.4612	TrainAcc 0.9371	ValidAcc 0.9340	TestAcc 0.9314	BestValid 0.9345
	Epoch 1825:	Loss 0.4628	TrainAcc 0.9373	ValidAcc 0.9344	TestAcc 0.9317	BestValid 0.9345
	Epoch 1850:	Loss 0.4587	TrainAcc 0.9368	ValidAcc 0.9340	TestAcc 0.9310	BestValid 0.9345
	Epoch 1875:	Loss 0.4601	TrainAcc 0.9370	ValidAcc 0.9336	TestAcc 0.9310	BestValid 0.9345
	Epoch 1900:	Loss 0.4581	TrainAcc 0.9367	ValidAcc 0.9334	TestAcc 0.9309	BestValid 0.9345
	Epoch 1925:	Loss 0.4532	TrainAcc 0.9378	ValidAcc 0.9342	TestAcc 0.9317	BestValid 0.9345
	Epoch 1950:	Loss 0.4534	TrainAcc 0.9378	ValidAcc 0.9345	TestAcc 0.9316	BestValid 0.9345
	Epoch 1975:	Loss 0.4508	TrainAcc 0.9377	ValidAcc 0.9340	TestAcc 0.9313	BestValid 0.9345
	Epoch 2000:	Loss 0.4508	TrainAcc 0.9379	ValidAcc 0.9343	TestAcc 0.9316	BestValid 0.9345
	Epoch 2025:	Loss 0.4512	TrainAcc 0.9384	ValidAcc 0.9348	TestAcc 0.9323	BestValid 0.9348
	Epoch 2050:	Loss 0.4519	TrainAcc 0.9371	ValidAcc 0.9335	TestAcc 0.9312	BestValid 0.9348
	Epoch 2075:	Loss 0.4486	TrainAcc 0.9387	ValidAcc 0.9351	TestAcc 0.9331	BestValid 0.9351
	Epoch 2100:	Loss 0.4467	TrainAcc 0.9388	ValidAcc 0.9349	TestAcc 0.9326	BestValid 0.9351
	Epoch 2125:	Loss 0.4481	TrainAcc 0.9394	ValidAcc 0.9362	TestAcc 0.9331	BestValid 0.9362
	Epoch 2150:	Loss 0.4471	TrainAcc 0.9392	ValidAcc 0.9355	TestAcc 0.9332	BestValid 0.9362
	Epoch 2175:	Loss 0.4436	TrainAcc 0.9397	ValidAcc 0.9354	TestAcc 0.9329	BestValid 0.9362
	Epoch 2200:	Loss 0.4433	TrainAcc 0.9400	ValidAcc 0.9366	TestAcc 0.9335	BestValid 0.9366
	Epoch 2225:	Loss 0.4424	TrainAcc 0.9391	ValidAcc 0.9355	TestAcc 0.9330	BestValid 0.9366
	Epoch 2250:	Loss 0.4435	TrainAcc 0.9394	ValidAcc 0.9357	TestAcc 0.9326	BestValid 0.9366
	Epoch 2275:	Loss 0.4415	TrainAcc 0.9410	ValidAcc 0.9371	TestAcc 0.9341	BestValid 0.9371
	Epoch 2300:	Loss 0.4398	TrainAcc 0.9401	ValidAcc 0.9367	TestAcc 0.9334	BestValid 0.9371
	Epoch 2325:	Loss 0.4395	TrainAcc 0.9403	ValidAcc 0.9363	TestAcc 0.9341	BestValid 0.9371
	Epoch 2350:	Loss 0.4389	TrainAcc 0.9401	ValidAcc 0.9358	TestAcc 0.9335	BestValid 0.9371
	Epoch 2375:	Loss 0.4395	TrainAcc 0.9407	ValidAcc 0.9366	TestAcc 0.9338	BestValid 0.9371
	Epoch 2400:	Loss 0.4390	TrainAcc 0.9408	ValidAcc 0.9363	TestAcc 0.9341	BestValid 0.9371
	Epoch 2425:	Loss 0.4365	TrainAcc 0.9417	ValidAcc 0.9375	TestAcc 0.9346	BestValid 0.9375
	Epoch 2450:	Loss 0.4393	TrainAcc 0.9406	ValidAcc 0.9363	TestAcc 0.9339	BestValid 0.9375
	Epoch 2475:	Loss 0.4351	TrainAcc 0.9407	ValidAcc 0.9367	TestAcc 0.9337	BestValid 0.9375
	Epoch 2500:	Loss 0.4331	TrainAcc 0.9401	ValidAcc 0.9361	TestAcc 0.9334	BestValid 0.9375
	Epoch 2525:	Loss 0.4334	TrainAcc 0.9413	ValidAcc 0.9370	TestAcc 0.9345	BestValid 0.9375
	Epoch 2550:	Loss 0.4336	TrainAcc 0.9411	ValidAcc 0.9373	TestAcc 0.9341	BestValid 0.9375
	Epoch 2575:	Loss 0.4345	TrainAcc 0.9414	ValidAcc 0.9376	TestAcc 0.9344	BestValid 0.9376
	Epoch 2600:	Loss 0.4322	TrainAcc 0.9414	ValidAcc 0.9370	TestAcc 0.9341	BestValid 0.9376
	Epoch 2625:	Loss 0.4306	TrainAcc 0.9408	ValidAcc 0.9365	TestAcc 0.9336	BestValid 0.9376
	Epoch 2650:	Loss 0.4314	TrainAcc 0.9415	ValidAcc 0.9367	TestAcc 0.9345	BestValid 0.9376
	Epoch 2675:	Loss 0.4286	TrainAcc 0.9429	ValidAcc 0.9389	TestAcc 0.9360	BestValid 0.9389
	Epoch 2700:	Loss 0.4296	TrainAcc 0.9418	ValidAcc 0.9376	TestAcc 0.9342	BestValid 0.9389
	Epoch 2725:	Loss 0.4273	TrainAcc 0.9430	ValidAcc 0.9392	TestAcc 0.9358	BestValid 0.9392
	Epoch 2750:	Loss 0.4317	TrainAcc 0.9426	ValidAcc 0.9376	TestAcc 0.9351	BestValid 0.9392
	Epoch 2775:	Loss 0.4310	TrainAcc 0.9420	ValidAcc 0.9371	TestAcc 0.9343	BestValid 0.9392
	Epoch 2800:	Loss 0.4240	TrainAcc 0.9426	ValidAcc 0.9381	TestAcc 0.9349	BestValid 0.9392
	Epoch 2825:	Loss 0.4229	TrainAcc 0.9428	ValidAcc 0.9387	TestAcc 0.9352	BestValid 0.9392
	Epoch 2850:	Loss 0.4272	TrainAcc 0.9429	ValidAcc 0.9386	TestAcc 0.9355	BestValid 0.9392
	Epoch 2875:	Loss 0.4223	TrainAcc 0.9434	ValidAcc 0.9384	TestAcc 0.9357	BestValid 0.9392
	Epoch 2900:	Loss 0.4215	TrainAcc 0.9419	ValidAcc 0.9373	TestAcc 0.9346	BestValid 0.9392
	Epoch 2925:	Loss 0.4232	TrainAcc 0.9421	ValidAcc 0.9375	TestAcc 0.9343	BestValid 0.9392
	Epoch 2950:	Loss 0.4208	TrainAcc 0.9439	ValidAcc 0.9388	TestAcc 0.9361	BestValid 0.9392
	Epoch 2975:	Loss 0.4205	TrainAcc 0.9444	ValidAcc 0.9394	TestAcc 0.9366	BestValid 0.9394
	Epoch 3000:	Loss 0.4252	TrainAcc 0.9436	ValidAcc 0.9390	TestAcc 0.9357	BestValid 0.9394
	Epoch 3025:	Loss 0.4199	TrainAcc 0.9425	ValidAcc 0.9375	TestAcc 0.9350	BestValid 0.9394
	Epoch 3050:	Loss 0.4198	TrainAcc 0.9442	ValidAcc 0.9391	TestAcc 0.9368	BestValid 0.9394
	Epoch 3075:	Loss 0.4177	TrainAcc 0.9435	ValidAcc 0.9391	TestAcc 0.9364	BestValid 0.9394
	Epoch 3100:	Loss 0.4161	TrainAcc 0.9431	ValidAcc 0.9375	TestAcc 0.9353	BestValid 0.9394
	Epoch 3125:	Loss 0.4185	TrainAcc 0.9440	ValidAcc 0.9392	TestAcc 0.9365	BestValid 0.9394
	Epoch 3150:	Loss 0.4165	TrainAcc 0.9429	ValidAcc 0.9385	TestAcc 0.9347	BestValid 0.9394
	Epoch 3175:	Loss 0.4160	TrainAcc 0.9436	ValidAcc 0.9389	TestAcc 0.9359	BestValid 0.9394
	Epoch 3200:	Loss 0.4177	TrainAcc 0.9433	ValidAcc 0.9380	TestAcc 0.9351	BestValid 0.9394
	Epoch 3225:	Loss 0.4173	TrainAcc 0.9434	ValidAcc 0.9392	TestAcc 0.9359	BestValid 0.9394
	Epoch 3250:	Loss 0.4128	TrainAcc 0.9449	ValidAcc 0.9396	TestAcc 0.9369	BestValid 0.9396
	Epoch 3275:	Loss 0.4195	TrainAcc 0.9449	ValidAcc 0.9397	TestAcc 0.9371	BestValid 0.9397
	Epoch 3300:	Loss 0.4119	TrainAcc 0.9440	ValidAcc 0.9389	TestAcc 0.9361	BestValid 0.9397
	Epoch 3325:	Loss 0.4139	TrainAcc 0.9441	ValidAcc 0.9385	TestAcc 0.9359	BestValid 0.9397
	Epoch 3350:	Loss 0.4161	TrainAcc 0.9445	ValidAcc 0.9391	TestAcc 0.9365	BestValid 0.9397
	Epoch 3375:	Loss 0.4155	TrainAcc 0.9448	ValidAcc 0.9394	TestAcc 0.9371	BestValid 0.9397
	Epoch 3400:	Loss 0.4108	TrainAcc 0.9451	ValidAcc 0.9396	TestAcc 0.9371	BestValid 0.9397
	Epoch 3425:	Loss 0.4098	TrainAcc 0.9446	ValidAcc 0.9392	TestAcc 0.9371	BestValid 0.9397
	Epoch 3450:	Loss 0.4115	TrainAcc 0.9455	ValidAcc 0.9405	TestAcc 0.9377	BestValid 0.9405
	Epoch 3475:	Loss 0.4105	TrainAcc 0.9452	ValidAcc 0.9398	TestAcc 0.9374	BestValid 0.9405
	Epoch 3500:	Loss 0.4092	TrainAcc 0.9455	ValidAcc 0.9402	TestAcc 0.9379	BestValid 0.9405
	Epoch 3525:	Loss 0.4107	TrainAcc 0.9451	ValidAcc 0.9398	TestAcc 0.9377	BestValid 0.9405
	Epoch 3550:	Loss 0.4088	TrainAcc 0.9458	ValidAcc 0.9410	TestAcc 0.9383	BestValid 0.9410
	Epoch 3575:	Loss 0.4110	TrainAcc 0.9461	ValidAcc 0.9416	TestAcc 0.9385	BestValid 0.9416
	Epoch 3600:	Loss 0.4044	TrainAcc 0.9451	ValidAcc 0.9398	TestAcc 0.9370	BestValid 0.9416
	Epoch 3625:	Loss 0.4112	TrainAcc 0.9461	ValidAcc 0.9404	TestAcc 0.9383	BestValid 0.9416
	Epoch 3650:	Loss 0.4084	TrainAcc 0.9468	ValidAcc 0.9411	TestAcc 0.9394	BestValid 0.9416
	Epoch 3675:	Loss 0.4087	TrainAcc 0.9464	ValidAcc 0.9414	TestAcc 0.9384	BestValid 0.9416
	Epoch 3700:	Loss 0.4061	TrainAcc 0.9457	ValidAcc 0.9402	TestAcc 0.9379	BestValid 0.9416
	Epoch 3725:	Loss 0.4052	TrainAcc 0.9445	ValidAcc 0.9392	TestAcc 0.9368	BestValid 0.9416
	Epoch 3750:	Loss 0.4066	TrainAcc 0.9462	ValidAcc 0.9411	TestAcc 0.9382	BestValid 0.9416
	Epoch 3775:	Loss 0.4062	TrainAcc 0.9474	ValidAcc 0.9418	TestAcc 0.9394	BestValid 0.9418
	Epoch 3800:	Loss 0.4011	TrainAcc 0.9460	ValidAcc 0.9408	TestAcc 0.9376	BestValid 0.9418
	Epoch 3825:	Loss 0.4038	TrainAcc 0.9463	ValidAcc 0.9407	TestAcc 0.9380	BestValid 0.9418
	Epoch 3850:	Loss 0.4056	TrainAcc 0.9473	ValidAcc 0.9415	TestAcc 0.9396	BestValid 0.9418
	Epoch 3875:	Loss 0.4007	TrainAcc 0.9477	ValidAcc 0.9421	TestAcc 0.9399	BestValid 0.9421
	Epoch 3900:	Loss 0.4037	TrainAcc 0.9465	ValidAcc 0.9411	TestAcc 0.9385	BestValid 0.9421
	Epoch 3925:	Loss 0.4003	TrainAcc 0.9463	ValidAcc 0.9412	TestAcc 0.9387	BestValid 0.9421
	Epoch 3950:	Loss 0.4039	TrainAcc 0.9470	ValidAcc 0.9416	TestAcc 0.9387	BestValid 0.9421
	Epoch 3975:	Loss 0.4030	TrainAcc 0.9467	ValidAcc 0.9413	TestAcc 0.9384	BestValid 0.9421
	Epoch 4000:	Loss 0.4018	TrainAcc 0.9470	ValidAcc 0.9414	TestAcc 0.9388	BestValid 0.9421
	Epoch 4025:	Loss 0.4002	TrainAcc 0.9464	ValidAcc 0.9412	TestAcc 0.9387	BestValid 0.9421
	Epoch 4050:	Loss 0.3988	TrainAcc 0.9463	ValidAcc 0.9408	TestAcc 0.9384	BestValid 0.9421
	Epoch 4075:	Loss 0.3966	TrainAcc 0.9489	ValidAcc 0.9430	TestAcc 0.9407	BestValid 0.9430
	Epoch 4100:	Loss 0.4009	TrainAcc 0.9472	ValidAcc 0.9417	TestAcc 0.9393	BestValid 0.9430
	Epoch 4125:	Loss 0.3990	TrainAcc 0.9468	ValidAcc 0.9415	TestAcc 0.9389	BestValid 0.9430
	Epoch 4150:	Loss 0.3972	TrainAcc 0.9469	ValidAcc 0.9415	TestAcc 0.9390	BestValid 0.9430
	Epoch 4175:	Loss 0.3975	TrainAcc 0.9477	ValidAcc 0.9424	TestAcc 0.9402	BestValid 0.9430
	Epoch 4200:	Loss 0.3987	TrainAcc 0.9465	ValidAcc 0.9414	TestAcc 0.9388	BestValid 0.9430
	Epoch 4225:	Loss 0.3979	TrainAcc 0.9480	ValidAcc 0.9425	TestAcc 0.9401	BestValid 0.9430
	Epoch 4250:	Loss 0.3961	TrainAcc 0.9467	ValidAcc 0.9421	TestAcc 0.9391	BestValid 0.9430
	Epoch 4275:	Loss 0.3945	TrainAcc 0.9468	ValidAcc 0.9421	TestAcc 0.9390	BestValid 0.9430
	Epoch 4300:	Loss 0.3969	TrainAcc 0.9472	ValidAcc 0.9421	TestAcc 0.9397	BestValid 0.9430
	Epoch 4325:	Loss 0.3932	TrainAcc 0.9471	ValidAcc 0.9423	TestAcc 0.9396	BestValid 0.9430
	Epoch 4350:	Loss 0.3919	TrainAcc 0.9476	ValidAcc 0.9422	TestAcc 0.9398	BestValid 0.9430
	Epoch 4375:	Loss 0.3936	TrainAcc 0.9479	ValidAcc 0.9424	TestAcc 0.9403	BestValid 0.9430
	Epoch 4400:	Loss 0.3920	TrainAcc 0.9475	ValidAcc 0.9419	TestAcc 0.9404	BestValid 0.9430
	Epoch 4425:	Loss 0.3927	TrainAcc 0.9453	ValidAcc 0.9396	TestAcc 0.9373	BestValid 0.9430
	Epoch 4450:	Loss 0.3912	TrainAcc 0.9484	ValidAcc 0.9424	TestAcc 0.9407	BestValid 0.9430
	Epoch 4475:	Loss 0.3931	TrainAcc 0.9470	ValidAcc 0.9409	TestAcc 0.9388	BestValid 0.9430
	Epoch 4500:	Loss 0.3946	TrainAcc 0.9457	ValidAcc 0.9404	TestAcc 0.9377	BestValid 0.9430
	Epoch 4525:	Loss 0.3935	TrainAcc 0.9494	ValidAcc 0.9436	TestAcc 0.9420	BestValid 0.9436
	Epoch 4550:	Loss 0.3895	TrainAcc 0.9480	ValidAcc 0.9426	TestAcc 0.9403	BestValid 0.9436
	Epoch 4575:	Loss 0.3922	TrainAcc 0.9482	ValidAcc 0.9424	TestAcc 0.9409	BestValid 0.9436
	Epoch 4600:	Loss 0.3899	TrainAcc 0.9479	ValidAcc 0.9425	TestAcc 0.9404	BestValid 0.9436
	Epoch 4625:	Loss 0.3917	TrainAcc 0.9482	ValidAcc 0.9426	TestAcc 0.9408	BestValid 0.9436
	Epoch 4650:	Loss 0.3919	TrainAcc 0.9471	ValidAcc 0.9414	TestAcc 0.9391	BestValid 0.9436
	Epoch 4675:	Loss 0.3889	TrainAcc 0.9487	ValidAcc 0.9432	TestAcc 0.9409	BestValid 0.9436
	Epoch 4700:	Loss 0.3887	TrainAcc 0.9478	ValidAcc 0.9421	TestAcc 0.9403	BestValid 0.9436
	Epoch 4725:	Loss 0.3879	TrainAcc 0.9498	ValidAcc 0.9443	TestAcc 0.9427	BestValid 0.9443
	Epoch 4750:	Loss 0.3874	TrainAcc 0.9487	ValidAcc 0.9433	TestAcc 0.9412	BestValid 0.9443
	Epoch 4775:	Loss 0.3879	TrainAcc 0.9487	ValidAcc 0.9429	TestAcc 0.9417	BestValid 0.9443
	Epoch 4800:	Loss 0.3878	TrainAcc 0.9486	ValidAcc 0.9431	TestAcc 0.9412	BestValid 0.9443
	Epoch 4825:	Loss 0.3868	TrainAcc 0.9491	ValidAcc 0.9438	TestAcc 0.9417	BestValid 0.9443
	Epoch 4850:	Loss 0.3875	TrainAcc 0.9464	ValidAcc 0.9408	TestAcc 0.9389	BestValid 0.9443
	Epoch 4875:	Loss 0.3863	TrainAcc 0.9488	ValidAcc 0.9434	TestAcc 0.9411	BestValid 0.9443
	Epoch 4900:	Loss 0.3839	TrainAcc 0.9475	ValidAcc 0.9416	TestAcc 0.9401	BestValid 0.9443
	Epoch 4925:	Loss 0.3837	TrainAcc 0.9499	ValidAcc 0.9440	TestAcc 0.9425	BestValid 0.9443
	Epoch 4950:	Loss 0.3849	TrainAcc 0.9477	ValidAcc 0.9424	TestAcc 0.9402	BestValid 0.9443
	Epoch 4975:	Loss 0.3874	TrainAcc 0.9502	ValidAcc 0.9444	TestAcc 0.9426	BestValid 0.9444
	Epoch 5000:	Loss 0.3844	TrainAcc 0.9485	ValidAcc 0.9427	TestAcc 0.9410	BestValid 0.9444
Node 1, Pre/Post-Pipelining: 2.160 / 28.983 ms, Bubble: 67.996 ms, Compute: 336.195 ms, Comm: 31.065 ms, Imbalance: 7.878 ms
Node 3, Pre/Post-Pipelining: 2.155 / 28.934 ms, Bubble: 67.286 ms, Compute: 317.878 ms, Comm: 36.044 ms, Imbalance: 22.826 ms
Node 2, Pre/Post-Pipelining: 2.157 / 28.973 ms, Bubble: 67.227 ms, Compute: 318.168 ms, Comm: 35.318 ms, Imbalance: 23.266 ms
Node 0, Pre/Post-Pipelining: 2.162 / 28.968 ms, Bubble: 67.961 ms, Compute: 336.481 ms, Comm: 30.449 ms, Imbalance: 8.239 ms
Node 4, Pre/Post-Pipelining: 2.157 / 28.988 ms, Bubble: 69.041 ms, Compute: 316.427 ms, Comm: 33.375 ms, Imbalance: 26.412 ms
Node 5, Pre/Post-Pipelining: 2.154 / 28.933 ms, Bubble: 69.100 ms, Compute: 316.672 ms, Comm: 33.463 ms, Imbalance: 26.095 ms
Node 6, Pre/Post-Pipelining: 2.161 / 36.886 ms, Bubble: 62.609 ms, Compute: 332.426 ms, Comm: 27.742 ms, Imbalance: 13.841 ms
Node 7, Pre/Post-Pipelining: 2.158 / 36.766 ms, Bubble: 62.746 ms, Compute: 333.496 ms, Comm: 26.859 ms, Imbalance: 13.604 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 2.162 ms
Cluster-Wide Average, Post-Pipelining Overhead: 28.968 ms
Cluster-Wide Average, Bubble: 67.961 ms
Cluster-Wide Average, Compute: 336.481 ms
Cluster-Wide Average, Communication: 30.449 ms
Cluster-Wide Average, Imbalance: 8.239 ms
Node 0, GPU memory consumption: 10.143 GB
Node 1, GPU memory consumption: 8.962 GB
Node 3, GPU memory consumption: 7.893 GB
Node 2, GPU memory consumption: 7.915 GB
Node 4, GPU memory consumption: 7.891 GB
Node 5, GPU memory consumption: 7.917 GB
Node 6, GPU memory consumption: 8.067 GB
Node 7, GPU memory consumption: 8.046 GB
Node 0, Graph-Level Communication Throughput: 101.297 Gbps, Time: 55.317 ms
Node 1, Graph-Level Communication Throughput: 102.407 Gbps, Time: 56.484 ms
Node 2, Graph-Level Communication Throughput: 103.320 Gbps, Time: 54.234 ms
Node 3, Graph-Level Communication Throughput: 105.187 Gbps, Time: 54.991 ms
Node 4, Graph-Level Communication Throughput: 105.951 Gbps, Time: 52.887 ms
Node 5, Graph-Level Communication Throughput: 103.243 Gbps, Time: 56.026 ms
Node 6, Graph-Level Communication Throughput: 106.222 Gbps, Time: 52.752 ms
Node 7, Graph-Level Communication Throughput: 102.429 Gbps, Time: 56.471 ms
------------------------node id 0,  per-epoch time: 0.953594 s---------------
------------------------node id 1,  per-epoch time: 0.953594 s---------------
------------------------node id 2,  per-epoch time: 0.953594 s---------------
------------------------node id 3,  per-epoch time: 0.953594 s---------------
------------------------node id 4,  per-epoch time: 0.953594 s---------------
------------------------node id 5,  per-epoch time: 0.953594 s---------------
------------------------node id 6,  per-epoch time: 0.953594 s---------------
------------------------node id 7,  per-epoch time: 0.953594 s---------------
************ Profiling Results ************
	Bubble: 628.421435 (ms) (65.93 percentage)
	Compute: 253.824380 (ms) (26.63 percentage)
	GraphCommComputeOverhead: 9.741905 (ms) (1.02 percentage)
	GraphCommNetwork: 54.900213 (ms) (5.76 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.302018 (ms) (0.66 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.041 GB
	Graph-level communication (cluster-wide, per-epoch): 5.303 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 6.356 GB
	Aggregated layer-level communication throughput: 281.411 Gbps
Highest valid_acc: 0.9444
Target test_acc: 0.9426
Epoch to reach the target acc: 4974
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
