gnerv1
Tue Aug  1 01:22:34 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   31C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   30C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   30C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   29C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 22%] Built target context
[ 36%] Built target core
Scanning dependencies of target cudahelp
[ 38%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 41%] Linking CXX static library libcudahelp.a
[ 77%] Built target cudahelp
[ 86%] Linking CXX executable estimate_comm_volume
[ 86%] Linking CXX executable gcn
[ 86%] Linking CXX executable graphsage
[ 86%] Linking CXX executable gcnii
[ 91%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_graphsage
[ 97%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_MULTI_NODES_gcnii
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.874 seconds.
Building the CSC structure...
        It takes 2.067 seconds.
Building the CSC structure...
        It takes 2.287 seconds.
Building the CSC structure...
        It takes 2.395 seconds.
Building the CSC structure...
        It takes 2.438 seconds.
Building the CSC structure...
        It takes 2.495 seconds.
Building the CSC structure...
        It takes 2.652 seconds.
Building the CSC structure...
        It takes 2.688 seconds.
Building the CSC structure...
        It takes 1.799 seconds.
        It takes 1.833 seconds.
        It takes 2.244 seconds.
Building the Feature Vector...
        It takes 2.342 seconds.
        It takes 2.334 seconds.
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 2.432 seconds.
        It takes 0.037 seconds.
        It takes 2.356 seconds.
        It takes 2.374 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.032 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.292 seconds.
Building the Label Vector...
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
        It takes 0.273 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.330 Gbps (per GPU), 442.640 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.063 Gbps (per GPU), 440.505 Gbps (aggregated)
The layer-level communication performance: 55.062 Gbps (per GPU), 440.492 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.838 Gbps (per GPU), 438.701 Gbps (aggregated)
The layer-level communication performance: 54.808 Gbps (per GPU), 438.467 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.619 Gbps (per GPU), 436.955 Gbps (aggregated)
The layer-level communication performance: 54.582 Gbps (per GPU), 436.659 Gbps (aggregated)
The layer-level communication performance: 54.553 Gbps (per GPU), 436.426 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.049 Gbps (per GPU), 1280.391 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.256 Gbps (per GPU), 1282.049 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.057 Gbps (per GPU), 1280.459 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.253 Gbps (per GPU), 1282.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.076 Gbps (per GPU), 1280.606 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.076 Gbps (per GPU), 1280.606 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.076 Gbps (per GPU), 1280.606 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.079 Gbps (per GPU), 1280.630 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 105.080 Gbps (per GPU), 840.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.081 Gbps (per GPU), 840.648 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.082 Gbps (per GPU), 840.654 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.071 Gbps (per GPU), 840.570 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.083 Gbps (per GPU), 840.661 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.082 Gbps (per GPU), 840.654 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.082 Gbps (per GPU), 840.654 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.079 Gbps (per GPU), 840.633 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.161 Gbps (per GPU), 265.286 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.161 Gbps (per GPU), 265.287 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.161 Gbps (per GPU), 265.290 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.161 Gbps (per GPU), 265.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.162 Gbps (per GPU), 265.294 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.160 Gbps (per GPU), 265.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.162 Gbps (per GPU), 265.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.160 Gbps (per GPU), 265.281 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.88ms  2.40ms  2.70ms  3.06  8.38K  3.53M
 chk_1  0.75ms  2.76ms  2.89ms  3.88  6.74K  3.60M
 chk_2  0.79ms  2.62ms  2.78ms  3.54  7.27K  3.53M
 chk_3  0.80ms  2.66ms  2.82ms  3.54  7.92K  3.61M
 chk_4  0.62ms  2.58ms  2.73ms  4.38  5.33K  3.68M
 chk_5  0.99ms  2.57ms  2.75ms  2.77 10.07K  3.45M
 chk_6  0.95ms  2.73ms  2.92ms  3.07  9.41K  3.48M
 chk_7  0.81ms  2.56ms  2.83ms  3.49  8.12K  3.60M
 chk_8  0.67ms  2.66ms  2.83ms  4.21  6.09K  3.64M
 chk_9  1.09ms  2.50ms  2.70ms  2.48 11.10K  3.38M
chk_10  0.65ms  2.71ms  2.88ms  4.47  5.67K  3.63M
chk_11  0.81ms  2.58ms  2.76ms  3.41  8.16K  3.54M
chk_12  0.79ms  2.78ms  2.96ms  3.76  7.24K  3.55M
chk_13  0.63ms  2.61ms  2.77ms  4.39  5.41K  3.68M
chk_14  0.77ms  2.86ms  3.03ms  3.92  7.14K  3.53M
chk_15  0.94ms  2.71ms  2.90ms  3.08  9.25K  3.49M
chk_16  0.59ms  2.53ms  2.69ms  4.54  4.78K  3.77M
chk_17  0.76ms  2.66ms  2.83ms  3.75  6.85K  3.60M
chk_18  0.80ms  2.48ms  2.63ms  3.28  7.47K  3.57M
chk_19  0.60ms  2.54ms  2.67ms  4.46  4.88K  3.75M
chk_20  0.76ms  2.56ms  2.69ms  3.53  7.00K  3.63M
chk_21  0.63ms  2.53ms  2.68ms  4.28  5.41K  3.68M
chk_22  1.09ms  2.75ms  2.95ms  2.71 11.07K  3.39M
chk_23  0.79ms  2.62ms  2.80ms  3.56  7.23K  3.64M
chk_24  1.00ms  2.70ms  2.87ms  2.87 10.13K  3.43M
chk_25  0.72ms  2.50ms  2.66ms  3.70  6.40K  3.57M
chk_26  0.66ms  2.71ms  2.86ms  4.36  5.78K  3.55M
chk_27  0.95ms  2.57ms  2.78ms  2.93  9.34K  3.48M
chk_28  0.72ms  2.89ms  3.07ms  4.27  6.37K  3.57M
chk_29  0.62ms  2.67ms  2.85ms  4.57  5.16K  3.78M
chk_30  0.63ms  2.59ms  2.75ms  4.36  5.44K  3.67M
chk_31  0.72ms  2.72ms  2.90ms  4.03  6.33K  3.63M
   Avg  0.78  2.63  2.81
   Max  1.09  2.89  3.07
   Min  0.59  2.40  2.63
 Ratio  1.83  1.21  1.17
   Var  0.02  0.01  0.01
Profiling takes 2.391 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 362.216 ms
Partition 0 [0, 5) has cost: 362.216 ms
Partition 1 [5, 9) has cost: 337.242 ms
Partition 2 [9, 13) has cost: 337.242 ms
Partition 3 [13, 17) has cost: 337.242 ms
Partition 4 [17, 21) has cost: 337.242 ms
Partition 5 [21, 25) has cost: 337.242 ms
Partition 6 [25, 29) has cost: 337.242 ms
Partition 7 [29, 33) has cost: 342.897 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.580 ms
GPU 0, Compute+Comm Time: 132.353 ms, Bubble Time: 28.845 ms, Imbalance Overhead: 3.381 ms
GPU 1, Compute+Comm Time: 125.380 ms, Bubble Time: 28.557 ms, Imbalance Overhead: 10.643 ms
GPU 2, Compute+Comm Time: 125.380 ms, Bubble Time: 28.618 ms, Imbalance Overhead: 10.582 ms
GPU 3, Compute+Comm Time: 125.380 ms, Bubble Time: 28.519 ms, Imbalance Overhead: 10.681 ms
GPU 4, Compute+Comm Time: 125.380 ms, Bubble Time: 28.467 ms, Imbalance Overhead: 10.733 ms
GPU 5, Compute+Comm Time: 125.380 ms, Bubble Time: 28.548 ms, Imbalance Overhead: 10.653 ms
GPU 6, Compute+Comm Time: 125.380 ms, Bubble Time: 28.795 ms, Imbalance Overhead: 10.405 ms
GPU 7, Compute+Comm Time: 126.569 ms, Bubble Time: 29.182 ms, Imbalance Overhead: 8.829 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 316.661 ms
GPU 0, Compute+Comm Time: 243.274 ms, Bubble Time: 56.533 ms, Imbalance Overhead: 16.854 ms
GPU 1, Compute+Comm Time: 238.809 ms, Bubble Time: 55.748 ms, Imbalance Overhead: 22.104 ms
GPU 2, Compute+Comm Time: 238.809 ms, Bubble Time: 55.170 ms, Imbalance Overhead: 22.682 ms
GPU 3, Compute+Comm Time: 238.809 ms, Bubble Time: 55.024 ms, Imbalance Overhead: 22.828 ms
GPU 4, Compute+Comm Time: 238.809 ms, Bubble Time: 55.099 ms, Imbalance Overhead: 22.753 ms
GPU 5, Compute+Comm Time: 238.809 ms, Bubble Time: 55.202 ms, Imbalance Overhead: 22.650 ms
GPU 6, Compute+Comm Time: 238.809 ms, Bubble Time: 54.985 ms, Imbalance Overhead: 22.867 ms
GPU 7, Compute+Comm Time: 256.810 ms, Bubble Time: 55.554 ms, Imbalance Overhead: 4.297 ms
The estimated cost of the whole pipeline: 505.303 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 699.458 ms
Partition 0 [0, 9) has cost: 699.458 ms
Partition 1 [9, 17) has cost: 674.484 ms
Partition 2 [17, 25) has cost: 674.484 ms
Partition 3 [25, 33) has cost: 680.139 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.746 ms
GPU 0, Compute+Comm Time: 141.223 ms, Bubble Time: 26.703 ms, Imbalance Overhead: 2.820 ms
GPU 1, Compute+Comm Time: 137.447 ms, Bubble Time: 26.247 ms, Imbalance Overhead: 7.052 ms
GPU 2, Compute+Comm Time: 137.447 ms, Bubble Time: 26.213 ms, Imbalance Overhead: 7.086 ms
GPU 3, Compute+Comm Time: 138.022 ms, Bubble Time: 25.980 ms, Imbalance Overhead: 6.745 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 315.970 ms
GPU 0, Compute+Comm Time: 254.914 ms, Bubble Time: 48.332 ms, Imbalance Overhead: 12.724 ms
GPU 1, Compute+Comm Time: 252.649 ms, Bubble Time: 48.544 ms, Imbalance Overhead: 14.777 ms
GPU 2, Compute+Comm Time: 252.649 ms, Bubble Time: 48.604 ms, Imbalance Overhead: 14.717 ms
GPU 3, Compute+Comm Time: 262.600 ms, Bubble Time: 49.601 ms, Imbalance Overhead: 3.770 ms
    The estimated cost with 2 DP ways is 511.052 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1373.942 ms
Partition 0 [0, 17) has cost: 1373.942 ms
Partition 1 [17, 33) has cost: 1354.623 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 215.855 ms
GPU 0, Compute+Comm Time: 187.590 ms, Bubble Time: 23.033 ms, Imbalance Overhead: 5.232 ms
GPU 1, Compute+Comm Time: 185.794 ms, Bubble Time: 23.625 ms, Imbalance Overhead: 6.436 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 352.555 ms
GPU 0, Compute+Comm Time: 304.828 ms, Bubble Time: 38.832 ms, Imbalance Overhead: 8.895 ms
GPU 1, Compute+Comm Time: 309.331 ms, Bubble Time: 37.755 ms, Imbalance Overhead: 5.469 ms
    The estimated cost with 4 DP ways is 596.830 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2728.565 ms
Partition 0 [0, 33) has cost: 2728.565 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 570.715 ms
GPU 0, Compute+Comm Time: 570.715 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 693.857 ms
GPU 0, Compute+Comm Time: 693.857 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1327.801 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5918	TrainAcc 0.3749	ValidAcc 0.4020	TestAcc 0.3984	BestValid 0.4020
	Epoch 50:	Loss 1.9593	TrainAcc 0.6521	ValidAcc 0.6774	TestAcc 0.6696	BestValid 0.6774
	Epoch 75:	Loss 1.5756	TrainAcc 0.7315	ValidAcc 0.7508	TestAcc 0.7450	BestValid 0.7508
	Epoch 100:	Loss 1.3431	TrainAcc 0.7772	ValidAcc 0.7923	TestAcc 0.7854	BestValid 0.7923
	Epoch 125:	Loss 1.1359	TrainAcc 0.8161	ValidAcc 0.8293	TestAcc 0.8222	BestValid 0.8293
	Epoch 150:	Loss 1.0351	TrainAcc 0.8338	ValidAcc 0.8435	TestAcc 0.8380	BestValid 0.8435
	Epoch 175:	Loss 0.9643	TrainAcc 0.8459	ValidAcc 0.8548	TestAcc 0.8485	BestValid 0.8548
	Epoch 200:	Loss 0.9019	TrainAcc 0.8496	ValidAcc 0.8572	TestAcc 0.8512	BestValid 0.8572
	Epoch 225:	Loss 0.8373	TrainAcc 0.8538	ValidAcc 0.8611	TestAcc 0.8552	BestValid 0.8611
	Epoch 250:	Loss 0.8050	TrainAcc 0.8502	ValidAcc 0.8577	TestAcc 0.8512	BestValid 0.8611
	Epoch 275:	Loss 0.7757	TrainAcc 0.8825	ValidAcc 0.8894	TestAcc 0.8830	BestValid 0.8894
	Epoch 300:	Loss 0.7520	TrainAcc 0.8809	ValidAcc 0.8890	TestAcc 0.8825	BestValid 0.8894
	Epoch 325:	Loss 0.7190	TrainAcc 0.8653	ValidAcc 0.8742	TestAcc 0.8667	BestValid 0.8894
	Epoch 350:	Loss 0.6976	TrainAcc 0.8866	ValidAcc 0.8979	TestAcc 0.8905	BestValid 0.8979
	Epoch 375:	Loss 0.6771	TrainAcc 0.6287	ValidAcc 0.6500	TestAcc 0.6463	BestValid 0.8979
	Epoch 400:	Loss 0.6633	TrainAcc 0.3480	ValidAcc 0.3424	TestAcc 0.3375	BestValid 0.8979
	Epoch 425:	Loss 0.6505	TrainAcc 0.2368	ValidAcc 0.2326	TestAcc 0.2267	BestValid 0.8979
	Epoch 450:	Loss 0.6426	TrainAcc 0.5290	ValidAcc 0.5262	TestAcc 0.5230	BestValid 0.8979
	Epoch 475:	Loss 0.6224	TrainAcc 0.7646	ValidAcc 0.7589	TestAcc 0.7512	BestValid 0.8979
	Epoch 500:	Loss 0.6181	TrainAcc 0.4646	ValidAcc 0.4784	TestAcc 0.4726	BestValid 0.8979
	Epoch 525:	Loss 0.6350	TrainAcc 0.2050	ValidAcc 0.2319	TestAcc 0.2339	BestValid 0.8979
	Epoch 550:	Loss 0.6191	TrainAcc 0.1955	ValidAcc 0.2247	TestAcc 0.2260	BestValid 0.8979
	Epoch 575:	Loss 0.6228	TrainAcc 0.2740	ValidAcc 0.2961	TestAcc 0.2984	BestValid 0.8979
	Epoch 600:	Loss 0.6055	TrainAcc 0.2698	ValidAcc 0.2908	TestAcc 0.2907	BestValid 0.8979
	Epoch 625:	Loss 0.6182	TrainAcc 0.1152	ValidAcc 0.0961	TestAcc 0.0935	BestValid 0.8979
	Epoch 650:	Loss 0.6453	TrainAcc 0.1108	ValidAcc 0.0945	TestAcc 0.0912	BestValid 0.8979
	Epoch 675:	Loss 0.6904	TrainAcc 0.0940	ValidAcc 0.0792	TestAcc 0.0777	BestValid 0.8979
	Epoch 700:	Loss 0.7132	TrainAcc 0.0858	ValidAcc 0.0812	TestAcc 0.0783	BestValid 0.8979
	Epoch 725:	Loss 0.8584	TrainAcc 0.0825	ValidAcc 0.0752	TestAcc 0.0755	BestValid 0.8979
	Epoch 750:	Loss 0.7679	TrainAcc 0.1574	ValidAcc 0.1501	TestAcc 0.1473	BestValid 0.8979
	Epoch 775:	Loss 0.7329	TrainAcc 0.2785	ValidAcc 0.2667	TestAcc 0.2630	BestValid 0.8979
	Epoch 800:	Loss 0.8046	TrainAcc 0.6116	ValidAcc 0.6479	TestAcc 0.6398	BestValid 0.8979
	Epoch 825:	Loss 0.9789	TrainAcc 0.6246	ValidAcc 0.6494	TestAcc 0.6459	BestValid 0.8979
	Epoch 850:	Loss 0.9405	TrainAcc 0.6154	ValidAcc 0.6265	TestAcc 0.6201	BestValid 0.8979
	Epoch 875:	Loss 0.8949	TrainAcc 0.5488	ValidAcc 0.5595	TestAcc 0.5545	BestValid 0.8979
	Epoch 900:	Loss 0.8806	TrainAcc 0.5399	ValidAcc 0.5501	TestAcc 0.5437	BestValid 0.8979
	Epoch 925:	Loss 0.8319	TrainAcc 0.5545	ValidAcc 0.5633	TestAcc 0.5568	BestValid 0.8979
	Epoch 950:	Loss 0.7441	TrainAcc 0.5329	ValidAcc 0.5471	TestAcc 0.5402	BestValid 0.8979
	Epoch 975:	Loss 0.7020	TrainAcc 0.4110	ValidAcc 0.4354	TestAcc 0.4279	BestValid 0.8979
	Epoch 1000:	Loss 0.6882	TrainAcc 0.3323	ValidAcc 0.3595	TestAcc 0.3558	BestValid 0.8979
	Epoch 1025:	Loss 0.7291	TrainAcc 0.2672	ValidAcc 0.2931	TestAcc 0.2929	BestValid 0.8979
	Epoch 1050:	Loss 0.7109	TrainAcc 0.4034	ValidAcc 0.4411	TestAcc 0.4402	BestValid 0.8979
	Epoch 1075:	Loss 0.7009	TrainAcc 0.1598	ValidAcc 0.1399	TestAcc 0.1424	BestValid 0.8979
	Epoch 1100:	Loss 0.6977	TrainAcc 0.1631	ValidAcc 0.1418	TestAcc 0.1469	BestValid 0.8979
	Epoch 1125:	Loss 0.7154	TrainAcc 0.2487	ValidAcc 0.2308	TestAcc 0.2296	BestValid 0.8979
	Epoch 1150:	Loss 0.6962	TrainAcc 0.5388	ValidAcc 0.5116	TestAcc 0.5060	BestValid 0.8979
	Epoch 1175:	Loss 0.7026	TrainAcc 0.8352	ValidAcc 0.8428	TestAcc 0.8374	BestValid 0.8979
	Epoch 1200:	Loss 0.6955	TrainAcc 0.6484	ValidAcc 0.6679	TestAcc 0.6609	BestValid 0.8979
	Epoch 1225:	Loss 0.6999	TrainAcc 0.4767	ValidAcc 0.4809	TestAcc 0.4725	BestValid 0.8979
	Epoch 1250:	Loss 0.6995	TrainAcc 0.4038	ValidAcc 0.4109	TestAcc 0.4057	BestValid 0.8979
	Epoch 1275:	Loss 0.6681	TrainAcc 0.6305	ValidAcc 0.6611	TestAcc 0.6508	BestValid 0.8979
	Epoch 1300:	Loss 0.6454	TrainAcc 0.7669	ValidAcc 0.7792	TestAcc 0.7738	BestValid 0.8979
	Epoch 1325:	Loss 0.6518	TrainAcc 0.8208	ValidAcc 0.8235	TestAcc 0.8203	BestValid 0.8979
	Epoch 1350:	Loss 0.6313	TrainAcc 0.8025	ValidAcc 0.8046	TestAcc 0.8008	BestValid 0.8979
	Epoch 1375:	Loss 0.6206	TrainAcc 0.8223	ValidAcc 0.8271	TestAcc 0.8219	BestValid 0.8979
	Epoch 1400:	Loss 0.6091	TrainAcc 0.8523	ValidAcc 0.8567	TestAcc 0.8506	BestValid 0.8979
	Epoch 1425:	Loss 0.6153	TrainAcc 0.8606	ValidAcc 0.8661	TestAcc 0.8590	BestValid 0.8979
	Epoch 1450:	Loss 0.6133	TrainAcc 0.8676	ValidAcc 0.8705	TestAcc 0.8650	BestValid 0.8979
	Epoch 1475:	Loss 0.5995	TrainAcc 0.8878	ValidAcc 0.8909	TestAcc 0.8849	BestValid 0.8979
	Epoch 1500:	Loss 0.5869	TrainAcc 0.9090	ValidAcc 0.9118	TestAcc 0.9065	BestValid 0.9118
	Epoch 1525:	Loss 0.5873	TrainAcc 0.9071	ValidAcc 0.9105	TestAcc 0.9055	BestValid 0.9118
	Epoch 1550:	Loss 0.5751	TrainAcc 0.9069	ValidAcc 0.9109	TestAcc 0.9060	BestValid 0.9118
	Epoch 1575:	Loss 0.5730	TrainAcc 0.9117	ValidAcc 0.9159	TestAcc 0.9111	BestValid 0.9159
	Epoch 1600:	Loss 0.5759	TrainAcc 0.9118	ValidAcc 0.9163	TestAcc 0.9113	BestValid 0.9163
	Epoch 1625:	Loss 0.5596	TrainAcc 0.9121	ValidAcc 0.9153	TestAcc 0.9118	BestValid 0.9163
	Epoch 1650:	Loss 0.5587	TrainAcc 0.9164	ValidAcc 0.9186	TestAcc 0.9155	BestValid 0.9186
	Epoch 1675:	Loss 0.5538	TrainAcc 0.9189	ValidAcc 0.9220	TestAcc 0.9184	BestValid 0.9220
	Epoch 1700:	Loss 0.5484	TrainAcc 0.9190	ValidAcc 0.9211	TestAcc 0.9184	BestValid 0.9220
	Epoch 1725:	Loss 0.5402	TrainAcc 0.9177	ValidAcc 0.9187	TestAcc 0.9164	BestValid 0.9220
	Epoch 1750:	Loss 0.5421	TrainAcc 0.9173	ValidAcc 0.9183	TestAcc 0.9156	BestValid 0.9220
	Epoch 1775:	Loss 0.5444	TrainAcc 0.9189	ValidAcc 0.9200	TestAcc 0.9172	BestValid 0.9220
	Epoch 1800:	Loss 0.5358	TrainAcc 0.9219	ValidAcc 0.9230	TestAcc 0.9204	BestValid 0.9230
	Epoch 1825:	Loss 0.5324	TrainAcc 0.9234	ValidAcc 0.9249	TestAcc 0.9220	BestValid 0.9249
	Epoch 1850:	Loss 0.5243	TrainAcc 0.9231	ValidAcc 0.9247	TestAcc 0.9216	BestValid 0.9249
	Epoch 1875:	Loss 0.5258	TrainAcc 0.9238	ValidAcc 0.9256	TestAcc 0.9224	BestValid 0.9256
	Epoch 1900:	Loss 0.5268	TrainAcc 0.9239	ValidAcc 0.9259	TestAcc 0.9225	BestValid 0.9259
	Epoch 1925:	Loss 0.5198	TrainAcc 0.9246	ValidAcc 0.9266	TestAcc 0.9231	BestValid 0.9266
	Epoch 1950:	Loss 0.5175	TrainAcc 0.9253	ValidAcc 0.9273	TestAcc 0.9241	BestValid 0.9273
	Epoch 1975:	Loss 0.5146	TrainAcc 0.9262	ValidAcc 0.9279	TestAcc 0.9249	BestValid 0.9279
	Epoch 2000:	Loss 0.5177	TrainAcc 0.9260	ValidAcc 0.9282	TestAcc 0.9251	BestValid 0.9282
	Epoch 2025:	Loss 0.5126	TrainAcc 0.9258	ValidAcc 0.9277	TestAcc 0.9248	BestValid 0.9282
	Epoch 2050:	Loss 0.5109	TrainAcc 0.9240	ValidAcc 0.9253	TestAcc 0.9224	BestValid 0.9282
	Epoch 2075:	Loss 0.5114	TrainAcc 0.9237	ValidAcc 0.9246	TestAcc 0.9218	BestValid 0.9282
	Epoch 2100:	Loss 0.5068	TrainAcc 0.9256	ValidAcc 0.9272	TestAcc 0.9238	BestValid 0.9282
	Epoch 2125:	Loss 0.5025	TrainAcc 0.9273	ValidAcc 0.9294	TestAcc 0.9259	BestValid 0.9294
	Epoch 2150:	Loss 0.5037	TrainAcc 0.9291	ValidAcc 0.9309	TestAcc 0.9276	BestValid 0.9309
	Epoch 2175:	Loss 0.4992	TrainAcc 0.9284	ValidAcc 0.9303	TestAcc 0.9271	BestValid 0.9309
	Epoch 2200:	Loss 0.5067	TrainAcc 0.9218	ValidAcc 0.9231	TestAcc 0.9200	BestValid 0.9309
	Epoch 2225:	Loss 0.4944	TrainAcc 0.9188	ValidAcc 0.9200	TestAcc 0.9171	BestValid 0.9309
	Epoch 2250:	Loss 0.4916	TrainAcc 0.9234	ValidAcc 0.9248	TestAcc 0.9214	BestValid 0.9309
	Epoch 2275:	Loss 0.4924	TrainAcc 0.9307	ValidAcc 0.9323	TestAcc 0.9288	BestValid 0.9323
	Epoch 2300:	Loss 0.4896	TrainAcc 0.9278	ValidAcc 0.9278	TestAcc 0.9248	BestValid 0.9323
	Epoch 2325:	Loss 0.4900	TrainAcc 0.9310	ValidAcc 0.9323	TestAcc 0.9291	BestValid 0.9323
	Epoch 2350:	Loss 0.4879	TrainAcc 0.9239	ValidAcc 0.9255	TestAcc 0.9220	BestValid 0.9323
	Epoch 2375:	Loss 0.4895	TrainAcc 0.9135	ValidAcc 0.9139	TestAcc 0.9111	BestValid 0.9323
	Epoch 2400:	Loss 0.4841	TrainAcc 0.9249	ValidAcc 0.9258	TestAcc 0.9224	BestValid 0.9323
	Epoch 2425:	Loss 0.4830	TrainAcc 0.9305	ValidAcc 0.9307	TestAcc 0.9279	BestValid 0.9323
	Epoch 2450:	Loss 0.4801	TrainAcc 0.9197	ValidAcc 0.9197	TestAcc 0.9173	BestValid 0.9323
	Epoch 2475:	Loss 0.4816	TrainAcc 0.9323	ValidAcc 0.9330	TestAcc 0.9305	BestValid 0.9330
	Epoch 2500:	Loss 0.4791	TrainAcc 0.9170	ValidAcc 0.9172	TestAcc 0.9146	BestValid 0.9330
	Epoch 2525:	Loss 0.4774	TrainAcc 0.8948	ValidAcc 0.8968	TestAcc 0.8928	BestValid 0.9330
	Epoch 2550:	Loss 0.4763	TrainAcc 0.9182	ValidAcc 0.9190	TestAcc 0.9154	BestValid 0.9330
	Epoch 2575:	Loss 0.4767	TrainAcc 0.9272	ValidAcc 0.9272	TestAcc 0.9239	BestValid 0.9330
	Epoch 2600:	Loss 0.4706	TrainAcc 0.9083	ValidAcc 0.9097	TestAcc 0.9063	BestValid 0.9330
	Epoch 2625:	Loss 0.4736	TrainAcc 0.9335	ValidAcc 0.9344	TestAcc 0.9315	BestValid 0.9344
	Epoch 2650:	Loss 0.4701	TrainAcc 0.8995	ValidAcc 0.9015	TestAcc 0.8970	BestValid 0.9344
	Epoch 2675:	Loss 0.4641	TrainAcc 0.8816	ValidAcc 0.8831	TestAcc 0.8788	BestValid 0.9344
	Epoch 2700:	Loss 0.4814	TrainAcc 0.9207	ValidAcc 0.9212	TestAcc 0.9180	BestValid 0.9344
	Epoch 2725:	Loss 0.4712	TrainAcc 0.9107	ValidAcc 0.9119	TestAcc 0.9082	BestValid 0.9344
	Epoch 2750:	Loss 0.4639	TrainAcc 0.8642	ValidAcc 0.8710	TestAcc 0.8638	BestValid 0.9344
	Epoch 2775:	Loss 0.4636	TrainAcc 0.9341	ValidAcc 0.9351	TestAcc 0.9321	BestValid 0.9351
	Epoch 2800:	Loss 0.4607	TrainAcc 0.8850	ValidAcc 0.8875	TestAcc 0.8823	BestValid 0.9351
	Epoch 2825:	Loss 0.4598	TrainAcc 0.8688	ValidAcc 0.8700	TestAcc 0.8661	BestValid 0.9351
	Epoch 2850:	Loss 0.4663	TrainAcc 0.9321	ValidAcc 0.9322	TestAcc 0.9292	BestValid 0.9351
	Epoch 2875:	Loss 0.4573	TrainAcc 0.9156	ValidAcc 0.9157	TestAcc 0.9127	BestValid 0.9351
	Epoch 2900:	Loss 0.4632	TrainAcc 0.8781	ValidAcc 0.8830	TestAcc 0.8766	BestValid 0.9351
	Epoch 2925:	Loss 0.4525	TrainAcc 0.9356	ValidAcc 0.9363	TestAcc 0.9340	BestValid 0.9363
	Epoch 2950:	Loss 0.4551	TrainAcc 0.8921	ValidAcc 0.8945	TestAcc 0.8892	BestValid 0.9363
	Epoch 2975:	Loss 0.4527	TrainAcc 0.8563	ValidAcc 0.8565	TestAcc 0.8519	BestValid 0.9363
	Epoch 3000:	Loss 0.4496	TrainAcc 0.9221	ValidAcc 0.9225	TestAcc 0.9195	BestValid 0.9363
	Epoch 3025:	Loss 0.4558	TrainAcc 0.9223	ValidAcc 0.9220	TestAcc 0.9191	BestValid 0.9363
	Epoch 3050:	Loss 0.4498	TrainAcc 0.8079	ValidAcc 0.8163	TestAcc 0.8109	BestValid 0.9363
	Epoch 3075:	Loss 0.4488	TrainAcc 0.9301	ValidAcc 0.9292	TestAcc 0.9266	BestValid 0.9363
	Epoch 3100:	Loss 0.4481	TrainAcc 0.8957	ValidAcc 0.8976	TestAcc 0.8936	BestValid 0.9363
	Epoch 3125:	Loss 0.4397	TrainAcc 0.8741	ValidAcc 0.8754	TestAcc 0.8723	BestValid 0.9363
	Epoch 3150:	Loss 0.4421	TrainAcc 0.9093	ValidAcc 0.9101	TestAcc 0.9066	BestValid 0.9363
	Epoch 3175:	Loss 0.4375	TrainAcc 0.9174	ValidAcc 0.9175	TestAcc 0.9144	BestValid 0.9363
	Epoch 3200:	Loss 0.4435	TrainAcc 0.8114	ValidAcc 0.8189	TestAcc 0.8137	BestValid 0.9363
	Epoch 3225:	Loss 0.4395	TrainAcc 0.9077	ValidAcc 0.9095	TestAcc 0.9059	BestValid 0.9363
	Epoch 3250:	Loss 0.4355	TrainAcc 0.9257	ValidAcc 0.9262	TestAcc 0.9232	BestValid 0.9363
	Epoch 3275:	Loss 0.4347	TrainAcc 0.8810	ValidAcc 0.8825	TestAcc 0.8793	BestValid 0.9363
	Epoch 3300:	Loss 0.4317	TrainAcc 0.9015	ValidAcc 0.9027	TestAcc 0.8996	BestValid 0.9363
	Epoch 3325:	Loss 0.4331	TrainAcc 0.9391	ValidAcc 0.9391	TestAcc 0.9360	BestValid 0.9391
	Epoch 3350:	Loss 0.4309	TrainAcc 0.9193	ValidAcc 0.9201	TestAcc 0.9167	BestValid 0.9391
	Epoch 3375:	Loss 0.4339	TrainAcc 0.9239	ValidAcc 0.9243	TestAcc 0.9213	BestValid 0.9391
	Epoch 3400:	Loss 0.4337	TrainAcc 0.9353	ValidAcc 0.9356	TestAcc 0.9332	BestValid 0.9391
	Epoch 3425:	Loss 0.4261	TrainAcc 0.8739	ValidAcc 0.8747	TestAcc 0.8725	BestValid 0.9391
	Epoch 3450:	Loss 0.4265	TrainAcc 0.9226	ValidAcc 0.9221	TestAcc 0.9193	BestValid 0.9391
	Epoch 3475:	Loss 0.4267	TrainAcc 0.9171	ValidAcc 0.9177	TestAcc 0.9137	BestValid 0.9391
	Epoch 3500:	Loss 0.4283	TrainAcc 0.9108	ValidAcc 0.9116	TestAcc 0.9080	BestValid 0.9391
	Epoch 3525:	Loss 0.4227	TrainAcc 0.4646	ValidAcc 0.4584	TestAcc 0.4559	BestValid 0.9391
	Epoch 3550:	Loss 0.4255	TrainAcc 0.9174	ValidAcc 0.9161	TestAcc 0.9152	BestValid 0.9391
	Epoch 3575:	Loss 0.4241	TrainAcc 0.8947	ValidAcc 0.8950	TestAcc 0.8935	BestValid 0.9391
	Epoch 3600:	Loss 0.4228	TrainAcc 0.9061	ValidAcc 0.9071	TestAcc 0.9037	BestValid 0.9391
	Epoch 3625:	Loss 0.4183	TrainAcc 0.8755	ValidAcc 0.8755	TestAcc 0.8709	BestValid 0.9391
	Epoch 3650:	Loss 0.4231	TrainAcc 0.9328	ValidAcc 0.9321	TestAcc 0.9296	BestValid 0.9391
	Epoch 3675:	Loss 0.4172	TrainAcc 0.9221	ValidAcc 0.9210	TestAcc 0.9198	BestValid 0.9391
	Epoch 3700:	Loss 0.4183	TrainAcc 0.9084	ValidAcc 0.9073	TestAcc 0.9057	BestValid 0.9391
	Epoch 3725:	Loss 0.4157	TrainAcc 0.9367	ValidAcc 0.9362	TestAcc 0.9333	BestValid 0.9391
	Epoch 3750:	Loss 0.4149	TrainAcc 0.9420	ValidAcc 0.9413	TestAcc 0.9389	BestValid 0.9413
	Epoch 3775:	Loss 0.4164	TrainAcc 0.8387	ValidAcc 0.8422	TestAcc 0.8382	BestValid 0.9413
	Epoch 3800:	Loss 0.4171	TrainAcc 0.6748	ValidAcc 0.6885	TestAcc 0.6863	BestValid 0.9413
	Epoch 3825:	Loss 0.4155	TrainAcc 0.8053	ValidAcc 0.8111	TestAcc 0.8084	BestValid 0.9413
	Epoch 3850:	Loss 0.4146	TrainAcc 0.9228	ValidAcc 0.9222	TestAcc 0.9188	BestValid 0.9413
	Epoch 3875:	Loss 0.4102	TrainAcc 0.9330	ValidAcc 0.9331	TestAcc 0.9301	BestValid 0.9413
	Epoch 3900:	Loss 0.4120	TrainAcc 0.8884	ValidAcc 0.8893	TestAcc 0.8879	BestValid 0.9413
	Epoch 3925:	Loss 0.4120	TrainAcc 0.9386	ValidAcc 0.9385	TestAcc 0.9356	BestValid 0.9413
	Epoch 3950:	Loss 0.4061	TrainAcc 0.7863	ValidAcc 0.7792	TestAcc 0.7736	BestValid 0.9413
	Epoch 3975:	Loss 0.4078	TrainAcc 0.8071	ValidAcc 0.7979	TestAcc 0.7955	BestValid 0.9413
	Epoch 4000:	Loss 0.4052	TrainAcc 0.8106	ValidAcc 0.7998	TestAcc 0.7958	BestValid 0.9413
	Epoch 4025:	Loss 0.4063	TrainAcc 0.7514	ValidAcc 0.7424	TestAcc 0.7381	BestValid 0.9413
	Epoch 4050:	Loss 0.3998	TrainAcc 0.9223	ValidAcc 0.9197	TestAcc 0.9177	BestValid 0.9413
	Epoch 4075:	Loss 0.4016	TrainAcc 0.9261	ValidAcc 0.9258	TestAcc 0.9230	BestValid 0.9413
	Epoch 4100:	Loss 0.4067	TrainAcc 0.7916	ValidAcc 0.7949	TestAcc 0.7890	BestValid 0.9413
	Epoch 4125:	Loss 0.4030	TrainAcc 0.6762	ValidAcc 0.6744	TestAcc 0.6731	BestValid 0.9413
	Epoch 4150:	Loss 0.4077	TrainAcc 0.8490	ValidAcc 0.8546	TestAcc 0.8501	BestValid 0.9413
	Epoch 4175:	Loss 0.4043	TrainAcc 0.8553	ValidAcc 0.8610	TestAcc 0.8563	BestValid 0.9413
	Epoch 4200:	Loss 0.4019	TrainAcc 0.9126	ValidAcc 0.9127	TestAcc 0.9089	BestValid 0.9413
	Epoch 4225:	Loss 0.4019	TrainAcc 0.9433	ValidAcc 0.9426	TestAcc 0.9401	BestValid 0.9426
	Epoch 4250:	Loss 0.4011	TrainAcc 0.9440	ValidAcc 0.9434	TestAcc 0.9414	BestValid 0.9434
	Epoch 4275:	Loss 0.4012	TrainAcc 0.9159	ValidAcc 0.9151	TestAcc 0.9142	BestValid 0.9434
	Epoch 4300:	Loss 0.3978	TrainAcc 0.7897	ValidAcc 0.7797	TestAcc 0.7766	BestValid 0.9434
	Epoch 4325:	Loss 0.3934	TrainAcc 0.7552	ValidAcc 0.7467	TestAcc 0.7426	BestValid 0.9434
	Epoch 4350:	Loss 0.3964	TrainAcc 0.7964	ValidAcc 0.7849	TestAcc 0.7819	BestValid 0.9434
	Epoch 4375:	Loss 0.3952	TrainAcc 0.9167	ValidAcc 0.9147	TestAcc 0.9134	BestValid 0.9434
	Epoch 4400:	Loss 0.3924	TrainAcc 0.9371	ValidAcc 0.9352	TestAcc 0.9331	BestValid 0.9434
	Epoch 4425:	Loss 0.3927	TrainAcc 0.9224	ValidAcc 0.9214	TestAcc 0.9188	BestValid 0.9434
	Epoch 4450:	Loss 0.3907	TrainAcc 0.8920	ValidAcc 0.8940	TestAcc 0.8894	BestValid 0.9434
	Epoch 4475:	Loss 0.3865	TrainAcc 0.8714	ValidAcc 0.8747	TestAcc 0.8696	BestValid 0.9434
	Epoch 4500:	Loss 0.3912	TrainAcc 0.8844	ValidAcc 0.8860	TestAcc 0.8831	BestValid 0.9434
	Epoch 4525:	Loss 0.3909	TrainAcc 0.8257	ValidAcc 0.8323	TestAcc 0.8262	BestValid 0.9434
	Epoch 4550:	Loss 0.3910	TrainAcc 0.9033	ValidAcc 0.9041	TestAcc 0.8990	BestValid 0.9434
	Epoch 4575:	Loss 0.3924	TrainAcc 0.9261	ValidAcc 0.9254	TestAcc 0.9224	BestValid 0.9434
	Epoch 4600:	Loss 0.3908	TrainAcc 0.9325	ValidAcc 0.9318	TestAcc 0.9289	BestValid 0.9434
	Epoch 4625:	Loss 0.3849	TrainAcc 0.6683	ValidAcc 0.6669	TestAcc 0.6621	BestValid 0.9434
	Epoch 4650:	Loss 0.3847	TrainAcc 0.9082	ValidAcc 0.9074	TestAcc 0.9051	BestValid 0.9434
	Epoch 4675:	Loss 0.3915	TrainAcc 0.8373	ValidAcc 0.8301	TestAcc 0.8254	BestValid 0.9434
	Epoch 4700:	Loss 0.3858	TrainAcc 0.7680	ValidAcc 0.7565	TestAcc 0.7536	BestValid 0.9434
	Epoch 4725:	Loss 0.3850	TrainAcc 0.9282	ValidAcc 0.9243	TestAcc 0.9245	BestValid 0.9434
	Epoch 4750:	Loss 0.3876	TrainAcc 0.9207	ValidAcc 0.9180	TestAcc 0.9171	BestValid 0.9434
	Epoch 4775:	Loss 0.3878	TrainAcc 0.9439	ValidAcc 0.9424	TestAcc 0.9403	BestValid 0.9434
	Epoch 4800:	Loss 0.3822	TrainAcc 0.9057	ValidAcc 0.9075	TestAcc 0.9019	BestValid 0.9434
	Epoch 4825:	Loss 0.3815	TrainAcc 0.9368	ValidAcc 0.9345	TestAcc 0.9324	BestValid 0.9434
	Epoch 4850:	Loss 0.3761	TrainAcc 0.9094	ValidAcc 0.9105	TestAcc 0.9060	BestValid 0.9434
	Epoch 4875:	Loss 0.3790	TrainAcc 0.9307	ValidAcc 0.9306	TestAcc 0.9276	BestValid 0.9434
	Epoch 4900:	Loss 0.3825	TrainAcc 0.9322	ValidAcc 0.9309	TestAcc 0.9280	BestValid 0.9434
	Epoch 4925:	Loss 0.3794	TrainAcc 0.9128	ValidAcc 0.9128	TestAcc 0.9093	BestValid 0.9434
	Epoch 4950:	Loss 0.3791	TrainAcc 0.9436	ValidAcc 0.9434	TestAcc 0.9407	BestValid 0.9434
	Epoch 4975:	Loss 0.3771	TrainAcc 0.9296	ValidAcc 0.9290	TestAcc 0.9264	BestValid 0.9434
	Epoch 5000:	Loss 0.3783	TrainAcc 0.9154	ValidAcc 0.9173	TestAcc 0.9133	BestValid 0.9434
Node 2, Pre/Post-Pipelining: 1.088 / 0.343 ms, Bubble: 81.577 ms, Compute: 248.712 ms, Comm: 47.175 ms, Imbalance: 34.363 ms
Node 3, Pre/Post-Pipelining: 1.090 / 0.351 ms, Bubble: 80.555 ms, Compute: 255.605 ms, Comm: 47.106 ms, Imbalance: 28.213 ms
Node 5, Pre/Post-Pipelining: 1.089 / 0.330 ms, Bubble: 81.691 ms, Compute: 252.390 ms, Comm: 45.014 ms, Imbalance: 32.728 ms
Node 1, Pre/Post-Pipelining: 1.091 / 0.336 ms, Bubble: 80.827 ms, Compute: 256.481 ms, Comm: 42.493 ms, Imbalance: 31.445 ms
Node 6, Pre/Post-Pipelining: 1.090 / 0.326 ms, Bubble: 81.917 ms, Compute: 253.958 ms, Comm: 40.501 ms, Imbalance: 35.512 ms
Node 7, Pre/Post-Pipelining: 1.092 / 15.805 ms, Bubble: 66.673 ms, Compute: 280.778 ms, Comm: 31.921 ms, Imbalance: 16.488 ms
Node 0, Pre/Post-Pipelining: 1.095 / 0.408 ms, Bubble: 80.723 ms, Compute: 286.903 ms, Comm: 32.624 ms, Imbalance: 10.282 ms
Node 4, Pre/Post-Pipelining: 1.088 / 0.369 ms, Bubble: 81.271 ms, Compute: 249.796 ms, Comm: 46.483 ms, Imbalance: 34.534 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.095 ms
Cluster-Wide Average, Post-Pipelining Overhead: 0.408 ms
Cluster-Wide Average, Bubble: 80.723 ms
Cluster-Wide Average, Compute: 286.903 ms
Cluster-Wide Average, Communication: 32.624 ms
Cluster-Wide Average, Imbalance: 10.282 ms
Node 0, GPU memory consumption: 8.059 GB
Node 2, GPU memory consumption: 6.042 GB
Node 1, GPU memory consumption: 6.042 GB
Node 3, GPU memory consumption: 6.018 GB
Node 7, GPU memory consumption: 6.171 GB
Node 4, GPU memory consumption: 6.018 GB
Node 6, GPU memory consumption: 6.042 GB
Node 5, GPU memory consumption: 6.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.891556 s---------------
------------------------node id 1,  per-epoch time: 0.891556 s---------------
------------------------node id 2,  per-epoch time: 0.891556 s---------------
------------------------node id 3,  per-epoch time: 0.891556 s---------------
------------------------node id 4,  per-epoch time: 0.891556 s---------------
------------------------node id 5,  per-epoch time: 0.891556 s---------------
------------------------node id 6,  per-epoch time: 0.891556 s---------------
------------------------node id 7,  per-epoch time: 0.891556 s---------------
************ Profiling Results ************
	Bubble: 631.294713 (ms) (70.84 percentage)
	Compute: 258.818582 (ms) (29.04 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 1.073359 (ms) (0.12 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 2.431 GB
	Aggregated layer-level communication throughput: 500.993 Gbps
Highest valid_acc: 0.9434
Target test_acc: 0.9414
Epoch to reach the target acc: 4249
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
