gnerv1
Mon Jul 31 20:53:05 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 31%   49C    P8    29W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   49C    P8    28W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   46C    P8    23W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   43C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 11%] Built target context
[ 36%] Built target core
Scanning dependencies of target cudahelp
[ 38%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 41%] Linking CXX static library libcudahelp.a
[ 77%] Built target cudahelp
[ 83%] Linking CXX executable gcn
[ 83%] Linking CXX executable graphsage
[ 86%] Linking CXX executable gcnii
[ 86%] Linking CXX executable estimate_comm_volume
[ 91%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target OSDI2023_MULTI_NODES_gcnii
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.856 seconds.
Building the CSC structure...
        It takes 2.039 seconds.
Building the CSC structure...
        It takes 2.407 seconds.
Building the CSC structure...
        It takes 2.640 seconds.
Building the CSC structure...
        It takes 2.924 seconds.
Building the CSC structure...
        It takes 2.924 seconds.
Building the CSC structure...
        It takes 2.925 seconds.
Building the CSC structure...
        It takes 2.925 seconds.
Building the CSC structure...
        It takes 1.816 seconds.
        It takes 1.857 seconds.
Building the Feature Vector...
        It takes 2.332 seconds.
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 2.336 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.822 seconds.
        It takes 2.823 seconds.
        It takes 2.823 seconds.
        It takes 2.823 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.295 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
        It takes 0.249 seconds.
Building the Label Vector...
Building the Feature Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.032 seconds.
Building the Feature Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.307 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.041 seconds.
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.278 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.031 seconds.
        It takes 0.289 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 55.323 Gbps (per GPU), 442.581 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.097 Gbps (per GPU), 440.773 Gbps (aggregated)
The layer-level communication performance: 55.102 Gbps (per GPU), 440.819 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.880 Gbps (per GPU), 439.042 Gbps (aggregated)
The layer-level communication performance: 54.855 Gbps (per GPU), 438.838 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.682 Gbps (per GPU), 437.458 Gbps (aggregated)
The layer-level communication performance: 54.635 Gbps (per GPU), 437.083 Gbps (aggregated)
The layer-level communication performance: 54.611 Gbps (per GPU), 436.890 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.938 Gbps (per GPU), 1271.507 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.935 Gbps (per GPU), 1271.483 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.942 Gbps (per GPU), 1271.533 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.938 Gbps (per GPU), 1271.507 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.917 Gbps (per GPU), 1271.338 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.914 Gbps (per GPU), 1271.314 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.917 Gbps (per GPU), 1271.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.912 Gbps (per GPU), 1271.295 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 99.284 Gbps (per GPU), 794.275 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.287 Gbps (per GPU), 794.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.284 Gbps (per GPU), 794.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.286 Gbps (per GPU), 794.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.285 Gbps (per GPU), 794.282 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.284 Gbps (per GPU), 794.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.286 Gbps (per GPU), 794.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 99.284 Gbps (per GPU), 794.275 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.127 Gbps (per GPU), 265.017 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.127 Gbps (per GPU), 265.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.127 Gbps (per GPU), 265.018 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.126 Gbps (per GPU), 265.009 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.126 Gbps (per GPU), 265.009 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.126 Gbps (per GPU), 265.010 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.127 Gbps (per GPU), 265.015 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.126 Gbps (per GPU), 265.009 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.46ms  2.73ms  3.05  8.38K  3.53M
 chk_1  0.76ms  2.81ms  2.94ms  3.88  6.74K  3.60M
 chk_2  0.80ms  2.70ms  2.83ms  3.54  7.27K  3.53M
 chk_3  0.81ms  2.73ms  2.87ms  3.56  7.92K  3.61M
 chk_4  0.63ms  2.66ms  2.79ms  4.41  5.33K  3.68M
 chk_5  1.01ms  2.64ms  2.81ms  2.78 10.07K  3.45M
 chk_6  0.97ms  2.79ms  2.96ms  3.07  9.41K  3.48M
 chk_7  0.82ms  2.66ms  2.79ms  3.39  8.12K  3.60M
 chk_8  0.68ms  2.75ms  2.89ms  4.22  6.09K  3.64M
 chk_9  1.10ms  2.58ms  2.75ms  2.49 11.10K  3.38M
chk_10  0.66ms  2.79ms  3.18ms  4.84  5.67K  3.63M
chk_11  0.83ms  2.64ms  2.79ms  3.37  8.16K  3.54M
chk_12  0.80ms  2.83ms  2.98ms  3.72  7.24K  3.55M
chk_13  0.64ms  2.67ms  2.80ms  4.37  5.41K  3.68M
chk_14  0.78ms  2.92ms  3.05ms  3.89  7.14K  3.53M
chk_15  0.96ms  2.79ms  2.92ms  3.06  9.25K  3.49M
chk_16  0.60ms  2.62ms  2.72ms  4.53  4.78K  3.77M
chk_17  0.77ms  2.72ms  2.86ms  3.72  6.85K  3.60M
chk_18  0.81ms  2.53ms  2.98ms  3.66  7.47K  3.57M
chk_19  0.61ms  2.61ms  2.72ms  4.45  4.88K  3.75M
chk_20  0.78ms  2.60ms  2.76ms  3.55  7.00K  3.63M
chk_21  0.64ms  2.60ms  2.73ms  4.26  5.41K  3.68M
chk_22  1.10ms  2.80ms  2.98ms  2.70 11.07K  3.39M
chk_23  0.80ms  2.69ms  2.83ms  3.55  7.23K  3.64M
chk_24  1.02ms  2.71ms  2.92ms  2.87 10.13K  3.43M
chk_25  0.73ms  2.55ms  2.71ms  3.69  6.40K  3.57M
chk_26  0.67ms  2.74ms  3.23ms  4.84  5.78K  3.55M
chk_27  0.97ms  2.65ms  2.80ms  2.89  9.34K  3.48M
chk_28  0.73ms  2.93ms  3.07ms  4.19  6.37K  3.57M
chk_29  0.64ms  2.74ms  2.85ms  4.49  5.16K  3.78M
chk_30  0.64ms  2.65ms  2.80ms  4.35  5.44K  3.67M
chk_31  0.73ms  2.76ms  2.93ms  4.01  6.33K  3.63M
   Avg  0.79  2.70  2.87
   Max  1.10  2.93  3.23
   Min  0.60  2.46  2.71
 Ratio  1.84  1.19  1.19
   Var  0.02  0.01  0.02
Profiling takes 2.434 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 370.729 ms
Partition 0 [0, 5) has cost: 370.729 ms
Partition 1 [5, 9) has cost: 345.348 ms
Partition 2 [9, 13) has cost: 345.348 ms
Partition 3 [13, 17) has cost: 345.348 ms
Partition 4 [17, 21) has cost: 345.348 ms
Partition 5 [21, 25) has cost: 345.348 ms
Partition 6 [25, 29) has cost: 345.348 ms
Partition 7 [29, 33) has cost: 350.958 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 167.417 ms
GPU 0, Compute+Comm Time: 135.257 ms, Bubble Time: 29.244 ms, Imbalance Overhead: 2.917 ms
GPU 1, Compute+Comm Time: 128.139 ms, Bubble Time: 28.985 ms, Imbalance Overhead: 10.294 ms
GPU 2, Compute+Comm Time: 128.139 ms, Bubble Time: 29.058 ms, Imbalance Overhead: 10.220 ms
GPU 3, Compute+Comm Time: 128.139 ms, Bubble Time: 29.045 ms, Imbalance Overhead: 10.234 ms
GPU 4, Compute+Comm Time: 128.139 ms, Bubble Time: 29.034 ms, Imbalance Overhead: 10.245 ms
GPU 5, Compute+Comm Time: 128.139 ms, Bubble Time: 29.097 ms, Imbalance Overhead: 10.182 ms
GPU 6, Compute+Comm Time: 128.139 ms, Bubble Time: 29.378 ms, Imbalance Overhead: 9.900 ms
GPU 7, Compute+Comm Time: 129.039 ms, Bubble Time: 29.785 ms, Imbalance Overhead: 8.594 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 322.609 ms
GPU 0, Compute+Comm Time: 248.870 ms, Bubble Time: 57.745 ms, Imbalance Overhead: 15.995 ms
GPU 1, Compute+Comm Time: 244.159 ms, Bubble Time: 56.899 ms, Imbalance Overhead: 21.551 ms
GPU 2, Compute+Comm Time: 244.159 ms, Bubble Time: 56.234 ms, Imbalance Overhead: 22.216 ms
GPU 3, Compute+Comm Time: 244.159 ms, Bubble Time: 56.060 ms, Imbalance Overhead: 22.390 ms
GPU 4, Compute+Comm Time: 244.159 ms, Bubble Time: 56.012 ms, Imbalance Overhead: 22.438 ms
GPU 5, Compute+Comm Time: 244.159 ms, Bubble Time: 56.005 ms, Imbalance Overhead: 22.445 ms
GPU 6, Compute+Comm Time: 244.159 ms, Bubble Time: 55.741 ms, Imbalance Overhead: 22.710 ms
GPU 7, Compute+Comm Time: 262.423 ms, Bubble Time: 56.230 ms, Imbalance Overhead: 3.957 ms
The estimated cost of the whole pipeline: 514.528 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 716.077 ms
Partition 0 [0, 9) has cost: 716.077 ms
Partition 1 [9, 17) has cost: 690.695 ms
Partition 2 [17, 25) has cost: 690.695 ms
Partition 3 [25, 33) has cost: 696.305 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.726 ms
GPU 0, Compute+Comm Time: 144.232 ms, Bubble Time: 27.166 ms, Imbalance Overhead: 2.327 ms
GPU 1, Compute+Comm Time: 140.379 ms, Bubble Time: 26.819 ms, Imbalance Overhead: 6.528 ms
GPU 2, Compute+Comm Time: 140.379 ms, Bubble Time: 26.756 ms, Imbalance Overhead: 6.591 ms
GPU 3, Compute+Comm Time: 140.786 ms, Bubble Time: 26.583 ms, Imbalance Overhead: 6.357 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 321.594 ms
GPU 0, Compute+Comm Time: 260.590 ms, Bubble Time: 49.396 ms, Imbalance Overhead: 11.607 ms
GPU 1, Compute+Comm Time: 258.196 ms, Bubble Time: 49.635 ms, Imbalance Overhead: 13.763 ms
GPU 2, Compute+Comm Time: 258.196 ms, Bubble Time: 49.704 ms, Imbalance Overhead: 13.694 ms
GPU 3, Compute+Comm Time: 268.285 ms, Bubble Time: 50.587 ms, Imbalance Overhead: 2.722 ms
    The estimated cost with 2 DP ways is 520.085 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1406.772 ms
Partition 0 [0, 17) has cost: 1406.772 ms
Partition 1 [17, 33) has cost: 1387.001 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 222.577 ms
GPU 0, Compute+Comm Time: 193.396 ms, Bubble Time: 23.719 ms, Imbalance Overhead: 5.462 ms
GPU 1, Compute+Comm Time: 191.481 ms, Bubble Time: 24.422 ms, Imbalance Overhead: 6.674 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 361.702 ms
GPU 0, Compute+Comm Time: 313.132 ms, Bubble Time: 39.624 ms, Imbalance Overhead: 8.946 ms
GPU 1, Compute+Comm Time: 317.566 ms, Bubble Time: 38.910 ms, Imbalance Overhead: 5.226 ms
    The estimated cost with 4 DP ways is 613.493 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2793.772 ms
Partition 0 [0, 33) has cost: 2793.772 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 573.255 ms
GPU 0, Compute+Comm Time: 573.255 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 699.213 ms
GPU 0, Compute+Comm Time: 699.213 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1336.092 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ************ Start Scheduling the Tasks in a Pipelined Fashion ******

****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 7, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.4596	TrainAcc 0.5039	ValidAcc 0.5291	TestAcc 0.5251	BestValid 0.5291
	Epoch 50:	Loss 1.8049	TrainAcc 0.6654	ValidAcc 0.6860	TestAcc 0.6819	BestValid 0.6860
	Epoch 75:	Loss 1.4575	TrainAcc 0.7423	ValidAcc 0.7591	TestAcc 0.7536	BestValid 0.7591
	Epoch 100:	Loss 1.2517	TrainAcc 0.7823	ValidAcc 0.7960	TestAcc 0.7910	BestValid 0.7960
	Epoch 125:	Loss 1.1216	TrainAcc 0.8011	ValidAcc 0.8146	TestAcc 0.8086	BestValid 0.8146
	Epoch 150:	Loss 1.0221	TrainAcc 0.8292	ValidAcc 0.8397	TestAcc 0.8348	BestValid 0.8397
	Epoch 175:	Loss 0.9517	TrainAcc 0.8440	ValidAcc 0.8541	TestAcc 0.8490	BestValid 0.8541
	Epoch 200:	Loss 0.8929	TrainAcc 0.8552	ValidAcc 0.8654	TestAcc 0.8577	BestValid 0.8654
	Epoch 225:	Loss 0.8597	TrainAcc 0.8635	ValidAcc 0.8717	TestAcc 0.8654	BestValid 0.8717
	Epoch 250:	Loss 0.8276	TrainAcc 0.8545	ValidAcc 0.8611	TestAcc 0.8571	BestValid 0.8717
	Epoch 275:	Loss 0.8261	TrainAcc 0.8442	ValidAcc 0.8536	TestAcc 0.8474	BestValid 0.8717
	Epoch 300:	Loss 0.8348	TrainAcc 0.8675	ValidAcc 0.8725	TestAcc 0.8670	BestValid 0.8725
	Epoch 325:	Loss 0.7707	TrainAcc 0.8705	ValidAcc 0.8789	TestAcc 0.8718	BestValid 0.8789
	Epoch 350:	Loss 0.7449	TrainAcc 0.8730	ValidAcc 0.8793	TestAcc 0.8757	BestValid 0.8793
	Epoch 375:	Loss 0.7272	TrainAcc 0.8814	ValidAcc 0.8860	TestAcc 0.8805	BestValid 0.8860
	Epoch 400:	Loss 0.7137	TrainAcc 0.8837	ValidAcc 0.8910	TestAcc 0.8851	BestValid 0.8910
	Epoch 425:	Loss 0.6940	TrainAcc 0.8961	ValidAcc 0.9002	TestAcc 0.8951	BestValid 0.9002
	Epoch 450:	Loss 0.6819	TrainAcc 0.8928	ValidAcc 0.8982	TestAcc 0.8935	BestValid 0.9002
	Epoch 475:	Loss 0.6666	TrainAcc 0.8955	ValidAcc 0.9011	TestAcc 0.8956	BestValid 0.9011
	Epoch 500:	Loss 0.6617	TrainAcc 0.8979	ValidAcc 0.9039	TestAcc 0.8977	BestValid 0.9039
	Epoch 525:	Loss 0.6603	TrainAcc 0.9004	ValidAcc 0.9054	TestAcc 0.8994	BestValid 0.9054
	Epoch 550:	Loss 0.6505	TrainAcc 0.8929	ValidAcc 0.8977	TestAcc 0.8941	BestValid 0.9054
	Epoch 575:	Loss 0.6500	TrainAcc 0.9066	ValidAcc 0.9115	TestAcc 0.9062	BestValid 0.9115
	Epoch 600:	Loss 0.6314	TrainAcc 0.9005	ValidAcc 0.9055	TestAcc 0.8995	BestValid 0.9115
	Epoch 625:	Loss 0.6713	TrainAcc 0.8680	ValidAcc 0.8711	TestAcc 0.8667	BestValid 0.9115
	Epoch 650:	Loss 0.6618	TrainAcc 0.8920	ValidAcc 0.8981	TestAcc 0.8926	BestValid 0.9115
	Epoch 675:	Loss 0.6480	TrainAcc 0.8956	ValidAcc 0.8985	TestAcc 0.8938	BestValid 0.9115
	Epoch 700:	Loss 0.6291	TrainAcc 0.8966	ValidAcc 0.9021	TestAcc 0.8965	BestValid 0.9115
	Epoch 725:	Loss 0.6213	TrainAcc 0.9076	ValidAcc 0.9116	TestAcc 0.9057	BestValid 0.9116
	Epoch 750:	Loss 0.5998	TrainAcc 0.9046	ValidAcc 0.9097	TestAcc 0.9045	BestValid 0.9116
	Epoch 775:	Loss 0.5965	TrainAcc 0.9127	ValidAcc 0.9157	TestAcc 0.9110	BestValid 0.9157
	Epoch 800:	Loss 0.5877	TrainAcc 0.9116	ValidAcc 0.9155	TestAcc 0.9100	BestValid 0.9157
	Epoch 825:	Loss 0.5821	TrainAcc 0.9135	ValidAcc 0.9172	TestAcc 0.9122	BestValid 0.9172
	Epoch 850:	Loss 0.5747	TrainAcc 0.9148	ValidAcc 0.9178	TestAcc 0.9128	BestValid 0.9178
	Epoch 875:	Loss 0.5765	TrainAcc 0.9136	ValidAcc 0.9179	TestAcc 0.9125	BestValid 0.9179
	Epoch 900:	Loss 0.5683	TrainAcc 0.9156	ValidAcc 0.9189	TestAcc 0.9137	BestValid 0.9189
	Epoch 925:	Loss 0.5705	TrainAcc 0.9156	ValidAcc 0.9192	TestAcc 0.9148	BestValid 0.9192
	Epoch 950:	Loss 0.5606	TrainAcc 0.9138	ValidAcc 0.9174	TestAcc 0.9121	BestValid 0.9192
	Epoch 975:	Loss 0.5712	TrainAcc 0.9149	ValidAcc 0.9179	TestAcc 0.9136	BestValid 0.9192
	Epoch 1000:	Loss 0.5710	TrainAcc 0.9166	ValidAcc 0.9192	TestAcc 0.9149	BestValid 0.9192
	Epoch 1025:	Loss 0.5626	TrainAcc 0.9130	ValidAcc 0.9165	TestAcc 0.9117	BestValid 0.9192
	Epoch 1050:	Loss 0.5615	TrainAcc 0.9181	ValidAcc 0.9215	TestAcc 0.9158	BestValid 0.9215
	Epoch 1075:	Loss 0.5517	TrainAcc 0.9128	ValidAcc 0.9160	TestAcc 0.9110	BestValid 0.9215
	Epoch 1100:	Loss 0.5560	TrainAcc 0.9153	ValidAcc 0.9186	TestAcc 0.9145	BestValid 0.9215
	Epoch 1125:	Loss 0.5466	TrainAcc 0.9181	ValidAcc 0.9207	TestAcc 0.9157	BestValid 0.9215
	Epoch 1150:	Loss 0.5396	TrainAcc 0.9147	ValidAcc 0.9184	TestAcc 0.9144	BestValid 0.9215
	Epoch 1175:	Loss 0.5400	TrainAcc 0.9207	ValidAcc 0.9226	TestAcc 0.9190	BestValid 0.9226
	Epoch 1200:	Loss 0.5360	TrainAcc 0.9193	ValidAcc 0.9219	TestAcc 0.9189	BestValid 0.9226
	Epoch 1225:	Loss 0.5317	TrainAcc 0.9212	ValidAcc 0.9230	TestAcc 0.9193	BestValid 0.9230
	Epoch 1250:	Loss 0.5301	TrainAcc 0.9225	ValidAcc 0.9262	TestAcc 0.9212	BestValid 0.9262
	Epoch 1275:	Loss 0.5305	TrainAcc 0.9216	ValidAcc 0.9243	TestAcc 0.9203	BestValid 0.9262
	Epoch 1300:	Loss 0.5264	TrainAcc 0.9220	ValidAcc 0.9254	TestAcc 0.9210	BestValid 0.9262
	Epoch 1325:	Loss 0.5324	TrainAcc 0.9224	ValidAcc 0.9245	TestAcc 0.9202	BestValid 0.9262
	Epoch 1350:	Loss 0.5325	TrainAcc 0.9161	ValidAcc 0.9185	TestAcc 0.9146	BestValid 0.9262
	Epoch 1375:	Loss 0.5340	TrainAcc 0.9183	ValidAcc 0.9201	TestAcc 0.9160	BestValid 0.9262
	Epoch 1400:	Loss 0.5308	TrainAcc 0.9206	ValidAcc 0.9225	TestAcc 0.9183	BestValid 0.9262
	Epoch 1425:	Loss 0.5243	TrainAcc 0.9164	ValidAcc 0.9192	TestAcc 0.9159	BestValid 0.9262
	Epoch 1450:	Loss 0.5202	TrainAcc 0.9232	ValidAcc 0.9259	TestAcc 0.9213	BestValid 0.9262
	Epoch 1475:	Loss 0.5198	TrainAcc 0.9223	ValidAcc 0.9255	TestAcc 0.9215	BestValid 0.9262
	Epoch 1500:	Loss 0.5145	TrainAcc 0.9228	ValidAcc 0.9238	TestAcc 0.9209	BestValid 0.9262
	Epoch 1525:	Loss 0.5056	TrainAcc 0.9249	ValidAcc 0.9283	TestAcc 0.9244	BestValid 0.9283
	Epoch 1550:	Loss 0.5097	TrainAcc 0.9220	ValidAcc 0.9233	TestAcc 0.9201	BestValid 0.9283
	Epoch 1575:	Loss 0.5052	TrainAcc 0.9248	ValidAcc 0.9277	TestAcc 0.9243	BestValid 0.9283
	Epoch 1600:	Loss 0.5047	TrainAcc 0.9262	ValidAcc 0.9274	TestAcc 0.9236	BestValid 0.9283
	Epoch 1625:	Loss 0.4995	TrainAcc 0.9235	ValidAcc 0.9255	TestAcc 0.9227	BestValid 0.9283
	Epoch 1650:	Loss 0.4997	TrainAcc 0.9267	ValidAcc 0.9281	TestAcc 0.9236	BestValid 0.9283
	Epoch 1675:	Loss 0.5043	TrainAcc 0.9208	ValidAcc 0.9212	TestAcc 0.9188	BestValid 0.9283
	Epoch 1700:	Loss 0.5037	TrainAcc 0.9250	ValidAcc 0.9270	TestAcc 0.9222	BestValid 0.9283
	Epoch 1725:	Loss 0.5056	TrainAcc 0.9250	ValidAcc 0.9254	TestAcc 0.9224	BestValid 0.9283
	Epoch 1750:	Loss 0.5041	TrainAcc 0.9239	ValidAcc 0.9263	TestAcc 0.9231	BestValid 0.9283
	Epoch 1775:	Loss 0.5062	TrainAcc 0.9275	ValidAcc 0.9286	TestAcc 0.9255	BestValid 0.9286
	Epoch 1800:	Loss 0.4975	TrainAcc 0.9257	ValidAcc 0.9290	TestAcc 0.9252	BestValid 0.9290
	Epoch 1825:	Loss 0.4954	TrainAcc 0.9294	ValidAcc 0.9305	TestAcc 0.9274	BestValid 0.9305
	Epoch 1850:	Loss 0.4895	TrainAcc 0.9292	ValidAcc 0.9315	TestAcc 0.9282	BestValid 0.9315
	Epoch 1875:	Loss 0.4890	TrainAcc 0.9289	ValidAcc 0.9297	TestAcc 0.9266	BestValid 0.9315
	Epoch 1900:	Loss 0.4893	TrainAcc 0.9307	ValidAcc 0.9332	TestAcc 0.9294	BestValid 0.9332
	Epoch 1925:	Loss 0.4884	TrainAcc 0.9293	ValidAcc 0.9307	TestAcc 0.9274	BestValid 0.9332
	Epoch 1950:	Loss 0.4828	TrainAcc 0.9302	ValidAcc 0.9319	TestAcc 0.9292	BestValid 0.9332
	Epoch 1975:	Loss 0.4889	TrainAcc 0.9296	ValidAcc 0.9311	TestAcc 0.9272	BestValid 0.9332
	Epoch 2000:	Loss 0.4899	TrainAcc 0.9250	ValidAcc 0.9270	TestAcc 0.9234	BestValid 0.9332
	Epoch 2025:	Loss 0.4938	TrainAcc 0.9276	ValidAcc 0.9285	TestAcc 0.9240	BestValid 0.9332
	Epoch 2050:	Loss 0.4866	TrainAcc 0.9267	ValidAcc 0.9285	TestAcc 0.9247	BestValid 0.9332
	Epoch 2075:	Loss 0.4928	TrainAcc 0.9259	ValidAcc 0.9264	TestAcc 0.9236	BestValid 0.9332
	Epoch 2100:	Loss 0.4903	TrainAcc 0.9272	ValidAcc 0.9295	TestAcc 0.9257	BestValid 0.9332
	Epoch 2125:	Loss 0.4858	TrainAcc 0.9274	ValidAcc 0.9279	TestAcc 0.9249	BestValid 0.9332
	Epoch 2150:	Loss 0.4839	TrainAcc 0.9293	ValidAcc 0.9311	TestAcc 0.9282	BestValid 0.9332
	Epoch 2175:	Loss 0.4824	TrainAcc 0.9317	ValidAcc 0.9316	TestAcc 0.9286	BestValid 0.9332
	Epoch 2200:	Loss 0.4762	TrainAcc 0.9309	ValidAcc 0.9327	TestAcc 0.9291	BestValid 0.9332
	Epoch 2225:	Loss 0.4736	TrainAcc 0.9334	ValidAcc 0.9340	TestAcc 0.9305	BestValid 0.9340
	Epoch 2250:	Loss 0.4801	TrainAcc 0.9320	ValidAcc 0.9334	TestAcc 0.9304	BestValid 0.9340
	Epoch 2275:	Loss 0.4741	TrainAcc 0.9308	ValidAcc 0.9313	TestAcc 0.9277	BestValid 0.9340
	Epoch 2300:	Loss 0.4764	TrainAcc 0.9307	ValidAcc 0.9326	TestAcc 0.9290	BestValid 0.9340
	Epoch 2325:	Loss 0.4865	TrainAcc 0.9219	ValidAcc 0.9220	TestAcc 0.9186	BestValid 0.9340
	Epoch 2350:	Loss 0.4859	TrainAcc 0.9299	ValidAcc 0.9317	TestAcc 0.9277	BestValid 0.9340
	Epoch 2375:	Loss 0.4700	TrainAcc 0.9310	ValidAcc 0.9316	TestAcc 0.9268	BestValid 0.9340
	Epoch 2400:	Loss 0.4721	TrainAcc 0.9303	ValidAcc 0.9329	TestAcc 0.9289	BestValid 0.9340
	Epoch 2425:	Loss 0.4650	TrainAcc 0.9334	ValidAcc 0.9352	TestAcc 0.9307	BestValid 0.9352
	Epoch 2450:	Loss 0.4623	TrainAcc 0.9314	ValidAcc 0.9329	TestAcc 0.9298	BestValid 0.9352
	Epoch 2475:	Loss 0.4614	TrainAcc 0.9346	ValidAcc 0.9357	TestAcc 0.9321	BestValid 0.9357
	Epoch 2500:	Loss 0.4561	TrainAcc 0.9349	ValidAcc 0.9357	TestAcc 0.9319	BestValid 0.9357
	Epoch 2525:	Loss 0.4575	TrainAcc 0.9331	ValidAcc 0.9345	TestAcc 0.9305	BestValid 0.9357
	Epoch 2550:	Loss 0.4601	TrainAcc 0.9352	ValidAcc 0.9361	TestAcc 0.9322	BestValid 0.9361
	Epoch 2575:	Loss 0.4650	TrainAcc 0.9303	ValidAcc 0.9310	TestAcc 0.9273	BestValid 0.9361
	Epoch 2600:	Loss 0.4622	TrainAcc 0.9350	ValidAcc 0.9354	TestAcc 0.9311	BestValid 0.9361
	Epoch 2625:	Loss 0.4653	TrainAcc 0.9319	ValidAcc 0.9333	TestAcc 0.9291	BestValid 0.9361
	Epoch 2650:	Loss 0.4627	TrainAcc 0.9336	ValidAcc 0.9331	TestAcc 0.9303	BestValid 0.9361
	Epoch 2675:	Loss 0.4621	TrainAcc 0.9326	ValidAcc 0.9340	TestAcc 0.9304	BestValid 0.9361
	Epoch 2700:	Loss 0.4632	TrainAcc 0.9325	ValidAcc 0.9324	TestAcc 0.9297	BestValid 0.9361
	Epoch 2725:	Loss 0.4622	TrainAcc 0.9337	ValidAcc 0.9347	TestAcc 0.9316	BestValid 0.9361
	Epoch 2750:	Loss 0.4667	TrainAcc 0.9342	ValidAcc 0.9334	TestAcc 0.9307	BestValid 0.9361
	Epoch 2775:	Loss 0.4515	TrainAcc 0.9348	ValidAcc 0.9366	TestAcc 0.9328	BestValid 0.9366
	Epoch 2800:	Loss 0.4557	TrainAcc 0.9355	ValidAcc 0.9359	TestAcc 0.9315	BestValid 0.9366
	Epoch 2825:	Loss 0.4536	TrainAcc 0.9350	ValidAcc 0.9371	TestAcc 0.9327	BestValid 0.9371
	Epoch 2850:	Loss 0.4531	TrainAcc 0.9332	ValidAcc 0.9335	TestAcc 0.9289	BestValid 0.9371
	Epoch 2875:	Loss 0.4521	TrainAcc 0.9345	ValidAcc 0.9360	TestAcc 0.9315	BestValid 0.9371
	Epoch 2900:	Loss 0.4529	TrainAcc 0.9326	ValidAcc 0.9332	TestAcc 0.9287	BestValid 0.9371
	Epoch 2925:	Loss 0.4478	TrainAcc 0.9357	ValidAcc 0.9367	TestAcc 0.9329	BestValid 0.9371
	Epoch 2950:	Loss 0.4428	TrainAcc 0.9351	ValidAcc 0.9361	TestAcc 0.9320	BestValid 0.9371
	Epoch 2975:	Loss 0.4486	TrainAcc 0.9353	ValidAcc 0.9359	TestAcc 0.9330	BestValid 0.9371
	Epoch 3000:	Loss 0.4482	TrainAcc 0.9366	ValidAcc 0.9381	TestAcc 0.9339	BestValid 0.9381
	Epoch 3025:	Loss 0.4472	TrainAcc 0.9373	ValidAcc 0.9381	TestAcc 0.9345	BestValid 0.9381
	Epoch 3050:	Loss 0.4415	TrainAcc 0.9345	ValidAcc 0.9358	TestAcc 0.9314	BestValid 0.9381
	Epoch 3075:	Loss 0.4513	TrainAcc 0.9342	ValidAcc 0.9354	TestAcc 0.9297	BestValid 0.9381
	Epoch 3100:	Loss 0.4601	TrainAcc 0.9288	ValidAcc 0.9296	TestAcc 0.9263	BestValid 0.9381
	Epoch 3125:	Loss 0.4695	TrainAcc 0.9335	ValidAcc 0.9327	TestAcc 0.9284	BestValid 0.9381
	Epoch 3150:	Loss 0.4598	TrainAcc 0.9323	ValidAcc 0.9330	TestAcc 0.9296	BestValid 0.9381
	Epoch 3175:	Loss 0.4609	TrainAcc 0.9314	ValidAcc 0.9307	TestAcc 0.9284	BestValid 0.9381
	Epoch 3200:	Loss 0.4517	TrainAcc 0.9322	ValidAcc 0.9341	TestAcc 0.9298	BestValid 0.9381
	Epoch 3225:	Loss 0.4563	TrainAcc 0.9310	ValidAcc 0.9304	TestAcc 0.9278	BestValid 0.9381
	Epoch 3250:	Loss 0.4499	TrainAcc 0.9342	ValidAcc 0.9368	TestAcc 0.9322	BestValid 0.9381
	Epoch 3275:	Loss 0.4437	TrainAcc 0.9320	ValidAcc 0.9326	TestAcc 0.9287	BestValid 0.9381
	Epoch 3300:	Loss 0.4433	TrainAcc 0.9380	ValidAcc 0.9384	TestAcc 0.9349	BestValid 0.9384
	Epoch 3325:	Loss 0.4401	TrainAcc 0.9352	ValidAcc 0.9363	TestAcc 0.9324	BestValid 0.9384
	Epoch 3350:	Loss 0.4373	TrainAcc 0.9393	ValidAcc 0.9391	TestAcc 0.9354	BestValid 0.9391
	Epoch 3375:	Loss 0.4293	TrainAcc 0.9374	ValidAcc 0.9381	TestAcc 0.9344	BestValid 0.9391
	Epoch 3400:	Loss 0.4281	TrainAcc 0.9391	ValidAcc 0.9390	TestAcc 0.9352	BestValid 0.9391
	Epoch 3425:	Loss 0.4330	TrainAcc 0.9386	ValidAcc 0.9399	TestAcc 0.9357	BestValid 0.9399
	Epoch 3450:	Loss 0.4262	TrainAcc 0.9388	ValidAcc 0.9381	TestAcc 0.9350	BestValid 0.9399
	Epoch 3475:	Loss 0.4276	TrainAcc 0.9392	ValidAcc 0.9398	TestAcc 0.9362	BestValid 0.9399
	Epoch 3500:	Loss 0.4356	TrainAcc 0.9391	ValidAcc 0.9391	TestAcc 0.9352	BestValid 0.9399
	Epoch 3525:	Loss 0.4258	TrainAcc 0.9367	ValidAcc 0.9373	TestAcc 0.9333	BestValid 0.9399
	Epoch 3550:	Loss 0.4356	TrainAcc 0.9361	ValidAcc 0.9353	TestAcc 0.9308	BestValid 0.9399
	Epoch 3575:	Loss 0.4500	TrainAcc 0.9264	ValidAcc 0.9272	TestAcc 0.9238	BestValid 0.9399
	Epoch 3600:	Loss 0.4575	TrainAcc 0.9322	ValidAcc 0.9315	TestAcc 0.9269	BestValid 0.9399
	Epoch 3625:	Loss 0.4609	TrainAcc 0.9286	ValidAcc 0.9300	TestAcc 0.9260	BestValid 0.9399
	Epoch 3650:	Loss 0.4457	TrainAcc 0.9342	ValidAcc 0.9335	TestAcc 0.9300	BestValid 0.9399
	Epoch 3675:	Loss 0.4336	TrainAcc 0.9357	ValidAcc 0.9357	TestAcc 0.9319	BestValid 0.9399
	Epoch 3700:	Loss 0.4326	TrainAcc 0.9366	ValidAcc 0.9374	TestAcc 0.9336	BestValid 0.9399
	Epoch 3725:	Loss 0.4268	TrainAcc 0.9400	ValidAcc 0.9397	TestAcc 0.9356	BestValid 0.9399
	Epoch 3750:	Loss 0.4215	TrainAcc 0.9391	ValidAcc 0.9392	TestAcc 0.9364	BestValid 0.9399
	Epoch 3775:	Loss 0.4217	TrainAcc 0.9402	ValidAcc 0.9403	TestAcc 0.9366	BestValid 0.9403
	Epoch 3800:	Loss 0.4221	TrainAcc 0.9412	ValidAcc 0.9409	TestAcc 0.9379	BestValid 0.9409
	Epoch 3825:	Loss 0.4212	TrainAcc 0.9378	ValidAcc 0.9381	TestAcc 0.9345	BestValid 0.9409
	Epoch 3850:	Loss 0.4273	TrainAcc 0.9411	ValidAcc 0.9418	TestAcc 0.9380	BestValid 0.9418
	Epoch 3875:	Loss 0.4257	TrainAcc 0.9350	ValidAcc 0.9355	TestAcc 0.9305	BestValid 0.9418
	Epoch 3900:	Loss 0.4318	TrainAcc 0.9403	ValidAcc 0.9402	TestAcc 0.9367	BestValid 0.9418
	Epoch 3925:	Loss 0.4294	TrainAcc 0.9355	ValidAcc 0.9357	TestAcc 0.9309	BestValid 0.9418
	Epoch 3950:	Loss 0.4264	TrainAcc 0.9398	ValidAcc 0.9403	TestAcc 0.9361	BestValid 0.9418
	Epoch 3975:	Loss 0.4235	TrainAcc 0.9371	ValidAcc 0.9374	TestAcc 0.9324	BestValid 0.9418
	Epoch 4000:	Loss 0.4280	TrainAcc 0.9386	ValidAcc 0.9384	TestAcc 0.9359	BestValid 0.9418
	Epoch 4025:	Loss 0.4228	TrainAcc 0.9384	ValidAcc 0.9389	TestAcc 0.9345	BestValid 0.9418
	Epoch 4050:	Loss 0.4247	TrainAcc 0.9407	ValidAcc 0.9400	TestAcc 0.9366	BestValid 0.9418
	Epoch 4075:	Loss 0.4184	TrainAcc 0.9395	ValidAcc 0.9401	TestAcc 0.9367	BestValid 0.9418
	Epoch 4100:	Loss 0.4203	TrainAcc 0.9417	ValidAcc 0.9407	TestAcc 0.9376	BestValid 0.9418
	Epoch 4125:	Loss 0.4230	TrainAcc 0.9383	ValidAcc 0.9382	TestAcc 0.9356	BestValid 0.9418
	Epoch 4150:	Loss 0.4248	TrainAcc 0.9415	ValidAcc 0.9408	TestAcc 0.9368	BestValid 0.9418
	Epoch 4175:	Loss 0.4312	TrainAcc 0.9375	ValidAcc 0.9374	TestAcc 0.9344	BestValid 0.9418
	Epoch 4200:	Loss 0.4431	TrainAcc 0.9408	ValidAcc 0.9394	TestAcc 0.9359	BestValid 0.9418
	Epoch 4225:	Loss 0.4526	TrainAcc 0.9352	ValidAcc 0.9357	TestAcc 0.9321	BestValid 0.9418
	Epoch 4250:	Loss 0.4467	TrainAcc 0.9321	ValidAcc 0.9316	TestAcc 0.9288	BestValid 0.9418
	Epoch 4275:	Loss 0.4536	TrainAcc 0.9361	ValidAcc 0.9366	TestAcc 0.9328	BestValid 0.9418
	Epoch 4300:	Loss 0.4374	TrainAcc 0.9376	ValidAcc 0.9379	TestAcc 0.9348	BestValid 0.9418
	Epoch 4325:	Loss 0.4267	TrainAcc 0.9381	ValidAcc 0.9375	TestAcc 0.9335	BestValid 0.9418
	Epoch 4350:	Loss 0.4222	TrainAcc 0.9417	ValidAcc 0.9420	TestAcc 0.9388	BestValid 0.9420
	Epoch 4375:	Loss 0.4178	TrainAcc 0.9382	ValidAcc 0.9369	TestAcc 0.9338	BestValid 0.9420
	Epoch 4400:	Loss 0.4124	TrainAcc 0.9428	ValidAcc 0.9427	TestAcc 0.9394	BestValid 0.9427
	Epoch 4425:	Loss 0.4076	TrainAcc 0.9385	ValidAcc 0.9381	TestAcc 0.9348	BestValid 0.9427
	Epoch 4450:	Loss 0.4153	TrainAcc 0.9427	ValidAcc 0.9427	TestAcc 0.9391	BestValid 0.9427
	Epoch 4475:	Loss 0.4141	TrainAcc 0.9398	ValidAcc 0.9399	TestAcc 0.9365	BestValid 0.9427
	Epoch 4500:	Loss 0.4091	TrainAcc 0.9431	ValidAcc 0.9425	TestAcc 0.9392	BestValid 0.9427
	Epoch 4525:	Loss 0.4086	TrainAcc 0.9425	ValidAcc 0.9419	TestAcc 0.9391	BestValid 0.9427
	Epoch 4550:	Loss 0.4046	TrainAcc 0.9411	ValidAcc 0.9402	TestAcc 0.9372	BestValid 0.9427
	Epoch 4575:	Loss 0.4011	TrainAcc 0.9429	ValidAcc 0.9430	TestAcc 0.9398	BestValid 0.9430
	Epoch 4600:	Loss 0.4110	TrainAcc 0.9389	ValidAcc 0.9386	TestAcc 0.9350	BestValid 0.9430
	Epoch 4625:	Loss 0.4133	TrainAcc 0.9437	ValidAcc 0.9440	TestAcc 0.9399	BestValid 0.9440
	Epoch 4650:	Loss 0.4179	TrainAcc 0.9399	ValidAcc 0.9393	TestAcc 0.9352	BestValid 0.9440
	Epoch 4675:	Loss 0.4182	TrainAcc 0.9419	ValidAcc 0.9413	TestAcc 0.9381	BestValid 0.9440
	Epoch 4700:	Loss 0.4284	TrainAcc 0.9376	ValidAcc 0.9367	TestAcc 0.9332	BestValid 0.9440
	Epoch 4725:	Loss 0.4247	TrainAcc 0.9367	ValidAcc 0.9362	TestAcc 0.9335	BestValid 0.9440
	Epoch 4750:	Loss 0.4289	TrainAcc 0.9395	ValidAcc 0.9393	TestAcc 0.9366	BestValid 0.9440
	Epoch 4775:	Loss 0.4228	TrainAcc 0.9376	ValidAcc 0.9356	TestAcc 0.9336	BestValid 0.9440
	Epoch 4800:	Loss 0.4130	TrainAcc 0.9423	ValidAcc 0.9427	TestAcc 0.9389	BestValid 0.9440
	Epoch 4825:	Loss 0.4124	TrainAcc 0.9405	ValidAcc 0.9394	TestAcc 0.9359	BestValid 0.9440
	Epoch 4850:	Loss 0.4058	TrainAcc 0.9435	ValidAcc 0.9431	TestAcc 0.9400	BestValid 0.9440
	Epoch 4875:	Loss 0.4099	TrainAcc 0.9422	ValidAcc 0.9413	TestAcc 0.9374	BestValid 0.9440
	Epoch 4900:	Loss 0.4061	TrainAcc 0.9430	ValidAcc 0.9424	TestAcc 0.9394	BestValid 0.9440
	Epoch 4925:	Loss 0.4029	TrainAcc 0.9424	ValidAcc 0.9414	TestAcc 0.9374	BestValid 0.9440
	Epoch 4950:	Loss 0.4014	TrainAcc 0.9425	ValidAcc 0.9414	TestAcc 0.9391	BestValid 0.9440
	Epoch 4975:	Loss 0.4061	TrainAcc 0.9421	ValidAcc 0.9415	TestAcc 0.9374	BestValid 0.9440
	Epoch 5000:	Loss 0.4062	TrainAcc 0.9437	ValidAcc 0.9426	TestAcc 0.9397	BestValid 0.9440
Node 0, Pre/Post-Pipelining: 0.123 / 1.507 ms, Bubble: 80.194 ms, Compute: 289.178 ms, Comm: 32.833 ms, Imbalance: 11.955 ms
Node 1, Pre/Post-Pipelining: 0.207 / 1.416 ms, Bubble: 80.684 ms, Compute: 258.628 ms, Comm: 43.000 ms, Imbalance: 32.519 ms
Node 3, Pre/Post-Pipelining: 0.261 / 1.383 ms, Bubble: 80.358 ms, Compute: 256.336 ms, Comm: 46.726 ms, Imbalance: 31.879 ms
Node 2, Pre/Post-Pipelining: 0.214 / 1.396 ms, Bubble: 81.278 ms, Compute: 250.136 ms, Comm: 47.511 ms, Imbalance: 36.542 ms
Node 7, Pre/Post-Pipelining: 0.275 / 16.725 ms, Bubble: 68.169 ms, Compute: 279.216 ms, Comm: 32.304 ms, Imbalance: 20.080 ms
Node 4, Pre/Post-Pipelining: 0.210 / 1.416 ms, Bubble: 80.681 ms, Compute: 250.010 ms, Comm: 46.246 ms, Imbalance: 38.421 ms
Node 5, Pre/Post-Pipelining: 0.217 / 1.411 ms, Bubble: 81.683 ms, Compute: 254.635 ms, Comm: 45.642 ms, Imbalance: 33.764 ms
Node 6, Pre/Post-Pipelining: 0.287 / 1.379 ms, Bubble: 83.167 ms, Compute: 253.743 ms, Comm: 41.504 ms, Imbalance: 37.763 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.123 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.507 ms
Cluster-Wide Average, Bubble: 80.194 ms
Cluster-Wide Average, Compute: 289.178 ms
Cluster-Wide Average, Communication: 32.833 ms
Cluster-Wide Average, Imbalance: 11.955 ms
Node 0, GPU memory consumption: 8.059 GB
Node 3, GPU memory consumption: 6.018 GB
Node 2, GPU memory consumption: 6.042 GB
Node 1, GPU memory consumption: 6.042 GB
Node 4, GPU memory consumption: 6.018 GB
Node 5, GPU memory consumption: 6.042 GB
Node 6, GPU memory consumption: 6.042 GB
Node 7, GPU memory consumption: 6.171 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.910073 s---------------
------------------------node id 1,  per-epoch time: 0.910073 s---------------
------------------------node id 2,  per-epoch time: 0.910073 s---------------
------------------------node id 3,  per-epoch time: 0.910073 s---------------
------------------------node id 4,  per-epoch time: 0.910073 s---------------
------------------------node id 5,  per-epoch time: 0.910073 s---------------
------------------------node id 6,  per-epoch time: 0.910073 s---------------
------------------------node id 7,  per-epoch time: 0.910073 s---------------
************ Profiling Results ************
	Bubble: 648.919531 (ms) (71.33 percentage)
	Compute: 259.660250 (ms) (28.54 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 1.110869 (ms) (0.12 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 2.431 GB
	Aggregated layer-level communication throughput: 497.340 Gbps
Highest valid_acc: 0.9440
Target test_acc: 0.9399
Epoch to reach the target acc: 4624
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
