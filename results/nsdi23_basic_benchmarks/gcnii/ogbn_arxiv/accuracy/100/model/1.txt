Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.110 seconds.
Building the CSC structure...
        It takes 0.111 seconds.
Building the CSC structure...
        It takes 0.111 seconds.
Building the CSC structure...
        It takes 0.111 seconds.
Building the CSC structure...
        It takes 0.112 seconds.
Building the CSC structure...
        It takes 0.112 seconds.
Building the CSC structure...
        It takes 0.112 seconds.
Building the CSC structure...
        It takes 0.115 seconds.
Building the CSC structure...
        It takes 0.067 seconds.
        It takes 0.065 seconds.
        It takes 0.067 seconds.
        It takes 0.069 seconds.
        It takes 0.069 seconds.
        It takes 0.070 seconds.
        It takes 0.068 seconds.
        It takes 0.069 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.223 seconds.
        It takes 0.222 seconds.
        It takes 0.223 seconds.
        It takes 0.223 seconds.
Building the Label Vector...Building the Label Vector...
Building the Label Vector...
Building the Label Vector...

        It takes 0.231 seconds.
        It takes 0.231 seconds.
        It takes 0.231 seconds.
        It takes 0.231 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.053 seconds.
        It takes 0.054 seconds.
        It takes 0.055 seconds.
        It takes 0.056 seconds.
        It takes 0.054 seconds.
        It takes 0.054 seconds.
        It takes 0.054 seconds.
        It takes 0.054 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 64.139 Gbps (per GPU), 513.112 Gbps (aggregated)
The layer-level communication performance: 64.099 Gbps (per GPU), 512.789 Gbps (aggregated)
The layer-level communication performance: 64.097 Gbps (per GPU), 512.776 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.063 Gbps (per GPU), 512.501 Gbps (aggregated)
The layer-level communication performance: 64.058 Gbps (per GPU), 512.464 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.029 Gbps (per GPU), 512.234 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.022 Gbps (per GPU), 512.179 Gbps (aggregated)
The layer-level communication performance: 64.017 Gbps (per GPU), 512.138 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.840 Gbps (per GPU), 1326.718 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.840 Gbps (per GPU), 1326.718 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.838 Gbps (per GPU), 1326.701 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.832 Gbps (per GPU), 1326.652 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.839 Gbps (per GPU), 1326.714 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.836 Gbps (per GPU), 1326.691 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.843 Gbps (per GPU), 1326.747 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.840 Gbps (per GPU), 1326.721 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.365 Gbps (per GPU), 914.923 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.366 Gbps (per GPU), 914.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.358 Gbps (per GPU), 914.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.365 Gbps (per GPU), 914.918 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.359 Gbps (per GPU), 914.869 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.365 Gbps (per GPU), 914.916 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.361 Gbps (per GPU), 914.886 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.350 Gbps (per GPU), 914.798 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.076 Gbps (per GPU), 360.608 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.076 Gbps (per GPU), 360.609 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.076 Gbps (per GPU), 360.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.076 Gbps (per GPU), 360.608 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.076 Gbps (per GPU), 360.608 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.075 Gbps (per GPU), 360.596 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.076 Gbps (per GPU), 360.606 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.074 Gbps (per GPU), 360.596 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.45ms  0.60ms  1.85  5.55K  0.06M
 chk_1  0.32ms  0.46ms  0.61ms  1.88  5.75K  0.05M
 chk_2  0.31ms  0.46ms  0.60ms  1.92  5.20K  0.07M
 chk_3  0.32ms  0.46ms  0.61ms  1.89  5.57K  0.06M
 chk_4  0.31ms  0.45ms  0.59ms  1.90  5.05K  0.08M
 chk_5  0.36ms  0.47ms  0.62ms  1.73  4.73K  0.11M
 chk_6  0.31ms  0.45ms  0.59ms  1.91  4.98K  0.08M
 chk_7  0.31ms  0.46ms  0.60ms  1.92  5.01K  0.09M
 chk_8  0.32ms  0.45ms  0.61ms  1.88  5.73K  0.05M
 chk_9  0.30ms  0.45ms  0.59ms  1.94  4.54K  0.11M
chk_10  0.32ms  0.45ms  0.61ms  1.93  5.36K  0.07M
chk_11  0.32ms  0.46ms  0.61ms  1.90  5.39K  0.08M
chk_12  0.33ms  0.45ms  0.61ms  1.86  5.77K  0.05M
chk_13  0.32ms  0.45ms  0.60ms  1.89  5.43K  0.06M
chk_14  0.32ms  0.44ms  0.60ms  1.87  5.46K  0.06M
chk_15  0.33ms  0.45ms  0.60ms  1.85  5.88K  0.04M
chk_16  0.32ms  0.45ms  0.60ms  1.90  5.50K  0.06M
chk_17  0.31ms  0.47ms  0.61ms  1.98  4.86K  0.09M
chk_18  0.32ms  0.48ms  0.63ms  1.98  5.39K  0.07M
chk_19  0.31ms  0.46ms  0.61ms  1.92  5.20K  0.07M
chk_20  0.32ms  0.45ms  0.60ms  1.86  5.51K  0.06M
chk_21  0.33ms  0.45ms  0.60ms  1.83  5.81K  0.05M
chk_22  0.32ms  0.45ms  0.60ms  1.88  5.32K  0.07M
chk_23  0.32ms  0.47ms  0.62ms  1.94  5.39K  0.07M
chk_24  0.30ms  0.45ms  0.59ms  1.97  4.62K  0.11M
chk_25  0.31ms  0.46ms  0.60ms  1.92  5.04K  0.08M
chk_26  0.30ms  0.45ms  0.59ms  1.94  4.55K  0.11M
chk_27  0.32ms  0.44ms  0.59ms  1.87  5.30K  0.06M
chk_28  0.32ms  0.45ms  0.60ms  1.86  5.58K  0.06M
chk_29  0.31ms  0.46ms  0.61ms  1.97  4.98K  0.09M
chk_30  0.32ms  0.45ms  0.60ms  1.88  5.50K  0.07M
chk_31  0.32ms  0.45ms  0.60ms  1.88  5.38K  0.07M
   Avg  0.32  0.45  0.60
   Max  0.36  0.48  0.63
   Min  0.30  0.44  0.59
 Ratio  1.18  1.09  1.07
   Var  0.00  0.00  0.00
Profiling takes 0.657 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 68.330 ms
Partition 0 [0, 5) has cost: 68.330 ms
Partition 1 [5, 9) has cost: 58.161 ms
Partition 2 [9, 13) has cost: 58.161 ms
Partition 3 [13, 17) has cost: 58.161 ms
Partition 4 [17, 21) has cost: 58.161 ms
Partition 5 [21, 25) has cost: 58.161 ms
Partition 6 [25, 29) has cost: 58.161 ms
Partition 7 [29, 33) has cost: 62.903 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 37.201 ms
GPU 0, Compute+Comm Time: 31.002 ms, Bubble Time: 6.199 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 27.516 ms, Bubble Time: 6.283 ms, Imbalance Overhead: 3.401 ms
GPU 2, Compute+Comm Time: 27.516 ms, Bubble Time: 6.410 ms, Imbalance Overhead: 3.275 ms
GPU 3, Compute+Comm Time: 27.516 ms, Bubble Time: 6.507 ms, Imbalance Overhead: 3.177 ms
GPU 4, Compute+Comm Time: 27.516 ms, Bubble Time: 6.605 ms, Imbalance Overhead: 3.079 ms
GPU 5, Compute+Comm Time: 27.516 ms, Bubble Time: 6.661 ms, Imbalance Overhead: 3.023 ms
GPU 6, Compute+Comm Time: 27.516 ms, Bubble Time: 6.734 ms, Imbalance Overhead: 2.951 ms
GPU 7, Compute+Comm Time: 28.576 ms, Bubble Time: 6.784 ms, Imbalance Overhead: 1.841 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 65.331 ms
GPU 0, Compute+Comm Time: 51.225 ms, Bubble Time: 11.906 ms, Imbalance Overhead: 2.200 ms
GPU 1, Compute+Comm Time: 47.543 ms, Bubble Time: 11.863 ms, Imbalance Overhead: 5.925 ms
GPU 2, Compute+Comm Time: 47.543 ms, Bubble Time: 11.732 ms, Imbalance Overhead: 6.056 ms
GPU 3, Compute+Comm Time: 47.543 ms, Bubble Time: 11.684 ms, Imbalance Overhead: 6.104 ms
GPU 4, Compute+Comm Time: 47.543 ms, Bubble Time: 11.551 ms, Imbalance Overhead: 6.238 ms
GPU 5, Compute+Comm Time: 47.543 ms, Bubble Time: 11.423 ms, Imbalance Overhead: 6.365 ms
GPU 6, Compute+Comm Time: 47.543 ms, Bubble Time: 11.218 ms, Imbalance Overhead: 6.570 ms
GPU 7, Compute+Comm Time: 54.225 ms, Bubble Time: 11.106 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 107.658 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 126.491 ms
Partition 0 [0, 9) has cost: 126.491 ms
Partition 1 [9, 17) has cost: 116.323 ms
Partition 2 [17, 25) has cost: 116.323 ms
Partition 3 [25, 33) has cost: 121.065 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.939 ms
GPU 0, Compute+Comm Time: 36.256 ms, Bubble Time: 6.581 ms, Imbalance Overhead: 0.102 ms
GPU 1, Compute+Comm Time: 34.501 ms, Bubble Time: 6.704 ms, Imbalance Overhead: 1.734 ms
GPU 2, Compute+Comm Time: 34.501 ms, Bubble Time: 6.772 ms, Imbalance Overhead: 1.665 ms
GPU 3, Compute+Comm Time: 35.030 ms, Bubble Time: 6.828 ms, Imbalance Overhead: 1.081 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 68.903 ms
GPU 0, Compute+Comm Time: 56.585 ms, Bubble Time: 11.005 ms, Imbalance Overhead: 1.313 ms
GPU 1, Compute+Comm Time: 54.737 ms, Bubble Time: 10.889 ms, Imbalance Overhead: 3.277 ms
GPU 2, Compute+Comm Time: 54.737 ms, Bubble Time: 10.782 ms, Imbalance Overhead: 3.384 ms
GPU 3, Compute+Comm Time: 58.145 ms, Bubble Time: 10.534 ms, Imbalance Overhead: 0.224 ms
    The estimated cost with 2 DP ways is 117.433 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 242.814 ms
Partition 0 [0, 17) has cost: 242.814 ms
Partition 1 [17, 33) has cost: 237.387 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 67.051 ms
GPU 0, Compute+Comm Time: 58.807 ms, Bubble Time: 7.300 ms, Imbalance Overhead: 0.943 ms
GPU 1, Compute+Comm Time: 58.191 ms, Bubble Time: 7.486 ms, Imbalance Overhead: 1.373 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 90.897 ms
GPU 0, Compute+Comm Time: 79.235 ms, Bubble Time: 10.153 ms, Imbalance Overhead: 1.509 ms
GPU 1, Compute+Comm Time: 80.048 ms, Bubble Time: 9.911 ms, Imbalance Overhead: 0.938 ms
    The estimated cost with 4 DP ways is 165.845 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 480.201 ms
Partition 0 [0, 33) has cost: 480.201 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 203.479 ms
GPU 0, Compute+Comm Time: 203.479 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 224.684 ms
GPU 0, Compute+Comm Time: 224.684 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 449.571 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.7827	TrainAcc 0.0643	ValidAcc 0.0548	TestAcc 0.0558	BestValid 0.0548
	Epoch 50:	Loss 3.7015	TrainAcc 0.0571	ValidAcc 0.0416	TestAcc 0.0411	BestValid 0.0548
	Epoch 75:	Loss 3.6946	TrainAcc 0.0484	ValidAcc 0.0332	TestAcc 0.0295	BestValid 0.0548
	Epoch 100:	Loss 3.7016	TrainAcc 0.0565	ValidAcc 0.0401	TestAcc 0.0346	BestValid 0.0548
Node 1, Pre/Post-Pipelining: 0.842 / 16.219 ms, Bubble: 34.426 ms, Compute: 59.880 ms, Comm: 23.449 ms, Imbalance: 19.891 ms
Node 4, Pre/Post-Pipelining: 0.820 / 16.282 ms, Bubble: 33.415 ms, Compute: 61.929 ms, Comm: 29.367 ms, Imbalance: 13.276 ms
Node 2, Pre/Post-Pipelining: 0.844 / 16.251 ms, Bubble: 34.343 ms, Compute: 61.340 ms, Comm: 27.605 ms, Imbalance: 14.474 ms
Node 5, Pre/Post-Pipelining: 0.832 / 16.288 ms, Bubble: 33.678 ms, Compute: 61.471 ms, Comm: 27.916 ms, Imbalance: 14.730 ms
Node 3, Pre/Post-Pipelining: 0.841 / 16.266 ms, Bubble: 33.358 ms, Compute: 62.211 ms, Comm: 29.315 ms, Imbalance: 12.740 ms
Node 6, Pre/Post-Pipelining: 0.827 / 16.298 ms, Bubble: 33.676 ms, Compute: 61.134 ms, Comm: 23.516 ms, Imbalance: 19.350 ms
Node 0, Pre/Post-Pipelining: 0.851 / 16.204 ms, Bubble: 34.886 ms, Compute: 69.719 ms, Comm: 17.557 ms, Imbalance: 15.296 ms
Node 7, Pre/Post-Pipelining: 0.818 / 30.594 ms, Bubble: 19.797 ms, Compute: 78.560 ms, Comm: 17.220 ms, Imbalance: 7.639 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.851 ms
Cluster-Wide Average, Post-Pipelining Overhead: 16.204 ms
Cluster-Wide Average, Bubble: 34.886 ms
Cluster-Wide Average, Compute: 69.719 ms
Cluster-Wide Average, Communication: 17.557 ms
Cluster-Wide Average, Imbalance: 15.296 ms
Node 0, GPU memory consumption: 5.018 GB
Node 6, GPU memory consumption: 3.647 GB
Node 1, GPU memory consumption: 3.647 GB
Node 4, GPU memory consumption: 3.624 GB
Node 5, GPU memory consumption: 3.647 GB
Node 2, GPU memory consumption: 3.647 GB
Node 7, GPU memory consumption: 3.727 GB
Node 3, GPU memory consumption: 3.624 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.489192 s---------------
------------------------node id 1,  per-epoch time: 0.489191 s---------------
------------------------node id 2,  per-epoch time: 0.489191 s---------------
------------------------node id 3,  per-epoch time: 0.489190 s---------------
------------------------node id 4,  per-epoch time: 0.489190 s---------------
------------------------node id 5,  per-epoch time: 0.489191 s---------------
------------------------node id 6,  per-epoch time: 0.489190 s---------------
------------------------node id 7,  per-epoch time: 0.489191 s---------------
************ Profiling Results ************
	Bubble: 412.317011 (ms) (86.21 percentage)
	Compute: 63.247556 (ms) (13.22 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.688608 (ms) (0.56 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.766 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 1.776 GB
	Aggregated layer-level communication throughput: 619.482 Gbps
Highest valid_acc: 0.0548
Target test_acc: 0.0558
Epoch to reach the target acc: 24
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
