Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.045 seconds.
        It takes 0.050 seconds.
        It takes 0.050 seconds.
        It takes 0.053 seconds.
        It takes 0.055 seconds.
        It takes 0.056 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.384 Gbps (per GPU), 507.069 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.339 Gbps (per GPU), 506.714 Gbps (aggregated)
The layer-level communication performance: 63.337 Gbps (per GPU), 506.699 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.303 Gbps (per GPU), 506.422 Gbps (aggregated)
The layer-level communication performance: 63.299 Gbps (per GPU), 506.389 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.267 Gbps (per GPU), 506.132 Gbps (aggregated)
The layer-level communication performance: 63.260 Gbps (per GPU), 506.078 Gbps (aggregated)
The layer-level communication performance: 63.255 Gbps (per GPU), 506.042 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.620 Gbps (per GPU), 1316.961 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.612 Gbps (per GPU), 1316.899 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.616 Gbps (per GPU), 1316.929 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.613 Gbps (per GPU), 1316.906 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.616 Gbps (per GPU), 1316.928 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.617 Gbps (per GPU), 1316.938 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.617 Gbps (per GPU), 1316.935 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.620 Gbps (per GPU), 1316.958 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.746 Gbps (per GPU), 909.966 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.745 Gbps (per GPU), 909.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.745 Gbps (per GPU), 909.957 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.743 Gbps (per GPU), 909.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.742 Gbps (per GPU), 909.939 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.736 Gbps (per GPU), 909.885 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.740 Gbps (per GPU), 909.918 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.733 Gbps (per GPU), 909.864 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.710 Gbps (per GPU), 365.682 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.710 Gbps (per GPU), 365.680 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.710 Gbps (per GPU), 365.681 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.710 Gbps (per GPU), 365.679 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.710 Gbps (per GPU), 365.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.710 Gbps (per GPU), 365.680 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.710 Gbps (per GPU), 365.680 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.709 Gbps (per GPU), 365.674 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.33ms  0.46ms  0.61ms  1.87  5.55K  0.06M
 chk_1  0.33ms  0.46ms  0.62ms  1.88  5.75K  0.05M
 chk_2  0.32ms  0.46ms  0.62ms  1.93  5.20K  0.07M
 chk_3  0.33ms  0.47ms  0.62ms  1.90  5.57K  0.06M
 chk_4  0.32ms  0.46ms  0.61ms  1.92  5.05K  0.08M
 chk_5  0.31ms  0.48ms  0.63ms  2.01  4.73K  0.11M
 chk_6  0.32ms  0.46ms  0.60ms  1.91  4.98K  0.08M
 chk_7  0.32ms  0.46ms  0.61ms  1.93  5.01K  0.09M
 chk_8  0.33ms  0.46ms  0.62ms  1.88  5.73K  0.05M
 chk_9  0.31ms  0.46ms  0.60ms  1.96  4.54K  0.11M
chk_10  0.33ms  0.46ms  0.62ms  1.89  5.36K  0.07M
chk_11  0.33ms  0.47ms  0.62ms  1.91  5.39K  0.08M
chk_12  0.33ms  0.46ms  0.62ms  1.88  5.77K  0.05M
chk_13  0.32ms  0.46ms  0.62ms  1.90  5.43K  0.06M
chk_14  0.32ms  0.45ms  0.61ms  1.89  5.46K  0.06M
chk_15  0.33ms  0.46ms  0.62ms  1.86  5.88K  0.04M
chk_16  0.32ms  0.46ms  0.62ms  1.91  5.50K  0.06M
chk_17  0.32ms  0.48ms  0.63ms  2.00  4.86K  0.09M
chk_18  0.32ms  0.49ms  0.64ms  1.99  5.39K  0.07M
chk_19  0.32ms  0.47ms  0.62ms  1.92  5.20K  0.07M
chk_20  0.33ms  0.46ms  0.61ms  1.87  5.51K  0.06M
chk_21  0.33ms  0.45ms  0.61ms  1.84  5.81K  0.05M
chk_22  0.32ms  0.46ms  0.61ms  1.90  5.32K  0.07M
chk_23  0.33ms  0.48ms  0.63ms  1.94  5.39K  0.07M
chk_24  0.31ms  0.46ms  0.61ms  1.97  4.62K  0.11M
chk_25  0.32ms  0.46ms  0.61ms  1.92  5.04K  0.08M
chk_26  0.31ms  0.46ms  0.60ms  1.94  4.55K  0.11M
chk_27  0.32ms  0.45ms  0.60ms  1.88  5.30K  0.06M
chk_28  0.33ms  0.46ms  0.61ms  1.86  5.58K  0.06M
chk_29  0.32ms  0.47ms  0.62ms  1.98  4.98K  0.09M
chk_30  0.33ms  0.46ms  0.62ms  1.89  5.50K  0.07M
chk_31  0.32ms  0.46ms  0.61ms  1.90  5.38K  0.07M
   Avg  0.32  0.46  0.62
   Max  0.33  0.49  0.64
   Min  0.31  0.45  0.60
 Ratio  1.08  1.08  1.07
   Var  0.00  0.00  0.00
Profiling takes 0.666 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 69.664 ms
Partition 0 [0, 5) has cost: 69.664 ms
Partition 1 [5, 9) has cost: 59.344 ms
Partition 2 [9, 13) has cost: 59.344 ms
Partition 3 [13, 17) has cost: 59.344 ms
Partition 4 [17, 21) has cost: 59.344 ms
Partition 5 [21, 25) has cost: 59.344 ms
Partition 6 [25, 29) has cost: 59.344 ms
Partition 7 [29, 33) has cost: 64.246 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 37.933 ms
GPU 0, Compute+Comm Time: 31.603 ms, Bubble Time: 6.330 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 28.047 ms, Bubble Time: 6.415 ms, Imbalance Overhead: 3.471 ms
GPU 2, Compute+Comm Time: 28.047 ms, Bubble Time: 6.537 ms, Imbalance Overhead: 3.349 ms
GPU 3, Compute+Comm Time: 28.047 ms, Bubble Time: 6.631 ms, Imbalance Overhead: 3.255 ms
GPU 4, Compute+Comm Time: 28.047 ms, Bubble Time: 6.726 ms, Imbalance Overhead: 3.160 ms
GPU 5, Compute+Comm Time: 28.047 ms, Bubble Time: 6.785 ms, Imbalance Overhead: 3.101 ms
GPU 6, Compute+Comm Time: 28.047 ms, Bubble Time: 6.856 ms, Imbalance Overhead: 3.031 ms
GPU 7, Compute+Comm Time: 29.158 ms, Bubble Time: 6.912 ms, Imbalance Overhead: 1.863 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 66.470 ms
GPU 0, Compute+Comm Time: 52.187 ms, Bubble Time: 12.069 ms, Imbalance Overhead: 2.214 ms
GPU 1, Compute+Comm Time: 48.396 ms, Bubble Time: 12.025 ms, Imbalance Overhead: 6.049 ms
GPU 2, Compute+Comm Time: 48.396 ms, Bubble Time: 11.936 ms, Imbalance Overhead: 6.138 ms
GPU 3, Compute+Comm Time: 48.396 ms, Bubble Time: 11.889 ms, Imbalance Overhead: 6.184 ms
GPU 4, Compute+Comm Time: 48.396 ms, Bubble Time: 11.755 ms, Imbalance Overhead: 6.319 ms
GPU 5, Compute+Comm Time: 48.396 ms, Bubble Time: 11.631 ms, Imbalance Overhead: 6.443 ms
GPU 6, Compute+Comm Time: 48.396 ms, Bubble Time: 11.425 ms, Imbalance Overhead: 6.648 ms
GPU 7, Compute+Comm Time: 55.159 ms, Bubble Time: 11.311 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 109.624 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 129.008 ms
Partition 0 [0, 9) has cost: 129.008 ms
Partition 1 [9, 17) has cost: 118.689 ms
Partition 2 [17, 25) has cost: 118.689 ms
Partition 3 [25, 33) has cost: 123.590 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 43.657 ms
GPU 0, Compute+Comm Time: 36.865 ms, Bubble Time: 6.696 ms, Imbalance Overhead: 0.097 ms
GPU 1, Compute+Comm Time: 35.075 ms, Bubble Time: 6.807 ms, Imbalance Overhead: 1.774 ms
GPU 2, Compute+Comm Time: 35.075 ms, Bubble Time: 6.871 ms, Imbalance Overhead: 1.710 ms
GPU 3, Compute+Comm Time: 35.632 ms, Bubble Time: 6.924 ms, Imbalance Overhead: 1.101 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 70.029 ms
GPU 0, Compute+Comm Time: 57.551 ms, Bubble Time: 11.146 ms, Imbalance Overhead: 1.332 ms
GPU 1, Compute+Comm Time: 55.652 ms, Bubble Time: 11.061 ms, Imbalance Overhead: 3.316 ms
GPU 2, Compute+Comm Time: 55.652 ms, Bubble Time: 10.955 ms, Imbalance Overhead: 3.422 ms
GPU 3, Compute+Comm Time: 59.077 ms, Bubble Time: 10.718 ms, Imbalance Overhead: 0.234 ms
    The estimated cost with 2 DP ways is 119.370 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 247.697 ms
Partition 0 [0, 17) has cost: 247.697 ms
Partition 1 [17, 33) has cost: 242.279 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 67.854 ms
GPU 0, Compute+Comm Time: 59.491 ms, Bubble Time: 7.394 ms, Imbalance Overhead: 0.969 ms
GPU 1, Compute+Comm Time: 58.873 ms, Bubble Time: 7.570 ms, Imbalance Overhead: 1.411 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.147 ms
GPU 0, Compute+Comm Time: 80.335 ms, Bubble Time: 10.297 ms, Imbalance Overhead: 1.515 ms
GPU 1, Compute+Comm Time: 81.112 ms, Bubble Time: 10.055 ms, Imbalance Overhead: 0.980 ms
    The estimated cost with 4 DP ways is 168.001 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 489.977 ms
Partition 0 [0, 33) has cost: 489.977 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 201.353 ms
GPU 0, Compute+Comm Time: 201.353 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 223.005 ms
GPU 0, Compute+Comm Time: 223.005 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 445.576 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.7760	TrainAcc 0.1850	ValidAcc 0.0845	TestAcc 0.0688	BestValid 0.0845
	Epoch 50:	Loss 3.7043	TrainAcc 0.1466	ValidAcc 0.0610	TestAcc 0.0477	BestValid 0.0845
	Epoch 75:	Loss 3.6954	TrainAcc 0.1252	ValidAcc 0.0510	TestAcc 0.0401	BestValid 0.0845
	Epoch 100:	Loss 3.6985	TrainAcc 0.1128	ValidAcc 0.0446	TestAcc 0.0356	BestValid 0.0845
Node 3, Pre/Post-Pipelining: 0.834 / 16.885 ms, Bubble: 33.810 ms, Compute: 61.873 ms, Comm: 29.461 ms, Imbalance: 14.166 ms
Node 2, Pre/Post-Pipelining: 0.827 / 16.872 ms, Bubble: 34.752 ms, Compute: 61.808 ms, Comm: 27.736 ms, Imbalance: 15.128 ms
Node 5, Pre/Post-Pipelining: 0.831 / 16.897 ms, Bubble: 33.946 ms, Compute: 61.363 ms, Comm: 27.902 ms, Imbalance: 16.273 ms
Node 0, Pre/Post-Pipelining: 0.835 / 16.855 ms, Bubble: 35.304 ms, Compute: 71.661 ms, Comm: 17.658 ms, Imbalance: 14.479 ms
Node 6, Pre/Post-Pipelining: 0.811 / 16.917 ms, Bubble: 33.975 ms, Compute: 61.199 ms, Comm: 23.618 ms, Imbalance: 20.608 ms
Node 1, Pre/Post-Pipelining: 0.837 / 16.832 ms, Bubble: 34.940 ms, Compute: 60.316 ms, Comm: 23.623 ms, Imbalance: 20.488 ms
Node 7, Pre/Post-Pipelining: 0.821 / 31.259 ms, Bubble: 19.784 ms, Compute: 80.476 ms, Comm: 17.249 ms, Imbalance: 7.267 ms
Node 4, Pre/Post-Pipelining: 0.825 / 16.889 ms, Bubble: 33.723 ms, Compute: 61.802 ms, Comm: 29.556 ms, Imbalance: 14.554 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.835 ms
Cluster-Wide Average, Post-Pipelining Overhead: 16.855 ms
Cluster-Wide Average, Bubble: 35.304 ms
Cluster-Wide Average, Compute: 71.661 ms
Cluster-Wide Average, Communication: 17.658 ms
Cluster-Wide Average, Imbalance: 14.479 ms
Node 2, GPU memory consumption: 3.647 GB
Node 0, GPU memory consumption: 5.018 GB
Node 3, GPU memory consumption: 3.624 GB
Node 1, GPU memory consumption: 3.647 GB
Node 6, GPU memory consumption: 3.647 GB
Node 7, GPU memory consumption: 3.727 GB
Node 5, GPU memory consumption: 3.647 GB
Node 4, GPU memory consumption: 3.624 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.492322 s---------------
------------------------node id 1,  per-epoch time: 0.492322 s---------------
------------------------node id 2,  per-epoch time: 0.492321 s---------------
------------------------node id 3,  per-epoch time: 0.492319 s---------------
------------------------node id 4,  per-epoch time: 0.492322 s---------------
------------------------node id 5,  per-epoch time: 0.492321 s---------------
------------------------node id 6,  per-epoch time: 0.492320 s---------------
------------------------node id 7,  per-epoch time: 0.492322 s---------------
************ Profiling Results ************
	Bubble: 415.114290 (ms) (86.20 percentage)
	Compute: 63.734751 (ms) (13.24 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.712813 (ms) (0.56 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.766 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 1.776 GB
	Aggregated layer-level communication throughput: 616.787 Gbps
Highest valid_acc: 0.0845
Target test_acc: 0.0688
Epoch to reach the target acc: 24
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
