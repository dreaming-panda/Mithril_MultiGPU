Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.044 seconds.
        It takes 0.043 seconds.
        It takes 0.050 seconds.
        It takes 0.052 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
        It takes 0.053 seconds.
Building the Feature Vector...
        It takes 0.057 seconds.
        It takes 0.054 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.025 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.761 Gbps (per GPU), 510.085 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.715 Gbps (per GPU), 509.722 Gbps (aggregated)
The layer-level communication performance: 63.714 Gbps (per GPU), 509.709 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.676 Gbps (per GPU), 509.409 Gbps (aggregated)
The layer-level communication performance: 63.671 Gbps (per GPU), 509.372 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.641 Gbps (per GPU), 509.128 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.634 Gbps (per GPU), 509.073 Gbps (aggregated)
The layer-level communication performance: 63.630 Gbps (per GPU), 509.037 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.878 Gbps (per GPU), 1327.026 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.872 Gbps (per GPU), 1326.977 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.881 Gbps (per GPU), 1327.046 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.873 Gbps (per GPU), 1326.987 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.879 Gbps (per GPU), 1327.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.882 Gbps (per GPU), 1327.055 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.879 Gbps (per GPU), 1327.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.878 Gbps (per GPU), 1327.023 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.502 Gbps (per GPU), 916.016 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.504 Gbps (per GPU), 916.035 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.504 Gbps (per GPU), 916.034 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.502 Gbps (per GPU), 916.017 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.497 Gbps (per GPU), 915.972 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.493 Gbps (per GPU), 915.940 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.491 Gbps (per GPU), 915.927 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.474 Gbps (per GPU), 915.793 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 46.296 Gbps (per GPU), 370.368 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.296 Gbps (per GPU), 370.369 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.296 Gbps (per GPU), 370.368 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.296 Gbps (per GPU), 370.367 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.296 Gbps (per GPU), 370.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.296 Gbps (per GPU), 370.365 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.295 Gbps (per GPU), 370.363 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.294 Gbps (per GPU), 370.348 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.44ms  0.59ms  1.86  5.55K  0.06M
 chk_1  0.32ms  0.45ms  0.60ms  1.89  5.75K  0.05M
 chk_2  0.31ms  0.45ms  0.60ms  1.92  5.20K  0.07M
 chk_3  0.32ms  0.46ms  0.61ms  1.90  5.57K  0.06M
 chk_4  0.31ms  0.45ms  0.59ms  1.92  5.05K  0.08M
 chk_5  0.30ms  0.47ms  0.61ms  2.01  4.73K  0.11M
 chk_6  0.31ms  0.44ms  0.59ms  1.91  4.98K  0.08M
 chk_7  0.31ms  0.45ms  0.60ms  1.93  5.01K  0.09M
 chk_8  0.32ms  0.45ms  0.60ms  1.87  5.73K  0.05M
 chk_9  0.30ms  0.45ms  0.59ms  1.95  4.54K  0.11M
chk_10  0.32ms  0.45ms  0.60ms  1.90  5.36K  0.07M
chk_11  0.32ms  0.46ms  0.61ms  1.92  5.39K  0.08M
chk_12  0.32ms  0.45ms  0.60ms  1.86  5.77K  0.05M
chk_13  0.32ms  0.45ms  0.60ms  1.89  5.43K  0.06M
chk_14  0.31ms  0.44ms  0.59ms  1.88  5.46K  0.06M
chk_15  0.32ms  0.45ms  0.60ms  1.85  5.88K  0.04M
chk_16  0.32ms  0.45ms  0.60ms  1.89  5.50K  0.06M
chk_17  0.31ms  0.47ms  0.61ms  1.98  4.86K  0.09M
chk_18  0.32ms  0.48ms  0.62ms  1.97  5.39K  0.07M
chk_19  0.31ms  0.45ms  0.60ms  1.92  5.20K  0.07M
chk_20  0.32ms  0.44ms  0.59ms  1.85  5.51K  0.06M
chk_21  0.32ms  0.44ms  0.59ms  1.83  5.81K  0.05M
chk_22  0.31ms  0.45ms  0.59ms  1.89  5.32K  0.07M
chk_23  0.32ms  0.47ms  0.61ms  1.93  5.39K  0.07M
chk_24  0.30ms  0.45ms  0.59ms  1.96  4.62K  0.11M
chk_25  0.31ms  0.45ms  0.60ms  1.92  5.04K  0.08M
chk_26  0.30ms  0.44ms  0.58ms  1.95  4.55K  0.11M
chk_27  0.31ms  0.44ms  0.58ms  1.87  5.30K  0.06M
chk_28  0.32ms  0.44ms  0.59ms  1.86  5.58K  0.06M
chk_29  0.31ms  0.46ms  0.60ms  1.96  4.98K  0.09M
chk_30  0.32ms  0.45ms  0.60ms  1.88  5.50K  0.07M
chk_31  0.32ms  0.45ms  0.60ms  1.89  5.38K  0.07M
   Avg  0.31  0.45  0.60
   Max  0.32  0.48  0.62
   Min  0.30  0.44  0.58
 Ratio  1.08  1.09  1.07
   Var  0.00  0.00  0.00
Profiling takes 0.655 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 67.802 ms
Partition 0 [0, 5) has cost: 67.802 ms
Partition 1 [5, 9) has cost: 57.751 ms
Partition 2 [9, 13) has cost: 57.751 ms
Partition 3 [13, 17) has cost: 57.751 ms
Partition 4 [17, 21) has cost: 57.751 ms
Partition 5 [21, 25) has cost: 57.751 ms
Partition 6 [25, 29) has cost: 57.751 ms
Partition 7 [29, 33) has cost: 62.468 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 37.107 ms
GPU 0, Compute+Comm Time: 30.916 ms, Bubble Time: 6.192 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 27.453 ms, Bubble Time: 6.271 ms, Imbalance Overhead: 3.383 ms
GPU 2, Compute+Comm Time: 27.453 ms, Bubble Time: 6.394 ms, Imbalance Overhead: 3.261 ms
GPU 3, Compute+Comm Time: 27.453 ms, Bubble Time: 6.486 ms, Imbalance Overhead: 3.169 ms
GPU 4, Compute+Comm Time: 27.453 ms, Bubble Time: 6.579 ms, Imbalance Overhead: 3.075 ms
GPU 5, Compute+Comm Time: 27.453 ms, Bubble Time: 6.636 ms, Imbalance Overhead: 3.019 ms
GPU 6, Compute+Comm Time: 27.453 ms, Bubble Time: 6.701 ms, Imbalance Overhead: 2.953 ms
GPU 7, Compute+Comm Time: 28.508 ms, Bubble Time: 6.751 ms, Imbalance Overhead: 1.848 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 64.929 ms
GPU 0, Compute+Comm Time: 50.958 ms, Bubble Time: 11.776 ms, Imbalance Overhead: 2.195 ms
GPU 1, Compute+Comm Time: 47.296 ms, Bubble Time: 11.739 ms, Imbalance Overhead: 5.893 ms
GPU 2, Compute+Comm Time: 47.296 ms, Bubble Time: 11.654 ms, Imbalance Overhead: 5.978 ms
GPU 3, Compute+Comm Time: 47.296 ms, Bubble Time: 11.599 ms, Imbalance Overhead: 6.033 ms
GPU 4, Compute+Comm Time: 47.296 ms, Bubble Time: 11.476 ms, Imbalance Overhead: 6.156 ms
GPU 5, Compute+Comm Time: 47.296 ms, Bubble Time: 11.351 ms, Imbalance Overhead: 6.281 ms
GPU 6, Compute+Comm Time: 47.296 ms, Bubble Time: 11.154 ms, Imbalance Overhead: 6.478 ms
GPU 7, Compute+Comm Time: 53.885 ms, Bubble Time: 11.044 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 107.138 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 125.553 ms
Partition 0 [0, 9) has cost: 125.553 ms
Partition 1 [9, 17) has cost: 115.502 ms
Partition 2 [17, 25) has cost: 115.502 ms
Partition 3 [25, 33) has cost: 120.219 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.842 ms
GPU 0, Compute+Comm Time: 36.162 ms, Bubble Time: 6.566 ms, Imbalance Overhead: 0.115 ms
GPU 1, Compute+Comm Time: 34.419 ms, Bubble Time: 6.686 ms, Imbalance Overhead: 1.737 ms
GPU 2, Compute+Comm Time: 34.419 ms, Bubble Time: 6.753 ms, Imbalance Overhead: 1.670 ms
GPU 3, Compute+Comm Time: 34.946 ms, Bubble Time: 6.797 ms, Imbalance Overhead: 1.099 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 68.513 ms
GPU 0, Compute+Comm Time: 56.305 ms, Bubble Time: 10.882 ms, Imbalance Overhead: 1.326 ms
GPU 1, Compute+Comm Time: 54.470 ms, Bubble Time: 10.810 ms, Imbalance Overhead: 3.233 ms
GPU 2, Compute+Comm Time: 54.470 ms, Bubble Time: 10.712 ms, Imbalance Overhead: 3.331 ms
GPU 3, Compute+Comm Time: 57.810 ms, Bubble Time: 10.476 ms, Imbalance Overhead: 0.226 ms
    The estimated cost with 2 DP ways is 116.923 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 241.056 ms
Partition 0 [0, 17) has cost: 241.056 ms
Partition 1 [17, 33) has cost: 235.721 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 66.950 ms
GPU 0, Compute+Comm Time: 58.684 ms, Bubble Time: 7.285 ms, Imbalance Overhead: 0.980 ms
GPU 1, Compute+Comm Time: 58.077 ms, Bubble Time: 7.468 ms, Imbalance Overhead: 1.405 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 90.590 ms
GPU 0, Compute+Comm Time: 78.973 ms, Bubble Time: 10.090 ms, Imbalance Overhead: 1.528 ms
GPU 1, Compute+Comm Time: 79.738 ms, Bubble Time: 9.865 ms, Imbalance Overhead: 0.988 ms
    The estimated cost with 4 DP ways is 165.417 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 476.777 ms
Partition 0 [0, 33) has cost: 476.777 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 198.506 ms
GPU 0, Compute+Comm Time: 198.506 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 219.538 ms
GPU 0, Compute+Comm Time: 219.538 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 438.946 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.7824	TrainAcc 0.1304	ValidAcc 0.2347	TestAcc 0.2210	BestValid 0.2347
	Epoch 50:	Loss 3.7067	TrainAcc 0.0703	ValidAcc 0.0879	TestAcc 0.1049	BestValid 0.2347
	Epoch 75:	Loss 3.6988	TrainAcc 0.0671	ValidAcc 0.0659	TestAcc 0.0807	BestValid 0.2347
	Epoch 100:	Loss 3.6983	TrainAcc 0.0654	ValidAcc 0.0650	TestAcc 0.0828	BestValid 0.2347
Node 0, Pre/Post-Pipelining: 0.857 / 16.814 ms, Bubble: 35.185 ms, Compute: 69.211 ms, Comm: 17.515 ms, Imbalance: 16.472 ms
Node 1, Pre/Post-Pipelining: 0.851 / 16.834 ms, Bubble: 34.606 ms, Compute: 60.298 ms, Comm: 23.616 ms, Imbalance: 20.020 ms
Node 3, Pre/Post-Pipelining: 0.846 / 16.891 ms, Bubble: 33.439 ms, Compute: 62.112 ms, Comm: 29.467 ms, Imbalance: 13.537 ms
Node 2, Pre/Post-Pipelining: 0.849 / 16.840 ms, Bubble: 34.499 ms, Compute: 60.067 ms, Comm: 27.781 ms, Imbalance: 16.359 ms
Node 4, Pre/Post-Pipelining: 0.841 / 16.878 ms, Bubble: 33.478 ms, Compute: 61.187 ms, Comm: 29.552 ms, Imbalance: 14.721 ms
Node 5, Pre/Post-Pipelining: 0.823 / 16.886 ms, Bubble: 33.668 ms, Compute: 60.609 ms, Comm: 27.918 ms, Imbalance: 16.536 ms
Node 7, Pre/Post-Pipelining: 0.833 / 31.222 ms, Bubble: 19.551 ms, Compute: 80.205 ms, Comm: 17.197 ms, Imbalance: 7.121 ms
Node 6, Pre/Post-Pipelining: 0.836 / 16.931 ms, Bubble: 33.622 ms, Compute: 62.181 ms, Comm: 23.512 ms, Imbalance: 19.258 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.857 ms
Cluster-Wide Average, Post-Pipelining Overhead: 16.814 ms
Cluster-Wide Average, Bubble: 35.185 ms
Cluster-Wide Average, Compute: 69.211 ms
Cluster-Wide Average, Communication: 17.515 ms
Cluster-Wide Average, Imbalance: 16.472 ms
Node 0, GPU memory consumption: 5.018 GB
Node 1, GPU memory consumption: 3.647 GB
Node 3, GPU memory consumption: 3.624 GB
Node 2, GPU memory consumption: 3.647 GB
Node 4, GPU memory consumption: 3.624 GB
Node 5, GPU memory consumption: 3.647 GB
Node 6, GPU memory consumption: 3.647 GB
Node 7, GPU memory consumption: 3.727 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.491087 s---------------
------------------------node id 1,  per-epoch time: 0.491087 s---------------
------------------------node id 2,  per-epoch time: 0.491087 s---------------
------------------------node id 3,  per-epoch time: 0.491086 s---------------
------------------------node id 4,  per-epoch time: 0.491087 s---------------
------------------------node id 5,  per-epoch time: 0.491087 s---------------
------------------------node id 6,  per-epoch time: 0.491088 s---------------
------------------------node id 7,  per-epoch time: 0.491088 s---------------
************ Profiling Results ************
	Bubble: 414.255844 (ms) (86.29 percentage)
	Compute: 63.153817 (ms) (13.15 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.681141 (ms) (0.56 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.766 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 1.776 GB
	Aggregated layer-level communication throughput: 617.550 Gbps
Highest valid_acc: 0.2347
Target test_acc: 0.2210
Epoch to reach the target acc: 24
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
