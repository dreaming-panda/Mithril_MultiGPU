Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.067 seconds.
Building the CSC structure...
        It takes 0.041 seconds.
        It takes 0.045 seconds.
        It takes 0.047 seconds.
        It takes 0.049 seconds.
        It takes 0.053 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.059 seconds.
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.023 seconds.
        It takes 0.027 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.026 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 64.138 Gbps (per GPU), 513.103 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.098 Gbps (per GPU), 512.787 Gbps (aggregated)
The layer-level communication performance: 64.117 Gbps (per GPU), 512.936 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.062 Gbps (per GPU), 512.495 Gbps (aggregated)
The layer-level communication performance: 64.079 Gbps (per GPU), 512.630 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.046 Gbps (per GPU), 512.367 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.019 Gbps (per GPU), 512.156 Gbps (aggregated)
The layer-level communication performance: 64.035 Gbps (per GPU), 512.281 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.149 Gbps (per GPU), 1329.191 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.153 Gbps (per GPU), 1329.224 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.149 Gbps (per GPU), 1329.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.148 Gbps (per GPU), 1329.188 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.145 Gbps (per GPU), 1329.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.144 Gbps (per GPU), 1329.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.151 Gbps (per GPU), 1329.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.146 Gbps (per GPU), 1329.165 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.464 Gbps (per GPU), 915.711 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.463 Gbps (per GPU), 915.703 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.464 Gbps (per GPU), 915.712 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.459 Gbps (per GPU), 915.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.464 Gbps (per GPU), 915.711 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.463 Gbps (per GPU), 915.706 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.460 Gbps (per GPU), 915.682 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.449 Gbps (per GPU), 915.591 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.271 Gbps (per GPU), 362.167 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.271 Gbps (per GPU), 362.168 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.271 Gbps (per GPU), 362.164 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.271 Gbps (per GPU), 362.165 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.271 Gbps (per GPU), 362.165 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.271 Gbps (per GPU), 362.165 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.271 Gbps (per GPU), 362.166 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.270 Gbps (per GPU), 362.162 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.71ms  1.33ms  1.60ms  2.26 21.17K  0.46M
 chk_1  0.71ms  1.39ms  1.67ms  2.36 21.17K  0.55M
 chk_2  0.71ms  1.29ms  1.56ms  2.21 21.17K  0.39M
 chk_3  0.70ms  1.24ms  1.52ms  2.15 21.17K  0.24M
 chk_4  0.71ms  1.19ms  1.47ms  2.08 21.17K  0.17M
 chk_5  0.71ms  1.20ms  1.47ms  2.08 21.17K  0.22M
 chk_6  0.71ms  1.20ms  1.47ms  2.09 21.17K  0.16M
 chk_7  0.71ms  1.19ms  1.46ms  2.06 21.17K  0.12M
   Avg  0.71  1.25  1.53
   Max  0.71  1.39  1.67
   Min  0.70  1.19  1.46
 Ratio  1.01  1.18  1.14
   Var  0.00  0.01  0.00
Profiling takes 0.409 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 45.759 ms
Partition 0 [0, 5) has cost: 45.759 ms
Partition 1 [5, 9) has cost: 40.111 ms
Partition 2 [9, 13) has cost: 40.111 ms
Partition 3 [13, 17) has cost: 40.111 ms
Partition 4 [17, 21) has cost: 40.111 ms
Partition 5 [21, 25) has cost: 40.111 ms
Partition 6 [25, 29) has cost: 40.111 ms
Partition 7 [29, 33) has cost: 42.290 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.299 ms
GPU 0, Compute+Comm Time: 22.474 ms, Bubble Time: 18.576 ms, Imbalance Overhead: 0.248 ms
GPU 1, Compute+Comm Time: 20.860 ms, Bubble Time: 18.619 ms, Imbalance Overhead: 1.820 ms
GPU 2, Compute+Comm Time: 20.860 ms, Bubble Time: 18.903 ms, Imbalance Overhead: 1.536 ms
GPU 3, Compute+Comm Time: 20.860 ms, Bubble Time: 19.105 ms, Imbalance Overhead: 1.334 ms
GPU 4, Compute+Comm Time: 20.860 ms, Bubble Time: 19.314 ms, Imbalance Overhead: 1.125 ms
GPU 5, Compute+Comm Time: 20.860 ms, Bubble Time: 19.515 ms, Imbalance Overhead: 0.924 ms
GPU 6, Compute+Comm Time: 20.860 ms, Bubble Time: 19.709 ms, Imbalance Overhead: 0.729 ms
GPU 7, Compute+Comm Time: 21.332 ms, Bubble Time: 19.928 ms, Imbalance Overhead: 0.038 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 73.260 ms
GPU 0, Compute+Comm Time: 37.856 ms, Bubble Time: 35.404 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.149 ms, Bubble Time: 35.060 ms, Imbalance Overhead: 2.051 ms
GPU 2, Compute+Comm Time: 36.149 ms, Bubble Time: 34.760 ms, Imbalance Overhead: 2.351 ms
GPU 3, Compute+Comm Time: 36.149 ms, Bubble Time: 34.445 ms, Imbalance Overhead: 2.666 ms
GPU 4, Compute+Comm Time: 36.149 ms, Bubble Time: 34.031 ms, Imbalance Overhead: 3.080 ms
GPU 5, Compute+Comm Time: 36.149 ms, Bubble Time: 33.627 ms, Imbalance Overhead: 3.484 ms
GPU 6, Compute+Comm Time: 36.149 ms, Bubble Time: 33.044 ms, Imbalance Overhead: 4.067 ms
GPU 7, Compute+Comm Time: 40.182 ms, Bubble Time: 32.929 ms, Imbalance Overhead: 0.149 ms
The estimated cost of the whole pipeline: 120.287 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 85.870 ms
Partition 0 [0, 9) has cost: 85.870 ms
Partition 1 [9, 17) has cost: 80.222 ms
Partition 2 [17, 25) has cost: 80.222 ms
Partition 3 [25, 33) has cost: 82.401 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 49.663 ms
GPU 0, Compute+Comm Time: 28.378 ms, Bubble Time: 20.527 ms, Imbalance Overhead: 0.758 ms
GPU 1, Compute+Comm Time: 27.569 ms, Bubble Time: 20.950 ms, Imbalance Overhead: 1.145 ms
GPU 2, Compute+Comm Time: 27.569 ms, Bubble Time: 21.411 ms, Imbalance Overhead: 0.684 ms
GPU 3, Compute+Comm Time: 27.805 ms, Bubble Time: 21.858 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 78.643 ms
GPU 0, Compute+Comm Time: 43.872 ms, Bubble Time: 34.772 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 43.016 ms, Bubble Time: 33.960 ms, Imbalance Overhead: 1.667 ms
GPU 2, Compute+Comm Time: 43.016 ms, Bubble Time: 33.116 ms, Imbalance Overhead: 2.511 ms
GPU 3, Compute+Comm Time: 45.035 ms, Bubble Time: 32.236 ms, Imbalance Overhead: 1.372 ms
    The estimated cost with 2 DP ways is 134.722 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 166.092 ms
Partition 0 [0, 17) has cost: 166.092 ms
Partition 1 [17, 33) has cost: 162.623 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 75.442 ms
GPU 0, Compute+Comm Time: 50.148 ms, Bubble Time: 24.423 ms, Imbalance Overhead: 0.871 ms
GPU 1, Compute+Comm Time: 49.861 ms, Bubble Time: 25.581 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 100.240 ms
GPU 0, Compute+Comm Time: 65.949 ms, Bubble Time: 34.291 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 66.535 ms, Bubble Time: 31.951 ms, Imbalance Overhead: 1.755 ms
    The estimated cost with 4 DP ways is 184.467 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 328.715 ms
Partition 0 [0, 33) has cost: 328.715 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 181.747 ms
GPU 0, Compute+Comm Time: 181.747 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 199.018 ms
GPU 0, Compute+Comm Time: 199.018 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 399.804 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.0667	TrainAcc 0.2615	ValidAcc 0.2690	TestAcc 0.2461	BestValid 0.2690
	Epoch 50:	Loss 2.7750	TrainAcc 0.2904	ValidAcc 0.3067	TestAcc 0.2759	BestValid 0.3067
	Epoch 75:	Loss 2.5125	TrainAcc 0.3699	ValidAcc 0.3574	TestAcc 0.3274	BestValid 0.3574
	Epoch 100:	Loss 2.3567	TrainAcc 0.4126	ValidAcc 0.3884	TestAcc 0.3608	BestValid 0.3884
Node 5, Pre/Post-Pipelining: 6.370 / 13.320 ms, Bubble: 0.531 ms, Compute: 148.173 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 1, Pre/Post-Pipelining: 6.368 / 13.283 ms, Bubble: 0.439 ms, Compute: 148.303 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 4, Pre/Post-Pipelining: 6.371 / 13.301 ms, Bubble: 0.466 ms, Compute: 148.249 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 0, Pre/Post-Pipelining: 6.367 / 13.261 ms, Bubble: 0.586 ms, Compute: 148.168 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 3, Pre/Post-Pipelining: 6.375 / 13.359 ms, Bubble: 0.057 ms, Compute: 148.604 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 6, Pre/Post-Pipelining: 6.376 / 13.341 ms, Bubble: 0.177 ms, Compute: 148.496 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 7, Pre/Post-Pipelining: 6.373 / 13.332 ms, Bubble: 0.642 ms, Compute: 148.030 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 6.373 / 13.328 ms, Bubble: 0.398 ms, Compute: 148.290 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.367 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.261 ms
Cluster-Wide Average, Bubble: 0.586 ms
Cluster-Wide Average, Compute: 148.168 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.015 ms
Node 0, GPU memory consumption: 15.387 GB
Node 1, GPU memory consumption: 14.751 GB
Node 4, GPU memory consumption: 14.725 GB
Node 5, GPU memory consumption: 14.749 GB
Node 2, GPU memory consumption: 14.749 GB
Node 3, GPU memory consumption: 14.725 GB
Node 6, GPU memory consumption: 14.749 GB
Node 7, GPU memory consumption: 14.725 GB
Node 0, Graph-Level Communication Throughput: 51.279 Gbps, Time: 88.025 ms
Node 1, Graph-Level Communication Throughput: 63.687 Gbps, Time: 84.303 ms
Node 2, Graph-Level Communication Throughput: 43.036 Gbps, Time: 88.272 ms
Node 3, Graph-Level Communication Throughput: 44.567 Gbps, Time: 88.924 ms
Node 4, Graph-Level Communication Throughput: 21.238 Gbps, Time: 91.811 ms
Node 5, Graph-Level Communication Throughput: 10.739 Gbps, Time: 92.340 ms
Node 6, Graph-Level Communication Throughput: 19.105 Gbps, Time: 91.547 ms
Node 7, Graph-Level Communication Throughput: 11.675 Gbps, Time: 92.027 ms
------------------------node id 0,  per-epoch time: 0.504005 s---------------
------------------------node id 1,  per-epoch time: 0.504004 s---------------
------------------------node id 2,  per-epoch time: 0.504010 s---------------
------------------------node id 3,  per-epoch time: 0.504004 s---------------
------------------------node id 4,  per-epoch time: 0.504004 s---------------
------------------------node id 5,  per-epoch time: 0.504003 s---------------
------------------------node id 6,  per-epoch time: 0.504005 s---------------
------------------------node id 7,  per-epoch time: 0.504005 s---------------
************ Profiling Results ************
	Bubble: 339.467729 (ms) (69.44 percentage)
	Compute: 41.588269 (ms) (8.51 percentage)
	GraphCommComputeOverhead: 5.545636 (ms) (1.13 percentage)
	GraphCommNetwork: 89.656592 (ms) (18.34 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.622967 (ms) (2.58 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 2.725 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 2.735 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.3884
Target test_acc: 0.3608
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
