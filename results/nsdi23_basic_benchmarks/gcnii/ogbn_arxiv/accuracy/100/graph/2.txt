Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.043 seconds.
        It takes 0.041 seconds.
        It takes 0.054 seconds.
        It takes 0.052 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.027 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.026 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 64.380 Gbps (per GPU), 515.041 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.335 Gbps (per GPU), 514.678 Gbps (aggregated)
The layer-level communication performance: 64.334 Gbps (per GPU), 514.670 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.297 Gbps (per GPU), 514.379 Gbps (aggregated)
The layer-level communication performance: 64.292 Gbps (per GPU), 514.339 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.261 Gbps (per GPU), 514.084 Gbps (aggregated)
The layer-level communication performance: 64.254 Gbps (per GPU), 514.031 Gbps (aggregated)
The layer-level communication performance: 64.249 Gbps (per GPU), 513.991 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.737 Gbps (per GPU), 1309.898 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.732 Gbps (per GPU), 1309.857 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.728 Gbps (per GPU), 1309.825 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.726 Gbps (per GPU), 1309.812 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.730 Gbps (per GPU), 1309.841 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.726 Gbps (per GPU), 1309.809 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.722 Gbps (per GPU), 1309.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.725 Gbps (per GPU), 1309.796 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.173 Gbps (per GPU), 913.382 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.173 Gbps (per GPU), 913.382 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.173 Gbps (per GPU), 913.382 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.172 Gbps (per GPU), 913.376 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.165 Gbps (per GPU), 913.322 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.165 Gbps (per GPU), 913.318 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.165 Gbps (per GPU), 913.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.155 Gbps (per GPU), 913.241 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.678 Gbps (per GPU), 365.420 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.678 Gbps (per GPU), 365.421 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.678 Gbps (per GPU), 365.422 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.677 Gbps (per GPU), 365.420 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.677 Gbps (per GPU), 365.419 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.677 Gbps (per GPU), 365.418 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.677 Gbps (per GPU), 365.419 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.675 Gbps (per GPU), 365.400 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.72ms  1.33ms  1.61ms  2.24 21.17K  0.46M
 chk_1  0.71ms  1.41ms  1.68ms  2.36 21.17K  0.55M
 chk_2  0.72ms  1.30ms  1.58ms  2.20 21.17K  0.39M
 chk_3  0.71ms  1.26ms  1.53ms  2.15 21.17K  0.24M
 chk_4  0.71ms  1.21ms  1.48ms  2.07 21.17K  0.17M
 chk_5  0.71ms  1.21ms  1.49ms  2.08 21.17K  0.22M
 chk_6  0.71ms  1.22ms  1.49ms  2.09 21.17K  0.16M
 chk_7  0.71ms  1.20ms  1.48ms  2.07 21.17K  0.12M
   Avg  0.72  1.27  1.54
   Max  0.72  1.41  1.68
   Min  0.71  1.20  1.48
 Ratio  1.01  1.17  1.14
   Var  0.00  0.00  0.01
Profiling takes 0.417 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 46.249 ms
Partition 0 [0, 5) has cost: 46.249 ms
Partition 1 [5, 9) has cost: 40.526 ms
Partition 2 [9, 13) has cost: 40.526 ms
Partition 3 [13, 17) has cost: 40.526 ms
Partition 4 [17, 21) has cost: 40.526 ms
Partition 5 [21, 25) has cost: 40.526 ms
Partition 6 [25, 29) has cost: 40.526 ms
Partition 7 [29, 33) has cost: 42.741 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.516 ms
GPU 0, Compute+Comm Time: 22.609 ms, Bubble Time: 18.674 ms, Imbalance Overhead: 0.233 ms
GPU 1, Compute+Comm Time: 20.961 ms, Bubble Time: 18.726 ms, Imbalance Overhead: 1.829 ms
GPU 2, Compute+Comm Time: 20.961 ms, Bubble Time: 19.003 ms, Imbalance Overhead: 1.552 ms
GPU 3, Compute+Comm Time: 20.961 ms, Bubble Time: 19.216 ms, Imbalance Overhead: 1.339 ms
GPU 4, Compute+Comm Time: 20.961 ms, Bubble Time: 19.428 ms, Imbalance Overhead: 1.126 ms
GPU 5, Compute+Comm Time: 20.961 ms, Bubble Time: 19.626 ms, Imbalance Overhead: 0.928 ms
GPU 6, Compute+Comm Time: 20.961 ms, Bubble Time: 19.819 ms, Imbalance Overhead: 0.735 ms
GPU 7, Compute+Comm Time: 21.443 ms, Bubble Time: 20.037 ms, Imbalance Overhead: 0.036 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 73.750 ms
GPU 0, Compute+Comm Time: 38.133 ms, Bubble Time: 35.617 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.399 ms, Bubble Time: 35.278 ms, Imbalance Overhead: 2.072 ms
GPU 2, Compute+Comm Time: 36.399 ms, Bubble Time: 34.987 ms, Imbalance Overhead: 2.364 ms
GPU 3, Compute+Comm Time: 36.399 ms, Bubble Time: 34.686 ms, Imbalance Overhead: 2.665 ms
GPU 4, Compute+Comm Time: 36.399 ms, Bubble Time: 34.266 ms, Imbalance Overhead: 3.085 ms
GPU 5, Compute+Comm Time: 36.399 ms, Bubble Time: 33.859 ms, Imbalance Overhead: 3.492 ms
GPU 6, Compute+Comm Time: 36.399 ms, Bubble Time: 33.282 ms, Imbalance Overhead: 4.069 ms
GPU 7, Compute+Comm Time: 40.474 ms, Bubble Time: 33.187 ms, Imbalance Overhead: 0.088 ms
The estimated cost of the whole pipeline: 121.029 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 86.775 ms
Partition 0 [0, 9) has cost: 86.775 ms
Partition 1 [9, 17) has cost: 81.052 ms
Partition 2 [17, 25) has cost: 81.052 ms
Partition 3 [25, 33) has cost: 83.267 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 50.011 ms
GPU 0, Compute+Comm Time: 28.599 ms, Bubble Time: 20.686 ms, Imbalance Overhead: 0.727 ms
GPU 1, Compute+Comm Time: 27.770 ms, Bubble Time: 21.098 ms, Imbalance Overhead: 1.143 ms
GPU 2, Compute+Comm Time: 27.770 ms, Bubble Time: 21.554 ms, Imbalance Overhead: 0.687 ms
GPU 3, Compute+Comm Time: 28.010 ms, Bubble Time: 22.001 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 79.262 ms
GPU 0, Compute+Comm Time: 44.250 ms, Bubble Time: 35.012 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 43.383 ms, Bubble Time: 34.214 ms, Imbalance Overhead: 1.666 ms
GPU 2, Compute+Comm Time: 43.383 ms, Bubble Time: 33.396 ms, Imbalance Overhead: 2.484 ms
GPU 3, Compute+Comm Time: 45.422 ms, Bubble Time: 32.531 ms, Imbalance Overhead: 1.308 ms
    The estimated cost with 2 DP ways is 135.737 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 167.827 ms
Partition 0 [0, 17) has cost: 167.827 ms
Partition 1 [17, 33) has cost: 164.319 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 75.688 ms
GPU 0, Compute+Comm Time: 50.315 ms, Bubble Time: 24.503 ms, Imbalance Overhead: 0.870 ms
GPU 1, Compute+Comm Time: 50.021 ms, Bubble Time: 25.668 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 100.708 ms
GPU 0, Compute+Comm Time: 66.267 ms, Bubble Time: 34.441 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 66.853 ms, Bubble Time: 32.118 ms, Imbalance Overhead: 1.736 ms
    The estimated cost with 4 DP ways is 185.216 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 332.145 ms
Partition 0 [0, 33) has cost: 332.145 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 180.381 ms
GPU 0, Compute+Comm Time: 180.381 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 197.785 ms
GPU 0, Compute+Comm Time: 197.785 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 397.075 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.1289	TrainAcc 0.2653	ValidAcc 0.2809	TestAcc 0.2564	BestValid 0.2809
	Epoch 50:	Loss 2.9024	TrainAcc 0.2799	ValidAcc 0.3011	TestAcc 0.2707	BestValid 0.3011
	Epoch 75:	Loss 2.6558	TrainAcc 0.3357	ValidAcc 0.3335	TestAcc 0.3059	BestValid 0.3335
Node 2, Pre/Post-Pipelining: 6.370 / 13.825 ms, Bubble: 0.373 ms, Compute: 148.587 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 1, Pre/Post-Pipelining: 6.370 / 13.869 ms, Bubble: 0.300 ms, Compute: 148.614 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 3, Pre/Post-Pipelining: 6.374 / 13.857 ms, Bubble: 0.071 ms, Compute: 148.850 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 6, Pre/Post-Pipelining: 6.374 / 13.853 ms, Bubble: 0.098 ms, Compute: 148.827 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 7, Pre/Post-Pipelining: 6.369 / 13.860 ms, Bubble: 0.548 ms, Compute: 148.369 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 4, Pre/Post-Pipelining: 6.371 / 13.836 ms, Bubble: 0.391 ms, Compute: 148.546 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
	Epoch 100:	Loss 2.4311	TrainAcc 0.4039	ValidAcc 0.3832	TestAcc 0.3539	BestValid 0.3832
Node 0, Pre/Post-Pipelining: 6.368 / 13.855 ms, Bubble: 0.440 ms, Compute: 148.479 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 5, Pre/Post-Pipelining: 6.369 / 13.863 ms, Bubble: 0.418 ms, Compute: 148.502 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.368 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.855 ms
Cluster-Wide Average, Bubble: 0.440 ms
Cluster-Wide Average, Compute: 148.479 ms
Cluster-Wide Average, Communication: 0.009 ms
Cluster-Wide Average, Imbalance: 0.016 ms
Node 0, GPU memory consumption: 15.387 GB
Node 2, GPU memory consumption: 14.749 GB
Node 3, GPU memory consumption: 14.725 GB
Node 1, GPU memory consumption: 14.751 GB
Node 5, GPU memory consumption: 14.749 GB
Node 6, GPU memory consumption: 14.749 GB
Node 4, GPU memory consumption: 14.725 GB
Node 7, GPU memory consumption: 14.725 GB
Node 0, Graph-Level Communication Throughput: 51.383 Gbps, Time: 87.846 ms
Node 1, Graph-Level Communication Throughput: 63.802 Gbps, Time: 84.152 ms
Node 2, Graph-Level Communication Throughput: 42.622 Gbps, Time: 89.128 ms
Node 3, Graph-Level Communication Throughput: 44.186 Gbps, Time: 89.691 ms
Node 4, Graph-Level Communication Throughput: 21.155 Gbps, Time: 92.171 ms
Node 5, Graph-Level Communication Throughput: 10.707 Gbps, Time: 92.615 ms
Node 6, Graph-Level Communication Throughput: 18.972 Gbps, Time: 92.187 ms
Node 7, Graph-Level Communication Throughput: 11.602 Gbps, Time: 92.602 ms
------------------------node id 0,  per-epoch time: 0.505497 s---------------
------------------------node id 1,  per-epoch time: 0.505493 s---------------
------------------------node id 2,  per-epoch time: 0.505493 s---------------
------------------------node id 3,  per-epoch time: 0.505494 s---------------
------------------------node id 4,  per-epoch time: 0.505494 s---------------
------------------------node id 5,  per-epoch time: 0.505498 s---------------
------------------------node id 6,  per-epoch time: 0.505493 s---------------
------------------------node id 7,  per-epoch time: 0.505493 s---------------
************ Profiling Results ************
	Bubble: 340.509484 (ms) (69.46 percentage)
	Compute: 41.509300 (ms) (8.47 percentage)
	GraphCommComputeOverhead: 5.527127 (ms) (1.13 percentage)
	GraphCommNetwork: 90.049577 (ms) (18.37 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.627495 (ms) (2.58 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 2.725 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 2.735 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.3832
Target test_acc: 0.3539
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
