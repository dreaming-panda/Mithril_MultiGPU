Initialized node 0 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.045 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.046 seconds.
        It takes 0.045 seconds.
        It takes 0.049 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
        It takes 0.053 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.776 Gbps (per GPU), 510.206 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.733 Gbps (per GPU), 509.867 Gbps (aggregated)
The layer-level communication performance: 63.731 Gbps (per GPU), 509.849 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.696 Gbps (per GPU), 509.571 Gbps (aggregated)
The layer-level communication performance: 63.692 Gbps (per GPU), 509.534 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.659 Gbps (per GPU), 509.274 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.652 Gbps (per GPU), 509.219 Gbps (aggregated)
The layer-level communication performance: 63.648 Gbps (per GPU), 509.182 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.792 Gbps (per GPU), 1310.339 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.790 Gbps (per GPU), 1310.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.786 Gbps (per GPU), 1310.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.785 Gbps (per GPU), 1310.282 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.777 Gbps (per GPU), 1310.215 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.787 Gbps (per GPU), 1310.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.784 Gbps (per GPU), 1310.272 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.787 Gbps (per GPU), 1310.295 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.174 Gbps (per GPU), 913.391 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.173 Gbps (per GPU), 913.384 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.174 Gbps (per GPU), 913.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.172 Gbps (per GPU), 913.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.174 Gbps (per GPU), 913.389 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.172 Gbps (per GPU), 913.379 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.171 Gbps (per GPU), 913.370 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.169 Gbps (per GPU), 913.349 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.401 Gbps (per GPU), 363.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.401 Gbps (per GPU), 363.212 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.401 Gbps (per GPU), 363.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.401 Gbps (per GPU), 363.210 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.401 Gbps (per GPU), 363.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.401 Gbps (per GPU), 363.208 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.400 Gbps (per GPU), 363.199 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.398 Gbps (per GPU), 363.183 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.72ms  1.34ms  1.61ms  2.25 21.17K  0.46M
 chk_1  0.71ms  1.41ms  1.68ms  2.36 21.17K  0.55M
 chk_2  0.71ms  1.30ms  1.58ms  2.22 21.17K  0.39M
 chk_3  0.71ms  1.26ms  1.54ms  2.16 21.17K  0.24M
 chk_4  0.71ms  1.20ms  1.49ms  2.08 21.17K  0.17M
 chk_5  0.71ms  1.21ms  1.49ms  2.09 21.17K  0.22M
 chk_6  0.71ms  1.22ms  1.49ms  2.10 21.17K  0.16M
 chk_7  0.71ms  1.20ms  1.48ms  2.08 21.17K  0.12M
   Avg  0.71  1.27  1.55
   Max  0.72  1.41  1.68
   Min  0.71  1.20  1.48
 Ratio  1.01  1.17  1.14
   Var  0.00  0.00  0.00
Profiling takes 0.422 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 46.235 ms
Partition 0 [0, 5) has cost: 46.235 ms
Partition 1 [5, 9) has cost: 40.527 ms
Partition 2 [9, 13) has cost: 40.527 ms
Partition 3 [13, 17) has cost: 40.527 ms
Partition 4 [17, 21) has cost: 40.527 ms
Partition 5 [21, 25) has cost: 40.527 ms
Partition 6 [25, 29) has cost: 40.527 ms
Partition 7 [29, 33) has cost: 42.760 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.684 ms
GPU 0, Compute+Comm Time: 22.694 ms, Bubble Time: 18.756 ms, Imbalance Overhead: 0.235 ms
GPU 1, Compute+Comm Time: 21.056 ms, Bubble Time: 18.805 ms, Imbalance Overhead: 1.823 ms
GPU 2, Compute+Comm Time: 21.056 ms, Bubble Time: 19.088 ms, Imbalance Overhead: 1.540 ms
GPU 3, Compute+Comm Time: 21.056 ms, Bubble Time: 19.288 ms, Imbalance Overhead: 1.339 ms
GPU 4, Compute+Comm Time: 21.056 ms, Bubble Time: 19.501 ms, Imbalance Overhead: 1.127 ms
GPU 5, Compute+Comm Time: 21.056 ms, Bubble Time: 19.700 ms, Imbalance Overhead: 0.928 ms
GPU 6, Compute+Comm Time: 21.056 ms, Bubble Time: 19.895 ms, Imbalance Overhead: 0.733 ms
GPU 7, Compute+Comm Time: 21.536 ms, Bubble Time: 20.110 ms, Imbalance Overhead: 0.037 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 73.905 ms
GPU 0, Compute+Comm Time: 38.217 ms, Bubble Time: 35.688 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.464 ms, Bubble Time: 35.355 ms, Imbalance Overhead: 2.086 ms
GPU 2, Compute+Comm Time: 36.464 ms, Bubble Time: 35.069 ms, Imbalance Overhead: 2.372 ms
GPU 3, Compute+Comm Time: 36.464 ms, Bubble Time: 34.769 ms, Imbalance Overhead: 2.672 ms
GPU 4, Compute+Comm Time: 36.464 ms, Bubble Time: 34.338 ms, Imbalance Overhead: 3.103 ms
GPU 5, Compute+Comm Time: 36.464 ms, Bubble Time: 33.938 ms, Imbalance Overhead: 3.502 ms
GPU 6, Compute+Comm Time: 36.464 ms, Bubble Time: 33.363 ms, Imbalance Overhead: 4.078 ms
GPU 7, Compute+Comm Time: 40.535 ms, Bubble Time: 33.252 ms, Imbalance Overhead: 0.119 ms
The estimated cost of the whole pipeline: 121.369 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 86.761 ms
Partition 0 [0, 9) has cost: 86.761 ms
Partition 1 [9, 17) has cost: 81.053 ms
Partition 2 [17, 25) has cost: 81.053 ms
Partition 3 [25, 33) has cost: 83.286 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 50.165 ms
GPU 0, Compute+Comm Time: 28.668 ms, Bubble Time: 20.741 ms, Imbalance Overhead: 0.756 ms
GPU 1, Compute+Comm Time: 27.848 ms, Bubble Time: 21.167 ms, Imbalance Overhead: 1.150 ms
GPU 2, Compute+Comm Time: 27.848 ms, Bubble Time: 21.625 ms, Imbalance Overhead: 0.692 ms
GPU 3, Compute+Comm Time: 28.090 ms, Bubble Time: 22.075 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 79.376 ms
GPU 0, Compute+Comm Time: 44.318 ms, Bubble Time: 35.058 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 43.440 ms, Bubble Time: 34.267 ms, Imbalance Overhead: 1.669 ms
GPU 2, Compute+Comm Time: 43.440 ms, Bubble Time: 33.448 ms, Imbalance Overhead: 2.488 ms
GPU 3, Compute+Comm Time: 45.477 ms, Bubble Time: 32.583 ms, Imbalance Overhead: 1.316 ms
    The estimated cost with 2 DP ways is 136.018 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 167.815 ms
Partition 0 [0, 17) has cost: 167.815 ms
Partition 1 [17, 33) has cost: 164.340 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 75.834 ms
GPU 0, Compute+Comm Time: 50.409 ms, Bubble Time: 24.548 ms, Imbalance Overhead: 0.877 ms
GPU 1, Compute+Comm Time: 50.118 ms, Bubble Time: 25.716 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 100.816 ms
GPU 0, Compute+Comm Time: 66.343 ms, Bubble Time: 34.473 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 66.926 ms, Bubble Time: 32.162 ms, Imbalance Overhead: 1.727 ms
    The estimated cost with 4 DP ways is 185.482 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 332.155 ms
Partition 0 [0, 33) has cost: 332.155 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 181.412 ms
GPU 0, Compute+Comm Time: 181.412 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 198.780 ms
GPU 0, Compute+Comm Time: 198.780 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 399.201 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.0993	TrainAcc 0.2688	ValidAcc 0.2868	TestAcc 0.2607	BestValid 0.2868
	Epoch 50:	Loss 2.8096	TrainAcc 0.2911	ValidAcc 0.3069	TestAcc 0.2766	BestValid 0.3069
	Epoch 75:	Loss 2.5193	TrainAcc 0.3792	ValidAcc 0.3633	TestAcc 0.3336	BestValid 0.3633
Node 1, Pre/Post-Pipelining: 6.371 / 13.928 ms, Bubble: 0.226 ms, Compute: 148.851 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 4, Pre/Post-Pipelining: 6.373 / 13.920 ms, Bubble: 0.414 ms, Compute: 148.662 ms, Comm: 0.010 ms, Imbalance: 0.019 ms
Node 2, Pre/Post-Pipelining: 6.371 / 13.891 ms, Bubble: 0.339 ms, Compute: 148.758 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 5, Pre/Post-Pipelining: 6.370 / 13.914 ms, Bubble: 0.423 ms, Compute: 148.665 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 3, Pre/Post-Pipelining: 6.372 / 13.906 ms, Bubble: 0.085 ms, Compute: 149.009 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 6, Pre/Post-Pipelining: 6.375 / 13.913 ms, Bubble: 0.086 ms, Compute: 148.992 ms, Comm: 0.008 ms, Imbalance: 0.019 ms
Node 7, Pre/Post-Pipelining: 6.366 / 13.874 ms, Bubble: 0.578 ms, Compute: 148.550 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
	Epoch 100:	Loss 2.3366	TrainAcc 0.4262	ValidAcc 0.3997	TestAcc 0.3727	BestValid 0.3997
Node 0, Pre/Post-Pipelining: 6.369 / 13.911 ms, Bubble: 0.353 ms, Compute: 148.724 ms, Comm: 0.010 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.369 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.911 ms
Cluster-Wide Average, Bubble: 0.353 ms
Cluster-Wide Average, Compute: 148.724 ms
Cluster-Wide Average, Communication: 0.010 ms
Cluster-Wide Average, Imbalance: 0.016 ms
Node 0, GPU memory consumption: 15.387 GB
Node 1, GPU memory consumption: 14.751 GB
Node 2, GPU memory consumption: 14.749 GB
Node 3, GPU memory consumption: 14.725 GB
Node 7, GPU memory consumption: 14.725 GB
Node 5, GPU memory consumption: 14.749 GB
Node 6, GPU memory consumption: 14.749 GB
Node 4, GPU memory consumption: 14.725 GB
Node 0, Graph-Level Communication Throughput: 51.294 Gbps, Time: 87.998 ms
Node 1, Graph-Level Communication Throughput: 63.738 Gbps, Time: 84.236 ms
Node 2, Graph-Level Communication Throughput: 42.597 Gbps, Time: 89.180 ms
Node 3, Graph-Level Communication Throughput: 44.016 Gbps, Time: 90.037 ms
Node 4, Graph-Level Communication Throughput: 21.243 Gbps, Time: 91.792 ms
Node 5, Graph-Level Communication Throughput: 10.690 Gbps, Time: 92.764 ms
Node 6, Graph-Level Communication Throughput: 18.978 Gbps, Time: 92.157 ms
Node 7, Graph-Level Communication Throughput: 11.489 Gbps, Time: 93.511 ms
------------------------node id 0,  per-epoch time: 0.537580 s---------------
------------------------node id 1,  per-epoch time: 0.537577 s---------------
------------------------node id 2,  per-epoch time: 0.537578 s---------------
------------------------node id 3,  per-epoch time: 0.537579 s---------------
------------------------node id 4,  per-epoch time: 0.537577 s---------------
------------------------node id 5,  per-epoch time: 0.537577 s---------------
------------------------node id 6,  per-epoch time: 0.537577 s---------------
------------------------node id 7,  per-epoch time: 0.537578 s---------------
************ Profiling Results ************
	Bubble: 370.908089 (ms) (71.22 percentage)
	Compute: 41.547179 (ms) (7.98 percentage)
	GraphCommComputeOverhead: 5.526549 (ms) (1.06 percentage)
	GraphCommNetwork: 90.209984 (ms) (17.32 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.629282 (ms) (2.42 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 2.725 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 2.735 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.3997
Target test_acc: 0.3727
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
