Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.043 seconds.
        It takes 0.050 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.054 seconds.
        It takes 0.052 seconds.
        It takes 0.053 seconds.
        It takes 0.055 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.960 Gbps (per GPU), 511.684 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.921 Gbps (per GPU), 511.366 Gbps (aggregated)
The layer-level communication performance: 63.919 Gbps (per GPU), 511.349 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.886 Gbps (per GPU), 511.086 Gbps (aggregated)
The layer-level communication performance: 63.881 Gbps (per GPU), 511.046 Gbps (aggregated)
The layer-level communication performance: 63.850 Gbps (per GPU), 510.803 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.843 Gbps (per GPU), 510.746 Gbps (aggregated)
The layer-level communication performance: 63.838 Gbps (per GPU), 510.707 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.094 Gbps (per GPU), 1328.754 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.089 Gbps (per GPU), 1328.714 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.083 Gbps (per GPU), 1328.661 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.091 Gbps (per GPU), 1328.731 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.080 Gbps (per GPU), 1328.642 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.098 Gbps (per GPU), 1328.786 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.098 Gbps (per GPU), 1328.783 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.091 Gbps (per GPU), 1328.724 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.990 Gbps (per GPU), 919.917 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.988 Gbps (per GPU), 919.904 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.987 Gbps (per GPU), 919.896 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.990 Gbps (per GPU), 919.919 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.986 Gbps (per GPU), 919.885 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.985 Gbps (per GPU), 919.883 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.982 Gbps (per GPU), 919.859 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.971 Gbps (per GPU), 919.771 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.091 Gbps (per GPU), 360.726 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.091 Gbps (per GPU), 360.728 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.091 Gbps (per GPU), 360.725 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.091 Gbps (per GPU), 360.724 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.091 Gbps (per GPU), 360.726 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.091 Gbps (per GPU), 360.725 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.090 Gbps (per GPU), 360.720 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.091 Gbps (per GPU), 360.725 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.28ms  0.35ms  0.49ms  1.76  5.55K  0.06M
 chk_1  0.28ms  0.35ms  0.48ms  1.74  5.75K  0.05M
 chk_2  0.27ms  0.35ms  0.49ms  1.80  5.20K  0.07M
 chk_3  0.28ms  0.35ms  0.49ms  1.76  5.57K  0.06M
 chk_4  0.27ms  0.35ms  0.49ms  1.82  5.05K  0.08M
 chk_5  0.26ms  0.36ms  0.50ms  1.89  4.73K  0.11M
 chk_6  0.27ms  0.35ms  0.49ms  1.82  4.98K  0.08M
 chk_7  0.27ms  0.35ms  0.49ms  1.82  5.01K  0.09M
 chk_8  0.28ms  0.34ms  0.48ms  1.73  5.73K  0.05M
 chk_9  0.26ms  0.35ms  0.49ms  1.86  4.54K  0.11M
chk_10  0.27ms  0.35ms  0.49ms  1.79  5.36K  0.07M
chk_11  0.28ms  0.35ms  0.49ms  1.79  5.39K  0.08M
chk_12  0.28ms  0.35ms  0.48ms  1.72  5.77K  0.05M
chk_13  0.28ms  0.35ms  0.49ms  1.77  5.43K  0.06M
chk_14  0.28ms  0.35ms  0.49ms  1.78  5.46K  0.06M
chk_15  0.28ms  0.35ms  0.48ms  1.73  5.88K  0.04M
chk_16  0.27ms  0.35ms  0.49ms  1.79  5.50K  0.06M
chk_17  0.27ms  0.35ms  0.49ms  1.85  4.86K  0.09M
chk_18  0.27ms  0.36ms  0.50ms  1.84  5.39K  0.07M
chk_19  0.27ms  0.35ms  0.49ms  1.80  5.20K  0.07M
chk_20  0.28ms  0.35ms  0.49ms  1.76  5.51K  0.06M
chk_21  0.28ms  0.34ms  0.48ms  1.73  5.81K  0.05M
chk_22  0.27ms  0.35ms  0.49ms  1.79  5.32K  0.07M
chk_23  0.27ms  0.36ms  0.50ms  1.82  5.39K  0.07M
chk_24  0.26ms  0.35ms  0.49ms  1.90  4.62K  0.11M
chk_25  0.27ms  0.35ms  0.49ms  1.83  5.04K  0.08M
chk_26  0.26ms  0.35ms  0.49ms  1.88  4.55K  0.11M
chk_27  0.27ms  0.35ms  0.49ms  1.78  5.30K  0.06M
chk_28  0.28ms  0.35ms  0.49ms  1.78  5.58K  0.06M
chk_29  0.27ms  0.35ms  0.50ms  1.86  4.98K  0.09M
chk_30  0.28ms  0.35ms  0.49ms  1.77  5.50K  0.07M
chk_31  0.27ms  0.35ms  0.49ms  1.78  5.38K  0.07M
   Avg  0.27  0.35  0.49
   Max  0.28  0.36  0.50
   Min  0.26  0.34  0.48
 Ratio  1.09  1.06  1.04
   Var  0.00  0.00  0.00
Profiling takes 0.550 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 98.624 ms
Partition 0 [0, 9) has cost: 98.624 ms
Partition 1 [9, 17) has cost: 89.911 ms
Partition 2 [17, 25) has cost: 89.911 ms
Partition 3 [25, 33) has cost: 89.911 ms
Partition 4 [33, 41) has cost: 89.911 ms
Partition 5 [41, 49) has cost: 89.911 ms
Partition 6 [49, 57) has cost: 89.911 ms
Partition 7 [57, 65) has cost: 94.334 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 47.587 ms
GPU 0, Compute+Comm Time: 39.438 ms, Bubble Time: 8.149 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.348 ms, Bubble Time: 8.227 ms, Imbalance Overhead: 3.012 ms
GPU 2, Compute+Comm Time: 36.348 ms, Bubble Time: 8.309 ms, Imbalance Overhead: 2.930 ms
GPU 3, Compute+Comm Time: 36.348 ms, Bubble Time: 8.376 ms, Imbalance Overhead: 2.862 ms
GPU 4, Compute+Comm Time: 36.348 ms, Bubble Time: 8.442 ms, Imbalance Overhead: 2.797 ms
GPU 5, Compute+Comm Time: 36.348 ms, Bubble Time: 8.517 ms, Imbalance Overhead: 2.721 ms
GPU 6, Compute+Comm Time: 36.348 ms, Bubble Time: 8.580 ms, Imbalance Overhead: 2.659 ms
GPU 7, Compute+Comm Time: 37.327 ms, Bubble Time: 8.626 ms, Imbalance Overhead: 1.633 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.835 ms
GPU 0, Compute+Comm Time: 67.851 ms, Bubble Time: 15.332 ms, Imbalance Overhead: 1.652 ms
GPU 1, Compute+Comm Time: 64.408 ms, Bubble Time: 15.286 ms, Imbalance Overhead: 5.141 ms
GPU 2, Compute+Comm Time: 64.408 ms, Bubble Time: 15.216 ms, Imbalance Overhead: 5.211 ms
GPU 3, Compute+Comm Time: 64.408 ms, Bubble Time: 15.170 ms, Imbalance Overhead: 5.257 ms
GPU 4, Compute+Comm Time: 64.408 ms, Bubble Time: 15.086 ms, Imbalance Overhead: 5.341 ms
GPU 5, Compute+Comm Time: 64.408 ms, Bubble Time: 15.001 ms, Imbalance Overhead: 5.426 ms
GPU 6, Compute+Comm Time: 64.408 ms, Bubble Time: 14.868 ms, Imbalance Overhead: 5.560 ms
GPU 7, Compute+Comm Time: 70.031 ms, Bubble Time: 14.798 ms, Imbalance Overhead: 0.006 ms
The estimated cost of the whole pipeline: 139.043 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 188.535 ms
Partition 0 [0, 17) has cost: 188.535 ms
Partition 1 [17, 33) has cost: 179.823 ms
Partition 2 [33, 49) has cost: 179.823 ms
Partition 3 [49, 65) has cost: 184.245 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 55.370 ms
GPU 0, Compute+Comm Time: 46.722 ms, Bubble Time: 8.537 ms, Imbalance Overhead: 0.111 ms
GPU 1, Compute+Comm Time: 45.168 ms, Bubble Time: 8.663 ms, Imbalance Overhead: 1.540 ms
GPU 2, Compute+Comm Time: 45.168 ms, Bubble Time: 8.735 ms, Imbalance Overhead: 1.467 ms
GPU 3, Compute+Comm Time: 45.660 ms, Bubble Time: 8.768 ms, Imbalance Overhead: 0.942 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 90.487 ms
GPU 0, Compute+Comm Time: 75.131 ms, Bubble Time: 14.324 ms, Imbalance Overhead: 1.032 ms
GPU 1, Compute+Comm Time: 73.407 ms, Bubble Time: 14.273 ms, Imbalance Overhead: 2.807 ms
GPU 2, Compute+Comm Time: 73.407 ms, Bubble Time: 14.217 ms, Imbalance Overhead: 2.863 ms
GPU 3, Compute+Comm Time: 76.260 ms, Bubble Time: 14.025 ms, Imbalance Overhead: 0.202 ms
    The estimated cost with 2 DP ways is 153.150 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 368.358 ms
Partition 0 [0, 33) has cost: 368.358 ms
Partition 1 [33, 65) has cost: 364.068 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 86.171 ms
GPU 0, Compute+Comm Time: 75.577 ms, Bubble Time: 9.364 ms, Imbalance Overhead: 1.230 ms
GPU 1, Compute+Comm Time: 75.047 ms, Bubble Time: 9.599 ms, Imbalance Overhead: 1.526 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 118.649 ms
GPU 0, Compute+Comm Time: 104.053 ms, Bubble Time: 13.195 ms, Imbalance Overhead: 1.400 ms
GPU 1, Compute+Comm Time: 104.620 ms, Bubble Time: 12.998 ms, Imbalance Overhead: 1.031 ms
    The estimated cost with 4 DP ways is 215.062 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 732.426 ms
Partition 0 [0, 65) has cost: 732.426 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 266.257 ms
GPU 0, Compute+Comm Time: 266.257 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 295.503 ms
GPU 0, Compute+Comm Time: 295.503 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 589.849 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.6864	TrainAcc 0.0617	ValidAcc 0.0387	TestAcc 0.0306	BestValid 0.0387
	Epoch 50:	Loss 4.2107	TrainAcc 0.1770	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 75:	Loss 4.0268	TrainAcc 0.1792	ValidAcc 0.0851	TestAcc 0.0670	BestValid 0.0851
	Epoch 100:	Loss 4.0304	TrainAcc 0.2026	ValidAcc 0.1464	TestAcc 0.1298	BestValid 0.1464
Node 1, Pre/Post-Pipelining: 1.054 / 10.921 ms, Bubble: 40.337 ms, Compute: 91.417 ms, Comm: 19.734 ms, Imbalance: 20.228 ms
Node 5, Pre/Post-Pipelining: 1.052 / 10.995 ms, Bubble: 39.426 ms, Compute: 94.999 ms, Comm: 19.796 ms, Imbalance: 17.547 ms
Node 4, Pre/Post-Pipelining: 1.054 / 10.941 ms, Bubble: 39.617 ms, Compute: 92.116 ms, Comm: 21.291 ms, Imbalance: 18.990 ms
Node 6, Pre/Post-Pipelining: 1.050 / 11.068 ms, Bubble: 39.199 ms, Compute: 96.838 ms, Comm: 17.189 ms, Imbalance: 18.447 ms
Node 3, Pre/Post-Pipelining: 1.052 / 10.976 ms, Bubble: 39.627 ms, Compute: 92.673 ms, Comm: 20.798 ms, Imbalance: 18.649 ms
Node 7, Pre/Post-Pipelining: 1.057 / 25.336 ms, Bubble: 24.970 ms, Compute: 114.370 ms, Comm: 13.976 ms, Imbalance: 4.001 ms
Node 2, Pre/Post-Pipelining: 1.053 / 10.994 ms, Bubble: 40.132 ms, Compute: 94.326 ms, Comm: 20.951 ms, Imbalance: 16.403 ms
Node 0, Pre/Post-Pipelining: 1.057 / 10.921 ms, Bubble: 40.745 ms, Compute: 101.979 ms, Comm: 15.265 ms, Imbalance: 13.513 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.057 ms
Cluster-Wide Average, Post-Pipelining Overhead: 10.921 ms
Cluster-Wide Average, Bubble: 40.745 ms
Cluster-Wide Average, Compute: 101.979 ms
Cluster-Wide Average, Communication: 15.265 ms
Cluster-Wide Average, Imbalance: 13.513 ms
Node 0, GPU memory consumption: 6.522 GB
Node 1, GPU memory consumption: 4.924 GB
Node 3, GPU memory consumption: 4.901 GB
Node 5, GPU memory consumption: 4.924 GB
Node 2, GPU memory consumption: 4.924 GB
Node 6, GPU memory consumption: 4.924 GB
Node 4, GPU memory consumption: 4.901 GB
Node 7, GPU memory consumption: 5.006 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.610141 s---------------
------------------------node id 1,  per-epoch time: 0.610132 s---------------
------------------------node id 2,  per-epoch time: 0.610137 s---------------
------------------------node id 3,  per-epoch time: 0.610133 s---------------
------------------------node id 4,  per-epoch time: 0.610132 s---------------
------------------------node id 5,  per-epoch time: 0.610130 s---------------
------------------------node id 6,  per-epoch time: 0.610132 s---------------
------------------------node id 7,  per-epoch time: 0.610136 s---------------
************ Profiling Results ************
	Bubble: 496.445724 (ms) (83.19 percentage)
	Compute: 96.069468 (ms) (16.10 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.240917 (ms) (0.71 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.130 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 1.139 GB
	Aggregated layer-level communication throughput: 521.386 Gbps
Highest valid_acc: 0.1464
Target test_acc: 0.1298
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
