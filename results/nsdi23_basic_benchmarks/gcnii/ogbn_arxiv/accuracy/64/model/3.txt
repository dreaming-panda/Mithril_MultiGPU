Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.044 seconds.
        It takes 0.041 seconds.
        It takes 0.044 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
        It takes 0.051 seconds.
        It takes 0.053 seconds.
        It takes 0.053 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.026 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
169343, 2484941, 2484941
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.333 Gbps (per GPU), 506.662 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.293 Gbps (per GPU), 506.346 Gbps (aggregated)
The layer-level communication performance: 63.292 Gbps (per GPU), 506.337 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.257 Gbps (per GPU), 506.059 Gbps (aggregated)
The layer-level communication performance: 63.253 Gbps (per GPU), 506.028 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.222 Gbps (per GPU), 505.775 Gbps (aggregated)
The layer-level communication performance: 63.215 Gbps (per GPU), 505.718 Gbps (aggregated)
The layer-level communication performance: 63.210 Gbps (per GPU), 505.679 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.696 Gbps (per GPU), 1325.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.686 Gbps (per GPU), 1325.486 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.698 Gbps (per GPU), 1325.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.686 Gbps (per GPU), 1325.486 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.691 Gbps (per GPU), 1325.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.693 Gbps (per GPU), 1325.545 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.695 Gbps (per GPU), 1325.559 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.692 Gbps (per GPU), 1325.535 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.436 Gbps (per GPU), 915.487 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.437 Gbps (per GPU), 915.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.436 Gbps (per GPU), 915.490 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.434 Gbps (per GPU), 915.475 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.431 Gbps (per GPU), 915.449 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.426 Gbps (per GPU), 915.411 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.420 Gbps (per GPU), 915.358 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.406 Gbps (per GPU), 915.249 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.293 Gbps (per GPU), 362.347 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.293 Gbps (per GPU), 362.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.293 Gbps (per GPU), 362.348 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.293 Gbps (per GPU), 362.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.293 Gbps (per GPU), 362.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.293 Gbps (per GPU), 362.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.291 Gbps (per GPU), 362.329 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.292 Gbps (per GPU), 362.339 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.27ms  0.35ms  0.48ms  1.76  5.55K  0.06M
 chk_1  0.28ms  0.35ms  0.48ms  1.74  5.75K  0.05M
 chk_2  0.27ms  0.35ms  0.48ms  1.79  5.20K  0.07M
 chk_3  0.28ms  0.35ms  0.48ms  1.75  5.57K  0.06M
 chk_4  0.27ms  0.35ms  0.48ms  1.81  5.05K  0.08M
 chk_5  0.26ms  0.36ms  0.49ms  1.89  4.73K  0.11M
 chk_6  0.27ms  0.35ms  0.48ms  1.81  4.98K  0.08M
 chk_7  0.27ms  0.35ms  0.49ms  1.82  5.01K  0.09M
 chk_8  0.28ms  0.34ms  0.48ms  1.73  5.73K  0.05M
 chk_9  0.26ms  0.35ms  0.48ms  1.86  4.54K  0.11M
chk_10  0.27ms  0.35ms  0.48ms  1.79  5.36K  0.07M
chk_11  0.27ms  0.35ms  0.49ms  1.78  5.39K  0.08M
chk_12  0.28ms  0.34ms  0.48ms  1.73  5.77K  0.05M
chk_13  0.27ms  0.34ms  0.48ms  1.77  5.43K  0.06M
chk_14  0.27ms  0.35ms  0.48ms  1.77  5.46K  0.06M
chk_15  0.28ms  0.34ms  0.48ms  1.71  5.88K  0.04M
chk_16  0.27ms  0.35ms  0.49ms  1.78  5.50K  0.06M
chk_17  0.26ms  0.35ms  0.49ms  1.85  4.86K  0.09M
chk_18  0.27ms  0.36ms  0.50ms  1.81  5.39K  0.07M
chk_19  0.27ms  0.35ms  0.48ms  1.78  5.20K  0.07M
chk_20  0.27ms  0.34ms  0.48ms  1.76  5.51K  0.06M
chk_21  0.28ms  0.34ms  0.47ms  1.71  5.81K  0.05M
chk_22  0.27ms  0.35ms  0.48ms  1.78  5.32K  0.07M
chk_23  0.27ms  0.35ms  0.49ms  1.81  5.39K  0.07M
chk_24  0.26ms  0.35ms  0.48ms  1.86  4.62K  0.11M
chk_25  0.27ms  0.35ms  0.48ms  1.81  5.04K  0.08M
chk_26  0.26ms  0.35ms  0.48ms  1.87  4.55K  0.11M
chk_27  0.27ms  0.35ms  0.48ms  1.77  5.30K  0.06M
chk_28  0.28ms  0.35ms  0.48ms  1.76  5.58K  0.06M
chk_29  0.27ms  0.35ms  0.49ms  1.83  4.98K  0.09M
chk_30  0.27ms  0.35ms  0.49ms  1.77  5.50K  0.07M
chk_31  0.27ms  0.35ms  0.48ms  1.78  5.38K  0.07M
   Avg  0.27  0.35  0.48
   Max  0.28  0.36  0.50
   Min  0.26  0.34  0.47
 Ratio  1.08  1.06  1.05
   Var  0.00  0.00  0.00
Profiling takes 0.546 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 97.692 ms
Partition 0 [0, 9) has cost: 97.692 ms
Partition 1 [9, 17) has cost: 89.036 ms
Partition 2 [17, 25) has cost: 89.036 ms
Partition 3 [25, 33) has cost: 89.036 ms
Partition 4 [33, 41) has cost: 89.036 ms
Partition 5 [41, 49) has cost: 89.036 ms
Partition 6 [49, 57) has cost: 89.036 ms
Partition 7 [57, 65) has cost: 93.384 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 47.268 ms
GPU 0, Compute+Comm Time: 39.166 ms, Bubble Time: 8.102 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.092 ms, Bubble Time: 8.176 ms, Imbalance Overhead: 3.001 ms
GPU 2, Compute+Comm Time: 36.092 ms, Bubble Time: 8.253 ms, Imbalance Overhead: 2.923 ms
GPU 3, Compute+Comm Time: 36.092 ms, Bubble Time: 8.321 ms, Imbalance Overhead: 2.856 ms
GPU 4, Compute+Comm Time: 36.092 ms, Bubble Time: 8.390 ms, Imbalance Overhead: 2.786 ms
GPU 5, Compute+Comm Time: 36.092 ms, Bubble Time: 8.454 ms, Imbalance Overhead: 2.723 ms
GPU 6, Compute+Comm Time: 36.092 ms, Bubble Time: 8.515 ms, Imbalance Overhead: 2.661 ms
GPU 7, Compute+Comm Time: 37.057 ms, Bubble Time: 8.567 ms, Imbalance Overhead: 1.643 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.153 ms
GPU 0, Compute+Comm Time: 67.279 ms, Bubble Time: 15.269 ms, Imbalance Overhead: 1.606 ms
GPU 1, Compute+Comm Time: 63.896 ms, Bubble Time: 15.222 ms, Imbalance Overhead: 5.035 ms
GPU 2, Compute+Comm Time: 63.896 ms, Bubble Time: 15.135 ms, Imbalance Overhead: 5.122 ms
GPU 3, Compute+Comm Time: 63.896 ms, Bubble Time: 15.064 ms, Imbalance Overhead: 5.193 ms
GPU 4, Compute+Comm Time: 63.896 ms, Bubble Time: 14.970 ms, Imbalance Overhead: 5.287 ms
GPU 5, Compute+Comm Time: 63.896 ms, Bubble Time: 14.912 ms, Imbalance Overhead: 5.345 ms
GPU 6, Compute+Comm Time: 63.896 ms, Bubble Time: 14.766 ms, Imbalance Overhead: 5.491 ms
GPU 7, Compute+Comm Time: 69.478 ms, Bubble Time: 14.675 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 137.993 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 186.728 ms
Partition 0 [0, 17) has cost: 186.728 ms
Partition 1 [17, 33) has cost: 178.072 ms
Partition 2 [33, 49) has cost: 178.072 ms
Partition 3 [49, 65) has cost: 182.420 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 55.107 ms
GPU 0, Compute+Comm Time: 46.540 ms, Bubble Time: 8.507 ms, Imbalance Overhead: 0.060 ms
GPU 1, Compute+Comm Time: 44.991 ms, Bubble Time: 8.619 ms, Imbalance Overhead: 1.497 ms
GPU 2, Compute+Comm Time: 44.991 ms, Bubble Time: 8.705 ms, Imbalance Overhead: 1.411 ms
GPU 3, Compute+Comm Time: 45.475 ms, Bubble Time: 8.737 ms, Imbalance Overhead: 0.895 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 89.774 ms
GPU 0, Compute+Comm Time: 74.571 ms, Bubble Time: 14.286 ms, Imbalance Overhead: 0.918 ms
GPU 1, Compute+Comm Time: 72.874 ms, Bubble Time: 14.204 ms, Imbalance Overhead: 2.695 ms
GPU 2, Compute+Comm Time: 72.874 ms, Bubble Time: 14.116 ms, Imbalance Overhead: 2.783 ms
GPU 3, Compute+Comm Time: 75.704 ms, Bubble Time: 13.921 ms, Imbalance Overhead: 0.149 ms
    The estimated cost with 2 DP ways is 152.125 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 364.800 ms
Partition 0 [0, 33) has cost: 364.800 ms
Partition 1 [33, 65) has cost: 360.492 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 86.205 ms
GPU 0, Compute+Comm Time: 75.586 ms, Bubble Time: 9.371 ms, Imbalance Overhead: 1.249 ms
GPU 1, Compute+Comm Time: 75.054 ms, Bubble Time: 9.630 ms, Imbalance Overhead: 1.522 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 118.322 ms
GPU 0, Compute+Comm Time: 103.694 ms, Bubble Time: 13.200 ms, Imbalance Overhead: 1.428 ms
GPU 1, Compute+Comm Time: 104.270 ms, Bubble Time: 12.941 ms, Imbalance Overhead: 1.111 ms
    The estimated cost with 4 DP ways is 214.753 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 725.292 ms
Partition 0 [0, 65) has cost: 725.292 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 264.949 ms
GPU 0, Compute+Comm Time: 264.949 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 293.809 ms
GPU 0, Compute+Comm Time: 293.809 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 586.695 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.4101	TrainAcc 0.0254	ValidAcc 0.0160	TestAcc 0.0112	BestValid 0.0160
	Epoch 50:	Loss 4.0842	TrainAcc 0.0536	ValidAcc 0.0491	TestAcc 0.0389	BestValid 0.0491
	Epoch 75:	Loss 4.1604	TrainAcc 0.0549	ValidAcc 0.0444	TestAcc 0.0339	BestValid 0.0491
	Epoch 100:	Loss 4.0401	TrainAcc 0.0455	ValidAcc 0.0354	TestAcc 0.0283	BestValid 0.0491
Node 0, Pre/Post-Pipelining: 1.069 / 10.797 ms, Bubble: 40.044 ms, Compute: 101.028 ms, Comm: 15.592 ms, Imbalance: 11.817 ms
Node 4, Pre/Post-Pipelining: 1.059 / 10.931 ms, Bubble: 38.958 ms, Compute: 95.912 ms, Comm: 21.162 ms, Imbalance: 12.821 ms
Node 1, Pre/Post-Pipelining: 1.063 / 10.824 ms, Bubble: 39.647 ms, Compute: 91.997 ms, Comm: 19.829 ms, Imbalance: 17.168 ms
Node 3, Pre/Post-Pipelining: 1.057 / 10.940 ms, Bubble: 38.834 ms, Compute: 96.585 ms, Comm: 20.349 ms, Imbalance: 12.769 ms
Node 5, Pre/Post-Pipelining: 1.064 / 10.836 ms, Bubble: 39.184 ms, Compute: 91.861 ms, Comm: 20.320 ms, Imbalance: 17.515 ms
Node 6, Pre/Post-Pipelining: 1.058 / 10.981 ms, Bubble: 38.824 ms, Compute: 96.388 ms, Comm: 17.583 ms, Imbalance: 15.823 ms
Node 7, Pre/Post-Pipelining: 1.072 / 25.026 ms, Bubble: 24.907 ms, Compute: 110.025 ms, Comm: 13.632 ms, Imbalance: 5.922 ms
Node 2, Pre/Post-Pipelining: 1.062 / 10.885 ms, Bubble: 39.429 ms, Compute: 95.991 ms, Comm: 20.505 ms, Imbalance: 12.791 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.069 ms
Cluster-Wide Average, Post-Pipelining Overhead: 10.797 ms
Cluster-Wide Average, Bubble: 40.044 ms
Cluster-Wide Average, Compute: 101.028 ms
Cluster-Wide Average, Communication: 15.592 ms
Cluster-Wide Average, Imbalance: 11.817 ms
Node 0, GPU memory consumption: 6.522 GB
Node 2, GPU memory consumption: 4.924 GB
Node 1, GPU memory consumption: 4.924 GB
Node 3, GPU memory consumption: 4.901 GB
Node 4, GPU memory consumption: 4.901 GB
Node 5, GPU memory consumption: 4.924 GB
Node 6, GPU memory consumption: 4.924 GB
Node 7, GPU memory consumption: 5.005 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.604425 s---------------
------------------------node id 1,  per-epoch time: 0.604427 s---------------
------------------------node id 2,  per-epoch time: 0.604427 s---------------
------------------------node id 3,  per-epoch time: 0.604425 s---------------
------------------------node id 4,  per-epoch time: 0.604424 s---------------
------------------------node id 5,  per-epoch time: 0.604426 s---------------
------------------------node id 6,  per-epoch time: 0.604424 s---------------
------------------------node id 7,  per-epoch time: 0.604428 s---------------
************ Profiling Results ************
	Bubble: 490.724438 (ms) (83.02 percentage)
	Compute: 96.129220 (ms) (16.26 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.251785 (ms) (0.72 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.130 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 1.139 GB
	Aggregated layer-level communication throughput: 521.489 Gbps
Highest valid_acc: 0.0491
Target test_acc: 0.0389
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
