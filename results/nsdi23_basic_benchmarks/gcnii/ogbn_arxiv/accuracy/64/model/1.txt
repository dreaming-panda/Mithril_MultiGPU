Initialized node 6 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.048 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.042 seconds.
        It takes 0.047 seconds.
        It takes 0.051 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.055 seconds.
Building the Feature Vector...
        It takes 0.056 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.026 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 64.089 Gbps (per GPU), 512.710 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.046 Gbps (per GPU), 512.368 Gbps (aggregated)
The layer-level communication performance: 64.044 Gbps (per GPU), 512.356 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.008 Gbps (per GPU), 512.066 Gbps (aggregated)
The layer-level communication performance: 64.004 Gbps (per GPU), 512.030 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.969 Gbps (per GPU), 511.754 Gbps (aggregated)
The layer-level communication performance: 63.962 Gbps (per GPU), 511.700 Gbps (aggregated)
The layer-level communication performance: 63.958 Gbps (per GPU), 511.662 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.074 Gbps (per GPU), 1328.592 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.060 Gbps (per GPU), 1328.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.059 Gbps (per GPU), 1328.474 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.061 Gbps (per GPU), 1328.490 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.059 Gbps (per GPU), 1328.471 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.057 Gbps (per GPU), 1328.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.054 Gbps (per GPU), 1328.435 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.065 Gbps (per GPU), 1328.517 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.169 Gbps (per GPU), 913.349 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.169 Gbps (per GPU), 913.355 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.168 Gbps (per GPU), 913.341 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.168 Gbps (per GPU), 913.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.163 Gbps (per GPU), 913.303 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.162 Gbps (per GPU), 913.294 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.164 Gbps (per GPU), 913.314 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.165 Gbps (per GPU), 913.323 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.612 Gbps (per GPU), 364.893 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.612 Gbps (per GPU), 364.895 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.612 Gbps (per GPU), 364.894 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.612 Gbps (per GPU), 364.893 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.612 Gbps (per GPU), 364.894 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.612 Gbps (per GPU), 364.893 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.611 Gbps (per GPU), 364.891 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.612 Gbps (per GPU), 364.893 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.27ms  0.35ms  0.49ms  1.77  5.55K  0.06M
 chk_1  0.28ms  0.35ms  0.48ms  1.75  5.75K  0.05M
 chk_2  0.27ms  0.35ms  0.48ms  1.79  5.20K  0.07M
 chk_3  0.27ms  0.35ms  0.48ms  1.76  5.57K  0.06M
 chk_4  0.27ms  0.35ms  0.49ms  1.80  5.05K  0.08M
 chk_5  0.26ms  0.36ms  0.49ms  1.89  4.73K  0.11M
 chk_6  0.27ms  0.35ms  0.48ms  1.81  4.98K  0.08M
 chk_7  0.27ms  0.35ms  0.49ms  1.83  5.01K  0.09M
 chk_8  0.28ms  0.34ms  0.48ms  1.75  5.73K  0.05M
 chk_9  0.26ms  0.35ms  0.48ms  1.86  4.54K  0.11M
chk_10  0.27ms  0.35ms  0.49ms  1.78  5.36K  0.07M
chk_11  0.27ms  0.35ms  0.49ms  1.80  5.39K  0.08M
chk_12  0.28ms  0.34ms  0.48ms  1.74  5.77K  0.05M
chk_13  0.27ms  0.34ms  0.49ms  1.78  5.43K  0.06M
chk_14  0.27ms  0.35ms  0.49ms  1.78  5.46K  0.06M
chk_15  0.28ms  0.34ms  0.48ms  1.72  5.88K  0.04M
chk_16  0.28ms  0.35ms  0.49ms  1.78  5.50K  0.06M
chk_17  0.26ms  0.35ms  0.49ms  1.84  4.86K  0.09M
chk_18  0.27ms  0.36ms  0.50ms  1.81  5.39K  0.07M
chk_19  0.27ms  0.35ms  0.48ms  1.78  5.20K  0.07M
chk_20  0.27ms  0.35ms  0.48ms  1.76  5.51K  0.06M
chk_21  0.28ms  0.34ms  0.48ms  1.71  5.81K  0.05M
chk_22  0.27ms  0.35ms  0.49ms  1.78  5.32K  0.07M
chk_23  0.27ms  0.36ms  0.49ms  1.82  5.39K  0.07M
chk_24  0.26ms  0.35ms  0.48ms  1.87  4.62K  0.11M
chk_25  0.27ms  0.35ms  0.48ms  1.81  5.04K  0.08M
chk_26  0.26ms  0.36ms  0.48ms  1.87  4.55K  0.11M
chk_27  0.27ms  0.35ms  0.48ms  1.78  5.30K  0.06M
chk_28  0.28ms  0.35ms  0.49ms  1.75  5.58K  0.06M
chk_29  0.27ms  0.35ms  0.49ms  1.84  4.98K  0.09M
chk_30  0.27ms  0.35ms  0.49ms  1.77  5.50K  0.07M
chk_31  0.27ms  0.35ms  0.49ms  1.79  5.38K  0.07M
   Avg  0.27  0.35  0.49
   Max  0.28  0.36  0.50
   Min  0.26  0.34  0.48
 Ratio  1.08  1.06  1.05
   Var  0.00  0.00  0.00
Profiling takes 0.549 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 98.184 ms
Partition 0 [0, 9) has cost: 98.184 ms
Partition 1 [9, 17) has cost: 89.507 ms
Partition 2 [17, 25) has cost: 89.507 ms
Partition 3 [25, 33) has cost: 89.507 ms
Partition 4 [33, 41) has cost: 89.507 ms
Partition 5 [41, 49) has cost: 89.507 ms
Partition 6 [49, 57) has cost: 89.507 ms
Partition 7 [57, 65) has cost: 93.875 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 47.359 ms
GPU 0, Compute+Comm Time: 39.248 ms, Bubble Time: 8.111 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.170 ms, Bubble Time: 8.212 ms, Imbalance Overhead: 2.976 ms
GPU 2, Compute+Comm Time: 36.170 ms, Bubble Time: 8.292 ms, Imbalance Overhead: 2.896 ms
GPU 3, Compute+Comm Time: 36.170 ms, Bubble Time: 8.366 ms, Imbalance Overhead: 2.823 ms
GPU 4, Compute+Comm Time: 36.170 ms, Bubble Time: 8.422 ms, Imbalance Overhead: 2.767 ms
GPU 5, Compute+Comm Time: 36.170 ms, Bubble Time: 8.487 ms, Imbalance Overhead: 2.702 ms
GPU 6, Compute+Comm Time: 36.170 ms, Bubble Time: 8.553 ms, Imbalance Overhead: 2.636 ms
GPU 7, Compute+Comm Time: 37.126 ms, Bubble Time: 8.611 ms, Imbalance Overhead: 1.622 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.569 ms
GPU 0, Compute+Comm Time: 67.572 ms, Bubble Time: 15.264 ms, Imbalance Overhead: 1.733 ms
GPU 1, Compute+Comm Time: 64.159 ms, Bubble Time: 15.232 ms, Imbalance Overhead: 5.178 ms
GPU 2, Compute+Comm Time: 64.159 ms, Bubble Time: 15.150 ms, Imbalance Overhead: 5.260 ms
GPU 3, Compute+Comm Time: 64.159 ms, Bubble Time: 15.086 ms, Imbalance Overhead: 5.324 ms
GPU 4, Compute+Comm Time: 64.159 ms, Bubble Time: 14.989 ms, Imbalance Overhead: 5.421 ms
GPU 5, Compute+Comm Time: 64.159 ms, Bubble Time: 14.912 ms, Imbalance Overhead: 5.498 ms
GPU 6, Compute+Comm Time: 64.159 ms, Bubble Time: 14.893 ms, Imbalance Overhead: 5.517 ms
GPU 7, Compute+Comm Time: 69.759 ms, Bubble Time: 14.808 ms, Imbalance Overhead: 0.002 ms
The estimated cost of the whole pipeline: 138.525 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 187.691 ms
Partition 0 [0, 17) has cost: 187.691 ms
Partition 1 [17, 33) has cost: 179.014 ms
Partition 2 [33, 49) has cost: 179.014 ms
Partition 3 [49, 65) has cost: 183.382 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 55.267 ms
GPU 0, Compute+Comm Time: 46.585 ms, Bubble Time: 8.523 ms, Imbalance Overhead: 0.159 ms
GPU 1, Compute+Comm Time: 45.038 ms, Bubble Time: 8.675 ms, Imbalance Overhead: 1.554 ms
GPU 2, Compute+Comm Time: 45.038 ms, Bubble Time: 8.751 ms, Imbalance Overhead: 1.477 ms
GPU 3, Compute+Comm Time: 45.514 ms, Bubble Time: 8.790 ms, Imbalance Overhead: 0.963 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 90.347 ms
GPU 0, Compute+Comm Time: 74.953 ms, Bubble Time: 14.281 ms, Imbalance Overhead: 1.113 ms
GPU 1, Compute+Comm Time: 73.247 ms, Bubble Time: 14.209 ms, Imbalance Overhead: 2.891 ms
GPU 2, Compute+Comm Time: 73.247 ms, Bubble Time: 14.108 ms, Imbalance Overhead: 2.992 ms
GPU 3, Compute+Comm Time: 76.091 ms, Bubble Time: 14.147 ms, Imbalance Overhead: 0.109 ms
    The estimated cost with 2 DP ways is 152.894 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 366.704 ms
Partition 0 [0, 33) has cost: 366.704 ms
Partition 1 [33, 65) has cost: 362.396 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 86.411 ms
GPU 0, Compute+Comm Time: 75.746 ms, Bubble Time: 9.398 ms, Imbalance Overhead: 1.266 ms
GPU 1, Compute+Comm Time: 75.209 ms, Bubble Time: 9.660 ms, Imbalance Overhead: 1.541 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 118.629 ms
GPU 0, Compute+Comm Time: 104.346 ms, Bubble Time: 13.242 ms, Imbalance Overhead: 1.042 ms
GPU 1, Compute+Comm Time: 104.927 ms, Bubble Time: 12.955 ms, Imbalance Overhead: 0.747 ms
    The estimated cost with 4 DP ways is 215.292 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 729.100 ms
Partition 0 [0, 65) has cost: 729.100 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 263.442 ms
GPU 0, Compute+Comm Time: 263.442 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 293.112 ms
GPU 0, Compute+Comm Time: 293.112 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 584.381 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.1279	TrainAcc 0.1494	ValidAcc 0.0685	TestAcc 0.0553	BestValid 0.0685
	Epoch 50:	Loss 4.2739	TrainAcc 0.1296	ValidAcc 0.0636	TestAcc 0.0511	BestValid 0.0685
	Epoch 75:	Loss 4.8949	TrainAcc 0.1702	ValidAcc 0.1065	TestAcc 0.0875	BestValid 0.1065
	Epoch 100:	Loss 4.9193	TrainAcc 0.0681	ValidAcc 0.0709	TestAcc 0.0884	BestValid 0.1065
Node 0, Pre/Post-Pipelining: 1.075 / 10.912 ms, Bubble: 40.795 ms, Compute: 101.417 ms, Comm: 15.519 ms, Imbalance: 13.715 ms
Node 5, Pre/Post-Pipelining: 1.079 / 10.923 ms, Bubble: 39.578 ms, Compute: 92.301 ms, Comm: 20.563 ms, Imbalance: 19.415 ms
Node 3, Pre/Post-Pipelining: 1.076 / 11.053 ms, Bubble: 39.490 ms, Compute: 96.638 ms, Comm: 20.676 ms, Imbalance: 14.732 ms
Node 4, Pre/Post-Pipelining: 1.081 / 10.933 ms, Bubble: 39.666 ms, Compute: 92.193 ms, Comm: 21.460 ms, Imbalance: 18.650 ms
Node 1, Pre/Post-Pipelining: 1.081 / 10.934 ms, Bubble: 40.372 ms, Compute: 92.282 ms, Comm: 19.824 ms, Imbalance: 19.145 ms
Node 2, Pre/Post-Pipelining: 1.062 / 10.992 ms, Bubble: 40.118 ms, Compute: 95.549 ms, Comm: 20.783 ms, Imbalance: 15.260 ms
Node 7, Pre/Post-Pipelining: 1.077 / 25.287 ms, Bubble: 24.906 ms, Compute: 114.565 ms, Comm: 13.995 ms, Imbalance: 3.853 ms
Node 6, Pre/Post-Pipelining: 1.059 / 11.057 ms, Bubble: 39.130 ms, Compute: 96.139 ms, Comm: 17.864 ms, Imbalance: 18.489 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.075 ms
Cluster-Wide Average, Post-Pipelining Overhead: 10.912 ms
Cluster-Wide Average, Bubble: 40.795 ms
Cluster-Wide Average, Compute: 101.417 ms
Cluster-Wide Average, Communication: 15.519 ms
Cluster-Wide Average, Imbalance: 13.715 ms
Node 0, GPU memory consumption: 6.522 GB
Node 1, GPU memory consumption: 4.924 GB
Node 3, GPU memory consumption: 4.901 GB
Node 2, GPU memory consumption: 4.924 GB
Node 4, GPU memory consumption: 4.901 GB
Node 5, GPU memory consumption: 4.924 GB
Node 7, GPU memory consumption: 5.005 GB
Node 6, GPU memory consumption: 4.924 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.607700 s---------------
------------------------node id 1,  per-epoch time: 0.607702 s---------------
------------------------node id 2,  per-epoch time: 0.607701 s---------------
------------------------node id 3,  per-epoch time: 0.607699 s---------------
------------------------node id 4,  per-epoch time: 0.607699 s---------------
------------------------node id 5,  per-epoch time: 0.607699 s---------------
------------------------node id 6,  per-epoch time: 0.607703 s---------------
------------------------node id 7,  per-epoch time: 0.607702 s---------------
************ Profiling Results ************
	Bubble: 493.779635 (ms) (83.06 percentage)
	Compute: 96.439463 (ms) (16.22 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.254562 (ms) (0.72 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.130 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 1.139 GB
	Aggregated layer-level communication throughput: 515.561 Gbps
Highest valid_acc: 0.1065
Target test_acc: 0.0875
Epoch to reach the target acc: 74
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
