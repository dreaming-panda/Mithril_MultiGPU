Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.066 seconds.
Building the CSC structure...
        It takes 0.042 seconds.
        It takes 0.047 seconds.
        It takes 0.054 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.024 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 64.198 Gbps (per GPU), 513.588 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.158 Gbps (per GPU), 513.263 Gbps (aggregated)
The layer-level communication performance: 64.157 Gbps (per GPU), 513.254 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.123 Gbps (per GPU), 512.985 Gbps (aggregated)
The layer-level communication performance: 64.119 Gbps (per GPU), 512.952 Gbps (aggregated)
The layer-level communication performance: 64.088 Gbps (per GPU), 512.706 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.081 Gbps (per GPU), 512.651 Gbps (aggregated)
The layer-level communication performance: 64.077 Gbps (per GPU), 512.615 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.988 Gbps (per GPU), 1327.903 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.975 Gbps (per GPU), 1327.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.985 Gbps (per GPU), 1327.876 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.977 Gbps (per GPU), 1327.817 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.982 Gbps (per GPU), 1327.860 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.989 Gbps (per GPU), 1327.909 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.983 Gbps (per GPU), 1327.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.985 Gbps (per GPU), 1327.883 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.246 Gbps (per GPU), 913.965 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.246 Gbps (per GPU), 913.971 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.244 Gbps (per GPU), 913.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.243 Gbps (per GPU), 913.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.240 Gbps (per GPU), 913.921 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.241 Gbps (per GPU), 913.931 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.243 Gbps (per GPU), 913.942 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.241 Gbps (per GPU), 913.931 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.930 Gbps (per GPU), 359.436 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.930 Gbps (per GPU), 359.437 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.929 Gbps (per GPU), 359.435 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.929 Gbps (per GPU), 359.435 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.929 Gbps (per GPU), 359.435 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.929 Gbps (per GPU), 359.434 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.929 Gbps (per GPU), 359.434 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.929 Gbps (per GPU), 359.429 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.57ms  0.89ms  1.12ms  1.96 21.17K  0.46M
 chk_1  0.57ms  0.93ms  1.16ms  2.04 21.17K  0.55M
 chk_2  0.57ms  0.86ms  1.09ms  1.92 21.17K  0.39M
 chk_3  0.57ms  0.83ms  1.06ms  1.85 21.17K  0.24M
 chk_4  0.57ms  0.80ms  1.03ms  1.80 21.17K  0.17M
 chk_5  0.57ms  0.81ms  1.04ms  1.82 21.17K  0.22M
 chk_6  0.57ms  0.80ms  1.03ms  1.80 21.17K  0.16M
 chk_7  0.57ms  0.79ms  1.02ms  1.78 21.17K  0.12M
   Avg  0.57  0.84  1.07
   Max  0.57  0.93  1.16
   Min  0.57  0.79  1.02
 Ratio  1.00  1.18  1.14
   Var  0.00  0.00  0.00
Profiling takes 0.322 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 58.260 ms
Partition 0 [0, 9) has cost: 58.260 ms
Partition 1 [9, 17) has cost: 53.692 ms
Partition 2 [17, 25) has cost: 53.692 ms
Partition 3 [25, 33) has cost: 53.692 ms
Partition 4 [33, 41) has cost: 53.692 ms
Partition 5 [41, 49) has cost: 53.692 ms
Partition 6 [49, 57) has cost: 53.692 ms
Partition 7 [57, 65) has cost: 55.530 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 44.748 ms
GPU 0, Compute+Comm Time: 23.897 ms, Bubble Time: 19.964 ms, Imbalance Overhead: 0.888 ms
GPU 1, Compute+Comm Time: 22.590 ms, Bubble Time: 19.987 ms, Imbalance Overhead: 2.171 ms
GPU 2, Compute+Comm Time: 22.590 ms, Bubble Time: 20.276 ms, Imbalance Overhead: 1.882 ms
GPU 3, Compute+Comm Time: 22.590 ms, Bubble Time: 20.498 ms, Imbalance Overhead: 1.660 ms
GPU 4, Compute+Comm Time: 22.590 ms, Bubble Time: 20.806 ms, Imbalance Overhead: 1.352 ms
GPU 5, Compute+Comm Time: 22.590 ms, Bubble Time: 21.080 ms, Imbalance Overhead: 1.078 ms
GPU 6, Compute+Comm Time: 22.590 ms, Bubble Time: 21.382 ms, Imbalance Overhead: 0.776 ms
GPU 7, Compute+Comm Time: 22.993 ms, Bubble Time: 21.716 ms, Imbalance Overhead: 0.040 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.067 ms
GPU 0, Compute+Comm Time: 43.341 ms, Bubble Time: 40.717 ms, Imbalance Overhead: 0.009 ms
GPU 1, Compute+Comm Time: 41.906 ms, Bubble Time: 40.138 ms, Imbalance Overhead: 2.024 ms
GPU 2, Compute+Comm Time: 41.906 ms, Bubble Time: 39.639 ms, Imbalance Overhead: 2.522 ms
GPU 3, Compute+Comm Time: 41.906 ms, Bubble Time: 39.176 ms, Imbalance Overhead: 2.985 ms
GPU 4, Compute+Comm Time: 41.906 ms, Bubble Time: 38.658 ms, Imbalance Overhead: 3.503 ms
GPU 5, Compute+Comm Time: 41.906 ms, Bubble Time: 38.244 ms, Imbalance Overhead: 3.917 ms
GPU 6, Compute+Comm Time: 41.906 ms, Bubble Time: 37.670 ms, Imbalance Overhead: 4.491 ms
GPU 7, Compute+Comm Time: 45.168 ms, Bubble Time: 37.631 ms, Imbalance Overhead: 1.268 ms
The estimated cost of the whole pipeline: 135.256 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 111.952 ms
Partition 0 [0, 17) has cost: 111.952 ms
Partition 1 [17, 33) has cost: 107.383 ms
Partition 2 [33, 49) has cost: 107.383 ms
Partition 3 [49, 65) has cost: 109.221 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 56.241 ms
GPU 0, Compute+Comm Time: 31.851 ms, Bubble Time: 23.122 ms, Imbalance Overhead: 1.268 ms
GPU 1, Compute+Comm Time: 31.197 ms, Bubble Time: 23.589 ms, Imbalance Overhead: 1.456 ms
GPU 2, Compute+Comm Time: 31.197 ms, Bubble Time: 24.188 ms, Imbalance Overhead: 0.857 ms
GPU 3, Compute+Comm Time: 31.400 ms, Bubble Time: 24.842 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.480 ms
GPU 0, Compute+Comm Time: 51.492 ms, Bubble Time: 40.988 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 50.772 ms, Bubble Time: 39.812 ms, Imbalance Overhead: 1.896 ms
GPU 2, Compute+Comm Time: 50.772 ms, Bubble Time: 38.707 ms, Imbalance Overhead: 3.002 ms
GPU 3, Compute+Comm Time: 52.404 ms, Bubble Time: 37.784 ms, Imbalance Overhead: 2.292 ms
    The estimated cost with 2 DP ways is 156.158 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 219.335 ms
Partition 0 [0, 33) has cost: 219.335 ms
Partition 1 [33, 65) has cost: 216.605 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 90.432 ms
GPU 0, Compute+Comm Time: 60.005 ms, Bubble Time: 29.240 ms, Imbalance Overhead: 1.186 ms
GPU 1, Compute+Comm Time: 59.780 ms, Bubble Time: 30.652 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 121.465 ms
GPU 0, Compute+Comm Time: 79.968 ms, Bubble Time: 41.498 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 80.422 ms, Bubble Time: 38.697 ms, Imbalance Overhead: 2.346 ms
    The estimated cost with 4 DP ways is 222.492 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 435.940 ms
Partition 0 [0, 65) has cost: 435.940 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 235.495 ms
GPU 0, Compute+Comm Time: 235.495 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 257.072 ms
GPU 0, Compute+Comm Time: 257.072 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 517.196 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 457)...
+++++++++ Node 0 initializing the weights for op[0, 457)...
+++++++++ Node 4 initializing the weights for op[0, 457)...
+++++++++ Node 5 initializing the weights for op[0, 457)...
+++++++++ Node 2 initializing the weights for op[0, 457)...
+++++++++ Node 3 initializing the weights for op[0, 457)...
+++++++++ Node 6 initializing the weights for op[0, 457)...
+++++++++ Node 7 initializing the weights for op[0, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000



*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.1746	TrainAcc 0.1885	ValidAcc 0.0949	TestAcc 0.0719	BestValid 0.0949
	Epoch 50:	Loss 2.9605	TrainAcc 0.2778	ValidAcc 0.2992	TestAcc 0.2693	BestValid 0.2992
	Epoch 75:	Loss 2.7782	TrainAcc 0.3040	ValidAcc 0.3116	TestAcc 0.2808	BestValid 0.3116
Node 1, Pre/Post-Pipelining: 8.340 / 17.090 ms, Bubble: 0.407 ms, Compute: 203.957 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 3, Pre/Post-Pipelining: 8.343 / 17.197 ms, Bubble: 0.088 ms, Compute: 204.176 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
	Epoch 100:	Loss 2.6102	TrainAcc 0.3547	ValidAcc 0.3430	TestAcc 0.3109	BestValid 0.3430
Node 4, Pre/Post-Pipelining: 8.375 / 17.103 ms, Bubble: 0.563 ms, Compute: 203.746 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 5, Pre/Post-Pipelining: 8.373 / 17.139 ms, Bubble: 0.636 ms, Compute: 203.642 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 6, Pre/Post-Pipelining: 8.395 / 17.285 ms, Bubble: 0.108 ms, Compute: 204.009 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 0, Pre/Post-Pipelining: 8.335 / 17.050 ms, Bubble: 0.585 ms, Compute: 203.816 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 2, Pre/Post-Pipelining: 8.345 / 17.143 ms, Bubble: 0.483 ms, Compute: 203.821 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 8.390 / 17.173 ms, Bubble: 0.670 ms, Compute: 203.567 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.335 ms
Cluster-Wide Average, Post-Pipelining Overhead: 17.050 ms
Cluster-Wide Average, Bubble: 0.585 ms
Cluster-Wide Average, Compute: 203.816 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.015 ms
Node 0, GPU memory consumption: 19.053 GB
Node 3, GPU memory consumption: 18.141 GB
Node 1, GPU memory consumption: 18.165 GB
Node 2, GPU memory consumption: 18.165 GB
Node 4, GPU memory consumption: 18.139 GB
Node 5, GPU memory consumption: 18.163 GB
Node 6, GPU memory consumption: 18.163 GB
Node 7, GPU memory consumption: 18.139 GB
Node 0, Graph-Level Communication Throughput: 47.396 Gbps, Time: 121.903 ms
Node 1, Graph-Level Communication Throughput: 58.703 Gbps, Time: 117.070 ms
Node 2, Graph-Level Communication Throughput: 39.942 Gbps, Time: 121.740 ms
Node 3, Graph-Level Communication Throughput: 41.195 Gbps, Time: 123.140 ms
Node 4, Graph-Level Communication Throughput: 19.553 Gbps, Time: 127.645 ms
Node 5, Graph-Level Communication Throughput: 9.923 Gbps, Time: 127.916 ms
Node 6, Graph-Level Communication Throughput: 17.773 Gbps, Time: 125.964 ms
Node 7, Graph-Level Communication Throughput: 10.789 Gbps, Time: 127.464 ms
------------------------node id 0,  per-epoch time: 0.656308 s---------------
------------------------node id 1,  per-epoch time: 0.656306 s---------------
------------------------node id 2,  per-epoch time: 0.656308 s---------------
------------------------node id 3,  per-epoch time: 0.656305 s---------------
------------------------node id 4,  per-epoch time: 0.656303 s---------------
------------------------node id 5,  per-epoch time: 0.656306 s---------------
------------------------node id 6,  per-epoch time: 0.656307 s---------------
------------------------node id 7,  per-epoch time: 0.656308 s---------------
************ Profiling Results ************
	Bubble: 431.652847 (ms) (67.71 percentage)
	Compute: 56.642527 (ms) (8.89 percentage)
	GraphCommComputeOverhead: 8.794458 (ms) (1.38 percentage)
	GraphCommNetwork: 124.112616 (ms) (19.47 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 16.302315 (ms) (2.56 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 3.488 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 3.496 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.3430
Target test_acc: 0.3109
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
