Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...Building the CSR structure...

Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.068 seconds.
Building the CSC structure...
        It takes 0.042 seconds.
        It takes 0.047 seconds.
        It takes 0.047 seconds.
Building the Feature Vector...
        It takes 0.052 seconds.
        It takes 0.049 seconds.
        It takes 0.050 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.027 seconds.
        It takes 0.027 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.669 Gbps (per GPU), 509.351 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.629 Gbps (per GPU), 509.034 Gbps (aggregated)
The layer-level communication performance: 63.628 Gbps (per GPU), 509.026 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.593 Gbps (per GPU), 508.744 Gbps (aggregated)
The layer-level communication performance: 63.589 Gbps (per GPU), 508.711 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.557 Gbps (per GPU), 508.457 Gbps (aggregated)
The layer-level communication performance: 63.550 Gbps (per GPU), 508.400 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.545 Gbps (per GPU), 508.363 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.882 Gbps (per GPU), 1327.059 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.866 Gbps (per GPU), 1326.930 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.864 Gbps (per GPU), 1326.914 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.864 Gbps (per GPU), 1326.911 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.861 Gbps (per GPU), 1326.888 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.861 Gbps (per GPU), 1326.891 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.859 Gbps (per GPU), 1326.872 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.868 Gbps (per GPU), 1326.947 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.372 Gbps (per GPU), 914.972 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.371 Gbps (per GPU), 914.970 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.370 Gbps (per GPU), 914.960 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.370 Gbps (per GPU), 914.958 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.364 Gbps (per GPU), 914.910 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.362 Gbps (per GPU), 914.892 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.356 Gbps (per GPU), 914.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.341 Gbps (per GPU), 914.727 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.770 Gbps (per GPU), 366.159 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.770 Gbps (per GPU), 366.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.770 Gbps (per GPU), 366.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.769 Gbps (per GPU), 366.153 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.770 Gbps (per GPU), 366.157 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.769 Gbps (per GPU), 366.153 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.770 Gbps (per GPU), 366.157 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.768 Gbps (per GPU), 366.147 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.57ms  0.90ms  1.13ms  1.97 21.17K  0.46M
 chk_1  0.58ms  0.94ms  1.17ms  2.03 21.17K  0.55M
 chk_2  0.57ms  0.87ms  1.10ms  1.92 21.17K  0.39M
 chk_3  0.57ms  0.84ms  1.06ms  1.85 21.17K  0.24M
 chk_4  0.57ms  0.80ms  1.03ms  1.79 21.17K  0.17M
 chk_5  0.57ms  0.81ms  1.04ms  1.82 21.17K  0.22M
 chk_6  0.57ms  0.80ms  1.03ms  1.80 21.17K  0.16M
 chk_7  0.57ms  0.79ms  1.02ms  1.78 21.17K  0.12M
   Avg  0.57  0.84  1.07
   Max  0.58  0.94  1.17
   Min  0.57  0.79  1.02
 Ratio  1.00  1.18  1.15
   Var  0.00  0.00  0.00
Profiling takes 0.317 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 58.637 ms
Partition 0 [0, 9) has cost: 58.637 ms
Partition 1 [9, 17) has cost: 54.041 ms
Partition 2 [17, 25) has cost: 54.041 ms
Partition 3 [25, 33) has cost: 54.041 ms
Partition 4 [33, 41) has cost: 54.041 ms
Partition 5 [41, 49) has cost: 54.041 ms
Partition 6 [49, 57) has cost: 54.041 ms
Partition 7 [57, 65) has cost: 55.882 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 45.073 ms
GPU 0, Compute+Comm Time: 24.061 ms, Bubble Time: 20.101 ms, Imbalance Overhead: 0.910 ms
GPU 1, Compute+Comm Time: 22.747 ms, Bubble Time: 20.118 ms, Imbalance Overhead: 2.208 ms
GPU 2, Compute+Comm Time: 22.747 ms, Bubble Time: 20.409 ms, Imbalance Overhead: 1.917 ms
GPU 3, Compute+Comm Time: 22.747 ms, Bubble Time: 20.628 ms, Imbalance Overhead: 1.698 ms
GPU 4, Compute+Comm Time: 22.747 ms, Bubble Time: 20.939 ms, Imbalance Overhead: 1.387 ms
GPU 5, Compute+Comm Time: 22.747 ms, Bubble Time: 21.223 ms, Imbalance Overhead: 1.103 ms
GPU 6, Compute+Comm Time: 22.747 ms, Bubble Time: 21.534 ms, Imbalance Overhead: 0.791 ms
GPU 7, Compute+Comm Time: 23.147 ms, Bubble Time: 21.879 ms, Imbalance Overhead: 0.046 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.645 ms
GPU 0, Compute+Comm Time: 43.628 ms, Bubble Time: 41.011 ms, Imbalance Overhead: 0.006 ms
GPU 1, Compute+Comm Time: 42.188 ms, Bubble Time: 40.424 ms, Imbalance Overhead: 2.033 ms
GPU 2, Compute+Comm Time: 42.188 ms, Bubble Time: 39.900 ms, Imbalance Overhead: 2.557 ms
GPU 3, Compute+Comm Time: 42.188 ms, Bubble Time: 39.429 ms, Imbalance Overhead: 3.028 ms
GPU 4, Compute+Comm Time: 42.188 ms, Bubble Time: 38.906 ms, Imbalance Overhead: 3.551 ms
GPU 5, Compute+Comm Time: 42.188 ms, Bubble Time: 38.500 ms, Imbalance Overhead: 3.957 ms
GPU 6, Compute+Comm Time: 42.188 ms, Bubble Time: 37.916 ms, Imbalance Overhead: 4.541 ms
GPU 7, Compute+Comm Time: 45.470 ms, Bubble Time: 37.874 ms, Imbalance Overhead: 1.301 ms
The estimated cost of the whole pipeline: 136.204 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 112.677 ms
Partition 0 [0, 17) has cost: 112.677 ms
Partition 1 [17, 33) has cost: 108.081 ms
Partition 2 [33, 49) has cost: 108.081 ms
Partition 3 [49, 65) has cost: 109.922 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 56.548 ms
GPU 0, Compute+Comm Time: 32.012 ms, Bubble Time: 23.229 ms, Imbalance Overhead: 1.306 ms
GPU 1, Compute+Comm Time: 31.353 ms, Bubble Time: 23.699 ms, Imbalance Overhead: 1.495 ms
GPU 2, Compute+Comm Time: 31.353 ms, Bubble Time: 24.317 ms, Imbalance Overhead: 0.877 ms
GPU 3, Compute+Comm Time: 31.556 ms, Bubble Time: 24.991 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.991 ms
GPU 0, Compute+Comm Time: 51.748 ms, Bubble Time: 41.243 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 51.024 ms, Bubble Time: 40.016 ms, Imbalance Overhead: 1.951 ms
GPU 2, Compute+Comm Time: 51.024 ms, Bubble Time: 38.893 ms, Imbalance Overhead: 3.074 ms
GPU 3, Compute+Comm Time: 52.667 ms, Bubble Time: 37.955 ms, Imbalance Overhead: 2.369 ms
    The estimated cost with 2 DP ways is 157.016 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 220.759 ms
Partition 0 [0, 33) has cost: 220.759 ms
Partition 1 [33, 65) has cost: 218.003 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 90.623 ms
GPU 0, Compute+Comm Time: 60.122 ms, Bubble Time: 29.279 ms, Imbalance Overhead: 1.222 ms
GPU 1, Compute+Comm Time: 59.894 ms, Bubble Time: 30.729 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 121.868 ms
GPU 0, Compute+Comm Time: 80.222 ms, Bubble Time: 41.646 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 80.681 ms, Bubble Time: 38.806 ms, Imbalance Overhead: 2.381 ms
    The estimated cost with 4 DP ways is 223.115 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 438.762 ms
Partition 0 [0, 65) has cost: 438.762 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 231.676 ms
GPU 0, Compute+Comm Time: 231.676 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 253.393 ms
GPU 0, Compute+Comm Time: 253.393 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 509.323 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 457)...
+++++++++ Node 4 initializing the weights for op[0, 457)...
+++++++++ Node 2 initializing the weights for op[0, 457)...
+++++++++ Node 5 initializing the weights for op[0, 457)...
+++++++++ Node 3 initializing the weights for op[0, 457)...
+++++++++ Node 0 initializing the weights for op[0, 457)...
+++++++++ Node 7 initializing the weights for op[0, 457)...
+++++++++ Node 6 initializing the weights for op[0, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2275	TrainAcc 0.2415	ValidAcc 0.2184	TestAcc 0.1894	BestValid 0.2184
	Epoch 50:	Loss 3.0056	TrainAcc 0.2770	ValidAcc 0.2992	TestAcc 0.2693	BestValid 0.2992
	Epoch 75:	Loss 2.8475	TrainAcc 0.2837	ValidAcc 0.3030	TestAcc 0.2722	BestValid 0.3030
Node 1, Pre/Post-Pipelining: 8.342 / 17.225 ms, Bubble: 0.493 ms, Compute: 204.144 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 3, Pre/Post-Pipelining: 8.349 / 17.352 ms, Bubble: 0.110 ms, Compute: 204.404 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 8.350 / 17.288 ms, Bubble: 0.466 ms, Compute: 204.103 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 5, Pre/Post-Pipelining: 8.376 / 17.415 ms, Bubble: 0.587 ms, Compute: 203.832 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
	Epoch 100:	Loss 2.6968	TrainAcc 0.3169	ValidAcc 0.3206	TestAcc 0.2875	BestValid 0.3206
Node 4, Pre/Post-Pipelining: 8.360 / 17.282 ms, Bubble: 0.555 ms, Compute: 204.012 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 0, Pre/Post-Pipelining: 8.341 / 17.192 ms, Bubble: 0.598 ms, Compute: 204.059 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 7, Pre/Post-Pipelining: 8.361 / 17.332 ms, Bubble: 0.804 ms, Compute: 203.706 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 8.367 / 17.433 ms, Bubble: 0.122 ms, Compute: 204.279 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.341 ms
Cluster-Wide Average, Post-Pipelining Overhead: 17.192 ms
Cluster-Wide Average, Bubble: 0.598 ms
Cluster-Wide Average, Compute: 204.059 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.016 ms
Node 0, GPU memory consumption: 19.053 GB
Node 2, GPU memory consumption: 18.165 GB
Node 1, GPU memory consumption: 18.165 GB
Node 3, GPU memory consumption: 18.141 GB
Node 4, GPU memory consumption: 18.139 GB
Node 5, GPU memory consumption: 18.163 GB
Node 6, GPU memory consumption: 18.163 GB
Node 7, GPU memory consumption: 18.139 GB
Node 0, Graph-Level Communication Throughput: 47.488 Gbps, Time: 121.666 ms
Node 1, Graph-Level Communication Throughput: 58.723 Gbps, Time: 117.031 ms
Node 2, Graph-Level Communication Throughput: 39.889 Gbps, Time: 121.901 ms
Node 3, Graph-Level Communication Throughput: 41.263 Gbps, Time: 122.936 ms
Node 4, Graph-Level Communication Throughput: 19.595 Gbps, Time: 127.370 ms
Node 5, Graph-Level Communication Throughput: 10.013 Gbps, Time: 126.767 ms
Node 6, Graph-Level Communication Throughput: 17.716 Gbps, Time: 126.367 ms
Node 7, Graph-Level Communication Throughput: 10.788 Gbps, Time: 127.474 ms
------------------------node id 0,  per-epoch time: 0.660155 s---------------
------------------------node id 1,  per-epoch time: 0.660152 s---------------
------------------------node id 4,  per-epoch time: 0.660154 s---------------
------------------------node id 2,  per-epoch time: 0.660152 s---------------
------------------------node id 5,  per-epoch time: 0.660152 s---------------
------------------------node id 3,  per-epoch time: 0.660151 s---------------
------------------------node id 6,  per-epoch time: 0.660155 s---------------
------------------------node id 7,  per-epoch time: 0.660154 s---------------
************ Profiling Results ************
	Bubble: 435.004041 (ms) (67.87 percentage)
	Compute: 56.747982 (ms) (8.85 percentage)
	GraphCommComputeOverhead: 8.848923 (ms) (1.38 percentage)
	GraphCommNetwork: 123.942587 (ms) (19.34 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 16.428519 (ms) (2.56 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 3.488 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 3.496 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.3206
Target test_acc: 0.2875
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
