Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.046 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.041 seconds.
        It takes 0.048 seconds.
        It takes 0.055 seconds.
        It takes 0.054 seconds.
Building the Feature Vector...
        It takes 0.054 seconds.
        It takes 0.053 seconds.
        It takes 0.057 seconds.
        It takes 0.055 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 64.277 Gbps (per GPU), 514.219 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.238 Gbps (per GPU), 513.907 Gbps (aggregated)
The layer-level communication performance: 64.237 Gbps (per GPU), 513.895 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.200 Gbps (per GPU), 513.599 Gbps (aggregated)
The layer-level communication performance: 64.195 Gbps (per GPU), 513.559 Gbps (aggregated)
The layer-level communication performance: 64.166 Gbps (per GPU), 513.326 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.158 Gbps (per GPU), 513.266 Gbps (aggregated)
The layer-level communication performance: 64.153 Gbps (per GPU), 513.228 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.804 Gbps (per GPU), 1318.429 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.800 Gbps (per GPU), 1318.397 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.787 Gbps (per GPU), 1318.300 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.800 Gbps (per GPU), 1318.403 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.787 Gbps (per GPU), 1318.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.796 Gbps (per GPU), 1318.371 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.802 Gbps (per GPU), 1318.413 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.800 Gbps (per GPU), 1318.403 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.255 Gbps (per GPU), 906.037 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.253 Gbps (per GPU), 906.026 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.255 Gbps (per GPU), 906.038 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.252 Gbps (per GPU), 906.019 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.241 Gbps (per GPU), 905.930 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.242 Gbps (per GPU), 905.934 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.245 Gbps (per GPU), 905.956 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.169 Gbps (per GPU), 905.353 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 46.185 Gbps (per GPU), 369.482 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.185 Gbps (per GPU), 369.483 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.185 Gbps (per GPU), 369.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.185 Gbps (per GPU), 369.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.185 Gbps (per GPU), 369.482 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.185 Gbps (per GPU), 369.482 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.185 Gbps (per GPU), 369.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.183 Gbps (per GPU), 369.465 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.57ms  0.89ms  1.12ms  1.95 21.17K  0.46M
 chk_1  0.57ms  0.94ms  1.17ms  2.04 21.17K  0.55M
 chk_2  0.57ms  0.87ms  1.09ms  1.91 21.17K  0.39M
 chk_3  0.57ms  0.83ms  1.06ms  1.85 21.17K  0.24M
 chk_4  0.57ms  0.80ms  1.03ms  1.79 21.17K  0.17M
 chk_5  0.57ms  0.81ms  1.04ms  1.81 21.17K  0.22M
 chk_6  0.57ms  0.80ms  1.03ms  1.80 21.17K  0.16M
 chk_7  0.57ms  0.79ms  1.02ms  1.77 21.17K  0.12M
   Avg  0.57  0.84  1.07
   Max  0.57  0.94  1.17
   Min  0.57  0.79  1.02
 Ratio  1.00  1.19  1.15
   Var  0.00  0.00  0.00
Profiling takes 0.314 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 58.363 ms
Partition 0 [0, 9) has cost: 58.363 ms
Partition 1 [9, 17) has cost: 53.783 ms
Partition 2 [17, 25) has cost: 53.783 ms
Partition 3 [25, 33) has cost: 53.783 ms
Partition 4 [33, 41) has cost: 53.783 ms
Partition 5 [41, 49) has cost: 53.783 ms
Partition 6 [49, 57) has cost: 53.783 ms
Partition 7 [57, 65) has cost: 55.611 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 44.972 ms
GPU 0, Compute+Comm Time: 23.920 ms, Bubble Time: 19.988 ms, Imbalance Overhead: 1.064 ms
GPU 1, Compute+Comm Time: 22.609 ms, Bubble Time: 19.975 ms, Imbalance Overhead: 2.389 ms
GPU 2, Compute+Comm Time: 22.609 ms, Bubble Time: 20.294 ms, Imbalance Overhead: 2.069 ms
GPU 3, Compute+Comm Time: 22.609 ms, Bubble Time: 20.544 ms, Imbalance Overhead: 1.819 ms
GPU 4, Compute+Comm Time: 22.609 ms, Bubble Time: 20.878 ms, Imbalance Overhead: 1.485 ms
GPU 5, Compute+Comm Time: 22.609 ms, Bubble Time: 21.188 ms, Imbalance Overhead: 1.175 ms
GPU 6, Compute+Comm Time: 22.609 ms, Bubble Time: 21.519 ms, Imbalance Overhead: 0.844 ms
GPU 7, Compute+Comm Time: 23.003 ms, Bubble Time: 21.886 ms, Imbalance Overhead: 0.083 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.274 ms
GPU 0, Compute+Comm Time: 43.399 ms, Bubble Time: 40.848 ms, Imbalance Overhead: 0.027 ms
GPU 1, Compute+Comm Time: 41.965 ms, Bubble Time: 40.243 ms, Imbalance Overhead: 2.066 ms
GPU 2, Compute+Comm Time: 41.965 ms, Bubble Time: 39.712 ms, Imbalance Overhead: 2.598 ms
GPU 3, Compute+Comm Time: 41.965 ms, Bubble Time: 39.243 ms, Imbalance Overhead: 3.067 ms
GPU 4, Compute+Comm Time: 41.965 ms, Bubble Time: 38.716 ms, Imbalance Overhead: 3.593 ms
GPU 5, Compute+Comm Time: 41.965 ms, Bubble Time: 38.288 ms, Imbalance Overhead: 4.022 ms
GPU 6, Compute+Comm Time: 41.965 ms, Bubble Time: 37.706 ms, Imbalance Overhead: 4.604 ms
GPU 7, Compute+Comm Time: 45.234 ms, Bubble Time: 37.686 ms, Imbalance Overhead: 1.354 ms
The estimated cost of the whole pipeline: 135.709 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 112.146 ms
Partition 0 [0, 17) has cost: 112.146 ms
Partition 1 [17, 33) has cost: 107.565 ms
Partition 2 [33, 49) has cost: 107.565 ms
Partition 3 [49, 65) has cost: 109.393 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 56.574 ms
GPU 0, Compute+Comm Time: 31.964 ms, Bubble Time: 23.158 ms, Imbalance Overhead: 1.452 ms
GPU 1, Compute+Comm Time: 31.307 ms, Bubble Time: 23.684 ms, Imbalance Overhead: 1.583 ms
GPU 2, Compute+Comm Time: 31.307 ms, Bubble Time: 24.354 ms, Imbalance Overhead: 0.912 ms
GPU 3, Compute+Comm Time: 31.507 ms, Bubble Time: 25.067 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.802 ms
GPU 0, Compute+Comm Time: 51.638 ms, Bubble Time: 41.163 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 50.921 ms, Bubble Time: 39.922 ms, Imbalance Overhead: 1.959 ms
GPU 2, Compute+Comm Time: 50.921 ms, Bubble Time: 38.806 ms, Imbalance Overhead: 3.075 ms
GPU 3, Compute+Comm Time: 52.557 ms, Bubble Time: 37.871 ms, Imbalance Overhead: 2.373 ms
    The estimated cost with 2 DP ways is 156.845 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 219.711 ms
Partition 0 [0, 33) has cost: 219.711 ms
Partition 1 [33, 65) has cost: 216.958 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 91.121 ms
GPU 0, Compute+Comm Time: 60.420 ms, Bubble Time: 29.375 ms, Imbalance Overhead: 1.325 ms
GPU 1, Compute+Comm Time: 60.190 ms, Bubble Time: 30.931 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 122.184 ms
GPU 0, Compute+Comm Time: 80.441 ms, Bubble Time: 41.744 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 80.898 ms, Bubble Time: 38.924 ms, Imbalance Overhead: 2.362 ms
    The estimated cost with 4 DP ways is 223.970 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 436.669 ms
Partition 0 [0, 65) has cost: 436.669 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 229.861 ms
GPU 0, Compute+Comm Time: 229.861 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 251.376 ms
GPU 0, Compute+Comm Time: 251.376 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 505.300 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 457)...
+++++++++ Node 1 initializing the weights for op[0, 457)...
+++++++++ Node 3 initializing the weights for op[0, 457)...
+++++++++ Node 0 initializing the weights for op[0, 457)...
+++++++++ Node 7 initializing the weights for op[0, 457)...
+++++++++ Node 2 initializing the weights for op[0, 457)...
+++++++++ Node 4 initializing the weights for op[0, 457)...
+++++++++ Node 6 initializing the weights for op[0, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.1845	TrainAcc 0.1798	ValidAcc 0.0772	TestAcc 0.0590	BestValid 0.0772
	Epoch 50:	Loss 2.9626	TrainAcc 0.2747	ValidAcc 0.2961	TestAcc 0.2671	BestValid 0.2961
	Epoch 75:	Loss 2.7779	TrainAcc 0.3126	ValidAcc 0.3155	TestAcc 0.2840	BestValid 0.3155
Node 3, Pre/Post-Pipelining: 8.347 / 16.920 ms, Bubble: 0.095 ms, Compute: 204.946 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 1, Pre/Post-Pipelining: 8.346 / 16.828 ms, Bubble: 0.353 ms, Compute: 204.772 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 4, Pre/Post-Pipelining: 8.353 / 16.895 ms, Bubble: 0.477 ms, Compute: 204.574 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
	Epoch 100:	Loss 2.6204	TrainAcc 0.3616	ValidAcc 0.3480	TestAcc 0.3151	BestValid 0.3480
Node 5, Pre/Post-Pipelining: 8.357 / 16.956 ms, Bubble: 0.534 ms, Compute: 204.459 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 8.344 / 16.779 ms, Bubble: 0.513 ms, Compute: 204.660 ms, Comm: 0.009 ms, Imbalance: 0.015 ms
Node 7, Pre/Post-Pipelining: 8.349 / 16.910 ms, Bubble: 0.660 ms, Compute: 204.383 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 0, Pre/Post-Pipelining: 8.340 / 16.746 ms, Bubble: 0.536 ms, Compute: 204.660 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 6, Pre/Post-Pipelining: 8.358 / 16.978 ms, Bubble: 0.113 ms, Compute: 204.855 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.340 ms
Cluster-Wide Average, Post-Pipelining Overhead: 16.746 ms
Cluster-Wide Average, Bubble: 0.536 ms
Cluster-Wide Average, Compute: 204.660 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.015 ms
Node 0, GPU memory consumption: 19.053 GB
Node 3, GPU memory consumption: 18.141 GB
Node 1, GPU memory consumption: 18.165 GB
Node 2, GPU memory consumption: 18.165 GB
Node 5, GPU memory consumption: 18.163 GB
Node 4, GPU memory consumption: 18.139 GB
Node 7, GPU memory consumption: 18.139 GB
Node 6, GPU memory consumption: 18.163 GB
Node 0, Graph-Level Communication Throughput: 47.226 Gbps, Time: 122.342 ms
Node 1, Graph-Level Communication Throughput: 58.709 Gbps, Time: 117.057 ms
Node 2, Graph-Level Communication Throughput: 39.464 Gbps, Time: 123.215 ms
Node 3, Graph-Level Communication Throughput: 41.054 Gbps, Time: 123.563 ms
Node 4, Graph-Level Communication Throughput: 19.519 Gbps, Time: 127.869 ms
Node 5, Graph-Level Communication Throughput: 9.970 Gbps, Time: 127.309 ms
Node 6, Graph-Level Communication Throughput: 17.582 Gbps, Time: 127.332 ms
Node 7, Graph-Level Communication Throughput: 10.703 Gbps, Time: 128.491 ms
------------------------node id 0,  per-epoch time: 0.656121 s---------------
------------------------node id 1,  per-epoch time: 0.656119 s---------------
------------------------node id 2,  per-epoch time: 0.656119 s---------------
------------------------node id 3,  per-epoch time: 0.656117 s---------------
------------------------node id 4,  per-epoch time: 0.656119 s---------------
------------------------node id 5,  per-epoch time: 0.656118 s---------------
------------------------node id 6,  per-epoch time: 0.656120 s---------------
------------------------node id 7,  per-epoch time: 0.656120 s---------------
************ Profiling Results ************
	Bubble: 430.676849 (ms) (67.59 percentage)
	Compute: 56.646109 (ms) (8.89 percentage)
	GraphCommComputeOverhead: 8.796237 (ms) (1.38 percentage)
	GraphCommNetwork: 124.651661 (ms) (19.56 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 16.413597 (ms) (2.58 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 3.488 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 3.496 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.3480
Target test_acc: 0.3151
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
