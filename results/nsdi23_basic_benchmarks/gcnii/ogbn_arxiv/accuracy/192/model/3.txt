Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
        It takes 0.043 seconds.
        It takes 0.046 seconds.
        It takes 0.053 seconds.
        It takes 0.054 seconds.
        It takes 0.058 seconds.
        It takes 0.056 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.025 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.721 Gbps (per GPU), 509.766 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.680 Gbps (per GPU), 509.438 Gbps (aggregated)
The layer-level communication performance: 63.677 Gbps (per GPU), 509.419 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.644 Gbps (per GPU), 509.155 Gbps (aggregated)
The layer-level communication performance: 63.640 Gbps (per GPU), 509.118 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.612 Gbps (per GPU), 508.894 Gbps (aggregated)
The layer-level communication performance: 63.605 Gbps (per GPU), 508.842 Gbps (aggregated)
The layer-level communication performance: 63.600 Gbps (per GPU), 508.801 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.694 Gbps (per GPU), 1317.548 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.693 Gbps (per GPU), 1317.542 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.686 Gbps (per GPU), 1317.488 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.693 Gbps (per GPU), 1317.546 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.688 Gbps (per GPU), 1317.504 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.688 Gbps (per GPU), 1317.507 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.686 Gbps (per GPU), 1317.484 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.691 Gbps (per GPU), 1317.529 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.511 Gbps (per GPU), 908.091 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.509 Gbps (per GPU), 908.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.511 Gbps (per GPU), 908.089 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.511 Gbps (per GPU), 908.091 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.504 Gbps (per GPU), 908.035 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.508 Gbps (per GPU), 908.063 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.506 Gbps (per GPU), 908.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.494 Gbps (per GPU), 907.950 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.433 Gbps (per GPU), 363.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.433 Gbps (per GPU), 363.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.433 Gbps (per GPU), 363.460 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.432 Gbps (per GPU), 363.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.433 Gbps (per GPU), 363.460 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.433 Gbps (per GPU), 363.461 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.432 Gbps (per GPU), 363.457 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.431 Gbps (per GPU), 363.449 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.42ms  0.77ms  0.96ms  2.30  5.55K  0.06M
 chk_1  0.42ms  0.80ms  0.99ms  2.34  5.75K  0.05M
 chk_2  0.40ms  0.73ms  0.91ms  2.30  5.20K  0.07M
 chk_3  0.42ms  0.80ms  0.98ms  2.36  5.57K  0.06M
 chk_4  0.38ms  0.71ms  0.90ms  2.35  5.05K  0.08M
 chk_5  0.37ms  0.74ms  0.92ms  2.50  4.73K  0.11M
 chk_6  0.38ms  0.70ms  0.88ms  2.34  4.98K  0.08M
 chk_7  0.38ms  0.72ms  0.90ms  2.37  5.01K  0.09M
 chk_8  0.43ms  0.80ms  0.99ms  2.30  5.73K  0.05M
 chk_9  0.36ms  0.72ms  0.89ms  2.46  4.54K  0.11M
chk_10  0.39ms  0.74ms  0.92ms  2.34  5.36K  0.07M
chk_11  0.40ms  0.75ms  0.93ms  2.35  5.39K  0.08M
chk_12  0.42ms  0.80ms  1.01ms  2.38  5.77K  0.05M
chk_13  0.40ms  0.75ms  0.93ms  2.35  5.43K  0.06M
chk_14  0.41ms  0.77ms  0.96ms  2.32  5.46K  0.06M
chk_15  0.43ms  0.80ms  0.99ms  2.33  5.88K  0.04M
chk_16  0.41ms  0.78ms  0.97ms  2.35  5.50K  0.06M
chk_17  0.37ms  0.74ms  0.92ms  2.45  4.86K  0.09M
chk_18  0.40ms  0.78ms  0.96ms  2.42  5.39K  0.07M
chk_19  0.39ms  0.73ms  0.92ms  2.36  5.20K  0.07M
chk_20  0.42ms  0.77ms  0.95ms  2.30  5.51K  0.06M
chk_21  0.43ms  0.79ms  0.97ms  2.28  5.81K  0.05M
chk_22  0.39ms  0.73ms  0.91ms  2.32  5.32K  0.07M
chk_23  0.39ms  0.76ms  0.95ms  2.39  5.39K  0.07M
chk_24  0.36ms  0.72ms  0.90ms  2.46  4.62K  0.11M
chk_25  0.38ms  0.72ms  0.90ms  2.37  5.04K  0.08M
chk_26  0.36ms  0.72ms  0.89ms  2.46  4.55K  0.11M
chk_27  0.39ms  0.72ms  0.90ms  2.31  5.30K  0.06M
chk_28  0.42ms  0.78ms  0.97ms  2.32  5.58K  0.06M
chk_29  0.38ms  0.73ms  0.91ms  2.41  4.98K  0.09M
chk_30  0.42ms  0.78ms  0.96ms  2.32  5.50K  0.07M
chk_31  0.40ms  0.74ms  0.92ms  2.32  5.38K  0.07M
   Avg  0.40  0.75  0.94
   Max  0.43  0.80  1.01
   Min  0.36  0.70  0.88
 Ratio  1.19  1.14  1.14
   Var  0.00  0.00  0.00
Profiling takes 0.918 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 205.456 ms
Partition 0 [0, 9) has cost: 205.456 ms
Partition 1 [9, 17) has cost: 192.754 ms
Partition 2 [17, 25) has cost: 192.754 ms
Partition 3 [25, 33) has cost: 192.754 ms
Partition 4 [33, 41) has cost: 192.754 ms
Partition 5 [41, 49) has cost: 192.754 ms
Partition 6 [49, 57) has cost: 192.754 ms
Partition 7 [57, 65) has cost: 198.623 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 100.621 ms
GPU 0, Compute+Comm Time: 80.798 ms, Bubble Time: 17.512 ms, Imbalance Overhead: 2.311 ms
GPU 1, Compute+Comm Time: 76.699 ms, Bubble Time: 17.612 ms, Imbalance Overhead: 6.310 ms
GPU 2, Compute+Comm Time: 76.699 ms, Bubble Time: 17.805 ms, Imbalance Overhead: 6.117 ms
GPU 3, Compute+Comm Time: 76.699 ms, Bubble Time: 17.860 ms, Imbalance Overhead: 6.061 ms
GPU 4, Compute+Comm Time: 76.699 ms, Bubble Time: 17.979 ms, Imbalance Overhead: 5.943 ms
GPU 5, Compute+Comm Time: 76.699 ms, Bubble Time: 18.058 ms, Imbalance Overhead: 5.863 ms
GPU 6, Compute+Comm Time: 76.699 ms, Bubble Time: 18.097 ms, Imbalance Overhead: 5.824 ms
GPU 7, Compute+Comm Time: 77.998 ms, Bubble Time: 18.269 ms, Imbalance Overhead: 4.354 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 195.150 ms
GPU 0, Compute+Comm Time: 153.282 ms, Bubble Time: 35.455 ms, Imbalance Overhead: 6.413 ms
GPU 1, Compute+Comm Time: 148.711 ms, Bubble Time: 35.190 ms, Imbalance Overhead: 11.249 ms
GPU 2, Compute+Comm Time: 148.711 ms, Bubble Time: 35.159 ms, Imbalance Overhead: 11.279 ms
GPU 3, Compute+Comm Time: 148.711 ms, Bubble Time: 34.986 ms, Imbalance Overhead: 11.452 ms
GPU 4, Compute+Comm Time: 148.711 ms, Bubble Time: 34.770 ms, Imbalance Overhead: 11.669 ms
GPU 5, Compute+Comm Time: 148.711 ms, Bubble Time: 34.632 ms, Imbalance Overhead: 11.807 ms
GPU 6, Compute+Comm Time: 148.711 ms, Bubble Time: 34.206 ms, Imbalance Overhead: 12.233 ms
GPU 7, Compute+Comm Time: 157.314 ms, Bubble Time: 33.980 ms, Imbalance Overhead: 3.855 ms
The estimated cost of the whole pipeline: 310.559 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 398.210 ms
Partition 0 [0, 17) has cost: 398.210 ms
Partition 1 [17, 33) has cost: 385.507 ms
Partition 2 [33, 49) has cost: 385.507 ms
Partition 3 [49, 65) has cost: 391.377 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 129.561 ms
GPU 0, Compute+Comm Time: 106.437 ms, Bubble Time: 19.961 ms, Imbalance Overhead: 3.164 ms
GPU 1, Compute+Comm Time: 104.332 ms, Bubble Time: 20.298 ms, Imbalance Overhead: 4.932 ms
GPU 2, Compute+Comm Time: 104.332 ms, Bubble Time: 20.458 ms, Imbalance Overhead: 4.771 ms
GPU 3, Compute+Comm Time: 104.983 ms, Bubble Time: 20.644 ms, Imbalance Overhead: 3.933 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 221.207 ms
GPU 0, Compute+Comm Time: 180.222 ms, Bubble Time: 35.350 ms, Imbalance Overhead: 5.636 ms
GPU 1, Compute+Comm Time: 177.902 ms, Bubble Time: 35.080 ms, Imbalance Overhead: 8.226 ms
GPU 2, Compute+Comm Time: 177.902 ms, Bubble Time: 34.765 ms, Imbalance Overhead: 8.541 ms
GPU 3, Compute+Comm Time: 182.307 ms, Bubble Time: 34.112 ms, Imbalance Overhead: 4.789 ms
    The estimated cost with 2 DP ways is 368.307 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 783.717 ms
Partition 0 [0, 33) has cost: 783.717 ms
Partition 1 [33, 65) has cost: 776.885 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 226.255 ms
GPU 0, Compute+Comm Time: 195.813 ms, Bubble Time: 24.517 ms, Imbalance Overhead: 5.925 ms
GPU 1, Compute+Comm Time: 195.067 ms, Bubble Time: 25.272 ms, Imbalance Overhead: 5.916 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 312.817 ms
GPU 0, Compute+Comm Time: 270.116 ms, Bubble Time: 35.064 ms, Imbalance Overhead: 7.637 ms
GPU 1, Compute+Comm Time: 271.177 ms, Bubble Time: 33.931 ms, Imbalance Overhead: 7.709 ms
    The estimated cost with 4 DP ways is 566.026 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1560.602 ms
Partition 0 [0, 65) has cost: 1560.602 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 762.214 ms
GPU 0, Compute+Comm Time: 762.214 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 839.058 ms
GPU 0, Compute+Comm Time: 839.058 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1681.336 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.4496	TrainAcc 0.0896	ValidAcc 0.1147	TestAcc 0.1026	BestValid 0.1147
	Epoch 50:	Loss 4.4489	TrainAcc 0.1647	ValidAcc 0.0785	TestAcc 0.0605	BestValid 0.1147
	Epoch 75:	Loss 3.8989	TrainAcc 0.1764	ValidAcc 0.0790	TestAcc 0.0601	BestValid 0.1147
	Epoch 100:	Loss 3.7591	TrainAcc 0.1749	ValidAcc 0.0790	TestAcc 0.0605	BestValid 0.1147
Node 1, Pre/Post-Pipelining: 4.083 / 34.960 ms, Bubble: 65.783 ms, Compute: 189.891 ms, Comm: 45.106 ms, Imbalance: 28.851 ms
Node 3, Pre/Post-Pipelining: 3.911 / 34.998 ms, Bubble: 64.586 ms, Compute: 190.796 ms, Comm: 55.998 ms, Imbalance: 18.442 ms
Node 5, Pre/Post-Pipelining: 3.911 / 34.892 ms, Bubble: 66.174 ms, Compute: 188.816 ms, Comm: 53.171 ms, Imbalance: 22.238 ms
Node 0, Pre/Post-Pipelining: 4.251 / 35.033 ms, Bubble: 66.425 ms, Compute: 204.833 ms, Comm: 35.240 ms, Imbalance: 22.760 ms
Node 2, Pre/Post-Pipelining: 3.746 / 34.947 ms, Bubble: 66.127 ms, Compute: 189.425 ms, Comm: 52.133 ms, Imbalance: 22.492 ms
Node 4, Pre/Post-Pipelining: 3.665 / 34.947 ms, Bubble: 65.163 ms, Compute: 190.327 ms, Comm: 56.680 ms, Imbalance: 18.364 ms
Node 6, Pre/Post-Pipelining: 3.002 / 35.060 ms, Bubble: 65.786 ms, Compute: 193.710 ms, Comm: 44.336 ms, Imbalance: 26.203 ms
Node 7, Pre/Post-Pipelining: 3.159 / 49.155 ms, Bubble: 52.649 ms, Compute: 208.606 ms, Comm: 33.079 ms, Imbalance: 21.423 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 4.251 ms
Cluster-Wide Average, Post-Pipelining Overhead: 35.033 ms
Cluster-Wide Average, Bubble: 66.425 ms
Cluster-Wide Average, Compute: 204.833 ms
Cluster-Wide Average, Communication: 35.240 ms
Cluster-Wide Average, Imbalance: 22.760 ms
Node 0, GPU memory consumption: 10.508 GB
Node 1, GPU memory consumption: 8.721 GB
Node 3, GPU memory consumption: 8.698 GB
Node 2, GPU memory consumption: 8.721 GB
Node 4, GPU memory consumption: 8.698 GB
Node 5, GPU memory consumption: 8.721 GB
Node 6, GPU memory consumption: 8.721 GB
Node 7, GPU memory consumption: 8.801 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 1.623613 s---------------
------------------------node id 1,  per-epoch time: 1.623613 s---------------
------------------------node id 4,  per-epoch time: 1.623616 s---------------
------------------------node id 2,  per-epoch time: 1.623613 s---------------
------------------------node id 5,  per-epoch time: 1.623615 s---------------
------------------------node id 3,  per-epoch time: 1.623613 s---------------
------------------------node id 6,  per-epoch time: 1.623614 s---------------
------------------------node id 7,  per-epoch time: 1.623614 s---------------
************ Profiling Results ************
	Bubble: 1371.557516 (ms) (87.39 percentage)
	Compute: 189.869375 (ms) (12.10 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 7.965305 (ms) (0.51 percentage)
	Layer-level communication (cluster-wide, per-epoch): 3.391 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.071 GB
	Total communication (cluster-wide, per-epoch): 3.463 GB
	Aggregated layer-level communication throughput: 620.265 Gbps
Highest valid_acc: 0.1147
Target test_acc: 0.1026
Epoch to reach the target acc: 24
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
