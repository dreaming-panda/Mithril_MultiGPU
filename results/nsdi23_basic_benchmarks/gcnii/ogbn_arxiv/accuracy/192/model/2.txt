Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
        It takes 0.051 seconds.
        It takes 0.055 seconds.
        It takes 0.052 seconds.
        It takes 0.052 seconds.
        It takes 0.053 seconds.
        It takes 0.054 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.023 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.575 Gbps (per GPU), 508.602 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.536 Gbps (per GPU), 508.284 Gbps (aggregated)
The layer-level communication performance: 63.534 Gbps (per GPU), 508.270 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.500 Gbps (per GPU), 507.997 Gbps (aggregated)
The layer-level communication performance: 63.495 Gbps (per GPU), 507.963 Gbps (aggregated)
The layer-level communication performance: 63.469 Gbps (per GPU), 507.754 Gbps (aggregated)
The layer-level communication performance: 63.462 Gbps (per GPU), 507.698 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.458 Gbps (per GPU), 507.661 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.736 Gbps (per GPU), 1317.885 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.718 Gbps (per GPU), 1317.743 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.719 Gbps (per GPU), 1317.749 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.728 Gbps (per GPU), 1317.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.729 Gbps (per GPU), 1317.834 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.735 Gbps (per GPU), 1317.879 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.722 Gbps (per GPU), 1317.775 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.722 Gbps (per GPU), 1317.772 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.857 Gbps (per GPU), 910.854 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.856 Gbps (per GPU), 910.850 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.856 Gbps (per GPU), 910.851 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.857 Gbps (per GPU), 910.855 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.855 Gbps (per GPU), 910.837 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.852 Gbps (per GPU), 910.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.840 Gbps (per GPU), 910.721 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.821 Gbps (per GPU), 910.569 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.058 Gbps (per GPU), 360.461 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.058 Gbps (per GPU), 360.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.057 Gbps (per GPU), 360.459 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.057 Gbps (per GPU), 360.456 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.057 Gbps (per GPU), 360.459 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.057 Gbps (per GPU), 360.456 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.057 Gbps (per GPU), 360.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.056 Gbps (per GPU), 360.449 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.76ms  0.95ms  2.30  5.55K  0.06M
 chk_1  0.41ms  0.80ms  0.98ms  2.36  5.75K  0.05M
 chk_2  0.39ms  0.72ms  0.90ms  2.33  5.20K  0.07M
 chk_3  0.41ms  0.79ms  0.97ms  2.36  5.57K  0.06M
 chk_4  0.38ms  0.70ms  0.88ms  2.33  5.05K  0.08M
 chk_5  0.36ms  0.73ms  0.91ms  2.52  4.73K  0.11M
 chk_6  0.37ms  0.70ms  0.87ms  2.34  4.98K  0.08M
 chk_7  0.37ms  0.71ms  0.89ms  2.38  5.01K  0.09M
 chk_8  0.42ms  0.79ms  0.97ms  2.31  5.73K  0.05M
 chk_9  0.36ms  0.71ms  0.88ms  2.48  4.54K  0.11M
chk_10  0.39ms  0.73ms  0.91ms  2.36  5.36K  0.07M
chk_11  0.39ms  0.74ms  0.92ms  2.37  5.39K  0.08M
chk_12  0.42ms  0.80ms  0.98ms  2.35  5.77K  0.05M
chk_13  0.39ms  0.73ms  0.92ms  2.36  5.43K  0.06M
chk_14  0.41ms  0.76ms  0.94ms  2.31  5.46K  0.06M
chk_15  0.42ms  0.80ms  0.98ms  2.34  5.88K  0.04M
chk_16  0.41ms  0.78ms  0.96ms  2.34  5.50K  0.06M
chk_17  0.37ms  0.73ms  0.91ms  2.47  4.86K  0.09M
chk_18  0.39ms  0.77ms  0.95ms  2.47  5.39K  0.07M
chk_19  0.38ms  0.73ms  0.91ms  2.39  5.20K  0.07M
chk_20  0.41ms  0.76ms  0.94ms  2.31  5.51K  0.06M
chk_21  0.42ms  0.78ms  0.96ms  2.30  5.81K  0.05M
chk_22  0.38ms  0.72ms  0.90ms  2.34  5.32K  0.07M
chk_23  0.39ms  0.75ms  0.94ms  2.41  5.39K  0.07M
chk_24  0.36ms  0.71ms  0.89ms  2.48  4.62K  0.11M
chk_25  0.37ms  0.71ms  0.89ms  2.39  5.04K  0.08M
chk_26  0.36ms  0.71ms  0.88ms  2.48  4.55K  0.11M
chk_27  0.38ms  0.71ms  0.89ms  2.32  5.30K  0.06M
chk_28  0.41ms  0.78ms  0.96ms  2.33  5.58K  0.06M
chk_29  0.37ms  0.72ms  0.90ms  2.42  4.98K  0.09M
chk_30  0.41ms  0.77ms  0.95ms  2.33  5.50K  0.07M
chk_31  0.39ms  0.73ms  0.91ms  2.35  5.38K  0.07M
   Avg  0.39  0.74  0.92
   Max  0.42  0.80  0.98
   Min  0.36  0.70  0.87
 Ratio  1.19  1.15  1.13
   Var  0.00  0.00  0.00
Profiling takes 0.906 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 202.913 ms
Partition 0 [0, 9) has cost: 202.913 ms
Partition 1 [9, 17) has cost: 190.438 ms
Partition 2 [17, 25) has cost: 190.438 ms
Partition 3 [25, 33) has cost: 190.438 ms
Partition 4 [33, 41) has cost: 190.438 ms
Partition 5 [41, 49) has cost: 190.438 ms
Partition 6 [49, 57) has cost: 190.438 ms
Partition 7 [57, 65) has cost: 196.217 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 99.583 ms
GPU 0, Compute+Comm Time: 79.906 ms, Bubble Time: 17.300 ms, Imbalance Overhead: 2.377 ms
GPU 1, Compute+Comm Time: 75.891 ms, Bubble Time: 17.397 ms, Imbalance Overhead: 6.295 ms
GPU 2, Compute+Comm Time: 75.891 ms, Bubble Time: 17.594 ms, Imbalance Overhead: 6.099 ms
GPU 3, Compute+Comm Time: 75.891 ms, Bubble Time: 17.657 ms, Imbalance Overhead: 6.035 ms
GPU 4, Compute+Comm Time: 75.891 ms, Bubble Time: 17.777 ms, Imbalance Overhead: 5.915 ms
GPU 5, Compute+Comm Time: 75.891 ms, Bubble Time: 17.859 ms, Imbalance Overhead: 5.833 ms
GPU 6, Compute+Comm Time: 75.891 ms, Bubble Time: 17.902 ms, Imbalance Overhead: 5.790 ms
GPU 7, Compute+Comm Time: 77.158 ms, Bubble Time: 18.083 ms, Imbalance Overhead: 4.342 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 193.418 ms
GPU 0, Compute+Comm Time: 151.790 ms, Bubble Time: 35.071 ms, Imbalance Overhead: 6.556 ms
GPU 1, Compute+Comm Time: 147.278 ms, Bubble Time: 34.800 ms, Imbalance Overhead: 11.339 ms
GPU 2, Compute+Comm Time: 147.278 ms, Bubble Time: 34.773 ms, Imbalance Overhead: 11.366 ms
GPU 3, Compute+Comm Time: 147.278 ms, Bubble Time: 34.604 ms, Imbalance Overhead: 11.536 ms
GPU 4, Compute+Comm Time: 147.278 ms, Bubble Time: 34.410 ms, Imbalance Overhead: 11.730 ms
GPU 5, Compute+Comm Time: 147.278 ms, Bubble Time: 34.284 ms, Imbalance Overhead: 11.856 ms
GPU 6, Compute+Comm Time: 147.278 ms, Bubble Time: 33.876 ms, Imbalance Overhead: 12.263 ms
GPU 7, Compute+Comm Time: 155.738 ms, Bubble Time: 33.669 ms, Imbalance Overhead: 4.010 ms
The estimated cost of the whole pipeline: 307.650 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 393.351 ms
Partition 0 [0, 17) has cost: 393.351 ms
Partition 1 [17, 33) has cost: 380.875 ms
Partition 2 [33, 49) has cost: 380.875 ms
Partition 3 [49, 65) has cost: 386.655 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 128.606 ms
GPU 0, Compute+Comm Time: 105.615 ms, Bubble Time: 19.780 ms, Imbalance Overhead: 3.211 ms
GPU 1, Compute+Comm Time: 103.551 ms, Bubble Time: 20.130 ms, Imbalance Overhead: 4.925 ms
GPU 2, Compute+Comm Time: 103.551 ms, Bubble Time: 20.307 ms, Imbalance Overhead: 4.748 ms
GPU 3, Compute+Comm Time: 104.184 ms, Bubble Time: 20.499 ms, Imbalance Overhead: 3.923 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 219.705 ms
GPU 0, Compute+Comm Time: 178.857 ms, Bubble Time: 35.014 ms, Imbalance Overhead: 5.834 ms
GPU 1, Compute+Comm Time: 176.575 ms, Bubble Time: 34.750 ms, Imbalance Overhead: 8.380 ms
GPU 2, Compute+Comm Time: 176.575 ms, Bubble Time: 34.479 ms, Imbalance Overhead: 8.651 ms
GPU 3, Compute+Comm Time: 180.912 ms, Bubble Time: 33.857 ms, Imbalance Overhead: 4.937 ms
    The estimated cost with 2 DP ways is 365.727 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 774.226 ms
Partition 0 [0, 33) has cost: 774.226 ms
Partition 1 [33, 65) has cost: 767.530 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 224.965 ms
GPU 0, Compute+Comm Time: 194.631 ms, Bubble Time: 24.346 ms, Imbalance Overhead: 5.988 ms
GPU 1, Compute+Comm Time: 193.892 ms, Bubble Time: 25.131 ms, Imbalance Overhead: 5.943 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 311.033 ms
GPU 0, Compute+Comm Time: 268.482 ms, Bubble Time: 34.809 ms, Imbalance Overhead: 7.741 ms
GPU 1, Compute+Comm Time: 269.545 ms, Bubble Time: 33.729 ms, Imbalance Overhead: 7.759 ms
    The estimated cost with 4 DP ways is 562.798 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1541.756 ms
Partition 0 [0, 65) has cost: 1541.756 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 767.152 ms
GPU 0, Compute+Comm Time: 767.152 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 843.575 ms
GPU 0, Compute+Comm Time: 843.575 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1691.263 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.4300	TrainAcc 0.1444	ValidAcc 0.0790	TestAcc 0.0773	BestValid 0.0790
	Epoch 50:	Loss 4.1035	TrainAcc 0.0801	ValidAcc 0.0411	TestAcc 0.0349	BestValid 0.0790
	Epoch 75:	Loss 3.7789	TrainAcc 0.0869	ValidAcc 0.0413	TestAcc 0.0353	BestValid 0.0790
	Epoch 100:	Loss 3.7440	TrainAcc 0.1685	ValidAcc 0.0746	TestAcc 0.0594	BestValid 0.0790
Node 1, Pre/Post-Pipelining: 3.999 / 34.091 ms, Bubble: 65.558 ms, Compute: 192.788 ms, Comm: 44.972 ms, Imbalance: 27.539 ms
Node 0, Pre/Post-Pipelining: 4.202 / 33.982 ms, Bubble: 66.740 ms, Compute: 201.565 ms, Comm: 35.274 ms, Imbalance: 27.165 ms
Node 4, Pre/Post-Pipelining: 3.429 / 34.099 ms, Bubble: 64.557 ms, Compute: 192.628 ms, Comm: 57.036 ms, Imbalance: 17.620 ms
Node 3, Pre/Post-Pipelining: 3.935 / 34.144 ms, Bubble: 64.169 ms, Compute: 194.417 ms, Comm: 56.173 ms, Imbalance: 16.342 ms
Node 5, Pre/Post-Pipelining: 3.673 / 33.990 ms, Bubble: 65.715 ms, Compute: 188.699 ms, Comm: 53.314 ms, Imbalance: 24.155 ms
Node 2, Pre/Post-Pipelining: 3.717 / 34.091 ms, Bubble: 65.652 ms, Compute: 192.702 ms, Comm: 52.067 ms, Imbalance: 20.959 ms
Node 6, Pre/Post-Pipelining: 3.434 / 34.147 ms, Bubble: 65.488 ms, Compute: 193.571 ms, Comm: 44.437 ms, Imbalance: 28.097 ms
Node 7, Pre/Post-Pipelining: 3.238 / 48.173 ms, Bubble: 52.356 ms, Compute: 208.560 ms, Comm: 33.111 ms, Imbalance: 23.207 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 4.202 ms
Cluster-Wide Average, Post-Pipelining Overhead: 33.982 ms
Cluster-Wide Average, Bubble: 66.740 ms
Cluster-Wide Average, Compute: 201.565 ms
Cluster-Wide Average, Communication: 35.274 ms
Cluster-Wide Average, Imbalance: 27.165 ms
Node 0, GPU memory consumption: 10.510 GB
Node 3, GPU memory consumption: 8.698 GB
Node 5, GPU memory consumption: 8.721 GB
Node 1, GPU memory consumption: 8.721 GB
Node 6, GPU memory consumption: 8.721 GB
Node 2, GPU memory consumption: 8.721 GB
Node 4, GPU memory consumption: 8.698 GB
Node 7, GPU memory consumption: 8.801 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 1.618268 s---------------
------------------------node id 1,  per-epoch time: 1.618266 s---------------
------------------------node id 2,  per-epoch time: 1.618267 s---------------
------------------------node id 3,  per-epoch time: 1.618267 s---------------
------------------------node id 4,  per-epoch time: 1.618269 s---------------
------------------------node id 5,  per-epoch time: 1.618270 s---------------
------------------------node id 6,  per-epoch time: 1.618269 s---------------
------------------------node id 7,  per-epoch time: 1.618272 s---------------
************ Profiling Results ************
	Bubble: 1365.438290 (ms) (87.30 percentage)
	Compute: 190.650595 (ms) (12.19 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 8.013682 (ms) (0.51 percentage)
	Layer-level communication (cluster-wide, per-epoch): 3.391 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.071 GB
	Total communication (cluster-wide, per-epoch): 3.463 GB
	Aggregated layer-level communication throughput: 619.206 Gbps
Highest valid_acc: 0.0790
Target test_acc: 0.0773
Epoch to reach the target acc: 24
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
