Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.044 seconds.
        It takes 0.052 seconds.
        It takes 0.054 seconds.
        It takes 0.049 seconds.
Building the Feature Vector...
        It takes 0.051 seconds.
        It takes 0.054 seconds.
        It takes 0.055 seconds.
        It takes 0.059 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
        It takes 0.058 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.711 Gbps (per GPU), 509.687 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.673 Gbps (per GPU), 509.382 Gbps (aggregated)
The layer-level communication performance: 63.671 Gbps (per GPU), 509.365 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.636 Gbps (per GPU), 509.088 Gbps (aggregated)
The layer-level communication performance: 63.631 Gbps (per GPU), 509.050 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.599 Gbps (per GPU), 508.792 Gbps (aggregated)
The layer-level communication performance: 63.592 Gbps (per GPU), 508.738 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.588 Gbps (per GPU), 508.703 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.226 Gbps (per GPU), 1329.810 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.224 Gbps (per GPU), 1329.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.220 Gbps (per GPU), 1329.764 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.223 Gbps (per GPU), 1329.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.223 Gbps (per GPU), 1329.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.213 Gbps (per GPU), 1329.701 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.227 Gbps (per GPU), 1329.814 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.223 Gbps (per GPU), 1329.784 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.959 Gbps (per GPU), 919.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.956 Gbps (per GPU), 919.649 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.959 Gbps (per GPU), 919.673 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.957 Gbps (per GPU), 919.655 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.956 Gbps (per GPU), 919.649 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.954 Gbps (per GPU), 919.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.949 Gbps (per GPU), 919.594 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.955 Gbps (per GPU), 919.640 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.827 Gbps (per GPU), 358.613 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.827 Gbps (per GPU), 358.612 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.827 Gbps (per GPU), 358.613 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.826 Gbps (per GPU), 358.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.826 Gbps (per GPU), 358.608 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.826 Gbps (per GPU), 358.608 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.826 Gbps (per GPU), 358.609 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.826 Gbps (per GPU), 358.611 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.76ms  0.94ms  2.32  5.55K  0.06M
 chk_1  0.42ms  0.79ms  0.97ms  2.35  5.75K  0.05M
 chk_2  0.38ms  0.72ms  0.89ms  2.37  5.20K  0.07M
 chk_3  0.41ms  0.78ms  0.97ms  2.39  5.57K  0.06M
 chk_4  0.37ms  0.70ms  0.88ms  2.36  5.05K  0.08M
 chk_5  0.36ms  0.73ms  0.90ms  2.50  4.73K  0.11M
 chk_6  0.37ms  0.69ms  0.87ms  2.35  4.98K  0.08M
 chk_7  0.37ms  0.71ms  0.88ms  2.38  5.01K  0.09M
 chk_8  0.41ms  0.79ms  0.97ms  2.34  5.73K  0.05M
 chk_9  0.35ms  0.70ms  0.88ms  2.47  4.54K  0.11M
chk_10  0.38ms  0.72ms  0.91ms  2.36  5.36K  0.07M
chk_11  0.39ms  0.73ms  0.92ms  2.37  5.39K  0.08M
chk_12  0.41ms  0.79ms  0.97ms  2.34  5.77K  0.05M
chk_13  0.39ms  0.73ms  0.92ms  2.37  5.43K  0.06M
chk_14  0.40ms  0.76ms  0.94ms  2.34  5.46K  0.06M
chk_15  0.42ms  0.79ms  0.98ms  2.33  5.88K  0.04M
chk_16  0.41ms  0.77ms  0.95ms  2.35  5.50K  0.06M
chk_17  0.36ms  0.73ms  0.90ms  2.48  4.86K  0.09M
chk_18  0.39ms  0.77ms  0.95ms  2.46  5.39K  0.07M
chk_19  0.38ms  0.72ms  0.90ms  2.37  5.20K  0.07M
chk_20  0.41ms  0.76ms  0.94ms  2.32  5.51K  0.06M
chk_21  0.42ms  0.77ms  0.96ms  2.31  5.81K  0.05M
chk_22  0.38ms  0.72ms  0.90ms  2.34  5.32K  0.07M
chk_23  0.39ms  0.75ms  0.93ms  2.42  5.39K  0.07M
chk_24  0.36ms  0.71ms  0.88ms  2.47  4.62K  0.11M
chk_25  0.37ms  0.71ms  0.89ms  2.39  5.04K  0.08M
chk_26  0.35ms  0.71ms  0.88ms  2.49  4.55K  0.11M
chk_27  0.38ms  0.70ms  0.88ms  2.32  5.30K  0.06M
chk_28  0.41ms  0.77ms  0.96ms  2.35  5.58K  0.06M
chk_29  0.37ms  0.72ms  0.90ms  2.42  4.98K  0.09M
chk_30  0.40ms  0.77ms  0.95ms  2.36  5.50K  0.07M
chk_31  0.39ms  0.72ms  0.91ms  2.34  5.38K  0.07M
   Avg  0.39  0.74  0.92
   Max  0.42  0.79  0.98
   Min  0.35  0.69  0.87
 Ratio  1.18  1.14  1.13
   Var  0.00  0.00  0.00
Profiling takes 0.902 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 201.920 ms
Partition 0 [0, 9) has cost: 201.920 ms
Partition 1 [9, 17) has cost: 189.522 ms
Partition 2 [17, 25) has cost: 189.522 ms
Partition 3 [25, 33) has cost: 189.522 ms
Partition 4 [33, 41) has cost: 189.522 ms
Partition 5 [41, 49) has cost: 189.522 ms
Partition 6 [49, 57) has cost: 189.522 ms
Partition 7 [57, 65) has cost: 195.303 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 99.078 ms
GPU 0, Compute+Comm Time: 79.589 ms, Bubble Time: 17.261 ms, Imbalance Overhead: 2.228 ms
GPU 1, Compute+Comm Time: 75.597 ms, Bubble Time: 17.369 ms, Imbalance Overhead: 6.113 ms
GPU 2, Compute+Comm Time: 75.597 ms, Bubble Time: 17.555 ms, Imbalance Overhead: 5.927 ms
GPU 3, Compute+Comm Time: 75.597 ms, Bubble Time: 17.606 ms, Imbalance Overhead: 5.875 ms
GPU 4, Compute+Comm Time: 75.597 ms, Bubble Time: 17.733 ms, Imbalance Overhead: 5.748 ms
GPU 5, Compute+Comm Time: 75.597 ms, Bubble Time: 17.802 ms, Imbalance Overhead: 5.680 ms
GPU 6, Compute+Comm Time: 75.597 ms, Bubble Time: 17.829 ms, Imbalance Overhead: 5.653 ms
GPU 7, Compute+Comm Time: 76.870 ms, Bubble Time: 17.990 ms, Imbalance Overhead: 4.218 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 192.478 ms
GPU 0, Compute+Comm Time: 151.095 ms, Bubble Time: 34.907 ms, Imbalance Overhead: 6.476 ms
GPU 1, Compute+Comm Time: 146.587 ms, Bubble Time: 34.642 ms, Imbalance Overhead: 11.249 ms
GPU 2, Compute+Comm Time: 146.587 ms, Bubble Time: 34.626 ms, Imbalance Overhead: 11.264 ms
GPU 3, Compute+Comm Time: 146.587 ms, Bubble Time: 34.468 ms, Imbalance Overhead: 11.423 ms
GPU 4, Compute+Comm Time: 146.587 ms, Bubble Time: 34.279 ms, Imbalance Overhead: 11.612 ms
GPU 5, Compute+Comm Time: 146.587 ms, Bubble Time: 34.155 ms, Imbalance Overhead: 11.736 ms
GPU 6, Compute+Comm Time: 146.587 ms, Bubble Time: 33.751 ms, Imbalance Overhead: 12.140 ms
GPU 7, Compute+Comm Time: 154.992 ms, Bubble Time: 33.545 ms, Imbalance Overhead: 3.941 ms
The estimated cost of the whole pipeline: 306.133 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 391.442 ms
Partition 0 [0, 17) has cost: 391.442 ms
Partition 1 [17, 33) has cost: 379.044 ms
Partition 2 [33, 49) has cost: 379.044 ms
Partition 3 [49, 65) has cost: 384.825 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 127.739 ms
GPU 0, Compute+Comm Time: 105.017 ms, Bubble Time: 19.697 ms, Imbalance Overhead: 3.025 ms
GPU 1, Compute+Comm Time: 102.970 ms, Bubble Time: 20.023 ms, Imbalance Overhead: 4.746 ms
GPU 2, Compute+Comm Time: 102.970 ms, Bubble Time: 20.206 ms, Imbalance Overhead: 4.564 ms
GPU 3, Compute+Comm Time: 103.610 ms, Bubble Time: 20.368 ms, Imbalance Overhead: 3.762 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 218.391 ms
GPU 0, Compute+Comm Time: 177.831 ms, Bubble Time: 34.832 ms, Imbalance Overhead: 5.728 ms
GPU 1, Compute+Comm Time: 175.549 ms, Bubble Time: 34.592 ms, Imbalance Overhead: 8.251 ms
GPU 2, Compute+Comm Time: 175.549 ms, Bubble Time: 34.323 ms, Imbalance Overhead: 8.520 ms
GPU 3, Compute+Comm Time: 179.856 ms, Bubble Time: 33.706 ms, Imbalance Overhead: 4.829 ms
    The estimated cost with 2 DP ways is 363.437 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 770.486 ms
Partition 0 [0, 33) has cost: 770.486 ms
Partition 1 [33, 65) has cost: 763.870 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 223.064 ms
GPU 0, Compute+Comm Time: 193.072 ms, Bubble Time: 24.179 ms, Imbalance Overhead: 5.813 ms
GPU 1, Compute+Comm Time: 192.350 ms, Bubble Time: 24.919 ms, Imbalance Overhead: 5.796 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 308.782 ms
GPU 0, Compute+Comm Time: 266.554 ms, Bubble Time: 34.570 ms, Imbalance Overhead: 7.657 ms
GPU 1, Compute+Comm Time: 267.597 ms, Bubble Time: 33.509 ms, Imbalance Overhead: 7.676 ms
    The estimated cost with 4 DP ways is 558.438 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1534.356 ms
Partition 0 [0, 65) has cost: 1534.356 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 770.474 ms
GPU 0, Compute+Comm Time: 770.474 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 846.430 ms
GPU 0, Compute+Comm Time: 846.430 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1697.749 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 6, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.1912	TrainAcc 0.0910	ValidAcc 0.0487	TestAcc 0.0390	BestValid 0.0487
	Epoch 50:	Loss 4.1114	TrainAcc 0.0757	ValidAcc 0.0490	TestAcc 0.0399	BestValid 0.0490
	Epoch 75:	Loss 3.8895	TrainAcc 0.0690	ValidAcc 0.0442	TestAcc 0.0367	BestValid 0.0490
	Epoch 100:	Loss 4.0539	TrainAcc 0.0937	ValidAcc 0.0537	TestAcc 0.0435	BestValid 0.0537
Node 0, Pre/Post-Pipelining: 4.190 / 36.606 ms, Bubble: 66.948 ms, Compute: 200.933 ms, Comm: 35.122 ms, Imbalance: 26.111 ms
Node 2, Pre/Post-Pipelining: 3.751 / 36.721 ms, Bubble: 65.958 ms, Compute: 192.487 ms, Comm: 51.828 ms, Imbalance: 19.489 ms
Node 5, Pre/Post-Pipelining: 3.864 / 36.579 ms, Bubble: 65.939 ms, Compute: 188.822 ms, Comm: 52.748 ms, Imbalance: 22.687 ms
Node 3, Pre/Post-Pipelining: 4.029 / 36.668 ms, Bubble: 64.660 ms, Compute: 190.220 ms, Comm: 55.530 ms, Imbalance: 19.214 ms
Node 1, Pre/Post-Pipelining: 3.808 / 36.646 ms, Bubble: 65.967 ms, Compute: 189.328 ms, Comm: 44.959 ms, Imbalance: 29.065 ms
Node 4, Pre/Post-Pipelining: 3.626 / 36.577 ms, Bubble: 65.133 ms, Compute: 189.077 ms, Comm: 56.082 ms, Imbalance: 20.081 ms
Node 6, Pre/Post-Pipelining: 3.125 / 36.744 ms, Bubble: 65.369 ms, Compute: 193.972 ms, Comm: 44.048 ms, Imbalance: 26.454 ms
Node 7, Pre/Post-Pipelining: 3.123 / 50.825 ms, Bubble: 51.912 ms, Compute: 211.786 ms, Comm: 33.177 ms, Imbalance: 18.591 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 4.190 ms
Cluster-Wide Average, Post-Pipelining Overhead: 36.606 ms
Cluster-Wide Average, Bubble: 66.948 ms
Cluster-Wide Average, Compute: 200.933 ms
Cluster-Wide Average, Communication: 35.122 ms
Cluster-Wide Average, Imbalance: 26.111 ms
Node 0, GPU memory consumption: 10.510 GB
Node 2, GPU memory consumption: 8.721 GB
Node 4, GPU memory consumption: 8.698 GB
Node 3, GPU memory consumption: 8.698 GB
Node 5, GPU memory consumption: 8.721 GB
Node 1, GPU memory consumption: 8.721 GB
Node 7, GPU memory consumption: 8.801 GB
Node 6, GPU memory consumption: 8.721 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 1.626777 s---------------
------------------------node id 1,  per-epoch time: 1.626935 s---------------
------------------------node id 4,  per-epoch time: 1.626981 s---------------
------------------------node id 2,  per-epoch time: 1.626827 s---------------
------------------------node id 5,  per-epoch time: 1.626832 s---------------
------------------------node id 3,  per-epoch time: 1.626880 s---------------
------------------------node id 6,  per-epoch time: 1.627055 s---------------
------------------------node id 7,  per-epoch time: 1.627196 s---------------
************ Profiling Results ************
	Bubble: 1374.639905 (ms) (87.42 percentage)
	Compute: 189.824705 (ms) (12.07 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 7.974371 (ms) (0.51 percentage)
	Layer-level communication (cluster-wide, per-epoch): 3.391 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.071 GB
	Total communication (cluster-wide, per-epoch): 3.463 GB
	Aggregated layer-level communication throughput: 623.999 Gbps
Highest valid_acc: 0.0537
Target test_acc: 0.0435
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
