Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.040 seconds.
        It takes 0.046 seconds.
        It takes 0.041 seconds.
        It takes 0.048 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.053 seconds.
        It takes 0.055 seconds.
        It takes 0.059 seconds.
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.978 Gbps (per GPU), 511.822 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.937 Gbps (per GPU), 511.496 Gbps (aggregated)
The layer-level communication performance: 63.937 Gbps (per GPU), 511.492 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.902 Gbps (per GPU), 511.216 Gbps (aggregated)
The layer-level communication performance: 63.897 Gbps (per GPU), 511.178 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.865 Gbps (per GPU), 510.921 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.858 Gbps (per GPU), 510.862 Gbps (aggregated)
The layer-level communication performance: 63.853 Gbps (per GPU), 510.824 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.848 Gbps (per GPU), 1318.785 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.829 Gbps (per GPU), 1318.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.834 Gbps (per GPU), 1318.669 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.853 Gbps (per GPU), 1318.824 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.847 Gbps (per GPU), 1318.776 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.848 Gbps (per GPU), 1318.782 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.843 Gbps (per GPU), 1318.747 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.848 Gbps (per GPU), 1318.782 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.757 Gbps (per GPU), 910.057 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.756 Gbps (per GPU), 910.045 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.757 Gbps (per GPU), 910.056 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.752 Gbps (per GPU), 910.020 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.749 Gbps (per GPU), 909.991 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.747 Gbps (per GPU), 909.975 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.742 Gbps (per GPU), 909.932 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.737 Gbps (per GPU), 909.896 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.775 Gbps (per GPU), 358.199 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.775 Gbps (per GPU), 358.200 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.775 Gbps (per GPU), 358.201 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.775 Gbps (per GPU), 358.197 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.775 Gbps (per GPU), 358.197 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.775 Gbps (per GPU), 358.197 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.775 Gbps (per GPU), 358.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.775 Gbps (per GPU), 358.198 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.98ms  2.36ms  2.74ms  2.80 21.17K  0.46M
 chk_1  0.98ms  2.52ms  2.89ms  2.96 21.17K  0.55M
 chk_2  0.98ms  2.33ms  2.70ms  2.77 21.17K  0.39M
 chk_3  0.97ms  2.23ms  2.61ms  2.67 21.17K  0.24M
 chk_4  0.98ms  2.14ms  2.51ms  2.57 21.17K  0.17M
 chk_5  0.98ms  2.16ms  2.53ms  2.59 21.17K  0.22M
 chk_6  0.98ms  2.15ms  2.53ms  2.58 21.17K  0.16M
 chk_7  0.98ms  2.12ms  2.50ms  2.55 21.17K  0.12M
   Avg  0.98  2.25  2.63
   Max  0.98  2.52  2.89
   Min  0.97  2.12  2.50
 Ratio  1.00  1.18  1.16
   Var  0.00  0.02  0.02
Profiling takes 0.633 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 151.841 ms
Partition 0 [0, 9) has cost: 151.841 ms
Partition 1 [9, 17) has cost: 144.022 ms
Partition 2 [17, 25) has cost: 144.022 ms
Partition 3 [25, 33) has cost: 144.022 ms
Partition 4 [33, 41) has cost: 144.022 ms
Partition 5 [41, 49) has cost: 144.022 ms
Partition 6 [49, 57) has cost: 144.022 ms
Partition 7 [57, 65) has cost: 147.028 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 118.511 ms
GPU 0, Compute+Comm Time: 62.105 ms, Bubble Time: 52.706 ms, Imbalance Overhead: 3.700 ms
GPU 1, Compute+Comm Time: 59.962 ms, Bubble Time: 52.466 ms, Imbalance Overhead: 6.083 ms
GPU 2, Compute+Comm Time: 59.962 ms, Bubble Time: 53.163 ms, Imbalance Overhead: 5.387 ms
GPU 3, Compute+Comm Time: 59.962 ms, Bubble Time: 53.855 ms, Imbalance Overhead: 4.694 ms
GPU 4, Compute+Comm Time: 59.962 ms, Bubble Time: 54.800 ms, Imbalance Overhead: 3.750 ms
GPU 5, Compute+Comm Time: 59.962 ms, Bubble Time: 55.687 ms, Imbalance Overhead: 2.862 ms
GPU 6, Compute+Comm Time: 59.962 ms, Bubble Time: 56.606 ms, Imbalance Overhead: 1.944 ms
GPU 7, Compute+Comm Time: 60.565 ms, Bubble Time: 57.592 ms, Imbalance Overhead: 0.354 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 231.919 ms
GPU 0, Compute+Comm Time: 118.989 ms, Bubble Time: 112.390 ms, Imbalance Overhead: 0.540 ms
GPU 1, Compute+Comm Time: 116.585 ms, Bubble Time: 110.620 ms, Imbalance Overhead: 4.714 ms
GPU 2, Compute+Comm Time: 116.585 ms, Bubble Time: 108.999 ms, Imbalance Overhead: 6.335 ms
GPU 3, Compute+Comm Time: 116.585 ms, Bubble Time: 107.401 ms, Imbalance Overhead: 7.933 ms
GPU 4, Compute+Comm Time: 116.585 ms, Bubble Time: 105.721 ms, Imbalance Overhead: 9.613 ms
GPU 5, Compute+Comm Time: 116.585 ms, Bubble Time: 104.517 ms, Imbalance Overhead: 10.817 ms
GPU 6, Compute+Comm Time: 116.585 ms, Bubble Time: 103.119 ms, Imbalance Overhead: 12.215 ms
GPU 7, Compute+Comm Time: 122.261 ms, Bubble Time: 103.555 ms, Imbalance Overhead: 6.103 ms
The estimated cost of the whole pipeline: 367.952 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 295.863 ms
Partition 0 [0, 17) has cost: 295.863 ms
Partition 1 [17, 33) has cost: 288.043 ms
Partition 2 [33, 49) has cost: 288.043 ms
Partition 3 [49, 65) has cost: 291.050 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 154.807 ms
GPU 0, Compute+Comm Time: 87.099 ms, Bubble Time: 63.515 ms, Imbalance Overhead: 4.192 ms
GPU 1, Compute+Comm Time: 86.026 ms, Bubble Time: 64.715 ms, Imbalance Overhead: 4.065 ms
GPU 2, Compute+Comm Time: 86.026 ms, Bubble Time: 66.565 ms, Imbalance Overhead: 2.215 ms
GPU 3, Compute+Comm Time: 86.328 ms, Bubble Time: 68.478 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 260.105 ms
GPU 0, Compute+Comm Time: 144.620 ms, Bubble Time: 115.485 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 143.417 ms, Bubble Time: 111.944 ms, Imbalance Overhead: 4.745 ms
GPU 2, Compute+Comm Time: 143.417 ms, Bubble Time: 108.449 ms, Imbalance Overhead: 8.239 ms
GPU 3, Compute+Comm Time: 146.258 ms, Bubble Time: 106.059 ms, Imbalance Overhead: 7.789 ms
    The estimated cost with 2 DP ways is 435.658 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 583.906 ms
Partition 0 [0, 33) has cost: 583.906 ms
Partition 1 [33, 65) has cost: 579.093 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 259.753 ms
GPU 0, Compute+Comm Time: 172.142 ms, Bubble Time: 83.953 ms, Imbalance Overhead: 3.658 ms
GPU 1, Compute+Comm Time: 171.757 ms, Bubble Time: 87.996 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 350.040 ms
GPU 0, Compute+Comm Time: 230.559 ms, Bubble Time: 119.481 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 231.379 ms, Bubble Time: 111.484 ms, Imbalance Overhead: 7.177 ms
    The estimated cost with 4 DP ways is 640.283 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1162.999 ms
Partition 0 [0, 65) has cost: 1162.999 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 700.430 ms
GPU 0, Compute+Comm Time: 700.430 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 763.187 ms
GPU 0, Compute+Comm Time: 763.187 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1536.797 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 22787 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
[proxy:0:1@gnerv2] HYD_pmcd_pmip_control_cmd_cb (../../../../src/pm/hydra/proxy/pmip_cb.c:480): assert (!closed) failed
[proxy:0:1@gnerv2] HYDT_dmxu_poll_wait_for_event (../../../../src/pm/hydra/lib/tools/demux/demux_poll.c:76): callback returned error status
[proxy:0:1@gnerv2] main (../../../../src/pm/hydra/proxy/pmip.c:190): demux engine error waiting for event
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Aborted (signal 6)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
