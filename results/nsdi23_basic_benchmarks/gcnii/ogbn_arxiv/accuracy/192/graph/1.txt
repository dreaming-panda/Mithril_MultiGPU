Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 0 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.044 seconds.
        It takes 0.046 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
        It takes 0.055 seconds.
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Feature Vector...
        It takes 0.056 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 61.464 Gbps (per GPU), 491.711 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 61.428 Gbps (per GPU), 491.420 Gbps (aggregated)
The layer-level communication performance: 61.426 Gbps (per GPU), 491.405 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 61.391 Gbps (per GPU), 491.124 Gbps (aggregated)
The layer-level communication performance: 61.386 Gbps (per GPU), 491.089 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 61.358 Gbps (per GPU), 490.862 Gbps (aggregated)
The layer-level communication performance: 61.351 Gbps (per GPU), 490.810 Gbps (aggregated)
The layer-level communication performance: 61.347 Gbps (per GPU), 490.774 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.848 Gbps (per GPU), 1310.784 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.849 Gbps (per GPU), 1310.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.846 Gbps (per GPU), 1310.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.846 Gbps (per GPU), 1310.768 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.847 Gbps (per GPU), 1310.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.856 Gbps (per GPU), 1310.848 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.841 Gbps (per GPU), 1310.730 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.841 Gbps (per GPU), 1310.730 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.229 Gbps (per GPU), 913.832 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.229 Gbps (per GPU), 913.830 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.229 Gbps (per GPU), 913.833 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.226 Gbps (per GPU), 913.809 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.226 Gbps (per GPU), 913.811 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.223 Gbps (per GPU), 913.788 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.227 Gbps (per GPU), 913.813 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.224 Gbps (per GPU), 913.791 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.707 Gbps (per GPU), 365.660 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.708 Gbps (per GPU), 365.662 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.707 Gbps (per GPU), 365.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.707 Gbps (per GPU), 365.657 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.706 Gbps (per GPU), 365.648 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.706 Gbps (per GPU), 365.649 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.706 Gbps (per GPU), 365.649 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.706 Gbps (per GPU), 365.648 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.98ms  2.36ms  2.73ms  2.79 21.17K  0.46M
 chk_1  0.98ms  2.51ms  2.90ms  2.95 21.17K  0.55M
 chk_2  0.98ms  2.34ms  2.70ms  2.76 21.17K  0.39M
 chk_3  0.98ms  2.24ms  2.61ms  2.66 21.17K  0.24M
 chk_4  0.98ms  2.14ms  2.52ms  2.57 21.17K  0.17M
 chk_5  0.98ms  2.16ms  2.54ms  2.59 21.17K  0.22M
 chk_6  0.98ms  2.15ms  2.53ms  2.58 21.17K  0.16M
 chk_7  0.98ms  2.12ms  2.50ms  2.55 21.17K  0.12M
   Avg  0.98  2.25  2.63
   Max  0.98  2.51  2.90
   Min  0.98  2.12  2.50
 Ratio  1.00  1.19  1.16
   Var  0.00  0.02  0.02
Profiling takes 0.662 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 152.025 ms
Partition 0 [0, 9) has cost: 152.025 ms
Partition 1 [9, 17) has cost: 144.186 ms
Partition 2 [17, 25) has cost: 144.186 ms
Partition 3 [25, 33) has cost: 144.186 ms
Partition 4 [33, 41) has cost: 144.186 ms
Partition 5 [41, 49) has cost: 144.186 ms
Partition 6 [49, 57) has cost: 144.186 ms
Partition 7 [57, 65) has cost: 147.184 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 119.702 ms
GPU 0, Compute+Comm Time: 62.845 ms, Bubble Time: 53.334 ms, Imbalance Overhead: 3.523 ms
GPU 1, Compute+Comm Time: 60.690 ms, Bubble Time: 53.133 ms, Imbalance Overhead: 5.879 ms
GPU 2, Compute+Comm Time: 60.690 ms, Bubble Time: 53.799 ms, Imbalance Overhead: 5.212 ms
GPU 3, Compute+Comm Time: 60.690 ms, Bubble Time: 54.451 ms, Imbalance Overhead: 4.561 ms
GPU 4, Compute+Comm Time: 60.690 ms, Bubble Time: 55.359 ms, Imbalance Overhead: 3.653 ms
GPU 5, Compute+Comm Time: 60.690 ms, Bubble Time: 56.224 ms, Imbalance Overhead: 2.788 ms
GPU 6, Compute+Comm Time: 60.690 ms, Bubble Time: 57.120 ms, Imbalance Overhead: 1.892 ms
GPU 7, Compute+Comm Time: 61.298 ms, Bubble Time: 58.090 ms, Imbalance Overhead: 0.314 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 233.322 ms
GPU 0, Compute+Comm Time: 119.742 ms, Bubble Time: 113.068 ms, Imbalance Overhead: 0.512 ms
GPU 1, Compute+Comm Time: 117.351 ms, Bubble Time: 111.265 ms, Imbalance Overhead: 4.706 ms
GPU 2, Compute+Comm Time: 117.351 ms, Bubble Time: 109.635 ms, Imbalance Overhead: 6.336 ms
GPU 3, Compute+Comm Time: 117.351 ms, Bubble Time: 108.021 ms, Imbalance Overhead: 7.950 ms
GPU 4, Compute+Comm Time: 117.351 ms, Bubble Time: 106.345 ms, Imbalance Overhead: 9.626 ms
GPU 5, Compute+Comm Time: 117.351 ms, Bubble Time: 105.162 ms, Imbalance Overhead: 10.809 ms
GPU 6, Compute+Comm Time: 117.351 ms, Bubble Time: 103.792 ms, Imbalance Overhead: 12.179 ms
GPU 7, Compute+Comm Time: 123.036 ms, Bubble Time: 104.189 ms, Imbalance Overhead: 6.097 ms
The estimated cost of the whole pipeline: 370.675 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 296.212 ms
Partition 0 [0, 17) has cost: 296.212 ms
Partition 1 [17, 33) has cost: 288.372 ms
Partition 2 [33, 49) has cost: 288.372 ms
Partition 3 [49, 65) has cost: 291.371 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 156.145 ms
GPU 0, Compute+Comm Time: 87.934 ms, Bubble Time: 64.176 ms, Imbalance Overhead: 4.035 ms
GPU 1, Compute+Comm Time: 86.855 ms, Bubble Time: 65.311 ms, Imbalance Overhead: 3.978 ms
GPU 2, Compute+Comm Time: 86.855 ms, Bubble Time: 67.117 ms, Imbalance Overhead: 2.173 ms
GPU 3, Compute+Comm Time: 87.160 ms, Bubble Time: 68.984 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 261.664 ms
GPU 0, Compute+Comm Time: 145.514 ms, Bubble Time: 116.149 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 144.314 ms, Bubble Time: 112.588 ms, Imbalance Overhead: 4.762 ms
GPU 2, Compute+Comm Time: 144.314 ms, Bubble Time: 109.057 ms, Imbalance Overhead: 8.292 ms
GPU 3, Compute+Comm Time: 147.158 ms, Bubble Time: 106.733 ms, Imbalance Overhead: 7.773 ms
    The estimated cost with 2 DP ways is 438.699 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 584.584 ms
Partition 0 [0, 33) has cost: 584.584 ms
Partition 1 [33, 65) has cost: 579.743 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 259.942 ms
GPU 0, Compute+Comm Time: 172.296 ms, Bubble Time: 84.072 ms, Imbalance Overhead: 3.574 ms
GPU 1, Compute+Comm Time: 171.911 ms, Bubble Time: 88.031 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 350.380 ms
GPU 0, Compute+Comm Time: 230.758 ms, Bubble Time: 119.621 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 231.575 ms, Bubble Time: 111.546 ms, Imbalance Overhead: 7.258 ms
    The estimated cost with 4 DP ways is 640.838 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1164.327 ms
Partition 0 [0, 65) has cost: 1164.327 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 687.011 ms
GPU 0, Compute+Comm Time: 687.011 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 749.975 ms
GPU 0, Compute+Comm Time: 749.975 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1508.835 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2734: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.grad != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 22674 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Aborted (signal 6)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
