Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.046 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.039 seconds.
Building the Feature Vector...
        It takes 0.053 seconds.
        It takes 0.054 seconds.
        It takes 0.052 seconds.
        It takes 0.055 seconds.
        It takes 0.053 seconds.
        It takes 0.055 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.802 Gbps (per GPU), 510.414 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.762 Gbps (per GPU), 510.098 Gbps (aggregated)
The layer-level communication performance: 63.760 Gbps (per GPU), 510.082 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.725 Gbps (per GPU), 509.800 Gbps (aggregated)
The layer-level communication performance: 63.721 Gbps (per GPU), 509.765 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.690 Gbps (per GPU), 509.516 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.683 Gbps (per GPU), 509.461 Gbps (aggregated)
The layer-level communication performance: 63.678 Gbps (per GPU), 509.422 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.164 Gbps (per GPU), 1321.311 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.152 Gbps (per GPU), 1321.219 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.154 Gbps (per GPU), 1321.233 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.166 Gbps (per GPU), 1321.327 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.167 Gbps (per GPU), 1321.337 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.159 Gbps (per GPU), 1321.275 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.158 Gbps (per GPU), 1321.266 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.157 Gbps (per GPU), 1321.255 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.175 Gbps (per GPU), 913.397 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.175 Gbps (per GPU), 913.400 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.167 Gbps (per GPU), 913.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.171 Gbps (per GPU), 913.366 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.168 Gbps (per GPU), 913.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.156 Gbps (per GPU), 913.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.159 Gbps (per GPU), 913.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.154 Gbps (per GPU), 913.235 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.988 Gbps (per GPU), 359.901 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.988 Gbps (per GPU), 359.904 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.988 Gbps (per GPU), 359.903 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.988 Gbps (per GPU), 359.901 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.988 Gbps (per GPU), 359.902 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.988 Gbps (per GPU), 359.901 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.987 Gbps (per GPU), 359.900 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.987 Gbps (per GPU), 359.897 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.99ms  2.37ms  2.76ms  2.79 21.17K  0.46M
 chk_1  0.99ms  2.54ms  2.92ms  2.96 21.17K  0.55M
 chk_2  0.99ms  2.34ms  2.73ms  2.76 21.17K  0.39M
 chk_3  0.99ms  2.25ms  2.63ms  2.67 21.17K  0.24M
 chk_4  0.99ms  2.15ms  2.53ms  2.57 21.17K  0.17M
 chk_5  0.99ms  2.17ms  2.55ms  2.58 21.17K  0.22M
 chk_6  0.99ms  2.16ms  2.54ms  2.58 21.17K  0.16M
 chk_7  0.99ms  2.14ms  2.52ms  2.55 21.17K  0.12M
   Avg  0.99  2.27  2.65
   Max  0.99  2.54  2.92
   Min  0.99  2.14  2.52
 Ratio  1.00  1.19  1.16
   Var  0.00  0.02  0.02
Profiling takes 0.639 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 152.951 ms
Partition 0 [0, 9) has cost: 152.951 ms
Partition 1 [9, 17) has cost: 145.051 ms
Partition 2 [17, 25) has cost: 145.051 ms
Partition 3 [25, 33) has cost: 145.051 ms
Partition 4 [33, 41) has cost: 145.051 ms
Partition 5 [41, 49) has cost: 145.051 ms
Partition 6 [49, 57) has cost: 145.051 ms
Partition 7 [57, 65) has cost: 148.097 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 119.340 ms
GPU 0, Compute+Comm Time: 62.519 ms, Bubble Time: 53.049 ms, Imbalance Overhead: 3.772 ms
GPU 1, Compute+Comm Time: 60.345 ms, Bubble Time: 52.804 ms, Imbalance Overhead: 6.191 ms
GPU 2, Compute+Comm Time: 60.345 ms, Bubble Time: 53.518 ms, Imbalance Overhead: 5.477 ms
GPU 3, Compute+Comm Time: 60.345 ms, Bubble Time: 54.218 ms, Imbalance Overhead: 4.777 ms
GPU 4, Compute+Comm Time: 60.345 ms, Bubble Time: 55.177 ms, Imbalance Overhead: 3.818 ms
GPU 5, Compute+Comm Time: 60.345 ms, Bubble Time: 56.074 ms, Imbalance Overhead: 2.922 ms
GPU 6, Compute+Comm Time: 60.345 ms, Bubble Time: 57.009 ms, Imbalance Overhead: 1.986 ms
GPU 7, Compute+Comm Time: 60.960 ms, Bubble Time: 58.017 ms, Imbalance Overhead: 0.363 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 233.588 ms
GPU 0, Compute+Comm Time: 119.753 ms, Bubble Time: 113.278 ms, Imbalance Overhead: 0.557 ms
GPU 1, Compute+Comm Time: 117.321 ms, Bubble Time: 111.457 ms, Imbalance Overhead: 4.810 ms
GPU 2, Compute+Comm Time: 117.321 ms, Bubble Time: 109.778 ms, Imbalance Overhead: 6.489 ms
GPU 3, Compute+Comm Time: 117.321 ms, Bubble Time: 108.140 ms, Imbalance Overhead: 8.127 ms
GPU 4, Compute+Comm Time: 117.321 ms, Bubble Time: 106.420 ms, Imbalance Overhead: 9.847 ms
GPU 5, Compute+Comm Time: 117.321 ms, Bubble Time: 105.191 ms, Imbalance Overhead: 11.076 ms
GPU 6, Compute+Comm Time: 117.321 ms, Bubble Time: 103.761 ms, Imbalance Overhead: 12.506 ms
GPU 7, Compute+Comm Time: 123.047 ms, Bubble Time: 104.209 ms, Imbalance Overhead: 6.332 ms
The estimated cost of the whole pipeline: 370.574 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 298.002 ms
Partition 0 [0, 17) has cost: 298.002 ms
Partition 1 [17, 33) has cost: 290.101 ms
Partition 2 [33, 49) has cost: 290.101 ms
Partition 3 [49, 65) has cost: 293.148 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 155.496 ms
GPU 0, Compute+Comm Time: 87.468 ms, Bubble Time: 63.762 ms, Imbalance Overhead: 4.266 ms
GPU 1, Compute+Comm Time: 86.379 ms, Bubble Time: 64.997 ms, Imbalance Overhead: 4.120 ms
GPU 2, Compute+Comm Time: 86.379 ms, Bubble Time: 66.865 ms, Imbalance Overhead: 2.252 ms
GPU 3, Compute+Comm Time: 86.684 ms, Bubble Time: 68.812 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 261.555 ms
GPU 0, Compute+Comm Time: 145.334 ms, Bubble Time: 116.221 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 144.118 ms, Bubble Time: 112.560 ms, Imbalance Overhead: 4.877 ms
GPU 2, Compute+Comm Time: 144.118 ms, Bubble Time: 108.982 ms, Imbalance Overhead: 8.454 ms
GPU 3, Compute+Comm Time: 146.984 ms, Bubble Time: 106.529 ms, Imbalance Overhead: 8.042 ms
    The estimated cost with 2 DP ways is 437.904 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 588.103 ms
Partition 0 [0, 33) has cost: 588.103 ms
Partition 1 [33, 65) has cost: 583.249 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 259.859 ms
GPU 0, Compute+Comm Time: 172.207 ms, Bubble Time: 83.963 ms, Imbalance Overhead: 3.690 ms
GPU 1, Compute+Comm Time: 171.813 ms, Bubble Time: 88.047 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 350.816 ms
GPU 0, Compute+Comm Time: 231.016 ms, Bubble Time: 119.800 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 231.844 ms, Bubble Time: 111.628 ms, Imbalance Overhead: 7.344 ms
    The estimated cost with 4 DP ways is 641.210 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1171.352 ms
Partition 0 [0, 65) has cost: 1171.352 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 697.798 ms
GPU 0, Compute+Comm Time: 697.798 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 761.090 ms
GPU 0, Compute+Comm Time: 761.090 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1531.832 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2698: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 22901 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
[proxy:0:1@gnerv2] HYD_pmcd_pmip_control_cmd_cb (../../../../src/pm/hydra/proxy/pmip_cb.c:480): assert (!closed) failed
[proxy:0:1@gnerv2] HYDT_dmxu_poll_wait_for_event (../../../../src/pm/hydra/lib/tools/demux/demux_poll.c:76): callback returned error status
[proxy:0:1@gnerv2] main (../../../../src/pm/hydra/proxy/pmip.c:190): demux engine error waiting for event
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Aborted (signal 6)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
