Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.066 seconds.
Building the CSC structure...
        It takes 0.072 seconds.
Building the CSC structure...
        It takes 0.046 seconds.
        It takes 0.042 seconds.
        It takes 0.044 seconds.
        It takes 0.049 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.051 seconds.
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 128
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.029 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.984 Gbps (per GPU), 511.875 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.943 Gbps (per GPU), 511.541 Gbps (aggregated)
The layer-level communication performance: 63.942 Gbps (per GPU), 511.535 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.907 Gbps (per GPU), 511.260 Gbps (aggregated)
The layer-level communication performance: 63.903 Gbps (per GPU), 511.224 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.871 Gbps (per GPU), 510.970 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.865 Gbps (per GPU), 510.922 Gbps (aggregated)
The layer-level communication performance: 63.860 Gbps (per GPU), 510.878 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.410 Gbps (per GPU), 1315.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.411 Gbps (per GPU), 1315.286 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.409 Gbps (per GPU), 1315.273 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.409 Gbps (per GPU), 1315.270 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.397 Gbps (per GPU), 1315.180 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.410 Gbps (per GPU), 1315.277 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.408 Gbps (per GPU), 1315.267 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.408 Gbps (per GPU), 1315.264 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.003 Gbps (per GPU), 912.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.003 Gbps (per GPU), 912.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.000 Gbps (per GPU), 911.998 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.000 Gbps (per GPU), 912.003 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.000 Gbps (per GPU), 911.996 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.999 Gbps (per GPU), 911.995 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.993 Gbps (per GPU), 911.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.938 Gbps (per GPU), 911.502 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.948 Gbps (per GPU), 359.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.948 Gbps (per GPU), 359.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.948 Gbps (per GPU), 359.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.948 Gbps (per GPU), 359.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.948 Gbps (per GPU), 359.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.947 Gbps (per GPU), 359.579 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.948 Gbps (per GPU), 359.581 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.948 Gbps (per GPU), 359.581 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.45ms  0.98ms  1.18ms  2.62  5.55K  0.06M
 chk_1  0.46ms  1.01ms  1.22ms  2.67  5.75K  0.05M
 chk_2  0.44ms  0.97ms  1.17ms  2.68  5.20K  0.07M
 chk_3  0.45ms  1.01ms  1.22ms  2.70  5.57K  0.06M
 chk_4  0.43ms  0.98ms  1.17ms  2.71  5.05K  0.08M
 chk_5  0.42ms  1.00ms  1.18ms  2.82  4.73K  0.11M
 chk_6  0.43ms  0.96ms  1.16ms  2.69  4.98K  0.08M
 chk_7  0.43ms  0.99ms  1.18ms  2.74  5.01K  0.09M
 chk_8  0.45ms  1.01ms  1.22ms  2.68  5.73K  0.05M
 chk_9  0.41ms  0.92ms  1.10ms  2.68  4.54K  0.11M
chk_10  0.44ms  0.98ms  1.19ms  2.69  5.36K  0.07M
chk_11  0.44ms  1.00ms  1.20ms  2.71  5.39K  0.08M
chk_12  0.46ms  1.01ms  1.22ms  2.66  5.77K  0.05M
chk_13  0.45ms  1.00ms  1.20ms  2.69  5.43K  0.06M
chk_14  0.45ms  0.98ms  1.19ms  2.66  5.46K  0.06M
chk_15  0.46ms  1.01ms  1.22ms  2.65  5.88K  0.04M
chk_16  0.45ms  0.99ms  1.28ms  2.85  5.50K  0.06M
chk_17  0.43ms  1.00ms  1.19ms  2.79  4.86K  0.09M
chk_18  0.45ms  1.03ms  1.24ms  2.79  5.39K  0.07M
chk_19  0.44ms  0.98ms  1.18ms  2.69  5.20K  0.07M
chk_20  0.45ms  0.97ms  1.18ms  2.63  5.51K  0.06M
chk_21  0.46ms  0.99ms  1.20ms  2.61  5.81K  0.05M
chk_22  0.44ms  0.97ms  1.18ms  2.66  5.32K  0.07M
chk_23  0.45ms  1.01ms  1.22ms  2.73  5.39K  0.07M
chk_24  0.41ms  0.96ms  1.15ms  2.79  4.62K  0.11M
chk_25  0.43ms  0.99ms  1.19ms  2.74  5.04K  0.08M
chk_26  0.41ms  0.92ms  1.11ms  2.69  4.55K  0.11M
chk_27  0.44ms  0.96ms  1.16ms  2.63  5.30K  0.06M
chk_28  0.45ms  0.99ms  1.20ms  2.66  5.58K  0.06M
chk_29  0.43ms  1.00ms  1.20ms  2.80  4.98K  0.09M
chk_30  0.45ms  0.99ms  1.20ms  2.68  5.50K  0.07M
chk_31  0.44ms  0.98ms  1.18ms  2.68  5.38K  0.07M
   Avg  0.44  0.99  1.19
   Max  0.46  1.03  1.28
   Min  0.41  0.92  1.10
 Ratio  1.12  1.13  1.16
   Var  0.00  0.00  0.00
Profiling takes 1.114 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 518.749 ms
Partition 0 [0, 17) has cost: 518.749 ms
Partition 1 [17, 33) has cost: 504.653 ms
Partition 2 [33, 49) has cost: 504.653 ms
Partition 3 [49, 65) has cost: 504.653 ms
Partition 4 [65, 81) has cost: 504.653 ms
Partition 5 [81, 97) has cost: 504.653 ms
Partition 6 [97, 113) has cost: 504.653 ms
Partition 7 [113, 129) has cost: 511.190 ms
The optimal partitioning:
[0, 17)
[17, 33)
[33, 49)
[49, 65)
[65, 81)
[81, 97)
[97, 113)
[113, 129)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 225.855 ms
GPU 0, Compute+Comm Time: 183.756 ms, Bubble Time: 39.775 ms, Imbalance Overhead: 2.325 ms
GPU 1, Compute+Comm Time: 179.284 ms, Bubble Time: 39.810 ms, Imbalance Overhead: 6.761 ms
GPU 2, Compute+Comm Time: 179.284 ms, Bubble Time: 40.031 ms, Imbalance Overhead: 6.540 ms
GPU 3, Compute+Comm Time: 179.284 ms, Bubble Time: 40.108 ms, Imbalance Overhead: 6.463 ms
GPU 4, Compute+Comm Time: 179.284 ms, Bubble Time: 40.316 ms, Imbalance Overhead: 6.255 ms
GPU 5, Compute+Comm Time: 179.284 ms, Bubble Time: 40.347 ms, Imbalance Overhead: 6.224 ms
GPU 6, Compute+Comm Time: 179.284 ms, Bubble Time: 40.432 ms, Imbalance Overhead: 6.139 ms
GPU 7, Compute+Comm Time: 180.734 ms, Bubble Time: 40.587 ms, Imbalance Overhead: 4.534 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 466.859 ms
GPU 0, Compute+Comm Time: 373.818 ms, Bubble Time: 83.933 ms, Imbalance Overhead: 9.107 ms
GPU 1, Compute+Comm Time: 368.732 ms, Bubble Time: 83.641 ms, Imbalance Overhead: 14.486 ms
GPU 2, Compute+Comm Time: 368.732 ms, Bubble Time: 83.481 ms, Imbalance Overhead: 14.646 ms
GPU 3, Compute+Comm Time: 368.732 ms, Bubble Time: 83.303 ms, Imbalance Overhead: 14.824 ms
GPU 4, Compute+Comm Time: 368.732 ms, Bubble Time: 82.933 ms, Imbalance Overhead: 15.194 ms
GPU 5, Compute+Comm Time: 368.732 ms, Bubble Time: 82.653 ms, Imbalance Overhead: 15.474 ms
GPU 6, Compute+Comm Time: 368.732 ms, Bubble Time: 82.061 ms, Imbalance Overhead: 16.066 ms
GPU 7, Compute+Comm Time: 378.356 ms, Bubble Time: 81.902 ms, Imbalance Overhead: 6.601 ms
The estimated cost of the whole pipeline: 727.349 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 1023.402 ms
Partition 0 [0, 33) has cost: 1023.402 ms
Partition 1 [33, 65) has cost: 1009.307 ms
Partition 2 [65, 97) has cost: 1009.307 ms
Partition 3 [97, 129) has cost: 1015.844 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 304.855 ms
GPU 0, Compute+Comm Time: 253.756 ms, Bubble Time: 47.554 ms, Imbalance Overhead: 3.545 ms
GPU 1, Compute+Comm Time: 251.493 ms, Bubble Time: 47.884 ms, Imbalance Overhead: 5.478 ms
GPU 2, Compute+Comm Time: 251.493 ms, Bubble Time: 48.024 ms, Imbalance Overhead: 5.338 ms
GPU 3, Compute+Comm Time: 252.220 ms, Bubble Time: 48.389 ms, Imbalance Overhead: 4.246 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 539.048 ms
GPU 0, Compute+Comm Time: 446.718 ms, Bubble Time: 85.589 ms, Imbalance Overhead: 6.742 ms
GPU 1, Compute+Comm Time: 444.109 ms, Bubble Time: 84.917 ms, Imbalance Overhead: 10.023 ms
GPU 2, Compute+Comm Time: 444.109 ms, Bubble Time: 84.353 ms, Imbalance Overhead: 10.586 ms
GPU 3, Compute+Comm Time: 449.007 ms, Bubble Time: 83.348 ms, Imbalance Overhead: 6.694 ms
    The estimated cost with 2 DP ways is 886.099 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 2032.709 ms
Partition 0 [0, 65) has cost: 2032.709 ms
Partition 1 [65, 129) has cost: 2025.150 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 564.731 ms
GPU 0, Compute+Comm Time: 491.884 ms, Bubble Time: 61.496 ms, Imbalance Overhead: 11.352 ms
GPU 1, Compute+Comm Time: 491.111 ms, Bubble Time: 62.841 ms, Imbalance Overhead: 10.780 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 785.656 ms
GPU 0, Compute+Comm Time: 685.819 ms, Bubble Time: 87.593 ms, Imbalance Overhead: 12.244 ms
GPU 1, Compute+Comm Time: 686.974 ms, Bubble Time: 85.262 ms, Imbalance Overhead: 13.420 ms
    The estimated cost with 4 DP ways is 1417.907 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 4057.859 ms
Partition 0 [0, 129) has cost: 4057.859 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 2043.199 ms
GPU 0, Compute+Comm Time: 2043.199 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 2240.500 ms
GPU 0, Compute+Comm Time: 2240.500 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 4497.884 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 118)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [118, 230)
*** Node 1, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [342, 454)
*** Node 3, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [230, 342)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [454, 566)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [566, 678)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [678, 790)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [790, 905)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[118, 230)...
+++++++++ Node 0 initializing the weights for op[0, 118)...
+++++++++ Node 5 initializing the weights for op[566, 678)...
+++++++++ Node 2 initializing the weights for op[230, 342)...
+++++++++ Node 3 initializing the weights for op[342, 454)...
+++++++++ Node 7 initializing the weights for op[790, 905)...
+++++++++ Node 4 initializing the weights for op[454, 566)...
+++++++++ Node 6 initializing the weights for op[678, 790)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 21.8544	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 50:	Loss 21.2933	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0587	BestValid 0.0763
	Epoch 75:	Loss 21.2216	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 100:	Loss 21.1693	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
Node 1, Pre/Post-Pipelining: 8.692 / 77.315 ms, Bubble: 131.318 ms, Compute: 491.953 ms, Comm: 71.133 ms, Imbalance: 42.662 ms
Node 4, Pre/Post-Pipelining: 8.285 / 77.302 ms, Bubble: 131.357 ms, Compute: 491.006 ms, Comm: 90.308 ms, Imbalance: 26.060 ms
Node 3, Pre/Post-Pipelining: 8.466 / 77.645 ms, Bubble: 129.833 ms, Compute: 497.738 ms, Comm: 90.810 ms, Imbalance: 19.114 ms
Node 5, Pre/Post-Pipelining: 8.661 / 77.305 ms, Bubble: 133.103 ms, Compute: 495.082 ms, Comm: 78.797 ms, Imbalance: 31.558 ms
Node 6, Pre/Post-Pipelining: 8.339 / 77.364 ms, Bubble: 133.581 ms, Compute: 492.664 ms, Comm: 68.291 ms, Imbalance: 43.927 ms
Node 7, Pre/Post-Pipelining: 8.120 / 91.671 ms, Bubble: 120.268 ms, Compute: 518.566 ms, Comm: 55.182 ms, Imbalance: 29.333 ms
Node 0, Pre/Post-Pipelining: 8.592 / 77.519 ms, Bubble: 132.346 ms, Compute: 512.776 ms, Comm: 54.575 ms, Imbalance: 36.810 ms
Node 2, Pre/Post-Pipelining: 8.427 / 77.524 ms, Bubble: 131.679 ms, Compute: 496.505 ms, Comm: 81.011 ms, Imbalance: 28.594 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.592 ms
Cluster-Wide Average, Post-Pipelining Overhead: 77.519 ms
Cluster-Wide Average, Bubble: 132.346 ms
Cluster-Wide Average, Compute: 512.776 ms
Cluster-Wide Average, Communication: 54.575 ms
Cluster-Wide Average, Imbalance: 36.810 ms
Node 0, GPU memory consumption: 22.467 GB
Node 2, GPU memory consumption: 19.391 GB
Node 4, GPU memory consumption: 19.370 GB
Node 3, GPU memory consumption: 19.368 GB
Node 5, GPU memory consumption: 19.391 GB
Node 1, GPU memory consumption: 19.391 GB
Node 6, GPU memory consumption: 19.391 GB
Node 7, GPU memory consumption: 19.471 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 4.267925 s---------------
------------------------node id 1,  per-epoch time: 4.267923 s---------------
------------------------node id 2,  per-epoch time: 4.267925 s---------------
------------------------node id 3,  per-epoch time: 4.267923 s---------------
------------------------node id 4,  per-epoch time: 4.267920 s---------------
------------------------node id 5,  per-epoch time: 4.267921 s---------------
------------------------node id 6,  per-epoch time: 4.267921 s---------------
------------------------node id 7,  per-epoch time: 4.267921 s---------------
************ Profiling Results ************
	Bubble: 3604.578060 (ms) (87.62 percentage)
	Compute: 489.187293 (ms) (11.89 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 20.094756 (ms) (0.49 percentage)
	Layer-level communication (cluster-wide, per-epoch): 4.522 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.251 GB
	Total communication (cluster-wide, per-epoch): 4.773 GB
	Aggregated layer-level communication throughput: 526.593 Gbps
Highest valid_acc: 0.0763
Target test_acc: 0.0587
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
