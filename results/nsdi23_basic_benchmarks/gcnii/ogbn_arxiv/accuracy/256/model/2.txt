Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.042 seconds.
        It takes 0.045 seconds.
        It takes 0.049 seconds.
        It takes 0.055 seconds.
        It takes 0.053 seconds.
        It takes 0.052 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 128
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.717 Gbps (per GPU), 509.739 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.676 Gbps (per GPU), 509.409 Gbps (aggregated)
The layer-level communication performance: 63.676 Gbps (per GPU), 509.405 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.639 Gbps (per GPU), 509.111 Gbps (aggregated)
The layer-level communication performance: 63.635 Gbps (per GPU), 509.076 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.601 Gbps (per GPU), 508.808 Gbps (aggregated)
The layer-level communication performance: 63.594 Gbps (per GPU), 508.753 Gbps (aggregated)
The layer-level communication performance: 63.590 Gbps (per GPU), 508.716 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.439 Gbps (per GPU), 1331.515 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.423 Gbps (per GPU), 1331.383 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.439 Gbps (per GPU), 1331.515 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.425 Gbps (per GPU), 1331.403 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.436 Gbps (per GPU), 1331.485 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.443 Gbps (per GPU), 1331.542 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.438 Gbps (per GPU), 1331.505 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.436 Gbps (per GPU), 1331.486 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.090 Gbps (per GPU), 912.718 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.089 Gbps (per GPU), 912.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.089 Gbps (per GPU), 912.715 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.089 Gbps (per GPU), 912.708 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.086 Gbps (per GPU), 912.689 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.085 Gbps (per GPU), 912.679 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.084 Gbps (per GPU), 912.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.074 Gbps (per GPU), 912.595 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.232 Gbps (per GPU), 361.853 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.232 Gbps (per GPU), 361.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.231 Gbps (per GPU), 361.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.231 Gbps (per GPU), 361.851 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.231 Gbps (per GPU), 361.850 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.231 Gbps (per GPU), 361.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.231 Gbps (per GPU), 361.844 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.231 Gbps (per GPU), 361.846 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.44ms  0.96ms  1.17ms  2.64  5.55K  0.06M
 chk_1  0.45ms  1.00ms  1.20ms  2.68  5.75K  0.05M
 chk_2  0.43ms  0.96ms  1.16ms  2.71  5.20K  0.07M
 chk_3  0.44ms  0.99ms  1.20ms  2.72  5.57K  0.06M
 chk_4  0.42ms  0.96ms  1.15ms  2.73  5.05K  0.08M
 chk_5  0.41ms  0.98ms  1.17ms  2.85  4.73K  0.11M
 chk_6  0.42ms  0.95ms  1.14ms  2.70  4.98K  0.08M
 chk_7  0.42ms  0.97ms  1.16ms  2.75  5.01K  0.09M
 chk_8  0.45ms  0.99ms  1.20ms  2.69  5.73K  0.05M
 chk_9  0.40ms  0.90ms  1.09ms  2.70  4.54K  0.11M
chk_10  0.43ms  0.97ms  1.17ms  2.70  5.36K  0.07M
chk_11  0.43ms  0.98ms  1.18ms  2.72  5.39K  0.08M
chk_12  0.45ms  0.99ms  1.20ms  2.67  5.77K  0.05M
chk_13  0.44ms  0.97ms  1.18ms  2.71  5.43K  0.06M
chk_14  0.45ms  0.96ms  1.16ms  2.56  5.46K  0.06M
chk_15  0.45ms  0.99ms  1.21ms  2.67  5.88K  0.04M
chk_16  0.44ms  0.97ms  1.18ms  2.68  5.50K  0.06M
chk_17  0.42ms  0.98ms  1.17ms  2.82  4.86K  0.09M
chk_18  0.43ms  1.02ms  1.23ms  2.84  5.39K  0.07M
chk_19  0.43ms  0.97ms  1.16ms  2.72  5.20K  0.07M
chk_20  0.44ms  0.96ms  1.16ms  2.63  5.51K  0.06M
chk_21  0.45ms  0.97ms  1.18ms  2.61  5.81K  0.05M
chk_22  0.43ms  0.96ms  1.16ms  2.68  5.32K  0.07M
chk_23  0.43ms  1.00ms  1.20ms  2.76  5.39K  0.07M
chk_24  0.40ms  0.95ms  1.13ms  2.82  4.62K  0.11M
chk_25  0.42ms  0.98ms  1.17ms  2.77  5.04K  0.08M
chk_26  0.40ms  0.90ms  1.09ms  2.72  4.55K  0.11M
chk_27  0.43ms  0.95ms  1.14ms  2.66  5.30K  0.06M
chk_28  0.44ms  0.98ms  1.18ms  2.67  5.58K  0.06M
chk_29  0.42ms  0.99ms  1.18ms  2.82  4.98K  0.09M
chk_30  0.45ms  0.97ms  1.17ms  2.58  5.50K  0.07M
chk_31  0.43ms  0.96ms  1.17ms  2.71  5.38K  0.07M
   Avg  0.43  0.97  1.17
   Max  0.45  1.02  1.23
   Min  0.40  0.90  1.09
 Ratio  1.13  1.13  1.13
   Var  0.00  0.00  0.00
Profiling takes 1.095 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 510.313 ms
Partition 0 [0, 17) has cost: 510.313 ms
Partition 1 [17, 33) has cost: 496.498 ms
Partition 2 [33, 49) has cost: 496.498 ms
Partition 3 [49, 65) has cost: 496.498 ms
Partition 4 [65, 81) has cost: 496.498 ms
Partition 5 [81, 97) has cost: 496.498 ms
Partition 6 [97, 113) has cost: 496.498 ms
Partition 7 [113, 129) has cost: 502.858 ms
The optimal partitioning:
[0, 17)
[17, 33)
[33, 49)
[49, 65)
[65, 81)
[81, 97)
[97, 113)
[113, 129)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 223.125 ms
GPU 0, Compute+Comm Time: 181.195 ms, Bubble Time: 39.433 ms, Imbalance Overhead: 2.497 ms
GPU 1, Compute+Comm Time: 176.828 ms, Bubble Time: 39.410 ms, Imbalance Overhead: 6.888 ms
GPU 2, Compute+Comm Time: 176.828 ms, Bubble Time: 39.560 ms, Imbalance Overhead: 6.738 ms
GPU 3, Compute+Comm Time: 176.828 ms, Bubble Time: 39.568 ms, Imbalance Overhead: 6.730 ms
GPU 4, Compute+Comm Time: 176.828 ms, Bubble Time: 39.693 ms, Imbalance Overhead: 6.605 ms
GPU 5, Compute+Comm Time: 176.828 ms, Bubble Time: 39.660 ms, Imbalance Overhead: 6.638 ms
GPU 6, Compute+Comm Time: 176.828 ms, Bubble Time: 39.759 ms, Imbalance Overhead: 6.539 ms
GPU 7, Compute+Comm Time: 178.238 ms, Bubble Time: 39.889 ms, Imbalance Overhead: 4.999 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 459.720 ms
GPU 0, Compute+Comm Time: 368.165 ms, Bubble Time: 82.674 ms, Imbalance Overhead: 8.882 ms
GPU 1, Compute+Comm Time: 363.215 ms, Bubble Time: 82.384 ms, Imbalance Overhead: 14.122 ms
GPU 2, Compute+Comm Time: 363.215 ms, Bubble Time: 82.164 ms, Imbalance Overhead: 14.341 ms
GPU 3, Compute+Comm Time: 363.215 ms, Bubble Time: 82.009 ms, Imbalance Overhead: 14.497 ms
GPU 4, Compute+Comm Time: 363.215 ms, Bubble Time: 81.687 ms, Imbalance Overhead: 14.818 ms
GPU 5, Compute+Comm Time: 363.215 ms, Bubble Time: 81.409 ms, Imbalance Overhead: 15.096 ms
GPU 6, Compute+Comm Time: 363.215 ms, Bubble Time: 80.825 ms, Imbalance Overhead: 15.681 ms
GPU 7, Compute+Comm Time: 372.662 ms, Bubble Time: 80.720 ms, Imbalance Overhead: 6.339 ms
The estimated cost of the whole pipeline: 716.988 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 1006.811 ms
Partition 0 [0, 33) has cost: 1006.811 ms
Partition 1 [33, 65) has cost: 992.996 ms
Partition 2 [65, 97) has cost: 992.996 ms
Partition 3 [97, 129) has cost: 999.356 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 301.150 ms
GPU 0, Compute+Comm Time: 250.382 ms, Bubble Time: 47.031 ms, Imbalance Overhead: 3.737 ms
GPU 1, Compute+Comm Time: 248.171 ms, Bubble Time: 47.218 ms, Imbalance Overhead: 5.761 ms
GPU 2, Compute+Comm Time: 248.171 ms, Bubble Time: 47.222 ms, Imbalance Overhead: 5.757 ms
GPU 3, Compute+Comm Time: 248.873 ms, Bubble Time: 47.647 ms, Imbalance Overhead: 4.630 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 530.810 ms
GPU 0, Compute+Comm Time: 439.981 ms, Bubble Time: 84.356 ms, Imbalance Overhead: 6.473 ms
GPU 1, Compute+Comm Time: 437.470 ms, Bubble Time: 83.570 ms, Imbalance Overhead: 9.770 ms
GPU 2, Compute+Comm Time: 437.470 ms, Bubble Time: 83.088 ms, Imbalance Overhead: 10.252 ms
GPU 3, Compute+Comm Time: 442.287 ms, Bubble Time: 82.143 ms, Imbalance Overhead: 6.380 ms
    The estimated cost with 2 DP ways is 873.558 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1999.807 ms
Partition 0 [0, 65) has cost: 1999.807 ms
Partition 1 [65, 129) has cost: 1992.352 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 561.788 ms
GPU 0, Compute+Comm Time: 489.201 ms, Bubble Time: 61.358 ms, Imbalance Overhead: 11.230 ms
GPU 1, Compute+Comm Time: 488.439 ms, Bubble Time: 62.353 ms, Imbalance Overhead: 10.996 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 778.698 ms
GPU 0, Compute+Comm Time: 680.227 ms, Bubble Time: 86.873 ms, Imbalance Overhead: 11.599 ms
GPU 1, Compute+Comm Time: 681.413 ms, Bubble Time: 84.650 ms, Imbalance Overhead: 12.635 ms
    The estimated cost with 4 DP ways is 1407.511 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 3992.159 ms
Partition 0 [0, 129) has cost: 3992.159 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 2029.309 ms
GPU 0, Compute+Comm Time: 2029.309 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 2222.881 ms
GPU 0, Compute+Comm Time: 2222.881 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 4464.799 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 118)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [118, 230)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [230, 342)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [342, 454)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [454, 566)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [566, 678)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [678, 790)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [790, 905)
*** Node 7, constructing the helper classes...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[230, 342)...
+++++++++ Node 0 initializing the weights for op[0, 118)...
+++++++++ Node 1 initializing the weights for op[118, 230)...
+++++++++ Node 3 initializing the weights for op[342, 454)...
+++++++++ Node 5 initializing the weights for op[566, 678)...
+++++++++ Node 4 initializing the weights for op[454, 566)...
+++++++++ Node 7 initializing the weights for op[790, 905)...
+++++++++ Node 6 initializing the weights for op[678, 790)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 21.5982	TrainAcc 0.1848	ValidAcc 0.2347	TestAcc 0.2188	BestValid 0.2347
	Epoch 50:	Loss 21.5291	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.2347
	Epoch 75:	Loss 19.2087	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.2347
	Epoch 100:	Loss 18.4556	TrainAcc 0.1196	ValidAcc 0.0498	TestAcc 0.0370	BestValid 0.2347
Node 1, Pre/Post-Pipelining: 8.625 / 76.556 ms, Bubble: 130.286 ms, Compute: 491.088 ms, Comm: 71.458 ms, Imbalance: 44.483 ms
Node 2, Pre/Post-Pipelining: 8.408 / 76.808 ms, Bubble: 130.322 ms, Compute: 497.375 ms, Comm: 81.285 ms, Imbalance: 29.051 ms
Node 4, Pre/Post-Pipelining: 8.092 / 76.735 ms, Bubble: 129.894 ms, Compute: 497.112 ms, Comm: 90.756 ms, Imbalance: 21.130 ms
Node 5, Pre/Post-Pipelining: 8.822 / 76.435 ms, Bubble: 132.122 ms, Compute: 489.754 ms, Comm: 79.378 ms, Imbalance: 37.786 ms
Node 6, Pre/Post-Pipelining: 8.023 / 76.870 ms, Bubble: 132.154 ms, Compute: 500.999 ms, Comm: 68.340 ms, Imbalance: 36.855 ms
Node 3, Pre/Post-Pipelining: 8.559 / 76.935 ms, Bubble: 128.451 ms, Compute: 499.400 ms, Comm: 91.369 ms, Imbalance: 18.600 ms
Node 7, Pre/Post-Pipelining: 8.233 / 90.683 ms, Bubble: 120.400 ms, Compute: 510.163 ms, Comm: 55.277 ms, Imbalance: 38.467 ms
Node 0, Pre/Post-Pipelining: 8.282 / 76.497 ms, Bubble: 131.862 ms, Compute: 504.511 ms, Comm: 54.671 ms, Imbalance: 45.951 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.282 ms
Cluster-Wide Average, Post-Pipelining Overhead: 76.497 ms
Cluster-Wide Average, Bubble: 131.862 ms
Cluster-Wide Average, Compute: 504.511 ms
Cluster-Wide Average, Communication: 54.671 ms
Cluster-Wide Average, Imbalance: 45.951 ms
Node 0, GPU memory consumption: 22.467 GB
Node 3, GPU memory consumption: 19.368 GB
Node 5, GPU memory consumption: 19.391 GB
Node 1, GPU memory consumption: 19.391 GB
Node 4, GPU memory consumption: 19.370 GB
Node 2, GPU memory consumption: 19.391 GB
Node 7, GPU memory consumption: 19.471 GB
Node 6, GPU memory consumption: 19.391 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 4.258834 s---------------
------------------------node id 1,  per-epoch time: 4.258831 s---------------
------------------------node id 4,  per-epoch time: 4.258832 s---------------
------------------------node id 2,  per-epoch time: 4.258828 s---------------
------------------------node id 3,  per-epoch time: 4.258829 s---------------
------------------------node id 5,  per-epoch time: 4.258833 s---------------
------------------------node id 6,  per-epoch time: 4.258831 s---------------
------------------------node id 7,  per-epoch time: 4.258834 s---------------
************ Profiling Results ************
	Bubble: 3596.078569 (ms) (87.62 percentage)
	Compute: 488.291804 (ms) (11.90 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 20.038166 (ms) (0.49 percentage)
	Layer-level communication (cluster-wide, per-epoch): 4.522 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.251 GB
	Total communication (cluster-wide, per-epoch): 4.773 GB
	Aggregated layer-level communication throughput: 524.435 Gbps
Highest valid_acc: 0.2347
Target test_acc: 0.2188
Epoch to reach the target acc: 24
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
