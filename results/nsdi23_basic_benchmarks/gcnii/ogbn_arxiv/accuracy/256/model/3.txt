Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.066 seconds.
Building the CSC structure...
        It takes 0.066 seconds.
Building the CSC structure...
        It takes 0.041 seconds.
        It takes 0.051 seconds.
        It takes 0.049 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
        It takes 0.055 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
        It takes 0.055 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 128
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.023 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.024 seconds.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.557 Gbps (per GPU), 508.459 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.520 Gbps (per GPU), 508.159 Gbps (aggregated)
The layer-level communication performance: 63.518 Gbps (per GPU), 508.146 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.486 Gbps (per GPU), 507.887 Gbps (aggregated)
The layer-level communication performance: 63.480 Gbps (per GPU), 507.844 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.452 Gbps (per GPU), 507.613 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.445 Gbps (per GPU), 507.560 Gbps (aggregated)
The layer-level communication performance: 63.441 Gbps (per GPU), 507.524 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.880 Gbps (per GPU), 1311.037 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.879 Gbps (per GPU), 1311.033 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.884 Gbps (per GPU), 1311.072 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.873 Gbps (per GPU), 1310.982 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.876 Gbps (per GPU), 1311.008 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.877 Gbps (per GPU), 1311.015 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.875 Gbps (per GPU), 1310.996 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.878 Gbps (per GPU), 1311.021 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.267 Gbps (per GPU), 914.137 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.266 Gbps (per GPU), 914.127 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.267 Gbps (per GPU), 914.137 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.266 Gbps (per GPU), 914.126 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.264 Gbps (per GPU), 914.111 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.263 Gbps (per GPU), 914.103 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.265 Gbps (per GPU), 914.122 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.263 Gbps (per GPU), 914.106 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.222 Gbps (per GPU), 361.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.222 Gbps (per GPU), 361.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.222 Gbps (per GPU), 361.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.222 Gbps (per GPU), 361.773 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.221 Gbps (per GPU), 361.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.221 Gbps (per GPU), 361.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.221 Gbps (per GPU), 361.770 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.222 Gbps (per GPU), 361.772 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.45ms  0.98ms  1.19ms  2.63  5.55K  0.06M
 chk_1  0.46ms  1.01ms  1.22ms  2.67  5.75K  0.05M
 chk_2  0.44ms  0.98ms  1.17ms  2.68  5.20K  0.07M
 chk_3  0.45ms  1.01ms  1.21ms  2.70  5.57K  0.06M
 chk_4  0.43ms  0.98ms  1.17ms  2.70  5.05K  0.08M
 chk_5  0.42ms  0.99ms  1.18ms  2.84  4.73K  0.11M
 chk_6  0.43ms  0.96ms  1.16ms  2.71  4.98K  0.08M
 chk_7  0.43ms  0.99ms  1.18ms  2.75  5.01K  0.09M
 chk_8  0.45ms  1.01ms  1.22ms  2.68  5.73K  0.05M
 chk_9  0.41ms  0.91ms  1.10ms  2.68  4.54K  0.11M
chk_10  0.44ms  0.98ms  1.19ms  2.69  5.36K  0.07M
chk_11  0.44ms  1.00ms  1.20ms  2.71  5.39K  0.08M
chk_12  0.46ms  1.01ms  1.22ms  2.66  5.77K  0.05M
chk_13  0.44ms  1.00ms  1.20ms  2.71  5.43K  0.06M
chk_14  0.44ms  0.98ms  1.18ms  2.67  5.46K  0.06M
chk_15  0.46ms  1.02ms  1.22ms  2.66  5.88K  0.04M
chk_16  0.45ms  0.99ms  1.20ms  2.69  5.50K  0.06M
chk_17  0.43ms  0.99ms  1.19ms  2.79  4.86K  0.09M
chk_18  0.44ms  1.04ms  1.24ms  2.80  5.39K  0.07M
chk_19  0.44ms  0.98ms  1.18ms  2.71  5.20K  0.07M
chk_20  0.45ms  0.98ms  1.18ms  2.63  5.51K  0.06M
chk_21  0.46ms  0.99ms  1.20ms  2.62  5.81K  0.05M
chk_22  0.44ms  0.97ms  1.18ms  2.67  5.32K  0.07M
chk_23  0.44ms  1.02ms  1.22ms  2.76  5.39K  0.07M
chk_24  0.41ms  0.96ms  1.15ms  2.79  4.62K  0.11M
chk_25  0.43ms  0.99ms  1.19ms  2.75  5.04K  0.08M
chk_26  0.41ms  0.92ms  1.11ms  2.68  4.55K  0.11M
chk_27  0.44ms  0.95ms  1.16ms  2.64  5.30K  0.06M
chk_28  0.45ms  0.99ms  1.20ms  2.67  5.58K  0.06M
chk_29  0.43ms  1.00ms  1.20ms  2.79  4.98K  0.09M
chk_30  0.45ms  0.99ms  1.20ms  2.66  5.50K  0.07M
chk_31  0.44ms  0.98ms  1.19ms  2.68  5.38K  0.07M
   Avg  0.44  0.99  1.19
   Max  0.46  1.04  1.24
   Min  0.41  0.91  1.10
 Ratio  1.12  1.13  1.13
   Var  0.00  0.00  0.00
Profiling takes 1.118 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 518.944 ms
Partition 0 [0, 17) has cost: 518.944 ms
Partition 1 [17, 33) has cost: 504.879 ms
Partition 2 [33, 49) has cost: 504.879 ms
Partition 3 [49, 65) has cost: 504.879 ms
Partition 4 [65, 81) has cost: 504.879 ms
Partition 5 [81, 97) has cost: 504.879 ms
Partition 6 [97, 113) has cost: 504.879 ms
Partition 7 [113, 129) has cost: 511.323 ms
The optimal partitioning:
[0, 17)
[17, 33)
[33, 49)
[49, 65)
[65, 81)
[81, 97)
[97, 113)
[113, 129)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 226.862 ms
GPU 0, Compute+Comm Time: 184.380 ms, Bubble Time: 40.003 ms, Imbalance Overhead: 2.479 ms
GPU 1, Compute+Comm Time: 179.911 ms, Bubble Time: 39.995 ms, Imbalance Overhead: 6.956 ms
GPU 2, Compute+Comm Time: 179.911 ms, Bubble Time: 40.184 ms, Imbalance Overhead: 6.767 ms
GPU 3, Compute+Comm Time: 179.911 ms, Bubble Time: 40.228 ms, Imbalance Overhead: 6.722 ms
GPU 4, Compute+Comm Time: 179.911 ms, Bubble Time: 40.415 ms, Imbalance Overhead: 6.536 ms
GPU 5, Compute+Comm Time: 179.911 ms, Bubble Time: 40.417 ms, Imbalance Overhead: 6.534 ms
GPU 6, Compute+Comm Time: 179.911 ms, Bubble Time: 40.508 ms, Imbalance Overhead: 6.443 ms
GPU 7, Compute+Comm Time: 181.320 ms, Bubble Time: 40.627 ms, Imbalance Overhead: 4.916 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 466.563 ms
GPU 0, Compute+Comm Time: 373.657 ms, Bubble Time: 83.768 ms, Imbalance Overhead: 9.138 ms
GPU 1, Compute+Comm Time: 368.621 ms, Bubble Time: 83.494 ms, Imbalance Overhead: 14.448 ms
GPU 2, Compute+Comm Time: 368.621 ms, Bubble Time: 83.329 ms, Imbalance Overhead: 14.613 ms
GPU 3, Compute+Comm Time: 368.621 ms, Bubble Time: 83.182 ms, Imbalance Overhead: 14.760 ms
GPU 4, Compute+Comm Time: 368.621 ms, Bubble Time: 82.845 ms, Imbalance Overhead: 15.097 ms
GPU 5, Compute+Comm Time: 368.621 ms, Bubble Time: 82.577 ms, Imbalance Overhead: 15.365 ms
GPU 6, Compute+Comm Time: 368.621 ms, Bubble Time: 81.997 ms, Imbalance Overhead: 15.945 ms
GPU 7, Compute+Comm Time: 378.218 ms, Bubble Time: 81.860 ms, Imbalance Overhead: 6.486 ms
The estimated cost of the whole pipeline: 728.096 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 1023.823 ms
Partition 0 [0, 33) has cost: 1023.823 ms
Partition 1 [33, 65) has cost: 1009.758 ms
Partition 2 [65, 97) has cost: 1009.758 ms
Partition 3 [97, 129) has cost: 1016.201 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 306.156 ms
GPU 0, Compute+Comm Time: 254.552 ms, Bubble Time: 47.740 ms, Imbalance Overhead: 3.864 ms
GPU 1, Compute+Comm Time: 252.291 ms, Bubble Time: 48.004 ms, Imbalance Overhead: 5.860 ms
GPU 2, Compute+Comm Time: 252.291 ms, Bubble Time: 48.082 ms, Imbalance Overhead: 5.782 ms
GPU 3, Compute+Comm Time: 253.000 ms, Bubble Time: 48.487 ms, Imbalance Overhead: 4.668 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 539.113 ms
GPU 0, Compute+Comm Time: 446.564 ms, Bubble Time: 85.470 ms, Imbalance Overhead: 7.080 ms
GPU 1, Compute+Comm Time: 444.010 ms, Bubble Time: 84.787 ms, Imbalance Overhead: 10.317 ms
GPU 2, Compute+Comm Time: 444.010 ms, Bubble Time: 84.315 ms, Imbalance Overhead: 10.789 ms
GPU 3, Compute+Comm Time: 448.893 ms, Bubble Time: 83.370 ms, Imbalance Overhead: 6.850 ms
    The estimated cost with 2 DP ways is 887.533 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 2033.581 ms
Partition 0 [0, 65) has cost: 2033.581 ms
Partition 1 [65, 129) has cost: 2025.959 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 564.986 ms
GPU 0, Compute+Comm Time: 492.091 ms, Bubble Time: 61.570 ms, Imbalance Overhead: 11.325 ms
GPU 1, Compute+Comm Time: 491.303 ms, Bubble Time: 62.793 ms, Imbalance Overhead: 10.890 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 784.695 ms
GPU 0, Compute+Comm Time: 685.160 ms, Bubble Time: 87.414 ms, Imbalance Overhead: 12.121 ms
GPU 1, Compute+Comm Time: 686.355 ms, Bubble Time: 85.208 ms, Imbalance Overhead: 13.133 ms
    The estimated cost with 4 DP ways is 1417.165 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 4059.540 ms
Partition 0 [0, 129) has cost: 4059.540 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 2032.717 ms
GPU 0, Compute+Comm Time: 2032.717 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 2228.842 ms
GPU 0, Compute+Comm Time: 2228.842 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 4474.637 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 118)
*** Node 0, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [454, 566)
*** Node 4, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [118, 230)
*** Node 1, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [566, 678)
*** Node 5, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [230, 342)
*** Node 2, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [678, 790)
*** Node 6, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [342, 454)
*** Node 3, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [790, 905)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 118)...
+++++++++ Node 1 initializing the weights for op[118, 230)...
+++++++++ Node 5 initializing the weights for op[566, 678)...
+++++++++ Node 2 initializing the weights for op[230, 342)...
+++++++++ Node 3 initializing the weights for op[342, 454)...
+++++++++ Node 7 initializing the weights for op[790, 905)...
+++++++++ Node 4 initializing the weights for op[454, 566)...
+++++++++ Node 6 initializing the weights for op[678, 790)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 23.9077	TrainAcc 0.0727	ValidAcc 0.0403	TestAcc 0.0338	BestValid 0.0403
	Epoch 50:	Loss 23.4101	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 75:	Loss 22.3838	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 100:	Loss 22.4004	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
Node 4, Pre/Post-Pipelining: 8.515 / 80.108 ms, Bubble: 131.501 ms, Compute: 490.139 ms, Comm: 90.029 ms, Imbalance: 24.897 ms
Node 5, Pre/Post-Pipelining: 8.323 / 80.087 ms, Bubble: 133.076 ms, Compute: 490.083 ms, Comm: 78.964 ms, Imbalance: 34.198 ms
Node 6, Pre/Post-Pipelining: 8.019 / 80.471 ms, Bubble: 133.044 ms, Compute: 500.049 ms, Comm: 68.455 ms, Imbalance: 34.050 ms
Node 7, Pre/Post-Pipelining: 7.957 / 94.674 ms, Bubble: 120.030 ms, Compute: 517.920 ms, Comm: 55.374 ms, Imbalance: 27.771 ms
Node 2, Pre/Post-Pipelining: 8.235 / 80.216 ms, Bubble: 132.002 ms, Compute: 491.234 ms, Comm: 81.327 ms, Imbalance: 31.367 ms
Node 3, Pre/Post-Pipelining: 8.290 / 80.324 ms, Bubble: 130.163 ms, Compute: 493.632 ms, Comm: 90.584 ms, Imbalance: 21.060 ms
Node 1, Pre/Post-Pipelining: 8.512 / 80.306 ms, Bubble: 130.962 ms, Compute: 496.915 ms, Comm: 71.526 ms, Imbalance: 35.310 ms
Node 0, Pre/Post-Pipelining: 8.651 / 80.360 ms, Bubble: 131.970 ms, Compute: 513.225 ms, Comm: 55.111 ms, Imbalance: 33.969 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.651 ms
Cluster-Wide Average, Post-Pipelining Overhead: 80.360 ms
Cluster-Wide Average, Bubble: 131.970 ms
Cluster-Wide Average, Compute: 513.225 ms
Cluster-Wide Average, Communication: 55.111 ms
Cluster-Wide Average, Imbalance: 33.969 ms
Node 0, GPU memory consumption: 22.467 GB
Node 4, GPU memory consumption: 19.370 GB
Node 3, GPU memory consumption: 19.368 GB
Node 6, GPU memory consumption: 19.391 GB
Node 2, GPU memory consumption: 19.391 GB
Node 7, GPU memory consumption: 19.471 GB
Node 5, GPU memory consumption: 19.391 GB
Node 1, GPU memory consumption: 19.391 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 4.371193 s---------------
------------------------node id 4,  per-epoch time: 4.371185 s---------------
------------------------node id 1,  per-epoch time: 4.371189 s---------------
------------------------node id 5,  per-epoch time: 4.371186 s---------------
------------------------node id 2,  per-epoch time: 4.371191 s---------------
------------------------node id 6,  per-epoch time: 4.371183 s---------------
------------------------node id 3,  per-epoch time: 4.371188 s---------------
------------------------node id 7,  per-epoch time: 4.371185 s---------------
************ Profiling Results ************
	Bubble: 3702.926370 (ms) (87.92 percentage)
	Compute: 488.841083 (ms) (11.61 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 20.039074 (ms) (0.48 percentage)
	Layer-level communication (cluster-wide, per-epoch): 4.522 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.251 GB
	Total communication (cluster-wide, per-epoch): 4.773 GB
	Aggregated layer-level communication throughput: 525.468 Gbps
Highest valid_acc: 0.0763
Target test_acc: 0.0586
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
