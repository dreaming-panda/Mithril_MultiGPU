Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.042 seconds.
        It takes 0.041 seconds.
Building the Feature Vector...
        It takes 0.055 seconds.
        It takes 0.051 seconds.
        It takes 0.050 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
        It takes 0.055 seconds.
        It takes 0.058 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 128
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.842 Gbps (per GPU), 510.736 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.799 Gbps (per GPU), 510.388 Gbps (aggregated)
The layer-level communication performance: 63.797 Gbps (per GPU), 510.378 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.762 Gbps (per GPU), 510.094 Gbps (aggregated)
The layer-level communication performance: 63.758 Gbps (per GPU), 510.063 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.727 Gbps (per GPU), 509.819 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.720 Gbps (per GPU), 509.761 Gbps (aggregated)
The layer-level communication performance: 63.715 Gbps (per GPU), 509.720 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.371 Gbps (per GPU), 1330.971 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.367 Gbps (per GPU), 1330.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.370 Gbps (per GPU), 1330.957 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.372 Gbps (per GPU), 1330.974 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.370 Gbps (per GPU), 1330.957 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.375 Gbps (per GPU), 1331.003 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.366 Gbps (per GPU), 1330.931 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.373 Gbps (per GPU), 1330.984 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.434 Gbps (per GPU), 915.471 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.433 Gbps (per GPU), 915.464 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.431 Gbps (per GPU), 915.446 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.421 Gbps (per GPU), 915.371 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.425 Gbps (per GPU), 915.399 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.421 Gbps (per GPU), 915.371 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.426 Gbps (per GPU), 915.405 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.419 Gbps (per GPU), 915.353 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.974 Gbps (per GPU), 367.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.974 Gbps (per GPU), 367.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.974 Gbps (per GPU), 367.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.973 Gbps (per GPU), 367.788 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.973 Gbps (per GPU), 367.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.973 Gbps (per GPU), 367.781 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.973 Gbps (per GPU), 367.786 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.973 Gbps (per GPU), 367.787 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.14ms  3.16ms  3.61ms  3.15 21.17K  0.46M
 chk_1  1.15ms  3.38ms  3.82ms  3.34 21.17K  0.55M
 chk_2  1.14ms  3.13ms  3.57ms  3.12 21.17K  0.39M
 chk_3  1.15ms  3.03ms  3.47ms  3.02 21.17K  0.24M
 chk_4  1.15ms  2.91ms  3.35ms  2.92 21.17K  0.17M
 chk_5  1.15ms  2.92ms  3.35ms  2.92 21.17K  0.22M
 chk_6  1.15ms  2.92ms  3.37ms  2.93 21.17K  0.16M
 chk_7  1.15ms  2.88ms  3.32ms  2.89 21.17K  0.12M
   Avg  1.15  3.04  3.48
   Max  1.15  3.38  3.82
   Min  1.14  2.88  3.32
 Ratio  1.00  1.17  1.15
   Var  0.00  0.03  0.03
Profiling takes 0.813 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 398.564 ms
Partition 0 [0, 17) has cost: 398.564 ms
Partition 1 [17, 33) has cost: 389.392 ms
Partition 2 [33, 49) has cost: 389.392 ms
Partition 3 [49, 65) has cost: 389.392 ms
Partition 4 [65, 81) has cost: 389.392 ms
Partition 5 [81, 97) has cost: 389.392 ms
Partition 6 [97, 113) has cost: 389.392 ms
Partition 7 [113, 129) has cost: 392.902 ms
The optimal partitioning:
[0, 17)
[17, 33)
[33, 49)
[49, 65)
[65, 81)
[81, 97)
[97, 113)
[113, 129)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 274.935 ms
GPU 0, Compute+Comm Time: 141.694 ms, Bubble Time: 121.630 ms, Imbalance Overhead: 11.610 ms
GPU 1, Compute+Comm Time: 139.119 ms, Bubble Time: 120.739 ms, Imbalance Overhead: 15.077 ms
GPU 2, Compute+Comm Time: 139.119 ms, Bubble Time: 122.317 ms, Imbalance Overhead: 13.498 ms
GPU 3, Compute+Comm Time: 139.119 ms, Bubble Time: 124.188 ms, Imbalance Overhead: 11.628 ms
GPU 4, Compute+Comm Time: 139.119 ms, Bubble Time: 126.660 ms, Imbalance Overhead: 9.156 ms
GPU 5, Compute+Comm Time: 139.119 ms, Bubble Time: 129.060 ms, Imbalance Overhead: 6.756 ms
GPU 6, Compute+Comm Time: 139.119 ms, Bubble Time: 131.466 ms, Imbalance Overhead: 4.349 ms
GPU 7, Compute+Comm Time: 139.810 ms, Bubble Time: 134.081 ms, Imbalance Overhead: 1.043 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 580.132 ms
GPU 0, Compute+Comm Time: 296.551 ms, Bubble Time: 281.582 ms, Imbalance Overhead: 2.000 ms
GPU 1, Compute+Comm Time: 293.732 ms, Bubble Time: 276.649 ms, Imbalance Overhead: 9.751 ms
GPU 2, Compute+Comm Time: 293.732 ms, Bubble Time: 272.067 ms, Imbalance Overhead: 14.334 ms
GPU 3, Compute+Comm Time: 293.732 ms, Bubble Time: 267.500 ms, Imbalance Overhead: 18.901 ms
GPU 4, Compute+Comm Time: 293.732 ms, Bubble Time: 262.817 ms, Imbalance Overhead: 23.583 ms
GPU 5, Compute+Comm Time: 293.732 ms, Bubble Time: 259.437 ms, Imbalance Overhead: 26.964 ms
GPU 6, Compute+Comm Time: 293.732 ms, Bubble Time: 256.333 ms, Imbalance Overhead: 30.067 ms
GPU 7, Compute+Comm Time: 300.329 ms, Bubble Time: 258.223 ms, Imbalance Overhead: 21.580 ms
The estimated cost of the whole pipeline: 897.821 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 787.956 ms
Partition 0 [0, 33) has cost: 787.956 ms
Partition 1 [33, 65) has cost: 778.785 ms
Partition 2 [65, 97) has cost: 778.785 ms
Partition 3 [97, 129) has cost: 782.294 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 373.891 ms
GPU 0, Compute+Comm Time: 209.145 ms, Bubble Time: 152.989 ms, Imbalance Overhead: 11.758 ms
GPU 1, Compute+Comm Time: 207.856 ms, Bubble Time: 155.909 ms, Imbalance Overhead: 10.127 ms
GPU 2, Compute+Comm Time: 207.856 ms, Bubble Time: 160.791 ms, Imbalance Overhead: 5.245 ms
GPU 3, Compute+Comm Time: 208.200 ms, Bubble Time: 165.691 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 658.593 ms
GPU 0, Compute+Comm Time: 365.773 ms, Bubble Time: 292.820 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 364.367 ms, Bubble Time: 283.296 ms, Imbalance Overhead: 10.930 ms
GPU 2, Compute+Comm Time: 364.367 ms, Bubble Time: 273.818 ms, Imbalance Overhead: 20.408 ms
GPU 3, Compute+Comm Time: 367.668 ms, Bubble Time: 268.087 ms, Imbalance Overhead: 22.838 ms
    The estimated cost with 2 DP ways is 1084.109 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1566.741 ms
Partition 0 [0, 65) has cost: 1566.741 ms
Partition 1 [65, 129) has cost: 1561.079 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 657.669 ms
GPU 0, Compute+Comm Time: 435.449 ms, Bubble Time: 212.525 ms, Imbalance Overhead: 9.695 ms
GPU 1, Compute+Comm Time: 434.980 ms, Bubble Time: 222.689 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 902.246 ms
GPU 0, Compute+Comm Time: 594.637 ms, Bubble Time: 307.609 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 595.576 ms, Bubble Time: 287.498 ms, Imbalance Overhead: 19.171 ms
    The estimated cost with 4 DP ways is 1637.911 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 3127.820 ms
Partition 0 [0, 129) has cost: 3127.820 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 1822.277 ms
GPU 0, Compute+Comm Time: 1822.277 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 1991.881 ms
GPU 0, Compute+Comm Time: 1991.881 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 4004.866 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 905)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 905)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 905)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 905)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 905)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 905)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 905)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 905)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 27436 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Aborted (signal 6)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
