Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.042 seconds.
        It takes 0.041 seconds.
        It takes 0.047 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
        It takes 0.055 seconds.
        It takes 0.052 seconds.
        It takes 0.054 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.024 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 128
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.026 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.793 Gbps (per GPU), 510.347 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.751 Gbps (per GPU), 510.005 Gbps (aggregated)
The layer-level communication performance: 63.748 Gbps (per GPU), 509.984 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.716 Gbps (per GPU), 509.728 Gbps (aggregated)
The layer-level communication performance: 63.711 Gbps (per GPU), 509.687 Gbps (aggregated)
The layer-level communication performance: 63.684 Gbps (per GPU), 509.468 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.676 Gbps (per GPU), 509.410 Gbps (aggregated)
The layer-level communication performance: 63.672 Gbps (per GPU), 509.373 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.262 Gbps (per GPU), 1330.097 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.263 Gbps (per GPU), 1330.106 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.261 Gbps (per GPU), 1330.087 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.258 Gbps (per GPU), 1330.067 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.258 Gbps (per GPU), 1330.064 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.253 Gbps (per GPU), 1330.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.243 Gbps (per GPU), 1329.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.264 Gbps (per GPU), 1330.110 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.981 Gbps (per GPU), 911.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.979 Gbps (per GPU), 911.834 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.981 Gbps (per GPU), 911.851 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.976 Gbps (per GPU), 911.808 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.974 Gbps (per GPU), 911.792 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.974 Gbps (per GPU), 911.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.972 Gbps (per GPU), 911.775 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.974 Gbps (per GPU), 911.789 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.390 Gbps (per GPU), 363.118 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.390 Gbps (per GPU), 363.117 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.390 Gbps (per GPU), 363.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.390 Gbps (per GPU), 363.116 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.389 Gbps (per GPU), 363.116 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.389 Gbps (per GPU), 363.113 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.389 Gbps (per GPU), 363.116 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.389 Gbps (per GPU), 363.116 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.14ms  3.16ms  3.60ms  3.14 21.17K  0.46M
 chk_1  1.14ms  3.38ms  3.82ms  3.34 21.17K  0.55M
 chk_2  1.14ms  3.13ms  3.57ms  3.12 21.17K  0.39M
 chk_3  1.14ms  3.03ms  3.47ms  3.03 21.17K  0.24M
 chk_4  1.15ms  2.90ms  3.34ms  2.92 21.17K  0.17M
 chk_5  1.15ms  2.91ms  3.35ms  2.93 21.17K  0.22M
 chk_6  1.15ms  2.93ms  3.37ms  2.93 21.17K  0.16M
 chk_7  1.15ms  2.88ms  3.32ms  2.90 21.17K  0.12M
   Avg  1.15  3.04  3.48
   Max  1.15  3.38  3.82
   Min  1.14  2.88  3.32
 Ratio  1.00  1.17  1.15
   Var  0.00  0.03  0.03
Profiling takes 0.806 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 398.267 ms
Partition 0 [0, 17) has cost: 398.267 ms
Partition 1 [17, 33) has cost: 389.103 ms
Partition 2 [33, 49) has cost: 389.103 ms
Partition 3 [49, 65) has cost: 389.103 ms
Partition 4 [65, 81) has cost: 389.103 ms
Partition 5 [81, 97) has cost: 389.103 ms
Partition 6 [97, 113) has cost: 389.103 ms
Partition 7 [113, 129) has cost: 392.629 ms
The optimal partitioning:
[0, 17)
[17, 33)
[33, 49)
[49, 65)
[65, 81)
[81, 97)
[97, 113)
[113, 129)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 274.766 ms
GPU 0, Compute+Comm Time: 141.613 ms, Bubble Time: 121.616 ms, Imbalance Overhead: 11.537 ms
GPU 1, Compute+Comm Time: 139.035 ms, Bubble Time: 120.672 ms, Imbalance Overhead: 15.060 ms
GPU 2, Compute+Comm Time: 139.035 ms, Bubble Time: 122.255 ms, Imbalance Overhead: 13.477 ms
GPU 3, Compute+Comm Time: 139.035 ms, Bubble Time: 124.066 ms, Imbalance Overhead: 11.666 ms
GPU 4, Compute+Comm Time: 139.035 ms, Bubble Time: 126.524 ms, Imbalance Overhead: 9.208 ms
GPU 5, Compute+Comm Time: 139.035 ms, Bubble Time: 128.956 ms, Imbalance Overhead: 6.776 ms
GPU 6, Compute+Comm Time: 139.035 ms, Bubble Time: 131.328 ms, Imbalance Overhead: 4.404 ms
GPU 7, Compute+Comm Time: 139.734 ms, Bubble Time: 133.940 ms, Imbalance Overhead: 1.093 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 579.622 ms
GPU 0, Compute+Comm Time: 296.387 ms, Bubble Time: 281.295 ms, Imbalance Overhead: 1.939 ms
GPU 1, Compute+Comm Time: 293.561 ms, Bubble Time: 276.428 ms, Imbalance Overhead: 9.633 ms
GPU 2, Compute+Comm Time: 293.561 ms, Bubble Time: 272.000 ms, Imbalance Overhead: 14.060 ms
GPU 3, Compute+Comm Time: 293.561 ms, Bubble Time: 267.389 ms, Imbalance Overhead: 18.671 ms
GPU 4, Compute+Comm Time: 293.561 ms, Bubble Time: 262.651 ms, Imbalance Overhead: 23.409 ms
GPU 5, Compute+Comm Time: 293.561 ms, Bubble Time: 259.337 ms, Imbalance Overhead: 26.723 ms
GPU 6, Compute+Comm Time: 293.561 ms, Bubble Time: 256.233 ms, Imbalance Overhead: 29.828 ms
GPU 7, Compute+Comm Time: 300.146 ms, Bubble Time: 258.050 ms, Imbalance Overhead: 21.425 ms
The estimated cost of the whole pipeline: 897.107 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 787.371 ms
Partition 0 [0, 33) has cost: 787.371 ms
Partition 1 [33, 65) has cost: 778.207 ms
Partition 2 [65, 97) has cost: 778.207 ms
Partition 3 [97, 129) has cost: 781.733 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 373.812 ms
GPU 0, Compute+Comm Time: 209.067 ms, Bubble Time: 152.967 ms, Imbalance Overhead: 11.778 ms
GPU 1, Compute+Comm Time: 207.777 ms, Bubble Time: 155.899 ms, Imbalance Overhead: 10.136 ms
GPU 2, Compute+Comm Time: 207.777 ms, Bubble Time: 160.818 ms, Imbalance Overhead: 5.217 ms
GPU 3, Compute+Comm Time: 208.128 ms, Bubble Time: 165.650 ms, Imbalance Overhead: 0.034 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 658.174 ms
GPU 0, Compute+Comm Time: 365.612 ms, Bubble Time: 292.545 ms, Imbalance Overhead: 0.017 ms
GPU 1, Compute+Comm Time: 364.195 ms, Bubble Time: 283.337 ms, Imbalance Overhead: 10.642 ms
GPU 2, Compute+Comm Time: 364.195 ms, Bubble Time: 273.777 ms, Imbalance Overhead: 20.202 ms
GPU 3, Compute+Comm Time: 367.488 ms, Bubble Time: 268.034 ms, Imbalance Overhead: 22.652 ms
    The estimated cost with 2 DP ways is 1083.585 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1565.578 ms
Partition 0 [0, 65) has cost: 1565.578 ms
Partition 1 [65, 129) has cost: 1559.939 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 659.345 ms
GPU 0, Compute+Comm Time: 436.598 ms, Bubble Time: 213.145 ms, Imbalance Overhead: 9.601 ms
GPU 1, Compute+Comm Time: 436.127 ms, Bubble Time: 223.218 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 903.885 ms
GPU 0, Compute+Comm Time: 595.904 ms, Bubble Time: 307.981 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 596.845 ms, Bubble Time: 288.391 ms, Imbalance Overhead: 18.649 ms
    The estimated cost with 4 DP ways is 1641.391 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 3125.517 ms
Partition 0 [0, 129) has cost: 3125.517 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 1843.901 ms
GPU 0, Compute+Comm Time: 1843.901 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 2013.197 ms
GPU 0, Compute+Comm Time: 2013.197 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 4049.953 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 905)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 905)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 905)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 905)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 905)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 905)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 905)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 905)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2698: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 27323 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
[proxy:0:1@gnerv2] HYD_pmcd_pmip_control_cmd_cb (../../../../src/pm/hydra/proxy/pmip_cb.c:480): assert (!closed) failed
[proxy:0:1@gnerv2] HYDT_dmxu_poll_wait_for_event (../../../../src/pm/hydra/lib/tools/demux/demux_poll.c:76): callback returned error status
[proxy:0:1@gnerv2] main (../../../../src/pm/hydra/proxy/pmip.c:190): demux engine error waiting for event
[mpiexec@gnerv1] HYDT_bscu_wait_for_completion (../../../../src/pm/hydra/lib/tools/bootstrap/utils/bscu_wait.c:109): one of the processes terminated badly; aborting
[mpiexec@gnerv1] HYDT_bsci_wait_for_completion (../../../../src/pm/hydra/lib/tools/bootstrap/src/bsci_wait.c:21): launcher returned error waiting for completion
[mpiexec@gnerv1] HYD_pmci_wait_for_completion (../../../../src/pm/hydra/mpiexec/pmiserv_pmci.c:197): launcher returned error waiting for completion
[mpiexec@gnerv1] main (../../../../src/pm/hydra/mpiexec/mpiexec.c:252): process manager error waiting for completion
