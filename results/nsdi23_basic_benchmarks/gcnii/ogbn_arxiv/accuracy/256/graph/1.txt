Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.067 seconds.
Building the CSC structure...
        It takes 0.040 seconds.
        It takes 0.044 seconds.
        It takes 0.051 seconds.
        It takes 0.054 seconds.
        It takes 0.050 seconds.
        It takes 0.054 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 128
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
169343, 2484941, 2484941
Number of vertices per chunk: 21168
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 64.049 Gbps (per GPU), 512.393 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.004 Gbps (per GPU), 512.035 Gbps (aggregated)
The layer-level communication performance: 64.003 Gbps (per GPU), 512.024 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.961 Gbps (per GPU), 511.686 Gbps (aggregated)
The layer-level communication performance: 63.966 Gbps (per GPU), 511.731 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.932 Gbps (per GPU), 511.452 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.925 Gbps (per GPU), 511.397 Gbps (aggregated)
The layer-level communication performance: 63.920 Gbps (per GPU), 511.359 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.818 Gbps (per GPU), 1310.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.817 Gbps (per GPU), 1310.534 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.815 Gbps (per GPU), 1310.522 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.820 Gbps (per GPU), 1310.557 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.813 Gbps (per GPU), 1310.503 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.812 Gbps (per GPU), 1310.496 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.810 Gbps (per GPU), 1310.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.809 Gbps (per GPU), 1310.470 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 112.250 Gbps (per GPU), 898.000 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.249 Gbps (per GPU), 897.992 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.249 Gbps (per GPU), 897.989 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.249 Gbps (per GPU), 897.992 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.242 Gbps (per GPU), 897.938 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.235 Gbps (per GPU), 897.878 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.235 Gbps (per GPU), 897.877 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.218 Gbps (per GPU), 897.747 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.930 Gbps (per GPU), 359.437 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.930 Gbps (per GPU), 359.437 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.930 Gbps (per GPU), 359.438 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.929 Gbps (per GPU), 359.434 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.929 Gbps (per GPU), 359.436 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.929 Gbps (per GPU), 359.435 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.929 Gbps (per GPU), 359.435 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.930 Gbps (per GPU), 359.437 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.15ms  3.17ms  3.61ms  3.14 21.17K  0.46M
 chk_1  1.15ms  3.39ms  3.83ms  3.32 21.17K  0.55M
 chk_2  1.15ms  3.13ms  3.58ms  3.12 21.17K  0.39M
 chk_3  1.15ms  3.03ms  3.48ms  3.02 21.17K  0.24M
 chk_4  1.15ms  2.91ms  3.36ms  2.91 21.17K  0.17M
 chk_5  1.15ms  2.92ms  3.36ms  2.91 21.17K  0.22M
 chk_6  1.15ms  2.94ms  3.38ms  2.94 21.17K  0.16M
 chk_7  1.15ms  2.89ms  3.34ms  2.90 21.17K  0.12M
   Avg  1.15  3.05  3.49
   Max  1.15  3.39  3.83
   Min  1.15  2.89  3.34
 Ratio  1.00  1.17  1.15
   Var  0.00  0.03  0.03
Profiling takes 0.814 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 399.319 ms
Partition 0 [0, 17) has cost: 399.319 ms
Partition 1 [17, 33) has cost: 390.105 ms
Partition 2 [33, 49) has cost: 390.105 ms
Partition 3 [49, 65) has cost: 390.105 ms
Partition 4 [65, 81) has cost: 390.105 ms
Partition 5 [81, 97) has cost: 390.105 ms
Partition 6 [97, 113) has cost: 390.105 ms
Partition 7 [113, 129) has cost: 393.658 ms
The optimal partitioning:
[0, 17)
[17, 33)
[33, 49)
[49, 65)
[65, 81)
[81, 97)
[97, 113)
[113, 129)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 275.609 ms
GPU 0, Compute+Comm Time: 141.997 ms, Bubble Time: 121.903 ms, Imbalance Overhead: 11.709 ms
GPU 1, Compute+Comm Time: 139.405 ms, Bubble Time: 120.991 ms, Imbalance Overhead: 15.213 ms
GPU 2, Compute+Comm Time: 139.405 ms, Bubble Time: 122.614 ms, Imbalance Overhead: 13.590 ms
GPU 3, Compute+Comm Time: 139.405 ms, Bubble Time: 124.501 ms, Imbalance Overhead: 11.703 ms
GPU 4, Compute+Comm Time: 139.405 ms, Bubble Time: 126.979 ms, Imbalance Overhead: 9.224 ms
GPU 5, Compute+Comm Time: 139.405 ms, Bubble Time: 129.430 ms, Imbalance Overhead: 6.774 ms
GPU 6, Compute+Comm Time: 139.405 ms, Bubble Time: 131.837 ms, Imbalance Overhead: 4.367 ms
GPU 7, Compute+Comm Time: 140.117 ms, Bubble Time: 134.430 ms, Imbalance Overhead: 1.062 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 580.852 ms
GPU 0, Compute+Comm Time: 296.860 ms, Bubble Time: 281.985 ms, Imbalance Overhead: 2.007 ms
GPU 1, Compute+Comm Time: 294.019 ms, Bubble Time: 277.014 ms, Imbalance Overhead: 9.819 ms
GPU 2, Compute+Comm Time: 294.019 ms, Bubble Time: 272.583 ms, Imbalance Overhead: 14.251 ms
GPU 3, Compute+Comm Time: 294.019 ms, Bubble Time: 267.932 ms, Imbalance Overhead: 18.901 ms
GPU 4, Compute+Comm Time: 294.019 ms, Bubble Time: 263.214 ms, Imbalance Overhead: 23.620 ms
GPU 5, Compute+Comm Time: 294.019 ms, Bubble Time: 259.790 ms, Imbalance Overhead: 27.044 ms
GPU 6, Compute+Comm Time: 294.019 ms, Bubble Time: 256.599 ms, Imbalance Overhead: 30.235 ms
GPU 7, Compute+Comm Time: 300.641 ms, Bubble Time: 258.482 ms, Imbalance Overhead: 21.729 ms
The estimated cost of the whole pipeline: 899.284 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 789.425 ms
Partition 0 [0, 33) has cost: 789.425 ms
Partition 1 [33, 65) has cost: 780.210 ms
Partition 2 [65, 97) has cost: 780.210 ms
Partition 3 [97, 129) has cost: 783.764 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 376.249 ms
GPU 0, Compute+Comm Time: 210.402 ms, Bubble Time: 153.884 ms, Imbalance Overhead: 11.963 ms
GPU 1, Compute+Comm Time: 209.104 ms, Bubble Time: 156.897 ms, Imbalance Overhead: 10.249 ms
GPU 2, Compute+Comm Time: 209.104 ms, Bubble Time: 161.884 ms, Imbalance Overhead: 5.261 ms
GPU 3, Compute+Comm Time: 209.460 ms, Bubble Time: 166.789 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 661.241 ms
GPU 0, Compute+Comm Time: 367.216 ms, Bubble Time: 293.944 ms, Imbalance Overhead: 0.081 ms
GPU 1, Compute+Comm Time: 365.800 ms, Bubble Time: 284.726 ms, Imbalance Overhead: 10.715 ms
GPU 2, Compute+Comm Time: 365.800 ms, Bubble Time: 275.154 ms, Imbalance Overhead: 20.287 ms
GPU 3, Compute+Comm Time: 369.114 ms, Bubble Time: 269.241 ms, Imbalance Overhead: 22.885 ms
    The estimated cost with 2 DP ways is 1089.364 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1569.635 ms
Partition 0 [0, 65) has cost: 1569.635 ms
Partition 1 [65, 129) has cost: 1563.974 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 666.733 ms
GPU 0, Compute+Comm Time: 441.471 ms, Bubble Time: 215.506 ms, Imbalance Overhead: 9.757 ms
GPU 1, Compute+Comm Time: 441.001 ms, Bubble Time: 225.732 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 911.916 ms
GPU 0, Compute+Comm Time: 601.244 ms, Bubble Time: 310.672 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 602.198 ms, Bubble Time: 291.052 ms, Imbalance Overhead: 18.666 ms
    The estimated cost with 4 DP ways is 1657.582 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 3133.609 ms
Partition 0 [0, 129) has cost: 3133.609 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 1862.041 ms
GPU 0, Compute+Comm Time: 1862.041 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 2031.677 ms
GPU 0, Compute+Comm Time: 2031.677 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 4088.404 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 905)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 905)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 905)
*** Node 4, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 905)
*** Node 3, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 905)
*** Node 5, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 905)
*** Node 2, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 905)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 905)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2698: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2698: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 27205 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Aborted (signal 6)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
