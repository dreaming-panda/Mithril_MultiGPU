Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.068 seconds.
Building the CSC structure...
        It takes 0.070 seconds.
Building the CSC structure...
        It takes 0.041 seconds.
        It takes 0.052 seconds.
        It takes 0.054 seconds.
        It takes 0.053 seconds.
        It takes 0.049 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.023 seconds.
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.691 Gbps (per GPU), 509.528 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.653 Gbps (per GPU), 509.223 Gbps (aggregated)
The layer-level communication performance: 63.652 Gbps (per GPU), 509.216 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.615 Gbps (per GPU), 508.923 Gbps (aggregated)
The layer-level communication performance: 63.611 Gbps (per GPU), 508.888 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.579 Gbps (per GPU), 508.631 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.572 Gbps (per GPU), 508.574 Gbps (aggregated)
The layer-level communication performance: 63.567 Gbps (per GPU), 508.537 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.175 Gbps (per GPU), 1329.398 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.171 Gbps (per GPU), 1329.372 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.173 Gbps (per GPU), 1329.382 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.172 Gbps (per GPU), 1329.375 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.168 Gbps (per GPU), 1329.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.167 Gbps (per GPU), 1329.339 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.172 Gbps (per GPU), 1329.379 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.177 Gbps (per GPU), 1329.415 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.831 Gbps (per GPU), 910.651 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.832 Gbps (per GPU), 910.653 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.829 Gbps (per GPU), 910.632 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.827 Gbps (per GPU), 910.618 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.821 Gbps (per GPU), 910.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.822 Gbps (per GPU), 910.572 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.812 Gbps (per GPU), 910.497 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.805 Gbps (per GPU), 910.442 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.506 Gbps (per GPU), 364.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.506 Gbps (per GPU), 364.051 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.506 Gbps (per GPU), 364.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.506 Gbps (per GPU), 364.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.506 Gbps (per GPU), 364.046 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.506 Gbps (per GPU), 364.048 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.506 Gbps (per GPU), 364.045 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.506 Gbps (per GPU), 364.049 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.98ms  2.37ms  2.75ms  2.80 21.17K  0.46M
 chk_1  0.98ms  2.52ms  2.91ms  2.96 21.17K  0.55M
 chk_2  0.98ms  2.34ms  2.72ms  2.77 21.17K  0.39M
 chk_3  0.98ms  2.24ms  2.62ms  2.68 21.17K  0.24M
 chk_4  0.98ms  2.15ms  2.53ms  2.57 21.17K  0.17M
 chk_5  1.03ms  2.16ms  2.55ms  2.47 21.17K  0.22M
 chk_6  0.98ms  2.16ms  2.54ms  2.58 21.17K  0.16M
 chk_7  0.98ms  2.13ms  2.51ms  2.55 21.17K  0.12M
   Avg  0.99  2.26  2.64
   Max  1.03  2.52  2.91
   Min  0.98  2.13  2.51
 Ratio  1.05  1.19  1.16
   Var  0.00  0.02  0.02
Profiling takes 0.637 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 152.538 ms
Partition 0 [0, 9) has cost: 152.538 ms
Partition 1 [9, 17) has cost: 144.633 ms
Partition 2 [17, 25) has cost: 144.633 ms
Partition 3 [25, 33) has cost: 144.633 ms
Partition 4 [33, 41) has cost: 144.633 ms
Partition 5 [41, 49) has cost: 144.633 ms
Partition 6 [49, 57) has cost: 144.633 ms
Partition 7 [57, 65) has cost: 147.679 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 118.867 ms
GPU 0, Compute+Comm Time: 62.358 ms, Bubble Time: 52.912 ms, Imbalance Overhead: 3.596 ms
GPU 1, Compute+Comm Time: 60.201 ms, Bubble Time: 52.709 ms, Imbalance Overhead: 5.958 ms
GPU 2, Compute+Comm Time: 60.201 ms, Bubble Time: 53.388 ms, Imbalance Overhead: 5.278 ms
GPU 3, Compute+Comm Time: 60.201 ms, Bubble Time: 54.067 ms, Imbalance Overhead: 4.599 ms
GPU 4, Compute+Comm Time: 60.201 ms, Bubble Time: 54.978 ms, Imbalance Overhead: 3.688 ms
GPU 5, Compute+Comm Time: 60.201 ms, Bubble Time: 55.850 ms, Imbalance Overhead: 2.817 ms
GPU 6, Compute+Comm Time: 60.201 ms, Bubble Time: 56.739 ms, Imbalance Overhead: 1.927 ms
GPU 7, Compute+Comm Time: 60.824 ms, Bubble Time: 57.729 ms, Imbalance Overhead: 0.314 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 232.940 ms
GPU 0, Compute+Comm Time: 119.526 ms, Bubble Time: 112.873 ms, Imbalance Overhead: 0.541 ms
GPU 1, Compute+Comm Time: 117.103 ms, Bubble Time: 111.086 ms, Imbalance Overhead: 4.751 ms
GPU 2, Compute+Comm Time: 117.103 ms, Bubble Time: 109.455 ms, Imbalance Overhead: 6.382 ms
GPU 3, Compute+Comm Time: 117.103 ms, Bubble Time: 107.836 ms, Imbalance Overhead: 8.001 ms
GPU 4, Compute+Comm Time: 117.103 ms, Bubble Time: 106.178 ms, Imbalance Overhead: 9.659 ms
GPU 5, Compute+Comm Time: 117.103 ms, Bubble Time: 104.986 ms, Imbalance Overhead: 10.851 ms
GPU 6, Compute+Comm Time: 117.103 ms, Bubble Time: 103.589 ms, Imbalance Overhead: 12.247 ms
GPU 7, Compute+Comm Time: 122.851 ms, Bubble Time: 104.025 ms, Imbalance Overhead: 6.064 ms
The estimated cost of the whole pipeline: 369.398 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 297.171 ms
Partition 0 [0, 17) has cost: 297.171 ms
Partition 1 [17, 33) has cost: 289.265 ms
Partition 2 [33, 49) has cost: 289.265 ms
Partition 3 [49, 65) has cost: 292.311 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 154.771 ms
GPU 0, Compute+Comm Time: 87.131 ms, Bubble Time: 63.568 ms, Imbalance Overhead: 4.071 ms
GPU 1, Compute+Comm Time: 86.052 ms, Bubble Time: 64.734 ms, Imbalance Overhead: 3.986 ms
GPU 2, Compute+Comm Time: 86.052 ms, Bubble Time: 66.553 ms, Imbalance Overhead: 2.166 ms
GPU 3, Compute+Comm Time: 86.363 ms, Bubble Time: 68.409 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 260.653 ms
GPU 0, Compute+Comm Time: 144.906 ms, Bubble Time: 115.747 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 143.694 ms, Bubble Time: 112.180 ms, Imbalance Overhead: 4.779 ms
GPU 2, Compute+Comm Time: 143.694 ms, Bubble Time: 108.638 ms, Imbalance Overhead: 8.320 ms
GPU 3, Compute+Comm Time: 146.592 ms, Bubble Time: 106.258 ms, Imbalance Overhead: 7.803 ms
    The estimated cost with 2 DP ways is 436.195 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 586.436 ms
Partition 0 [0, 33) has cost: 586.436 ms
Partition 1 [33, 65) has cost: 581.577 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 259.871 ms
GPU 0, Compute+Comm Time: 172.238 ms, Bubble Time: 84.029 ms, Imbalance Overhead: 3.603 ms
GPU 1, Compute+Comm Time: 171.856 ms, Bubble Time: 88.015 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 350.656 ms
GPU 0, Compute+Comm Time: 230.935 ms, Bubble Time: 119.721 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 231.800 ms, Bubble Time: 111.622 ms, Imbalance Overhead: 7.234 ms
    The estimated cost with 4 DP ways is 641.053 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1168.013 ms
Partition 0 [0, 65) has cost: 1168.013 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 690.016 ms
GPU 0, Compute+Comm Time: 690.016 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 753.257 ms
GPU 0, Compute+Comm Time: 753.257 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1515.437 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2750: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.grad != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 23128 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
[proxy:0:1@gnerv2] HYD_pmcd_pmip_control_cmd_cb (../../../../src/pm/hydra/proxy/pmip_cb.c:480): assert (!closed) failed
[proxy:0:1@gnerv2] HYDT_dmxu_poll_wait_for_event (../../../../src/pm/hydra/lib/tools/demux/demux_poll.c:76): callback returned error status
[proxy:0:1@gnerv2] main (../../../../src/pm/hydra/proxy/pmip.c:190): demux engine error waiting for event
[mpiexec@gnerv1] HYDT_bscu_wait_for_completion (../../../../src/pm/hydra/lib/tools/bootstrap/utils/bscu_wait.c:109): one of the processes terminated badly; aborting
[mpiexec@gnerv1] HYDT_bsci_wait_for_completion (../../../../src/pm/hydra/lib/tools/bootstrap/src/bsci_wait.c:21): launcher returned error waiting for completion
[mpiexec@gnerv1] HYD_pmci_wait_for_completion (../../../../src/pm/hydra/mpiexec/pmiserv_pmci.c:197): launcher returned error waiting for completion
[mpiexec@gnerv1] main (../../../../src/pm/hydra/mpiexec/mpiexec.c:252): process manager error waiting for completion
