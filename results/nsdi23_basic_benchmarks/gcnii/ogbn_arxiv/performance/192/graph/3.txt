Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
        It takes 0.052 seconds.
        It takes 0.055 seconds.
        It takes 0.053 seconds.
        It takes 0.053 seconds.
        It takes 0.054 seconds.
        It takes 0.055 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 64.025 Gbps (per GPU), 512.198 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.985 Gbps (per GPU), 511.879 Gbps (aggregated)
The layer-level communication performance: 63.983 Gbps (per GPU), 511.866 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.950 Gbps (per GPU), 511.601 Gbps (aggregated)
The layer-level communication performance: 63.945 Gbps (per GPU), 511.564 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.917 Gbps (per GPU), 511.338 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.910 Gbps (per GPU), 511.281 Gbps (aggregated)
The layer-level communication performance: 63.906 Gbps (per GPU), 511.246 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.323 Gbps (per GPU), 1322.583 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.318 Gbps (per GPU), 1322.547 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.319 Gbps (per GPU), 1322.554 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.318 Gbps (per GPU), 1322.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.322 Gbps (per GPU), 1322.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.325 Gbps (per GPU), 1322.599 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.320 Gbps (per GPU), 1322.561 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.321 Gbps (per GPU), 1322.570 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.668 Gbps (per GPU), 917.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.665 Gbps (per GPU), 917.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.667 Gbps (per GPU), 917.332 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.661 Gbps (per GPU), 917.289 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.668 Gbps (per GPU), 917.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.657 Gbps (per GPU), 917.259 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.663 Gbps (per GPU), 917.306 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.649 Gbps (per GPU), 917.189 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.671 Gbps (per GPU), 365.366 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.671 Gbps (per GPU), 365.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.671 Gbps (per GPU), 365.367 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.670 Gbps (per GPU), 365.360 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.671 Gbps (per GPU), 365.367 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.671 Gbps (per GPU), 365.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.671 Gbps (per GPU), 365.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.670 Gbps (per GPU), 365.363 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.98ms  2.37ms  2.74ms  2.79 21.17K  0.46M
 chk_1  0.98ms  2.53ms  2.91ms  2.96 21.17K  0.55M
 chk_2  0.98ms  2.34ms  2.72ms  2.77 21.17K  0.39M
 chk_3  0.98ms  2.24ms  2.62ms  2.66 21.17K  0.24M
 chk_4  0.98ms  2.15ms  2.52ms  2.57 21.17K  0.17M
 chk_5  0.98ms  2.16ms  2.54ms  2.58 21.17K  0.22M
 chk_6  0.98ms  2.15ms  2.53ms  2.57 21.17K  0.16M
 chk_7  0.98ms  2.12ms  2.50ms  2.54 21.17K  0.12M
   Avg  0.98  2.26  2.63
   Max  0.98  2.53  2.91
   Min  0.98  2.12  2.50
 Ratio  1.00  1.19  1.16
   Var  0.00  0.02  0.02
Profiling takes 0.633 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 152.238 ms
Partition 0 [0, 9) has cost: 152.238 ms
Partition 1 [9, 17) has cost: 144.375 ms
Partition 2 [17, 25) has cost: 144.375 ms
Partition 3 [25, 33) has cost: 144.375 ms
Partition 4 [33, 41) has cost: 144.375 ms
Partition 5 [41, 49) has cost: 144.375 ms
Partition 6 [49, 57) has cost: 144.375 ms
Partition 7 [57, 65) has cost: 147.402 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 118.578 ms
GPU 0, Compute+Comm Time: 62.164 ms, Bubble Time: 52.735 ms, Imbalance Overhead: 3.679 ms
GPU 1, Compute+Comm Time: 60.007 ms, Bubble Time: 52.523 ms, Imbalance Overhead: 6.048 ms
GPU 2, Compute+Comm Time: 60.007 ms, Bubble Time: 53.210 ms, Imbalance Overhead: 5.361 ms
GPU 3, Compute+Comm Time: 60.007 ms, Bubble Time: 53.892 ms, Imbalance Overhead: 4.678 ms
GPU 4, Compute+Comm Time: 60.007 ms, Bubble Time: 54.818 ms, Imbalance Overhead: 3.753 ms
GPU 5, Compute+Comm Time: 60.007 ms, Bubble Time: 55.713 ms, Imbalance Overhead: 2.858 ms
GPU 6, Compute+Comm Time: 60.007 ms, Bubble Time: 56.639 ms, Imbalance Overhead: 1.932 ms
GPU 7, Compute+Comm Time: 60.620 ms, Bubble Time: 57.630 ms, Imbalance Overhead: 0.328 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 232.761 ms
GPU 0, Compute+Comm Time: 119.283 ms, Bubble Time: 112.915 ms, Imbalance Overhead: 0.564 ms
GPU 1, Compute+Comm Time: 116.869 ms, Bubble Time: 111.066 ms, Imbalance Overhead: 4.826 ms
GPU 2, Compute+Comm Time: 116.869 ms, Bubble Time: 109.376 ms, Imbalance Overhead: 6.516 ms
GPU 3, Compute+Comm Time: 116.869 ms, Bubble Time: 107.716 ms, Imbalance Overhead: 8.176 ms
GPU 4, Compute+Comm Time: 116.869 ms, Bubble Time: 105.988 ms, Imbalance Overhead: 9.904 ms
GPU 5, Compute+Comm Time: 116.869 ms, Bubble Time: 104.759 ms, Imbalance Overhead: 11.134 ms
GPU 6, Compute+Comm Time: 116.869 ms, Bubble Time: 103.347 ms, Imbalance Overhead: 12.545 ms
GPU 7, Compute+Comm Time: 122.575 ms, Bubble Time: 103.797 ms, Imbalance Overhead: 6.390 ms
The estimated cost of the whole pipeline: 368.906 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 296.612 ms
Partition 0 [0, 17) has cost: 296.612 ms
Partition 1 [17, 33) has cost: 288.750 ms
Partition 2 [33, 49) has cost: 288.750 ms
Partition 3 [49, 65) has cost: 291.776 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 154.679 ms
GPU 0, Compute+Comm Time: 87.019 ms, Bubble Time: 63.451 ms, Imbalance Overhead: 4.209 ms
GPU 1, Compute+Comm Time: 85.940 ms, Bubble Time: 64.632 ms, Imbalance Overhead: 4.108 ms
GPU 2, Compute+Comm Time: 85.940 ms, Bubble Time: 66.500 ms, Imbalance Overhead: 2.240 ms
GPU 3, Compute+Comm Time: 86.250 ms, Bubble Time: 68.429 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 260.761 ms
GPU 0, Compute+Comm Time: 144.868 ms, Bubble Time: 115.893 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 143.656 ms, Bubble Time: 112.209 ms, Imbalance Overhead: 4.895 ms
GPU 2, Compute+Comm Time: 143.656 ms, Bubble Time: 108.584 ms, Imbalance Overhead: 8.520 ms
GPU 3, Compute+Comm Time: 146.511 ms, Bubble Time: 106.172 ms, Imbalance Overhead: 8.077 ms
    The estimated cost with 2 DP ways is 436.212 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 585.362 ms
Partition 0 [0, 33) has cost: 585.362 ms
Partition 1 [33, 65) has cost: 580.526 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 258.419 ms
GPU 0, Compute+Comm Time: 171.237 ms, Bubble Time: 83.481 ms, Imbalance Overhead: 3.701 ms
GPU 1, Compute+Comm Time: 170.854 ms, Bubble Time: 87.565 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 349.383 ms
GPU 0, Compute+Comm Time: 230.029 ms, Bubble Time: 119.354 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 230.850 ms, Bubble Time: 111.086 ms, Imbalance Overhead: 7.448 ms
    The estimated cost with 4 DP ways is 638.192 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1165.888 ms
Partition 0 [0, 65) has cost: 1165.888 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 687.687 ms
GPU 0, Compute+Comm Time: 687.687 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 751.047 ms
GPU 0, Compute+Comm Time: 751.047 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1510.671 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 23244 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Aborted (signal 6)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
