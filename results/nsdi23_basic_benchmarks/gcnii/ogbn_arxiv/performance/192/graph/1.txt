Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.048 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.040 seconds.
        It takes 0.053 seconds.
        It takes 0.050 seconds.
        It takes 0.052 seconds.
        It takes 0.053 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
        It takes 0.056 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.027 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 64.138 Gbps (per GPU), 513.101 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.101 Gbps (per GPU), 512.811 Gbps (aggregated)
The layer-level communication performance: 64.100 Gbps (per GPU), 512.798 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.066 Gbps (per GPU), 512.531 Gbps (aggregated)
The layer-level communication performance: 64.061 Gbps (per GPU), 512.485 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.031 Gbps (per GPU), 512.249 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.024 Gbps (per GPU), 512.194 Gbps (aggregated)
The layer-level communication performance: 64.020 Gbps (per GPU), 512.156 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.223 Gbps (per GPU), 1321.782 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.220 Gbps (per GPU), 1321.759 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.217 Gbps (per GPU), 1321.737 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.222 Gbps (per GPU), 1321.776 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.215 Gbps (per GPU), 1321.724 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.216 Gbps (per GPU), 1321.730 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.215 Gbps (per GPU), 1321.724 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.184 Gbps (per GPU), 1321.470 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.157 Gbps (per GPU), 913.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.157 Gbps (per GPU), 913.257 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.153 Gbps (per GPU), 913.224 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.150 Gbps (per GPU), 913.201 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.149 Gbps (per GPU), 913.193 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.145 Gbps (per GPU), 913.162 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.137 Gbps (per GPU), 913.098 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.137 Gbps (per GPU), 913.097 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.211 Gbps (per GPU), 361.684 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.211 Gbps (per GPU), 361.685 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.211 Gbps (per GPU), 361.687 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.210 Gbps (per GPU), 361.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.210 Gbps (per GPU), 361.682 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.210 Gbps (per GPU), 361.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.210 Gbps (per GPU), 361.684 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.210 Gbps (per GPU), 361.683 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.99ms  2.38ms  2.76ms  2.79 21.17K  0.46M
 chk_1  0.99ms  2.54ms  2.92ms  2.95 21.17K  0.55M
 chk_2  0.99ms  2.34ms  2.73ms  2.76 21.17K  0.39M
 chk_3  0.99ms  2.25ms  2.63ms  2.67 21.17K  0.24M
 chk_4  0.99ms  2.16ms  2.54ms  2.57 21.17K  0.17M
 chk_5  0.99ms  2.17ms  2.55ms  2.58 21.17K  0.22M
 chk_6  0.99ms  2.16ms  2.55ms  2.57 21.17K  0.16M
 chk_7  0.99ms  2.13ms  2.52ms  2.55 21.17K  0.12M
   Avg  0.99  2.27  2.65
   Max  0.99  2.54  2.92
   Min  0.99  2.13  2.52
 Ratio  1.00  1.19  1.16
   Var  0.00  0.02  0.02
Profiling takes 0.637 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 152.977 ms
Partition 0 [0, 9) has cost: 152.977 ms
Partition 1 [9, 17) has cost: 145.070 ms
Partition 2 [17, 25) has cost: 145.070 ms
Partition 3 [25, 33) has cost: 145.070 ms
Partition 4 [33, 41) has cost: 145.070 ms
Partition 5 [41, 49) has cost: 145.070 ms
Partition 6 [49, 57) has cost: 145.070 ms
Partition 7 [57, 65) has cost: 148.128 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 119.243 ms
GPU 0, Compute+Comm Time: 62.457 ms, Bubble Time: 52.966 ms, Imbalance Overhead: 3.820 ms
GPU 1, Compute+Comm Time: 60.281 ms, Bubble Time: 52.750 ms, Imbalance Overhead: 6.213 ms
GPU 2, Compute+Comm Time: 60.281 ms, Bubble Time: 53.467 ms, Imbalance Overhead: 5.496 ms
GPU 3, Compute+Comm Time: 60.281 ms, Bubble Time: 54.178 ms, Imbalance Overhead: 4.785 ms
GPU 4, Compute+Comm Time: 60.281 ms, Bubble Time: 55.136 ms, Imbalance Overhead: 3.826 ms
GPU 5, Compute+Comm Time: 60.281 ms, Bubble Time: 56.047 ms, Imbalance Overhead: 2.915 ms
GPU 6, Compute+Comm Time: 60.281 ms, Bubble Time: 56.985 ms, Imbalance Overhead: 1.977 ms
GPU 7, Compute+Comm Time: 60.896 ms, Bubble Time: 58.009 ms, Imbalance Overhead: 0.337 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 233.400 ms
GPU 0, Compute+Comm Time: 119.676 ms, Bubble Time: 113.173 ms, Imbalance Overhead: 0.550 ms
GPU 1, Compute+Comm Time: 117.233 ms, Bubble Time: 111.344 ms, Imbalance Overhead: 4.822 ms
GPU 2, Compute+Comm Time: 117.233 ms, Bubble Time: 109.675 ms, Imbalance Overhead: 6.491 ms
GPU 3, Compute+Comm Time: 117.233 ms, Bubble Time: 108.050 ms, Imbalance Overhead: 8.116 ms
GPU 4, Compute+Comm Time: 117.233 ms, Bubble Time: 106.346 ms, Imbalance Overhead: 9.820 ms
GPU 5, Compute+Comm Time: 117.233 ms, Bubble Time: 105.126 ms, Imbalance Overhead: 11.041 ms
GPU 6, Compute+Comm Time: 117.233 ms, Bubble Time: 103.698 ms, Imbalance Overhead: 12.468 ms
GPU 7, Compute+Comm Time: 122.963 ms, Bubble Time: 104.141 ms, Imbalance Overhead: 6.295 ms
The estimated cost of the whole pipeline: 370.275 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 298.047 ms
Partition 0 [0, 17) has cost: 298.047 ms
Partition 1 [17, 33) has cost: 290.140 ms
Partition 2 [33, 49) has cost: 290.140 ms
Partition 3 [49, 65) has cost: 293.198 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 155.365 ms
GPU 0, Compute+Comm Time: 87.375 ms, Bubble Time: 63.683 ms, Imbalance Overhead: 4.307 ms
GPU 1, Compute+Comm Time: 86.285 ms, Bubble Time: 64.921 ms, Imbalance Overhead: 4.159 ms
GPU 2, Compute+Comm Time: 86.285 ms, Bubble Time: 66.820 ms, Imbalance Overhead: 2.259 ms
GPU 3, Compute+Comm Time: 86.592 ms, Bubble Time: 68.773 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 261.369 ms
GPU 0, Compute+Comm Time: 145.248 ms, Bubble Time: 116.121 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 144.030 ms, Bubble Time: 112.479 ms, Imbalance Overhead: 4.860 ms
GPU 2, Compute+Comm Time: 144.030 ms, Bubble Time: 108.927 ms, Imbalance Overhead: 8.413 ms
GPU 3, Compute+Comm Time: 146.898 ms, Bubble Time: 106.477 ms, Imbalance Overhead: 7.994 ms
    The estimated cost with 2 DP ways is 437.571 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 588.187 ms
Partition 0 [0, 33) has cost: 588.187 ms
Partition 1 [33, 65) has cost: 583.338 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 259.745 ms
GPU 0, Compute+Comm Time: 172.108 ms, Bubble Time: 83.882 ms, Imbalance Overhead: 3.755 ms
GPU 1, Compute+Comm Time: 171.714 ms, Bubble Time: 88.030 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 350.693 ms
GPU 0, Compute+Comm Time: 230.949 ms, Bubble Time: 119.744 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 231.778 ms, Bubble Time: 111.619 ms, Imbalance Overhead: 7.296 ms
    The estimated cost with 4 DP ways is 640.959 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1171.525 ms
Partition 0 [0, 65) has cost: 1171.525 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 694.648 ms
GPU 0, Compute+Comm Time: 694.648 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 757.858 ms
GPU 0, Compute+Comm Time: 757.858 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1525.131 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2734: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.grad != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 23015 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
[proxy:0:1@gnerv2] HYD_pmcd_pmip_control_cmd_cb (../../../../src/pm/hydra/proxy/pmip_cb.c:480): assert (!closed) failed
[proxy:0:1@gnerv2] HYDT_dmxu_poll_wait_for_event (../../../../src/pm/hydra/lib/tools/demux/demux_poll.c:76): callback returned error status
[proxy:0:1@gnerv2] main (../../../../src/pm/hydra/proxy/pmip.c:190): demux engine error waiting for event
[mpiexec@gnerv1] HYDT_bscu_wait_for_completion (../../../../src/pm/hydra/lib/tools/bootstrap/utils/bscu_wait.c:109): one of the processes terminated badly; aborting
[mpiexec@gnerv1] HYDT_bsci_wait_for_completion (../../../../src/pm/hydra/lib/tools/bootstrap/src/bsci_wait.c:21): launcher returned error waiting for completion
[mpiexec@gnerv1] HYD_pmci_wait_for_completion (../../../../src/pm/hydra/mpiexec/pmiserv_pmci.c:197): launcher returned error waiting for completion
[mpiexec@gnerv1] main (../../../../src/pm/hydra/mpiexec/mpiexec.c:252): process manager error waiting for completion
