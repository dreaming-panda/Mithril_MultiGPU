Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.066 seconds.
Building the CSC structure...
        It takes 0.044 seconds.
Building the Feature Vector...
        It takes 0.051 seconds.
        It takes 0.053 seconds.
        It takes 0.055 seconds.
        It takes 0.052 seconds.
        It takes 0.053 seconds.
        It takes 0.053 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
        It takes 0.064 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
        It takes 0.024 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.781 Gbps (per GPU), 510.245 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.739 Gbps (per GPU), 509.909 Gbps (aggregated)
The layer-level communication performance: 63.737 Gbps (per GPU), 509.895 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.703 Gbps (per GPU), 509.621 Gbps (aggregated)
The layer-level communication performance: 63.698 Gbps (per GPU), 509.581 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.668 Gbps (per GPU), 509.341 Gbps (aggregated)
The layer-level communication performance: 63.660 Gbps (per GPU), 509.284 Gbps (aggregated)
The layer-level communication performance: 63.656 Gbps (per GPU), 509.246 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.854 Gbps (per GPU), 1310.835 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.852 Gbps (per GPU), 1310.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.850 Gbps (per GPU), 1310.797 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.853 Gbps (per GPU), 1310.823 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.851 Gbps (per GPU), 1310.806 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.838 Gbps (per GPU), 1310.701 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.849 Gbps (per GPU), 1310.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.850 Gbps (per GPU), 1310.804 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.099 Gbps (per GPU), 912.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.098 Gbps (per GPU), 912.786 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.099 Gbps (per GPU), 912.791 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.096 Gbps (per GPU), 912.768 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.099 Gbps (per GPU), 912.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.092 Gbps (per GPU), 912.738 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.094 Gbps (per GPU), 912.754 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.032 Gbps (per GPU), 912.255 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.522 Gbps (per GPU), 356.180 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.523 Gbps (per GPU), 356.182 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.522 Gbps (per GPU), 356.178 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.522 Gbps (per GPU), 356.175 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.522 Gbps (per GPU), 356.179 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.522 Gbps (per GPU), 356.175 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.522 Gbps (per GPU), 356.177 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.521 Gbps (per GPU), 356.172 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.42ms  0.78ms  0.97ms  2.30  5.55K  0.06M
 chk_1  0.42ms  0.81ms  0.99ms  2.34  5.75K  0.05M
 chk_2  0.39ms  0.73ms  0.91ms  2.36  5.20K  0.07M
 chk_3  0.42ms  0.80ms  0.99ms  2.36  5.57K  0.06M
 chk_4  0.38ms  0.71ms  0.90ms  2.36  5.05K  0.08M
 chk_5  0.37ms  0.74ms  0.92ms  2.50  4.73K  0.11M
 chk_6  0.38ms  0.70ms  0.88ms  2.33  4.98K  0.08M
 chk_7  0.38ms  0.72ms  0.90ms  2.38  5.01K  0.09M
 chk_8  0.42ms  0.80ms  0.99ms  2.35  5.73K  0.05M
 chk_9  0.36ms  0.72ms  0.89ms  2.47  4.54K  0.11M
chk_10  0.39ms  0.74ms  0.92ms  2.35  5.36K  0.07M
chk_11  0.39ms  0.75ms  0.93ms  2.37  5.39K  0.08M
chk_12  0.42ms  0.80ms  0.99ms  2.34  5.77K  0.05M
chk_13  0.40ms  0.75ms  0.93ms  2.36  5.43K  0.06M
chk_14  0.41ms  0.77ms  0.96ms  2.32  5.46K  0.06M
chk_15  0.43ms  0.81ms  1.00ms  2.33  5.88K  0.04M
chk_16  0.42ms  0.79ms  0.97ms  2.34  5.50K  0.06M
chk_17  0.37ms  0.74ms  0.92ms  2.46  4.86K  0.09M
chk_18  0.39ms  0.78ms  0.97ms  2.45  5.39K  0.07M
chk_19  0.39ms  0.73ms  0.92ms  2.38  5.20K  0.07M
chk_20  0.42ms  0.77ms  0.96ms  2.32  5.51K  0.06M
chk_21  0.43ms  0.79ms  0.98ms  2.30  5.81K  0.05M
chk_22  0.39ms  0.73ms  0.91ms  2.33  5.32K  0.07M
chk_23  0.40ms  0.76ms  0.95ms  2.41  5.39K  0.07M
chk_24  0.36ms  0.72ms  0.90ms  2.47  4.62K  0.11M
chk_25  0.38ms  0.72ms  0.90ms  2.37  5.04K  0.08M
chk_26  0.36ms  0.72ms  0.90ms  2.47  4.55K  0.11M
chk_27  0.39ms  0.72ms  0.90ms  2.31  5.30K  0.06M
chk_28  0.42ms  0.79ms  0.98ms  2.34  5.58K  0.06M
chk_29  0.38ms  0.73ms  0.91ms  2.40  4.98K  0.09M
chk_30  0.42ms  0.78ms  0.97ms  2.32  5.50K  0.07M
chk_31  0.39ms  0.74ms  0.92ms  2.34  5.38K  0.07M
   Avg  0.40  0.75  0.94
   Max  0.43  0.81  1.00
   Min  0.36  0.70  0.88
 Ratio  1.18  1.15  1.13
   Var  0.00  0.00  0.00
Profiling takes 0.922 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 205.717 ms
Partition 0 [0, 9) has cost: 205.717 ms
Partition 1 [9, 17) has cost: 193.027 ms
Partition 2 [17, 25) has cost: 193.027 ms
Partition 3 [25, 33) has cost: 193.027 ms
Partition 4 [33, 41) has cost: 193.027 ms
Partition 5 [41, 49) has cost: 193.027 ms
Partition 6 [49, 57) has cost: 193.027 ms
Partition 7 [57, 65) has cost: 198.945 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 100.769 ms
GPU 0, Compute+Comm Time: 80.912 ms, Bubble Time: 17.517 ms, Imbalance Overhead: 2.340 ms
GPU 1, Compute+Comm Time: 76.813 ms, Bubble Time: 17.624 ms, Imbalance Overhead: 6.331 ms
GPU 2, Compute+Comm Time: 76.813 ms, Bubble Time: 17.825 ms, Imbalance Overhead: 6.130 ms
GPU 3, Compute+Comm Time: 76.813 ms, Bubble Time: 17.887 ms, Imbalance Overhead: 6.068 ms
GPU 4, Compute+Comm Time: 76.813 ms, Bubble Time: 18.010 ms, Imbalance Overhead: 5.945 ms
GPU 5, Compute+Comm Time: 76.813 ms, Bubble Time: 18.087 ms, Imbalance Overhead: 5.868 ms
GPU 6, Compute+Comm Time: 76.813 ms, Bubble Time: 18.123 ms, Imbalance Overhead: 5.832 ms
GPU 7, Compute+Comm Time: 78.116 ms, Bubble Time: 18.301 ms, Imbalance Overhead: 4.352 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 195.561 ms
GPU 0, Compute+Comm Time: 153.455 ms, Bubble Time: 35.464 ms, Imbalance Overhead: 6.643 ms
GPU 1, Compute+Comm Time: 148.840 ms, Bubble Time: 35.198 ms, Imbalance Overhead: 11.523 ms
GPU 2, Compute+Comm Time: 148.840 ms, Bubble Time: 35.188 ms, Imbalance Overhead: 11.534 ms
GPU 3, Compute+Comm Time: 148.840 ms, Bubble Time: 35.029 ms, Imbalance Overhead: 11.693 ms
GPU 4, Compute+Comm Time: 148.840 ms, Bubble Time: 34.849 ms, Imbalance Overhead: 11.873 ms
GPU 5, Compute+Comm Time: 148.840 ms, Bubble Time: 34.732 ms, Imbalance Overhead: 11.990 ms
GPU 6, Compute+Comm Time: 148.840 ms, Bubble Time: 34.329 ms, Imbalance Overhead: 12.393 ms
GPU 7, Compute+Comm Time: 157.431 ms, Bubble Time: 34.111 ms, Imbalance Overhead: 4.019 ms
The estimated cost of the whole pipeline: 311.147 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 398.744 ms
Partition 0 [0, 17) has cost: 398.744 ms
Partition 1 [17, 33) has cost: 386.055 ms
Partition 2 [33, 49) has cost: 386.055 ms
Partition 3 [49, 65) has cost: 391.972 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 129.853 ms
GPU 0, Compute+Comm Time: 106.692 ms, Bubble Time: 19.993 ms, Imbalance Overhead: 3.168 ms
GPU 1, Compute+Comm Time: 104.588 ms, Bubble Time: 20.345 ms, Imbalance Overhead: 4.920 ms
GPU 2, Compute+Comm Time: 104.588 ms, Bubble Time: 20.517 ms, Imbalance Overhead: 4.748 ms
GPU 3, Compute+Comm Time: 105.247 ms, Bubble Time: 20.698 ms, Imbalance Overhead: 3.908 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 221.783 ms
GPU 0, Compute+Comm Time: 180.552 ms, Bubble Time: 35.359 ms, Imbalance Overhead: 5.872 ms
GPU 1, Compute+Comm Time: 178.215 ms, Bubble Time: 35.122 ms, Imbalance Overhead: 8.446 ms
GPU 2, Compute+Comm Time: 178.215 ms, Bubble Time: 34.877 ms, Imbalance Overhead: 8.691 ms
GPU 3, Compute+Comm Time: 182.619 ms, Bubble Time: 34.268 ms, Imbalance Overhead: 4.896 ms
    The estimated cost with 2 DP ways is 369.218 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 784.799 ms
Partition 0 [0, 33) has cost: 784.799 ms
Partition 1 [33, 65) has cost: 778.027 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 225.667 ms
GPU 0, Compute+Comm Time: 195.319 ms, Bubble Time: 24.434 ms, Imbalance Overhead: 5.913 ms
GPU 1, Compute+Comm Time: 194.578 ms, Bubble Time: 25.214 ms, Imbalance Overhead: 5.874 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 312.532 ms
GPU 0, Compute+Comm Time: 269.736 ms, Bubble Time: 34.992 ms, Imbalance Overhead: 7.803 ms
GPU 1, Compute+Comm Time: 270.802 ms, Bubble Time: 33.950 ms, Imbalance Overhead: 7.779 ms
    The estimated cost with 4 DP ways is 565.108 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1562.826 ms
Partition 0 [0, 65) has cost: 1562.826 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 776.549 ms
GPU 0, Compute+Comm Time: 776.549 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 853.768 ms
GPU 0, Compute+Comm Time: 853.768 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1711.832 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.4265
	Epoch 50:	Loss 4.0893
	Epoch 75:	Loss 3.7796
Node 4, Pre/Post-Pipelining: 3.011 / 35.401 ms, Bubble: 65.303 ms, Compute: 189.648 ms, Comm: 56.403 ms, Imbalance: 20.261 ms
	Epoch 100:	Loss 3.7342
Node 2, Pre/Post-Pipelining: 3.021 / 35.456 ms, Bubble: 66.440 ms, Compute: 189.169 ms, Comm: 52.351 ms, Imbalance: 23.252 ms
Node 6, Pre/Post-Pipelining: 3.010 / 35.468 ms, Bubble: 65.958 ms, Compute: 189.801 ms, Comm: 44.265 ms, Imbalance: 31.326 ms
Node 1, Pre/Post-Pipelining: 3.016 / 35.519 ms, Bubble: 66.106 ms, Compute: 191.768 ms, Comm: 45.262 ms, Imbalance: 27.453 ms
Node 5, Pre/Post-Pipelining: 3.005 / 35.465 ms, Bubble: 66.071 ms, Compute: 191.194 ms, Comm: 52.848 ms, Imbalance: 21.276 ms
Node 3, Pre/Post-Pipelining: 3.017 / 35.514 ms, Bubble: 64.839 ms, Compute: 190.515 ms, Comm: 55.870 ms, Imbalance: 19.595 ms
Node 7, Pre/Post-Pipelining: 3.012 / 49.785 ms, Bubble: 52.172 ms, Compute: 212.371 ms, Comm: 33.144 ms, Imbalance: 18.920 ms
Node 0, Pre/Post-Pipelining: 3.020 / 35.534 ms, Bubble: 66.870 ms, Compute: 205.167 ms, Comm: 35.378 ms, Imbalance: 22.856 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 3.020 ms
Cluster-Wide Average, Post-Pipelining Overhead: 35.534 ms
Cluster-Wide Average, Bubble: 66.870 ms
Cluster-Wide Average, Compute: 205.167 ms
Cluster-Wide Average, Communication: 35.378 ms
Cluster-Wide Average, Imbalance: 22.856 ms
Node 0, GPU memory consumption: 10.510 GB
Node 1, GPU memory consumption: 8.721 GB
Node 3, GPU memory consumption: 8.698 GB
Node 2, GPU memory consumption: 8.721 GB
Node 5, GPU memory consumption: 8.721 GB
Node 7, GPU memory consumption: 8.801 GB
Node 6, GPU memory consumption: 8.721 GB
Node 4, GPU memory consumption: 8.698 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.369508 s---------------
------------------------node id 1,  per-epoch time: 0.369504 s---------------
------------------------node id 2,  per-epoch time: 0.369502 s---------------
------------------------node id 3,  per-epoch time: 0.369506 s---------------
------------------------node id 4,  per-epoch time: 0.369493 s---------------
------------------------node id 5,  per-epoch time: 0.369504 s---------------
------------------------node id 6,  per-epoch time: 0.369502 s---------------
------------------------node id 7,  per-epoch time: 0.369506 s---------------
************ Profiling Results ************
	Bubble: 179.675442 (ms) (47.57 percentage)
	Compute: 190.075524 (ms) (50.32 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 7.983059 (ms) (2.11 percentage)
	Layer-level communication (cluster-wide, per-epoch): 3.391 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.071 GB
	Total communication (cluster-wide, per-epoch): 3.463 GB
	Aggregated layer-level communication throughput: 620.628 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
