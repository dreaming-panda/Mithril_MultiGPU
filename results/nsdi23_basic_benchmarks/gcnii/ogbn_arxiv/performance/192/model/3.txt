Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.046 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.042 seconds.
        It takes 0.050 seconds.
Building the Feature Vector...
        It takes 0.052 seconds.
        It takes 0.052 seconds.
        It takes 0.057 seconds.
        It takes 0.052 seconds.
        It takes 0.055 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
        It takes 0.022 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.022 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.362 Gbps (per GPU), 506.893 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.319 Gbps (per GPU), 506.549 Gbps (aggregated)
The layer-level communication performance: 63.317 Gbps (per GPU), 506.539 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.284 Gbps (per GPU), 506.271 Gbps (aggregated)
The layer-level communication performance: 63.279 Gbps (per GPU), 506.229 Gbps (aggregated)
The layer-level communication performance: 63.252 Gbps (per GPU), 506.019 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.246 Gbps (per GPU), 505.965 Gbps (aggregated)
The layer-level communication performance: 63.241 Gbps (per GPU), 505.931 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.898 Gbps (per GPU), 1327.186 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.903 Gbps (per GPU), 1327.226 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.899 Gbps (per GPU), 1327.193 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.900 Gbps (per GPU), 1327.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.897 Gbps (per GPU), 1327.174 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.902 Gbps (per GPU), 1327.216 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.898 Gbps (per GPU), 1327.183 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.895 Gbps (per GPU), 1327.161 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.331 Gbps (per GPU), 914.644 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.330 Gbps (per GPU), 914.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.328 Gbps (per GPU), 914.622 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.322 Gbps (per GPU), 914.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.323 Gbps (per GPU), 914.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.318 Gbps (per GPU), 914.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.260 Gbps (per GPU), 914.083 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 4): 114.245 Gbps (per GPU), 913.958 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.851 Gbps (per GPU), 366.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.851 Gbps (per GPU), 366.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.851 Gbps (per GPU), 366.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.851 Gbps (per GPU), 366.804 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.851 Gbps (per GPU), 366.806 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.850 Gbps (per GPU), 366.801 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.851 Gbps (per GPU), 366.804 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.849 Gbps (per GPU), 366.788 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.77ms  0.95ms  2.33  5.55K  0.06M
 chk_1  0.42ms  0.80ms  0.98ms  2.35  5.75K  0.05M
 chk_2  0.38ms  0.72ms  0.90ms  2.36  5.20K  0.07M
 chk_3  0.41ms  0.79ms  0.97ms  2.37  5.57K  0.06M
 chk_4  0.37ms  0.70ms  0.88ms  2.36  5.05K  0.08M
 chk_5  0.36ms  0.73ms  0.91ms  2.52  4.73K  0.11M
 chk_6  0.37ms  0.69ms  0.87ms  2.34  4.98K  0.08M
 chk_7  0.37ms  0.71ms  0.88ms  2.38  5.01K  0.09M
 chk_8  0.41ms  0.79ms  0.98ms  2.36  5.73K  0.05M
 chk_9  0.36ms  0.71ms  0.88ms  2.47  4.54K  0.11M
chk_10  0.39ms  0.73ms  0.91ms  2.35  5.36K  0.07M
chk_11  0.39ms  0.74ms  0.92ms  2.37  5.39K  0.08M
chk_12  0.42ms  0.79ms  0.98ms  2.35  5.77K  0.05M
chk_13  0.39ms  0.74ms  0.91ms  2.34  5.43K  0.06M
chk_14  0.41ms  0.77ms  0.94ms  2.32  5.46K  0.06M
chk_15  0.42ms  0.80ms  0.98ms  2.32  5.88K  0.04M
chk_16  0.41ms  0.78ms  0.96ms  2.34  5.50K  0.06M
chk_17  0.37ms  0.73ms  0.91ms  2.47  4.86K  0.09M
chk_18  0.39ms  0.77ms  0.95ms  2.45  5.39K  0.07M
chk_19  0.38ms  0.72ms  0.90ms  2.38  5.20K  0.07M
chk_20  0.41ms  0.77ms  0.94ms  2.30  5.51K  0.06M
chk_21  0.42ms  0.78ms  0.96ms  2.29  5.81K  0.05M
chk_22  0.39ms  0.72ms  0.90ms  2.32  5.32K  0.07M
chk_23  0.39ms  0.76ms  0.94ms  2.41  5.39K  0.07M
chk_24  0.36ms  0.71ms  0.88ms  2.48  4.62K  0.11M
chk_25  0.37ms  0.71ms  0.89ms  2.38  5.04K  0.08M
chk_26  0.36ms  0.71ms  0.88ms  2.48  4.55K  0.11M
chk_27  0.38ms  0.71ms  0.89ms  2.32  5.30K  0.06M
chk_28  0.41ms  0.78ms  0.96ms  2.33  5.58K  0.06M
chk_29  0.37ms  0.72ms  0.90ms  2.43  4.98K  0.09M
chk_30  0.41ms  0.77ms  0.95ms  2.34  5.50K  0.07M
chk_31  0.39ms  0.73ms  0.91ms  2.34  5.38K  0.07M
   Avg  0.39  0.74  0.92
   Max  0.42  0.80  0.98
   Min  0.36  0.69  0.87
 Ratio  1.19  1.16  1.13
   Var  0.00  0.00  0.00
Profiling takes 0.904 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 202.860 ms
Partition 0 [0, 9) has cost: 202.860 ms
Partition 1 [9, 17) has cost: 190.398 ms
Partition 2 [17, 25) has cost: 190.398 ms
Partition 3 [25, 33) has cost: 190.398 ms
Partition 4 [33, 41) has cost: 190.398 ms
Partition 5 [41, 49) has cost: 190.398 ms
Partition 6 [49, 57) has cost: 190.398 ms
Partition 7 [57, 65) has cost: 196.149 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 99.843 ms
GPU 0, Compute+Comm Time: 79.962 ms, Bubble Time: 17.369 ms, Imbalance Overhead: 2.512 ms
GPU 1, Compute+Comm Time: 75.950 ms, Bubble Time: 17.471 ms, Imbalance Overhead: 6.422 ms
GPU 2, Compute+Comm Time: 75.950 ms, Bubble Time: 17.669 ms, Imbalance Overhead: 6.224 ms
GPU 3, Compute+Comm Time: 75.950 ms, Bubble Time: 17.731 ms, Imbalance Overhead: 6.161 ms
GPU 4, Compute+Comm Time: 75.950 ms, Bubble Time: 17.851 ms, Imbalance Overhead: 6.042 ms
GPU 5, Compute+Comm Time: 75.950 ms, Bubble Time: 17.926 ms, Imbalance Overhead: 5.966 ms
GPU 6, Compute+Comm Time: 75.950 ms, Bubble Time: 17.963 ms, Imbalance Overhead: 5.929 ms
GPU 7, Compute+Comm Time: 77.210 ms, Bubble Time: 18.146 ms, Imbalance Overhead: 4.487 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 193.793 ms
GPU 0, Compute+Comm Time: 151.780 ms, Bubble Time: 35.124 ms, Imbalance Overhead: 6.890 ms
GPU 1, Compute+Comm Time: 147.290 ms, Bubble Time: 34.854 ms, Imbalance Overhead: 11.649 ms
GPU 2, Compute+Comm Time: 147.290 ms, Bubble Time: 34.845 ms, Imbalance Overhead: 11.659 ms
GPU 3, Compute+Comm Time: 147.290 ms, Bubble Time: 34.693 ms, Imbalance Overhead: 11.811 ms
GPU 4, Compute+Comm Time: 147.290 ms, Bubble Time: 34.495 ms, Imbalance Overhead: 12.008 ms
GPU 5, Compute+Comm Time: 147.290 ms, Bubble Time: 34.378 ms, Imbalance Overhead: 12.125 ms
GPU 6, Compute+Comm Time: 147.290 ms, Bubble Time: 33.979 ms, Imbalance Overhead: 12.524 ms
GPU 7, Compute+Comm Time: 155.740 ms, Bubble Time: 33.763 ms, Imbalance Overhead: 4.290 ms
The estimated cost of the whole pipeline: 308.318 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 393.259 ms
Partition 0 [0, 17) has cost: 393.259 ms
Partition 1 [17, 33) has cost: 380.797 ms
Partition 2 [33, 49) has cost: 380.797 ms
Partition 3 [49, 65) has cost: 386.547 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 128.577 ms
GPU 0, Compute+Comm Time: 105.496 ms, Bubble Time: 19.807 ms, Imbalance Overhead: 3.273 ms
GPU 1, Compute+Comm Time: 103.437 ms, Bubble Time: 20.157 ms, Imbalance Overhead: 4.983 ms
GPU 2, Compute+Comm Time: 103.437 ms, Bubble Time: 20.332 ms, Imbalance Overhead: 4.807 ms
GPU 3, Compute+Comm Time: 104.071 ms, Bubble Time: 20.512 ms, Imbalance Overhead: 3.994 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 219.771 ms
GPU 0, Compute+Comm Time: 178.707 ms, Bubble Time: 35.022 ms, Imbalance Overhead: 6.041 ms
GPU 1, Compute+Comm Time: 176.436 ms, Bubble Time: 34.795 ms, Imbalance Overhead: 8.541 ms
GPU 2, Compute+Comm Time: 176.436 ms, Bubble Time: 34.515 ms, Imbalance Overhead: 8.820 ms
GPU 3, Compute+Comm Time: 180.770 ms, Bubble Time: 33.911 ms, Imbalance Overhead: 5.090 ms
    The estimated cost with 2 DP ways is 365.765 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 774.055 ms
Partition 0 [0, 33) has cost: 774.055 ms
Partition 1 [33, 65) has cost: 767.343 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 224.570 ms
GPU 0, Compute+Comm Time: 194.245 ms, Bubble Time: 24.320 ms, Imbalance Overhead: 6.005 ms
GPU 1, Compute+Comm Time: 193.511 ms, Bubble Time: 25.102 ms, Imbalance Overhead: 5.958 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 310.775 ms
GPU 0, Compute+Comm Time: 268.034 ms, Bubble Time: 34.759 ms, Imbalance Overhead: 7.982 ms
GPU 1, Compute+Comm Time: 269.098 ms, Bubble Time: 33.717 ms, Imbalance Overhead: 7.960 ms
    The estimated cost with 4 DP ways is 562.112 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1541.399 ms
Partition 0 [0, 65) has cost: 1541.399 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 755.085 ms
GPU 0, Compute+Comm Time: 755.085 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 831.679 ms
GPU 0, Compute+Comm Time: 831.679 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1666.102 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.4495
	Epoch 50:	Loss 4.4501
	Epoch 75:	Loss 3.9078
	Epoch 100:	Loss 3.7828
Node 1, Pre/Post-Pipelining: 3.011 / 34.868 ms, Bubble: 66.215 ms, Compute: 189.589 ms, Comm: 45.123 ms, Imbalance: 30.140 ms
Node 0, Pre/Post-Pipelining: 3.015 / 34.876 ms, Bubble: 67.117 ms, Compute: 201.848 ms, Comm: 35.146 ms, Imbalance: 26.661 ms
Node 2, Pre/Post-Pipelining: 3.020 / 34.918 ms, Bubble: 66.239 ms, Compute: 191.655 ms, Comm: 52.168 ms, Imbalance: 21.442 ms
Node 5, Pre/Post-Pipelining: 3.005 / 34.871 ms, Bubble: 66.298 ms, Compute: 191.187 ms, Comm: 52.787 ms, Imbalance: 21.570 ms
Node 4, Pre/Post-Pipelining: 3.003 / 34.832 ms, Bubble: 65.479 ms, Compute: 188.920 ms, Comm: 56.720 ms, Imbalance: 20.988 ms
Node 7, Pre/Post-Pipelining: 3.006 / 49.233 ms, Bubble: 52.415 ms, Compute: 212.543 ms, Comm: 33.377 ms, Imbalance: 18.698 ms
Node 6, Pre/Post-Pipelining: 3.002 / 34.959 ms, Bubble: 66.013 ms, Compute: 192.943 ms, Comm: 44.281 ms, Imbalance: 28.402 ms
Node 3, Pre/Post-Pipelining: 3.010 / 34.982 ms, Bubble: 64.752 ms, Compute: 193.617 ms, Comm: 56.113 ms, Imbalance: 16.715 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 3.015 ms
Cluster-Wide Average, Post-Pipelining Overhead: 34.876 ms
Cluster-Wide Average, Bubble: 67.117 ms
Cluster-Wide Average, Compute: 201.848 ms
Cluster-Wide Average, Communication: 35.146 ms
Cluster-Wide Average, Imbalance: 26.661 ms
Node 0, GPU memory consumption: 10.508 GB
Node 1, GPU memory consumption: 8.721 GB
Node 2, GPU memory consumption: 8.721 GB
Node 4, GPU memory consumption: 8.698 GB
Node 3, GPU memory consumption: 8.698 GB
Node 5, GPU memory consumption: 8.721 GB
Node 6, GPU memory consumption: 8.721 GB
Node 7, GPU memory consumption: 8.801 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.369340 s---------------
------------------------node id 1,  per-epoch time: 0.369333 s---------------
------------------------node id 2,  per-epoch time: 0.369339 s---------------
------------------------node id 3,  per-epoch time: 0.369342 s---------------
------------------------node id 4,  per-epoch time: 0.369339 s---------------
------------------------node id 5,  per-epoch time: 0.369337 s---------------
------------------------node id 6,  per-epoch time: 0.369339 s---------------
------------------------node id 7,  per-epoch time: 0.369338 s---------------
************ Profiling Results ************
	Bubble: 179.143662 (ms) (47.44 percentage)
	Compute: 190.475710 (ms) (50.44 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 7.989558 (ms) (2.12 percentage)
	Layer-level communication (cluster-wide, per-epoch): 3.391 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.071 GB
	Total communication (cluster-wide, per-epoch): 3.463 GB
	Aggregated layer-level communication throughput: 620.310 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
