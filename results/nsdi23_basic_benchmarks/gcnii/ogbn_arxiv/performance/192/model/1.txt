Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.044 seconds.
        It takes 0.050 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
        It takes 0.055 seconds.
        It takes 0.053 seconds.
        It takes 0.056 seconds.
        It takes 0.055 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 192
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.023 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 5292
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.887 Gbps (per GPU), 511.099 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.847 Gbps (per GPU), 510.777 Gbps (aggregated)
The layer-level communication performance: 63.845 Gbps (per GPU), 510.759 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.811 Gbps (per GPU), 510.488 Gbps (aggregated)
The layer-level communication performance: 63.806 Gbps (per GPU), 510.446 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.777 Gbps (per GPU), 510.213 Gbps (aggregated)
The layer-level communication performance: 63.769 Gbps (per GPU), 510.155 Gbps (aggregated)
The layer-level communication performance: 63.764 Gbps (per GPU), 510.113 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.960 Gbps (per GPU), 1327.679 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.958 Gbps (per GPU), 1327.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.954 Gbps (per GPU), 1327.633 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.959 Gbps (per GPU), 1327.669 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.955 Gbps (per GPU), 1327.636 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.959 Gbps (per GPU), 1327.669 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.950 Gbps (per GPU), 1327.600 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.951 Gbps (per GPU), 1327.611 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.486 Gbps (per GPU), 915.890 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.484 Gbps (per GPU), 915.869 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.485 Gbps (per GPU), 915.882 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.486 Gbps (per GPU), 915.888 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.483 Gbps (per GPU), 915.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.482 Gbps (per GPU), 915.852 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.483 Gbps (per GPU), 915.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.482 Gbps (per GPU), 915.857 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 43.825 Gbps (per GPU), 350.600 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 43.825 Gbps (per GPU), 350.597 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 43.825 Gbps (per GPU), 350.599 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 43.825 Gbps (per GPU), 350.599 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 43.825 Gbps (per GPU), 350.599 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 43.825 Gbps (per GPU), 350.596 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 43.825 Gbps (per GPU), 350.598 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 43.822 Gbps (per GPU), 350.578 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.41ms  0.76ms  0.94ms  2.32  5.55K  0.06M
 chk_1  0.42ms  0.79ms  0.97ms  2.34  5.75K  0.05M
 chk_2  0.38ms  0.72ms  0.90ms  2.36  5.20K  0.07M
 chk_3  0.41ms  0.78ms  0.96ms  2.36  5.57K  0.06M
 chk_4  0.37ms  0.70ms  0.88ms  2.36  5.05K  0.08M
 chk_5  0.36ms  0.73ms  0.90ms  2.51  4.73K  0.11M
 chk_6  0.37ms  0.69ms  0.87ms  2.34  4.98K  0.08M
 chk_7  0.37ms  0.71ms  0.88ms  2.37  5.01K  0.09M
 chk_8  0.42ms  0.79ms  0.97ms  2.33  5.73K  0.05M
 chk_9  0.36ms  0.71ms  0.88ms  2.47  4.54K  0.11M
chk_10  0.38ms  0.72ms  0.90ms  2.35  5.36K  0.07M
chk_11  0.39ms  0.74ms  0.91ms  2.36  5.39K  0.08M
chk_12  0.41ms  0.79ms  0.97ms  2.34  5.77K  0.05M
chk_13  0.39ms  0.73ms  0.91ms  2.36  5.43K  0.06M
chk_14  0.40ms  0.76ms  0.94ms  2.32  5.46K  0.06M
chk_15  0.42ms  0.79ms  0.98ms  2.31  5.88K  0.04M
chk_16  0.41ms  0.77ms  0.95ms  2.33  5.50K  0.06M
chk_17  0.36ms  0.73ms  0.90ms  2.47  4.86K  0.09M
chk_18  0.39ms  0.77ms  0.95ms  2.46  5.39K  0.07M
chk_19  0.38ms  0.72ms  0.90ms  2.37  5.20K  0.07M
chk_20  0.41ms  0.76ms  0.94ms  2.30  5.51K  0.06M
chk_21  0.42ms  0.78ms  0.96ms  2.29  5.81K  0.05M
chk_22  0.38ms  0.71ms  0.89ms  2.33  5.32K  0.07M
chk_23  0.39ms  0.75ms  0.93ms  2.39  5.39K  0.07M
chk_24  0.36ms  0.71ms  0.88ms  2.46  4.62K  0.11M
chk_25  0.37ms  0.71ms  0.89ms  2.38  5.04K  0.08M
chk_26  0.35ms  0.71ms  0.88ms  2.47  4.55K  0.11M
chk_27  0.38ms  0.70ms  0.88ms  2.30  5.30K  0.06M
chk_28  0.41ms  0.77ms  0.95ms  2.33  5.58K  0.06M
chk_29  0.37ms  0.72ms  0.90ms  2.41  4.98K  0.09M
chk_30  0.41ms  0.77ms  0.95ms  2.32  5.50K  0.07M
chk_31  0.39ms  0.72ms  0.90ms  2.34  5.38K  0.07M
   Avg  0.39  0.74  0.92
   Max  0.42  0.79  0.98
   Min  0.35  0.69  0.87
 Ratio  1.19  1.14  1.13
   Var  0.00  0.00  0.00
Profiling takes 0.906 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 202.113 ms
Partition 0 [0, 9) has cost: 202.113 ms
Partition 1 [9, 17) has cost: 189.679 ms
Partition 2 [17, 25) has cost: 189.679 ms
Partition 3 [25, 33) has cost: 189.679 ms
Partition 4 [33, 41) has cost: 189.679 ms
Partition 5 [41, 49) has cost: 189.679 ms
Partition 6 [49, 57) has cost: 189.679 ms
Partition 7 [57, 65) has cost: 195.367 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 99.090 ms
GPU 0, Compute+Comm Time: 79.584 ms, Bubble Time: 17.211 ms, Imbalance Overhead: 2.295 ms
GPU 1, Compute+Comm Time: 75.584 ms, Bubble Time: 17.327 ms, Imbalance Overhead: 6.178 ms
GPU 2, Compute+Comm Time: 75.584 ms, Bubble Time: 17.528 ms, Imbalance Overhead: 5.978 ms
GPU 3, Compute+Comm Time: 75.584 ms, Bubble Time: 17.595 ms, Imbalance Overhead: 5.911 ms
GPU 4, Compute+Comm Time: 75.584 ms, Bubble Time: 17.724 ms, Imbalance Overhead: 5.781 ms
GPU 5, Compute+Comm Time: 75.584 ms, Bubble Time: 17.803 ms, Imbalance Overhead: 5.703 ms
GPU 6, Compute+Comm Time: 75.584 ms, Bubble Time: 17.842 ms, Imbalance Overhead: 5.663 ms
GPU 7, Compute+Comm Time: 76.827 ms, Bubble Time: 18.016 ms, Imbalance Overhead: 4.247 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 192.229 ms
GPU 0, Compute+Comm Time: 151.111 ms, Bubble Time: 34.927 ms, Imbalance Overhead: 6.191 ms
GPU 1, Compute+Comm Time: 146.666 ms, Bubble Time: 34.656 ms, Imbalance Overhead: 10.908 ms
GPU 2, Compute+Comm Time: 146.666 ms, Bubble Time: 34.626 ms, Imbalance Overhead: 10.937 ms
GPU 3, Compute+Comm Time: 146.666 ms, Bubble Time: 34.456 ms, Imbalance Overhead: 11.108 ms
GPU 4, Compute+Comm Time: 146.666 ms, Bubble Time: 34.216 ms, Imbalance Overhead: 11.348 ms
GPU 5, Compute+Comm Time: 146.666 ms, Bubble Time: 34.045 ms, Imbalance Overhead: 11.518 ms
GPU 6, Compute+Comm Time: 146.666 ms, Bubble Time: 33.593 ms, Imbalance Overhead: 11.971 ms
GPU 7, Compute+Comm Time: 155.100 ms, Bubble Time: 33.335 ms, Imbalance Overhead: 3.794 ms
The estimated cost of the whole pipeline: 305.885 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 391.792 ms
Partition 0 [0, 17) has cost: 391.792 ms
Partition 1 [17, 33) has cost: 379.357 ms
Partition 2 [33, 49) has cost: 379.357 ms
Partition 3 [49, 65) has cost: 385.045 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 127.831 ms
GPU 0, Compute+Comm Time: 105.054 ms, Bubble Time: 19.660 ms, Imbalance Overhead: 3.117 ms
GPU 1, Compute+Comm Time: 103.003 ms, Bubble Time: 20.016 ms, Imbalance Overhead: 4.812 ms
GPU 2, Compute+Comm Time: 103.003 ms, Bubble Time: 20.201 ms, Imbalance Overhead: 4.627 ms
GPU 3, Compute+Comm Time: 103.627 ms, Bubble Time: 20.386 ms, Imbalance Overhead: 3.818 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 218.218 ms
GPU 0, Compute+Comm Time: 177.879 ms, Bubble Time: 34.854 ms, Imbalance Overhead: 5.485 ms
GPU 1, Compute+Comm Time: 175.628 ms, Bubble Time: 34.587 ms, Imbalance Overhead: 8.003 ms
GPU 2, Compute+Comm Time: 175.628 ms, Bubble Time: 34.216 ms, Imbalance Overhead: 8.374 ms
GPU 3, Compute+Comm Time: 179.958 ms, Bubble Time: 33.526 ms, Imbalance Overhead: 4.734 ms
    The estimated cost with 2 DP ways is 363.352 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 771.149 ms
Partition 0 [0, 33) has cost: 771.149 ms
Partition 1 [33, 65) has cost: 764.403 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 223.682 ms
GPU 0, Compute+Comm Time: 193.558 ms, Bubble Time: 24.203 ms, Imbalance Overhead: 5.921 ms
GPU 1, Compute+Comm Time: 192.827 ms, Bubble Time: 24.997 ms, Imbalance Overhead: 5.857 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 309.092 ms
GPU 0, Compute+Comm Time: 267.002 ms, Bubble Time: 34.640 ms, Imbalance Overhead: 7.451 ms
GPU 1, Compute+Comm Time: 268.074 ms, Bubble Time: 33.420 ms, Imbalance Overhead: 7.599 ms
    The estimated cost with 4 DP ways is 559.413 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 1535.552 ms
Partition 0 [0, 65) has cost: 1535.552 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 786.559 ms
GPU 0, Compute+Comm Time: 786.559 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 862.356 ms
GPU 0, Compute+Comm Time: 862.356 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1731.361 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.1914
	Epoch 50:	Loss 4.1121
	Epoch 75:	Loss 3.8918
	Epoch 100:	Loss 4.0546
Node 1, Pre/Post-Pipelining: 3.026 / 34.819 ms, Bubble: 65.577 ms, Compute: 189.400 ms, Comm: 44.926 ms, Imbalance: 29.559 ms
Node 5, Pre/Post-Pipelining: 3.003 / 34.779 ms, Bubble: 65.671 ms, Compute: 188.668 ms, Comm: 53.004 ms, Imbalance: 23.071 ms
Node 0, Pre/Post-Pipelining: 3.020 / 34.786 ms, Bubble: 66.614 ms, Compute: 200.745 ms, Comm: 35.067 ms, Imbalance: 26.841 ms
Node 2, Pre/Post-Pipelining: 3.032 / 34.886 ms, Bubble: 65.494 ms, Compute: 192.367 ms, Comm: 51.985 ms, Imbalance: 20.056 ms
Node 4, Pre/Post-Pipelining: 3.000 / 34.846 ms, Bubble: 64.549 ms, Compute: 190.205 ms, Comm: 56.671 ms, Imbalance: 19.010 ms
Node 7, Pre/Post-Pipelining: 3.005 / 48.949 ms, Bubble: 52.215 ms, Compute: 208.078 ms, Comm: 32.965 ms, Imbalance: 22.513 ms
Node 6, Pre/Post-Pipelining: 3.002 / 34.867 ms, Bubble: 65.534 ms, Compute: 189.727 ms, Comm: 44.239 ms, Imbalance: 30.732 ms
Node 3, Pre/Post-Pipelining: 3.027 / 34.951 ms, Bubble: 63.898 ms, Compute: 193.962 ms, Comm: 56.053 ms, Imbalance: 15.677 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 3.020 ms
Cluster-Wide Average, Post-Pipelining Overhead: 34.786 ms
Cluster-Wide Average, Bubble: 66.614 ms
Cluster-Wide Average, Compute: 200.745 ms
Cluster-Wide Average, Communication: 35.067 ms
Cluster-Wide Average, Imbalance: 26.841 ms
Node 0, GPU memory consumption: 10.508 GB
Node 1, GPU memory consumption: 8.721 GB
Node 4, GPU memory consumption: 8.698 GB
Node 3, GPU memory consumption: 8.698 GB
Node 5, GPU memory consumption: 8.721 GB
Node 6, GPU memory consumption: 8.721 GB
Node 7, GPU memory consumption: 8.801 GB
Node 2, GPU memory consumption: 8.721 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.367743 s---------------
------------------------node id 1,  per-epoch time: 0.367742 s---------------
------------------------node id 4,  per-epoch time: 0.367749 s---------------
------------------------node id 2,  per-epoch time: 0.367748 s---------------
------------------------node id 5,  per-epoch time: 0.367741 s---------------
------------------------node id 3,  per-epoch time: 0.367749 s---------------
------------------------node id 6,  per-epoch time: 0.367750 s---------------
------------------------node id 7,  per-epoch time: 0.367750 s---------------
************ Profiling Results ************
	Bubble: 179.203233 (ms) (47.59 percentage)
	Compute: 189.378159 (ms) (50.30 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 7.937514 (ms) (2.11 percentage)
	Layer-level communication (cluster-wide, per-epoch): 3.391 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.071 GB
	Total communication (cluster-wide, per-epoch): 3.463 GB
	Aggregated layer-level communication throughput: 621.639 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
