Initialized node 5 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.039 seconds.
        It takes 0.042 seconds.
        It takes 0.044 seconds.
        It takes 0.051 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
        It takes 0.055 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.834 Gbps (per GPU), 510.674 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.807 Gbps (per GPU), 510.455 Gbps (aggregated)
The layer-level communication performance: 63.792 Gbps (per GPU), 510.338 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.762 Gbps (per GPU), 510.092 Gbps (aggregated)
The layer-level communication performance: 63.756 Gbps (per GPU), 510.052 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.724 Gbps (per GPU), 509.793 Gbps (aggregated)
The layer-level communication performance: 63.717 Gbps (per GPU), 509.736 Gbps (aggregated)
The layer-level communication performance: 63.712 Gbps (per GPU), 509.699 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.697 Gbps (per GPU), 1317.575 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.692 Gbps (per GPU), 1317.536 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.693 Gbps (per GPU), 1317.543 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.693 Gbps (per GPU), 1317.546 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.692 Gbps (per GPU), 1317.533 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.694 Gbps (per GPU), 1317.555 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.693 Gbps (per GPU), 1317.546 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.690 Gbps (per GPU), 1317.520 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.917 Gbps (per GPU), 911.338 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.918 Gbps (per GPU), 911.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.914 Gbps (per GPU), 911.311 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.911 Gbps (per GPU), 911.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.908 Gbps (per GPU), 911.266 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.912 Gbps (per GPU), 911.298 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.911 Gbps (per GPU), 911.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.902 Gbps (per GPU), 911.216 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.719 Gbps (per GPU), 365.750 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.719 Gbps (per GPU), 365.752 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.718 Gbps (per GPU), 365.746 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.718 Gbps (per GPU), 365.747 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.718 Gbps (per GPU), 365.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.719 Gbps (per GPU), 365.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.719 Gbps (per GPU), 365.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.718 Gbps (per GPU), 365.743 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.71ms  1.33ms  1.60ms  2.26 21.17K  0.46M
 chk_1  0.71ms  1.40ms  1.68ms  2.36 21.17K  0.55M
 chk_2  0.71ms  1.29ms  1.57ms  2.21 21.17K  0.39M
 chk_3  0.71ms  1.25ms  1.52ms  2.15 21.17K  0.24M
 chk_4  0.71ms  1.20ms  1.47ms  2.07 21.17K  0.17M
 chk_5  0.71ms  1.21ms  1.48ms  2.08 21.17K  0.22M
 chk_6  0.71ms  1.21ms  1.48ms  2.10 21.17K  0.16M
 chk_7  0.71ms  1.19ms  1.47ms  2.07 21.17K  0.12M
   Avg  0.71  1.26  1.53
   Max  0.71  1.40  1.68
   Min  0.71  1.19  1.47
 Ratio  1.01  1.17  1.14
   Var  0.00  0.00  0.01
Profiling takes 0.415 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 45.992 ms
Partition 0 [0, 5) has cost: 45.992 ms
Partition 1 [5, 9) has cost: 40.313 ms
Partition 2 [9, 13) has cost: 40.313 ms
Partition 3 [13, 17) has cost: 40.313 ms
Partition 4 [17, 21) has cost: 40.313 ms
Partition 5 [21, 25) has cost: 40.313 ms
Partition 6 [25, 29) has cost: 40.313 ms
Partition 7 [29, 33) has cost: 42.508 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.486 ms
GPU 0, Compute+Comm Time: 22.594 ms, Bubble Time: 18.672 ms, Imbalance Overhead: 0.220 ms
GPU 1, Compute+Comm Time: 20.971 ms, Bubble Time: 18.719 ms, Imbalance Overhead: 1.796 ms
GPU 2, Compute+Comm Time: 20.971 ms, Bubble Time: 19.002 ms, Imbalance Overhead: 1.513 ms
GPU 3, Compute+Comm Time: 20.971 ms, Bubble Time: 19.202 ms, Imbalance Overhead: 1.313 ms
GPU 4, Compute+Comm Time: 20.971 ms, Bubble Time: 19.411 ms, Imbalance Overhead: 1.104 ms
GPU 5, Compute+Comm Time: 20.971 ms, Bubble Time: 19.604 ms, Imbalance Overhead: 0.912 ms
GPU 6, Compute+Comm Time: 20.971 ms, Bubble Time: 19.794 ms, Imbalance Overhead: 0.721 ms
GPU 7, Compute+Comm Time: 21.441 ms, Bubble Time: 20.010 ms, Imbalance Overhead: 0.035 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 73.594 ms
GPU 0, Compute+Comm Time: 38.045 ms, Bubble Time: 35.549 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.321 ms, Bubble Time: 35.214 ms, Imbalance Overhead: 2.059 ms
GPU 2, Compute+Comm Time: 36.321 ms, Bubble Time: 34.921 ms, Imbalance Overhead: 2.352 ms
GPU 3, Compute+Comm Time: 36.321 ms, Bubble Time: 34.613 ms, Imbalance Overhead: 2.659 ms
GPU 4, Compute+Comm Time: 36.321 ms, Bubble Time: 34.206 ms, Imbalance Overhead: 3.067 ms
GPU 5, Compute+Comm Time: 36.321 ms, Bubble Time: 33.791 ms, Imbalance Overhead: 3.482 ms
GPU 6, Compute+Comm Time: 36.321 ms, Bubble Time: 33.210 ms, Imbalance Overhead: 4.062 ms
GPU 7, Compute+Comm Time: 40.377 ms, Bubble Time: 33.090 ms, Imbalance Overhead: 0.127 ms
The estimated cost of the whole pipeline: 120.834 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 86.306 ms
Partition 0 [0, 9) has cost: 86.306 ms
Partition 1 [9, 17) has cost: 80.627 ms
Partition 2 [17, 25) has cost: 80.627 ms
Partition 3 [25, 33) has cost: 82.821 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 49.933 ms
GPU 0, Compute+Comm Time: 28.548 ms, Bubble Time: 20.658 ms, Imbalance Overhead: 0.727 ms
GPU 1, Compute+Comm Time: 27.735 ms, Bubble Time: 21.080 ms, Imbalance Overhead: 1.118 ms
GPU 2, Compute+Comm Time: 27.735 ms, Bubble Time: 21.522 ms, Imbalance Overhead: 0.676 ms
GPU 3, Compute+Comm Time: 27.970 ms, Bubble Time: 21.963 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 79.030 ms
GPU 0, Compute+Comm Time: 44.109 ms, Bubble Time: 34.921 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 43.243 ms, Bubble Time: 34.119 ms, Imbalance Overhead: 1.668 ms
GPU 2, Compute+Comm Time: 43.243 ms, Bubble Time: 33.288 ms, Imbalance Overhead: 2.499 ms
GPU 3, Compute+Comm Time: 45.272 ms, Bubble Time: 32.419 ms, Imbalance Overhead: 1.338 ms
    The estimated cost with 2 DP ways is 135.411 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 166.933 ms
Partition 0 [0, 17) has cost: 166.933 ms
Partition 1 [17, 33) has cost: 163.448 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 75.786 ms
GPU 0, Compute+Comm Time: 50.384 ms, Bubble Time: 24.548 ms, Imbalance Overhead: 0.854 ms
GPU 1, Compute+Comm Time: 50.095 ms, Bubble Time: 25.692 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 100.678 ms
GPU 0, Compute+Comm Time: 66.246 ms, Bubble Time: 34.432 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 66.827 ms, Bubble Time: 32.104 ms, Imbalance Overhead: 1.747 ms
    The estimated cost with 4 DP ways is 185.287 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 330.380 ms
Partition 0 [0, 33) has cost: 330.380 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 180.148 ms
GPU 0, Compute+Comm Time: 180.148 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 197.482 ms
GPU 0, Compute+Comm Time: 197.482 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 396.511 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.1289
	Epoch 50:	Loss 2.9024
	Epoch 75:	Loss 2.6558
	Epoch 100:	Loss 2.4311
Node 5, Pre/Post-Pipelining: 6.363 / 13.220 ms, Bubble: 0.628 ms, Compute: 148.395 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 1, Pre/Post-Pipelining: 6.369 / 13.242 ms, Bubble: 0.346 ms, Compute: 148.645 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 7, Pre/Post-Pipelining: 6.365 / 13.223 ms, Bubble: 0.581 ms, Compute: 148.430 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 0, Pre/Post-Pipelining: 6.366 / 13.242 ms, Bubble: 0.469 ms, Compute: 148.526 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 4, Pre/Post-Pipelining: 6.373 / 13.287 ms, Bubble: 0.424 ms, Compute: 148.514 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 6.370 / 13.249 ms, Bubble: 0.447 ms, Compute: 148.537 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 6, Pre/Post-Pipelining: 6.373 / 13.292 ms, Bubble: 0.234 ms, Compute: 148.704 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 3, Pre/Post-Pipelining: 6.375 / 13.312 ms, Bubble: 0.039 ms, Compute: 148.875 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.366 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.242 ms
Cluster-Wide Average, Bubble: 0.469 ms
Cluster-Wide Average, Compute: 148.526 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.015 ms
Node 0, GPU memory consumption: 15.387 GB
Node 1, GPU memory consumption: 14.751 GB
Node 2, GPU memory consumption: 14.749 GB
Node 5, GPU memory consumption: 14.749 GB
Node 3, GPU memory consumption: 14.725 GB
Node 6, GPU memory consumption: 14.749 GB
Node 4, GPU memory consumption: 14.725 GB
Node 7, GPU memory consumption: 14.725 GB
Node 0, Graph-Level Communication Throughput: 51.277 Gbps, Time: 88.028 ms
Node 1, Graph-Level Communication Throughput: 63.545 Gbps, Time: 84.492 ms
Node 2, Graph-Level Communication Throughput: 42.704 Gbps, Time: 88.957 ms
Node 3, Graph-Level Communication Throughput: 44.410 Gbps, Time: 89.239 ms
Node 4, Graph-Level Communication Throughput: 21.256 Gbps, Time: 91.733 ms
Node 5, Graph-Level Communication Throughput: 10.627 Gbps, Time: 93.312 ms
Node 6, Graph-Level Communication Throughput: 19.079 Gbps, Time: 91.673 ms
Node 7, Graph-Level Communication Throughput: 11.513 Gbps, Time: 93.315 ms
------------------------node id 0,  per-epoch time: 0.168674 s---------------
------------------------node id 1,  per-epoch time: 0.168674 s---------------
------------------------node id 2,  per-epoch time: 0.168675 s---------------
------------------------node id 3,  per-epoch time: 0.168676 s---------------
------------------------node id 4,  per-epoch time: 0.168674 s---------------
------------------------node id 5,  per-epoch time: 0.168672 s---------------
------------------------node id 6,  per-epoch time: 0.168676 s---------------
------------------------node id 7,  per-epoch time: 0.168674 s---------------
************ Profiling Results ************
	Bubble: 20.663513 (ms) (12.13 percentage)
	Compute: 41.517307 (ms) (24.37 percentage)
	GraphCommComputeOverhead: 5.497568 (ms) (3.23 percentage)
	GraphCommNetwork: 90.093795 (ms) (52.89 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.581359 (ms) (7.39 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 2.725 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 2.735 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
