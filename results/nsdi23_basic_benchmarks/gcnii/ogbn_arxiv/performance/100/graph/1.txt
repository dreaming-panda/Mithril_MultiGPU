Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.042 seconds.
        It takes 0.040 seconds.
        It takes 0.047 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
        It takes 0.055 seconds.
        It takes 0.054 seconds.
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.996 Gbps (per GPU), 511.968 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.955 Gbps (per GPU), 511.643 Gbps (aggregated)
The layer-level communication performance: 63.954 Gbps (per GPU), 511.632 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.917 Gbps (per GPU), 511.335 Gbps (aggregated)
The layer-level communication performance: 63.912 Gbps (per GPU), 511.296 Gbps (aggregated)
The layer-level communication performance: 63.882 Gbps (per GPU), 511.058 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.875 Gbps (per GPU), 511.001 Gbps (aggregated)
The layer-level communication performance: 63.870 Gbps (per GPU), 510.962 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.753 Gbps (per GPU), 1318.021 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.755 Gbps (per GPU), 1318.041 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.752 Gbps (per GPU), 1318.015 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.741 Gbps (per GPU), 1317.931 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.749 Gbps (per GPU), 1317.995 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.752 Gbps (per GPU), 1318.018 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.752 Gbps (per GPU), 1318.015 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.751 Gbps (per GPU), 1318.008 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.012 Gbps (per GPU), 912.096 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.011 Gbps (per GPU), 912.086 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.012 Gbps (per GPU), 912.099 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.010 Gbps (per GPU), 912.077 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.008 Gbps (per GPU), 912.064 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.010 Gbps (per GPU), 912.082 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.007 Gbps (per GPU), 912.052 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.009 Gbps (per GPU), 912.069 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.793 Gbps (per GPU), 366.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.793 Gbps (per GPU), 366.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.793 Gbps (per GPU), 366.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.793 Gbps (per GPU), 366.343 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.793 Gbps (per GPU), 366.343 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.793 Gbps (per GPU), 366.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.793 Gbps (per GPU), 366.341 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.793 Gbps (per GPU), 366.343 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.71ms  1.33ms  1.60ms  2.26 21.17K  0.46M
 chk_1  0.71ms  1.40ms  1.67ms  2.36 21.17K  0.55M
 chk_2  0.71ms  1.29ms  1.56ms  2.20 21.17K  0.39M
 chk_3  0.71ms  1.25ms  1.52ms  2.14 21.17K  0.24M
 chk_4  0.71ms  1.20ms  1.47ms  2.07 21.17K  0.17M
 chk_5  0.71ms  1.20ms  1.48ms  2.08 21.17K  0.22M
 chk_6  0.71ms  1.21ms  1.48ms  2.09 21.17K  0.16M
 chk_7  0.71ms  1.19ms  1.46ms  2.06 21.17K  0.12M
   Avg  0.71  1.26  1.53
   Max  0.71  1.40  1.67
   Min  0.71  1.19  1.46
 Ratio  1.00  1.18  1.14
   Var  0.00  0.01  0.01
Profiling takes 0.412 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 45.949 ms
Partition 0 [0, 5) has cost: 45.949 ms
Partition 1 [5, 9) has cost: 40.274 ms
Partition 2 [9, 13) has cost: 40.274 ms
Partition 3 [13, 17) has cost: 40.274 ms
Partition 4 [17, 21) has cost: 40.274 ms
Partition 5 [21, 25) has cost: 40.274 ms
Partition 6 [25, 29) has cost: 40.274 ms
Partition 7 [29, 33) has cost: 42.460 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.430 ms
GPU 0, Compute+Comm Time: 22.551 ms, Bubble Time: 18.640 ms, Imbalance Overhead: 0.239 ms
GPU 1, Compute+Comm Time: 20.927 ms, Bubble Time: 18.685 ms, Imbalance Overhead: 1.819 ms
GPU 2, Compute+Comm Time: 20.927 ms, Bubble Time: 18.971 ms, Imbalance Overhead: 1.532 ms
GPU 3, Compute+Comm Time: 20.927 ms, Bubble Time: 19.173 ms, Imbalance Overhead: 1.331 ms
GPU 4, Compute+Comm Time: 20.927 ms, Bubble Time: 19.381 ms, Imbalance Overhead: 1.122 ms
GPU 5, Compute+Comm Time: 20.927 ms, Bubble Time: 19.581 ms, Imbalance Overhead: 0.922 ms
GPU 6, Compute+Comm Time: 20.927 ms, Bubble Time: 19.773 ms, Imbalance Overhead: 0.731 ms
GPU 7, Compute+Comm Time: 21.407 ms, Bubble Time: 19.988 ms, Imbalance Overhead: 0.036 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 73.544 ms
GPU 0, Compute+Comm Time: 37.988 ms, Bubble Time: 35.556 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.282 ms, Bubble Time: 35.205 ms, Imbalance Overhead: 2.057 ms
GPU 2, Compute+Comm Time: 36.282 ms, Bubble Time: 34.898 ms, Imbalance Overhead: 2.364 ms
GPU 3, Compute+Comm Time: 36.282 ms, Bubble Time: 34.571 ms, Imbalance Overhead: 2.691 ms
GPU 4, Compute+Comm Time: 36.282 ms, Bubble Time: 34.153 ms, Imbalance Overhead: 3.109 ms
GPU 5, Compute+Comm Time: 36.282 ms, Bubble Time: 33.749 ms, Imbalance Overhead: 3.513 ms
GPU 6, Compute+Comm Time: 36.282 ms, Bubble Time: 33.153 ms, Imbalance Overhead: 4.109 ms
GPU 7, Compute+Comm Time: 40.333 ms, Bubble Time: 33.043 ms, Imbalance Overhead: 0.168 ms
The estimated cost of the whole pipeline: 120.723 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 86.223 ms
Partition 0 [0, 9) has cost: 86.223 ms
Partition 1 [9, 17) has cost: 80.548 ms
Partition 2 [17, 25) has cost: 80.548 ms
Partition 3 [25, 33) has cost: 82.734 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 49.883 ms
GPU 0, Compute+Comm Time: 28.501 ms, Bubble Time: 20.619 ms, Imbalance Overhead: 0.763 ms
GPU 1, Compute+Comm Time: 27.688 ms, Bubble Time: 21.051 ms, Imbalance Overhead: 1.144 ms
GPU 2, Compute+Comm Time: 27.688 ms, Bubble Time: 21.512 ms, Imbalance Overhead: 0.683 ms
GPU 3, Compute+Comm Time: 27.929 ms, Bubble Time: 21.954 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 79.014 ms
GPU 0, Compute+Comm Time: 44.056 ms, Bubble Time: 34.957 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 43.200 ms, Bubble Time: 34.130 ms, Imbalance Overhead: 1.683 ms
GPU 2, Compute+Comm Time: 43.200 ms, Bubble Time: 33.262 ms, Imbalance Overhead: 2.552 ms
GPU 3, Compute+Comm Time: 45.226 ms, Bubble Time: 32.360 ms, Imbalance Overhead: 1.428 ms
    The estimated cost with 2 DP ways is 135.342 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 166.771 ms
Partition 0 [0, 17) has cost: 166.771 ms
Partition 1 [17, 33) has cost: 163.281 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 75.731 ms
GPU 0, Compute+Comm Time: 50.344 ms, Bubble Time: 24.526 ms, Imbalance Overhead: 0.860 ms
GPU 1, Compute+Comm Time: 50.057 ms, Bubble Time: 25.674 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 100.663 ms
GPU 0, Compute+Comm Time: 66.220 ms, Bubble Time: 34.444 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 66.807 ms, Bubble Time: 32.070 ms, Imbalance Overhead: 1.787 ms
    The estimated cost with 4 DP ways is 185.214 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 330.052 ms
Partition 0 [0, 33) has cost: 330.052 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 179.891 ms
GPU 0, Compute+Comm Time: 179.891 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 197.280 ms
GPU 0, Compute+Comm Time: 197.280 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 396.029 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.0993
	Epoch 50:	Loss 2.8096
	Epoch 75:	Loss 2.5193
	Epoch 100:	Loss 2.3366
Node 1, Pre/Post-Pipelining: 6.368 / 13.500 ms, Bubble: 0.403 ms, Compute: 148.663 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 0, Pre/Post-Pipelining: 6.366 / 13.496 ms, Bubble: 0.532 ms, Compute: 148.540 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 5, Pre/Post-Pipelining: 6.365 / 13.498 ms, Bubble: 0.591 ms, Compute: 148.482 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 4, Pre/Post-Pipelining: 6.370 / 13.501 ms, Bubble: 0.470 ms, Compute: 148.597 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 7, Pre/Post-Pipelining: 6.369 / 13.530 ms, Bubble: 0.618 ms, Compute: 148.411 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 6.375 / 13.562 ms, Bubble: 0.221 ms, Compute: 148.770 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 3, Pre/Post-Pipelining: 6.375 / 13.544 ms, Bubble: 0.064 ms, Compute: 148.956 ms, Comm: 0.008 ms, Imbalance: 0.013 ms
Node 2, Pre/Post-Pipelining: 6.368 / 13.501 ms, Bubble: 0.506 ms, Compute: 148.561 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.366 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.496 ms
Cluster-Wide Average, Bubble: 0.532 ms
Cluster-Wide Average, Compute: 148.540 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.015 ms
Node 0, GPU memory consumption: 15.387 GB
Node 1, GPU memory consumption: 14.751 GB
Node 3, GPU memory consumption: 14.725 GB
Node 4, GPU memory consumption: 14.725 GB
Node 7, GPU memory consumption: 14.725 GB
Node 2, GPU memory consumption: 14.749 GB
Node 5, GPU memory consumption: 14.749 GB
Node 6, GPU memory consumption: 14.749 GB
Node 0, Graph-Level Communication Throughput: 51.169 Gbps, Time: 88.213 ms
Node 1, Graph-Level Communication Throughput: 63.398 Gbps, Time: 84.688 ms
Node 4, Graph-Level Communication Throughput: 21.103 Gbps, Time: 92.401 ms
Node 2, Graph-Level Communication Throughput: 42.576 Gbps, Time: 89.226 ms
Node 3, Graph-Level Communication Throughput: 44.186 Gbps, Time: 89.690 ms
Node 5, Graph-Level Communication Throughput: 10.631 Gbps, Time: 93.277 ms
Node 6, Graph-Level Communication Throughput: 19.093 Gbps, Time: 91.602 ms
Node 7, Graph-Level Communication Throughput: 11.585 Gbps, Time: 92.740 ms
------------------------node id 0,  per-epoch time: 0.169005 s---------------
------------------------node id 1,  per-epoch time: 0.169005 s---------------
------------------------node id 2,  per-epoch time: 0.169008 s---------------
------------------------node id 3,  per-epoch time: 0.169007 s---------------
------------------------node id 4,  per-epoch time: 0.169006 s---------------
------------------------node id 5,  per-epoch time: 0.169004 s---------------
------------------------node id 6,  per-epoch time: 0.169005 s---------------
------------------------node id 7,  per-epoch time: 0.169006 s---------------
************ Profiling Results ************
	Bubble: 20.962325 (ms) (12.28 percentage)
	Compute: 41.450490 (ms) (24.28 percentage)
	GraphCommComputeOverhead: 5.505697 (ms) (3.22 percentage)
	GraphCommNetwork: 90.230075 (ms) (52.85 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.585101 (ms) (7.37 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 2.725 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 2.735 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
