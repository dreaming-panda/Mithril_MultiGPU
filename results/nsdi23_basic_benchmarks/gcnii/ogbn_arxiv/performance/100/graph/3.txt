Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.043 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.040 seconds.
        It takes 0.046 seconds.
        It takes 0.049 seconds.
Building the Feature Vector...
        It takes 0.053 seconds.
        It takes 0.052 seconds.
        It takes 0.052 seconds.
        It takes 0.054 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.054 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.855 Gbps (per GPU), 510.837 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.812 Gbps (per GPU), 510.499 Gbps (aggregated)
The layer-level communication performance: 63.810 Gbps (per GPU), 510.483 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.776 Gbps (per GPU), 510.212 Gbps (aggregated)
The layer-level communication performance: 63.772 Gbps (per GPU), 510.176 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.741 Gbps (per GPU), 509.925 Gbps (aggregated)
The layer-level communication performance: 63.729 Gbps (per GPU), 509.829 Gbps (aggregated)
The layer-level communication performance: 63.733 Gbps (per GPU), 509.867 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.952 Gbps (per GPU), 1327.620 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.964 Gbps (per GPU), 1327.712 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.954 Gbps (per GPU), 1327.633 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.959 Gbps (per GPU), 1327.672 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.957 Gbps (per GPU), 1327.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.955 Gbps (per GPU), 1327.637 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.953 Gbps (per GPU), 1327.627 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.952 Gbps (per GPU), 1327.617 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.275 Gbps (per GPU), 914.201 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.277 Gbps (per GPU), 914.219 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.277 Gbps (per GPU), 914.213 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.272 Gbps (per GPU), 914.176 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.277 Gbps (per GPU), 914.213 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.261 Gbps (per GPU), 914.091 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.264 Gbps (per GPU), 914.115 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.253 Gbps (per GPU), 914.021 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.459 Gbps (per GPU), 363.671 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.459 Gbps (per GPU), 363.669 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.459 Gbps (per GPU), 363.670 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.459 Gbps (per GPU), 363.672 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.459 Gbps (per GPU), 363.669 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.459 Gbps (per GPU), 363.670 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.458 Gbps (per GPU), 363.668 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.458 Gbps (per GPU), 363.663 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.71ms  1.33ms  1.60ms  2.26 21.17K  0.46M
 chk_1  0.71ms  1.40ms  1.67ms  2.36 21.17K  0.55M
 chk_2  0.71ms  1.29ms  1.56ms  2.21 21.17K  0.39M
 chk_3  0.71ms  1.24ms  1.52ms  2.15 21.17K  0.24M
 chk_4  0.71ms  1.20ms  1.47ms  2.08 21.17K  0.17M
 chk_5  0.71ms  1.20ms  1.48ms  2.09 21.17K  0.22M
 chk_6  0.71ms  1.24ms  1.48ms  2.10 21.17K  0.16M
 chk_7  0.71ms  1.19ms  1.46ms  2.07 21.17K  0.12M
   Avg  0.71  1.26  1.53
   Max  0.71  1.40  1.67
   Min  0.71  1.19  1.46
 Ratio  1.00  1.18  1.14
   Var  0.00  0.00  0.00
Profiling takes 0.413 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 45.988 ms
Partition 0 [0, 5) has cost: 45.988 ms
Partition 1 [5, 9) has cost: 40.329 ms
Partition 2 [9, 13) has cost: 40.329 ms
Partition 3 [13, 17) has cost: 40.329 ms
Partition 4 [17, 21) has cost: 40.329 ms
Partition 5 [21, 25) has cost: 40.329 ms
Partition 6 [25, 29) has cost: 40.329 ms
Partition 7 [29, 33) has cost: 42.496 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 41.404 ms
GPU 0, Compute+Comm Time: 22.534 ms, Bubble Time: 18.629 ms, Imbalance Overhead: 0.242 ms
GPU 1, Compute+Comm Time: 20.917 ms, Bubble Time: 18.676 ms, Imbalance Overhead: 1.811 ms
GPU 2, Compute+Comm Time: 20.917 ms, Bubble Time: 18.959 ms, Imbalance Overhead: 1.528 ms
GPU 3, Compute+Comm Time: 20.917 ms, Bubble Time: 19.163 ms, Imbalance Overhead: 1.324 ms
GPU 4, Compute+Comm Time: 20.917 ms, Bubble Time: 19.368 ms, Imbalance Overhead: 1.119 ms
GPU 5, Compute+Comm Time: 20.917 ms, Bubble Time: 19.566 ms, Imbalance Overhead: 0.921 ms
GPU 6, Compute+Comm Time: 20.917 ms, Bubble Time: 19.759 ms, Imbalance Overhead: 0.728 ms
GPU 7, Compute+Comm Time: 21.393 ms, Bubble Time: 19.975 ms, Imbalance Overhead: 0.037 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 73.706 ms
GPU 0, Compute+Comm Time: 38.077 ms, Bubble Time: 35.629 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.385 ms, Bubble Time: 35.165 ms, Imbalance Overhead: 2.156 ms
GPU 2, Compute+Comm Time: 36.385 ms, Bubble Time: 34.962 ms, Imbalance Overhead: 2.359 ms
GPU 3, Compute+Comm Time: 36.385 ms, Bubble Time: 34.647 ms, Imbalance Overhead: 2.674 ms
GPU 4, Compute+Comm Time: 36.385 ms, Bubble Time: 34.229 ms, Imbalance Overhead: 3.092 ms
GPU 5, Compute+Comm Time: 36.385 ms, Bubble Time: 33.827 ms, Imbalance Overhead: 3.494 ms
GPU 6, Compute+Comm Time: 36.385 ms, Bubble Time: 33.240 ms, Imbalance Overhead: 4.081 ms
GPU 7, Compute+Comm Time: 40.427 ms, Bubble Time: 33.141 ms, Imbalance Overhead: 0.138 ms
The estimated cost of the whole pipeline: 120.865 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 86.317 ms
Partition 0 [0, 9) has cost: 86.317 ms
Partition 1 [9, 17) has cost: 80.659 ms
Partition 2 [17, 25) has cost: 80.659 ms
Partition 3 [25, 33) has cost: 82.826 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 49.777 ms
GPU 0, Compute+Comm Time: 28.447 ms, Bubble Time: 20.581 ms, Imbalance Overhead: 0.749 ms
GPU 1, Compute+Comm Time: 27.638 ms, Bubble Time: 21.003 ms, Imbalance Overhead: 1.136 ms
GPU 2, Compute+Comm Time: 27.638 ms, Bubble Time: 21.458 ms, Imbalance Overhead: 0.682 ms
GPU 3, Compute+Comm Time: 27.873 ms, Bubble Time: 21.904 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 79.180 ms
GPU 0, Compute+Comm Time: 44.232 ms, Bubble Time: 34.879 ms, Imbalance Overhead: 0.070 ms
GPU 1, Compute+Comm Time: 43.398 ms, Bubble Time: 34.288 ms, Imbalance Overhead: 1.494 ms
GPU 2, Compute+Comm Time: 43.398 ms, Bubble Time: 33.513 ms, Imbalance Overhead: 2.269 ms
GPU 3, Compute+Comm Time: 45.421 ms, Bubble Time: 32.628 ms, Imbalance Overhead: 1.131 ms
    The estimated cost with 2 DP ways is 135.406 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 166.976 ms
Partition 0 [0, 17) has cost: 166.976 ms
Partition 1 [17, 33) has cost: 163.484 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 75.594 ms
GPU 0, Compute+Comm Time: 50.250 ms, Bubble Time: 24.476 ms, Imbalance Overhead: 0.868 ms
GPU 1, Compute+Comm Time: 49.963 ms, Bubble Time: 25.631 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 100.985 ms
GPU 0, Compute+Comm Time: 66.603 ms, Bubble Time: 34.382 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 67.215 ms, Bubble Time: 32.510 ms, Imbalance Overhead: 1.260 ms
    The estimated cost with 4 DP ways is 185.408 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 330.461 ms
Partition 0 [0, 33) has cost: 330.461 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 181.068 ms
GPU 0, Compute+Comm Time: 181.068 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 198.426 ms
GPU 0, Compute+Comm Time: 198.426 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 398.468 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.0667
	Epoch 50:	Loss 2.7750
	Epoch 75:	Loss 2.5125
	Epoch 100:	Loss 2.3567
Node 0, Pre/Post-Pipelining: 6.364 / 13.734 ms, Bubble: 0.464 ms, Compute: 148.325 ms, Comm: 0.007 ms, Imbalance: 0.015 ms
Node 1, Pre/Post-Pipelining: 6.368 / 13.761 ms, Bubble: 0.288 ms, Compute: 148.471 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 3, Pre/Post-Pipelining: 6.374 / 13.817 ms, Bubble: 0.037 ms, Compute: 148.657 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 6, Pre/Post-Pipelining: 6.372 / 13.772 ms, Bubble: 0.238 ms, Compute: 148.505 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 2, Pre/Post-Pipelining: 6.371 / 13.781 ms, Bubble: 0.378 ms, Compute: 148.357 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 7, Pre/Post-Pipelining: 6.364 / 13.738 ms, Bubble: 0.537 ms, Compute: 148.250 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 5, Pre/Post-Pipelining: 6.366 / 13.773 ms, Bubble: 0.552 ms, Compute: 148.197 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 4, Pre/Post-Pipelining: 6.371 / 13.799 ms, Bubble: 0.442 ms, Compute: 148.275 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.364 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.734 ms
Cluster-Wide Average, Bubble: 0.464 ms
Cluster-Wide Average, Compute: 148.325 ms
Cluster-Wide Average, Communication: 0.007 ms
Cluster-Wide Average, Imbalance: 0.015 ms
Node 0, GPU memory consumption: 15.387 GB
Node 3, GPU memory consumption: 14.725 GB
Node 2, GPU memory consumption: 14.749 GB
Node 1, GPU memory consumption: 14.751 GB
Node 5, GPU memory consumption: 14.749 GB
Node 7, GPU memory consumption: 14.725 GB
Node 4, GPU memory consumption: 14.725 GB
Node 6, GPU memory consumption: 14.749 GB
Node 0, Graph-Level Communication Throughput: 51.217 Gbps, Time: 88.131 ms
Node 1, Graph-Level Communication Throughput: 63.663 Gbps, Time: 84.336 ms
Node 2, Graph-Level Communication Throughput: 42.848 Gbps, Time: 88.658 ms
Node 3, Graph-Level Communication Throughput: 44.424 Gbps, Time: 89.211 ms
Node 4, Graph-Level Communication Throughput: 21.296 Gbps, Time: 91.560 ms
Node 5, Graph-Level Communication Throughput: 10.732 Gbps, Time: 92.398 ms
Node 6, Graph-Level Communication Throughput: 19.031 Gbps, Time: 91.901 ms
Node 7, Graph-Level Communication Throughput: 11.526 Gbps, Time: 93.210 ms
------------------------node id 0,  per-epoch time: 0.168959 s---------------
------------------------node id 1,  per-epoch time: 0.168959 s---------------
------------------------node id 2,  per-epoch time: 0.168961 s---------------
------------------------node id 3,  per-epoch time: 0.168960 s---------------
------------------------node id 4,  per-epoch time: 0.168962 s---------------
------------------------node id 5,  per-epoch time: 0.168961 s---------------
------------------------node id 6,  per-epoch time: 0.168959 s---------------
------------------------node id 7,  per-epoch time: 0.168961 s---------------
************ Profiling Results ************
	Bubble: 21.144513 (ms) (12.39 percentage)
	Compute: 41.566417 (ms) (24.35 percentage)
	GraphCommComputeOverhead: 5.504169 (ms) (3.22 percentage)
	GraphCommNetwork: 89.925897 (ms) (52.67 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.584074 (ms) (7.37 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 2.725 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 2.735 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
