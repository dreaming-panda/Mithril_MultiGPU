Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.048 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
        It takes 0.044 seconds.
        It takes 0.050 seconds.
        It takes 0.041 seconds.
        It takes 0.053 seconds.
        It takes 0.045 seconds.
        It takes 0.051 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.903 Gbps (per GPU), 511.226 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.861 Gbps (per GPU), 510.891 Gbps (aggregated)
The layer-level communication performance: 63.860 Gbps (per GPU), 510.883 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.824 Gbps (per GPU), 510.588 Gbps (aggregated)
The layer-level communication performance: 63.819 Gbps (per GPU), 510.555 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.790 Gbps (per GPU), 510.323 Gbps (aggregated)
The layer-level communication performance: 63.784 Gbps (per GPU), 510.270 Gbps (aggregated)
The layer-level communication performance: 63.779 Gbps (per GPU), 510.231 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.932 Gbps (per GPU), 1327.459 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.931 Gbps (per GPU), 1327.449 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.915 Gbps (per GPU), 1327.321 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.930 Gbps (per GPU), 1327.440 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.916 Gbps (per GPU), 1327.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.930 Gbps (per GPU), 1327.443 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.935 Gbps (per GPU), 1327.479 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.932 Gbps (per GPU), 1327.459 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.451 Gbps (per GPU), 915.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.452 Gbps (per GPU), 915.618 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.450 Gbps (per GPU), 915.601 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.450 Gbps (per GPU), 915.601 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.445 Gbps (per GPU), 915.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.443 Gbps (per GPU), 915.541 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.434 Gbps (per GPU), 915.474 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 4): 114.352 Gbps (per GPU), 914.813 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.543 Gbps (per GPU), 364.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.543 Gbps (per GPU), 364.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.542 Gbps (per GPU), 364.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.542 Gbps (per GPU), 364.336 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.543 Gbps (per GPU), 364.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.542 Gbps (per GPU), 364.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.542 Gbps (per GPU), 364.336 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.542 Gbps (per GPU), 364.339 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.45ms  0.59ms  1.86  5.55K  0.06M
 chk_1  0.32ms  0.45ms  0.60ms  1.87  5.75K  0.05M
 chk_2  0.31ms  0.45ms  0.60ms  1.91  5.20K  0.07M
 chk_3  0.32ms  0.46ms  0.60ms  1.89  5.57K  0.06M
 chk_4  0.31ms  0.45ms  0.59ms  1.91  5.05K  0.08M
 chk_5  0.30ms  0.47ms  0.61ms  2.01  4.73K  0.11M
 chk_6  0.31ms  0.44ms  0.58ms  1.90  4.98K  0.08M
 chk_7  0.31ms  0.45ms  0.59ms  1.92  5.01K  0.09M
 chk_8  0.32ms  0.45ms  0.60ms  1.87  5.73K  0.05M
 chk_9  0.30ms  0.44ms  0.58ms  1.94  4.54K  0.11M
chk_10  0.32ms  0.45ms  0.60ms  1.90  5.36K  0.07M
chk_11  0.32ms  0.46ms  0.60ms  1.91  5.39K  0.08M
chk_12  0.32ms  0.45ms  0.60ms  1.88  5.77K  0.05M
chk_13  0.32ms  0.45ms  0.60ms  1.89  5.43K  0.06M
chk_14  0.32ms  0.44ms  0.59ms  1.87  5.46K  0.06M
chk_15  0.32ms  0.45ms  0.60ms  1.84  5.88K  0.04M
chk_16  0.32ms  0.45ms  0.60ms  1.89  5.50K  0.06M
chk_17  0.31ms  0.47ms  0.61ms  1.99  4.86K  0.09M
chk_18  0.32ms  0.48ms  0.62ms  1.96  5.39K  0.07M
chk_19  0.31ms  0.45ms  0.60ms  1.93  5.20K  0.07M
chk_20  0.32ms  0.44ms  0.59ms  1.85  5.51K  0.06M
chk_21  0.32ms  0.44ms  0.59ms  1.83  5.81K  0.05M
chk_22  0.32ms  0.45ms  0.59ms  1.88  5.32K  0.07M
chk_23  0.32ms  0.46ms  0.61ms  1.94  5.39K  0.07M
chk_24  0.30ms  0.45ms  0.59ms  1.97  4.62K  0.11M
chk_25  0.31ms  0.45ms  0.60ms  1.92  5.04K  0.08M
chk_26  0.30ms  0.44ms  0.58ms  1.95  4.55K  0.11M
chk_27  0.31ms  0.44ms  0.58ms  1.87  5.30K  0.06M
chk_28  0.32ms  0.44ms  0.59ms  1.86  5.58K  0.06M
chk_29  0.31ms  0.46ms  0.61ms  1.96  4.98K  0.09M
chk_30  0.32ms  0.45ms  0.60ms  1.87  5.50K  0.07M
chk_31  0.32ms  0.45ms  0.60ms  1.89  5.38K  0.07M
   Avg  0.31  0.45  0.60
   Max  0.32  0.48  0.62
   Min  0.30  0.44  0.58
 Ratio  1.09  1.09  1.07
   Var  0.00  0.00  0.00
Profiling takes 0.646 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 67.787 ms
Partition 0 [0, 5) has cost: 67.787 ms
Partition 1 [5, 9) has cost: 57.740 ms
Partition 2 [9, 13) has cost: 57.740 ms
Partition 3 [13, 17) has cost: 57.740 ms
Partition 4 [17, 21) has cost: 57.740 ms
Partition 5 [21, 25) has cost: 57.740 ms
Partition 6 [25, 29) has cost: 57.740 ms
Partition 7 [29, 33) has cost: 62.426 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 37.030 ms
GPU 0, Compute+Comm Time: 30.852 ms, Bubble Time: 6.178 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 27.394 ms, Bubble Time: 6.260 ms, Imbalance Overhead: 3.376 ms
GPU 2, Compute+Comm Time: 27.394 ms, Bubble Time: 6.387 ms, Imbalance Overhead: 3.248 ms
GPU 3, Compute+Comm Time: 27.394 ms, Bubble Time: 6.485 ms, Imbalance Overhead: 3.151 ms
GPU 4, Compute+Comm Time: 27.394 ms, Bubble Time: 6.578 ms, Imbalance Overhead: 3.057 ms
GPU 5, Compute+Comm Time: 27.394 ms, Bubble Time: 6.640 ms, Imbalance Overhead: 2.996 ms
GPU 6, Compute+Comm Time: 27.394 ms, Bubble Time: 6.710 ms, Imbalance Overhead: 2.926 ms
GPU 7, Compute+Comm Time: 28.445 ms, Bubble Time: 6.760 ms, Imbalance Overhead: 1.825 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 64.938 ms
GPU 0, Compute+Comm Time: 50.941 ms, Bubble Time: 11.799 ms, Imbalance Overhead: 2.198 ms
GPU 1, Compute+Comm Time: 47.306 ms, Bubble Time: 11.758 ms, Imbalance Overhead: 5.874 ms
GPU 2, Compute+Comm Time: 47.306 ms, Bubble Time: 11.675 ms, Imbalance Overhead: 5.957 ms
GPU 3, Compute+Comm Time: 47.306 ms, Bubble Time: 11.617 ms, Imbalance Overhead: 6.014 ms
GPU 4, Compute+Comm Time: 47.306 ms, Bubble Time: 11.489 ms, Imbalance Overhead: 6.142 ms
GPU 5, Compute+Comm Time: 47.306 ms, Bubble Time: 11.363 ms, Imbalance Overhead: 6.269 ms
GPU 6, Compute+Comm Time: 47.306 ms, Bubble Time: 11.161 ms, Imbalance Overhead: 6.470 ms
GPU 7, Compute+Comm Time: 53.895 ms, Bubble Time: 11.043 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 107.066 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 125.527 ms
Partition 0 [0, 9) has cost: 125.527 ms
Partition 1 [9, 17) has cost: 115.481 ms
Partition 2 [17, 25) has cost: 115.481 ms
Partition 3 [25, 33) has cost: 120.166 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.786 ms
GPU 0, Compute+Comm Time: 36.108 ms, Bubble Time: 6.554 ms, Imbalance Overhead: 0.125 ms
GPU 1, Compute+Comm Time: 34.367 ms, Bubble Time: 6.683 ms, Imbalance Overhead: 1.736 ms
GPU 2, Compute+Comm Time: 34.367 ms, Bubble Time: 6.759 ms, Imbalance Overhead: 1.660 ms
GPU 3, Compute+Comm Time: 34.892 ms, Bubble Time: 6.808 ms, Imbalance Overhead: 1.086 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 68.537 ms
GPU 0, Compute+Comm Time: 56.295 ms, Bubble Time: 10.895 ms, Imbalance Overhead: 1.347 ms
GPU 1, Compute+Comm Time: 54.471 ms, Bubble Time: 10.821 ms, Imbalance Overhead: 3.245 ms
GPU 2, Compute+Comm Time: 54.471 ms, Bubble Time: 10.725 ms, Imbalance Overhead: 3.341 ms
GPU 3, Compute+Comm Time: 57.810 ms, Bubble Time: 10.483 ms, Imbalance Overhead: 0.245 ms
    The estimated cost with 2 DP ways is 116.890 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 241.008 ms
Partition 0 [0, 17) has cost: 241.008 ms
Partition 1 [17, 33) has cost: 235.647 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 66.902 ms
GPU 0, Compute+Comm Time: 58.639 ms, Bubble Time: 7.280 ms, Imbalance Overhead: 0.983 ms
GPU 1, Compute+Comm Time: 58.028 ms, Bubble Time: 7.482 ms, Imbalance Overhead: 1.392 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 90.542 ms
GPU 0, Compute+Comm Time: 78.942 ms, Bubble Time: 10.101 ms, Imbalance Overhead: 1.500 ms
GPU 1, Compute+Comm Time: 79.711 ms, Bubble Time: 9.876 ms, Imbalance Overhead: 0.956 ms
    The estimated cost with 4 DP ways is 165.317 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 476.655 ms
Partition 0 [0, 33) has cost: 476.655 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 201.416 ms
GPU 0, Compute+Comm Time: 201.416 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 222.491 ms
GPU 0, Compute+Comm Time: 222.491 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 445.102 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.7824
	Epoch 50:	Loss 3.7067
	Epoch 75:	Loss 3.6988
	Epoch 100:	Loss 3.6983
Node 1, Pre/Post-Pipelining: 0.847 / 16.376 ms, Bubble: 34.696 ms, Compute: 59.887 ms, Comm: 23.504 ms, Imbalance: 20.594 ms
Node 0, Pre/Post-Pipelining: 0.849 / 16.366 ms, Bubble: 35.229 ms, Compute: 69.015 ms, Comm: 17.532 ms, Imbalance: 16.719 ms
Node 4, Pre/Post-Pipelining: 0.840 / 16.432 ms, Bubble: 33.362 ms, Compute: 61.509 ms, Comm: 29.501 ms, Imbalance: 14.641 ms
Node 3, Pre/Post-Pipelining: 0.840 / 16.437 ms, Bubble: 33.373 ms, Compute: 62.225 ms, Comm: 29.416 ms, Imbalance: 13.637 ms
Node 6, Pre/Post-Pipelining: 0.824 / 16.480 ms, Bubble: 33.561 ms, Compute: 61.587 ms, Comm: 23.492 ms, Imbalance: 20.075 ms
Node 2, Pre/Post-Pipelining: 0.841 / 16.415 ms, Bubble: 34.409 ms, Compute: 61.385 ms, Comm: 27.635 ms, Imbalance: 15.332 ms
Node 7, Pre/Post-Pipelining: 0.828 / 30.778 ms, Bubble: 19.450 ms, Compute: 80.286 ms, Comm: 17.175 ms, Imbalance: 7.247 ms
Node 5, Pre/Post-Pipelining: 0.818 / 16.447 ms, Bubble: 33.553 ms, Compute: 60.997 ms, Comm: 27.809 ms, Imbalance: 16.483 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.849 ms
Cluster-Wide Average, Post-Pipelining Overhead: 16.366 ms
Cluster-Wide Average, Bubble: 35.229 ms
Cluster-Wide Average, Compute: 69.015 ms
Cluster-Wide Average, Communication: 17.532 ms
Cluster-Wide Average, Imbalance: 16.719 ms
Node 0, GPU memory consumption: 5.018 GB
Node 3, GPU memory consumption: 3.624 GB
Node 1, GPU memory consumption: 3.647 GB
Node 2, GPU memory consumption: 3.647 GB
Node 6, GPU memory consumption: 3.647 GB
Node 4, GPU memory consumption: 3.624 GB
Node 5, GPU memory consumption: 3.647 GB
Node 7, GPU memory consumption: 3.727 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.156381 s---------------
------------------------node id 1,  per-epoch time: 0.156380 s---------------
------------------------node id 2,  per-epoch time: 0.156386 s---------------
------------------------node id 3,  per-epoch time: 0.156383 s---------------
------------------------node id 4,  per-epoch time: 0.156381 s---------------
------------------------node id 5,  per-epoch time: 0.156385 s---------------
------------------------node id 6,  per-epoch time: 0.156383 s---------------
------------------------node id 7,  per-epoch time: 0.156385 s---------------
************ Profiling Results ************
	Bubble: 96.189587 (ms) (59.31 percentage)
	Compute: 63.303863 (ms) (39.03 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.687288 (ms) (1.66 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.766 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 1.776 GB
	Aggregated layer-level communication throughput: 619.110 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
