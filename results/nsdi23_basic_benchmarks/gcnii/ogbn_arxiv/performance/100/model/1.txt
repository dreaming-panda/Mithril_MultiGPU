Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.045 seconds.
Building the CSC structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.048 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.041 seconds.
        It takes 0.045 seconds.
        It takes 0.042 seconds.
        It takes 0.048 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.055 seconds.
        It takes 0.053 seconds.
        It takes 0.053 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
Number of vertices per chunk: 5292
169343, 2484941, 2484941
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.631 Gbps (per GPU), 509.050 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.596 Gbps (per GPU), 508.772 Gbps (aggregated)
The layer-level communication performance: 63.589 Gbps (per GPU), 508.715 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.555 Gbps (per GPU), 508.442 Gbps (aggregated)
The layer-level communication performance: 63.551 Gbps (per GPU), 508.408 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.521 Gbps (per GPU), 508.170 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.520 Gbps (per GPU), 508.162 Gbps (aggregated)
The layer-level communication performance: 63.510 Gbps (per GPU), 508.077 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.936 Gbps (per GPU), 1319.491 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.932 Gbps (per GPU), 1319.456 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.935 Gbps (per GPU), 1319.479 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.935 Gbps (per GPU), 1319.479 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.932 Gbps (per GPU), 1319.456 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.937 Gbps (per GPU), 1319.492 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.930 Gbps (per GPU), 1319.440 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.933 Gbps (per GPU), 1319.463 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.540 Gbps (per GPU), 916.320 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.540 Gbps (per GPU), 916.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.539 Gbps (per GPU), 916.310 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.536 Gbps (per GPU), 916.286 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.532 Gbps (per GPU), 916.259 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.530 Gbps (per GPU), 916.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.527 Gbps (per GPU), 916.214 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.470 Gbps (per GPU), 915.761 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.943 Gbps (per GPU), 367.545 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.943 Gbps (per GPU), 367.547 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.943 Gbps (per GPU), 367.543 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.943 Gbps (per GPU), 367.541 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.943 Gbps (per GPU), 367.545 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.943 Gbps (per GPU), 367.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.943 Gbps (per GPU), 367.543 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.943 Gbps (per GPU), 367.544 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.45ms  0.59ms  1.86  5.55K  0.06M
 chk_1  0.32ms  0.45ms  0.60ms  1.88  5.75K  0.05M
 chk_2  0.31ms  0.45ms  0.60ms  1.91  5.20K  0.07M
 chk_3  0.32ms  0.46ms  0.60ms  1.90  5.57K  0.06M
 chk_4  0.31ms  0.45ms  0.59ms  1.89  5.05K  0.08M
 chk_5  0.30ms  0.47ms  0.61ms  2.01  4.73K  0.11M
 chk_6  0.31ms  0.44ms  0.59ms  1.90  4.98K  0.08M
 chk_7  0.31ms  0.45ms  0.59ms  1.92  5.01K  0.09M
 chk_8  0.32ms  0.45ms  0.60ms  1.88  5.73K  0.05M
 chk_9  0.30ms  0.44ms  0.58ms  1.94  4.54K  0.11M
chk_10  0.32ms  0.45ms  0.60ms  1.90  5.36K  0.07M
chk_11  0.32ms  0.46ms  0.60ms  1.90  5.39K  0.08M
chk_12  0.32ms  0.45ms  0.60ms  1.87  5.77K  0.05M
chk_13  0.31ms  0.45ms  0.59ms  1.89  5.43K  0.06M
chk_14  0.31ms  0.44ms  0.59ms  1.87  5.46K  0.06M
chk_15  0.32ms  0.45ms  0.60ms  1.85  5.88K  0.04M
chk_16  0.32ms  0.45ms  0.60ms  1.89  5.50K  0.06M
chk_17  0.31ms  0.47ms  0.61ms  1.98  4.86K  0.09M
chk_18  0.32ms  0.48ms  0.62ms  1.97  5.39K  0.07M
chk_19  0.31ms  0.45ms  0.60ms  1.92  5.20K  0.07M
chk_20  0.32ms  0.45ms  0.59ms  1.86  5.51K  0.06M
chk_21  0.32ms  0.44ms  0.59ms  1.83  5.81K  0.05M
chk_22  0.31ms  0.45ms  0.59ms  1.88  5.32K  0.07M
chk_23  0.32ms  0.47ms  0.61ms  1.94  5.39K  0.07M
chk_24  0.30ms  0.45ms  0.59ms  1.95  4.62K  0.11M
chk_25  0.31ms  0.45ms  0.60ms  1.93  5.04K  0.08M
chk_26  0.30ms  0.44ms  0.58ms  1.95  4.55K  0.11M
chk_27  0.31ms  0.44ms  0.59ms  1.87  5.30K  0.06M
chk_28  0.32ms  0.44ms  0.59ms  1.86  5.58K  0.06M
chk_29  0.31ms  0.46ms  0.60ms  1.96  4.98K  0.09M
chk_30  0.32ms  0.45ms  0.60ms  1.88  5.50K  0.07M
chk_31  0.32ms  0.45ms  0.60ms  1.89  5.38K  0.07M
   Avg  0.31  0.45  0.60
   Max  0.32  0.48  0.62
   Min  0.30  0.44  0.58
 Ratio  1.08  1.09  1.07
   Var  0.00  0.00  0.00
Profiling takes 0.649 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 67.820 ms
Partition 0 [0, 5) has cost: 67.820 ms
Partition 1 [5, 9) has cost: 57.775 ms
Partition 2 [9, 13) has cost: 57.775 ms
Partition 3 [13, 17) has cost: 57.775 ms
Partition 4 [17, 21) has cost: 57.775 ms
Partition 5 [21, 25) has cost: 57.775 ms
Partition 6 [25, 29) has cost: 57.775 ms
Partition 7 [29, 33) has cost: 62.448 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 37.060 ms
GPU 0, Compute+Comm Time: 30.878 ms, Bubble Time: 6.183 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 27.417 ms, Bubble Time: 6.266 ms, Imbalance Overhead: 3.377 ms
GPU 2, Compute+Comm Time: 27.417 ms, Bubble Time: 6.389 ms, Imbalance Overhead: 3.254 ms
GPU 3, Compute+Comm Time: 27.417 ms, Bubble Time: 6.482 ms, Imbalance Overhead: 3.162 ms
GPU 4, Compute+Comm Time: 27.417 ms, Bubble Time: 6.574 ms, Imbalance Overhead: 3.069 ms
GPU 5, Compute+Comm Time: 27.417 ms, Bubble Time: 6.635 ms, Imbalance Overhead: 3.008 ms
GPU 6, Compute+Comm Time: 27.417 ms, Bubble Time: 6.703 ms, Imbalance Overhead: 2.940 ms
GPU 7, Compute+Comm Time: 28.464 ms, Bubble Time: 6.754 ms, Imbalance Overhead: 1.842 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 65.033 ms
GPU 0, Compute+Comm Time: 51.017 ms, Bubble Time: 11.826 ms, Imbalance Overhead: 2.190 ms
GPU 1, Compute+Comm Time: 47.391 ms, Bubble Time: 11.785 ms, Imbalance Overhead: 5.857 ms
GPU 2, Compute+Comm Time: 47.391 ms, Bubble Time: 11.703 ms, Imbalance Overhead: 5.939 ms
GPU 3, Compute+Comm Time: 47.391 ms, Bubble Time: 11.644 ms, Imbalance Overhead: 5.998 ms
GPU 4, Compute+Comm Time: 47.391 ms, Bubble Time: 11.510 ms, Imbalance Overhead: 6.133 ms
GPU 5, Compute+Comm Time: 47.391 ms, Bubble Time: 11.378 ms, Imbalance Overhead: 6.265 ms
GPU 6, Compute+Comm Time: 47.391 ms, Bubble Time: 11.173 ms, Imbalance Overhead: 6.469 ms
GPU 7, Compute+Comm Time: 53.975 ms, Bubble Time: 11.059 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 107.198 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 125.595 ms
Partition 0 [0, 9) has cost: 125.595 ms
Partition 1 [9, 17) has cost: 115.550 ms
Partition 2 [17, 25) has cost: 115.550 ms
Partition 3 [25, 33) has cost: 120.223 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.853 ms
GPU 0, Compute+Comm Time: 36.167 ms, Bubble Time: 6.568 ms, Imbalance Overhead: 0.118 ms
GPU 1, Compute+Comm Time: 34.427 ms, Bubble Time: 6.688 ms, Imbalance Overhead: 1.739 ms
GPU 2, Compute+Comm Time: 34.427 ms, Bubble Time: 6.750 ms, Imbalance Overhead: 1.676 ms
GPU 3, Compute+Comm Time: 34.951 ms, Bubble Time: 6.799 ms, Imbalance Overhead: 1.103 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 68.698 ms
GPU 0, Compute+Comm Time: 56.423 ms, Bubble Time: 10.939 ms, Imbalance Overhead: 1.336 ms
GPU 1, Compute+Comm Time: 54.601 ms, Bubble Time: 10.863 ms, Imbalance Overhead: 3.233 ms
GPU 2, Compute+Comm Time: 54.601 ms, Bubble Time: 10.748 ms, Imbalance Overhead: 3.349 ms
GPU 3, Compute+Comm Time: 57.938 ms, Bubble Time: 10.499 ms, Imbalance Overhead: 0.261 ms
    The estimated cost with 2 DP ways is 117.128 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 241.145 ms
Partition 0 [0, 17) has cost: 241.145 ms
Partition 1 [17, 33) has cost: 235.773 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 66.877 ms
GPU 0, Compute+Comm Time: 58.634 ms, Bubble Time: 7.282 ms, Imbalance Overhead: 0.961 ms
GPU 1, Compute+Comm Time: 58.023 ms, Bubble Time: 7.458 ms, Imbalance Overhead: 1.396 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 90.620 ms
GPU 0, Compute+Comm Time: 79.004 ms, Bubble Time: 10.133 ms, Imbalance Overhead: 1.482 ms
GPU 1, Compute+Comm Time: 79.770 ms, Bubble Time: 9.874 ms, Imbalance Overhead: 0.976 ms
    The estimated cost with 4 DP ways is 165.372 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 476.918 ms
Partition 0 [0, 33) has cost: 476.918 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 199.820 ms
GPU 0, Compute+Comm Time: 199.820 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 220.931 ms
GPU 0, Compute+Comm Time: 220.931 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 441.789 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.7827
	Epoch 50:	Loss 3.7015
	Epoch 75:	Loss 3.6946
	Epoch 100:	Loss 3.7016
Node 0, Pre/Post-Pipelining: 0.846 / 16.157 ms, Bubble: 34.674 ms, Compute: 69.359 ms, Comm: 17.558 ms, Imbalance: 15.219 ms
Node 1, Pre/Post-Pipelining: 0.835 / 16.190 ms, Bubble: 34.186 ms, Compute: 60.635 ms, Comm: 23.550 ms, Imbalance: 18.609 ms
Node 2, Pre/Post-Pipelining: 0.837 / 16.195 ms, Bubble: 34.126 ms, Compute: 60.804 ms, Comm: 27.652 ms, Imbalance: 14.550 ms
Node 3, Pre/Post-Pipelining: 0.839 / 16.206 ms, Bubble: 33.154 ms, Compute: 61.029 ms, Comm: 29.338 ms, Imbalance: 13.500 ms
Node 4, Pre/Post-Pipelining: 0.818 / 16.238 ms, Bubble: 33.114 ms, Compute: 61.802 ms, Comm: 29.472 ms, Imbalance: 12.930 ms
Node 7, Pre/Post-Pipelining: 0.818 / 30.386 ms, Bubble: 19.703 ms, Compute: 78.109 ms, Comm: 17.171 ms, Imbalance: 7.815 ms
Node 5, Pre/Post-Pipelining: 0.822 / 16.229 ms, Bubble: 33.360 ms, Compute: 61.187 ms, Comm: 27.921 ms, Imbalance: 14.671 ms
Node 6, Pre/Post-Pipelining: 0.818 / 16.275 ms, Bubble: 33.304 ms, Compute: 62.044 ms, Comm: 23.494 ms, Imbalance: 18.127 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.846 ms
Cluster-Wide Average, Post-Pipelining Overhead: 16.157 ms
Cluster-Wide Average, Bubble: 34.674 ms
Cluster-Wide Average, Compute: 69.359 ms
Cluster-Wide Average, Communication: 17.558 ms
Cluster-Wide Average, Imbalance: 15.219 ms
Node 0, GPU memory consumption: 5.018 GB
Node 3, GPU memory consumption: 3.624 GB
Node 1, GPU memory consumption: 3.647 GB
Node 2, GPU memory consumption: 3.647 GB
Node 4, GPU memory consumption: 3.624 GB
Node 5, GPU memory consumption: 3.647 GB
Node 6, GPU memory consumption: 3.647 GB
Node 7, GPU memory consumption: 3.727 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.154500 s---------------
------------------------node id 1,  per-epoch time: 0.154502 s---------------
------------------------node id 2,  per-epoch time: 0.154502 s---------------
------------------------node id 3,  per-epoch time: 0.154503 s---------------
------------------------node id 4,  per-epoch time: 0.154504 s---------------
------------------------node id 5,  per-epoch time: 0.154505 s---------------
------------------------node id 6,  per-epoch time: 0.154508 s---------------
------------------------node id 7,  per-epoch time: 0.154506 s---------------
************ Profiling Results ************
	Bubble: 94.485800 (ms) (58.97 percentage)
	Compute: 63.074191 (ms) (39.36 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.680418 (ms) (1.67 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.766 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 1.776 GB
	Aggregated layer-level communication throughput: 618.814 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
