Initialized node 0 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.065 seconds.
Building the CSC structure...
        It takes 0.041 seconds.
        It takes 0.042 seconds.
        It takes 0.052 seconds.
        It takes 0.054 seconds.
        It takes 0.055 seconds.
        It takes 0.054 seconds.
        It takes 0.050 seconds.
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.025 seconds.
        It takes 0.026 seconds.
        It takes 0.025 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
train nodes 90941, valid nodes 29799, test nodes 48603
169343, 2484941, 2484941
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.361 Gbps (per GPU), 506.890 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.320 Gbps (per GPU), 506.558 Gbps (aggregated)
The layer-level communication performance: 63.319 Gbps (per GPU), 506.549 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.284 Gbps (per GPU), 506.274 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.280 Gbps (per GPU), 506.240 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.248 Gbps (per GPU), 505.983 Gbps (aggregated)
The layer-level communication performance: 63.240 Gbps (per GPU), 505.923 Gbps (aggregated)
The layer-level communication performance: 63.236 Gbps (per GPU), 505.888 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.043 Gbps (per GPU), 1328.346 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.040 Gbps (per GPU), 1328.322 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.040 Gbps (per GPU), 1328.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.037 Gbps (per GPU), 1328.297 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.042 Gbps (per GPU), 1328.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.047 Gbps (per GPU), 1328.372 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.042 Gbps (per GPU), 1328.339 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.043 Gbps (per GPU), 1328.343 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.455 Gbps (per GPU), 915.643 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.456 Gbps (per GPU), 915.644 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.452 Gbps (per GPU), 915.617 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.453 Gbps (per GPU), 915.624 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.450 Gbps (per GPU), 915.601 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.448 Gbps (per GPU), 915.581 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.440 Gbps (per GPU), 915.520 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.420 Gbps (per GPU), 915.363 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 46.133 Gbps (per GPU), 369.062 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.133 Gbps (per GPU), 369.063 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.133 Gbps (per GPU), 369.063 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.133 Gbps (per GPU), 369.060 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.132 Gbps (per GPU), 369.058 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.131 Gbps (per GPU), 369.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.133 Gbps (per GPU), 369.061 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.132 Gbps (per GPU), 369.058 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.32ms  0.45ms  0.59ms  1.86  5.55K  0.06M
 chk_1  0.32ms  0.45ms  0.60ms  1.88  5.75K  0.05M
 chk_2  0.31ms  0.45ms  0.60ms  1.92  5.20K  0.07M
 chk_3  0.32ms  0.46ms  0.60ms  1.90  5.57K  0.06M
 chk_4  0.31ms  0.45ms  0.59ms  1.90  5.05K  0.08M
 chk_5  0.30ms  0.47ms  0.61ms  2.01  4.73K  0.11M
 chk_6  0.31ms  0.44ms  0.58ms  1.90  4.98K  0.08M
 chk_7  0.31ms  0.45ms  0.59ms  1.91  5.01K  0.09M
 chk_8  0.32ms  0.45ms  0.60ms  1.88  5.73K  0.05M
 chk_9  0.30ms  0.44ms  0.58ms  1.94  4.54K  0.11M
chk_10  0.31ms  0.45ms  0.60ms  1.90  5.36K  0.07M
chk_11  0.32ms  0.46ms  0.60ms  1.91  5.39K  0.08M
chk_12  0.32ms  0.45ms  0.60ms  1.86  5.77K  0.05M
chk_13  0.31ms  0.45ms  0.59ms  1.88  5.43K  0.06M
chk_14  0.32ms  0.44ms  0.59ms  1.87  5.46K  0.06M
chk_15  0.32ms  0.45ms  0.60ms  1.84  5.88K  0.04M
chk_16  0.32ms  0.45ms  0.60ms  1.89  5.50K  0.06M
chk_17  0.30ms  0.47ms  0.61ms  2.00  4.86K  0.09M
chk_18  0.32ms  0.48ms  0.62ms  1.97  5.39K  0.07M
chk_19  0.31ms  0.45ms  0.60ms  1.91  5.20K  0.07M
chk_20  0.32ms  0.44ms  0.59ms  1.86  5.51K  0.06M
chk_21  0.32ms  0.44ms  0.59ms  1.83  5.81K  0.05M
chk_22  0.31ms  0.45ms  0.59ms  1.89  5.32K  0.07M
chk_23  0.32ms  0.47ms  0.61ms  1.93  5.39K  0.07M
chk_24  0.30ms  0.45ms  0.59ms  1.96  4.62K  0.11M
chk_25  0.31ms  0.45ms  0.60ms  1.92  5.04K  0.08M
chk_26  0.30ms  0.44ms  0.58ms  1.95  4.55K  0.11M
chk_27  0.31ms  0.44ms  0.58ms  1.86  5.30K  0.06M
chk_28  0.32ms  0.44ms  0.59ms  1.86  5.58K  0.06M
chk_29  0.31ms  0.46ms  0.60ms  1.96  4.98K  0.09M
chk_30  0.32ms  0.45ms  0.60ms  1.89  5.50K  0.07M
chk_31  0.32ms  0.45ms  0.60ms  1.88  5.38K  0.07M
   Avg  0.31  0.45  0.60
   Max  0.32  0.48  0.62
   Min  0.30  0.44  0.58
 Ratio  1.08  1.09  1.07
   Var  0.00  0.00  0.00
Profiling takes 0.646 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 67.743 ms
Partition 0 [0, 5) has cost: 67.743 ms
Partition 1 [5, 9) has cost: 57.713 ms
Partition 2 [9, 13) has cost: 57.713 ms
Partition 3 [13, 17) has cost: 57.713 ms
Partition 4 [17, 21) has cost: 57.713 ms
Partition 5 [21, 25) has cost: 57.713 ms
Partition 6 [25, 29) has cost: 57.713 ms
Partition 7 [29, 33) has cost: 62.368 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 37.115 ms
GPU 0, Compute+Comm Time: 30.937 ms, Bubble Time: 6.179 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 27.481 ms, Bubble Time: 6.265 ms, Imbalance Overhead: 3.370 ms
GPU 2, Compute+Comm Time: 27.481 ms, Bubble Time: 6.393 ms, Imbalance Overhead: 3.242 ms
GPU 3, Compute+Comm Time: 27.481 ms, Bubble Time: 6.492 ms, Imbalance Overhead: 3.142 ms
GPU 4, Compute+Comm Time: 27.481 ms, Bubble Time: 6.586 ms, Imbalance Overhead: 3.048 ms
GPU 5, Compute+Comm Time: 27.481 ms, Bubble Time: 6.648 ms, Imbalance Overhead: 2.986 ms
GPU 6, Compute+Comm Time: 27.481 ms, Bubble Time: 6.722 ms, Imbalance Overhead: 2.913 ms
GPU 7, Compute+Comm Time: 28.512 ms, Bubble Time: 6.778 ms, Imbalance Overhead: 1.826 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 64.951 ms
GPU 0, Compute+Comm Time: 50.961 ms, Bubble Time: 11.800 ms, Imbalance Overhead: 2.190 ms
GPU 1, Compute+Comm Time: 47.337 ms, Bubble Time: 11.756 ms, Imbalance Overhead: 5.858 ms
GPU 2, Compute+Comm Time: 47.337 ms, Bubble Time: 11.672 ms, Imbalance Overhead: 5.941 ms
GPU 3, Compute+Comm Time: 47.337 ms, Bubble Time: 11.617 ms, Imbalance Overhead: 5.997 ms
GPU 4, Compute+Comm Time: 47.337 ms, Bubble Time: 11.483 ms, Imbalance Overhead: 6.131 ms
GPU 5, Compute+Comm Time: 47.337 ms, Bubble Time: 11.355 ms, Imbalance Overhead: 6.258 ms
GPU 6, Compute+Comm Time: 47.337 ms, Bubble Time: 11.151 ms, Imbalance Overhead: 6.462 ms
GPU 7, Compute+Comm Time: 53.911 ms, Bubble Time: 11.040 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 107.170 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 125.456 ms
Partition 0 [0, 9) has cost: 125.456 ms
Partition 1 [9, 17) has cost: 115.426 ms
Partition 2 [17, 25) has cost: 115.426 ms
Partition 3 [25, 33) has cost: 120.081 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 42.871 ms
GPU 0, Compute+Comm Time: 36.192 ms, Bubble Time: 6.554 ms, Imbalance Overhead: 0.124 ms
GPU 1, Compute+Comm Time: 34.453 ms, Bubble Time: 6.683 ms, Imbalance Overhead: 1.734 ms
GPU 2, Compute+Comm Time: 34.453 ms, Bubble Time: 6.759 ms, Imbalance Overhead: 1.658 ms
GPU 3, Compute+Comm Time: 34.967 ms, Bubble Time: 6.821 ms, Imbalance Overhead: 1.083 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 68.590 ms
GPU 0, Compute+Comm Time: 56.357 ms, Bubble Time: 10.918 ms, Imbalance Overhead: 1.316 ms
GPU 1, Compute+Comm Time: 54.542 ms, Bubble Time: 10.840 ms, Imbalance Overhead: 3.209 ms
GPU 2, Compute+Comm Time: 54.542 ms, Bubble Time: 10.725 ms, Imbalance Overhead: 3.324 ms
GPU 3, Compute+Comm Time: 57.878 ms, Bubble Time: 10.481 ms, Imbalance Overhead: 0.232 ms
    The estimated cost with 2 DP ways is 117.034 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 240.882 ms
Partition 0 [0, 17) has cost: 240.882 ms
Partition 1 [17, 33) has cost: 235.507 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 67.032 ms
GPU 0, Compute+Comm Time: 58.757 ms, Bubble Time: 7.283 ms, Imbalance Overhead: 0.992 ms
GPU 1, Compute+Comm Time: 58.143 ms, Bubble Time: 7.485 ms, Imbalance Overhead: 1.404 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 90.669 ms
GPU 0, Compute+Comm Time: 79.021 ms, Bubble Time: 10.132 ms, Imbalance Overhead: 1.515 ms
GPU 1, Compute+Comm Time: 79.798 ms, Bubble Time: 9.872 ms, Imbalance Overhead: 0.999 ms
    The estimated cost with 4 DP ways is 165.586 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 476.388 ms
Partition 0 [0, 33) has cost: 476.388 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 199.127 ms
GPU 0, Compute+Comm Time: 199.127 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 220.127 ms
GPU 0, Compute+Comm Time: 220.127 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 440.217 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.7760
	Epoch 50:	Loss 3.7043
	Epoch 75:	Loss 3.6954
	Epoch 100:	Loss 3.6985
Node 0, Pre/Post-Pipelining: 0.860 / 16.718 ms, Bubble: 35.294 ms, Compute: 68.860 ms, Comm: 17.532 ms, Imbalance: 16.848 ms
Node 1, Pre/Post-Pipelining: 0.852 / 16.743 ms, Bubble: 34.702 ms, Compute: 60.063 ms, Comm: 23.611 ms, Imbalance: 20.327 ms
Node 2, Pre/Post-Pipelining: 0.849 / 16.769 ms, Bubble: 34.486 ms, Compute: 61.450 ms, Comm: 27.725 ms, Imbalance: 15.136 ms
Node 3, Pre/Post-Pipelining: 0.848 / 16.794 ms, Bubble: 33.435 ms, Compute: 61.937 ms, Comm: 29.422 ms, Imbalance: 13.891 ms
Node 4, Pre/Post-Pipelining: 0.829 / 16.784 ms, Bubble: 33.463 ms, Compute: 60.874 ms, Comm: 29.471 ms, Imbalance: 15.241 ms
Node 5, Pre/Post-Pipelining: 0.834 / 16.805 ms, Bubble: 33.680 ms, Compute: 60.845 ms, Comm: 27.970 ms, Imbalance: 16.409 ms
Node 6, Pre/Post-Pipelining: 0.822 / 16.835 ms, Bubble: 33.666 ms, Compute: 61.608 ms, Comm: 23.623 ms, Imbalance: 19.867 ms
Node 7, Pre/Post-Pipelining: 0.836 / 31.171 ms, Bubble: 19.509 ms, Compute: 80.408 ms, Comm: 17.290 ms, Imbalance: 7.000 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.860 ms
Cluster-Wide Average, Post-Pipelining Overhead: 16.718 ms
Cluster-Wide Average, Bubble: 35.294 ms
Cluster-Wide Average, Compute: 68.860 ms
Cluster-Wide Average, Communication: 17.532 ms
Cluster-Wide Average, Imbalance: 16.848 ms
Node 0, GPU memory consumption: 5.018 GB
Node 1, GPU memory consumption: 3.647 GB
Node 3, GPU memory consumption: 3.624 GB
Node 2, GPU memory consumption: 3.647 GB
Node 5, GPU memory consumption: 3.647 GB
Node 7, GPU memory consumption: 3.727 GB
Node 4, GPU memory consumption: 3.624 GB
Node 6, GPU memory consumption: 3.647 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.156798 s---------------
------------------------node id 1,  per-epoch time: 0.156799 s---------------
------------------------node id 2,  per-epoch time: 0.156800 s---------------
------------------------node id 3,  per-epoch time: 0.156801 s---------------
------------------------node id 4,  per-epoch time: 0.156799 s---------------
------------------------node id 5,  per-epoch time: 0.156804 s---------------
------------------------node id 6,  per-epoch time: 0.156802 s---------------
------------------------node id 7,  per-epoch time: 0.156804 s---------------
************ Profiling Results ************
	Bubble: 96.740479 (ms) (59.49 percentage)
	Compute: 63.197917 (ms) (38.86 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.680616 (ms) (1.65 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.766 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 1.776 GB
	Aggregated layer-level communication throughput: 617.282 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
