Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.041 seconds.
        It takes 0.044 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
        It takes 0.056 seconds.
        It takes 0.053 seconds.
        It takes 0.054 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.028 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.815 Gbps (per GPU), 510.519 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.773 Gbps (per GPU), 510.184 Gbps (aggregated)
The layer-level communication performance: 63.772 Gbps (per GPU), 510.180 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.736 Gbps (per GPU), 509.884 Gbps (aggregated)
The layer-level communication performance: 63.732 Gbps (per GPU), 509.854 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.701 Gbps (per GPU), 509.604 Gbps (aggregated)
The layer-level communication performance: 63.693 Gbps (per GPU), 509.545 Gbps (aggregated)
The layer-level communication performance: 63.689 Gbps (per GPU), 509.514 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.277 Gbps (per GPU), 1330.215 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.252 Gbps (per GPU), 1330.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.266 Gbps (per GPU), 1330.130 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.253 Gbps (per GPU), 1330.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.264 Gbps (per GPU), 1330.110 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.268 Gbps (per GPU), 1330.143 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.264 Gbps (per GPU), 1330.113 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.266 Gbps (per GPU), 1330.130 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.298 Gbps (per GPU), 914.381 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.298 Gbps (per GPU), 914.380 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.298 Gbps (per GPU), 914.382 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.297 Gbps (per GPU), 914.373 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.291 Gbps (per GPU), 914.328 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.282 Gbps (per GPU), 914.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.283 Gbps (per GPU), 914.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.270 Gbps (per GPU), 914.157 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.744 Gbps (per GPU), 365.949 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.744 Gbps (per GPU), 365.950 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.743 Gbps (per GPU), 365.947 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.743 Gbps (per GPU), 365.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.744 Gbps (per GPU), 365.948 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.743 Gbps (per GPU), 365.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.743 Gbps (per GPU), 365.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.743 Gbps (per GPU), 365.945 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.57ms  0.90ms  1.13ms  1.97 21.17K  0.46M
 chk_1  0.57ms  0.93ms  1.17ms  2.03 21.17K  0.55M
 chk_2  0.57ms  0.87ms  1.10ms  1.92 21.17K  0.39M
 chk_3  0.57ms  0.83ms  1.06ms  1.85 21.17K  0.24M
 chk_4  0.57ms  0.80ms  1.03ms  1.80 21.17K  0.17M
 chk_5  0.57ms  0.81ms  1.05ms  1.82 21.17K  0.22M
 chk_6  0.57ms  0.80ms  1.03ms  1.80 21.17K  0.16M
 chk_7  0.57ms  0.79ms  1.02ms  1.77 21.17K  0.12M
   Avg  0.57  0.84  1.07
   Max  0.57  0.93  1.17
   Min  0.57  0.79  1.02
 Ratio  1.00  1.18  1.15
   Var  0.00  0.00  0.00
Profiling takes 0.316 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 58.530 ms
Partition 0 [0, 9) has cost: 58.530 ms
Partition 1 [9, 17) has cost: 53.940 ms
Partition 2 [17, 25) has cost: 53.940 ms
Partition 3 [25, 33) has cost: 53.940 ms
Partition 4 [33, 41) has cost: 53.940 ms
Partition 5 [41, 49) has cost: 53.940 ms
Partition 6 [49, 57) has cost: 53.940 ms
Partition 7 [57, 65) has cost: 55.782 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 44.976 ms
GPU 0, Compute+Comm Time: 24.011 ms, Bubble Time: 20.070 ms, Imbalance Overhead: 0.895 ms
GPU 1, Compute+Comm Time: 22.697 ms, Bubble Time: 20.075 ms, Imbalance Overhead: 2.204 ms
GPU 2, Compute+Comm Time: 22.697 ms, Bubble Time: 20.366 ms, Imbalance Overhead: 1.913 ms
GPU 3, Compute+Comm Time: 22.697 ms, Bubble Time: 20.588 ms, Imbalance Overhead: 1.691 ms
GPU 4, Compute+Comm Time: 22.697 ms, Bubble Time: 20.902 ms, Imbalance Overhead: 1.377 ms
GPU 5, Compute+Comm Time: 22.697 ms, Bubble Time: 21.177 ms, Imbalance Overhead: 1.102 ms
GPU 6, Compute+Comm Time: 22.697 ms, Bubble Time: 21.482 ms, Imbalance Overhead: 0.797 ms
GPU 7, Compute+Comm Time: 23.100 ms, Bubble Time: 21.821 ms, Imbalance Overhead: 0.055 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.500 ms
GPU 0, Compute+Comm Time: 43.551 ms, Bubble Time: 40.936 ms, Imbalance Overhead: 0.013 ms
GPU 1, Compute+Comm Time: 42.112 ms, Bubble Time: 40.353 ms, Imbalance Overhead: 2.034 ms
GPU 2, Compute+Comm Time: 42.112 ms, Bubble Time: 39.835 ms, Imbalance Overhead: 2.553 ms
GPU 3, Compute+Comm Time: 42.112 ms, Bubble Time: 39.373 ms, Imbalance Overhead: 3.015 ms
GPU 4, Compute+Comm Time: 42.112 ms, Bubble Time: 38.848 ms, Imbalance Overhead: 3.540 ms
GPU 5, Compute+Comm Time: 42.112 ms, Bubble Time: 38.435 ms, Imbalance Overhead: 3.953 ms
GPU 6, Compute+Comm Time: 42.112 ms, Bubble Time: 37.848 ms, Imbalance Overhead: 4.540 ms
GPU 7, Compute+Comm Time: 45.388 ms, Bubble Time: 37.813 ms, Imbalance Overhead: 1.299 ms
The estimated cost of the whole pipeline: 135.950 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 112.470 ms
Partition 0 [0, 17) has cost: 112.470 ms
Partition 1 [17, 33) has cost: 107.880 ms
Partition 2 [33, 49) has cost: 107.880 ms
Partition 3 [49, 65) has cost: 109.722 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 56.453 ms
GPU 0, Compute+Comm Time: 31.973 ms, Bubble Time: 23.209 ms, Imbalance Overhead: 1.271 ms
GPU 1, Compute+Comm Time: 31.315 ms, Bubble Time: 23.676 ms, Imbalance Overhead: 1.462 ms
GPU 2, Compute+Comm Time: 31.315 ms, Bubble Time: 24.277 ms, Imbalance Overhead: 0.861 ms
GPU 3, Compute+Comm Time: 31.517 ms, Bubble Time: 24.936 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.853 ms
GPU 0, Compute+Comm Time: 51.678 ms, Bubble Time: 41.174 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 50.957 ms, Bubble Time: 39.959 ms, Imbalance Overhead: 1.937 ms
GPU 2, Compute+Comm Time: 50.957 ms, Bubble Time: 38.854 ms, Imbalance Overhead: 3.042 ms
GPU 3, Compute+Comm Time: 52.596 ms, Bubble Time: 37.908 ms, Imbalance Overhead: 2.348 ms
    The estimated cost with 2 DP ways is 156.770 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 220.349 ms
Partition 0 [0, 33) has cost: 220.349 ms
Partition 1 [33, 65) has cost: 217.602 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 90.610 ms
GPU 0, Compute+Comm Time: 60.125 ms, Bubble Time: 29.297 ms, Imbalance Overhead: 1.188 ms
GPU 1, Compute+Comm Time: 59.897 ms, Bubble Time: 30.713 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 121.833 ms
GPU 0, Compute+Comm Time: 80.212 ms, Bubble Time: 41.621 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 80.669 ms, Bubble Time: 38.819 ms, Imbalance Overhead: 2.344 ms
    The estimated cost with 4 DP ways is 223.064 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 437.951 ms
Partition 0 [0, 65) has cost: 437.951 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 231.756 ms
GPU 0, Compute+Comm Time: 231.756 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 253.456 ms
GPU 0, Compute+Comm Time: 253.456 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 509.472 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 6 initializing the weights for op[0, 457)...
+++++++++ Node 4 initializing the weights for op[0, 457)...
+++++++++ Node 5 initializing the weights for op[0, 457)...
+++++++++ Node 1 initializing the weights for op[0, 457)...
+++++++++ Node 0 initializing the weights for op[0, 457)...
+++++++++ Node 7 initializing the weights for op[0, 457)...
+++++++++ Node 2 initializing the weights for op[0, 457)...
+++++++++ Node 3 initializing the weights for op[0, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.1845
	Epoch 50:	Loss 2.9626
	Epoch 75:	Loss 2.7779
	Epoch 100:	Loss 2.6204
Node 0, Pre/Post-Pipelining: 8.336 / 16.963 ms, Bubble: 0.746 ms, Compute: 203.996 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 1, Pre/Post-Pipelining: 8.339 / 17.011 ms, Bubble: 0.488 ms, Compute: 204.203 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 2, Pre/Post-Pipelining: 8.346 / 17.075 ms, Bubble: 0.580 ms, Compute: 204.035 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 3, Pre/Post-Pipelining: 8.351 / 17.147 ms, Bubble: 0.063 ms, Compute: 204.479 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 4, Pre/Post-Pipelining: 8.350 / 17.187 ms, Bubble: 0.438 ms, Compute: 204.064 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 8.336 / 17.121 ms, Bubble: 0.750 ms, Compute: 203.828 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 5, Pre/Post-Pipelining: 8.344 / 17.195 ms, Bubble: 0.563 ms, Compute: 203.929 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 6, Pre/Post-Pipelining: 8.350 / 17.225 ms, Bubble: 0.115 ms, Compute: 204.344 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.336 ms
Cluster-Wide Average, Post-Pipelining Overhead: 16.963 ms
Cluster-Wide Average, Bubble: 0.746 ms
Cluster-Wide Average, Compute: 203.996 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.015 ms
Node 0, GPU memory consumption: 19.053 GB
Node 2, GPU memory consumption: 18.165 GB
Node 4, GPU memory consumption: 18.139 GB
Node 1, GPU memory consumption: 18.165 GB
Node 5, GPU memory consumption: 18.163 GB
Node 3, GPU memory consumption: 18.141 GB
Node 6, GPU memory consumption: 18.163 GB
Node 7, GPU memory consumption: 18.139 GB
Node 0, Graph-Level Communication Throughput: 47.429 Gbps, Time: 121.816 ms
Node 1, Graph-Level Communication Throughput: 58.701 Gbps, Time: 117.075 ms
Node 2, Graph-Level Communication Throughput: 39.930 Gbps, Time: 121.775 ms
Node 3, Graph-Level Communication Throughput: 41.220 Gbps, Time: 123.066 ms
Node 4, Graph-Level Communication Throughput: 19.747 Gbps, Time: 126.389 ms
Node 5, Graph-Level Communication Throughput: 9.965 Gbps, Time: 127.375 ms
Node 6, Graph-Level Communication Throughput: 17.717 Gbps, Time: 126.358 ms
Node 7, Graph-Level Communication Throughput: 10.744 Gbps, Time: 128.001 ms
------------------------node id 0,  per-epoch time: 0.230112 s---------------
------------------------node id 1,  per-epoch time: 0.230114 s---------------
------------------------node id 2,  per-epoch time: 0.230115 s---------------
------------------------node id 3,  per-epoch time: 0.230115 s---------------
------------------------node id 4,  per-epoch time: 0.230114 s---------------
------------------------node id 5,  per-epoch time: 0.230115 s---------------
------------------------node id 6,  per-epoch time: 0.230110 s---------------
------------------------node id 7,  per-epoch time: 0.230114 s---------------
************ Profiling Results ************
	Bubble: 26.425527 (ms) (11.37 percentage)
	Compute: 56.748639 (ms) (24.42 percentage)
	GraphCommComputeOverhead: 8.849614 (ms) (3.81 percentage)
	GraphCommNetwork: 123.985986 (ms) (53.34 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 16.419078 (ms) (7.06 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 3.488 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 3.496 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
