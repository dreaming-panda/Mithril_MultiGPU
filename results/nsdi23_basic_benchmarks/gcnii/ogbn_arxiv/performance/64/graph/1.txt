Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.042 seconds.
        It takes 0.048 seconds.
        It takes 0.046 seconds.
Building the Feature Vector...
        It takes 0.052 seconds.
        It takes 0.052 seconds.
        It takes 0.058 seconds.
Building the Feature Vector...
        It takes 0.058 seconds.
Building the Feature Vector...
        It takes 0.060 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.023 seconds.
        It takes 0.027 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.026 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.857 Gbps (per GPU), 510.857 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.817 Gbps (per GPU), 510.533 Gbps (aggregated)
The layer-level communication performance: 63.815 Gbps (per GPU), 510.523 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.779 Gbps (per GPU), 510.233 Gbps (aggregated)
The layer-level communication performance: 63.774 Gbps (per GPU), 510.196 Gbps (aggregated)
The layer-level communication performance: 63.745 Gbps (per GPU), 509.960 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.738 Gbps (per GPU), 509.901 Gbps (aggregated)
The layer-level communication performance: 63.733 Gbps (per GPU), 509.862 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.726 Gbps (per GPU), 1309.809 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.718 Gbps (per GPU), 1309.745 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.717 Gbps (per GPU), 1309.735 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.726 Gbps (per GPU), 1309.809 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.718 Gbps (per GPU), 1309.742 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.719 Gbps (per GPU), 1309.754 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.719 Gbps (per GPU), 1309.754 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.721 Gbps (per GPU), 1309.768 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.076 Gbps (per GPU), 912.608 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.075 Gbps (per GPU), 912.604 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.071 Gbps (per GPU), 912.571 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.075 Gbps (per GPU), 912.598 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.072 Gbps (per GPU), 912.578 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.065 Gbps (per GPU), 912.522 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.073 Gbps (per GPU), 912.585 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.006 Gbps (per GPU), 912.046 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.837 Gbps (per GPU), 366.695 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.837 Gbps (per GPU), 366.694 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.837 Gbps (per GPU), 366.696 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.836 Gbps (per GPU), 366.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.837 Gbps (per GPU), 366.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.837 Gbps (per GPU), 366.692 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.837 Gbps (per GPU), 366.693 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.836 Gbps (per GPU), 366.692 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.58ms  0.91ms  1.15ms  1.97 21.17K  0.46M
 chk_1  0.58ms  0.95ms  1.19ms  2.04 21.17K  0.55M
 chk_2  0.58ms  0.88ms  1.12ms  1.92 21.17K  0.39M
 chk_3  0.58ms  0.85ms  1.08ms  1.85 21.17K  0.24M
 chk_4  0.58ms  0.81ms  1.05ms  1.80 21.17K  0.17M
 chk_5  0.58ms  0.83ms  1.06ms  1.82 21.17K  0.22M
 chk_6  0.58ms  0.82ms  1.05ms  1.80 21.17K  0.16M
 chk_7  0.58ms  0.80ms  1.04ms  1.78 21.17K  0.12M
   Avg  0.58  0.86  1.09
   Max  0.58  0.95  1.19
   Min  0.58  0.80  1.04
 Ratio  1.00  1.18  1.14
   Var  0.00  0.00  0.00
Profiling takes 0.330 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 59.454 ms
Partition 0 [0, 9) has cost: 59.454 ms
Partition 1 [9, 17) has cost: 54.793 ms
Partition 2 [17, 25) has cost: 54.793 ms
Partition 3 [25, 33) has cost: 54.793 ms
Partition 4 [33, 41) has cost: 54.793 ms
Partition 5 [41, 49) has cost: 54.793 ms
Partition 6 [49, 57) has cost: 54.793 ms
Partition 7 [57, 65) has cost: 56.673 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 45.569 ms
GPU 0, Compute+Comm Time: 24.342 ms, Bubble Time: 20.338 ms, Imbalance Overhead: 0.889 ms
GPU 1, Compute+Comm Time: 23.004 ms, Bubble Time: 20.356 ms, Imbalance Overhead: 2.209 ms
GPU 2, Compute+Comm Time: 23.004 ms, Bubble Time: 20.649 ms, Imbalance Overhead: 1.916 ms
GPU 3, Compute+Comm Time: 23.004 ms, Bubble Time: 20.871 ms, Imbalance Overhead: 1.694 ms
GPU 4, Compute+Comm Time: 23.004 ms, Bubble Time: 21.182 ms, Imbalance Overhead: 1.383 ms
GPU 5, Compute+Comm Time: 23.004 ms, Bubble Time: 21.459 ms, Imbalance Overhead: 1.106 ms
GPU 6, Compute+Comm Time: 23.004 ms, Bubble Time: 21.760 ms, Imbalance Overhead: 0.805 ms
GPU 7, Compute+Comm Time: 23.418 ms, Bubble Time: 22.107 ms, Imbalance Overhead: 0.045 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 85.541 ms
GPU 0, Compute+Comm Time: 44.117 ms, Bubble Time: 41.423 ms, Imbalance Overhead: 0.001 ms
GPU 1, Compute+Comm Time: 42.651 ms, Bubble Time: 40.842 ms, Imbalance Overhead: 2.048 ms
GPU 2, Compute+Comm Time: 42.651 ms, Bubble Time: 40.332 ms, Imbalance Overhead: 2.557 ms
GPU 3, Compute+Comm Time: 42.651 ms, Bubble Time: 39.869 ms, Imbalance Overhead: 3.020 ms
GPU 4, Compute+Comm Time: 42.651 ms, Bubble Time: 39.344 ms, Imbalance Overhead: 3.546 ms
GPU 5, Compute+Comm Time: 42.651 ms, Bubble Time: 38.928 ms, Imbalance Overhead: 3.962 ms
GPU 6, Compute+Comm Time: 42.651 ms, Bubble Time: 38.348 ms, Imbalance Overhead: 4.541 ms
GPU 7, Compute+Comm Time: 45.974 ms, Bubble Time: 38.303 ms, Imbalance Overhead: 1.264 ms
The estimated cost of the whole pipeline: 137.665 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 114.247 ms
Partition 0 [0, 17) has cost: 114.247 ms
Partition 1 [17, 33) has cost: 109.586 ms
Partition 2 [33, 49) has cost: 109.586 ms
Partition 3 [49, 65) has cost: 111.466 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 57.225 ms
GPU 0, Compute+Comm Time: 32.420 ms, Bubble Time: 23.539 ms, Imbalance Overhead: 1.266 ms
GPU 1, Compute+Comm Time: 31.750 ms, Bubble Time: 24.010 ms, Imbalance Overhead: 1.465 ms
GPU 2, Compute+Comm Time: 31.750 ms, Bubble Time: 24.614 ms, Imbalance Overhead: 0.861 ms
GPU 3, Compute+Comm Time: 31.958 ms, Bubble Time: 25.267 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 94.023 ms
GPU 0, Compute+Comm Time: 52.361 ms, Bubble Time: 41.663 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 51.626 ms, Bubble Time: 40.462 ms, Imbalance Overhead: 1.935 ms
GPU 2, Compute+Comm Time: 51.626 ms, Bubble Time: 39.352 ms, Imbalance Overhead: 3.045 ms
GPU 3, Compute+Comm Time: 53.289 ms, Bubble Time: 38.425 ms, Imbalance Overhead: 2.308 ms
    The estimated cost with 2 DP ways is 158.811 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 223.833 ms
Partition 0 [0, 33) has cost: 223.833 ms
Partition 1 [33, 65) has cost: 221.052 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 91.180 ms
GPU 0, Compute+Comm Time: 60.504 ms, Bubble Time: 29.480 ms, Imbalance Overhead: 1.195 ms
GPU 1, Compute+Comm Time: 60.272 ms, Bubble Time: 30.908 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 122.710 ms
GPU 0, Compute+Comm Time: 80.790 ms, Bubble Time: 41.920 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 81.252 ms, Bubble Time: 39.099 ms, Imbalance Overhead: 2.359 ms
    The estimated cost with 4 DP ways is 224.584 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 444.885 ms
Partition 0 [0, 65) has cost: 444.885 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 231.639 ms
GPU 0, Compute+Comm Time: 231.639 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 253.551 ms
GPU 0, Compute+Comm Time: 253.551 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 509.449 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 457)...
+++++++++ Node 1 initializing the weights for op[0, 457)...
+++++++++ Node 5 initializing the weights for op[0, 457)...
+++++++++ Node 0 initializing the weights for op[0, 457)...
+++++++++ Node 2 initializing the weights for op[0, 457)...
+++++++++ Node 3 initializing the weights for op[0, 457)...
+++++++++ Node 6 initializing the weights for op[0, 457)...
+++++++++ Node 7 initializing the weights for op[0, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.1746
	Epoch 50:	Loss 2.9605
	Epoch 75:	Loss 2.7782
Node 3, Pre/Post-Pipelining: 8.342 / 16.925 ms, Bubble: 0.099 ms, Compute: 205.459 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 4, Pre/Post-Pipelining: 8.346 / 16.958 ms, Bubble: 0.544 ms, Compute: 204.978 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 2, Pre/Post-Pipelining: 8.348 / 16.940 ms, Bubble: 0.525 ms, Compute: 205.007 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 7, Pre/Post-Pipelining: 8.341 / 16.980 ms, Bubble: 0.807 ms, Compute: 204.694 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 1, Pre/Post-Pipelining: 8.342 / 16.928 ms, Bubble: 0.542 ms, Compute: 205.008 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 5, Pre/Post-Pipelining: 8.346 / 16.967 ms, Bubble: 0.607 ms, Compute: 204.902 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
	Epoch 100:	Loss 2.6102
Node 0, Pre/Post-Pipelining: 8.341 / 16.911 ms, Bubble: 0.667 ms, Compute: 204.905 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 8.355 / 17.110 ms, Bubble: 0.076 ms, Compute: 205.282 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.341 ms
Cluster-Wide Average, Post-Pipelining Overhead: 16.911 ms
Cluster-Wide Average, Bubble: 0.667 ms
Cluster-Wide Average, Compute: 204.905 ms
Cluster-Wide Average, Communication: 0.010 ms
Cluster-Wide Average, Imbalance: 0.017 ms
Node 0, GPU memory consumption: 19.053 GB
Node 1, GPU memory consumption: 18.165 GB
Node 2, GPU memory consumption: 18.165 GB
Node 3, GPU memory consumption: 18.141 GB
Node 4, GPU memory consumption: 18.139 GB
Node 5, GPU memory consumption: 18.163 GB
Node 7, GPU memory consumption: 18.139 GB
Node 6, GPU memory consumption: 18.163 GB
Node 0, Graph-Level Communication Throughput: 47.531 Gbps, Time: 121.555 ms
Node 1, Graph-Level Communication Throughput: 58.648 Gbps, Time: 117.180 ms
Node 2, Graph-Level Communication Throughput: 39.672 Gbps, Time: 122.569 ms
Node 3, Graph-Level Communication Throughput: 40.615 Gbps, Time: 124.897 ms
Node 4, Graph-Level Communication Throughput: 19.425 Gbps, Time: 128.489 ms
Node 5, Graph-Level Communication Throughput: 9.854 Gbps, Time: 128.805 ms
Node 6, Graph-Level Communication Throughput: 17.623 Gbps, Time: 127.032 ms
Node 7, Graph-Level Communication Throughput: 10.664 Gbps, Time: 128.954 ms
------------------------node id 0,  per-epoch time: 0.230901 s---------------
------------------------node id 1,  per-epoch time: 0.230898 s---------------
------------------------node id 2,  per-epoch time: 0.230898 s---------------
------------------------node id 3,  per-epoch time: 0.230898 s---------------
------------------------node id 4,  per-epoch time: 0.230897 s---------------
------------------------node id 5,  per-epoch time: 0.230900 s---------------
------------------------node id 6,  per-epoch time: 0.230900 s---------------
------------------------node id 7,  per-epoch time: 0.230897 s---------------
************ Profiling Results ************
	Bubble: 26.324317 (ms) (11.29 percentage)
	Compute: 56.720685 (ms) (24.32 percentage)
	GraphCommComputeOverhead: 8.819661 (ms) (3.78 percentage)
	GraphCommNetwork: 124.939072 (ms) (53.57 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 16.413934 (ms) (7.04 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 3.488 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 3.496 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
