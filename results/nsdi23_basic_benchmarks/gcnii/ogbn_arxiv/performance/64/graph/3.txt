Initialized node 5 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.045 seconds.
        It takes 0.049 seconds.
        It takes 0.053 seconds.
        It takes 0.053 seconds.
        It takes 0.050 seconds.
        It takes 0.056 seconds.
        It takes 0.053 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
        It takes 0.024 seconds.
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 65)
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.680 Gbps (per GPU), 509.439 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.639 Gbps (per GPU), 509.110 Gbps (aggregated)
The layer-level communication performance: 63.638 Gbps (per GPU), 509.104 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.602 Gbps (per GPU), 508.820 Gbps (aggregated)
The layer-level communication performance: 63.598 Gbps (per GPU), 508.782 Gbps (aggregated)
The layer-level communication performance: 63.569 Gbps (per GPU), 508.550 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.562 Gbps (per GPU), 508.494 Gbps (aggregated)
The layer-level communication performance: 63.557 Gbps (per GPU), 508.458 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.754 Gbps (per GPU), 1318.034 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.762 Gbps (per GPU), 1318.092 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.758 Gbps (per GPU), 1318.060 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.743 Gbps (per GPU), 1317.940 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.753 Gbps (per GPU), 1318.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.754 Gbps (per GPU), 1318.031 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.755 Gbps (per GPU), 1318.037 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.753 Gbps (per GPU), 1318.021 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.579 Gbps (per GPU), 916.632 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.578 Gbps (per GPU), 916.624 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.576 Gbps (per GPU), 916.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.578 Gbps (per GPU), 916.625 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.576 Gbps (per GPU), 916.609 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.575 Gbps (per GPU), 916.603 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.577 Gbps (per GPU), 916.619 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.514 Gbps (per GPU), 916.114 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.135 Gbps (per GPU), 361.077 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.135 Gbps (per GPU), 361.079 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.135 Gbps (per GPU), 361.078 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.134 Gbps (per GPU), 361.074 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.135 Gbps (per GPU), 361.077 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.134 Gbps (per GPU), 361.075 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.135 Gbps (per GPU), 361.077 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.134 Gbps (per GPU), 361.071 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.57ms  0.90ms  1.13ms  1.97 21.17K  0.46M
 chk_1  0.57ms  0.93ms  1.16ms  2.03 21.17K  0.55M
 chk_2  0.57ms  0.87ms  1.10ms  1.92 21.17K  0.39M
 chk_3  0.57ms  0.83ms  1.06ms  1.86 21.17K  0.24M
 chk_4  0.57ms  0.80ms  1.03ms  1.80 21.17K  0.17M
 chk_5  0.57ms  0.81ms  1.04ms  1.82 21.17K  0.22M
 chk_6  0.57ms  0.80ms  1.03ms  1.80 21.17K  0.16M
 chk_7  0.57ms  0.79ms  1.02ms  1.78 21.17K  0.12M
   Avg  0.57  0.84  1.07
   Max  0.57  0.93  1.16
   Min  0.57  0.79  1.02
 Ratio  1.00  1.18  1.14
   Var  0.00  0.00  0.00
Profiling takes 0.314 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 58.520 ms
Partition 0 [0, 9) has cost: 58.520 ms
Partition 1 [9, 17) has cost: 53.932 ms
Partition 2 [17, 25) has cost: 53.932 ms
Partition 3 [25, 33) has cost: 53.932 ms
Partition 4 [33, 41) has cost: 53.932 ms
Partition 5 [41, 49) has cost: 53.932 ms
Partition 6 [49, 57) has cost: 53.932 ms
Partition 7 [57, 65) has cost: 55.771 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 44.924 ms
GPU 0, Compute+Comm Time: 24.001 ms, Bubble Time: 20.057 ms, Imbalance Overhead: 0.866 ms
GPU 1, Compute+Comm Time: 22.688 ms, Bubble Time: 20.076 ms, Imbalance Overhead: 2.159 ms
GPU 2, Compute+Comm Time: 22.688 ms, Bubble Time: 20.356 ms, Imbalance Overhead: 1.880 ms
GPU 3, Compute+Comm Time: 22.688 ms, Bubble Time: 20.573 ms, Imbalance Overhead: 1.663 ms
GPU 4, Compute+Comm Time: 22.688 ms, Bubble Time: 20.879 ms, Imbalance Overhead: 1.357 ms
GPU 5, Compute+Comm Time: 22.688 ms, Bubble Time: 21.146 ms, Imbalance Overhead: 1.090 ms
GPU 6, Compute+Comm Time: 22.688 ms, Bubble Time: 21.450 ms, Imbalance Overhead: 0.786 ms
GPU 7, Compute+Comm Time: 23.094 ms, Bubble Time: 21.788 ms, Imbalance Overhead: 0.042 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.566 ms
GPU 0, Compute+Comm Time: 43.569 ms, Bubble Time: 40.980 ms, Imbalance Overhead: 0.016 ms
GPU 1, Compute+Comm Time: 42.136 ms, Bubble Time: 40.389 ms, Imbalance Overhead: 2.040 ms
GPU 2, Compute+Comm Time: 42.136 ms, Bubble Time: 39.865 ms, Imbalance Overhead: 2.565 ms
GPU 3, Compute+Comm Time: 42.136 ms, Bubble Time: 39.399 ms, Imbalance Overhead: 3.031 ms
GPU 4, Compute+Comm Time: 42.136 ms, Bubble Time: 38.864 ms, Imbalance Overhead: 3.566 ms
GPU 5, Compute+Comm Time: 42.136 ms, Bubble Time: 38.435 ms, Imbalance Overhead: 3.995 ms
GPU 6, Compute+Comm Time: 42.136 ms, Bubble Time: 37.863 ms, Imbalance Overhead: 4.566 ms
GPU 7, Compute+Comm Time: 45.411 ms, Bubble Time: 37.829 ms, Imbalance Overhead: 1.325 ms
The estimated cost of the whole pipeline: 135.964 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 112.452 ms
Partition 0 [0, 17) has cost: 112.452 ms
Partition 1 [17, 33) has cost: 107.864 ms
Partition 2 [33, 49) has cost: 107.864 ms
Partition 3 [49, 65) has cost: 109.703 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 56.533 ms
GPU 0, Compute+Comm Time: 32.034 ms, Bubble Time: 23.265 ms, Imbalance Overhead: 1.234 ms
GPU 1, Compute+Comm Time: 31.377 ms, Bubble Time: 23.711 ms, Imbalance Overhead: 1.446 ms
GPU 2, Compute+Comm Time: 31.377 ms, Bubble Time: 24.294 ms, Imbalance Overhead: 0.863 ms
GPU 3, Compute+Comm Time: 31.580 ms, Bubble Time: 24.953 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 93.062 ms
GPU 0, Compute+Comm Time: 51.803 ms, Bubble Time: 41.259 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 51.089 ms, Bubble Time: 40.031 ms, Imbalance Overhead: 1.942 ms
GPU 2, Compute+Comm Time: 51.089 ms, Bubble Time: 38.920 ms, Imbalance Overhead: 3.054 ms
GPU 3, Compute+Comm Time: 52.727 ms, Bubble Time: 38.008 ms, Imbalance Overhead: 2.326 ms
    The estimated cost with 2 DP ways is 157.075 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 220.316 ms
Partition 0 [0, 33) has cost: 220.316 ms
Partition 1 [33, 65) has cost: 217.567 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 90.432 ms
GPU 0, Compute+Comm Time: 60.017 ms, Bubble Time: 29.261 ms, Imbalance Overhead: 1.153 ms
GPU 1, Compute+Comm Time: 59.790 ms, Bubble Time: 30.642 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 121.739 ms
GPU 0, Compute+Comm Time: 80.145 ms, Bubble Time: 41.593 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 80.607 ms, Bubble Time: 38.782 ms, Imbalance Overhead: 2.349 ms
    The estimated cost with 4 DP ways is 222.779 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 437.882 ms
Partition 0 [0, 65) has cost: 437.882 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 234.555 ms
GPU 0, Compute+Comm Time: 234.555 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 256.342 ms
GPU 0, Compute+Comm Time: 256.342 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 515.441 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 457)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 457)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 457)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 457)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 457)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 457)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 457)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 457)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 457)...
+++++++++ Node 0 initializing the weights for op[0, 457)...
+++++++++ Node 3 initializing the weights for op[0, 457)...
+++++++++ Node 1 initializing the weights for op[0, 457)...
+++++++++ Node 2 initializing the weights for op[0, 457)...
+++++++++ Node 5 initializing the weights for op[0, 457)...
+++++++++ Node 7 initializing the weights for op[0, 457)...
+++++++++ Node 6 initializing the weights for op[0, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2275
	Epoch 50:	Loss 3.0056
	Epoch 75:	Loss 2.8475
	Epoch 100:	Loss 2.6968
Node 0, Pre/Post-Pipelining: 8.335 / 16.324 ms, Bubble: 0.682 ms, Compute: 205.197 ms, Comm: 0.008 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 8.342 / 16.399 ms, Bubble: 0.583 ms, Compute: 205.213 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 4, Pre/Post-Pipelining: 8.346 / 16.489 ms, Bubble: 0.437 ms, Compute: 205.270 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 3, Pre/Post-Pipelining: 8.348 / 16.475 ms, Bubble: 0.057 ms, Compute: 205.665 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 7, Pre/Post-Pipelining: 8.339 / 16.532 ms, Bubble: 0.687 ms, Compute: 204.979 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 1, Pre/Post-Pipelining: 8.344 / 16.462 ms, Bubble: 0.377 ms, Compute: 205.356 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 5, Pre/Post-Pipelining: 8.348 / 16.505 ms, Bubble: 0.614 ms, Compute: 205.071 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 6, Pre/Post-Pipelining: 8.353 / 16.633 ms, Bubble: 0.124 ms, Compute: 205.431 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.335 ms
Cluster-Wide Average, Post-Pipelining Overhead: 16.324 ms
Cluster-Wide Average, Bubble: 0.682 ms
Cluster-Wide Average, Compute: 205.197 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.017 ms
Node 0, GPU memory consumption: 19.053 GB
Node 3, GPU memory consumption: 18.141 GB
Node 2, GPU memory consumption: 18.165 GB
Node 1, GPU memory consumption: 18.165 GB
Node 4, GPU memory consumption: 18.139 GB
Node 5, GPU memory consumption: 18.163 GB
Node 6, GPU memory consumption: 18.163 GB
Node 7, GPU memory consumption: 18.139 GB
Node 0, Graph-Level Communication Throughput: 46.915 Gbps, Time: 123.153 ms
Node 1, Graph-Level Communication Throughput: 58.652 Gbps, Time: 117.171 ms
Node 2, Graph-Level Communication Throughput: 39.365 Gbps, Time: 123.524 ms
Node 3, Graph-Level Communication Throughput: 40.667 Gbps, Time: 124.738 ms
Node 4, Graph-Level Communication Throughput: 19.400 Gbps, Time: 128.651 ms
Node 5, Graph-Level Communication Throughput: 9.837 Gbps, Time: 129.029 ms
Node 6, Graph-Level Communication Throughput: 17.560 Gbps, Time: 127.486 ms
Node 7, Graph-Level Communication Throughput: 10.659 Gbps, Time: 129.023 ms
------------------------node id 0,  per-epoch time: 0.230612 s---------------
------------------------node id 1,  per-epoch time: 0.230616 s---------------
------------------------node id 2,  per-epoch time: 0.230614 s---------------
------------------------node id 3,  per-epoch time: 0.230616 s---------------
------------------------node id 4,  per-epoch time: 0.230615 s---------------
------------------------node id 5,  per-epoch time: 0.230617 s---------------
------------------------node id 6,  per-epoch time: 0.230615 s---------------
------------------------node id 7,  per-epoch time: 0.230615 s---------------
************ Profiling Results ************
	Bubble: 25.788130 (ms) (11.07 percentage)
	Compute: 56.590231 (ms) (24.30 percentage)
	GraphCommComputeOverhead: 8.783466 (ms) (3.77 percentage)
	GraphCommNetwork: 125.350420 (ms) (53.82 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 16.400869 (ms) (7.04 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 3.488 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 3.496 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
