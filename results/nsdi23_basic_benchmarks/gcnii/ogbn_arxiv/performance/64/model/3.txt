Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.050 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.062 seconds.
Building the CSC structure...
        It takes 0.067 seconds.
Building the CSC structure...
        It takes 0.041 seconds.
        It takes 0.044 seconds.
        It takes 0.045 seconds.
        It takes 0.051 seconds.
        It takes 0.050 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
        It takes 0.053 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.024 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 5292
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.989 Gbps (per GPU), 511.909 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.947 Gbps (per GPU), 511.576 Gbps (aggregated)
The layer-level communication performance: 63.946 Gbps (per GPU), 511.568 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.910 Gbps (per GPU), 511.279 Gbps (aggregated)
The layer-level communication performance: 63.906 Gbps (per GPU), 511.250 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.878 Gbps (per GPU), 511.026 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.872 Gbps (per GPU), 510.973 Gbps (aggregated)
The layer-level communication performance: 63.867 Gbps (per GPU), 510.934 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.949 Gbps (per GPU), 1327.594 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.948 Gbps (per GPU), 1327.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.943 Gbps (per GPU), 1327.548 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.950 Gbps (per GPU), 1327.604 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.948 Gbps (per GPU), 1327.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.929 Gbps (per GPU), 1327.430 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.945 Gbps (per GPU), 1327.558 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.946 Gbps (per GPU), 1327.567 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.156 Gbps (per GPU), 913.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.157 Gbps (per GPU), 913.257 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.158 Gbps (per GPU), 913.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.157 Gbps (per GPU), 913.259 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.153 Gbps (per GPU), 913.224 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.148 Gbps (per GPU), 913.181 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.149 Gbps (per GPU), 913.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.107 Gbps (per GPU), 912.858 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.537 Gbps (per GPU), 364.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.537 Gbps (per GPU), 364.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.537 Gbps (per GPU), 364.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.537 Gbps (per GPU), 364.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.537 Gbps (per GPU), 364.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.537 Gbps (per GPU), 364.294 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.537 Gbps (per GPU), 364.292 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.537 Gbps (per GPU), 364.294 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.28ms  0.35ms  0.53ms  1.93  5.55K  0.06M
 chk_1  0.28ms  0.34ms  0.48ms  1.75  5.75K  0.05M
 chk_2  0.27ms  0.35ms  0.48ms  1.81  5.20K  0.07M
 chk_3  0.28ms  0.35ms  0.49ms  1.76  5.57K  0.06M
 chk_4  0.27ms  0.35ms  0.49ms  1.83  5.05K  0.08M
 chk_5  0.26ms  0.36ms  0.49ms  1.88  4.73K  0.11M
 chk_6  0.27ms  0.35ms  0.48ms  1.82  4.98K  0.08M
 chk_7  0.26ms  0.35ms  0.49ms  1.84  5.01K  0.09M
 chk_8  0.28ms  0.34ms  0.48ms  1.74  5.73K  0.05M
 chk_9  0.26ms  0.35ms  0.48ms  1.86  4.54K  0.11M
chk_10  0.27ms  0.35ms  0.49ms  1.78  5.36K  0.07M
chk_11  0.27ms  0.35ms  0.49ms  1.78  5.39K  0.08M
chk_12  0.28ms  0.34ms  0.48ms  1.72  5.77K  0.05M
chk_13  0.27ms  0.35ms  0.48ms  1.77  5.43K  0.06M
chk_14  0.27ms  0.35ms  0.49ms  1.79  5.46K  0.06M
chk_15  0.28ms  0.34ms  0.48ms  1.71  5.88K  0.04M
chk_16  0.27ms  0.35ms  0.49ms  1.79  5.50K  0.06M
chk_17  0.33ms  0.35ms  0.49ms  1.48  4.86K  0.09M
chk_18  0.27ms  0.36ms  0.50ms  1.83  5.39K  0.07M
chk_19  0.27ms  0.35ms  0.48ms  1.80  5.20K  0.07M
chk_20  0.27ms  0.35ms  0.48ms  1.77  5.51K  0.06M
chk_21  0.28ms  0.34ms  0.48ms  1.72  5.81K  0.05M
chk_22  0.27ms  0.35ms  0.49ms  1.79  5.32K  0.07M
chk_23  0.27ms  0.35ms  0.49ms  1.79  5.39K  0.07M
chk_24  0.26ms  0.35ms  0.48ms  1.87  4.62K  0.11M
chk_25  0.27ms  0.35ms  0.49ms  1.81  5.04K  0.08M
chk_26  0.26ms  0.35ms  0.48ms  1.87  4.55K  0.11M
chk_27  0.27ms  0.35ms  0.48ms  1.78  5.30K  0.06M
chk_28  0.28ms  0.35ms  0.49ms  1.76  5.58K  0.06M
chk_29  0.26ms  0.35ms  0.49ms  1.85  4.98K  0.09M
chk_30  0.27ms  0.36ms  0.49ms  1.77  5.50K  0.07M
chk_31  0.27ms  0.35ms  0.49ms  1.78  5.38K  0.07M
   Avg  0.27  0.35  0.49
   Max  0.33  0.36  0.53
   Min  0.26  0.34  0.48
 Ratio  1.28  1.06  1.11
   Var  0.00  0.00  0.00
Profiling takes 0.553 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 97.985 ms
Partition 0 [0, 9) has cost: 97.985 ms
Partition 1 [9, 17) has cost: 89.257 ms
Partition 2 [17, 25) has cost: 89.257 ms
Partition 3 [25, 33) has cost: 89.257 ms
Partition 4 [33, 41) has cost: 89.257 ms
Partition 5 [41, 49) has cost: 89.257 ms
Partition 6 [49, 57) has cost: 89.257 ms
Partition 7 [57, 65) has cost: 93.683 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 47.246 ms
GPU 0, Compute+Comm Time: 39.136 ms, Bubble Time: 8.110 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.066 ms, Bubble Time: 8.200 ms, Imbalance Overhead: 2.981 ms
GPU 2, Compute+Comm Time: 36.066 ms, Bubble Time: 8.268 ms, Imbalance Overhead: 2.912 ms
GPU 3, Compute+Comm Time: 36.066 ms, Bubble Time: 8.334 ms, Imbalance Overhead: 2.846 ms
GPU 4, Compute+Comm Time: 36.066 ms, Bubble Time: 8.384 ms, Imbalance Overhead: 2.797 ms
GPU 5, Compute+Comm Time: 36.066 ms, Bubble Time: 8.443 ms, Imbalance Overhead: 2.737 ms
GPU 6, Compute+Comm Time: 36.066 ms, Bubble Time: 8.502 ms, Imbalance Overhead: 2.679 ms
GPU 7, Compute+Comm Time: 37.042 ms, Bubble Time: 8.549 ms, Imbalance Overhead: 1.655 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.495 ms
GPU 0, Compute+Comm Time: 67.481 ms, Bubble Time: 15.218 ms, Imbalance Overhead: 1.796 ms
GPU 1, Compute+Comm Time: 64.031 ms, Bubble Time: 15.184 ms, Imbalance Overhead: 5.280 ms
GPU 2, Compute+Comm Time: 64.031 ms, Bubble Time: 15.163 ms, Imbalance Overhead: 5.300 ms
GPU 3, Compute+Comm Time: 64.031 ms, Bubble Time: 15.112 ms, Imbalance Overhead: 5.351 ms
GPU 4, Compute+Comm Time: 64.031 ms, Bubble Time: 15.037 ms, Imbalance Overhead: 5.426 ms
GPU 5, Compute+Comm Time: 64.031 ms, Bubble Time: 14.978 ms, Imbalance Overhead: 5.486 ms
GPU 6, Compute+Comm Time: 64.031 ms, Bubble Time: 14.874 ms, Imbalance Overhead: 5.590 ms
GPU 7, Compute+Comm Time: 69.688 ms, Bubble Time: 14.789 ms, Imbalance Overhead: 0.018 ms
The estimated cost of the whole pipeline: 138.328 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 187.242 ms
Partition 0 [0, 17) has cost: 187.242 ms
Partition 1 [17, 33) has cost: 178.514 ms
Partition 2 [33, 49) has cost: 178.514 ms
Partition 3 [49, 65) has cost: 182.940 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 55.110 ms
GPU 0, Compute+Comm Time: 46.488 ms, Bubble Time: 8.525 ms, Imbalance Overhead: 0.097 ms
GPU 1, Compute+Comm Time: 44.943 ms, Bubble Time: 8.653 ms, Imbalance Overhead: 1.514 ms
GPU 2, Compute+Comm Time: 44.943 ms, Bubble Time: 8.708 ms, Imbalance Overhead: 1.459 ms
GPU 3, Compute+Comm Time: 45.430 ms, Bubble Time: 8.733 ms, Imbalance Overhead: 0.947 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 90.397 ms
GPU 0, Compute+Comm Time: 74.842 ms, Bubble Time: 14.251 ms, Imbalance Overhead: 1.304 ms
GPU 1, Compute+Comm Time: 73.098 ms, Bubble Time: 14.305 ms, Imbalance Overhead: 2.995 ms
GPU 2, Compute+Comm Time: 73.098 ms, Bubble Time: 14.249 ms, Imbalance Overhead: 3.051 ms
GPU 3, Compute+Comm Time: 75.996 ms, Bubble Time: 14.187 ms, Imbalance Overhead: 0.214 ms
    The estimated cost with 2 DP ways is 152.782 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 365.756 ms
Partition 0 [0, 33) has cost: 365.756 ms
Partition 1 [33, 65) has cost: 361.455 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 86.274 ms
GPU 0, Compute+Comm Time: 75.651 ms, Bubble Time: 9.404 ms, Imbalance Overhead: 1.219 ms
GPU 1, Compute+Comm Time: 75.121 ms, Bubble Time: 9.618 ms, Imbalance Overhead: 1.535 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 119.129 ms
GPU 0, Compute+Comm Time: 104.222 ms, Bubble Time: 13.198 ms, Imbalance Overhead: 1.708 ms
GPU 1, Compute+Comm Time: 104.817 ms, Bubble Time: 13.205 ms, Imbalance Overhead: 1.107 ms
    The estimated cost with 4 DP ways is 215.672 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 727.211 ms
Partition 0 [0, 65) has cost: 727.211 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 263.735 ms
GPU 0, Compute+Comm Time: 263.735 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 293.450 ms
GPU 0, Compute+Comm Time: 293.450 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 585.044 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.4101
	Epoch 50:	Loss 4.0842
	Epoch 75:	Loss 4.1605
	Epoch 100:	Loss 4.0399
Node 4, Pre/Post-Pipelining: 1.063 / 10.740 ms, Bubble: 39.839 ms, Compute: 91.757 ms, Comm: 21.428 ms, Imbalance: 18.967 ms
Node 7, Pre/Post-Pipelining: 1.063 / 25.099 ms, Bubble: 25.143 ms, Compute: 114.183 ms, Comm: 13.899 ms, Imbalance: 4.016 ms
Node 5, Pre/Post-Pipelining: 1.059 / 10.798 ms, Bubble: 39.611 ms, Compute: 94.727 ms, Comm: 20.311 ms, Imbalance: 17.083 ms
Node 6, Pre/Post-Pipelining: 1.061 / 10.786 ms, Bubble: 39.581 ms, Compute: 92.341 ms, Comm: 17.684 ms, Imbalance: 22.155 ms
Node 1, Pre/Post-Pipelining: 1.059 / 10.803 ms, Bubble: 40.381 ms, Compute: 95.660 ms, Comm: 18.143 ms, Imbalance: 17.354 ms
Node 0, Pre/Post-Pipelining: 1.062 / 10.788 ms, Bubble: 40.581 ms, Compute: 105.677 ms, Comm: 14.658 ms, Imbalance: 10.415 ms
Node 2, Pre/Post-Pipelining: 1.057 / 10.792 ms, Bubble: 40.299 ms, Compute: 95.534 ms, Comm: 19.784 ms, Imbalance: 16.015 ms
Node 3, Pre/Post-Pipelining: 1.053 / 10.863 ms, Bubble: 39.539 ms, Compute: 96.934 ms, Comm: 20.637 ms, Imbalance: 14.312 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.062 ms
Cluster-Wide Average, Post-Pipelining Overhead: 10.788 ms
Cluster-Wide Average, Bubble: 40.581 ms
Cluster-Wide Average, Compute: 105.677 ms
Cluster-Wide Average, Communication: 14.658 ms
Cluster-Wide Average, Imbalance: 10.415 ms
Node 0, GPU memory consumption: 6.522 GB
Node 4, GPU memory consumption: 4.901 GB
Node 3, GPU memory consumption: 4.901 GB
Node 6, GPU memory consumption: 4.924 GB
Node 5, GPU memory consumption: 4.924 GB
Node 7, GPU memory consumption: 5.005 GB
Node 2, GPU memory consumption: 4.924 GB
Node 1, GPU memory consumption: 4.924 GB
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.183849 s---------------
------------------------node id 1,  per-epoch time: 0.183845 s---------------
------------------------node id 4,  per-epoch time: 0.183845 s---------------
------------------------node id 2,  per-epoch time: 0.183848 s---------------
------------------------node id 5,  per-epoch time: 0.183846 s---------------
------------------------node id 3,  per-epoch time: 0.183854 s---------------
------------------------node id 6,  per-epoch time: 0.183846 s---------------
------------------------node id 7,  per-epoch time: 0.183846 s---------------
************ Profiling Results ************
	Bubble: 90.636391 (ms) (47.20 percentage)
	Compute: 97.098407 (ms) (50.57 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.273437 (ms) (2.23 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.130 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 1.139 GB
	Aggregated layer-level communication throughput: 530.127 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
