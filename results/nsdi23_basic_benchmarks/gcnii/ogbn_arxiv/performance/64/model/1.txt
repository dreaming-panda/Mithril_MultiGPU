Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.045 seconds.
        It takes 0.043 seconds.
        It takes 0.052 seconds.
        It takes 0.054 seconds.
        It takes 0.051 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.055 seconds.
        It takes 0.053 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.026 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.026 seconds.
        It takes 0.026 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.927 Gbps (per GPU), 511.412 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.886 Gbps (per GPU), 511.089 Gbps (aggregated)
The layer-level communication performance: 63.885 Gbps (per GPU), 511.078 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.848 Gbps (per GPU), 510.784 Gbps (aggregated)
The layer-level communication performance: 63.843 Gbps (per GPU), 510.745 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.815 Gbps (per GPU), 510.517 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.808 Gbps (per GPU), 510.462 Gbps (aggregated)
The layer-level communication performance: 63.803 Gbps (per GPU), 510.425 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.462 Gbps (per GPU), 1315.698 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.462 Gbps (per GPU), 1315.696 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.459 Gbps (per GPU), 1315.670 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.460 Gbps (per GPU), 1315.676 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.458 Gbps (per GPU), 1315.667 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.447 Gbps (per GPU), 1315.576 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.459 Gbps (per GPU), 1315.670 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.459 Gbps (per GPU), 1315.670 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.051 Gbps (per GPU), 912.408 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.051 Gbps (per GPU), 912.412 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.046 Gbps (per GPU), 912.366 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.047 Gbps (per GPU), 912.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.042 Gbps (per GPU), 912.339 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.043 Gbps (per GPU), 912.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.038 Gbps (per GPU), 912.303 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.029 Gbps (per GPU), 912.230 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 46.023 Gbps (per GPU), 368.187 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.023 Gbps (per GPU), 368.187 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.023 Gbps (per GPU), 368.183 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.023 Gbps (per GPU), 368.184 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.023 Gbps (per GPU), 368.187 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.023 Gbps (per GPU), 368.186 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.023 Gbps (per GPU), 368.183 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.023 Gbps (per GPU), 368.185 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.28ms  0.36ms  0.51ms  1.78  5.55K  0.06M
 chk_1  0.29ms  0.36ms  0.50ms  1.75  5.75K  0.05M
 chk_2  0.28ms  0.36ms  0.50ms  1.82  5.20K  0.07M
 chk_3  0.29ms  0.37ms  0.51ms  1.78  5.57K  0.06M
 chk_4  0.28ms  0.36ms  0.51ms  1.84  5.05K  0.08M
 chk_5  0.27ms  0.37ms  0.51ms  1.91  4.73K  0.11M
 chk_6  0.27ms  0.36ms  0.50ms  1.83  4.98K  0.08M
 chk_7  0.27ms  0.37ms  0.51ms  1.85  5.01K  0.09M
 chk_8  0.28ms  0.36ms  0.50ms  1.76  5.73K  0.05M
 chk_9  0.27ms  0.36ms  0.51ms  1.88  4.54K  0.11M
chk_10  0.28ms  0.36ms  0.51ms  1.80  5.36K  0.07M
chk_11  0.28ms  0.37ms  0.51ms  1.81  5.39K  0.08M
chk_12  0.29ms  0.36ms  0.50ms  1.75  5.77K  0.05M
chk_13  0.28ms  0.36ms  0.50ms  1.79  5.43K  0.06M
chk_14  0.28ms  0.36ms  0.51ms  1.79  5.46K  0.06M
chk_15  0.29ms  0.36ms  0.50ms  1.75  5.88K  0.04M
chk_16  0.28ms  0.37ms  0.51ms  1.82  5.50K  0.06M
chk_17  0.27ms  0.37ms  0.51ms  1.88  4.86K  0.09M
chk_18  0.28ms  0.37ms  0.52ms  1.85  5.39K  0.07M
chk_19  0.28ms  0.36ms  0.51ms  1.81  5.20K  0.07M
chk_20  0.28ms  0.36ms  0.51ms  1.78  5.51K  0.06M
chk_21  0.29ms  0.36ms  0.50ms  1.73  5.81K  0.05M
chk_22  0.28ms  0.36ms  0.51ms  1.79  5.32K  0.07M
chk_23  0.28ms  0.37ms  0.51ms  1.83  5.39K  0.07M
chk_24  0.27ms  0.36ms  0.50ms  1.88  4.62K  0.11M
chk_25  0.28ms  0.36ms  0.51ms  1.83  5.04K  0.08M
chk_26  0.27ms  0.36ms  0.50ms  1.88  4.55K  0.11M
chk_27  0.28ms  0.36ms  0.51ms  1.80  5.30K  0.06M
chk_28  0.28ms  0.36ms  0.51ms  1.78  5.58K  0.06M
chk_29  0.28ms  0.37ms  0.51ms  1.85  4.98K  0.09M
chk_30  0.28ms  0.36ms  0.51ms  1.78  5.50K  0.07M
chk_31  0.28ms  0.36ms  0.51ms  1.79  5.38K  0.07M
   Avg  0.28  0.36  0.51
   Max  0.29  0.37  0.52
   Min  0.27  0.36  0.50
 Ratio  1.07  1.05  1.05
   Var  0.00  0.00  0.00
Profiling takes 0.569 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 101.994 ms
Partition 0 [0, 9) has cost: 101.994 ms
Partition 1 [9, 17) has cost: 93.044 ms
Partition 2 [17, 25) has cost: 93.044 ms
Partition 3 [25, 33) has cost: 93.044 ms
Partition 4 [33, 41) has cost: 93.044 ms
Partition 5 [41, 49) has cost: 93.044 ms
Partition 6 [49, 57) has cost: 93.044 ms
Partition 7 [57, 65) has cost: 97.625 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 49.029 ms
GPU 0, Compute+Comm Time: 40.619 ms, Bubble Time: 8.409 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 37.444 ms, Bubble Time: 8.489 ms, Imbalance Overhead: 3.096 ms
GPU 2, Compute+Comm Time: 37.444 ms, Bubble Time: 8.568 ms, Imbalance Overhead: 3.017 ms
GPU 3, Compute+Comm Time: 37.444 ms, Bubble Time: 8.646 ms, Imbalance Overhead: 2.939 ms
GPU 4, Compute+Comm Time: 37.444 ms, Bubble Time: 8.709 ms, Imbalance Overhead: 2.876 ms
GPU 5, Compute+Comm Time: 37.444 ms, Bubble Time: 8.774 ms, Imbalance Overhead: 2.811 ms
GPU 6, Compute+Comm Time: 37.444 ms, Bubble Time: 8.839 ms, Imbalance Overhead: 2.746 ms
GPU 7, Compute+Comm Time: 38.466 ms, Bubble Time: 8.885 ms, Imbalance Overhead: 1.678 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 87.488 ms
GPU 0, Compute+Comm Time: 70.009 ms, Bubble Time: 15.849 ms, Imbalance Overhead: 1.630 ms
GPU 1, Compute+Comm Time: 66.450 ms, Bubble Time: 15.801 ms, Imbalance Overhead: 5.237 ms
GPU 2, Compute+Comm Time: 66.450 ms, Bubble Time: 15.695 ms, Imbalance Overhead: 5.343 ms
GPU 3, Compute+Comm Time: 66.450 ms, Bubble Time: 15.632 ms, Imbalance Overhead: 5.406 ms
GPU 4, Compute+Comm Time: 66.450 ms, Bubble Time: 15.541 ms, Imbalance Overhead: 5.497 ms
GPU 5, Compute+Comm Time: 66.450 ms, Bubble Time: 15.463 ms, Imbalance Overhead: 5.575 ms
GPU 6, Compute+Comm Time: 66.450 ms, Bubble Time: 15.339 ms, Imbalance Overhead: 5.699 ms
GPU 7, Compute+Comm Time: 72.225 ms, Bubble Time: 15.263 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 143.343 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 195.038 ms
Partition 0 [0, 17) has cost: 195.038 ms
Partition 1 [17, 33) has cost: 186.087 ms
Partition 2 [33, 49) has cost: 186.087 ms
Partition 3 [49, 65) has cost: 190.668 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 56.844 ms
GPU 0, Compute+Comm Time: 47.994 ms, Bubble Time: 8.786 ms, Imbalance Overhead: 0.065 ms
GPU 1, Compute+Comm Time: 46.398 ms, Bubble Time: 8.905 ms, Imbalance Overhead: 1.541 ms
GPU 2, Compute+Comm Time: 46.398 ms, Bubble Time: 8.989 ms, Imbalance Overhead: 1.457 ms
GPU 3, Compute+Comm Time: 46.910 ms, Bubble Time: 9.017 ms, Imbalance Overhead: 0.917 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 93.034 ms
GPU 0, Compute+Comm Time: 77.305 ms, Bubble Time: 14.789 ms, Imbalance Overhead: 0.940 ms
GPU 1, Compute+Comm Time: 75.520 ms, Bubble Time: 14.680 ms, Imbalance Overhead: 2.834 ms
GPU 2, Compute+Comm Time: 75.520 ms, Bubble Time: 14.597 ms, Imbalance Overhead: 2.917 ms
GPU 3, Compute+Comm Time: 78.447 ms, Bubble Time: 14.418 ms, Imbalance Overhead: 0.169 ms
    The estimated cost with 2 DP ways is 157.372 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 381.125 ms
Partition 0 [0, 33) has cost: 381.125 ms
Partition 1 [33, 65) has cost: 376.756 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 87.943 ms
GPU 0, Compute+Comm Time: 77.087 ms, Bubble Time: 9.559 ms, Imbalance Overhead: 1.297 ms
GPU 1, Compute+Comm Time: 76.544 ms, Bubble Time: 9.813 ms, Imbalance Overhead: 1.586 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 121.525 ms
GPU 0, Compute+Comm Time: 106.566 ms, Bubble Time: 13.551 ms, Imbalance Overhead: 1.408 ms
GPU 1, Compute+Comm Time: 107.149 ms, Bubble Time: 13.299 ms, Imbalance Overhead: 1.077 ms
    The estimated cost with 4 DP ways is 219.942 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 757.881 ms
Partition 0 [0, 65) has cost: 757.881 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 262.612 ms
GPU 0, Compute+Comm Time: 262.612 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 292.914 ms
GPU 0, Compute+Comm Time: 292.914 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 583.302 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.1279
	Epoch 50:	Loss 4.2739
	Epoch 75:	Loss 4.8956
	Epoch 100:	Loss 4.9943
Node 0, Pre/Post-Pipelining: 1.054 / 10.671 ms, Bubble: 40.431 ms, Compute: 105.550 ms, Comm: 15.276 ms, Imbalance: 10.103 ms
Node 1, Pre/Post-Pipelining: 1.052 / 10.626 ms, Bubble: 40.322 ms, Compute: 92.026 ms, Comm: 19.270 ms, Imbalance: 20.086 ms
Node 5, Pre/Post-Pipelining: 1.053 / 10.630 ms, Bubble: 39.746 ms, Compute: 91.879 ms, Comm: 21.202 ms, Imbalance: 19.034 ms
Node 7, Pre/Post-Pipelining: 1.056 / 24.896 ms, Bubble: 25.330 ms, Compute: 113.956 ms, Comm: 13.963 ms, Imbalance: 4.100 ms
Node 2, Pre/Post-Pipelining: 1.052 / 10.656 ms, Bubble: 40.241 ms, Compute: 92.844 ms, Comm: 20.369 ms, Imbalance: 18.296 ms
Node 4, Pre/Post-Pipelining: 1.053 / 10.641 ms, Bubble: 39.768 ms, Compute: 92.104 ms, Comm: 21.790 ms, Imbalance: 18.340 ms
Node 6, Pre/Post-Pipelining: 1.051 / 10.704 ms, Bubble: 39.504 ms, Compute: 93.939 ms, Comm: 18.576 ms, Imbalance: 19.733 ms
Node 3, Pre/Post-Pipelining: 1.049 / 10.741 ms, Bubble: 39.550 ms, Compute: 95.784 ms, Comm: 20.587 ms, Imbalance: 15.619 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.054 ms
Cluster-Wide Average, Post-Pipelining Overhead: 10.671 ms
Cluster-Wide Average, Bubble: 40.431 ms
Cluster-Wide Average, Compute: 105.550 ms
Cluster-Wide Average, Communication: 15.276 ms
Cluster-Wide Average, Imbalance: 10.103 ms
Node 0, GPU memory consumption: 6.522 GB
Node 1, GPU memory consumption: 4.924 GB
Node 3, GPU memory consumption: 4.901 GB
Node 4, GPU memory consumption: 4.901 GB
Node 2, GPU memory consumption: 4.924 GB
Node 5, GPU memory consumption: 4.924 GB
Node 6, GPU memory consumption: 4.924 GB
Node 7, GPU memory consumption: 5.005 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.183752 s---------------
------------------------node id 1,  per-epoch time: 0.183753 s---------------
------------------------node id 2,  per-epoch time: 0.183754 s---------------
------------------------node id 3,  per-epoch time: 0.183755 s---------------
------------------------node id 4,  per-epoch time: 0.183755 s---------------
------------------------node id 5,  per-epoch time: 0.183753 s---------------
------------------------node id 6,  per-epoch time: 0.183754 s---------------
------------------------node id 7,  per-epoch time: 0.183752 s---------------
************ Profiling Results ************
	Bubble: 91.351013 (ms) (47.68 percentage)
	Compute: 95.998744 (ms) (50.11 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.235187 (ms) (2.21 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.130 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 1.139 GB
	Aggregated layer-level communication throughput: 514.373 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
