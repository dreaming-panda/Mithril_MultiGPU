Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.056 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.064 seconds.
Building the CSC structure...
        It takes 0.044 seconds.
        It takes 0.049 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
        It takes 0.051 seconds.
        It takes 0.053 seconds.
        It takes 0.054 seconds.
        It takes 0.059 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.022 seconds.
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.062 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 64
The number of hidden units: 64
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.025 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 4, layer [33, 41)
GPU 5, layer [41, 49)
GPU 6, layer [49, 57)
GPU 7, layer [57, 65)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.934 Gbps (per GPU), 511.475 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.893 Gbps (per GPU), 511.141 Gbps (aggregated)
The layer-level communication performance: 63.890 Gbps (per GPU), 511.122 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.855 Gbps (per GPU), 510.841 Gbps (aggregated)
The layer-level communication performance: 63.849 Gbps (per GPU), 510.790 Gbps (aggregated)
The layer-level communication performance: 63.820 Gbps (per GPU), 510.563 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.814 Gbps (per GPU), 510.511 Gbps (aggregated)
The layer-level communication performance: 63.809 Gbps (per GPU), 510.469 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.278 Gbps (per GPU), 1330.222 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.261 Gbps (per GPU), 1330.090 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.275 Gbps (per GPU), 1330.199 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.263 Gbps (per GPU), 1330.100 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.276 Gbps (per GPU), 1330.212 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.277 Gbps (per GPU), 1330.215 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.276 Gbps (per GPU), 1330.212 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.280 Gbps (per GPU), 1330.242 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.786 Gbps (per GPU), 918.285 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.784 Gbps (per GPU), 918.274 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.785 Gbps (per GPU), 918.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.782 Gbps (per GPU), 918.255 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.774 Gbps (per GPU), 918.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.776 Gbps (per GPU), 918.210 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.769 Gbps (per GPU), 918.153 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.757 Gbps (per GPU), 918.054 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.953 Gbps (per GPU), 359.626 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.953 Gbps (per GPU), 359.628 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.953 Gbps (per GPU), 359.627 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.953 Gbps (per GPU), 359.625 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.953 Gbps (per GPU), 359.622 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.953 Gbps (per GPU), 359.624 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.953 Gbps (per GPU), 359.621 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.953 Gbps (per GPU), 359.623 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.27ms  0.35ms  0.49ms  1.78  5.55K  0.06M
 chk_1  0.27ms  0.34ms  0.51ms  1.87  5.75K  0.05M
 chk_2  0.27ms  0.35ms  0.49ms  1.80  5.20K  0.07M
 chk_3  0.27ms  0.35ms  0.48ms  1.76  5.57K  0.06M
 chk_4  0.27ms  0.35ms  0.49ms  1.81  5.05K  0.08M
 chk_5  0.26ms  0.36ms  0.49ms  1.90  4.73K  0.11M
 chk_6  0.27ms  0.34ms  0.48ms  1.81  4.98K  0.08M
 chk_7  0.27ms  0.35ms  0.48ms  1.83  5.01K  0.09M
 chk_8  0.28ms  0.34ms  0.48ms  1.73  5.73K  0.05M
 chk_9  0.26ms  0.35ms  0.48ms  1.88  4.54K  0.11M
chk_10  0.27ms  0.35ms  0.48ms  1.77  5.36K  0.07M
chk_11  0.27ms  0.35ms  0.49ms  1.80  5.39K  0.08M
chk_12  0.28ms  0.34ms  0.48ms  1.74  5.77K  0.05M
chk_13  0.27ms  0.34ms  0.48ms  1.78  5.43K  0.06M
chk_14  0.27ms  0.35ms  0.49ms  1.79  5.46K  0.06M
chk_15  0.28ms  0.34ms  0.48ms  1.72  5.88K  0.04M
chk_16  0.27ms  0.35ms  0.49ms  1.79  5.50K  0.06M
chk_17  0.26ms  0.35ms  0.49ms  1.85  4.86K  0.09M
chk_18  0.27ms  0.36ms  0.50ms  1.82  5.39K  0.07M
chk_19  0.27ms  0.35ms  0.48ms  1.79  5.20K  0.07M
chk_20  0.27ms  0.34ms  0.48ms  1.77  5.51K  0.06M
chk_21  0.28ms  0.34ms  0.47ms  1.71  5.81K  0.05M
chk_22  0.27ms  0.35ms  0.48ms  1.78  5.32K  0.07M
chk_23  0.27ms  0.35ms  0.49ms  1.80  5.39K  0.07M
chk_24  0.26ms  0.35ms  0.48ms  1.88  4.62K  0.11M
chk_25  0.27ms  0.35ms  0.48ms  1.81  5.04K  0.08M
chk_26  0.26ms  0.34ms  0.48ms  1.87  4.55K  0.11M
chk_27  0.27ms  0.35ms  0.48ms  1.78  5.30K  0.06M
chk_28  0.28ms  0.35ms  0.48ms  1.75  5.58K  0.06M
chk_29  0.27ms  0.35ms  0.49ms  1.83  4.98K  0.09M
chk_30  0.27ms  0.34ms  0.49ms  1.79  5.50K  0.07M
chk_31  0.27ms  0.35ms  0.48ms  1.78  5.38K  0.07M
   Avg  0.27  0.35  0.48
   Max  0.28  0.36  0.51
   Min  0.26  0.34  0.47
 Ratio  1.09  1.05  1.08
   Var  0.00  0.00  0.00
Profiling takes 0.546 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 97.431 ms
Partition 0 [0, 9) has cost: 97.431 ms
Partition 1 [9, 17) has cost: 88.807 ms
Partition 2 [17, 25) has cost: 88.807 ms
Partition 3 [25, 33) has cost: 88.807 ms
Partition 4 [33, 41) has cost: 88.807 ms
Partition 5 [41, 49) has cost: 88.807 ms
Partition 6 [49, 57) has cost: 88.807 ms
Partition 7 [57, 65) has cost: 93.214 ms
The optimal partitioning:
[0, 9)
[9, 17)
[17, 25)
[25, 33)
[33, 41)
[41, 49)
[49, 57)
[57, 65)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 47.087 ms
GPU 0, Compute+Comm Time: 39.017 ms, Bubble Time: 8.071 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 35.960 ms, Bubble Time: 8.153 ms, Imbalance Overhead: 2.974 ms
GPU 2, Compute+Comm Time: 35.960 ms, Bubble Time: 8.238 ms, Imbalance Overhead: 2.889 ms
GPU 3, Compute+Comm Time: 35.960 ms, Bubble Time: 8.305 ms, Imbalance Overhead: 2.822 ms
GPU 4, Compute+Comm Time: 35.960 ms, Bubble Time: 8.364 ms, Imbalance Overhead: 2.764 ms
GPU 5, Compute+Comm Time: 35.960 ms, Bubble Time: 8.435 ms, Imbalance Overhead: 2.692 ms
GPU 6, Compute+Comm Time: 35.960 ms, Bubble Time: 8.511 ms, Imbalance Overhead: 2.616 ms
GPU 7, Compute+Comm Time: 36.962 ms, Bubble Time: 8.563 ms, Imbalance Overhead: 1.562 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 83.897 ms
GPU 0, Compute+Comm Time: 67.102 ms, Bubble Time: 15.184 ms, Imbalance Overhead: 1.612 ms
GPU 1, Compute+Comm Time: 63.696 ms, Bubble Time: 15.172 ms, Imbalance Overhead: 5.030 ms
GPU 2, Compute+Comm Time: 63.696 ms, Bubble Time: 15.088 ms, Imbalance Overhead: 5.114 ms
GPU 3, Compute+Comm Time: 63.696 ms, Bubble Time: 15.023 ms, Imbalance Overhead: 5.178 ms
GPU 4, Compute+Comm Time: 63.696 ms, Bubble Time: 14.926 ms, Imbalance Overhead: 5.276 ms
GPU 5, Compute+Comm Time: 63.696 ms, Bubble Time: 14.852 ms, Imbalance Overhead: 5.349 ms
GPU 6, Compute+Comm Time: 63.696 ms, Bubble Time: 14.728 ms, Imbalance Overhead: 5.474 ms
GPU 7, Compute+Comm Time: 69.263 ms, Bubble Time: 14.634 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 137.534 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 186.238 ms
Partition 0 [0, 17) has cost: 186.238 ms
Partition 1 [17, 33) has cost: 177.614 ms
Partition 2 [33, 49) has cost: 177.614 ms
Partition 3 [49, 65) has cost: 182.021 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 54.962 ms
GPU 0, Compute+Comm Time: 46.342 ms, Bubble Time: 8.487 ms, Imbalance Overhead: 0.132 ms
GPU 1, Compute+Comm Time: 44.802 ms, Bubble Time: 8.614 ms, Imbalance Overhead: 1.546 ms
GPU 2, Compute+Comm Time: 44.802 ms, Bubble Time: 8.687 ms, Imbalance Overhead: 1.472 ms
GPU 3, Compute+Comm Time: 45.320 ms, Bubble Time: 8.731 ms, Imbalance Overhead: 0.911 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 89.593 ms
GPU 0, Compute+Comm Time: 74.431 ms, Bubble Time: 14.243 ms, Imbalance Overhead: 0.919 ms
GPU 1, Compute+Comm Time: 72.726 ms, Bubble Time: 14.213 ms, Imbalance Overhead: 2.655 ms
GPU 2, Compute+Comm Time: 72.726 ms, Bubble Time: 14.103 ms, Imbalance Overhead: 2.764 ms
GPU 3, Compute+Comm Time: 75.550 ms, Bubble Time: 13.902 ms, Imbalance Overhead: 0.141 ms
    The estimated cost with 2 DP ways is 151.783 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 363.851 ms
Partition 0 [0, 33) has cost: 363.851 ms
Partition 1 [33, 65) has cost: 359.634 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 85.922 ms
GPU 0, Compute+Comm Time: 75.333 ms, Bubble Time: 9.342 ms, Imbalance Overhead: 1.248 ms
GPU 1, Compute+Comm Time: 74.834 ms, Bubble Time: 9.578 ms, Imbalance Overhead: 1.511 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 118.221 ms
GPU 0, Compute+Comm Time: 103.515 ms, Bubble Time: 13.187 ms, Imbalance Overhead: 1.520 ms
GPU 1, Compute+Comm Time: 104.079 ms, Bubble Time: 12.941 ms, Imbalance Overhead: 1.201 ms
    The estimated cost with 4 DP ways is 214.350 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 723.486 ms
Partition 0 [0, 65) has cost: 723.486 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 266.667 ms
GPU 0, Compute+Comm Time: 266.667 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 295.482 ms
GPU 0, Compute+Comm Time: 295.482 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 590.256 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [230, 286)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [286, 342)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [342, 398)
*** Node 6, constructing the helper classes...
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [62, 118)
*** Node 1, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [398, 457)
*** Node 7, constructing the helper classes...
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [118, 174)
*** Node 2, constructing the helper classes...
*** Node 3 owns the model-level partition [174, 230)
*** Node 3, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[62, 118)...
+++++++++ Node 2 initializing the weights for op[118, 174)...
+++++++++ Node 4 initializing the weights for op[230, 286)...
+++++++++ Node 3 initializing the weights for op[174, 230)...
+++++++++ Node 5 initializing the weights for op[286, 342)...
+++++++++ Node 6 initializing the weights for op[342, 398)...
+++++++++ Node 7 initializing the weights for op[398, 457)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.6864
	Epoch 50:	Loss 4.2102
	Epoch 75:	Loss 4.0244
	Epoch 100:	Loss 4.0290
Node 0, Pre/Post-Pipelining: 1.063 / 10.789 ms, Bubble: 40.314 ms, Compute: 100.437 ms, Comm: 15.088 ms, Imbalance: 13.563 ms
Node 1, Pre/Post-Pipelining: 1.057 / 10.807 ms, Bubble: 39.817 ms, Compute: 91.289 ms, Comm: 19.454 ms, Imbalance: 18.994 ms
Node 4, Pre/Post-Pipelining: 1.059 / 10.835 ms, Bubble: 39.139 ms, Compute: 91.972 ms, Comm: 21.015 ms, Imbalance: 17.769 ms
Node 5, Pre/Post-Pipelining: 1.051 / 10.887 ms, Bubble: 38.962 ms, Compute: 94.614 ms, Comm: 19.620 ms, Imbalance: 16.446 ms
Node 7, Pre/Post-Pipelining: 1.060 / 25.098 ms, Bubble: 24.641 ms, Compute: 112.350 ms, Comm: 13.777 ms, Imbalance: 4.515 ms
Node 6, Pre/Post-Pipelining: 1.051 / 10.966 ms, Bubble: 38.733 ms, Compute: 96.168 ms, Comm: 16.951 ms, Imbalance: 17.649 ms
Node 3, Pre/Post-Pipelining: 1.052 / 10.969 ms, Bubble: 38.923 ms, Compute: 96.298 ms, Comm: 20.640 ms, Imbalance: 13.574 ms
Node 2, Pre/Post-Pipelining: 1.055 / 10.851 ms, Bubble: 39.666 ms, Compute: 92.758 ms, Comm: 20.777 ms, Imbalance: 16.504 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.063 ms
Cluster-Wide Average, Post-Pipelining Overhead: 10.789 ms
Cluster-Wide Average, Bubble: 40.314 ms
Cluster-Wide Average, Compute: 100.437 ms
Cluster-Wide Average, Communication: 15.088 ms
Cluster-Wide Average, Imbalance: 13.563 ms
Node 0, GPU memory consumption: 6.522 GB
Node 1, GPU memory consumption: 4.924 GB
Node 2, GPU memory consumption: 4.924 GB
Node 3, GPU memory consumption: 4.901 GB
Node 4, GPU memory consumption: 4.901 GB
Node 5, GPU memory consumption: 4.924 GB
Node 6, GPU memory consumption: 4.924 GB
Node 7, GPU memory consumption: 5.006 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.181923 s---------------
------------------------node id 1,  per-epoch time: 0.181923 s---------------
------------------------node id 2,  per-epoch time: 0.181934 s---------------
------------------------node id 3,  per-epoch time: 0.181932 s---------------
------------------------node id 4,  per-epoch time: 0.181924 s---------------
------------------------node id 5,  per-epoch time: 0.181924 s---------------
------------------------node id 6,  per-epoch time: 0.181925 s---------------
------------------------node id 7,  per-epoch time: 0.181926 s---------------
************ Profiling Results ************
	Bubble: 89.903232 (ms) (47.35 percentage)
	Compute: 95.732867 (ms) (50.42 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.232022 (ms) (2.23 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.130 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.008 GB
	Total communication (cluster-wide, per-epoch): 1.139 GB
	Aggregated layer-level communication throughput: 527.326 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
