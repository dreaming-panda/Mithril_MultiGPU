Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.051 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.057 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.066 seconds.
Building the CSC structure...
        It takes 0.041 seconds.
        It takes 0.056 seconds.
        It takes 0.053 seconds.
        It takes 0.053 seconds.
        It takes 0.055 seconds.
        It takes 0.052 seconds.
Building the Feature Vector...
        It takes 0.054 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.061 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.058 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.028 seconds.
        It takes 0.023 seconds.
        It takes 0.023 seconds.
        It takes 0.024 seconds.
        It takes 0.170 seconds.
        It takes 0.170 seconds.
        It takes 0.160 seconds.
        It takes 0.169 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.058 seconds.
        It takes 0.058 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/32_parts
The number of GCNII layers: 128
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.059 seconds.
        It takes 0.059 seconds.
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 5546) 1-[5546, 11300) 2-[11300, 16503) 3-[16503, 22077) 4-[22077, 27123) 5-[27123, 31854) 6-[31854, 36834) 7-[36834, 41844) 8-[41844, 47574) ... 31-[163964, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 5292
GPU 0, layer [0, 17)
GPU 1, layer [17, 33)
GPU 2, layer [33, 49)
GPU 3, layer [49, 65)
GPU 4, layer [65, 81)
GPU 5, layer [81, 97)
GPU 6, layer [97, 113)
GPU 7, layer [113, 129)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
169343, 2484941, 2484941
Number of vertices per chunk: 5292
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.954 Gbps (per GPU), 511.635 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.913 Gbps (per GPU), 511.300 Gbps (aggregated)
The layer-level communication performance: 63.911 Gbps (per GPU), 511.290 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.876 Gbps (per GPU), 511.007 Gbps (aggregated)
The layer-level communication performance: 63.873 Gbps (per GPU), 510.988 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.841 Gbps (per GPU), 510.727 Gbps (aggregated)
The layer-level communication performance: 63.834 Gbps (per GPU), 510.669 Gbps (aggregated)
The layer-level communication performance: 63.829 Gbps (per GPU), 510.634 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.108 Gbps (per GPU), 1312.861 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.113 Gbps (per GPU), 1312.903 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.116 Gbps (per GPU), 1312.929 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.108 Gbps (per GPU), 1312.867 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.097 Gbps (per GPU), 1312.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.105 Gbps (per GPU), 1312.842 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.102 Gbps (per GPU), 1312.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.086 Gbps (per GPU), 1312.685 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 111.061 Gbps (per GPU), 888.486 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 111.063 Gbps (per GPU), 888.500 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 111.061 Gbps (per GPU), 888.485 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 111.062 Gbps (per GPU), 888.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 111.052 Gbps (per GPU), 888.417 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 111.059 Gbps (per GPU), 888.476 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 111.057 Gbps (per GPU), 888.452 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 111.045 Gbps (per GPU), 888.356 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.712 Gbps (per GPU), 357.699 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.713 Gbps (per GPU), 357.701 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.712 Gbps (per GPU), 357.698 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.712 Gbps (per GPU), 357.698 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.713 Gbps (per GPU), 357.701 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.712 Gbps (per GPU), 357.697 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.712 Gbps (per GPU), 357.696 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.712 Gbps (per GPU), 357.697 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.45ms  0.97ms  1.18ms  2.63  5.55K  0.06M
 chk_1  0.48ms  1.01ms  1.22ms  2.56  5.75K  0.05M
 chk_2  0.44ms  0.98ms  1.17ms  2.69  5.20K  0.07M
 chk_3  0.45ms  1.00ms  1.21ms  2.70  5.57K  0.06M
 chk_4  0.43ms  0.98ms  1.17ms  2.72  5.05K  0.08M
 chk_5  0.42ms  1.00ms  1.18ms  2.83  4.73K  0.11M
 chk_6  0.43ms  0.96ms  1.15ms  2.71  4.98K  0.08M
 chk_7  0.43ms  0.99ms  1.18ms  2.75  5.01K  0.09M
 chk_8  0.45ms  1.01ms  1.21ms  2.68  5.73K  0.05M
 chk_9  0.41ms  0.92ms  1.10ms  2.67  4.54K  0.11M
chk_10  0.44ms  0.98ms  1.18ms  2.69  5.36K  0.07M
chk_11  0.44ms  1.01ms  1.20ms  2.71  5.39K  0.08M
chk_12  0.46ms  1.01ms  1.22ms  2.67  5.77K  0.05M
chk_13  0.44ms  0.99ms  1.20ms  2.71  5.43K  0.06M
chk_14  0.44ms  0.97ms  1.32ms  2.96  5.46K  0.06M
chk_15  0.46ms  1.01ms  1.22ms  2.66  5.88K  0.04M
chk_16  0.45ms  0.99ms  1.19ms  2.68  5.50K  0.06M
chk_17  0.46ms  1.00ms  1.19ms  2.57  4.86K  0.09M
chk_18  0.44ms  1.04ms  1.24ms  2.80  5.39K  0.07M
chk_19  0.44ms  0.98ms  1.18ms  2.69  5.20K  0.07M
chk_20  0.45ms  0.97ms  1.18ms  2.62  5.51K  0.06M
chk_21  0.46ms  0.98ms  1.20ms  2.61  5.81K  0.05M
chk_22  0.44ms  0.97ms  1.17ms  2.66  5.32K  0.07M
chk_23  0.44ms  1.01ms  1.21ms  2.75  5.39K  0.07M
chk_24  0.41ms  0.96ms  1.15ms  2.78  4.62K  0.11M
chk_25  0.43ms  0.99ms  1.19ms  2.76  5.04K  0.08M
chk_26  0.41ms  0.92ms  1.10ms  2.68  4.55K  0.11M
chk_27  0.44ms  0.96ms  1.16ms  2.62  5.30K  0.06M
chk_28  0.45ms  0.99ms  1.20ms  2.67  5.58K  0.06M
chk_29  0.43ms  1.00ms  1.19ms  2.80  4.98K  0.09M
chk_30  0.45ms  1.03ms  1.19ms  2.67  5.50K  0.07M
chk_31  0.44ms  0.98ms  1.18ms  2.68  5.38K  0.07M
   Avg  0.44  0.99  1.19
   Max  0.48  1.04  1.32
   Min  0.41  0.92  1.10
 Ratio  1.16  1.13  1.20
   Var  0.00  0.00  0.00
Profiling takes 1.107 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 518.848 ms
Partition 0 [0, 17) has cost: 518.848 ms
Partition 1 [17, 33) has cost: 504.755 ms
Partition 2 [33, 49) has cost: 504.755 ms
Partition 3 [49, 65) has cost: 504.755 ms
Partition 4 [65, 81) has cost: 504.755 ms
Partition 5 [81, 97) has cost: 504.755 ms
Partition 6 [97, 113) has cost: 504.755 ms
Partition 7 [113, 129) has cost: 511.228 ms
The optimal partitioning:
[0, 17)
[17, 33)
[33, 49)
[49, 65)
[65, 81)
[81, 97)
[97, 113)
[113, 129)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 227.662 ms
GPU 0, Compute+Comm Time: 184.230 ms, Bubble Time: 39.883 ms, Imbalance Overhead: 3.549 ms
GPU 1, Compute+Comm Time: 179.776 ms, Bubble Time: 39.884 ms, Imbalance Overhead: 8.003 ms
GPU 2, Compute+Comm Time: 179.776 ms, Bubble Time: 40.079 ms, Imbalance Overhead: 7.808 ms
GPU 3, Compute+Comm Time: 179.776 ms, Bubble Time: 40.131 ms, Imbalance Overhead: 7.755 ms
GPU 4, Compute+Comm Time: 179.776 ms, Bubble Time: 40.294 ms, Imbalance Overhead: 7.593 ms
GPU 5, Compute+Comm Time: 179.776 ms, Bubble Time: 40.303 ms, Imbalance Overhead: 7.584 ms
GPU 6, Compute+Comm Time: 179.776 ms, Bubble Time: 40.444 ms, Imbalance Overhead: 7.442 ms
GPU 7, Compute+Comm Time: 181.174 ms, Bubble Time: 40.602 ms, Imbalance Overhead: 5.886 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 469.732 ms
GPU 0, Compute+Comm Time: 373.437 ms, Bubble Time: 83.447 ms, Imbalance Overhead: 12.847 ms
GPU 1, Compute+Comm Time: 368.362 ms, Bubble Time: 83.193 ms, Imbalance Overhead: 18.176 ms
GPU 2, Compute+Comm Time: 368.362 ms, Bubble Time: 83.722 ms, Imbalance Overhead: 17.647 ms
GPU 3, Compute+Comm Time: 368.362 ms, Bubble Time: 84.136 ms, Imbalance Overhead: 17.233 ms
GPU 4, Compute+Comm Time: 368.362 ms, Bubble Time: 84.331 ms, Imbalance Overhead: 17.038 ms
GPU 5, Compute+Comm Time: 368.362 ms, Bubble Time: 84.746 ms, Imbalance Overhead: 16.624 ms
GPU 6, Compute+Comm Time: 368.362 ms, Bubble Time: 84.826 ms, Imbalance Overhead: 16.543 ms
GPU 7, Compute+Comm Time: 378.001 ms, Bubble Time: 85.349 ms, Imbalance Overhead: 6.381 ms
The estimated cost of the whole pipeline: 732.263 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 1023.604 ms
Partition 0 [0, 33) has cost: 1023.604 ms
Partition 1 [33, 65) has cost: 1009.510 ms
Partition 2 [65, 97) has cost: 1009.510 ms
Partition 3 [97, 129) has cost: 1015.983 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 306.056 ms
GPU 0, Compute+Comm Time: 254.704 ms, Bubble Time: 47.640 ms, Imbalance Overhead: 3.711 ms
GPU 1, Compute+Comm Time: 252.449 ms, Bubble Time: 47.919 ms, Imbalance Overhead: 5.687 ms
GPU 2, Compute+Comm Time: 252.449 ms, Bubble Time: 48.012 ms, Imbalance Overhead: 5.595 ms
GPU 3, Compute+Comm Time: 253.145 ms, Bubble Time: 48.440 ms, Imbalance Overhead: 4.471 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 542.494 ms
GPU 0, Compute+Comm Time: 447.030 ms, Bubble Time: 85.187 ms, Imbalance Overhead: 10.277 ms
GPU 1, Compute+Comm Time: 444.444 ms, Bubble Time: 85.939 ms, Imbalance Overhead: 12.110 ms
GPU 2, Compute+Comm Time: 444.444 ms, Bubble Time: 86.576 ms, Imbalance Overhead: 11.473 ms
GPU 3, Compute+Comm Time: 449.360 ms, Bubble Time: 86.879 ms, Imbalance Overhead: 6.254 ms
    The estimated cost with 2 DP ways is 890.977 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 2033.114 ms
Partition 0 [0, 65) has cost: 2033.114 ms
Partition 1 [65, 129) has cost: 2025.494 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 575.574 ms
GPU 0, Compute+Comm Time: 501.667 ms, Bubble Time: 62.630 ms, Imbalance Overhead: 11.277 ms
GPU 1, Compute+Comm Time: 500.870 ms, Bubble Time: 63.827 ms, Imbalance Overhead: 10.877 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 798.284 ms
GPU 0, Compute+Comm Time: 695.775 ms, Bubble Time: 88.389 ms, Imbalance Overhead: 14.120 ms
GPU 1, Compute+Comm Time: 696.952 ms, Bubble Time: 88.676 ms, Imbalance Overhead: 12.656 ms
    The estimated cost with 4 DP ways is 1442.551 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 4058.608 ms
Partition 0 [0, 129) has cost: 4058.608 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 2056.235 ms
GPU 0, Compute+Comm Time: 2056.235 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 2253.951 ms
GPU 0, Compute+Comm Time: 2253.951 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 4525.695 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 118)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [230, 342)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [342, 454)
*** Node 3, constructing the helper classes...
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [118, 230)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [454, 566)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [566, 678)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [678, 790)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [790, 905)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[230, 342)...
+++++++++ Node 1 initializing the weights for op[118, 230)...
+++++++++ Node 0 initializing the weights for op[0, 118)...
+++++++++ Node 4 initializing the weights for op[454, 566)...
+++++++++ Node 3 initializing the weights for op[342, 454)...
+++++++++ Node 7 initializing the weights for op[790, 905)...
+++++++++ Node 5 initializing the weights for op[566, 678)...
+++++++++ Node 6 initializing the weights for op[678, 790)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 21.8615
	Epoch 50:	Loss 21.3439
	Epoch 75:	Loss 21.2663
Node 4, Pre/Post-Pipelining: 7.956 / 73.679 ms, Bubble: 131.410 ms, Compute: 490.995 ms, Comm: 90.474 ms, Imbalance: 26.023 ms
Node 7, Pre/Post-Pipelining: 7.953 / 87.905 ms, Bubble: 121.188 ms, Compute: 511.140 ms, Comm: 55.421 ms, Imbalance: 36.202 ms
Node 1, Pre/Post-Pipelining: 7.969 / 73.793 ms, Bubble: 130.950 ms, Compute: 494.235 ms, Comm: 71.368 ms, Imbalance: 40.475 ms
Node 3, Pre/Post-Pipelining: 7.969 / 73.984 ms, Bubble: 129.690 ms, Compute: 498.451 ms, Comm: 91.068 ms, Imbalance: 18.304 ms
	Epoch 100:	Loss 21.1626
Node 5, Pre/Post-Pipelining: 7.949 / 73.892 ms, Bubble: 132.856 ms, Compute: 497.369 ms, Comm: 78.763 ms, Imbalance: 29.401 ms
Node 6, Pre/Post-Pipelining: 7.943 / 74.086 ms, Bubble: 133.174 ms, Compute: 499.366 ms, Comm: 68.370 ms, Imbalance: 37.177 ms
Node 2, Pre/Post-Pipelining: 7.968 / 73.957 ms, Bubble: 131.401 ms, Compute: 495.853 ms, Comm: 81.450 ms, Imbalance: 29.095 ms
Node 0, Pre/Post-Pipelining: 7.972 / 73.979 ms, Bubble: 131.855 ms, Compute: 513.831 ms, Comm: 54.593 ms, Imbalance: 36.123 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 7.972 ms
Cluster-Wide Average, Post-Pipelining Overhead: 73.979 ms
Cluster-Wide Average, Bubble: 131.855 ms
Cluster-Wide Average, Compute: 513.831 ms
Cluster-Wide Average, Communication: 54.593 ms
Cluster-Wide Average, Imbalance: 36.123 ms
Node 0, GPU memory consumption: 22.467 GB
Node 2, GPU memory consumption: 19.391 GB
Node 4, GPU memory consumption: 19.370 GB
Node 1, GPU memory consumption: 19.391 GB
Node 5, GPU memory consumption: 19.391 GB
Node 3, GPU memory consumption: 19.368 GB
Node 7, GPU memory consumption: 19.471 GB
Node 6, GPU memory consumption: 19.391 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.819059 s---------------
------------------------node id 1,  per-epoch time: 0.819032 s---------------
------------------------node id 2,  per-epoch time: 0.819055 s---------------
------------------------node id 3,  per-epoch time: 0.819035 s---------------
------------------------node id 4,  per-epoch time: 0.819012 s---------------
------------------------node id 5,  per-epoch time: 0.819051 s---------------
------------------------node id 6,  per-epoch time: 0.819052 s---------------
------------------------node id 7,  per-epoch time: 0.819021 s---------------
************ Profiling Results ************
	Bubble: 327.215376 (ms) (39.10 percentage)
	Compute: 489.469853 (ms) (58.49 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 20.166715 (ms) (2.41 percentage)
	Layer-level communication (cluster-wide, per-epoch): 4.522 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.251 GB
	Total communication (cluster-wide, per-epoch): 4.773 GB
	Aggregated layer-level communication throughput: 525.345 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0011
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
