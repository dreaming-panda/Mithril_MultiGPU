Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.043 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.059 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.067 seconds.
Building the CSC structure...
        It takes 0.066 seconds.
Building the CSC structure...
        It takes 0.053 seconds.
        It takes 0.044 seconds.
        It takes 0.051 seconds.
        It takes 0.051 seconds.
        It takes 0.053 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.059 seconds.
        It takes 0.055 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.055 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.023 seconds.
        It takes 0.027 seconds.
        It takes 0.027 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 128
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.027 seconds.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
169343, 2484941, 2484941
169343, 2484941, 2484941
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 64.004 Gbps (per GPU), 512.028 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.962 Gbps (per GPU), 511.698 Gbps (aggregated)
The layer-level communication performance: 63.960 Gbps (per GPU), 511.683 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.927 Gbps (per GPU), 511.418 Gbps (aggregated)
The layer-level communication performance: 63.924 Gbps (per GPU), 511.389 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.891 Gbps (per GPU), 511.128 Gbps (aggregated)
The layer-level communication performance: 63.884 Gbps (per GPU), 511.074 Gbps (aggregated)
The layer-level communication performance: 63.879 Gbps (per GPU), 511.034 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.369 Gbps (per GPU), 1314.949 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.364 Gbps (per GPU), 1314.913 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.368 Gbps (per GPU), 1314.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.362 Gbps (per GPU), 1314.893 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.360 Gbps (per GPU), 1314.880 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.360 Gbps (per GPU), 1314.884 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.363 Gbps (per GPU), 1314.906 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.364 Gbps (per GPU), 1314.909 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.154 Gbps (per GPU), 913.229 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.152 Gbps (per GPU), 913.220 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.154 Gbps (per GPU), 913.229 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.147 Gbps (per GPU), 913.173 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.150 Gbps (per GPU), 913.199 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.147 Gbps (per GPU), 913.175 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.149 Gbps (per GPU), 913.190 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.147 Gbps (per GPU), 913.177 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.963 Gbps (per GPU), 359.707 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.963 Gbps (per GPU), 359.708 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.963 Gbps (per GPU), 359.706 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.963 Gbps (per GPU), 359.705 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.963 Gbps (per GPU), 359.705 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.963 Gbps (per GPU), 359.705 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.963 Gbps (per GPU), 359.704 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.963 Gbps (per GPU), 359.705 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.16ms  3.18ms  3.63ms  3.14 21.17K  0.46M
 chk_1  1.16ms  3.39ms  3.84ms  3.32 21.17K  0.55M
 chk_2  1.15ms  3.15ms  3.58ms  3.10 21.17K  0.39M
 chk_3  1.16ms  3.04ms  3.48ms  3.01 21.17K  0.24M
 chk_4  1.16ms  2.92ms  3.36ms  2.91 21.17K  0.17M
 chk_5  1.16ms  2.93ms  3.36ms  2.91 21.17K  0.22M
 chk_6  1.16ms  2.94ms  3.38ms  2.92 21.17K  0.16M
 chk_7  1.16ms  2.90ms  3.34ms  2.89 21.17K  0.12M
   Avg  1.16  3.06  3.50
   Max  1.16  3.39  3.84
   Min  1.15  2.90  3.34
 Ratio  1.00  1.17  1.15
   Var  0.00  0.03  0.03
Profiling takes 0.810 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 400.492 ms
Partition 0 [0, 17) has cost: 400.492 ms
Partition 1 [17, 33) has cost: 391.240 ms
Partition 2 [33, 49) has cost: 391.240 ms
Partition 3 [49, 65) has cost: 391.240 ms
Partition 4 [65, 81) has cost: 391.240 ms
Partition 5 [81, 97) has cost: 391.240 ms
Partition 6 [97, 113) has cost: 391.240 ms
Partition 7 [113, 129) has cost: 394.777 ms
The optimal partitioning:
[0, 17)
[17, 33)
[33, 49)
[49, 65)
[65, 81)
[81, 97)
[97, 113)
[113, 129)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 275.712 ms
GPU 0, Compute+Comm Time: 142.227 ms, Bubble Time: 122.108 ms, Imbalance Overhead: 11.378 ms
GPU 1, Compute+Comm Time: 139.621 ms, Bubble Time: 121.218 ms, Imbalance Overhead: 14.873 ms
GPU 2, Compute+Comm Time: 139.621 ms, Bubble Time: 122.767 ms, Imbalance Overhead: 13.324 ms
GPU 3, Compute+Comm Time: 139.621 ms, Bubble Time: 124.591 ms, Imbalance Overhead: 11.500 ms
GPU 4, Compute+Comm Time: 139.621 ms, Bubble Time: 127.017 ms, Imbalance Overhead: 9.074 ms
GPU 5, Compute+Comm Time: 139.621 ms, Bubble Time: 129.410 ms, Imbalance Overhead: 6.681 ms
GPU 6, Compute+Comm Time: 139.621 ms, Bubble Time: 131.771 ms, Imbalance Overhead: 4.320 ms
GPU 7, Compute+Comm Time: 140.333 ms, Bubble Time: 134.345 ms, Imbalance Overhead: 1.034 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 581.991 ms
GPU 0, Compute+Comm Time: 297.794 ms, Bubble Time: 282.378 ms, Imbalance Overhead: 1.819 ms
GPU 1, Compute+Comm Time: 294.968 ms, Bubble Time: 277.521 ms, Imbalance Overhead: 9.502 ms
GPU 2, Compute+Comm Time: 294.968 ms, Bubble Time: 273.127 ms, Imbalance Overhead: 13.896 ms
GPU 3, Compute+Comm Time: 294.968 ms, Bubble Time: 268.516 ms, Imbalance Overhead: 18.506 ms
GPU 4, Compute+Comm Time: 294.968 ms, Bubble Time: 263.916 ms, Imbalance Overhead: 23.107 ms
GPU 5, Compute+Comm Time: 294.968 ms, Bubble Time: 260.570 ms, Imbalance Overhead: 26.453 ms
GPU 6, Compute+Comm Time: 294.968 ms, Bubble Time: 257.498 ms, Imbalance Overhead: 29.525 ms
GPU 7, Compute+Comm Time: 301.615 ms, Bubble Time: 259.200 ms, Imbalance Overhead: 21.177 ms
The estimated cost of the whole pipeline: 900.588 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 791.731 ms
Partition 0 [0, 33) has cost: 791.731 ms
Partition 1 [33, 65) has cost: 782.479 ms
Partition 2 [65, 97) has cost: 782.479 ms
Partition 3 [97, 129) has cost: 786.017 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 376.000 ms
GPU 0, Compute+Comm Time: 210.425 ms, Bubble Time: 153.984 ms, Imbalance Overhead: 11.591 ms
GPU 1, Compute+Comm Time: 209.121 ms, Bubble Time: 156.841 ms, Imbalance Overhead: 10.038 ms
GPU 2, Compute+Comm Time: 209.121 ms, Bubble Time: 161.714 ms, Imbalance Overhead: 5.165 ms
GPU 3, Compute+Comm Time: 209.476 ms, Bubble Time: 166.524 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 661.673 ms
GPU 0, Compute+Comm Time: 367.659 ms, Bubble Time: 293.951 ms, Imbalance Overhead: 0.062 ms
GPU 1, Compute+Comm Time: 366.247 ms, Bubble Time: 284.812 ms, Imbalance Overhead: 10.614 ms
GPU 2, Compute+Comm Time: 366.247 ms, Bubble Time: 275.322 ms, Imbalance Overhead: 20.104 ms
GPU 3, Compute+Comm Time: 369.571 ms, Bubble Time: 269.657 ms, Imbalance Overhead: 22.444 ms
    The estimated cost with 2 DP ways is 1089.557 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1574.211 ms
Partition 0 [0, 65) has cost: 1574.211 ms
Partition 1 [65, 129) has cost: 1568.497 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 659.317 ms
GPU 0, Compute+Comm Time: 436.592 ms, Bubble Time: 213.160 ms, Imbalance Overhead: 9.564 ms
GPU 1, Compute+Comm Time: 436.122 ms, Bubble Time: 223.195 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 905.010 ms
GPU 0, Compute+Comm Time: 596.695 ms, Bubble Time: 308.315 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 597.650 ms, Bubble Time: 288.854 ms, Imbalance Overhead: 18.506 ms
    The estimated cost with 4 DP ways is 1642.543 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 3142.707 ms
Partition 0 [0, 129) has cost: 3142.707 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 1860.603 ms
GPU 0, Compute+Comm Time: 1860.603 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 2030.603 ms
GPU 0, Compute+Comm Time: 2030.603 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 4085.766 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 905)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 905)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 905)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 905)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 905)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 905)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 905)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 905)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2698: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 27664 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Aborted (signal 6)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
