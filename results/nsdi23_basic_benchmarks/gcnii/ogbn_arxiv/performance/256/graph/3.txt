Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.046 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.055 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.060 seconds.
Building the CSC structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.041 seconds.
        It takes 0.046 seconds.
        It takes 0.043 seconds.
        It takes 0.048 seconds.
        It takes 0.054 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Feature Vector...
        It takes 0.054 seconds.
        It takes 0.057 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.063 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.057 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.025 seconds.
        It takes 0.025 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
        It takes 0.066 seconds.
Building the Label Vector...
        It takes 0.024 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_arxiv/8_parts
The number of GCNII layers: 128
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 8
        It takes 0.024 seconds.
        It takes 0.026 seconds.
        It takes 0.027 seconds.
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
169343, 2484941, 2484941
Number of vertices per chunk: 21168
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 21167) 1-[21167, 42335) 2-[42335, 63503) 3-[63503, 84671) 4-[84671, 105839) 5-[105839, 127007) 6-[127007, 148175) 7-[148175, 169343)
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 129)
WARNING: the current version only applies to linear GNN models!
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 21168
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.799 Gbps (per GPU), 510.394 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.757 Gbps (per GPU), 510.059 Gbps (aggregated)
The layer-level communication performance: 63.757 Gbps (per GPU), 510.052 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.723 Gbps (per GPU), 509.785 Gbps (aggregated)
The layer-level communication performance: 63.718 Gbps (per GPU), 509.744 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.687 Gbps (per GPU), 509.499 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.680 Gbps (per GPU), 509.442 Gbps (aggregated)
The layer-level communication performance: 63.675 Gbps (per GPU), 509.402 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.892 Gbps (per GPU), 1319.136 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.878 Gbps (per GPU), 1319.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.890 Gbps (per GPU), 1319.123 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.874 Gbps (per GPU), 1318.993 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.890 Gbps (per GPU), 1319.122 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.889 Gbps (per GPU), 1319.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.891 Gbps (per GPU), 1319.126 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.894 Gbps (per GPU), 1319.151 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.293 Gbps (per GPU), 914.347 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.293 Gbps (per GPU), 914.341 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.286 Gbps (per GPU), 914.292 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.293 Gbps (per GPU), 914.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.288 Gbps (per GPU), 914.300 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.292 Gbps (per GPU), 914.334 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.289 Gbps (per GPU), 914.311 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.287 Gbps (per GPU), 914.297 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.524 Gbps (per GPU), 364.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.525 Gbps (per GPU), 364.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.524 Gbps (per GPU), 364.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.524 Gbps (per GPU), 364.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.524 Gbps (per GPU), 364.194 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.524 Gbps (per GPU), 364.193 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.524 Gbps (per GPU), 364.193 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.524 Gbps (per GPU), 364.192 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.15ms  3.17ms  3.61ms  3.13 21.17K  0.46M
 chk_1  1.15ms  3.39ms  3.83ms  3.32 21.17K  0.55M
 chk_2  1.15ms  3.15ms  3.59ms  3.12 21.17K  0.39M
 chk_3  1.15ms  3.04ms  3.47ms  3.02 21.17K  0.24M
 chk_4  1.15ms  2.92ms  3.36ms  2.91 21.17K  0.17M
 chk_5  1.15ms  2.92ms  3.36ms  2.91 21.17K  0.22M
 chk_6  1.16ms  2.93ms  3.38ms  2.92 21.17K  0.16M
 chk_7  1.16ms  2.90ms  3.34ms  2.89 21.17K  0.12M
   Avg  1.15  3.05  3.49
   Max  1.16  3.39  3.83
   Min  1.15  2.90  3.34
 Ratio  1.01  1.17  1.15
   Var  0.00  0.03  0.03
Profiling takes 0.805 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 400.031 ms
Partition 0 [0, 17) has cost: 400.031 ms
Partition 1 [17, 33) has cost: 390.803 ms
Partition 2 [33, 49) has cost: 390.803 ms
Partition 3 [49, 65) has cost: 390.803 ms
Partition 4 [65, 81) has cost: 390.803 ms
Partition 5 [81, 97) has cost: 390.803 ms
Partition 6 [97, 113) has cost: 390.803 ms
Partition 7 [113, 129) has cost: 394.325 ms
The optimal partitioning:
[0, 17)
[17, 33)
[33, 49)
[49, 65)
[65, 81)
[81, 97)
[97, 113)
[113, 129)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 275.881 ms
GPU 0, Compute+Comm Time: 142.168 ms, Bubble Time: 122.094 ms, Imbalance Overhead: 11.619 ms
GPU 1, Compute+Comm Time: 139.572 ms, Bubble Time: 121.132 ms, Imbalance Overhead: 15.177 ms
GPU 2, Compute+Comm Time: 139.572 ms, Bubble Time: 122.671 ms, Imbalance Overhead: 13.638 ms
GPU 3, Compute+Comm Time: 139.572 ms, Bubble Time: 124.547 ms, Imbalance Overhead: 11.762 ms
GPU 4, Compute+Comm Time: 139.572 ms, Bubble Time: 127.024 ms, Imbalance Overhead: 9.284 ms
GPU 5, Compute+Comm Time: 139.572 ms, Bubble Time: 129.471 ms, Imbalance Overhead: 6.837 ms
GPU 6, Compute+Comm Time: 139.572 ms, Bubble Time: 131.890 ms, Imbalance Overhead: 4.419 ms
GPU 7, Compute+Comm Time: 140.269 ms, Bubble Time: 134.494 ms, Imbalance Overhead: 1.117 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 581.612 ms
GPU 0, Compute+Comm Time: 297.544 ms, Bubble Time: 282.159 ms, Imbalance Overhead: 1.910 ms
GPU 1, Compute+Comm Time: 294.720 ms, Bubble Time: 277.297 ms, Imbalance Overhead: 9.596 ms
GPU 2, Compute+Comm Time: 294.720 ms, Bubble Time: 272.860 ms, Imbalance Overhead: 14.033 ms
GPU 3, Compute+Comm Time: 294.720 ms, Bubble Time: 268.271 ms, Imbalance Overhead: 18.622 ms
GPU 4, Compute+Comm Time: 294.720 ms, Bubble Time: 263.638 ms, Imbalance Overhead: 23.255 ms
GPU 5, Compute+Comm Time: 294.720 ms, Bubble Time: 260.286 ms, Imbalance Overhead: 26.607 ms
GPU 6, Compute+Comm Time: 294.720 ms, Bubble Time: 257.269 ms, Imbalance Overhead: 29.624 ms
GPU 7, Compute+Comm Time: 301.351 ms, Bubble Time: 259.059 ms, Imbalance Overhead: 21.203 ms
The estimated cost of the whole pipeline: 900.368 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 790.834 ms
Partition 0 [0, 33) has cost: 790.834 ms
Partition 1 [33, 65) has cost: 781.607 ms
Partition 2 [65, 97) has cost: 781.607 ms
Partition 3 [97, 129) has cost: 785.128 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 375.863 ms
GPU 0, Compute+Comm Time: 210.255 ms, Bubble Time: 153.808 ms, Imbalance Overhead: 11.800 ms
GPU 1, Compute+Comm Time: 208.956 ms, Bubble Time: 156.649 ms, Imbalance Overhead: 10.258 ms
GPU 2, Compute+Comm Time: 208.956 ms, Bubble Time: 161.632 ms, Imbalance Overhead: 5.276 ms
GPU 3, Compute+Comm Time: 209.305 ms, Bubble Time: 166.559 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 661.005 ms
GPU 0, Compute+Comm Time: 367.327 ms, Bubble Time: 293.678 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 365.910 ms, Bubble Time: 284.446 ms, Imbalance Overhead: 10.649 ms
GPU 2, Compute+Comm Time: 365.910 ms, Bubble Time: 274.915 ms, Imbalance Overhead: 20.180 ms
GPU 3, Compute+Comm Time: 369.228 ms, Bubble Time: 269.356 ms, Imbalance Overhead: 22.420 ms
    The estimated cost with 2 DP ways is 1088.712 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1572.441 ms
Partition 0 [0, 65) has cost: 1572.441 ms
Partition 1 [65, 129) has cost: 1566.735 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 658.934 ms
GPU 0, Compute+Comm Time: 436.261 ms, Bubble Time: 212.878 ms, Imbalance Overhead: 9.796 ms
GPU 1, Compute+Comm Time: 435.786 ms, Bubble Time: 223.148 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 904.063 ms
GPU 0, Compute+Comm Time: 596.001 ms, Bubble Time: 308.062 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 596.950 ms, Bubble Time: 288.413 ms, Imbalance Overhead: 18.700 ms
    The estimated cost with 4 DP ways is 1641.147 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 3139.176 ms
Partition 0 [0, 129) has cost: 3139.176 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 1839.501 ms
GPU 0, Compute+Comm Time: 1839.501 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 2009.098 ms
GPU 0, Compute+Comm Time: 2009.098 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 4041.028 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 905)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 905)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 905)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 905)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 905)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 905)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 905)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 905)
*** Node 7, constructing the helper classes...
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2734: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.grad != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.
gcnii: /shared_ssd_storage/jingjichen/Mithril_MultiGPU/core/src/cuda/cuda_hybrid_parallel.cc:2721: CUDAVertexTensorDataGradManager::CUDAVertexTensorDataGradManager(CUDAOperatorsAndTensorsManager*, CUDAVertexIdTranslationTable*, int, int, VertexId, Tensor*, Tensor*): Assertion `lvt.data != NULL' failed.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   PID 27813 RUNNING AT gnerv1
=   EXIT CODE: 6
=   CLEANING UP REMAINING PROCESSES
=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES
===================================================================================
YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Aborted (signal 6)
This typically refers to a problem with your application.
Please see the FAQ page for debugging suggestions
