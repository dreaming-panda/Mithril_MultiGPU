Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.841 seconds.
Building the CSC structure...
        It takes 2.063 seconds.
Building the CSC structure...
        It takes 2.077 seconds.
Building the CSC structure...
        It takes 2.281 seconds.
Building the CSC structure...
        It takes 2.369 seconds.
Building the CSC structure...
        It takes 2.403 seconds.
Building the CSC structure...
        It takes 2.425 seconds.
Building the CSC structure...
        It takes 2.626 seconds.
Building the CSC structure...
        It takes 1.826 seconds.
        It takes 1.876 seconds.
        It takes 1.867 seconds.
        It takes 2.222 seconds.
Building the Feature Vector...
        It takes 2.363 seconds.
        It takes 2.288 seconds.
        It takes 2.331 seconds.
        It takes 0.239 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 2.391 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.302 seconds.
Building the Label Vector...
        It takes 0.303 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.033 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.313 seconds.
Building the Label Vector...
        It takes 0.313 seconds.
Building the Label Vector...
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.041 seconds.
        It takes 0.037 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.273 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
csr in-out ready !Start Cost Model Initialization...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.949 Gbps (per GPU), 511.593 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.909 Gbps (per GPU), 511.273 Gbps (aggregated)
The layer-level communication performance: 63.907 Gbps (per GPU), 511.259 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.873 Gbps (per GPU), 510.986 Gbps (aggregated)
The layer-level communication performance: 63.868 Gbps (per GPU), 510.947 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.838 Gbps (per GPU), 510.702 Gbps (aggregated)
The layer-level communication performance: 63.830 Gbps (per GPU), 510.638 Gbps (aggregated)
The layer-level communication performance: 63.826 Gbps (per GPU), 510.608 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.861 Gbps (per GPU), 1318.885 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.856 Gbps (per GPU), 1318.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.860 Gbps (per GPU), 1318.879 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.854 Gbps (per GPU), 1318.834 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.857 Gbps (per GPU), 1318.860 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.856 Gbps (per GPU), 1318.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.855 Gbps (per GPU), 1318.840 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.853 Gbps (per GPU), 1318.821 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.302 Gbps (per GPU), 914.414 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.302 Gbps (per GPU), 914.417 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.302 Gbps (per GPU), 914.417 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.299 Gbps (per GPU), 914.391 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.302 Gbps (per GPU), 914.416 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.302 Gbps (per GPU), 914.415 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.302 Gbps (per GPU), 914.412 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.302 Gbps (per GPU), 914.417 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.566 Gbps (per GPU), 364.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.524 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.566 Gbps (per GPU), 364.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.566 Gbps (per GPU), 364.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.523 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.517 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.522 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.519 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.44ms  2.76ms  3.10  8.38K  3.53M
 chk_1  0.75ms  2.80ms  2.95ms  3.92  6.74K  3.60M
 chk_2  0.79ms  2.69ms  2.85ms  3.59  7.27K  3.53M
 chk_3  0.80ms  2.75ms  2.89ms  3.59  7.92K  3.61M
 chk_4  0.63ms  2.66ms  2.79ms  4.44  5.33K  3.68M
 chk_5  1.01ms  2.63ms  2.82ms  2.80 10.07K  3.45M
 chk_6  0.96ms  2.78ms  2.97ms  3.09  9.41K  3.48M
 chk_7  0.82ms  2.64ms  2.82ms  3.44  8.12K  3.60M
 chk_8  0.68ms  2.75ms  2.92ms  4.28  6.09K  3.64M
 chk_9  1.10ms  2.56ms  2.77ms  2.52 11.10K  3.38M
chk_10  0.65ms  2.78ms  2.94ms  4.52  5.67K  3.63M
chk_11  0.82ms  2.63ms  2.81ms  3.41  8.16K  3.54M
chk_12  0.80ms  2.85ms  3.01ms  3.78  7.24K  3.55M
chk_13  0.64ms  2.68ms  2.83ms  4.46  5.41K  3.68M
chk_14  0.78ms  2.92ms  3.07ms  3.93  7.14K  3.53M
chk_15  0.95ms  2.77ms  2.97ms  3.11  9.25K  3.49M
chk_16  0.59ms  2.61ms  2.77ms  4.66  4.78K  3.77M
chk_17  0.76ms  2.72ms  2.88ms  3.78  6.85K  3.60M
chk_18  0.81ms  2.54ms  2.70ms  3.34  7.47K  3.57M
chk_19  0.60ms  2.62ms  2.75ms  4.56  4.88K  3.75M
chk_20  0.77ms  2.60ms  2.78ms  3.62  7.00K  3.63M
chk_21  0.63ms  2.60ms  2.76ms  4.36  5.41K  3.68M
chk_22  1.10ms  2.80ms  3.00ms  2.74 11.07K  3.39M
chk_23  0.79ms  2.73ms  2.90ms  3.67  7.23K  3.64M
chk_24  1.01ms  2.76ms  2.99ms  2.96 10.13K  3.43M
chk_25  0.73ms  2.59ms  2.76ms  3.80  6.40K  3.57M
chk_26  0.66ms  2.78ms  2.94ms  4.46  5.78K  3.55M
chk_27  0.96ms  2.67ms  2.87ms  2.99  9.34K  3.48M
chk_28  0.72ms  2.98ms  3.12ms  4.31  6.37K  3.57M
chk_29  0.63ms  2.77ms  2.92ms  4.65  5.16K  3.78M
chk_30  0.64ms  2.65ms  2.82ms  4.43  5.44K  3.67M
chk_31  0.72ms  2.78ms  2.96ms  4.09  6.33K  3.63M
   Avg  0.79  2.70  2.88
   Max  1.10  2.98  3.12
   Min  0.59  2.44  2.70
 Ratio  1.85  1.22  1.15
   Var  0.02  0.01  0.01
Profiling takes 2.439 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 371.316 ms
Partition 0 [0, 5) has cost: 371.316 ms
Partition 1 [5, 9) has cost: 346.120 ms
Partition 2 [9, 13) has cost: 346.120 ms
Partition 3 [13, 17) has cost: 346.120 ms
Partition 4 [17, 21) has cost: 346.120 ms
Partition 5 [21, 25) has cost: 346.120 ms
Partition 6 [25, 29) has cost: 346.120 ms
Partition 7 [29, 33) has cost: 351.675 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.342 ms
GPU 0, Compute+Comm Time: 133.507 ms, Bubble Time: 29.179 ms, Imbalance Overhead: 2.657 ms
GPU 1, Compute+Comm Time: 126.471 ms, Bubble Time: 28.810 ms, Imbalance Overhead: 10.061 ms
GPU 2, Compute+Comm Time: 126.471 ms, Bubble Time: 28.775 ms, Imbalance Overhead: 10.096 ms
GPU 3, Compute+Comm Time: 126.471 ms, Bubble Time: 28.653 ms, Imbalance Overhead: 10.218 ms
GPU 4, Compute+Comm Time: 126.471 ms, Bubble Time: 28.589 ms, Imbalance Overhead: 10.282 ms
GPU 5, Compute+Comm Time: 126.471 ms, Bubble Time: 28.633 ms, Imbalance Overhead: 10.238 ms
GPU 6, Compute+Comm Time: 126.471 ms, Bubble Time: 28.895 ms, Imbalance Overhead: 9.976 ms
GPU 7, Compute+Comm Time: 127.669 ms, Bubble Time: 29.248 ms, Imbalance Overhead: 8.425 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 321.235 ms
GPU 0, Compute+Comm Time: 247.320 ms, Bubble Time: 57.162 ms, Imbalance Overhead: 16.752 ms
GPU 1, Compute+Comm Time: 242.963 ms, Bubble Time: 56.417 ms, Imbalance Overhead: 21.855 ms
GPU 2, Compute+Comm Time: 242.963 ms, Bubble Time: 55.868 ms, Imbalance Overhead: 22.404 ms
GPU 3, Compute+Comm Time: 242.963 ms, Bubble Time: 55.783 ms, Imbalance Overhead: 22.489 ms
GPU 4, Compute+Comm Time: 242.963 ms, Bubble Time: 55.821 ms, Imbalance Overhead: 22.450 ms
GPU 5, Compute+Comm Time: 242.963 ms, Bubble Time: 55.976 ms, Imbalance Overhead: 22.295 ms
GPU 6, Compute+Comm Time: 242.963 ms, Bubble Time: 55.857 ms, Imbalance Overhead: 22.414 ms
GPU 7, Compute+Comm Time: 261.125 ms, Bubble Time: 56.551 ms, Imbalance Overhead: 3.559 ms
The estimated cost of the whole pipeline: 510.906 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 717.436 ms
Partition 0 [0, 9) has cost: 717.436 ms
Partition 1 [9, 17) has cost: 692.239 ms
Partition 2 [17, 25) has cost: 692.239 ms
Partition 3 [25, 33) has cost: 697.794 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.320 ms
GPU 0, Compute+Comm Time: 143.711 ms, Bubble Time: 26.503 ms, Imbalance Overhead: 3.106 ms
GPU 1, Compute+Comm Time: 139.808 ms, Bubble Time: 26.553 ms, Imbalance Overhead: 6.959 ms
GPU 2, Compute+Comm Time: 139.808 ms, Bubble Time: 26.392 ms, Imbalance Overhead: 7.120 ms
GPU 3, Compute+Comm Time: 140.374 ms, Bubble Time: 26.828 ms, Imbalance Overhead: 6.118 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 324.049 ms
GPU 0, Compute+Comm Time: 261.391 ms, Bubble Time: 50.227 ms, Imbalance Overhead: 12.431 ms
GPU 1, Compute+Comm Time: 259.297 ms, Bubble Time: 49.812 ms, Imbalance Overhead: 14.940 ms
GPU 2, Compute+Comm Time: 259.297 ms, Bubble Time: 50.211 ms, Imbalance Overhead: 14.542 ms
GPU 3, Compute+Comm Time: 269.585 ms, Bubble Time: 50.109 ms, Imbalance Overhead: 4.356 ms
    The estimated cost with 2 DP ways is 522.238 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1409.675 ms
Partition 0 [0, 17) has cost: 1409.675 ms
Partition 1 [17, 33) has cost: 1390.033 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 209.062 ms
GPU 0, Compute+Comm Time: 182.940 ms, Bubble Time: 21.329 ms, Imbalance Overhead: 4.793 ms
GPU 1, Compute+Comm Time: 181.148 ms, Bubble Time: 22.186 ms, Imbalance Overhead: 5.727 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 349.225 ms
GPU 0, Compute+Comm Time: 303.596 ms, Bubble Time: 37.810 ms, Imbalance Overhead: 7.819 ms
GPU 1, Compute+Comm Time: 308.019 ms, Bubble Time: 37.336 ms, Imbalance Overhead: 3.870 ms
    The estimated cost with 4 DP ways is 586.201 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2799.709 ms
Partition 0 [0, 33) has cost: 2799.709 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 456.371 ms
GPU 0, Compute+Comm Time: 456.371 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 582.878 ms
GPU 0, Compute+Comm Time: 582.878 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1091.211 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5557
	Epoch 50:	Loss 1.8796
	Epoch 75:	Loss 1.4990
	Epoch 100:	Loss 1.2745
	Epoch 125:	Loss 1.0880
	Epoch 150:	Loss 0.9941
	Epoch 175:	Loss 0.9264
	Epoch 200:	Loss 0.8769
	Epoch 225:	Loss 0.8248
	Epoch 250:	Loss 0.7941
	Epoch 275:	Loss 0.7657
	Epoch 300:	Loss 0.7452
	Epoch 325:	Loss 0.7193
	Epoch 350:	Loss 0.7027
	Epoch 375:	Loss 0.6839
	Epoch 400:	Loss 0.6759
	Epoch 425:	Loss 0.6587
	Epoch 450:	Loss 0.6451
	Epoch 475:	Loss 0.6344
	Epoch 500:	Loss 0.6274
	Epoch 525:	Loss 0.6169
	Epoch 550:	Loss 0.6092
	Epoch 575:	Loss 0.5996
	Epoch 600:	Loss 0.5950
	Epoch 625:	Loss 0.5878
	Epoch 650:	Loss 0.5826
	Epoch 675:	Loss 0.5769
	Epoch 700:	Loss 0.5697
	Epoch 725:	Loss 0.5629
	Epoch 750:	Loss 0.5563
	Epoch 775:	Loss 0.5545
	Epoch 800:	Loss 0.5485
	Epoch 825:	Loss 0.5453
	Epoch 850:	Loss 0.5401
	Epoch 875:	Loss 0.5360
	Epoch 900:	Loss 0.5345
	Epoch 925:	Loss 0.5300
	Epoch 950:	Loss 0.5270
	Epoch 975:	Loss 0.5275
	Epoch 1000:	Loss 0.5186
	Epoch 1025:	Loss 0.5155
	Epoch 1050:	Loss 0.5119
	Epoch 1075:	Loss 0.5115
	Epoch 1100:	Loss 0.5095
	Epoch 1125:	Loss 0.5070
	Epoch 1150:	Loss 0.5014
	Epoch 1175:	Loss 0.5014
	Epoch 1200:	Loss 0.5003
	Epoch 1225:	Loss 0.4979
	Epoch 1250:	Loss 0.4936
	Epoch 1275:	Loss 0.4905
	Epoch 1300:	Loss 0.4875
	Epoch 1325:	Loss 0.4877
	Epoch 1350:	Loss 0.4844
	Epoch 1375:	Loss 0.4845
	Epoch 1400:	Loss 0.4781
	Epoch 1425:	Loss 0.4817
	Epoch 1450:	Loss 0.4786
	Epoch 1475:	Loss 0.4798
	Epoch 1500:	Loss 0.4783
	Epoch 1525:	Loss 0.4740
	Epoch 1550:	Loss 0.4723
	Epoch 1575:	Loss 0.4723
	Epoch 1600:	Loss 0.4721
	Epoch 1625:	Loss 0.4705
	Epoch 1650:	Loss 0.4642
	Epoch 1675:	Loss 0.4659
	Epoch 1700:	Loss 0.4656
	Epoch 1725:	Loss 0.4631
	Epoch 1750:	Loss 0.4617
	Epoch 1775:	Loss 0.4617
	Epoch 1800:	Loss 0.4589
	Epoch 1825:	Loss 0.4576
	Epoch 1850:	Loss 0.4576
	Epoch 1875:	Loss 0.4580
	Epoch 1900:	Loss 0.4512
	Epoch 1925:	Loss 0.4541
	Epoch 1950:	Loss 0.4513
	Epoch 1975:	Loss 0.4523
	Epoch 2000:	Loss 0.4475
	Epoch 2025:	Loss 0.4471
	Epoch 2050:	Loss 0.4460
	Epoch 2075:	Loss 0.4468
	Epoch 2100:	Loss 0.4441
	Epoch 2125:	Loss 0.4444
	Epoch 2150:	Loss 0.4444
	Epoch 2175:	Loss 0.4410
	Epoch 2200:	Loss 0.4421
	Epoch 2225:	Loss 0.4435
	Epoch 2250:	Loss 0.4418
	Epoch 2275:	Loss 0.4389
	Epoch 2300:	Loss 0.4387
	Epoch 2325:	Loss 0.4354
	Epoch 2350:	Loss 0.4401
	Epoch 2375:	Loss 0.4355
	Epoch 2400:	Loss 0.4326
	Epoch 2425:	Loss 0.4338
	Epoch 2450:	Loss 0.4282
	Epoch 2475:	Loss 0.4325
	Epoch 2500:	Loss 0.4289
	Epoch 2525:	Loss 0.4322
	Epoch 2550:	Loss 0.4289
	Epoch 2575:	Loss 0.4303
	Epoch 2600:	Loss 0.4300
	Epoch 2625:	Loss 0.4263
	Epoch 2650:	Loss 0.4279
	Epoch 2675:	Loss 0.4254
	Epoch 2700:	Loss 0.4312
	Epoch 2725:	Loss 0.4272
	Epoch 2750:	Loss 0.4261
	Epoch 2775:	Loss 0.4230
	Epoch 2800:	Loss 0.4239
	Epoch 2825:	Loss 0.4253
	Epoch 2850:	Loss 0.4224
	Epoch 2875:	Loss 0.4185
	Epoch 2900:	Loss 0.4208
	Epoch 2925:	Loss 0.4206
	Epoch 2950:	Loss 0.4194
	Epoch 2975:	Loss 0.4196
	Epoch 3000:	Loss 0.4195
	Epoch 3025:	Loss 0.4159
	Epoch 3050:	Loss 0.4179
	Epoch 3075:	Loss 0.4139
	Epoch 3100:	Loss 0.4173
	Epoch 3125:	Loss 0.4141
	Epoch 3150:	Loss 0.4165
	Epoch 3175:	Loss 0.4121
	Epoch 3200:	Loss 0.4135
	Epoch 3225:	Loss 0.4128
	Epoch 3250:	Loss 0.4126
	Epoch 3275:	Loss 0.4136
	Epoch 3300:	Loss 0.4110
	Epoch 3325:	Loss 0.4064
	Epoch 3350:	Loss 0.4093
	Epoch 3375:	Loss 0.4089
	Epoch 3400:	Loss 0.4096
	Epoch 3425:	Loss 0.4096
	Epoch 3450:	Loss 0.4156
	Epoch 3475:	Loss 0.4082
	Epoch 3500:	Loss 0.4106
	Epoch 3525:	Loss 0.4053
	Epoch 3550:	Loss 0.4078
	Epoch 3575:	Loss 0.4053
	Epoch 3600:	Loss 0.4053
	Epoch 3625:	Loss 0.4027
	Epoch 3650:	Loss 0.4044
	Epoch 3675:	Loss 0.4007
	Epoch 3700:	Loss 0.3998
	Epoch 3725:	Loss 0.4030
	Epoch 3750:	Loss 0.4010
	Epoch 3775:	Loss 0.3997
	Epoch 3800:	Loss 0.4036
	Epoch 3825:	Loss 0.4007
	Epoch 3850:	Loss 0.4014
	Epoch 3875:	Loss 0.3983
	Epoch 3900:	Loss 0.3969
	Epoch 3925:	Loss 0.3959
	Epoch 3950:	Loss 0.3969
	Epoch 3975:	Loss 0.3986
	Epoch 4000:	Loss 0.3942
	Epoch 4025:	Loss 0.3949
	Epoch 4050:	Loss 0.3957
	Epoch 4075:	Loss 0.3966
	Epoch 4100:	Loss 0.3985
	Epoch 4125:	Loss 0.3950
	Epoch 4150:	Loss 0.3954
	Epoch 4175:	Loss 0.3944
	Epoch 4200:	Loss 0.3903
	Epoch 4225:	Loss 0.3964
	Epoch 4250:	Loss 0.3888
	Epoch 4275:	Loss 0.3943
	Epoch 4300:	Loss 0.3929
	Epoch 4325:	Loss 0.3879
	Epoch 4350:	Loss 0.3896
	Epoch 4375:	Loss 0.3910
	Epoch 4400:	Loss 0.3913
	Epoch 4425:	Loss 0.3902
	Epoch 4450:	Loss 0.3918
	Epoch 4475:	Loss 0.3908
	Epoch 4500:	Loss 0.3889
	Epoch 4525:	Loss 0.3890
	Epoch 4550:	Loss 0.3893
	Epoch 4575:	Loss 0.3850
	Epoch 4600:	Loss 0.3846
	Epoch 4625:	Loss 0.3867
	Epoch 4650:	Loss 0.3831
	Epoch 4675:	Loss 0.3867
	Epoch 4700:	Loss 0.3850
	Epoch 4725:	Loss 0.3825
	Epoch 4750:	Loss 0.3817
	Epoch 4775:	Loss 0.3823
	Epoch 4800:	Loss 0.3845
	Epoch 4825:	Loss 0.3822
	Epoch 4850:	Loss 0.3772
	Epoch 4875:	Loss 0.3829
	Epoch 4900:	Loss 0.3791
	Epoch 4925:	Loss 0.3821
	Epoch 4950:	Loss 0.3791
	Epoch 4975:	Loss 0.3805
	Epoch 5000:	Loss 0.3809
Node 2, Pre/Post-Pipelining: 2.161 / 30.793 ms, Bubble: 68.611 ms, Compute: 322.805 ms, Comm: 35.525 ms, Imbalance: 24.211 ms
Node 0, Pre/Post-Pipelining: 2.166 / 30.801 ms, Bubble: 69.116 ms, Compute: 343.884 ms, Comm: 30.444 ms, Imbalance: 6.741 ms
Node 7, Pre/Post-Pipelining: 2.163 / 38.796 ms, Bubble: 63.645 ms, Compute: 339.075 ms, Comm: 27.271 ms, Imbalance: 13.585 ms
Node 1, Pre/Post-Pipelining: 2.163 / 30.852 ms, Bubble: 69.123 ms, Compute: 343.508 ms, Comm: 31.121 ms, Imbalance: 6.387 ms
Node 4, Pre/Post-Pipelining: 2.160 / 30.794 ms, Bubble: 70.261 ms, Compute: 322.408 ms, Comm: 33.549 ms, Imbalance: 26.132 ms
Node 3, Pre/Post-Pipelining: 2.159 / 30.790 ms, Bubble: 68.636 ms, Compute: 322.153 ms, Comm: 36.324 ms, Imbalance: 24.069 ms
Node 5, Pre/Post-Pipelining: 2.158 / 30.795 ms, Bubble: 70.267 ms, Compute: 322.613 ms, Comm: 33.694 ms, Imbalance: 25.815 ms
Node 6, Pre/Post-Pipelining: 2.162 / 38.867 ms, Bubble: 63.563 ms, Compute: 338.254 ms, Comm: 28.040 ms, Imbalance: 13.687 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 2.166 ms
Cluster-Wide Average, Post-Pipelining Overhead: 30.801 ms
Cluster-Wide Average, Bubble: 69.116 ms
Cluster-Wide Average, Compute: 343.884 ms
Cluster-Wide Average, Communication: 30.444 ms
Cluster-Wide Average, Imbalance: 6.741 ms
Node 0, GPU memory consumption: 10.143 GB
Node 7, GPU memory consumption: 8.046 GB
Node 1, GPU memory consumption: 8.962 GB
Node 4, GPU memory consumption: 7.891 GB
Node 3, GPU memory consumption: 7.893 GB
Node 6, GPU memory consumption: 8.067 GB
Node 2, GPU memory consumption: 7.915 GB
Node 5, GPU memory consumption: 7.917 GB
Node 0, Graph-Level Communication Throughput: 97.608 Gbps, Time: 57.407 ms
Node 1, Graph-Level Communication Throughput: 105.753 Gbps, Time: 54.697 ms
Node 2, Graph-Level Communication Throughput: 102.487 Gbps, Time: 54.675 ms
Node 3, Graph-Level Communication Throughput: 105.820 Gbps, Time: 54.662 ms
Node 4, Graph-Level Communication Throughput: 101.703 Gbps, Time: 55.096 ms
Node 5, Graph-Level Communication Throughput: 107.065 Gbps, Time: 54.026 ms
Node 6, Graph-Level Communication Throughput: 104.734 Gbps, Time: 53.501 ms
Node 7, Graph-Level Communication Throughput: 106.008 Gbps, Time: 54.565 ms
------------------------node id 0,  per-epoch time: 0.483511 s---------------
------------------------node id 1,  per-epoch time: 0.483511 s---------------
------------------------node id 2,  per-epoch time: 0.483511 s---------------
------------------------node id 3,  per-epoch time: 0.483511 s---------------
------------------------node id 4,  per-epoch time: 0.483511 s---------------
------------------------node id 5,  per-epoch time: 0.483511 s---------------
------------------------node id 6,  per-epoch time: 0.483511 s---------------
------------------------node id 7,  per-epoch time: 0.483511 s---------------
************ Profiling Results ************
	Bubble: 152.945191 (ms) (31.63 percentage)
	Compute: 259.448973 (ms) (53.65 percentage)
	GraphCommComputeOverhead: 9.913372 (ms) (2.05 percentage)
	GraphCommNetwork: 54.833926 (ms) (11.34 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.439684 (ms) (1.33 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.041 GB
	Graph-level communication (cluster-wide, per-epoch): 5.303 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 6.356 GB
	Aggregated layer-level communication throughput: 279.594 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
