Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.858 seconds.
Building the CSC structure...
        It takes 1.922 seconds.
Building the CSC structure...
        It takes 2.047 seconds.
Building the CSC structure...
        It takes 2.393 seconds.
Building the CSC structure...
        It takes 2.459 seconds.
Building the CSC structure...
        It takes 2.594 seconds.
Building the CSC structure...
        It takes 2.612 seconds.
Building the CSC structure...
        It takes 2.668 seconds.
Building the CSC structure...
        It takes 1.807 seconds.
        It takes 1.847 seconds.
        It takes 1.934 seconds.
Building the Feature Vector...
        It takes 2.286 seconds.
Building the Feature Vector...
        It takes 2.390 seconds.
        It takes 2.257 seconds.
        It takes 2.293 seconds.
        It takes 0.255 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 2.386 seconds.
        It takes 0.040 seconds.
Building the Feature Vector...
        It takes 0.246 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.248 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Building the Feature Vector...
        It takes 0.302 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.276 seconds.
Building the Label Vector...
232965, 114848857, 114848857
        It takes 0.031 seconds.
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.290 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 62.920 Gbps (per GPU), 503.358 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.880 Gbps (per GPU), 503.038 Gbps (aggregated)
The layer-level communication performance: 62.892 Gbps (per GPU), 503.137 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.845 Gbps (per GPU), 502.758 Gbps (aggregated)
The layer-level communication performance: 62.853 Gbps (per GPU), 502.827 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.823 Gbps (per GPU), 502.581 Gbps (aggregated)
The layer-level communication performance: 62.802 Gbps (per GPU), 502.415 Gbps (aggregated)
The layer-level communication performance: 62.812 Gbps (per GPU), 502.493 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.780 Gbps (per GPU), 1326.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.793 Gbps (per GPU), 1326.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.777 Gbps (per GPU), 1326.217 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.771 Gbps (per GPU), 1326.170 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.777 Gbps (per GPU), 1326.216 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.757 Gbps (per GPU), 1326.056 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.779 Gbps (per GPU), 1326.233 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.781 Gbps (per GPU), 1326.249 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.622 Gbps (per GPU), 916.980 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.622 Gbps (per GPU), 916.977 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.623 Gbps (per GPU), 916.981 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.622 Gbps (per GPU), 916.974 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.619 Gbps (per GPU), 916.956 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.621 Gbps (per GPU), 916.966 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.620 Gbps (per GPU), 916.962 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.621 Gbps (per GPU), 916.967 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.956 Gbps (per GPU), 367.648 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.956 Gbps (per GPU), 367.647 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.956 Gbps (per GPU), 367.645 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.956 Gbps (per GPU), 367.648 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.956 Gbps (per GPU), 367.646 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.956 Gbps (per GPU), 367.644 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.956 Gbps (per GPU), 367.647 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.953 Gbps (per GPU), 367.628 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.51ms  2.80ms  3.11  8.38K  3.53M
 chk_1  0.76ms  2.83ms  3.01ms  3.97  6.74K  3.60M
 chk_2  0.81ms  2.73ms  2.92ms  3.62  7.27K  3.53M
 chk_3  0.81ms  2.76ms  2.95ms  3.63  7.92K  3.61M
 chk_4  0.64ms  2.71ms  2.87ms  4.51  5.33K  3.68M
 chk_5  1.01ms  2.67ms  2.88ms  2.84 10.07K  3.45M
 chk_6  0.97ms  2.83ms  3.01ms  3.11  9.41K  3.48M
 chk_7  0.83ms  2.70ms  2.87ms  3.45  8.12K  3.60M
 chk_8  0.69ms  2.80ms  2.96ms  4.30  6.09K  3.64M
 chk_9  1.11ms  2.61ms  2.81ms  2.53 11.10K  3.38M
chk_10  0.66ms  2.83ms  2.99ms  4.51  5.67K  3.63M
chk_11  0.83ms  2.71ms  2.86ms  3.43  8.16K  3.54M
chk_12  0.81ms  2.90ms  3.07ms  3.79  7.24K  3.55M
chk_13  0.65ms  2.75ms  2.91ms  4.51  5.41K  3.68M
chk_14  0.79ms  2.96ms  3.13ms  3.96  7.14K  3.53M
chk_15  0.96ms  2.84ms  3.04ms  3.16  9.25K  3.49M
chk_16  0.60ms  2.70ms  2.84ms  4.71  4.78K  3.77M
chk_17  0.77ms  2.82ms  2.94ms  3.81  6.85K  3.60M
chk_18  0.82ms  2.61ms  2.76ms  3.37  7.47K  3.57M
chk_19  0.61ms  2.69ms  2.83ms  4.61  4.88K  3.75M
chk_20  0.78ms  2.66ms  2.85ms  3.64  7.00K  3.63M
chk_21  0.64ms  2.66ms  2.82ms  4.39  5.41K  3.68M
chk_22  1.11ms  2.86ms  3.05ms  2.75 11.07K  3.39M
chk_23  0.80ms  2.77ms  2.91ms  3.62  7.23K  3.64M
chk_24  1.02ms  2.86ms  3.02ms  2.96 10.13K  3.43M
chk_25  0.74ms  2.64ms  2.81ms  3.82  6.40K  3.57M
chk_26  0.67ms  2.85ms  3.03ms  4.53  5.78K  3.55M
chk_27  0.97ms  2.74ms  2.96ms  3.06  9.34K  3.48M
chk_28  0.73ms  2.99ms  3.17ms  4.32  6.37K  3.57M
chk_29  0.64ms  2.81ms  2.98ms  4.66  5.16K  3.78M
chk_30  0.65ms  2.71ms  2.87ms  4.44  5.44K  3.67M
chk_31  0.73ms  2.86ms  3.03ms  4.13  6.33K  3.63M
   Avg  0.80  2.76  2.94
   Max  1.11  2.99  3.17
   Min  0.60  2.51  2.76
 Ratio  1.85  1.19  1.15
   Var  0.02  0.01  0.01
Profiling takes 2.489 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 379.119 ms
Partition 0 [0, 5) has cost: 379.119 ms
Partition 1 [5, 9) has cost: 353.589 ms
Partition 2 [9, 13) has cost: 353.589 ms
Partition 3 [13, 17) has cost: 353.589 ms
Partition 4 [17, 21) has cost: 353.589 ms
Partition 5 [21, 25) has cost: 353.589 ms
Partition 6 [25, 29) has cost: 353.589 ms
Partition 7 [29, 33) has cost: 359.132 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 168.101 ms
GPU 0, Compute+Comm Time: 136.292 ms, Bubble Time: 29.665 ms, Imbalance Overhead: 2.144 ms
GPU 1, Compute+Comm Time: 129.146 ms, Bubble Time: 29.310 ms, Imbalance Overhead: 9.645 ms
GPU 2, Compute+Comm Time: 129.146 ms, Bubble Time: 29.282 ms, Imbalance Overhead: 9.672 ms
GPU 3, Compute+Comm Time: 129.146 ms, Bubble Time: 29.207 ms, Imbalance Overhead: 9.748 ms
GPU 4, Compute+Comm Time: 129.146 ms, Bubble Time: 29.123 ms, Imbalance Overhead: 9.832 ms
GPU 5, Compute+Comm Time: 129.146 ms, Bubble Time: 29.081 ms, Imbalance Overhead: 9.874 ms
GPU 6, Compute+Comm Time: 129.146 ms, Bubble Time: 29.266 ms, Imbalance Overhead: 9.689 ms
GPU 7, Compute+Comm Time: 130.354 ms, Bubble Time: 29.595 ms, Imbalance Overhead: 8.152 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 326.950 ms
GPU 0, Compute+Comm Time: 252.475 ms, Bubble Time: 58.134 ms, Imbalance Overhead: 16.342 ms
GPU 1, Compute+Comm Time: 248.140 ms, Bubble Time: 57.440 ms, Imbalance Overhead: 21.370 ms
GPU 2, Compute+Comm Time: 248.140 ms, Bubble Time: 56.967 ms, Imbalance Overhead: 21.844 ms
GPU 3, Compute+Comm Time: 248.140 ms, Bubble Time: 56.964 ms, Imbalance Overhead: 21.847 ms
GPU 4, Compute+Comm Time: 248.140 ms, Bubble Time: 57.018 ms, Imbalance Overhead: 21.792 ms
GPU 5, Compute+Comm Time: 248.140 ms, Bubble Time: 57.081 ms, Imbalance Overhead: 21.730 ms
GPU 6, Compute+Comm Time: 248.140 ms, Bubble Time: 56.895 ms, Imbalance Overhead: 21.915 ms
GPU 7, Compute+Comm Time: 266.523 ms, Bubble Time: 57.374 ms, Imbalance Overhead: 3.054 ms
The estimated cost of the whole pipeline: 519.804 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 732.708 ms
Partition 0 [0, 9) has cost: 732.708 ms
Partition 1 [9, 17) has cost: 707.179 ms
Partition 2 [17, 25) has cost: 707.179 ms
Partition 3 [25, 33) has cost: 712.721 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 176.139 ms
GPU 0, Compute+Comm Time: 146.397 ms, Bubble Time: 26.934 ms, Imbalance Overhead: 2.808 ms
GPU 1, Compute+Comm Time: 142.437 ms, Bubble Time: 26.918 ms, Imbalance Overhead: 6.783 ms
GPU 2, Compute+Comm Time: 142.437 ms, Bubble Time: 26.728 ms, Imbalance Overhead: 6.974 ms
GPU 3, Compute+Comm Time: 142.999 ms, Bubble Time: 27.082 ms, Imbalance Overhead: 6.057 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 328.839 ms
GPU 0, Compute+Comm Time: 266.255 ms, Bubble Time: 50.824 ms, Imbalance Overhead: 11.760 ms
GPU 1, Compute+Comm Time: 264.117 ms, Bubble Time: 50.543 ms, Imbalance Overhead: 14.179 ms
GPU 2, Compute+Comm Time: 264.117 ms, Bubble Time: 50.959 ms, Imbalance Overhead: 13.763 ms
GPU 3, Compute+Comm Time: 274.521 ms, Bubble Time: 50.727 ms, Imbalance Overhead: 3.591 ms
    The estimated cost with 2 DP ways is 530.227 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1439.887 ms
Partition 0 [0, 17) has cost: 1439.887 ms
Partition 1 [17, 33) has cost: 1419.900 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 211.437 ms
GPU 0, Compute+Comm Time: 185.374 ms, Bubble Time: 21.440 ms, Imbalance Overhead: 4.623 ms
GPU 1, Compute+Comm Time: 183.556 ms, Bubble Time: 22.307 ms, Imbalance Overhead: 5.574 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 354.258 ms
GPU 0, Compute+Comm Time: 308.379 ms, Bubble Time: 38.249 ms, Imbalance Overhead: 7.629 ms
GPU 1, Compute+Comm Time: 312.893 ms, Bubble Time: 37.545 ms, Imbalance Overhead: 3.819 ms
    The estimated cost with 4 DP ways is 593.979 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2859.787 ms
Partition 0 [0, 33) has cost: 2859.787 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 455.149 ms
GPU 0, Compute+Comm Time: 455.149 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 583.623 ms
GPU 0, Compute+Comm Time: 583.623 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1090.710 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 4, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5524
	Epoch 50:	Loss 1.8738
	Epoch 75:	Loss 1.5056
	Epoch 100:	Loss 1.2748
	Epoch 125:	Loss 1.0974
	Epoch 150:	Loss 1.0045
	Epoch 175:	Loss 0.9325
	Epoch 200:	Loss 0.8770
	Epoch 225:	Loss 0.8287
	Epoch 250:	Loss 0.7966
	Epoch 275:	Loss 0.7685
	Epoch 300:	Loss 0.7452
	Epoch 325:	Loss 0.7201
	Epoch 350:	Loss 0.7005
	Epoch 375:	Loss 0.6874
	Epoch 400:	Loss 0.6724
	Epoch 425:	Loss 0.6609
	Epoch 450:	Loss 0.6495
	Epoch 475:	Loss 0.6384
	Epoch 500:	Loss 0.6280
	Epoch 525:	Loss 0.6198
	Epoch 550:	Loss 0.6143
	Epoch 575:	Loss 0.6013
	Epoch 600:	Loss 0.5975
	Epoch 625:	Loss 0.5888
	Epoch 650:	Loss 0.5809
	Epoch 675:	Loss 0.5752
	Epoch 700:	Loss 0.5725
	Epoch 725:	Loss 0.5678
	Epoch 750:	Loss 0.5606
	Epoch 775:	Loss 0.5601
	Epoch 800:	Loss 0.5518
	Epoch 825:	Loss 0.5500
	Epoch 850:	Loss 0.5465
	Epoch 875:	Loss 0.5381
	Epoch 900:	Loss 0.5331
	Epoch 925:	Loss 0.5331
	Epoch 950:	Loss 0.5316
	Epoch 975:	Loss 0.5288
	Epoch 1000:	Loss 0.5229
	Epoch 1025:	Loss 0.5194
	Epoch 1050:	Loss 0.5161
	Epoch 1075:	Loss 0.5142
	Epoch 1100:	Loss 0.5134
	Epoch 1125:	Loss 0.5105
	Epoch 1150:	Loss 0.5091
	Epoch 1175:	Loss 0.5070
	Epoch 1200:	Loss 0.5013
	Epoch 1225:	Loss 0.4999
	Epoch 1250:	Loss 0.4984
	Epoch 1275:	Loss 0.4969
	Epoch 1300:	Loss 0.4949
	Epoch 1325:	Loss 0.4925
	Epoch 1350:	Loss 0.4883
	Epoch 1375:	Loss 0.4869
	Epoch 1400:	Loss 0.4860
	Epoch 1425:	Loss 0.4896
	Epoch 1450:	Loss 0.4833
	Epoch 1475:	Loss 0.4828
	Epoch 1500:	Loss 0.4797
	Epoch 1525:	Loss 0.4738
	Epoch 1550:	Loss 0.4763
	Epoch 1575:	Loss 0.4744
	Epoch 1600:	Loss 0.4730
	Epoch 1625:	Loss 0.4769
	Epoch 1650:	Loss 0.4727
	Epoch 1675:	Loss 0.4729
	Epoch 1700:	Loss 0.4705
	Epoch 1725:	Loss 0.4658
	Epoch 1750:	Loss 0.4663
	Epoch 1775:	Loss 0.4652
	Epoch 1800:	Loss 0.4615
	Epoch 1825:	Loss 0.4620
	Epoch 1850:	Loss 0.4620
	Epoch 1875:	Loss 0.4578
	Epoch 1900:	Loss 0.4585
	Epoch 1925:	Loss 0.4605
	Epoch 1950:	Loss 0.4532
	Epoch 1975:	Loss 0.4542
	Epoch 2000:	Loss 0.4520
	Epoch 2025:	Loss 0.4517
	Epoch 2050:	Loss 0.4532
	Epoch 2075:	Loss 0.4523
	Epoch 2100:	Loss 0.4494
	Epoch 2125:	Loss 0.4508
	Epoch 2150:	Loss 0.4495
	Epoch 2175:	Loss 0.4461
	Epoch 2200:	Loss 0.4471
	Epoch 2225:	Loss 0.4482
	Epoch 2250:	Loss 0.4477
	Epoch 2275:	Loss 0.4468
	Epoch 2300:	Loss 0.4460
	Epoch 2325:	Loss 0.4445
	Epoch 2350:	Loss 0.4421
	Epoch 2375:	Loss 0.4420
	Epoch 2400:	Loss 0.4408
	Epoch 2425:	Loss 0.4391
	Epoch 2450:	Loss 0.4421
	Epoch 2475:	Loss 0.4378
	Epoch 2500:	Loss 0.4362
	Epoch 2525:	Loss 0.4341
	Epoch 2550:	Loss 0.4395
	Epoch 2575:	Loss 0.4310
	Epoch 2600:	Loss 0.4335
	Epoch 2625:	Loss 0.4306
	Epoch 2650:	Loss 0.4334
	Epoch 2675:	Loss 0.4317
	Epoch 2700:	Loss 0.4316
	Epoch 2725:	Loss 0.4305
	Epoch 2750:	Loss 0.4314
	Epoch 2775:	Loss 0.4271
	Epoch 2800:	Loss 0.4263
	Epoch 2825:	Loss 0.4290
	Epoch 2850:	Loss 0.4269
	Epoch 2875:	Loss 0.4260
	Epoch 2900:	Loss 0.4250
	Epoch 2925:	Loss 0.4238
	Epoch 2950:	Loss 0.4247
	Epoch 2975:	Loss 0.4242
	Epoch 3000:	Loss 0.4216
	Epoch 3025:	Loss 0.4211
	Epoch 3050:	Loss 0.4225
	Epoch 3075:	Loss 0.4229
	Epoch 3100:	Loss 0.4191
	Epoch 3125:	Loss 0.4241
	Epoch 3150:	Loss 0.4201
	Epoch 3175:	Loss 0.4182
	Epoch 3200:	Loss 0.4197
	Epoch 3225:	Loss 0.4180
	Epoch 3250:	Loss 0.4161
	Epoch 3275:	Loss 0.4137
	Epoch 3300:	Loss 0.4129
	Epoch 3325:	Loss 0.4197
	Epoch 3350:	Loss 0.4164
	Epoch 3375:	Loss 0.4148
	Epoch 3400:	Loss 0.4157
	Epoch 3425:	Loss 0.4167
	Epoch 3450:	Loss 0.4138
	Epoch 3475:	Loss 0.4147
	Epoch 3500:	Loss 0.4112
	Epoch 3525:	Loss 0.4113
	Epoch 3550:	Loss 0.4107
	Epoch 3575:	Loss 0.4147
	Epoch 3600:	Loss 0.4110
	Epoch 3625:	Loss 0.4101
	Epoch 3650:	Loss 0.4126
	Epoch 3675:	Loss 0.4093
	Epoch 3700:	Loss 0.4106
	Epoch 3725:	Loss 0.4096
	Epoch 3750:	Loss 0.4069
	Epoch 3775:	Loss 0.4051
	Epoch 3800:	Loss 0.4074
	Epoch 3825:	Loss 0.4078
	Epoch 3850:	Loss 0.4070
	Epoch 3875:	Loss 0.4074
	Epoch 3900:	Loss 0.4051
	Epoch 3925:	Loss 0.4060
	Epoch 3950:	Loss 0.4051
	Epoch 3975:	Loss 0.4042
	Epoch 4000:	Loss 0.4010
	Epoch 4025:	Loss 0.4069
	Epoch 4050:	Loss 0.4060
	Epoch 4075:	Loss 0.4049
	Epoch 4100:	Loss 0.4002
	Epoch 4125:	Loss 0.4041
	Epoch 4150:	Loss 0.3985
	Epoch 4175:	Loss 0.3994
	Epoch 4200:	Loss 0.4004
	Epoch 4225:	Loss 0.3985
	Epoch 4250:	Loss 0.3977
	Epoch 4275:	Loss 0.3966
	Epoch 4300:	Loss 0.3963
	Epoch 4325:	Loss 0.3999
	Epoch 4350:	Loss 0.3984
	Epoch 4375:	Loss 0.3937
	Epoch 4400:	Loss 0.3952
	Epoch 4425:	Loss 0.3959
	Epoch 4450:	Loss 0.3930
	Epoch 4475:	Loss 0.3963
	Epoch 4500:	Loss 0.3927
	Epoch 4525:	Loss 0.3927
	Epoch 4550:	Loss 0.3934
	Epoch 4575:	Loss 0.3918
	Epoch 4600:	Loss 0.3912
	Epoch 4625:	Loss 0.3923
	Epoch 4650:	Loss 0.3919
	Epoch 4675:	Loss 0.3932
	Epoch 4700:	Loss 0.3905
	Epoch 4725:	Loss 0.3923
	Epoch 4750:	Loss 0.3926
	Epoch 4775:	Loss 0.3915
	Epoch 4800:	Loss 0.3914
	Epoch 4825:	Loss 0.3892
	Epoch 4850:	Loss 0.3888
	Epoch 4875:	Loss 0.3865
	Epoch 4900:	Loss 0.3869
	Epoch 4925:	Loss 0.3879
	Epoch 4950:	Loss 0.3883
	Epoch 4975:	Loss 0.3852
	Epoch 5000:	Loss 0.3898
Node 2, Pre/Post-Pipelining: 2.159 / 29.763 ms, Bubble: 68.561 ms, Compute: 322.676 ms, Comm: 35.480 ms, Imbalance: 24.126 ms
Node 5, Pre/Post-Pipelining: 2.158 / 29.794 ms, Bubble: 70.209 ms, Compute: 322.741 ms, Comm: 33.825 ms, Imbalance: 25.262 ms
Node 1, Pre/Post-Pipelining: 2.164 / 29.838 ms, Bubble: 68.975 ms, Compute: 343.916 ms, Comm: 30.999 ms, Imbalance: 5.917 ms
Node 0, Pre/Post-Pipelining: 2.162 / 29.825 ms, Bubble: 68.975 ms, Compute: 344.393 ms, Comm: 30.324 ms, Imbalance: 6.130 ms
Node 7, Pre/Post-Pipelining: 2.166 / 37.771 ms, Bubble: 63.743 ms, Compute: 337.471 ms, Comm: 27.486 ms, Imbalance: 14.584 ms
Node 4, Pre/Post-Pipelining: 2.157 / 29.789 ms, Bubble: 70.197 ms, Compute: 322.458 ms, Comm: 33.813 ms, Imbalance: 25.519 ms
Node 6, Pre/Post-Pipelining: 2.162 / 37.789 ms, Bubble: 63.732 ms, Compute: 336.464 ms, Comm: 28.417 ms, Imbalance: 14.711 ms
Node 3, Pre/Post-Pipelining: 2.156 / 29.821 ms, Bubble: 68.478 ms, Compute: 322.219 ms, Comm: 36.294 ms, Imbalance: 23.788 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 2.162 ms
Cluster-Wide Average, Post-Pipelining Overhead: 29.825 ms
Cluster-Wide Average, Bubble: 68.975 ms
Cluster-Wide Average, Compute: 344.393 ms
Cluster-Wide Average, Communication: 30.324 ms
Cluster-Wide Average, Imbalance: 6.130 ms
Node 0, GPU memory consumption: 10.143 GB
Node 1, GPU memory consumption: 8.962 GB
Node 2, GPU memory consumption: 7.915 GB
Node 6, GPU memory consumption: 8.067 GB
Node 3, GPU memory consumption: 7.891 GB
Node 7, GPU memory consumption: 8.044 GB
Node 4, GPU memory consumption: 7.891 GB
Node 5, GPU memory consumption: 7.915 GB
Node 0, Graph-Level Communication Throughput: 99.264 Gbps, Time: 56.450 ms
Node 4, Graph-Level Communication Throughput: 101.174 Gbps, Time: 55.384 ms
Node 1, Graph-Level Communication Throughput: 105.215 Gbps, Time: 54.976 ms
Node 5, Graph-Level Communication Throughput: 106.646 Gbps, Time: 54.238 ms
Node 2, Graph-Level Communication Throughput: 100.287 Gbps, Time: 55.874 ms
Node 6, Graph-Level Communication Throughput: 104.182 Gbps, Time: 53.785 ms
Node 3, Graph-Level Communication Throughput: 109.842 Gbps, Time: 52.661 ms
Node 7, Graph-Level Communication Throughput: 108.040 Gbps, Time: 53.539 ms
------------------------node id 0,  per-epoch time: 0.482163 s---------------
------------------------node id 1,  per-epoch time: 0.482163 s---------------
------------------------node id 2,  per-epoch time: 0.482163 s---------------
------------------------node id 3,  per-epoch time: 0.482163 s---------------
------------------------node id 4,  per-epoch time: 0.482163 s---------------
------------------------node id 5,  per-epoch time: 0.482163 s---------------
------------------------node id 6,  per-epoch time: 0.482163 s---------------
------------------------node id 7,  per-epoch time: 0.482163 s---------------
************ Profiling Results ************
	Bubble: 151.883390 (ms) (31.50 percentage)
	Compute: 259.422272 (ms) (53.80 percentage)
	GraphCommComputeOverhead: 9.874885 (ms) (2.05 percentage)
	GraphCommNetwork: 54.618137 (ms) (11.33 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.438017 (ms) (1.34 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.041 GB
	Graph-level communication (cluster-wide, per-epoch): 5.303 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 6.356 GB
	Aggregated layer-level communication throughput: 278.863 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
