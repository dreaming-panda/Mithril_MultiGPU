Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.855 seconds.
Building the CSC structure...
        It takes 2.027 seconds.
Building the CSC structure...
        It takes 2.232 seconds.
Building the CSC structure...
        It takes 2.351 seconds.
Building the CSC structure...
        It takes 2.443 seconds.
Building the CSC structure...
        It takes 2.613 seconds.
Building the CSC structure...
        It takes 2.628 seconds.
Building the CSC structure...
        It takes 2.695 seconds.
Building the CSC structure...
        It takes 1.822 seconds.
        It takes 1.873 seconds.
        It takes 2.234 seconds.
        It takes 2.282 seconds.
Building the Feature Vector...
        It takes 2.363 seconds.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 2.323 seconds.
        It takes 0.036 seconds.
        It takes 2.396 seconds.
        It takes 2.375 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.297 seconds.
Building the Label Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.033 seconds.
        It takes 0.290 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.281 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 62.622 Gbps (per GPU), 500.977 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.584 Gbps (per GPU), 500.672 Gbps (aggregated)
The layer-level communication performance: 62.583 Gbps (per GPU), 500.662 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.550 Gbps (per GPU), 500.396 Gbps (aggregated)
The layer-level communication performance: 62.546 Gbps (per GPU), 500.366 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.517 Gbps (per GPU), 500.138 Gbps (aggregated)
The layer-level communication performance: 62.510 Gbps (per GPU), 500.080 Gbps (aggregated)
The layer-level communication performance: 62.505 Gbps (per GPU), 500.044 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.086 Gbps (per GPU), 1328.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.084 Gbps (per GPU), 1328.672 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.086 Gbps (per GPU), 1328.684 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.074 Gbps (per GPU), 1328.596 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.082 Gbps (per GPU), 1328.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.083 Gbps (per GPU), 1328.665 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.083 Gbps (per GPU), 1328.661 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.082 Gbps (per GPU), 1328.656 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.663 Gbps (per GPU), 917.306 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.663 Gbps (per GPU), 917.308 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.663 Gbps (per GPU), 917.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.662 Gbps (per GPU), 917.294 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.661 Gbps (per GPU), 917.287 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.662 Gbps (per GPU), 917.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.662 Gbps (per GPU), 917.298 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.662 Gbps (per GPU), 917.299 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.003 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.001 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.003 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.000 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.001 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.374 Gbps (per GPU), 362.992 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.000 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.374 Gbps (per GPU), 362.991 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.48ms  2.80ms  3.12  8.38K  3.53M
 chk_1  0.75ms  2.86ms  2.99ms  3.97  6.74K  3.60M
 chk_2  0.80ms  2.71ms  2.90ms  3.63  7.27K  3.53M
 chk_3  0.81ms  2.77ms  2.93ms  3.63  7.92K  3.61M
 chk_4  0.64ms  2.71ms  2.86ms  4.45  5.33K  3.68M
 chk_5  1.01ms  2.69ms  2.89ms  2.85 10.07K  3.45M
 chk_6  0.97ms  2.83ms  3.03ms  3.13  9.41K  3.48M
 chk_7  0.82ms  2.69ms  2.86ms  3.48  8.12K  3.60M
 chk_8  0.68ms  2.79ms  3.05ms  4.47  6.09K  3.64M
 chk_9  1.11ms  2.60ms  2.79ms  2.52 11.10K  3.38M
chk_10  0.65ms  2.84ms  2.98ms  4.56  5.67K  3.63M
chk_11  0.83ms  2.69ms  2.87ms  3.46  8.16K  3.54M
chk_12  0.80ms  2.89ms  3.06ms  3.81  7.24K  3.55M
chk_13  0.65ms  2.74ms  2.89ms  4.47  5.41K  3.68M
chk_14  0.79ms  2.94ms  3.12ms  3.97  7.14K  3.53M
chk_15  0.96ms  2.82ms  3.07ms  3.20  9.25K  3.49M
chk_16  0.60ms  2.67ms  2.82ms  4.70  4.78K  3.77M
chk_17  0.77ms  2.79ms  2.95ms  3.83  6.85K  3.60M
chk_18  0.82ms  2.61ms  2.75ms  3.37  7.47K  3.57M
chk_19  0.61ms  2.66ms  2.82ms  4.63  4.88K  3.75M
chk_20  0.78ms  2.71ms  2.83ms  3.65  7.00K  3.63M
chk_21  0.64ms  2.66ms  2.80ms  4.39  5.41K  3.68M
chk_22  1.10ms  2.87ms  3.04ms  2.75 11.07K  3.39M
chk_23  0.80ms  2.77ms  2.91ms  3.65  7.23K  3.64M
chk_24  1.02ms  2.83ms  3.00ms  2.94 10.13K  3.43M
chk_25  0.73ms  2.64ms  2.79ms  3.82  6.40K  3.57M
chk_26  0.66ms  2.82ms  3.00ms  4.53  5.78K  3.55M
chk_27  0.96ms  2.73ms  2.92ms  3.04  9.34K  3.48M
chk_28  0.73ms  2.97ms  3.15ms  4.33  6.37K  3.57M
chk_29  0.63ms  2.81ms  2.97ms  4.70  5.16K  3.78M
chk_30  0.64ms  2.69ms  2.86ms  4.48  5.44K  3.67M
chk_31  0.73ms  2.83ms  3.02ms  4.15  6.33K  3.63M
   Avg  0.79  2.75  2.93
   Max  1.11  2.97  3.15
   Min  0.60  2.48  2.75
 Ratio  1.84  1.20  1.15
   Var  0.02  0.01  0.01
Profiling takes 2.480 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 377.902 ms
Partition 0 [0, 5) has cost: 377.902 ms
Partition 1 [5, 9) has cost: 352.522 ms
Partition 2 [9, 13) has cost: 352.522 ms
Partition 3 [13, 17) has cost: 352.522 ms
Partition 4 [17, 21) has cost: 352.522 ms
Partition 5 [21, 25) has cost: 352.522 ms
Partition 6 [25, 29) has cost: 352.522 ms
Partition 7 [29, 33) has cost: 358.114 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 167.776 ms
GPU 0, Compute+Comm Time: 136.025 ms, Bubble Time: 29.401 ms, Imbalance Overhead: 2.350 ms
GPU 1, Compute+Comm Time: 128.922 ms, Bubble Time: 29.109 ms, Imbalance Overhead: 9.744 ms
GPU 2, Compute+Comm Time: 128.922 ms, Bubble Time: 29.187 ms, Imbalance Overhead: 9.666 ms
GPU 3, Compute+Comm Time: 128.922 ms, Bubble Time: 29.111 ms, Imbalance Overhead: 9.743 ms
GPU 4, Compute+Comm Time: 128.922 ms, Bubble Time: 29.089 ms, Imbalance Overhead: 9.764 ms
GPU 5, Compute+Comm Time: 128.922 ms, Bubble Time: 29.146 ms, Imbalance Overhead: 9.708 ms
GPU 6, Compute+Comm Time: 128.922 ms, Bubble Time: 29.394 ms, Imbalance Overhead: 9.460 ms
GPU 7, Compute+Comm Time: 130.116 ms, Bubble Time: 29.751 ms, Imbalance Overhead: 7.908 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 325.689 ms
GPU 0, Compute+Comm Time: 251.807 ms, Bubble Time: 58.038 ms, Imbalance Overhead: 15.844 ms
GPU 1, Compute+Comm Time: 247.409 ms, Bubble Time: 57.303 ms, Imbalance Overhead: 20.978 ms
GPU 2, Compute+Comm Time: 247.409 ms, Bubble Time: 56.723 ms, Imbalance Overhead: 21.558 ms
GPU 3, Compute+Comm Time: 247.409 ms, Bubble Time: 56.628 ms, Imbalance Overhead: 21.652 ms
GPU 4, Compute+Comm Time: 247.409 ms, Bubble Time: 56.654 ms, Imbalance Overhead: 21.627 ms
GPU 5, Compute+Comm Time: 247.409 ms, Bubble Time: 56.725 ms, Imbalance Overhead: 21.556 ms
GPU 6, Compute+Comm Time: 247.409 ms, Bubble Time: 56.480 ms, Imbalance Overhead: 21.801 ms
GPU 7, Compute+Comm Time: 265.686 ms, Bubble Time: 57.061 ms, Imbalance Overhead: 2.942 ms
The estimated cost of the whole pipeline: 518.138 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 730.423 ms
Partition 0 [0, 9) has cost: 730.423 ms
Partition 1 [9, 17) has cost: 705.043 ms
Partition 2 [17, 25) has cost: 705.043 ms
Partition 3 [25, 33) has cost: 710.636 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 175.906 ms
GPU 0, Compute+Comm Time: 146.168 ms, Bubble Time: 26.680 ms, Imbalance Overhead: 3.058 ms
GPU 1, Compute+Comm Time: 142.225 ms, Bubble Time: 26.905 ms, Imbalance Overhead: 6.776 ms
GPU 2, Compute+Comm Time: 142.225 ms, Bubble Time: 26.892 ms, Imbalance Overhead: 6.789 ms
GPU 3, Compute+Comm Time: 142.775 ms, Bubble Time: 27.319 ms, Imbalance Overhead: 5.812 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 327.745 ms
GPU 0, Compute+Comm Time: 265.732 ms, Bubble Time: 50.847 ms, Imbalance Overhead: 11.165 ms
GPU 1, Compute+Comm Time: 263.518 ms, Bubble Time: 50.473 ms, Imbalance Overhead: 13.754 ms
GPU 2, Compute+Comm Time: 263.518 ms, Bubble Time: 50.828 ms, Imbalance Overhead: 13.398 ms
GPU 3, Compute+Comm Time: 273.868 ms, Bubble Time: 50.473 ms, Imbalance Overhead: 3.404 ms
    The estimated cost with 2 DP ways is 528.833 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1435.467 ms
Partition 0 [0, 17) has cost: 1435.467 ms
Partition 1 [17, 33) has cost: 1415.679 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 211.669 ms
GPU 0, Compute+Comm Time: 185.121 ms, Bubble Time: 21.279 ms, Imbalance Overhead: 5.268 ms
GPU 1, Compute+Comm Time: 183.319 ms, Bubble Time: 22.575 ms, Imbalance Overhead: 5.775 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 353.842 ms
GPU 0, Compute+Comm Time: 307.922 ms, Bubble Time: 38.364 ms, Imbalance Overhead: 7.555 ms
GPU 1, Compute+Comm Time: 312.336 ms, Bubble Time: 37.408 ms, Imbalance Overhead: 4.098 ms
    The estimated cost with 4 DP ways is 593.787 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2851.146 ms
Partition 0 [0, 33) has cost: 2851.146 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 459.517 ms
GPU 0, Compute+Comm Time: 459.517 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 587.515 ms
GPU 0, Compute+Comm Time: 587.515 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1099.383 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5507
	Epoch 50:	Loss 1.9078
	Epoch 75:	Loss 1.5316
	Epoch 100:	Loss 1.3181
	Epoch 125:	Loss 1.1438
	Epoch 150:	Loss 1.0416
	Epoch 175:	Loss 0.9755
	Epoch 200:	Loss 0.9207
	Epoch 225:	Loss 0.8708
	Epoch 250:	Loss 0.8379
	Epoch 275:	Loss 0.8182
	Epoch 300:	Loss 0.7840
	Epoch 325:	Loss 0.7583
	Epoch 350:	Loss 0.7398
	Epoch 375:	Loss 0.7249
	Epoch 400:	Loss 0.7058
	Epoch 425:	Loss 0.6920
	Epoch 450:	Loss 0.6850
	Epoch 475:	Loss 0.6713
	Epoch 500:	Loss 0.6673
	Epoch 525:	Loss 0.6542
	Epoch 550:	Loss 0.6442
	Epoch 575:	Loss 0.6351
	Epoch 600:	Loss 0.6286
	Epoch 625:	Loss 0.6230
	Epoch 650:	Loss 0.6152
	Epoch 675:	Loss 0.6119
	Epoch 700:	Loss 0.6051
	Epoch 725:	Loss 0.6004
	Epoch 750:	Loss 0.5977
	Epoch 775:	Loss 0.5896
	Epoch 800:	Loss 0.5822
	Epoch 825:	Loss 0.5843
	Epoch 850:	Loss 0.5727
	Epoch 875:	Loss 0.5751
	Epoch 900:	Loss 0.5715
	Epoch 925:	Loss 0.5648
	Epoch 950:	Loss 0.5622
	Epoch 975:	Loss 0.5579
	Epoch 1000:	Loss 0.5523
	Epoch 1025:	Loss 0.5500
	Epoch 1050:	Loss 0.5521
	Epoch 1075:	Loss 0.5441
	Epoch 1100:	Loss 0.5456
	Epoch 1125:	Loss 0.5413
	Epoch 1150:	Loss 0.5388
	Epoch 1175:	Loss 0.5380
	Epoch 1200:	Loss 0.5328
	Epoch 1225:	Loss 0.5332
	Epoch 1250:	Loss 0.5289
	Epoch 1275:	Loss 0.5261
	Epoch 1300:	Loss 0.5202
	Epoch 1325:	Loss 0.5193
	Epoch 1350:	Loss 0.5207
	Epoch 1375:	Loss 0.5205
	Epoch 1400:	Loss 0.5186
	Epoch 1425:	Loss 0.5142
	Epoch 1450:	Loss 0.5134
	Epoch 1475:	Loss 0.5114
	Epoch 1500:	Loss 0.5139
	Epoch 1525:	Loss 0.5031
	Epoch 1550:	Loss 0.5087
	Epoch 1575:	Loss 0.5051
	Epoch 1600:	Loss 0.5028
	Epoch 1625:	Loss 0.5035
	Epoch 1650:	Loss 0.4983
	Epoch 1675:	Loss 0.4997
	Epoch 1700:	Loss 0.4938
	Epoch 1725:	Loss 0.4941
	Epoch 1750:	Loss 0.4942
	Epoch 1775:	Loss 0.4911
	Epoch 1800:	Loss 0.4898
	Epoch 1825:	Loss 0.4920
	Epoch 1850:	Loss 0.4851
	Epoch 1875:	Loss 0.4914
	Epoch 1900:	Loss 0.4843
	Epoch 1925:	Loss 0.4845
	Epoch 1950:	Loss 0.4820
	Epoch 1975:	Loss 0.4792
	Epoch 2000:	Loss 0.4803
	Epoch 2025:	Loss 0.4829
	Epoch 2050:	Loss 0.4832
	Epoch 2075:	Loss 0.4800
	Epoch 2100:	Loss 0.4738
	Epoch 2125:	Loss 0.4790
	Epoch 2150:	Loss 0.4744
	Epoch 2175:	Loss 0.4763
	Epoch 2200:	Loss 0.4789
	Epoch 2225:	Loss 0.4673
	Epoch 2250:	Loss 0.4725
	Epoch 2275:	Loss 0.4698
	Epoch 2300:	Loss 0.4706
	Epoch 2325:	Loss 0.4749
	Epoch 2350:	Loss 0.4659
	Epoch 2375:	Loss 0.4656
	Epoch 2400:	Loss 0.4646
	Epoch 2425:	Loss 0.4636
	Epoch 2450:	Loss 0.4667
	Epoch 2475:	Loss 0.4614
	Epoch 2500:	Loss 0.4629
	Epoch 2525:	Loss 0.4655
	Epoch 2550:	Loss 0.4638
	Epoch 2575:	Loss 0.4572
	Epoch 2600:	Loss 0.4600
	Epoch 2625:	Loss 0.4595
	Epoch 2650:	Loss 0.4590
	Epoch 2675:	Loss 0.4563
	Epoch 2700:	Loss 0.4578
	Epoch 2725:	Loss 0.4580
	Epoch 2750:	Loss 0.4595
	Epoch 2775:	Loss 0.4573
	Epoch 2800:	Loss 0.4525
	Epoch 2825:	Loss 0.4497
	Epoch 2850:	Loss 0.4575
	Epoch 2875:	Loss 0.4492
	Epoch 2900:	Loss 0.4537
	Epoch 2925:	Loss 0.4501
	Epoch 2950:	Loss 0.4494
	Epoch 2975:	Loss 0.4594
	Epoch 3000:	Loss 0.4491
	Epoch 3025:	Loss 0.4527
	Epoch 3050:	Loss 0.4468
	Epoch 3075:	Loss 0.4486
	Epoch 3100:	Loss 0.4451
	Epoch 3125:	Loss 0.4447
	Epoch 3150:	Loss 0.4477
	Epoch 3175:	Loss 0.4472
	Epoch 3200:	Loss 0.4403
	Epoch 3225:	Loss 0.4411
	Epoch 3250:	Loss 0.4439
	Epoch 3275:	Loss 0.4456
	Epoch 3300:	Loss 0.4436
	Epoch 3325:	Loss 0.4464
	Epoch 3350:	Loss 0.4437
	Epoch 3375:	Loss 0.4399
	Epoch 3400:	Loss 0.4399
	Epoch 3425:	Loss 0.4436
	Epoch 3450:	Loss 0.4395
	Epoch 3475:	Loss 0.4409
	Epoch 3500:	Loss 0.4391
	Epoch 3525:	Loss 0.4401
	Epoch 3550:	Loss 0.4366
	Epoch 3575:	Loss 0.4342
	Epoch 3600:	Loss 0.4341
	Epoch 3625:	Loss 0.4340
	Epoch 3650:	Loss 0.4360
	Epoch 3675:	Loss 0.4351
	Epoch 3700:	Loss 0.4367
	Epoch 3725:	Loss 0.4350
	Epoch 3750:	Loss 0.4315
	Epoch 3775:	Loss 0.4349
	Epoch 3800:	Loss 0.4385
	Epoch 3825:	Loss 0.4315
	Epoch 3850:	Loss 0.4345
	Epoch 3875:	Loss 0.4315
	Epoch 3900:	Loss 0.4314
	Epoch 3925:	Loss 0.4329
	Epoch 3950:	Loss 0.4344
	Epoch 3975:	Loss 0.4323
	Epoch 4000:	Loss 0.4276
	Epoch 4025:	Loss 0.4256
	Epoch 4050:	Loss 0.4294
	Epoch 4075:	Loss 0.4267
	Epoch 4100:	Loss 0.4277
	Epoch 4125:	Loss 0.4275
	Epoch 4150:	Loss 0.4240
	Epoch 4175:	Loss 0.4234
	Epoch 4200:	Loss 0.4263
	Epoch 4225:	Loss 0.4266
	Epoch 4250:	Loss 0.4236
	Epoch 4275:	Loss 0.4255
	Epoch 4300:	Loss 0.4236
	Epoch 4325:	Loss 0.4264
	Epoch 4350:	Loss 0.4238
	Epoch 4375:	Loss 0.4233
	Epoch 4400:	Loss 0.4207
	Epoch 4425:	Loss 0.4221
	Epoch 4450:	Loss 0.4187
	Epoch 4475:	Loss 0.4188
	Epoch 4500:	Loss 0.4212
	Epoch 4525:	Loss 0.4237
	Epoch 4550:	Loss 0.4182
	Epoch 4575:	Loss 0.4178
	Epoch 4600:	Loss 0.4184
	Epoch 4625:	Loss 0.4217
	Epoch 4650:	Loss 0.4174
	Epoch 4675:	Loss 0.4184
	Epoch 4700:	Loss 0.4180
	Epoch 4725:	Loss 0.4130
	Epoch 4750:	Loss 0.4213
	Epoch 4775:	Loss 0.4134
	Epoch 4800:	Loss 0.4218
	Epoch 4825:	Loss 0.4180
	Epoch 4850:	Loss 0.4169
	Epoch 4875:	Loss 0.4162
	Epoch 4900:	Loss 0.4156
	Epoch 4925:	Loss 0.4167
	Epoch 4950:	Loss 0.4148
	Epoch 4975:	Loss 0.4173
	Epoch 5000:	Loss 0.4153
Node 1, Pre/Post-Pipelining: 1.093 / 12.013 ms, Bubble: 83.303 ms, Compute: 262.406 ms, Comm: 43.048 ms, Imbalance: 31.675 ms
Node 3, Pre/Post-Pipelining: 1.093 / 11.995 ms, Bubble: 82.780 ms, Compute: 258.170 ms, Comm: 47.801 ms, Imbalance: 32.065 ms
Node 7, Pre/Post-Pipelining: 1.098 / 27.769 ms, Bubble: 67.582 ms, Compute: 290.690 ms, Comm: 32.501 ms, Imbalance: 14.015 ms
Node 2, Pre/Post-Pipelining: 1.089 / 12.031 ms, Bubble: 83.853 ms, Compute: 255.734 ms, Comm: 47.793 ms, Imbalance: 33.634 ms
Node 0, Pre/Post-Pipelining: 1.095 / 12.056 ms, Bubble: 83.425 ms, Compute: 290.504 ms, Comm: 32.977 ms, Imbalance: 12.874 ms
Node 5, Pre/Post-Pipelining: 1.089 / 11.998 ms, Bubble: 83.255 ms, Compute: 260.868 ms, Comm: 45.877 ms, Imbalance: 30.960 ms
Node 4, Pre/Post-Pipelining: 1.088 / 12.024 ms, Bubble: 83.152 ms, Compute: 255.395 ms, Comm: 47.230 ms, Imbalance: 35.523 ms
Node 6, Pre/Post-Pipelining: 1.088 / 12.045 ms, Bubble: 83.203 ms, Compute: 262.315 ms, Comm: 41.307 ms, Imbalance: 34.182 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.095 ms
Cluster-Wide Average, Post-Pipelining Overhead: 12.056 ms
Cluster-Wide Average, Bubble: 83.425 ms
Cluster-Wide Average, Compute: 290.504 ms
Cluster-Wide Average, Communication: 32.977 ms
Cluster-Wide Average, Imbalance: 12.874 ms
Node 0, GPU memory consumption: 8.059 GB
Node 1, GPU memory consumption: 6.042 GB
Node 3, GPU memory consumption: 6.018 GB
Node 4, GPU memory consumption: 6.018 GB
Node 2, GPU memory consumption: 6.042 GB
Node 7, GPU memory consumption: 6.171 GB
Node 5, GPU memory consumption: 6.042 GB
Node 6, GPU memory consumption: 6.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.433609 s---------------
------------------------node id 1,  per-epoch time: 0.433609 s---------------
------------------------node id 4,  per-epoch time: 0.433609 s---------------
------------------------node id 2,  per-epoch time: 0.433609 s---------------
------------------------node id 5,  per-epoch time: 0.433609 s---------------
------------------------node id 3,  per-epoch time: 0.433609 s---------------
------------------------node id 6,  per-epoch time: 0.433609 s---------------
------------------------node id 7,  per-epoch time: 0.433609 s---------------
************ Profiling Results ************
	Bubble: 167.408062 (ms) (38.60 percentage)
	Compute: 262.760376 (ms) (60.58 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.550408 (ms) (0.82 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 2.441 GB
	Aggregated layer-level communication throughput: 493.272 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
