Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.825 seconds.
Building the CSC structure...
        It takes 2.336 seconds.
Building the CSC structure...
        It takes 2.361 seconds.
Building the CSC structure...
        It takes 2.384 seconds.
Building the CSC structure...
        It takes 2.406 seconds.
Building the CSC structure...
        It takes 2.621 seconds.
Building the CSC structure...
        It takes 2.633 seconds.
Building the CSC structure...
        It takes 2.692 seconds.
Building the CSC structure...
        It takes 1.892 seconds.
        It takes 2.179 seconds.
        It takes 2.321 seconds.
        It takes 2.379 seconds.
        It takes 2.326 seconds.
        It takes 2.283 seconds.
Building the Feature Vector...
        It takes 2.388 seconds.
        It takes 2.393 seconds.
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.301 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.301 seconds.
        It takes 0.284 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.039 seconds.
        It takes 0.041 seconds.
Building the Feature Vector...
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Building the Feature Vector...
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 62.779 Gbps (per GPU), 502.234 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.739 Gbps (per GPU), 501.909 Gbps (aggregated)
The layer-level communication performance: 62.740 Gbps (per GPU), 501.922 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.700 Gbps (per GPU), 501.601 Gbps (aggregated)
The layer-level communication performance: 62.695 Gbps (per GPU), 501.556 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.665 Gbps (per GPU), 501.323 Gbps (aggregated)
The layer-level communication performance: 62.657 Gbps (per GPU), 501.256 Gbps (aggregated)
The layer-level communication performance: 62.653 Gbps (per GPU), 501.220 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.236 Gbps (per GPU), 1329.886 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.237 Gbps (per GPU), 1329.898 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.224 Gbps (per GPU), 1329.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.233 Gbps (per GPU), 1329.863 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.229 Gbps (per GPU), 1329.830 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.230 Gbps (per GPU), 1329.836 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.230 Gbps (per GPU), 1329.836 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.228 Gbps (per GPU), 1329.826 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.310 Gbps (per GPU), 914.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.312 Gbps (per GPU), 914.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.311 Gbps (per GPU), 914.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.311 Gbps (per GPU), 914.487 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.311 Gbps (per GPU), 914.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.310 Gbps (per GPU), 914.482 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.310 Gbps (per GPU), 914.484 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.311 Gbps (per GPU), 914.488 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.776 Gbps (per GPU), 358.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.776 Gbps (per GPU), 358.210 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.776 Gbps (per GPU), 358.208 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.776 Gbps (per GPU), 358.208 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.776 Gbps (per GPU), 358.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.776 Gbps (per GPU), 358.208 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.776 Gbps (per GPU), 358.207 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.776 Gbps (per GPU), 358.208 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.45ms  2.77ms  3.11  8.38K  3.53M
 chk_1  0.75ms  2.81ms  2.96ms  3.93  6.74K  3.60M
 chk_2  0.80ms  2.68ms  2.86ms  3.58  7.27K  3.53M
 chk_3  0.80ms  2.73ms  2.89ms  3.60  7.92K  3.61M
 chk_4  0.63ms  2.66ms  2.82ms  4.48  5.33K  3.68M
 chk_5  1.01ms  2.65ms  2.85ms  2.82 10.07K  3.45M
 chk_6  0.96ms  2.79ms  2.99ms  3.11  9.41K  3.48M
 chk_7  0.82ms  2.65ms  2.82ms  3.44  8.12K  3.60M
 chk_8  0.68ms  2.76ms  2.92ms  4.28  6.09K  3.64M
 chk_9  1.10ms  2.57ms  2.77ms  2.51 11.10K  3.38M
chk_10  0.65ms  2.80ms  2.95ms  4.53  5.67K  3.63M
chk_11  0.82ms  2.65ms  2.83ms  3.44  8.16K  3.54M
chk_12  0.80ms  2.86ms  3.02ms  3.79  7.24K  3.55M
chk_13  0.64ms  2.70ms  2.86ms  4.49  5.41K  3.68M
chk_14  0.78ms  2.93ms  3.09ms  3.95  7.14K  3.53M
chk_15  0.95ms  2.80ms  2.98ms  3.12  9.25K  3.49M
chk_16  0.60ms  2.63ms  2.78ms  4.66  4.78K  3.77M
chk_17  0.76ms  2.78ms  2.90ms  3.80  6.85K  3.60M
chk_18  0.81ms  2.56ms  2.71ms  3.35  7.47K  3.57M
chk_19  0.61ms  2.65ms  2.76ms  4.54  4.88K  3.75M
chk_20  0.77ms  2.62ms  2.77ms  3.60  7.00K  3.63M
chk_21  0.64ms  2.62ms  2.76ms  4.34  5.41K  3.68M
chk_22  1.10ms  2.82ms  3.03ms  2.75 11.07K  3.39M
chk_23  0.79ms  2.72ms  2.86ms  3.60  7.23K  3.64M
chk_24  1.01ms  2.80ms  2.98ms  2.95 10.13K  3.43M
chk_25  0.73ms  2.60ms  2.75ms  3.77  6.40K  3.57M
chk_26  0.66ms  2.81ms  2.95ms  4.48  5.78K  3.55M
chk_27  0.96ms  2.68ms  2.91ms  3.03  9.34K  3.48M
chk_28  0.73ms  2.98ms  3.15ms  4.33  6.37K  3.57M
chk_29  0.63ms  2.77ms  2.93ms  4.65  5.16K  3.78M
chk_30  0.64ms  2.66ms  2.83ms  4.44  5.44K  3.67M
chk_31  0.72ms  2.81ms  2.96ms  4.10  6.33K  3.63M
   Avg  0.79  2.72  2.89
   Max  1.10  2.98  3.15
   Min  0.60  2.45  2.71
 Ratio  1.85  1.22  1.16
   Var  0.02  0.01  0.01
Profiling takes 2.448 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 373.182 ms
Partition 0 [0, 5) has cost: 373.182 ms
Partition 1 [5, 9) has cost: 347.928 ms
Partition 2 [9, 13) has cost: 347.928 ms
Partition 3 [13, 17) has cost: 347.928 ms
Partition 4 [17, 21) has cost: 347.928 ms
Partition 5 [21, 25) has cost: 347.928 ms
Partition 6 [25, 29) has cost: 347.928 ms
Partition 7 [29, 33) has cost: 353.364 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 166.270 ms
GPU 0, Compute+Comm Time: 134.424 ms, Bubble Time: 29.380 ms, Imbalance Overhead: 2.466 ms
GPU 1, Compute+Comm Time: 127.361 ms, Bubble Time: 29.031 ms, Imbalance Overhead: 9.878 ms
GPU 2, Compute+Comm Time: 127.361 ms, Bubble Time: 29.016 ms, Imbalance Overhead: 9.893 ms
GPU 3, Compute+Comm Time: 127.361 ms, Bubble Time: 28.874 ms, Imbalance Overhead: 10.035 ms
GPU 4, Compute+Comm Time: 127.361 ms, Bubble Time: 28.785 ms, Imbalance Overhead: 10.124 ms
GPU 5, Compute+Comm Time: 127.361 ms, Bubble Time: 28.797 ms, Imbalance Overhead: 10.112 ms
GPU 6, Compute+Comm Time: 127.361 ms, Bubble Time: 29.019 ms, Imbalance Overhead: 9.890 ms
GPU 7, Compute+Comm Time: 128.500 ms, Bubble Time: 29.361 ms, Imbalance Overhead: 8.410 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 322.817 ms
GPU 0, Compute+Comm Time: 248.614 ms, Bubble Time: 57.311 ms, Imbalance Overhead: 16.893 ms
GPU 1, Compute+Comm Time: 244.316 ms, Bubble Time: 56.583 ms, Imbalance Overhead: 21.918 ms
GPU 2, Compute+Comm Time: 244.316 ms, Bubble Time: 56.050 ms, Imbalance Overhead: 22.451 ms
GPU 3, Compute+Comm Time: 244.316 ms, Bubble Time: 56.012 ms, Imbalance Overhead: 22.489 ms
GPU 4, Compute+Comm Time: 244.316 ms, Bubble Time: 56.133 ms, Imbalance Overhead: 22.368 ms
GPU 5, Compute+Comm Time: 244.316 ms, Bubble Time: 56.296 ms, Imbalance Overhead: 22.205 ms
GPU 6, Compute+Comm Time: 244.316 ms, Bubble Time: 56.178 ms, Imbalance Overhead: 22.323 ms
GPU 7, Compute+Comm Time: 262.508 ms, Bubble Time: 56.845 ms, Imbalance Overhead: 3.465 ms
The estimated cost of the whole pipeline: 513.542 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 721.110 ms
Partition 0 [0, 9) has cost: 721.110 ms
Partition 1 [9, 17) has cost: 695.856 ms
Partition 2 [17, 25) has cost: 695.856 ms
Partition 3 [25, 33) has cost: 701.292 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 174.038 ms
GPU 0, Compute+Comm Time: 144.696 ms, Bubble Time: 26.630 ms, Imbalance Overhead: 2.712 ms
GPU 1, Compute+Comm Time: 140.780 ms, Bubble Time: 26.721 ms, Imbalance Overhead: 6.538 ms
GPU 2, Compute+Comm Time: 140.780 ms, Bubble Time: 26.535 ms, Imbalance Overhead: 6.723 ms
GPU 3, Compute+Comm Time: 141.293 ms, Bubble Time: 26.868 ms, Imbalance Overhead: 5.877 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 325.334 ms
GPU 0, Compute+Comm Time: 262.815 ms, Bubble Time: 50.176 ms, Imbalance Overhead: 12.343 ms
GPU 1, Compute+Comm Time: 260.743 ms, Bubble Time: 49.878 ms, Imbalance Overhead: 14.713 ms
GPU 2, Compute+Comm Time: 260.743 ms, Bubble Time: 50.423 ms, Imbalance Overhead: 14.168 ms
GPU 3, Compute+Comm Time: 271.046 ms, Bubble Time: 50.324 ms, Imbalance Overhead: 3.964 ms
    The estimated cost with 2 DP ways is 524.341 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1416.967 ms
Partition 0 [0, 17) has cost: 1416.967 ms
Partition 1 [17, 33) has cost: 1397.148 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 210.183 ms
GPU 0, Compute+Comm Time: 184.201 ms, Bubble Time: 21.364 ms, Imbalance Overhead: 4.618 ms
GPU 1, Compute+Comm Time: 182.379 ms, Bubble Time: 22.302 ms, Imbalance Overhead: 5.503 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 350.935 ms
GPU 0, Compute+Comm Time: 305.409 ms, Bubble Time: 37.879 ms, Imbalance Overhead: 7.646 ms
GPU 1, Compute+Comm Time: 309.856 ms, Bubble Time: 37.429 ms, Imbalance Overhead: 3.650 ms
    The estimated cost with 4 DP ways is 589.174 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2814.115 ms
Partition 0 [0, 33) has cost: 2814.115 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 462.679 ms
GPU 0, Compute+Comm Time: 462.679 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 589.611 ms
GPU 0, Compute+Comm Time: 589.611 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1104.904 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5719
	Epoch 50:	Loss 1.9244
	Epoch 75:	Loss 1.5380
	Epoch 100:	Loss 1.3130
	Epoch 125:	Loss 1.1247
	Epoch 150:	Loss 1.0324
	Epoch 175:	Loss 0.9689
	Epoch 200:	Loss 0.9107
	Epoch 225:	Loss 0.8581
	Epoch 250:	Loss 0.8284
	Epoch 275:	Loss 0.8017
	Epoch 300:	Loss 0.7770
	Epoch 325:	Loss 0.7547
	Epoch 350:	Loss 0.7353
	Epoch 375:	Loss 0.7140
	Epoch 400:	Loss 0.7022
	Epoch 425:	Loss 0.6892
	Epoch 450:	Loss 0.6793
	Epoch 475:	Loss 0.6670
	Epoch 500:	Loss 0.6596
	Epoch 525:	Loss 0.6494
	Epoch 550:	Loss 0.6435
	Epoch 575:	Loss 0.6337
	Epoch 600:	Loss 0.6234
	Epoch 625:	Loss 0.6123
	Epoch 650:	Loss 0.6096
	Epoch 675:	Loss 0.6049
	Epoch 700:	Loss 0.5990
	Epoch 725:	Loss 0.5928
	Epoch 750:	Loss 0.5901
	Epoch 775:	Loss 0.5861
	Epoch 800:	Loss 0.5862
	Epoch 825:	Loss 0.5760
	Epoch 850:	Loss 0.5708
	Epoch 875:	Loss 0.5728
	Epoch 900:	Loss 0.5631
	Epoch 925:	Loss 0.5580
	Epoch 950:	Loss 0.5535
	Epoch 975:	Loss 0.5553
	Epoch 1000:	Loss 0.5526
	Epoch 1025:	Loss 0.5506
	Epoch 1050:	Loss 0.5470
	Epoch 1075:	Loss 0.5439
	Epoch 1100:	Loss 0.5415
	Epoch 1125:	Loss 0.5355
	Epoch 1150:	Loss 0.5346
	Epoch 1175:	Loss 0.5353
	Epoch 1200:	Loss 0.5319
	Epoch 1225:	Loss 0.5299
	Epoch 1250:	Loss 0.5274
	Epoch 1275:	Loss 0.5221
	Epoch 1300:	Loss 0.5203
	Epoch 1325:	Loss 0.5201
	Epoch 1350:	Loss 0.5131
	Epoch 1375:	Loss 0.5163
	Epoch 1400:	Loss 0.5084
	Epoch 1425:	Loss 0.5141
	Epoch 1450:	Loss 0.5131
	Epoch 1475:	Loss 0.5089
	Epoch 1500:	Loss 0.5034
	Epoch 1525:	Loss 0.5039
	Epoch 1550:	Loss 0.4981
	Epoch 1575:	Loss 0.5001
	Epoch 1600:	Loss 0.5026
	Epoch 1625:	Loss 0.4946
	Epoch 1650:	Loss 0.4956
	Epoch 1675:	Loss 0.4952
	Epoch 1700:	Loss 0.4927
	Epoch 1725:	Loss 0.4909
	Epoch 1750:	Loss 0.4926
	Epoch 1775:	Loss 0.4954
	Epoch 1800:	Loss 0.4876
	Epoch 1825:	Loss 0.4889
	Epoch 1850:	Loss 0.4822
	Epoch 1875:	Loss 0.4860
	Epoch 1900:	Loss 0.4830
	Epoch 1925:	Loss 0.4796
	Epoch 1950:	Loss 0.4805
	Epoch 1975:	Loss 0.4768
	Epoch 2000:	Loss 0.4785
	Epoch 2025:	Loss 0.4757
	Epoch 2050:	Loss 0.4769
	Epoch 2075:	Loss 0.4783
	Epoch 2100:	Loss 0.4748
	Epoch 2125:	Loss 0.4753
	Epoch 2150:	Loss 0.4721
	Epoch 2175:	Loss 0.4711
	Epoch 2200:	Loss 0.4774
	Epoch 2225:	Loss 0.4685
	Epoch 2250:	Loss 0.4688
	Epoch 2275:	Loss 0.4674
	Epoch 2300:	Loss 0.4668
	Epoch 2325:	Loss 0.4649
	Epoch 2350:	Loss 0.4638
	Epoch 2375:	Loss 0.4631
	Epoch 2400:	Loss 0.4641
	Epoch 2425:	Loss 0.4597
	Epoch 2450:	Loss 0.4605
	Epoch 2475:	Loss 0.4598
	Epoch 2500:	Loss 0.4574
	Epoch 2525:	Loss 0.4603
	Epoch 2550:	Loss 0.4579
	Epoch 2575:	Loss 0.4583
	Epoch 2600:	Loss 0.4542
	Epoch 2625:	Loss 0.4606
	Epoch 2650:	Loss 0.4553
	Epoch 2675:	Loss 0.4519
	Epoch 2700:	Loss 0.4580
	Epoch 2725:	Loss 0.4562
	Epoch 2750:	Loss 0.4532
	Epoch 2775:	Loss 0.4508
	Epoch 2800:	Loss 0.4517
	Epoch 2825:	Loss 0.4474
	Epoch 2850:	Loss 0.4533
	Epoch 2875:	Loss 0.4522
	Epoch 2900:	Loss 0.4539
	Epoch 2925:	Loss 0.4480
	Epoch 2950:	Loss 0.4517
	Epoch 2975:	Loss 0.4436
	Epoch 3000:	Loss 0.4457
	Epoch 3025:	Loss 0.4462
	Epoch 3050:	Loss 0.4472
	Epoch 3075:	Loss 0.4454
	Epoch 3100:	Loss 0.4436
	Epoch 3125:	Loss 0.4394
	Epoch 3150:	Loss 0.4410
	Epoch 3175:	Loss 0.4387
	Epoch 3200:	Loss 0.4446
	Epoch 3225:	Loss 0.4440
	Epoch 3250:	Loss 0.4396
	Epoch 3275:	Loss 0.4375
	Epoch 3300:	Loss 0.4359
	Epoch 3325:	Loss 0.4395
	Epoch 3350:	Loss 0.4405
	Epoch 3375:	Loss 0.4370
	Epoch 3400:	Loss 0.4383
	Epoch 3425:	Loss 0.4337
	Epoch 3450:	Loss 0.4346
	Epoch 3475:	Loss 0.4339
	Epoch 3500:	Loss 0.4353
	Epoch 3525:	Loss 0.4327
	Epoch 3550:	Loss 0.4352
	Epoch 3575:	Loss 0.4346
	Epoch 3600:	Loss 0.4345
	Epoch 3625:	Loss 0.4323
	Epoch 3650:	Loss 0.4365
	Epoch 3675:	Loss 0.4300
	Epoch 3700:	Loss 0.4361
	Epoch 3725:	Loss 0.4293
	Epoch 3750:	Loss 0.4310
	Epoch 3775:	Loss 0.4327
	Epoch 3800:	Loss 0.4302
	Epoch 3825:	Loss 0.4306
	Epoch 3850:	Loss 0.4285
	Epoch 3875:	Loss 0.4280
	Epoch 3900:	Loss 0.4292
	Epoch 3925:	Loss 0.4303
	Epoch 3950:	Loss 0.4242
	Epoch 3975:	Loss 0.4307
	Epoch 4000:	Loss 0.4273
	Epoch 4025:	Loss 0.4260
	Epoch 4050:	Loss 0.4219
	Epoch 4075:	Loss 0.4252
	Epoch 4100:	Loss 0.4270
	Epoch 4125:	Loss 0.4183
	Epoch 4150:	Loss 0.4240
	Epoch 4175:	Loss 0.4248
	Epoch 4200:	Loss 0.4222
	Epoch 4225:	Loss 0.4203
	Epoch 4250:	Loss 0.4214
	Epoch 4275:	Loss 0.4240
	Epoch 4300:	Loss 0.4209
	Epoch 4325:	Loss 0.4194
	Epoch 4350:	Loss 0.4176
	Epoch 4375:	Loss 0.4189
	Epoch 4400:	Loss 0.4196
	Epoch 4425:	Loss 0.4200
	Epoch 4450:	Loss 0.4170
	Epoch 4475:	Loss 0.4119
	Epoch 4500:	Loss 0.4145
	Epoch 4525:	Loss 0.4159
	Epoch 4550:	Loss 0.4195
	Epoch 4575:	Loss 0.4145
	Epoch 4600:	Loss 0.4154
	Epoch 4625:	Loss 0.4070
	Epoch 4650:	Loss 0.4162
	Epoch 4675:	Loss 0.4189
	Epoch 4700:	Loss 0.4149
	Epoch 4725:	Loss 0.4147
	Epoch 4750:	Loss 0.4185
	Epoch 4775:	Loss 0.4177
	Epoch 4800:	Loss 0.4092
	Epoch 4825:	Loss 0.4081
	Epoch 4850:	Loss 0.4083
	Epoch 4875:	Loss 0.4109
	Epoch 4900:	Loss 0.4076
	Epoch 4925:	Loss 0.4101
	Epoch 4950:	Loss 0.4088
	Epoch 4975:	Loss 0.4074
	Epoch 5000:	Loss 0.4123
Node 1, Pre/Post-Pipelining: 1.092 / 12.180 ms, Bubble: 83.109 ms, Compute: 262.367 ms, Comm: 42.915 ms, Imbalance: 31.541 ms
Node 4, Pre/Post-Pipelining: 1.090 / 12.168 ms, Bubble: 82.984 ms, Compute: 255.118 ms, Comm: 47.142 ms, Imbalance: 35.582 ms
Node 6, Pre/Post-Pipelining: 1.090 / 12.152 ms, Bubble: 83.147 ms, Compute: 260.227 ms, Comm: 41.190 ms, Imbalance: 36.018 ms
Node 7, Pre/Post-Pipelining: 1.097 / 27.850 ms, Bubble: 67.542 ms, Compute: 289.970 ms, Comm: 32.429 ms, Imbalance: 14.417 ms
Node 0, Pre/Post-Pipelining: 1.094 / 12.214 ms, Bubble: 83.214 ms, Compute: 290.250 ms, Comm: 32.886 ms, Imbalance: 12.922 ms
Node 5, Pre/Post-Pipelining: 1.089 / 12.172 ms, Bubble: 83.043 ms, Compute: 261.954 ms, Comm: 45.621 ms, Imbalance: 29.825 ms
Node 2, Pre/Post-Pipelining: 1.088 / 12.190 ms, Bubble: 83.646 ms, Compute: 255.875 ms, Comm: 47.740 ms, Imbalance: 33.259 ms
Node 3, Pre/Post-Pipelining: 1.088 / 12.194 ms, Bubble: 82.478 ms, Compute: 259.754 ms, Comm: 47.809 ms, Imbalance: 30.182 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.094 ms
Cluster-Wide Average, Post-Pipelining Overhead: 12.214 ms
Cluster-Wide Average, Bubble: 83.214 ms
Cluster-Wide Average, Compute: 290.250 ms
Cluster-Wide Average, Communication: 32.886 ms
Cluster-Wide Average, Imbalance: 12.922 ms
Node 0, GPU memory consumption: 8.059 GB
Node 2, GPU memory consumption: 6.042 GB
Node 1, GPU memory consumption: 6.042 GB
Node 3, GPU memory consumption: 6.018 GB
Node 4, GPU memory consumption: 6.018 GB
Node 5, GPU memory consumption: 6.042 GB
Node 6, GPU memory consumption: 6.042 GB
Node 7, GPU memory consumption: 6.171 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.433266 s---------------
------------------------node id 1,  per-epoch time: 0.433266 s---------------
------------------------node id 2,  per-epoch time: 0.433267 s---------------
------------------------node id 3,  per-epoch time: 0.433267 s---------------
------------------------node id 4,  per-epoch time: 0.433266 s---------------
------------------------node id 5,  per-epoch time: 0.433266 s---------------
------------------------node id 6,  per-epoch time: 0.433266 s---------------
------------------------node id 7,  per-epoch time: 0.433266 s---------------
************ Profiling Results ************
	Bubble: 167.130830 (ms) (38.57 percentage)
	Compute: 262.677578 (ms) (60.61 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.560266 (ms) (0.82 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 2.441 GB
	Aggregated layer-level communication throughput: 494.446 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
