Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.871 seconds.
Building the CSC structure...
        It takes 1.884 seconds.
Building the CSC structure...
        It takes 2.035 seconds.
Building the CSC structure...
        It takes 2.056 seconds.
Building the CSC structure...
        It takes 2.287 seconds.
Building the CSC structure...
        It takes 2.431 seconds.
Building the CSC structure...
        It takes 2.572 seconds.
Building the CSC structure...
        It takes 2.660 seconds.
Building the CSC structure...
        It takes 1.820 seconds.
        It takes 1.847 seconds.
        It takes 1.864 seconds.
        It takes 1.866 seconds.
        It takes 2.215 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.323 seconds.
        It takes 2.353 seconds.
        It takes 0.302 seconds.
        It takes 0.283 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.043 seconds.
Building the Feature Vector...
        It takes 2.378 seconds.
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.279 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.032 seconds.
        It takes 0.256 seconds.
Building the Label Vector...
GPU 0, layer [0, 33)
        It takes 0.030 seconds.
GPU 0, layer [0, 33)
        It takes 0.307 seconds.
Building the Label Vector...
        It takes 0.045 seconds.
Building the Feature Vector...
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 62.878 Gbps (per GPU), 503.022 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.837 Gbps (per GPU), 502.697 Gbps (aggregated)
The layer-level communication performance: 62.836 Gbps (per GPU), 502.692 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.801 Gbps (per GPU), 502.405 Gbps (aggregated)
The layer-level communication performance: 62.796 Gbps (per GPU), 502.369 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.770 Gbps (per GPU), 502.158 Gbps (aggregated)
The layer-level communication performance: 62.760 Gbps (per GPU), 502.081 Gbps (aggregated)
The layer-level communication performance: 62.756 Gbps (per GPU), 502.049 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 161.006 Gbps (per GPU), 1288.044 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.006 Gbps (per GPU), 1288.047 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.004 Gbps (per GPU), 1288.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.004 Gbps (per GPU), 1288.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.004 Gbps (per GPU), 1288.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.000 Gbps (per GPU), 1287.998 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.999 Gbps (per GPU), 1287.991 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.001 Gbps (per GPU), 1288.010 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.890 Gbps (per GPU), 911.123 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.891 Gbps (per GPU), 911.129 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.891 Gbps (per GPU), 911.124 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.889 Gbps (per GPU), 911.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.890 Gbps (per GPU), 911.121 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.891 Gbps (per GPU), 911.125 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.890 Gbps (per GPU), 911.117 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.890 Gbps (per GPU), 911.119 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 46.099 Gbps (per GPU), 368.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.099 Gbps (per GPU), 368.791 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.099 Gbps (per GPU), 368.792 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.099 Gbps (per GPU), 368.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.099 Gbps (per GPU), 368.788 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.098 Gbps (per GPU), 368.784 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.098 Gbps (per GPU), 368.788 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.098 Gbps (per GPU), 368.784 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.52ms  9.77ms 10.03ms  3.98 29.12K 14.23M
 chk_1  2.52ms  5.26ms  5.62ms  2.23 29.12K  6.56M
 chk_2  2.56ms 17.09ms 17.32ms  6.77 29.12K 24.68M
 chk_3  2.56ms 17.04ms 17.30ms  6.76 29.12K 22.95M
 chk_4  2.57ms  5.08ms  5.43ms  2.12 29.12K  6.33M
 chk_5  2.56ms  9.20ms  9.48ms  3.70 29.12K 12.05M
 chk_6  2.56ms 10.31ms 10.64ms  4.16 29.12K 14.60M
 chk_7  2.56ms  9.54ms 10.42ms  4.08 29.12K 13.21M
   Avg  2.55 10.41 10.78
   Max  2.57 17.09 17.32
   Min  2.52  5.08  5.43
 Ratio  1.02  3.37  3.19
   Var  0.00 18.26 17.87
Profiling takes 2.213 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 353.579 ms
Partition 0 [0, 5) has cost: 353.579 ms
Partition 1 [5, 9) has cost: 333.182 ms
Partition 2 [9, 13) has cost: 333.182 ms
Partition 3 [13, 17) has cost: 333.182 ms
Partition 4 [17, 21) has cost: 333.182 ms
Partition 5 [21, 25) has cost: 333.182 ms
Partition 6 [25, 29) has cost: 333.182 ms
Partition 7 [29, 33) has cost: 336.132 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 306.784 ms
GPU 0, Compute+Comm Time: 126.369 ms, Bubble Time: 131.869 ms, Imbalance Overhead: 48.545 ms
GPU 1, Compute+Comm Time: 121.262 ms, Bubble Time: 122.483 ms, Imbalance Overhead: 63.038 ms
GPU 2, Compute+Comm Time: 121.262 ms, Bubble Time: 112.438 ms, Imbalance Overhead: 73.083 ms
GPU 3, Compute+Comm Time: 121.262 ms, Bubble Time: 113.101 ms, Imbalance Overhead: 72.420 ms
GPU 4, Compute+Comm Time: 121.262 ms, Bubble Time: 122.703 ms, Imbalance Overhead: 62.818 ms
GPU 5, Compute+Comm Time: 121.262 ms, Bubble Time: 131.732 ms, Imbalance Overhead: 53.789 ms
GPU 6, Compute+Comm Time: 121.262 ms, Bubble Time: 140.679 ms, Imbalance Overhead: 44.843 ms
GPU 7, Compute+Comm Time: 121.906 ms, Bubble Time: 150.777 ms, Imbalance Overhead: 34.100 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 601.831 ms
GPU 0, Compute+Comm Time: 237.938 ms, Bubble Time: 297.330 ms, Imbalance Overhead: 66.563 ms
GPU 1, Compute+Comm Time: 235.632 ms, Bubble Time: 278.141 ms, Imbalance Overhead: 88.057 ms
GPU 2, Compute+Comm Time: 235.632 ms, Bubble Time: 260.314 ms, Imbalance Overhead: 105.884 ms
GPU 3, Compute+Comm Time: 235.632 ms, Bubble Time: 242.244 ms, Imbalance Overhead: 123.955 ms
GPU 4, Compute+Comm Time: 235.632 ms, Bubble Time: 222.382 ms, Imbalance Overhead: 143.816 ms
GPU 5, Compute+Comm Time: 235.632 ms, Bubble Time: 220.558 ms, Imbalance Overhead: 145.640 ms
GPU 6, Compute+Comm Time: 235.632 ms, Bubble Time: 239.989 ms, Imbalance Overhead: 126.210 ms
GPU 7, Compute+Comm Time: 250.922 ms, Bubble Time: 257.330 ms, Imbalance Overhead: 93.579 ms
The estimated cost of the whole pipeline: 954.045 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 686.761 ms
Partition 0 [0, 9) has cost: 686.761 ms
Partition 1 [9, 17) has cost: 666.365 ms
Partition 2 [17, 25) has cost: 666.365 ms
Partition 3 [25, 33) has cost: 669.314 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 299.373 ms
GPU 0, Compute+Comm Time: 145.600 ms, Bubble Time: 115.768 ms, Imbalance Overhead: 38.006 ms
GPU 1, Compute+Comm Time: 143.044 ms, Bubble Time: 96.339 ms, Imbalance Overhead: 59.990 ms
GPU 2, Compute+Comm Time: 143.044 ms, Bubble Time: 115.039 ms, Imbalance Overhead: 41.289 ms
GPU 3, Compute+Comm Time: 143.236 ms, Bubble Time: 133.015 ms, Imbalance Overhead: 23.122 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 570.100 ms
GPU 0, Compute+Comm Time: 271.098 ms, Bubble Time: 255.314 ms, Imbalance Overhead: 43.688 ms
GPU 1, Compute+Comm Time: 270.192 ms, Bubble Time: 219.416 ms, Imbalance Overhead: 80.491 ms
GPU 2, Compute+Comm Time: 270.192 ms, Bubble Time: 181.358 ms, Imbalance Overhead: 118.550 ms
GPU 3, Compute+Comm Time: 277.840 ms, Bubble Time: 218.130 ms, Imbalance Overhead: 74.130 ms
    The estimated cost with 2 DP ways is 912.947 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1353.126 ms
Partition 0 [0, 17) has cost: 1353.126 ms
Partition 1 [17, 33) has cost: 1335.679 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 313.200 ms
GPU 0, Compute+Comm Time: 197.540 ms, Bubble Time: 80.152 ms, Imbalance Overhead: 35.508 ms
GPU 1, Compute+Comm Time: 196.372 ms, Bubble Time: 116.828 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 555.039 ms
GPU 0, Compute+Comm Time: 344.801 ms, Bubble Time: 210.238 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 348.199 ms, Bubble Time: 136.282 ms, Imbalance Overhead: 70.558 ms
    The estimated cost with 4 DP ways is 911.651 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2688.805 ms
Partition 0 [0, 33) has cost: 2688.805 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 408.325 ms
GPU 0, Compute+Comm Time: 408.325 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 594.036 ms
GPU 0, Compute+Comm Time: 594.036 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1052.479 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.3436
	Epoch 50:	Loss 1.7027
	Epoch 75:	Loss 1.3613
	Epoch 100:	Loss 1.1577
	Epoch 125:	Loss 1.0257
	Epoch 150:	Loss 0.9260
	Epoch 175:	Loss 0.8569
	Epoch 200:	Loss 0.8007
	Epoch 225:	Loss 0.7572
	Epoch 250:	Loss 0.7271
	Epoch 275:	Loss 0.6984
	Epoch 300:	Loss 0.6711
	Epoch 325:	Loss 0.6506
	Epoch 350:	Loss 0.6364
	Epoch 375:	Loss 0.6198
	Epoch 400:	Loss 0.6089
	Epoch 425:	Loss 0.5995
	Epoch 450:	Loss 0.5826
	Epoch 475:	Loss 0.5747
	Epoch 500:	Loss 0.5662
	Epoch 525:	Loss 0.5593
	Epoch 550:	Loss 0.5485
	Epoch 575:	Loss 0.5412
	Epoch 600:	Loss 0.5381
	Epoch 625:	Loss 0.5314
	Epoch 650:	Loss 0.5286
	Epoch 675:	Loss 0.5180
	Epoch 700:	Loss 0.5166
	Epoch 725:	Loss 0.5141
	Epoch 750:	Loss 0.5062
	Epoch 775:	Loss 0.5048
	Epoch 800:	Loss 0.5014
	Epoch 825:	Loss 0.4989
	Epoch 850:	Loss 0.4926
	Epoch 875:	Loss 0.4920
	Epoch 900:	Loss 0.4885
	Epoch 925:	Loss 0.4858
	Epoch 950:	Loss 0.4826
	Epoch 975:	Loss 0.4761
	Epoch 1000:	Loss 0.4761
	Epoch 1025:	Loss 0.4794
	Epoch 1050:	Loss 0.4734
	Epoch 1075:	Loss 0.4712
	Epoch 1100:	Loss 0.4621
	Epoch 1125:	Loss 0.4608
	Epoch 1150:	Loss 0.4585
	Epoch 1175:	Loss 0.4602
	Epoch 1200:	Loss 0.4630
	Epoch 1225:	Loss 0.4716
	Epoch 1250:	Loss 0.4534
	Epoch 1275:	Loss 0.4504
	Epoch 1300:	Loss 0.4504
	Epoch 1325:	Loss 0.4489
	Epoch 1350:	Loss 0.4473
	Epoch 1375:	Loss 0.4439
	Epoch 1400:	Loss 0.4430
	Epoch 1425:	Loss 0.4403
	Epoch 1450:	Loss 0.4392
	Epoch 1475:	Loss 0.4390
	Epoch 1500:	Loss 0.4368
	Epoch 1525:	Loss 0.4352
	Epoch 1550:	Loss 0.4335
	Epoch 1575:	Loss 0.4324
	Epoch 1600:	Loss 0.4335
	Epoch 1625:	Loss 0.4284
	Epoch 1650:	Loss 0.4297
	Epoch 1675:	Loss 0.4285
	Epoch 1700:	Loss 0.4256
	Epoch 1725:	Loss 0.4291
	Epoch 1750:	Loss 0.4313
	Epoch 1775:	Loss 0.4258
	Epoch 1800:	Loss 0.4226
	Epoch 1825:	Loss 0.4239
	Epoch 1850:	Loss 0.4301
	Epoch 1875:	Loss 0.4165
	Epoch 1900:	Loss 0.4173
	Epoch 1925:	Loss 0.4191
	Epoch 1950:	Loss 0.4153
	Epoch 1975:	Loss 0.4151
	Epoch 2000:	Loss 0.4151
	Epoch 2025:	Loss 0.4149
	Epoch 2050:	Loss 0.4112
	Epoch 2075:	Loss 0.4128
	Epoch 2100:	Loss 0.4102
	Epoch 2125:	Loss 0.4104
	Epoch 2150:	Loss 0.4095
	Epoch 2175:	Loss 0.4120
	Epoch 2200:	Loss 0.4096
	Epoch 2225:	Loss 0.4067
	Epoch 2250:	Loss 0.4103
	Epoch 2275:	Loss 0.4035
	Epoch 2300:	Loss 0.4071
	Epoch 2325:	Loss 0.4032
	Epoch 2350:	Loss 0.4042
	Epoch 2375:	Loss 0.4043
	Epoch 2400:	Loss 0.4014
	Epoch 2425:	Loss 0.3996
	Epoch 2450:	Loss 0.4031
	Epoch 2475:	Loss 0.3995
	Epoch 2500:	Loss 0.3993
	Epoch 2525:	Loss 0.3979
	Epoch 2550:	Loss 0.4080
	Epoch 2575:	Loss 0.3976
	Epoch 2600:	Loss 0.3948
	Epoch 2625:	Loss 0.3951
	Epoch 2650:	Loss 0.3955
	Epoch 2675:	Loss 0.3944
	Epoch 2700:	Loss 0.3947
	Epoch 2725:	Loss 0.3941
	Epoch 2750:	Loss 0.3917
	Epoch 2775:	Loss 0.4031
	Epoch 2800:	Loss 0.3928
	Epoch 2825:	Loss 0.3957
	Epoch 2850:	Loss 0.3885
	Epoch 2875:	Loss 0.3929
	Epoch 2900:	Loss 0.3879
	Epoch 2925:	Loss 0.3920
	Epoch 2950:	Loss 0.3934
	Epoch 2975:	Loss 0.3876
	Epoch 3000:	Loss 0.3932
	Epoch 3025:	Loss 0.3897
	Epoch 3050:	Loss 0.3840
	Epoch 3075:	Loss 0.3838
	Epoch 3100:	Loss 0.3850
	Epoch 3125:	Loss 0.3901
	Epoch 3150:	Loss 0.3845
	Epoch 3175:	Loss 0.3827
	Epoch 3200:	Loss 0.3845
	Epoch 3225:	Loss 0.3795
	Epoch 3250:	Loss 0.3789
	Epoch 3275:	Loss 0.3877
	Epoch 3300:	Loss 0.3857
	Epoch 3325:	Loss 0.3807
	Epoch 3350:	Loss 0.3802
	Epoch 3375:	Loss 0.3822
	Epoch 3400:	Loss 0.3799
	Epoch 3425:	Loss 0.3837
	Epoch 3450:	Loss 0.3800
	Epoch 3475:	Loss 0.3833
	Epoch 3500:	Loss 0.3795
	Epoch 3525:	Loss 0.3920
	Epoch 3550:	Loss 0.3826
	Epoch 3575:	Loss 0.3766
	Epoch 3600:	Loss 0.4053
	Epoch 3625:	Loss 0.3755
	Epoch 3650:	Loss 0.3792
	Epoch 3675:	Loss 0.3758
	Epoch 3700:	Loss 0.3860
	Epoch 3725:	Loss 0.3748
	Epoch 3750:	Loss 0.3747
	Epoch 3775:	Loss 0.3746
	Epoch 3800:	Loss 0.3759
	Epoch 3825:	Loss 0.3753
	Epoch 3850:	Loss 0.3739
	Epoch 3875:	Loss 0.3760
	Epoch 3900:	Loss 0.3727
	Epoch 3925:	Loss 0.3718
	Epoch 3950:	Loss 0.3713
	Epoch 3975:	Loss 0.3728
	Epoch 4000:	Loss 0.3711
	Epoch 4025:	Loss 0.3738
	Epoch 4050:	Loss 0.3724
	Epoch 4075:	Loss 0.3739
	Epoch 4100:	Loss 0.3741
	Epoch 4125:	Loss 0.3716
	Epoch 4150:	Loss 0.3695
	Epoch 4175:	Loss 0.3728
	Epoch 4200:	Loss 0.3672
	Epoch 4225:	Loss 0.3690
	Epoch 4250:	Loss 0.3674
	Epoch 4275:	Loss 0.3768
	Epoch 4300:	Loss 0.3692
	Epoch 4325:	Loss 0.3718
	Epoch 4350:	Loss 0.3672
	Epoch 4375:	Loss 0.3701
	Epoch 4400:	Loss 0.3678
	Epoch 4425:	Loss 0.3649
	Epoch 4450:	Loss 0.3678
	Epoch 4475:	Loss 0.3727
	Epoch 4500:	Loss 0.3619
	Epoch 4525:	Loss 0.3655
	Epoch 4550:	Loss 0.3694
	Epoch 4575:	Loss 0.3644
	Epoch 4600:	Loss 0.3681
	Epoch 4625:	Loss 0.3653
	Epoch 4650:	Loss 0.3646
	Epoch 4675:	Loss 0.3612
	Epoch 4700:	Loss 0.3633
	Epoch 4725:	Loss 0.3635
	Epoch 4750:	Loss 0.3644
	Epoch 4775:	Loss 0.3633
	Epoch 4800:	Loss 0.3592
	Epoch 4825:	Loss 0.3612
	Epoch 4850:	Loss 0.3624
	Epoch 4875:	Loss 0.3665
	Epoch 4900:	Loss 0.3584
	Epoch 4925:	Loss 0.3642
	Epoch 4950:	Loss 0.3712
	Epoch 4975:	Loss 0.3623
Node 6, Pre/Post-Pipelining: 8.588 / 17.992 ms, Bubble: 0.774 ms, Compute: 886.637 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
	Epoch 5000:	Loss 0.3628
Node 0, Pre/Post-Pipelining: 8.584 / 18.010 ms, Bubble: 1.237 ms, Compute: 886.154 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 4, Pre/Post-Pipelining: 8.587 / 17.982 ms, Bubble: 1.361 ms, Compute: 886.056 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 8.582 / 17.966 ms, Bubble: 1.159 ms, Compute: 886.283 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 3, Pre/Post-Pipelining: 8.593 / 18.045 ms, Bubble: 0.052 ms, Compute: 887.298 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 2, Pre/Post-Pipelining: 8.595 / 18.062 ms, Bubble: 0.432 ms, Compute: 886.892 ms, Comm: 0.012 ms, Imbalance: 0.018 ms
Node 5, Pre/Post-Pipelining: 8.585 / 18.005 ms, Bubble: 1.165 ms, Compute: 886.230 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 1, Pre/Post-Pipelining: 8.586 / 18.026 ms, Bubble: 1.124 ms, Compute: 886.239 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.584 ms
Cluster-Wide Average, Post-Pipelining Overhead: 18.010 ms
Cluster-Wide Average, Bubble: 1.237 ms
Cluster-Wide Average, Compute: 886.154 ms
Cluster-Wide Average, Communication: 0.010 ms
Cluster-Wide Average, Imbalance: 0.017 ms
Node 0, GPU memory consumption: 22.329 GB
Node 1, GPU memory consumption: 21.518 GB
Node 2, GPU memory consumption: 21.526 GB
Node 3, GPU memory consumption: 21.505 GB
Node 6, GPU memory consumption: 21.522 GB
Node 7, GPU memory consumption: 21.499 GB
Node 4, GPU memory consumption: 21.495 GB
Node 5, GPU memory consumption: 21.518 GB
Node 0, Graph-Level Communication Throughput: 25.450 Gbps, Time: 627.325 ms
Node 4, Graph-Level Communication Throughput: 11.461 Gbps, Time: 734.245 ms
Node 1, Graph-Level Communication Throughput: 23.736 Gbps, Time: 715.429 ms
Node 5, Graph-Level Communication Throughput: 19.086 Gbps, Time: 639.882 ms
Node 2, Graph-Level Communication Throughput: 36.260 Gbps, Time: 487.764 ms
Node 6, Graph-Level Communication Throughput: 26.387 Gbps, Time: 620.786 ms
Node 3, Graph-Level Communication Throughput: 49.477 Gbps, Time: 467.757 ms
Node 7, Graph-Level Communication Throughput: 21.389 Gbps, Time: 636.539 ms
------------------------node id 0,  per-epoch time: 0.914060 s---------------
------------------------node id 1,  per-epoch time: 0.914060 s---------------
------------------------node id 2,  per-epoch time: 0.914060 s---------------
------------------------node id 3,  per-epoch time: 0.914060 s---------------
------------------------node id 4,  per-epoch time: 0.914060 s---------------
------------------------node id 5,  per-epoch time: 0.914060 s---------------
------------------------node id 6,  per-epoch time: 0.914060 s---------------
------------------------node id 7,  per-epoch time: 0.914060 s---------------
************ Profiling Results ************
	Bubble: 27.860567 (ms) (3.05 percentage)
	Compute: 230.765282 (ms) (25.25 percentage)
	GraphCommComputeOverhead: 19.235789 (ms) (2.10 percentage)
	GraphCommNetwork: 616.214782 (ms) (67.41 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.999725 (ms) (2.19 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 14.493 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
