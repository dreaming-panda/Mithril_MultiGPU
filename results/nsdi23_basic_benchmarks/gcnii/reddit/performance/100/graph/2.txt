Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.845 seconds.
Building the CSC structure...
        It takes 2.032 seconds.
Building the CSC structure...
        It takes 2.398 seconds.
Building the CSC structure...
        It takes 2.453 seconds.
Building the CSC structure...
        It takes 2.618 seconds.
Building the CSC structure...
        It takes 2.627 seconds.
Building the CSC structure...
        It takes 2.678 seconds.
Building the CSC structure...
        It takes 2.709 seconds.
Building the CSC structure...
        It takes 1.851 seconds.
        It takes 1.872 seconds.
Building the Feature Vector...
        It takes 2.366 seconds.
        It takes 2.358 seconds.
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 2.321 seconds.
        It takes 2.342 seconds.
        It takes 0.036 seconds.
Building the Feature Vector...
        It takes 2.347 seconds.
        It takes 2.370 seconds.
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 33)
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.325 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.030 seconds.
        It takes 0.246 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 62.977 Gbps (per GPU), 503.819 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.934 Gbps (per GPU), 503.473 Gbps (aggregated)
The layer-level communication performance: 62.932 Gbps (per GPU), 503.457 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.897 Gbps (per GPU), 503.174 Gbps (aggregated)
The layer-level communication performance: 62.891 Gbps (per GPU), 503.126 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.864 Gbps (per GPU), 502.913 Gbps (aggregated)
The layer-level communication performance: 62.856 Gbps (per GPU), 502.849 Gbps (aggregated)
The layer-level communication performance: 62.851 Gbps (per GPU), 502.805 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.949 Gbps (per GPU), 1327.593 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.947 Gbps (per GPU), 1327.574 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.942 Gbps (per GPU), 1327.535 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.945 Gbps (per GPU), 1327.561 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.945 Gbps (per GPU), 1327.561 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.945 Gbps (per GPU), 1327.561 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.947 Gbps (per GPU), 1327.574 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.945 Gbps (per GPU), 1327.558 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.907 Gbps (per GPU), 919.255 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.907 Gbps (per GPU), 919.254 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.907 Gbps (per GPU), 919.256 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.907 Gbps (per GPU), 919.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.907 Gbps (per GPU), 919.254 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.906 Gbps (per GPU), 919.250 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.906 Gbps (per GPU), 919.248 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.907 Gbps (per GPU), 919.254 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.212 Gbps (per GPU), 361.695 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.212 Gbps (per GPU), 361.696 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.212 Gbps (per GPU), 361.695 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.212 Gbps (per GPU), 361.696 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.212 Gbps (per GPU), 361.693 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.212 Gbps (per GPU), 361.696 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.212 Gbps (per GPU), 361.694 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.212 Gbps (per GPU), 361.695 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.54ms  9.75ms 10.10ms  3.98 29.12K 14.23M
 chk_1  2.55ms  5.27ms  5.62ms  2.21 29.12K  6.56M
 chk_2  2.55ms 17.09ms 17.54ms  6.88 29.12K 24.68M
 chk_3  2.56ms 17.05ms 17.42ms  6.81 29.12K 22.95M
 chk_4  2.56ms  5.09ms  5.49ms  2.14 29.12K  6.33M
 chk_5  2.57ms  9.47ms  9.68ms  3.77 29.12K 12.05M
 chk_6  2.57ms 10.47ms 10.83ms  4.22 29.12K 14.60M
 chk_7  2.57ms  9.56ms 10.10ms  3.94 29.12K 13.21M
   Avg  2.56 10.47 10.85
   Max  2.57 17.09 17.54
   Min  2.54  5.09  5.49
 Ratio  1.01  3.36  3.20
   Var  0.00 18.19 18.30
Profiling takes 2.216 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 355.506 ms
Partition 0 [0, 5) has cost: 355.506 ms
Partition 1 [5, 9) has cost: 335.046 ms
Partition 2 [9, 13) has cost: 335.046 ms
Partition 3 [13, 17) has cost: 335.046 ms
Partition 4 [17, 21) has cost: 335.046 ms
Partition 5 [21, 25) has cost: 335.046 ms
Partition 6 [25, 29) has cost: 335.046 ms
Partition 7 [29, 33) has cost: 338.064 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 307.988 ms
GPU 0, Compute+Comm Time: 127.234 ms, Bubble Time: 133.031 ms, Imbalance Overhead: 47.724 ms
GPU 1, Compute+Comm Time: 122.113 ms, Bubble Time: 123.675 ms, Imbalance Overhead: 62.201 ms
GPU 2, Compute+Comm Time: 122.113 ms, Bubble Time: 113.555 ms, Imbalance Overhead: 72.320 ms
GPU 3, Compute+Comm Time: 122.113 ms, Bubble Time: 114.146 ms, Imbalance Overhead: 71.730 ms
GPU 4, Compute+Comm Time: 122.113 ms, Bubble Time: 123.482 ms, Imbalance Overhead: 62.394 ms
GPU 5, Compute+Comm Time: 122.113 ms, Bubble Time: 132.210 ms, Imbalance Overhead: 53.666 ms
GPU 6, Compute+Comm Time: 122.113 ms, Bubble Time: 140.861 ms, Imbalance Overhead: 45.014 ms
GPU 7, Compute+Comm Time: 122.799 ms, Bubble Time: 150.826 ms, Imbalance Overhead: 34.364 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 602.668 ms
GPU 0, Compute+Comm Time: 238.940 ms, Bubble Time: 297.243 ms, Imbalance Overhead: 66.486 ms
GPU 1, Compute+Comm Time: 236.608 ms, Bubble Time: 277.619 ms, Imbalance Overhead: 88.441 ms
GPU 2, Compute+Comm Time: 236.608 ms, Bubble Time: 260.126 ms, Imbalance Overhead: 105.935 ms
GPU 3, Compute+Comm Time: 236.608 ms, Bubble Time: 242.350 ms, Imbalance Overhead: 123.711 ms
GPU 4, Compute+Comm Time: 236.608 ms, Bubble Time: 222.784 ms, Imbalance Overhead: 143.277 ms
GPU 5, Compute+Comm Time: 236.608 ms, Bubble Time: 221.028 ms, Imbalance Overhead: 145.033 ms
GPU 6, Compute+Comm Time: 236.608 ms, Bubble Time: 240.734 ms, Imbalance Overhead: 125.326 ms
GPU 7, Compute+Comm Time: 251.946 ms, Bubble Time: 258.222 ms, Imbalance Overhead: 92.500 ms
The estimated cost of the whole pipeline: 956.190 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 690.552 ms
Partition 0 [0, 9) has cost: 690.552 ms
Partition 1 [9, 17) has cost: 670.092 ms
Partition 2 [17, 25) has cost: 670.092 ms
Partition 3 [25, 33) has cost: 673.110 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 300.171 ms
GPU 0, Compute+Comm Time: 146.645 ms, Bubble Time: 116.826 ms, Imbalance Overhead: 36.699 ms
GPU 1, Compute+Comm Time: 144.078 ms, Bubble Time: 97.360 ms, Imbalance Overhead: 58.733 ms
GPU 2, Compute+Comm Time: 144.078 ms, Bubble Time: 115.456 ms, Imbalance Overhead: 40.638 ms
GPU 3, Compute+Comm Time: 144.335 ms, Bubble Time: 132.835 ms, Imbalance Overhead: 23.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 571.103 ms
GPU 0, Compute+Comm Time: 272.959 ms, Bubble Time: 255.000 ms, Imbalance Overhead: 43.144 ms
GPU 1, Compute+Comm Time: 271.851 ms, Bubble Time: 219.731 ms, Imbalance Overhead: 79.521 ms
GPU 2, Compute+Comm Time: 271.851 ms, Bubble Time: 182.260 ms, Imbalance Overhead: 116.992 ms
GPU 3, Compute+Comm Time: 279.527 ms, Bubble Time: 219.450 ms, Imbalance Overhead: 72.126 ms
    The estimated cost with 2 DP ways is 914.837 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1360.644 ms
Partition 0 [0, 17) has cost: 1360.644 ms
Partition 1 [17, 33) has cost: 1343.202 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 313.908 ms
GPU 0, Compute+Comm Time: 198.353 ms, Bubble Time: 81.154 ms, Imbalance Overhead: 34.400 ms
GPU 1, Compute+Comm Time: 197.278 ms, Bubble Time: 116.629 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 556.241 ms
GPU 0, Compute+Comm Time: 346.045 ms, Bubble Time: 210.196 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 349.294 ms, Bubble Time: 137.456 ms, Imbalance Overhead: 69.490 ms
    The estimated cost with 4 DP ways is 913.656 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2703.846 ms
Partition 0 [0, 33) has cost: 2703.846 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 412.841 ms
GPU 0, Compute+Comm Time: 412.841 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 598.871 ms
GPU 0, Compute+Comm Time: 598.871 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1062.298 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.3223
	Epoch 50:	Loss 1.7024
	Epoch 75:	Loss 1.3715
	Epoch 100:	Loss 1.1726
	Epoch 125:	Loss 1.0379
	Epoch 150:	Loss 0.9396
	Epoch 175:	Loss 0.8656
	Epoch 200:	Loss 0.8128
	Epoch 225:	Loss 0.7695
	Epoch 250:	Loss 0.7340
	Epoch 275:	Loss 0.7043
	Epoch 300:	Loss 0.6805
	Epoch 325:	Loss 0.6651
	Epoch 350:	Loss 0.6418
	Epoch 375:	Loss 0.6262
	Epoch 400:	Loss 0.6113
	Epoch 425:	Loss 0.5995
	Epoch 450:	Loss 0.5885
	Epoch 475:	Loss 0.5819
	Epoch 500:	Loss 0.5697
	Epoch 525:	Loss 0.5603
	Epoch 550:	Loss 0.5573
	Epoch 575:	Loss 0.5478
	Epoch 600:	Loss 0.5404
	Epoch 625:	Loss 0.5403
	Epoch 650:	Loss 0.5283
	Epoch 675:	Loss 0.5254
	Epoch 700:	Loss 0.5220
	Epoch 725:	Loss 0.5169
	Epoch 750:	Loss 0.5115
	Epoch 775:	Loss 0.5127
	Epoch 800:	Loss 0.5036
	Epoch 825:	Loss 0.5081
	Epoch 850:	Loss 0.4974
	Epoch 875:	Loss 0.4945
	Epoch 900:	Loss 0.4962
	Epoch 925:	Loss 0.4875
	Epoch 950:	Loss 0.4842
	Epoch 975:	Loss 0.4885
	Epoch 1000:	Loss 0.4763
	Epoch 1025:	Loss 0.4858
	Epoch 1050:	Loss 0.4800
	Epoch 1075:	Loss 0.4752
	Epoch 1100:	Loss 0.4703
	Epoch 1125:	Loss 0.4673
	Epoch 1150:	Loss 0.4647
	Epoch 1175:	Loss 0.4635
	Epoch 1200:	Loss 0.4593
	Epoch 1225:	Loss 0.4607
	Epoch 1250:	Loss 0.4587
	Epoch 1275:	Loss 0.4629
	Epoch 1300:	Loss 0.4529
	Epoch 1325:	Loss 0.4496
	Epoch 1350:	Loss 0.4484
	Epoch 1375:	Loss 0.4489
	Epoch 1400:	Loss 0.4504
	Epoch 1425:	Loss 0.4505
	Epoch 1450:	Loss 0.4548
	Epoch 1475:	Loss 0.4475
	Epoch 1500:	Loss 0.4376
	Epoch 1525:	Loss 0.4464
	Epoch 1550:	Loss 0.4405
	Epoch 1575:	Loss 0.4392
	Epoch 1600:	Loss 0.4381
	Epoch 1625:	Loss 0.4364
	Epoch 1650:	Loss 0.4434
	Epoch 1675:	Loss 0.4323
	Epoch 1700:	Loss 0.4323
	Epoch 1725:	Loss 0.4298
	Epoch 1750:	Loss 0.4301
	Epoch 1775:	Loss 0.4294
	Epoch 1800:	Loss 0.4307
	Epoch 1825:	Loss 0.4287
	Epoch 1850:	Loss 0.4342
	Epoch 1875:	Loss 0.4223
	Epoch 1900:	Loss 0.4245
	Epoch 1925:	Loss 0.4246
	Epoch 1950:	Loss 0.4222
	Epoch 1975:	Loss 0.4199
	Epoch 2000:	Loss 0.4377
	Epoch 2025:	Loss 0.4236
	Epoch 2050:	Loss 0.4221
	Epoch 2075:	Loss 0.4210
	Epoch 2100:	Loss 0.4325
	Epoch 2125:	Loss 0.4171
	Epoch 2150:	Loss 0.4162
	Epoch 2175:	Loss 0.4146
	Epoch 2200:	Loss 0.4165
	Epoch 2225:	Loss 0.4172
	Epoch 2250:	Loss 0.4129
	Epoch 2275:	Loss 0.4196
	Epoch 2300:	Loss 0.4111
	Epoch 2325:	Loss 0.4064
	Epoch 2350:	Loss 0.4112
	Epoch 2375:	Loss 0.4084
	Epoch 2400:	Loss 0.4075
	Epoch 2425:	Loss 0.4104
	Epoch 2450:	Loss 0.4151
	Epoch 2475:	Loss 0.4066
	Epoch 2500:	Loss 0.4175
	Epoch 2525:	Loss 0.4012
	Epoch 2550:	Loss 0.4051
	Epoch 2575:	Loss 0.4045
	Epoch 2600:	Loss 0.4071
	Epoch 2625:	Loss 0.4047
	Epoch 2650:	Loss 0.4094
	Epoch 2675:	Loss 0.4053
	Epoch 2700:	Loss 0.4085
	Epoch 2725:	Loss 0.3989
	Epoch 2750:	Loss 0.3990
	Epoch 2775:	Loss 0.3981
	Epoch 2800:	Loss 0.4030
	Epoch 2825:	Loss 0.4002
	Epoch 2850:	Loss 0.3979
	Epoch 2875:	Loss 0.3976
	Epoch 2900:	Loss 0.3988
	Epoch 2925:	Loss 0.3949
	Epoch 2950:	Loss 0.3941
	Epoch 2975:	Loss 0.3932
	Epoch 3000:	Loss 0.3949
	Epoch 3025:	Loss 0.3921
	Epoch 3050:	Loss 0.3942
	Epoch 3075:	Loss 0.3914
	Epoch 3100:	Loss 0.3895
	Epoch 3125:	Loss 0.3962
	Epoch 3150:	Loss 0.3900
	Epoch 3175:	Loss 0.3909
	Epoch 3200:	Loss 0.3886
	Epoch 3225:	Loss 0.3881
	Epoch 3250:	Loss 0.3866
	Epoch 3275:	Loss 0.3918
	Epoch 3300:	Loss 0.3864
	Epoch 3325:	Loss 0.3904
	Epoch 3350:	Loss 0.3846
	Epoch 3375:	Loss 0.3862
	Epoch 3400:	Loss 0.3876
	Epoch 3425:	Loss 0.3871
	Epoch 3450:	Loss 0.3831
	Epoch 3475:	Loss 0.3844
	Epoch 3500:	Loss 0.3809
	Epoch 3525:	Loss 0.3861
	Epoch 3550:	Loss 0.3842
	Epoch 3575:	Loss 0.3841
	Epoch 3600:	Loss 0.3836
	Epoch 3625:	Loss 0.3849
	Epoch 3650:	Loss 0.3834
	Epoch 3675:	Loss 0.3825
	Epoch 3700:	Loss 0.3863
	Epoch 3725:	Loss 0.3806
	Epoch 3750:	Loss 0.3796
	Epoch 3775:	Loss 0.3818
	Epoch 3800:	Loss 0.3802
	Epoch 3825:	Loss 0.3793
	Epoch 3850:	Loss 0.3794
	Epoch 3875:	Loss 0.3833
	Epoch 3900:	Loss 0.3766
	Epoch 3925:	Loss 0.3785
	Epoch 3950:	Loss 0.3801
	Epoch 3975:	Loss 0.3768
	Epoch 4000:	Loss 0.3760
	Epoch 4025:	Loss 0.3899
	Epoch 4050:	Loss 0.3779
	Epoch 4075:	Loss 0.3769
	Epoch 4100:	Loss 0.3745
	Epoch 4125:	Loss 0.3874
	Epoch 4150:	Loss 0.3806
	Epoch 4175:	Loss 0.3757
	Epoch 4200:	Loss 0.3725
	Epoch 4225:	Loss 0.3730
	Epoch 4250:	Loss 0.3731
	Epoch 4275:	Loss 0.3737
	Epoch 4300:	Loss 0.3713
	Epoch 4325:	Loss 0.3702
	Epoch 4350:	Loss 0.3741
	Epoch 4375:	Loss 0.3704
	Epoch 4400:	Loss 0.3716
	Epoch 4425:	Loss 0.3691
	Epoch 4450:	Loss 0.3700
	Epoch 4475:	Loss 0.3718
	Epoch 4500:	Loss 0.3711
	Epoch 4525:	Loss 0.3768
	Epoch 4550:	Loss 0.3763
	Epoch 4575:	Loss 0.3740
	Epoch 4600:	Loss 0.3677
	Epoch 4625:	Loss 0.3706
	Epoch 4650:	Loss 0.3719
	Epoch 4675:	Loss 0.3717
	Epoch 4700:	Loss 0.3665
	Epoch 4725:	Loss 0.3685
	Epoch 4750:	Loss 0.3779
	Epoch 4775:	Loss 0.3656
	Epoch 4800:	Loss 0.3756
	Epoch 4825:	Loss 0.3673
	Epoch 4850:	Loss 0.3625
	Epoch 4875:	Loss 0.3670
	Epoch 4900:	Loss 0.3670
	Epoch 4925:	Loss 0.3642
	Epoch 4950:	Loss 0.3687
	Epoch 4975:	Loss 0.3661
	Epoch 5000:	Loss 0.3672
Node 0, Pre/Post-Pipelining: 8.584 / 17.834 ms, Bubble: 1.294 ms, Compute: 885.942 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 8.584 / 17.823 ms, Bubble: 1.285 ms, Compute: 885.960 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 1, Pre/Post-Pipelining: 8.586 / 17.809 ms, Bubble: 1.082 ms, Compute: 886.172 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 2, Pre/Post-Pipelining: 8.592 / 17.844 ms, Bubble: 0.470 ms, Compute: 886.749 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 4, Pre/Post-Pipelining: 8.588 / 17.817 ms, Bubble: 1.401 ms, Compute: 885.848 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 5, Pre/Post-Pipelining: 8.585 / 17.815 ms, Bubble: 1.221 ms, Compute: 886.033 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 8.591 / 17.850 ms, Bubble: 0.788 ms, Compute: 886.426 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 3, Pre/Post-Pipelining: 8.596 / 17.931 ms, Bubble: 0.051 ms, Compute: 887.067 ms, Comm: 0.011 ms, Imbalance: 0.020 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.584 ms
Cluster-Wide Average, Post-Pipelining Overhead: 17.834 ms
Cluster-Wide Average, Bubble: 1.294 ms
Cluster-Wide Average, Compute: 885.942 ms
Cluster-Wide Average, Communication: 0.010 ms
Cluster-Wide Average, Imbalance: 0.017 ms
Node 0, GPU memory consumption: 22.329 GB
Node 2, GPU memory consumption: 21.526 GB
Node 1, GPU memory consumption: 21.518 GB
Node 3, GPU memory consumption: 21.505 GB
Node 4, GPU memory consumption: 21.495 GB
Node 5, GPU memory consumption: 21.518 GB
Node 6, GPU memory consumption: 21.522 GB
Node 7, GPU memory consumption: 21.499 GB
Node 0, Graph-Level Communication Throughput: 25.455 Gbps, Time: 627.198 ms
Node 1, Graph-Level Communication Throughput: 23.722 Gbps, Time: 715.849 ms
Node 2, Graph-Level Communication Throughput: 36.240 Gbps, Time: 488.033 ms
Node 3, Graph-Level Communication Throughput: 49.559 Gbps, Time: 466.979 ms
Node 4, Graph-Level Communication Throughput: 11.468 Gbps, Time: 733.770 ms
Node 5, Graph-Level Communication Throughput: 19.081 Gbps, Time: 640.054 ms
Node 6, Graph-Level Communication Throughput: 26.429 Gbps, Time: 619.806 ms
Node 7, Graph-Level Communication Throughput: 21.443 Gbps, Time: 634.936 ms
------------------------node id 0,  per-epoch time: 0.913729 s---------------
------------------------node id 1,  per-epoch time: 0.913729 s---------------
------------------------node id 2,  per-epoch time: 0.913729 s---------------
------------------------node id 3,  per-epoch time: 0.913729 s---------------
------------------------node id 4,  per-epoch time: 0.913729 s---------------
------------------------node id 5,  per-epoch time: 0.913729 s---------------
------------------------node id 6,  per-epoch time: 0.913729 s---------------
------------------------node id 7,  per-epoch time: 0.913729 s---------------
************ Profiling Results ************
	Bubble: 27.736730 (ms) (3.04 percentage)
	Compute: 230.908689 (ms) (25.27 percentage)
	GraphCommComputeOverhead: 19.247833 (ms) (2.11 percentage)
	GraphCommNetwork: 615.827008 (ms) (67.40 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 20.024576 (ms) (2.19 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 14.493 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
