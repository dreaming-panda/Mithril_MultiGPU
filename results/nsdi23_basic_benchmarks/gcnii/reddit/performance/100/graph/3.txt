Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.021 seconds.
Building the CSC structure...
        It takes 2.044 seconds.
Building the CSC structure...
        It takes 2.332 seconds.
Building the CSC structure...
        It takes 2.376 seconds.
Building the CSC structure...
        It takes 2.397 seconds.
Building the CSC structure...
        It takes 2.408 seconds.
Building the CSC structure...
        It takes 2.645 seconds.
Building the CSC structure...
        It takes 2.704 seconds.
Building the CSC structure...
        It takes 1.869 seconds.
        It takes 1.873 seconds.
        It takes 2.240 seconds.
        It takes 2.301 seconds.
        It takes 2.323 seconds.
        It takes 2.339 seconds.
        It takes 2.346 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.350 seconds.
        It takes 0.253 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.295 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 64.341 Gbps (per GPU), 514.727 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.299 Gbps (per GPU), 514.388 Gbps (aggregated)
The layer-level communication performance: 64.295 Gbps (per GPU), 514.363 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.260 Gbps (per GPU), 514.082 Gbps (aggregated)
The layer-level communication performance: 64.254 Gbps (per GPU), 514.032 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.226 Gbps (per GPU), 513.807 Gbps (aggregated)
The layer-level communication performance: 64.219 Gbps (per GPU), 513.749 Gbps (aggregated)
The layer-level communication performance: 64.214 Gbps (per GPU), 513.713 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.850 Gbps (per GPU), 1310.804 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.840 Gbps (per GPU), 1310.717 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.832 Gbps (per GPU), 1310.653 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.836 Gbps (per GPU), 1310.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.831 Gbps (per GPU), 1310.650 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.831 Gbps (per GPU), 1310.650 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.832 Gbps (per GPU), 1310.653 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.832 Gbps (per GPU), 1310.659 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.157 Gbps (per GPU), 913.254 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.158 Gbps (per GPU), 913.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.157 Gbps (per GPU), 913.256 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.157 Gbps (per GPU), 913.253 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.157 Gbps (per GPU), 913.257 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.157 Gbps (per GPU), 913.254 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.155 Gbps (per GPU), 913.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.156 Gbps (per GPU), 913.248 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.606 Gbps (per GPU), 364.849 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.606 Gbps (per GPU), 364.851 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.606 Gbps (per GPU), 364.850 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.606 Gbps (per GPU), 364.849 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.606 Gbps (per GPU), 364.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.606 Gbps (per GPU), 364.848 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.606 Gbps (per GPU), 364.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.606 Gbps (per GPU), 364.847 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.55ms  9.88ms 10.23ms  4.02 29.12K 14.23M
 chk_1  2.55ms  5.30ms  5.71ms  2.24 29.12K  6.56M
 chk_2  2.58ms 17.32ms 17.82ms  6.89 29.12K 24.68M
 chk_3  2.58ms 17.25ms 17.63ms  6.84 29.12K 22.95M
 chk_4  2.59ms  5.17ms  5.55ms  2.15 29.12K  6.33M
 chk_5  2.58ms  9.42ms  9.72ms  3.77 29.12K 12.05M
 chk_6  2.59ms 10.54ms 10.91ms  4.22 29.12K 14.60M
 chk_7  2.59ms  9.71ms 10.16ms  3.93 29.12K 13.21M
   Avg  2.57 10.57 10.97
   Max  2.59 17.32 17.82
   Min  2.55  5.17  5.55
 Ratio  1.02  3.35  3.21
   Var  0.00 18.71 18.89
Profiling takes 2.240 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 358.941 ms
Partition 0 [0, 5) has cost: 358.941 ms
Partition 1 [5, 9) has cost: 338.344 ms
Partition 2 [9, 13) has cost: 338.344 ms
Partition 3 [13, 17) has cost: 338.344 ms
Partition 4 [17, 21) has cost: 338.344 ms
Partition 5 [21, 25) has cost: 338.344 ms
Partition 6 [25, 29) has cost: 338.344 ms
Partition 7 [29, 33) has cost: 341.489 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 310.608 ms
GPU 0, Compute+Comm Time: 128.136 ms, Bubble Time: 133.818 ms, Imbalance Overhead: 48.654 ms
GPU 1, Compute+Comm Time: 122.975 ms, Bubble Time: 124.508 ms, Imbalance Overhead: 63.125 ms
GPU 2, Compute+Comm Time: 122.975 ms, Bubble Time: 114.432 ms, Imbalance Overhead: 73.201 ms
GPU 3, Compute+Comm Time: 122.975 ms, Bubble Time: 115.021 ms, Imbalance Overhead: 72.612 ms
GPU 4, Compute+Comm Time: 122.975 ms, Bubble Time: 124.575 ms, Imbalance Overhead: 63.058 ms
GPU 5, Compute+Comm Time: 122.975 ms, Bubble Time: 133.509 ms, Imbalance Overhead: 54.124 ms
GPU 6, Compute+Comm Time: 122.975 ms, Bubble Time: 142.363 ms, Imbalance Overhead: 45.270 ms
GPU 7, Compute+Comm Time: 123.685 ms, Bubble Time: 152.443 ms, Imbalance Overhead: 34.480 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 609.548 ms
GPU 0, Compute+Comm Time: 240.977 ms, Bubble Time: 300.695 ms, Imbalance Overhead: 67.876 ms
GPU 1, Compute+Comm Time: 238.543 ms, Bubble Time: 280.773 ms, Imbalance Overhead: 90.232 ms
GPU 2, Compute+Comm Time: 238.543 ms, Bubble Time: 262.887 ms, Imbalance Overhead: 108.118 ms
GPU 3, Compute+Comm Time: 238.543 ms, Bubble Time: 244.710 ms, Imbalance Overhead: 126.295 ms
GPU 4, Compute+Comm Time: 238.543 ms, Bubble Time: 224.849 ms, Imbalance Overhead: 146.156 ms
GPU 5, Compute+Comm Time: 238.543 ms, Bubble Time: 222.961 ms, Imbalance Overhead: 148.044 ms
GPU 6, Compute+Comm Time: 238.543 ms, Bubble Time: 243.133 ms, Imbalance Overhead: 127.872 ms
GPU 7, Compute+Comm Time: 253.978 ms, Bubble Time: 261.028 ms, Imbalance Overhead: 94.542 ms
The estimated cost of the whole pipeline: 966.164 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 697.285 ms
Partition 0 [0, 9) has cost: 697.285 ms
Partition 1 [9, 17) has cost: 676.689 ms
Partition 2 [17, 25) has cost: 676.689 ms
Partition 3 [25, 33) has cost: 679.833 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 302.654 ms
GPU 0, Compute+Comm Time: 147.521 ms, Bubble Time: 117.381 ms, Imbalance Overhead: 37.751 ms
GPU 1, Compute+Comm Time: 144.937 ms, Bubble Time: 97.997 ms, Imbalance Overhead: 59.720 ms
GPU 2, Compute+Comm Time: 144.937 ms, Bubble Time: 116.515 ms, Imbalance Overhead: 41.201 ms
GPU 3, Compute+Comm Time: 145.259 ms, Bubble Time: 134.303 ms, Imbalance Overhead: 23.092 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 577.663 ms
GPU 0, Compute+Comm Time: 274.954 ms, Bubble Time: 258.012 ms, Imbalance Overhead: 44.697 ms
GPU 1, Compute+Comm Time: 273.761 ms, Bubble Time: 221.949 ms, Imbalance Overhead: 81.953 ms
GPU 2, Compute+Comm Time: 273.761 ms, Bubble Time: 183.658 ms, Imbalance Overhead: 120.244 ms
GPU 3, Compute+Comm Time: 281.484 ms, Bubble Time: 221.725 ms, Imbalance Overhead: 74.454 ms
    The estimated cost with 2 DP ways is 924.332 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1373.974 ms
Partition 0 [0, 17) has cost: 1373.974 ms
Partition 1 [17, 33) has cost: 1356.522 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 316.061 ms
GPU 0, Compute+Comm Time: 199.520 ms, Bubble Time: 81.323 ms, Imbalance Overhead: 35.219 ms
GPU 1, Compute+Comm Time: 198.433 ms, Bubble Time: 117.629 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 562.561 ms
GPU 0, Compute+Comm Time: 349.733 ms, Bubble Time: 212.828 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 352.949 ms, Bubble Time: 138.474 ms, Imbalance Overhead: 71.137 ms
    The estimated cost with 4 DP ways is 922.553 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2730.496 ms
Partition 0 [0, 33) has cost: 2730.496 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 412.819 ms
GPU 0, Compute+Comm Time: 412.819 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 602.170 ms
GPU 0, Compute+Comm Time: 602.170 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1065.739 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.3048
	Epoch 50:	Loss 1.6912
	Epoch 75:	Loss 1.3715
	Epoch 100:	Loss 1.1805
	Epoch 125:	Loss 1.0534
	Epoch 150:	Loss 0.9580
	Epoch 175:	Loss 0.8842
	Epoch 200:	Loss 0.8292
	Epoch 225:	Loss 0.7843
	Epoch 250:	Loss 0.7477
	Epoch 275:	Loss 0.7150
	Epoch 300:	Loss 0.6903
	Epoch 325:	Loss 0.6667
	Epoch 350:	Loss 0.6521
	Epoch 375:	Loss 0.6328
	Epoch 400:	Loss 0.6175
	Epoch 425:	Loss 0.6051
	Epoch 450:	Loss 0.5972
	Epoch 475:	Loss 0.5797
	Epoch 500:	Loss 0.5719
	Epoch 525:	Loss 0.5600
	Epoch 550:	Loss 0.5609
	Epoch 575:	Loss 0.5560
	Epoch 600:	Loss 0.5458
	Epoch 625:	Loss 0.5409
	Epoch 650:	Loss 0.5322
	Epoch 675:	Loss 0.5259
	Epoch 700:	Loss 0.5209
	Epoch 725:	Loss 0.5136
	Epoch 750:	Loss 0.5142
	Epoch 775:	Loss 0.5050
	Epoch 800:	Loss 0.5026
	Epoch 825:	Loss 0.4995
	Epoch 850:	Loss 0.4949
	Epoch 875:	Loss 0.4896
	Epoch 900:	Loss 0.4853
	Epoch 925:	Loss 0.4855
	Epoch 950:	Loss 0.4844
	Epoch 975:	Loss 0.4781
	Epoch 1000:	Loss 0.4801
	Epoch 1025:	Loss 0.4707
	Epoch 1050:	Loss 0.4750
	Epoch 1075:	Loss 0.4653
	Epoch 1100:	Loss 0.4649
	Epoch 1125:	Loss 0.4715
	Epoch 1150:	Loss 0.4619
	Epoch 1175:	Loss 0.4605
	Epoch 1200:	Loss 0.4558
	Epoch 1225:	Loss 0.4558
	Epoch 1250:	Loss 0.4537
	Epoch 1275:	Loss 0.4486
	Epoch 1300:	Loss 0.4524
	Epoch 1325:	Loss 0.4458
	Epoch 1350:	Loss 0.4452
	Epoch 1375:	Loss 0.4472
	Epoch 1400:	Loss 0.4434
	Epoch 1425:	Loss 0.4400
	Epoch 1450:	Loss 0.4374
	Epoch 1475:	Loss 0.4404
	Epoch 1500:	Loss 0.4367
	Epoch 1525:	Loss 0.4444
	Epoch 1550:	Loss 0.4341
	Epoch 1575:	Loss 0.4344
	Epoch 1600:	Loss 0.4291
	Epoch 1625:	Loss 0.4262
	Epoch 1650:	Loss 0.4306
	Epoch 1675:	Loss 0.4261
	Epoch 1700:	Loss 0.4250
	Epoch 1725:	Loss 0.4288
	Epoch 1750:	Loss 0.4262
	Epoch 1775:	Loss 0.4228
	Epoch 1800:	Loss 0.4261
	Epoch 1825:	Loss 0.4182
	Epoch 1850:	Loss 0.4225
	Epoch 1875:	Loss 0.4157
	Epoch 1900:	Loss 0.4191
	Epoch 1925:	Loss 0.4145
	Epoch 1950:	Loss 0.4142
	Epoch 1975:	Loss 0.4173
	Epoch 2000:	Loss 0.4148
	Epoch 2025:	Loss 0.4124
	Epoch 2050:	Loss 0.4090
	Epoch 2075:	Loss 0.4132
	Epoch 2100:	Loss 0.4094
	Epoch 2125:	Loss 0.4095
	Epoch 2150:	Loss 0.4108
	Epoch 2175:	Loss 0.4076
	Epoch 2200:	Loss 0.4099
	Epoch 2225:	Loss 0.4075
	Epoch 2250:	Loss 0.4026
	Epoch 2275:	Loss 0.4014
	Epoch 2300:	Loss 0.4071
	Epoch 2325:	Loss 0.4022
	Epoch 2350:	Loss 0.4043
	Epoch 2375:	Loss 0.4030
	Epoch 2400:	Loss 0.3982
	Epoch 2425:	Loss 0.3986
	Epoch 2450:	Loss 0.3964
	Epoch 2475:	Loss 0.3986
	Epoch 2500:	Loss 0.4077
	Epoch 2525:	Loss 0.3962
	Epoch 2550:	Loss 0.3954
	Epoch 2575:	Loss 0.3965
	Epoch 2600:	Loss 0.4019
	Epoch 2625:	Loss 0.3960
	Epoch 2650:	Loss 0.3953
	Epoch 2675:	Loss 0.3946
	Epoch 2700:	Loss 0.3934
	Epoch 2725:	Loss 0.3944
	Epoch 2750:	Loss 0.3919
	Epoch 2775:	Loss 0.3897
	Epoch 2800:	Loss 0.3912
	Epoch 2825:	Loss 0.3925
	Epoch 2850:	Loss 0.3948
	Epoch 2875:	Loss 0.3894
	Epoch 2900:	Loss 0.3859
	Epoch 2925:	Loss 0.3922
	Epoch 2950:	Loss 0.3878
	Epoch 2975:	Loss 0.3914
	Epoch 3000:	Loss 0.3871
	Epoch 3025:	Loss 0.3923
	Epoch 3050:	Loss 0.3892
	Epoch 3075:	Loss 0.3875
	Epoch 3100:	Loss 0.3847
	Epoch 3125:	Loss 0.3846
	Epoch 3150:	Loss 0.3841
	Epoch 3175:	Loss 0.3804
	Epoch 3200:	Loss 0.3831
	Epoch 3225:	Loss 0.3805
	Epoch 3250:	Loss 0.3822
	Epoch 3275:	Loss 0.3815
	Epoch 3300:	Loss 0.3791
	Epoch 3325:	Loss 0.3884
	Epoch 3350:	Loss 0.3769
	Epoch 3375:	Loss 0.3819
	Epoch 3400:	Loss 0.3861
	Epoch 3425:	Loss 0.3850
	Epoch 3450:	Loss 0.3803
	Epoch 3475:	Loss 0.3769
	Epoch 3500:	Loss 0.3791
	Epoch 3525:	Loss 0.3796
	Epoch 3550:	Loss 0.3773
	Epoch 3575:	Loss 0.3881
	Epoch 3600:	Loss 0.3791
	Epoch 3625:	Loss 0.3750
	Epoch 3650:	Loss 0.3747
	Epoch 3675:	Loss 0.3782
	Epoch 3700:	Loss 0.3764
	Epoch 3725:	Loss 0.3712
	Epoch 3750:	Loss 0.3794
	Epoch 3775:	Loss 0.3756
	Epoch 3800:	Loss 0.3727
	Epoch 3825:	Loss 0.3753
	Epoch 3850:	Loss 0.3843
	Epoch 3875:	Loss 0.3746
	Epoch 3900:	Loss 0.3698
	Epoch 3925:	Loss 0.3757
	Epoch 3950:	Loss 0.3838
	Epoch 3975:	Loss 0.3688
	Epoch 4000:	Loss 0.3683
	Epoch 4025:	Loss 0.3745
	Epoch 4050:	Loss 0.3714
	Epoch 4075:	Loss 0.3691
	Epoch 4100:	Loss 0.3733
	Epoch 4125:	Loss 0.3727
	Epoch 4150:	Loss 0.3750
	Epoch 4175:	Loss 0.3682
	Epoch 4200:	Loss 0.3673
	Epoch 4225:	Loss 0.3727
	Epoch 4250:	Loss 0.3710
	Epoch 4275:	Loss 0.3679
	Epoch 4300:	Loss 0.3678
	Epoch 4325:	Loss 0.3665
	Epoch 4350:	Loss 0.3696
	Epoch 4375:	Loss 0.3695
	Epoch 4400:	Loss 0.3643
	Epoch 4425:	Loss 0.3674
	Epoch 4450:	Loss 0.3646
	Epoch 4475:	Loss 0.3729
	Epoch 4500:	Loss 0.3686
	Epoch 4525:	Loss 0.3653
	Epoch 4550:	Loss 0.3679
	Epoch 4575:	Loss 0.3675
	Epoch 4600:	Loss 0.3638
	Epoch 4625:	Loss 0.3601
	Epoch 4650:	Loss 0.3668
	Epoch 4675:	Loss 0.3608
	Epoch 4700:	Loss 0.3612
	Epoch 4725:	Loss 0.3613
	Epoch 4750:	Loss 0.3663
	Epoch 4775:	Loss 0.3651
	Epoch 4800:	Loss 0.3674
	Epoch 4825:	Loss 0.3606
	Epoch 4850:	Loss 0.3728
	Epoch 4875:	Loss 0.3604
	Epoch 4900:	Loss 0.3636
	Epoch 4925:	Loss 0.3595
	Epoch 4950:	Loss 0.3615
	Epoch 4975:	Loss 0.3580
Node 2, Pre/Post-Pipelining: 8.591 / 17.800 ms, Bubble: 0.363 ms, Compute: 884.783 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 8.581 / 17.739 ms, Bubble: 1.170 ms, Compute: 884.046 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 4, Pre/Post-Pipelining: 8.585 / 17.740 ms, Bubble: 1.285 ms, Compute: 883.928 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 3, Pre/Post-Pipelining: 8.591 / 17.831 ms, Bubble: 0.042 ms, Compute: 885.071 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 8.589 / 17.790 ms, Bubble: 0.665 ms, Compute: 884.495 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 5, Pre/Post-Pipelining: 8.585 / 17.790 ms, Bubble: 1.076 ms, Compute: 884.083 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 1, Pre/Post-Pipelining: 8.586 / 17.800 ms, Bubble: 1.063 ms, Compute: 884.075 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
	Epoch 5000:	Loss 0.3647
Node 0, Pre/Post-Pipelining: 8.585 / 17.802 ms, Bubble: 1.162 ms, Compute: 883.983 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.585 ms
Cluster-Wide Average, Post-Pipelining Overhead: 17.802 ms
Cluster-Wide Average, Bubble: 1.162 ms
Cluster-Wide Average, Compute: 883.983 ms
Cluster-Wide Average, Communication: 0.010 ms
Cluster-Wide Average, Imbalance: 0.017 ms
Node 0, GPU memory consumption: 22.329 GB
Node 3, GPU memory consumption: 21.505 GB
Node 1, GPU memory consumption: 21.518 GB
Node 2, GPU memory consumption: 21.526 GB
Node 7, GPU memory consumption: 21.499 GB
Node 4, GPU memory consumption: 21.495 GB
Node 6, GPU memory consumption: 21.522 GB
Node 5, GPU memory consumption: 21.518 GB
Node 0, Graph-Level Communication Throughput: 25.536 Gbps, Time: 625.199 ms
Node 1, Graph-Level Communication Throughput: 23.801 Gbps, Time: 713.470 ms
Node 2, Graph-Level Communication Throughput: 36.376 Gbps, Time: 486.201 ms
Node 3, Graph-Level Communication Throughput: 49.641 Gbps, Time: 466.207 ms
Node 4, Graph-Level Communication Throughput: 11.489 Gbps, Time: 732.423 ms
Node 5, Graph-Level Communication Throughput: 19.144 Gbps, Time: 637.958 ms
Node 6, Graph-Level Communication Throughput: 26.486 Gbps, Time: 618.479 ms
Node 7, Graph-Level Communication Throughput: 21.499 Gbps, Time: 633.288 ms
------------------------node id 0,  per-epoch time: 0.911609 s---------------
------------------------node id 1,  per-epoch time: 0.911609 s---------------
------------------------node id 2,  per-epoch time: 0.911609 s---------------
------------------------node id 3,  per-epoch time: 0.911609 s---------------
------------------------node id 4,  per-epoch time: 0.911609 s---------------
------------------------node id 5,  per-epoch time: 0.911609 s---------------
------------------------node id 6,  per-epoch time: 0.911609 s---------------
------------------------node id 7,  per-epoch time: 0.911609 s---------------
************ Profiling Results ************
	Bubble: 27.584094 (ms) (3.03 percentage)
	Compute: 230.721191 (ms) (25.31 percentage)
	GraphCommComputeOverhead: 19.197252 (ms) (2.11 percentage)
	GraphCommNetwork: 614.152036 (ms) (67.37 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.976113 (ms) (2.19 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 14.493 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
