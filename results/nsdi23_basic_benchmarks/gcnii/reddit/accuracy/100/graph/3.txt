Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.882 seconds.
Building the CSC structure...
        It takes 2.315 seconds.
Building the CSC structure...
        It takes 2.331 seconds.
Building the CSC structure...
        It takes 2.370 seconds.
Building the CSC structure...
        It takes 2.414 seconds.
Building the CSC structure...
        It takes 2.552 seconds.
Building the CSC structure...
        It takes 2.681 seconds.
Building the CSC structure...
        It takes 2.705 seconds.
Building the CSC structure...
        It takes 2.218 seconds.
        It takes 2.212 seconds.
        It takes 2.230 seconds.
        It takes 2.291 seconds.
        It takes 2.351 seconds.
        It takes 2.350 seconds.
        It takes 2.373 seconds.
        It takes 2.468 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.278 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.329 seconds.
Building the Label Vector...
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.273 seconds.
Building the Label Vector...
        It takes 0.046 seconds.
        It takes 0.039 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.237 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
        It takes 0.273 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.245 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 33)
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.022 Gbps (per GPU), 504.179 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.981 Gbps (per GPU), 503.847 Gbps (aggregated)
The layer-level communication performance: 62.980 Gbps (per GPU), 503.839 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.944 Gbps (per GPU), 503.554 Gbps (aggregated)
The layer-level communication performance: 62.940 Gbps (per GPU), 503.517 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.912 Gbps (per GPU), 503.299 Gbps (aggregated)
The layer-level communication performance: 62.904 Gbps (per GPU), 503.233 Gbps (aggregated)
The layer-level communication performance: 62.898 Gbps (per GPU), 503.187 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.856 Gbps (per GPU), 1310.851 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.856 Gbps (per GPU), 1310.851 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.854 Gbps (per GPU), 1310.829 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.853 Gbps (per GPU), 1310.826 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.853 Gbps (per GPU), 1310.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.852 Gbps (per GPU), 1310.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.850 Gbps (per GPU), 1310.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.852 Gbps (per GPU), 1310.819 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.263 Gbps (per GPU), 914.107 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.263 Gbps (per GPU), 914.104 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.263 Gbps (per GPU), 914.105 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.263 Gbps (per GPU), 914.101 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.263 Gbps (per GPU), 914.105 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.263 Gbps (per GPU), 914.103 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.261 Gbps (per GPU), 914.087 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.262 Gbps (per GPU), 914.096 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.443 Gbps (per GPU), 363.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.443 Gbps (per GPU), 363.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.443 Gbps (per GPU), 363.546 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.442 Gbps (per GPU), 363.536 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.443 Gbps (per GPU), 363.544 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.443 Gbps (per GPU), 363.545 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.443 Gbps (per GPU), 363.543 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.443 Gbps (per GPU), 363.544 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.55ms  9.74ms 10.18ms  4.00 29.12K 14.23M
 chk_1  2.56ms  5.26ms  5.67ms  2.22 29.12K  6.56M
 chk_2  2.56ms 17.18ms 17.61ms  6.87 29.12K 24.68M
 chk_3  2.57ms 17.15ms 17.52ms  6.81 29.12K 22.95M
 chk_4  2.57ms  5.13ms  5.48ms  2.13 29.12K  6.33M
 chk_5  2.57ms  9.46ms  9.55ms  3.71 29.12K 12.05M
 chk_6  2.57ms 10.45ms 10.72ms  4.17 29.12K 14.60M
 chk_7  2.57ms  9.59ms 10.56ms  4.10 29.12K 13.21M
   Avg  2.57 10.49 10.91
   Max  2.57 17.18 17.61
   Min  2.55  5.13  5.48
 Ratio  1.01  3.35  3.21
   Var  0.00 18.46 18.51
Profiling takes 2.227 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 356.289 ms
Partition 0 [0, 5) has cost: 356.289 ms
Partition 1 [5, 9) has cost: 335.768 ms
Partition 2 [9, 13) has cost: 335.768 ms
Partition 3 [13, 17) has cost: 335.768 ms
Partition 4 [17, 21) has cost: 335.768 ms
Partition 5 [21, 25) has cost: 335.768 ms
Partition 6 [25, 29) has cost: 335.768 ms
Partition 7 [29, 33) has cost: 339.115 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 309.159 ms
GPU 0, Compute+Comm Time: 127.393 ms, Bubble Time: 133.719 ms, Imbalance Overhead: 48.048 ms
GPU 1, Compute+Comm Time: 122.248 ms, Bubble Time: 124.185 ms, Imbalance Overhead: 62.726 ms
GPU 2, Compute+Comm Time: 122.248 ms, Bubble Time: 113.999 ms, Imbalance Overhead: 72.913 ms
GPU 3, Compute+Comm Time: 122.248 ms, Bubble Time: 114.463 ms, Imbalance Overhead: 72.448 ms
GPU 4, Compute+Comm Time: 122.248 ms, Bubble Time: 123.991 ms, Imbalance Overhead: 62.920 ms
GPU 5, Compute+Comm Time: 122.248 ms, Bubble Time: 132.873 ms, Imbalance Overhead: 54.039 ms
GPU 6, Compute+Comm Time: 122.248 ms, Bubble Time: 141.715 ms, Imbalance Overhead: 45.197 ms
GPU 7, Compute+Comm Time: 123.550 ms, Bubble Time: 151.197 ms, Imbalance Overhead: 34.412 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 605.168 ms
GPU 0, Compute+Comm Time: 239.223 ms, Bubble Time: 298.783 ms, Imbalance Overhead: 67.162 ms
GPU 1, Compute+Comm Time: 237.178 ms, Bubble Time: 278.796 ms, Imbalance Overhead: 89.194 ms
GPU 2, Compute+Comm Time: 237.178 ms, Bubble Time: 260.897 ms, Imbalance Overhead: 107.093 ms
GPU 3, Compute+Comm Time: 237.178 ms, Bubble Time: 242.764 ms, Imbalance Overhead: 125.226 ms
GPU 4, Compute+Comm Time: 237.178 ms, Bubble Time: 222.901 ms, Imbalance Overhead: 145.089 ms
GPU 5, Compute+Comm Time: 237.178 ms, Bubble Time: 221.074 ms, Imbalance Overhead: 146.916 ms
GPU 6, Compute+Comm Time: 237.178 ms, Bubble Time: 241.086 ms, Imbalance Overhead: 126.903 ms
GPU 7, Compute+Comm Time: 252.554 ms, Bubble Time: 258.869 ms, Imbalance Overhead: 93.745 ms
The estimated cost of the whole pipeline: 960.043 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 692.057 ms
Partition 0 [0, 9) has cost: 692.057 ms
Partition 1 [9, 17) has cost: 671.536 ms
Partition 2 [17, 25) has cost: 671.536 ms
Partition 3 [25, 33) has cost: 674.883 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 301.285 ms
GPU 0, Compute+Comm Time: 146.745 ms, Bubble Time: 117.281 ms, Imbalance Overhead: 37.259 ms
GPU 1, Compute+Comm Time: 144.164 ms, Bubble Time: 97.499 ms, Imbalance Overhead: 59.622 ms
GPU 2, Compute+Comm Time: 144.164 ms, Bubble Time: 115.909 ms, Imbalance Overhead: 41.213 ms
GPU 3, Compute+Comm Time: 144.814 ms, Bubble Time: 133.242 ms, Imbalance Overhead: 23.229 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 573.611 ms
GPU 0, Compute+Comm Time: 273.356 ms, Bubble Time: 256.367 ms, Imbalance Overhead: 43.888 ms
GPU 1, Compute+Comm Time: 272.401 ms, Bubble Time: 220.335 ms, Imbalance Overhead: 80.875 ms
GPU 2, Compute+Comm Time: 272.401 ms, Bubble Time: 182.144 ms, Imbalance Overhead: 119.066 ms
GPU 3, Compute+Comm Time: 280.094 ms, Bubble Time: 219.938 ms, Imbalance Overhead: 73.579 ms
    The estimated cost with 2 DP ways is 918.641 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1363.593 ms
Partition 0 [0, 17) has cost: 1363.593 ms
Partition 1 [17, 33) has cost: 1346.419 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 315.282 ms
GPU 0, Compute+Comm Time: 198.860 ms, Bubble Time: 81.450 ms, Imbalance Overhead: 34.972 ms
GPU 1, Compute+Comm Time: 198.089 ms, Bubble Time: 117.193 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 558.853 ms
GPU 0, Compute+Comm Time: 347.291 ms, Bubble Time: 211.562 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 350.594 ms, Bubble Time: 137.339 ms, Imbalance Overhead: 70.920 ms
    The estimated cost with 4 DP ways is 917.842 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2710.013 ms
Partition 0 [0, 33) has cost: 2710.013 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 412.529 ms
GPU 0, Compute+Comm Time: 412.529 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 600.211 ms
GPU 0, Compute+Comm Time: 600.211 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1063.377 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.3048	TrainAcc 0.5582	ValidAcc 0.5817	TestAcc 0.5775	BestValid 0.5817
	Epoch 50:	Loss 1.6912	TrainAcc 0.6860	ValidAcc 0.7080	TestAcc 0.7034	BestValid 0.7080
	Epoch 75:	Loss 1.3715	TrainAcc 0.7489	ValidAcc 0.7648	TestAcc 0.7595	BestValid 0.7648
	Epoch 100:	Loss 1.1805	TrainAcc 0.7865	ValidAcc 0.7997	TestAcc 0.7960	BestValid 0.7997
	Epoch 125:	Loss 1.0534	TrainAcc 0.8146	ValidAcc 0.8277	TestAcc 0.8228	BestValid 0.8277
	Epoch 150:	Loss 0.9580	TrainAcc 0.8343	ValidAcc 0.8453	TestAcc 0.8398	BestValid 0.8453
	Epoch 175:	Loss 0.8842	TrainAcc 0.8488	ValidAcc 0.8585	TestAcc 0.8533	BestValid 0.8585
	Epoch 200:	Loss 0.8292	TrainAcc 0.8629	ValidAcc 0.8713	TestAcc 0.8664	BestValid 0.8713
	Epoch 225:	Loss 0.7843	TrainAcc 0.8720	ValidAcc 0.8795	TestAcc 0.8747	BestValid 0.8795
	Epoch 250:	Loss 0.7477	TrainAcc 0.8789	ValidAcc 0.8861	TestAcc 0.8810	BestValid 0.8861
	Epoch 275:	Loss 0.7150	TrainAcc 0.8841	ValidAcc 0.8908	TestAcc 0.8851	BestValid 0.8908
	Epoch 300:	Loss 0.6903	TrainAcc 0.8870	ValidAcc 0.8934	TestAcc 0.8876	BestValid 0.8934
	Epoch 325:	Loss 0.6667	TrainAcc 0.8923	ValidAcc 0.8984	TestAcc 0.8933	BestValid 0.8984
	Epoch 350:	Loss 0.6521	TrainAcc 0.8956	ValidAcc 0.9020	TestAcc 0.8966	BestValid 0.9020
	Epoch 375:	Loss 0.6328	TrainAcc 0.8982	ValidAcc 0.9049	TestAcc 0.8996	BestValid 0.9049
	Epoch 400:	Loss 0.6175	TrainAcc 0.8985	ValidAcc 0.9047	TestAcc 0.8996	BestValid 0.9049
	Epoch 425:	Loss 0.6051	TrainAcc 0.8990	ValidAcc 0.9058	TestAcc 0.9004	BestValid 0.9058
	Epoch 450:	Loss 0.5972	TrainAcc 0.9020	ValidAcc 0.9081	TestAcc 0.9031	BestValid 0.9081
	Epoch 475:	Loss 0.5797	TrainAcc 0.9031	ValidAcc 0.9091	TestAcc 0.9044	BestValid 0.9091
	Epoch 500:	Loss 0.5719	TrainAcc 0.9070	ValidAcc 0.9132	TestAcc 0.9084	BestValid 0.9132
	Epoch 525:	Loss 0.5600	TrainAcc 0.9092	ValidAcc 0.9158	TestAcc 0.9111	BestValid 0.9158
	Epoch 550:	Loss 0.5609	TrainAcc 0.9102	ValidAcc 0.9167	TestAcc 0.9119	BestValid 0.9167
	Epoch 575:	Loss 0.5560	TrainAcc 0.9111	ValidAcc 0.9174	TestAcc 0.9125	BestValid 0.9174
	Epoch 600:	Loss 0.5458	TrainAcc 0.9129	ValidAcc 0.9191	TestAcc 0.9141	BestValid 0.9191
	Epoch 625:	Loss 0.5409	TrainAcc 0.9121	ValidAcc 0.9175	TestAcc 0.9130	BestValid 0.9191
	Epoch 650:	Loss 0.5322	TrainAcc 0.9125	ValidAcc 0.9180	TestAcc 0.9131	BestValid 0.9191
	Epoch 675:	Loss 0.5259	TrainAcc 0.9124	ValidAcc 0.9179	TestAcc 0.9128	BestValid 0.9191
	Epoch 700:	Loss 0.5209	TrainAcc 0.9133	ValidAcc 0.9187	TestAcc 0.9142	BestValid 0.9191
	Epoch 725:	Loss 0.5136	TrainAcc 0.9134	ValidAcc 0.9188	TestAcc 0.9138	BestValid 0.9191
	Epoch 750:	Loss 0.5142	TrainAcc 0.9151	ValidAcc 0.9203	TestAcc 0.9155	BestValid 0.9203
	Epoch 775:	Loss 0.5050	TrainAcc 0.9106	ValidAcc 0.9143	TestAcc 0.9103	BestValid 0.9203
	Epoch 800:	Loss 0.5026	TrainAcc 0.9162	ValidAcc 0.9213	TestAcc 0.9164	BestValid 0.9213
	Epoch 825:	Loss 0.4995	TrainAcc 0.9161	ValidAcc 0.9207	TestAcc 0.9163	BestValid 0.9213
	Epoch 850:	Loss 0.4949	TrainAcc 0.9201	ValidAcc 0.9251	TestAcc 0.9200	BestValid 0.9251
	Epoch 875:	Loss 0.4896	TrainAcc 0.9213	ValidAcc 0.9267	TestAcc 0.9219	BestValid 0.9267
	Epoch 900:	Loss 0.4853	TrainAcc 0.9193	ValidAcc 0.9244	TestAcc 0.9191	BestValid 0.9267
	Epoch 925:	Loss 0.4855	TrainAcc 0.9185	ValidAcc 0.9227	TestAcc 0.9188	BestValid 0.9267
	Epoch 950:	Loss 0.4844	TrainAcc 0.9157	ValidAcc 0.9201	TestAcc 0.9158	BestValid 0.9267
	Epoch 975:	Loss 0.4781	TrainAcc 0.9206	ValidAcc 0.9245	TestAcc 0.9205	BestValid 0.9267
	Epoch 1000:	Loss 0.4801	TrainAcc 0.9145	ValidAcc 0.9188	TestAcc 0.9149	BestValid 0.9267
	Epoch 1025:	Loss 0.4707	TrainAcc 0.9150	ValidAcc 0.9193	TestAcc 0.9147	BestValid 0.9267
	Epoch 1050:	Loss 0.4750	TrainAcc 0.9197	ValidAcc 0.9238	TestAcc 0.9195	BestValid 0.9267
	Epoch 1075:	Loss 0.4653	TrainAcc 0.9168	ValidAcc 0.9206	TestAcc 0.9166	BestValid 0.9267
	Epoch 1100:	Loss 0.4649	TrainAcc 0.9117	ValidAcc 0.9146	TestAcc 0.9114	BestValid 0.9267
	Epoch 1125:	Loss 0.4715	TrainAcc 0.9202	ValidAcc 0.9235	TestAcc 0.9195	BestValid 0.9267
	Epoch 1150:	Loss 0.4618	TrainAcc 0.9181	ValidAcc 0.9212	TestAcc 0.9174	BestValid 0.9267
	Epoch 1175:	Loss 0.4605	TrainAcc 0.9205	ValidAcc 0.9240	TestAcc 0.9198	BestValid 0.9267
	Epoch 1200:	Loss 0.4558	TrainAcc 0.9214	ValidAcc 0.9243	TestAcc 0.9204	BestValid 0.9267
	Epoch 1225:	Loss 0.4558	TrainAcc 0.9205	ValidAcc 0.9233	TestAcc 0.9198	BestValid 0.9267
	Epoch 1250:	Loss 0.4537	TrainAcc 0.9211	ValidAcc 0.9237	TestAcc 0.9203	BestValid 0.9267
	Epoch 1275:	Loss 0.4486	TrainAcc 0.9241	ValidAcc 0.9267	TestAcc 0.9229	BestValid 0.9267
	Epoch 1300:	Loss 0.4524	TrainAcc 0.9237	ValidAcc 0.9262	TestAcc 0.9221	BestValid 0.9267
	Epoch 1325:	Loss 0.4458	TrainAcc 0.9222	ValidAcc 0.9246	TestAcc 0.9212	BestValid 0.9267
	Epoch 1350:	Loss 0.4452	TrainAcc 0.9250	ValidAcc 0.9276	TestAcc 0.9240	BestValid 0.9276
	Epoch 1375:	Loss 0.4472	TrainAcc 0.9218	ValidAcc 0.9241	TestAcc 0.9203	BestValid 0.9276
	Epoch 1400:	Loss 0.4434	TrainAcc 0.9239	ValidAcc 0.9261	TestAcc 0.9223	BestValid 0.9276
	Epoch 1425:	Loss 0.4400	TrainAcc 0.9259	ValidAcc 0.9283	TestAcc 0.9246	BestValid 0.9283
	Epoch 1450:	Loss 0.4374	TrainAcc 0.9241	ValidAcc 0.9259	TestAcc 0.9229	BestValid 0.9283
	Epoch 1475:	Loss 0.4404	TrainAcc 0.9254	ValidAcc 0.9274	TestAcc 0.9244	BestValid 0.9283
	Epoch 1500:	Loss 0.4367	TrainAcc 0.9259	ValidAcc 0.9274	TestAcc 0.9244	BestValid 0.9283
	Epoch 1525:	Loss 0.4444	TrainAcc 0.9292	ValidAcc 0.9313	TestAcc 0.9274	BestValid 0.9313
	Epoch 1550:	Loss 0.4341	TrainAcc 0.9241	ValidAcc 0.9260	TestAcc 0.9225	BestValid 0.9313
	Epoch 1575:	Loss 0.4344	TrainAcc 0.9249	ValidAcc 0.9265	TestAcc 0.9233	BestValid 0.9313
	Epoch 1600:	Loss 0.4291	TrainAcc 0.9190	ValidAcc 0.9201	TestAcc 0.9177	BestValid 0.9313
	Epoch 1625:	Loss 0.4262	TrainAcc 0.9144	ValidAcc 0.9157	TestAcc 0.9130	BestValid 0.9313
	Epoch 1650:	Loss 0.4306	TrainAcc 0.9261	ValidAcc 0.9272	TestAcc 0.9246	BestValid 0.9313
	Epoch 1675:	Loss 0.4261	TrainAcc 0.9214	ValidAcc 0.9222	TestAcc 0.9199	BestValid 0.9313
	Epoch 1700:	Loss 0.4250	TrainAcc 0.9149	ValidAcc 0.9154	TestAcc 0.9131	BestValid 0.9313
	Epoch 1725:	Loss 0.4288	TrainAcc 0.9294	ValidAcc 0.9302	TestAcc 0.9274	BestValid 0.9313
	Epoch 1750:	Loss 0.4262	TrainAcc 0.9198	ValidAcc 0.9204	TestAcc 0.9180	BestValid 0.9313
	Epoch 1775:	Loss 0.4228	TrainAcc 0.9297	ValidAcc 0.9307	TestAcc 0.9280	BestValid 0.9313
	Epoch 1800:	Loss 0.4261	TrainAcc 0.9255	ValidAcc 0.9263	TestAcc 0.9238	BestValid 0.9313
	Epoch 1825:	Loss 0.4182	TrainAcc 0.9155	ValidAcc 0.9160	TestAcc 0.9135	BestValid 0.9313
	Epoch 1850:	Loss 0.4225	TrainAcc 0.9278	ValidAcc 0.9284	TestAcc 0.9257	BestValid 0.9313
	Epoch 1875:	Loss 0.4157	TrainAcc 0.9263	ValidAcc 0.9271	TestAcc 0.9242	BestValid 0.9313
	Epoch 1900:	Loss 0.4191	TrainAcc 0.9288	ValidAcc 0.9291	TestAcc 0.9267	BestValid 0.9313
	Epoch 1925:	Loss 0.4145	TrainAcc 0.9246	ValidAcc 0.9250	TestAcc 0.9224	BestValid 0.9313
	Epoch 1950:	Loss 0.4142	TrainAcc 0.9222	ValidAcc 0.9227	TestAcc 0.9208	BestValid 0.9313
	Epoch 1975:	Loss 0.4173	TrainAcc 0.9296	ValidAcc 0.9298	TestAcc 0.9269	BestValid 0.9313
	Epoch 2000:	Loss 0.4148	TrainAcc 0.9275	ValidAcc 0.9282	TestAcc 0.9256	BestValid 0.9313
	Epoch 2025:	Loss 0.4124	TrainAcc 0.9217	ValidAcc 0.9217	TestAcc 0.9200	BestValid 0.9313
	Epoch 2050:	Loss 0.4090	TrainAcc 0.9245	ValidAcc 0.9248	TestAcc 0.9225	BestValid 0.9313
	Epoch 2075:	Loss 0.4132	TrainAcc 0.9204	ValidAcc 0.9204	TestAcc 0.9189	BestValid 0.9313
	Epoch 2100:	Loss 0.4094	TrainAcc 0.9310	ValidAcc 0.9316	TestAcc 0.9285	BestValid 0.9316
	Epoch 2125:	Loss 0.4095	TrainAcc 0.9270	ValidAcc 0.9273	TestAcc 0.9251	BestValid 0.9316
	Epoch 2150:	Loss 0.4108	TrainAcc 0.9240	ValidAcc 0.9243	TestAcc 0.9221	BestValid 0.9316
	Epoch 2175:	Loss 0.4076	TrainAcc 0.9310	ValidAcc 0.9309	TestAcc 0.9281	BestValid 0.9316
	Epoch 2200:	Loss 0.4099	TrainAcc 0.9273	ValidAcc 0.9274	TestAcc 0.9252	BestValid 0.9316
	Epoch 2225:	Loss 0.4075	TrainAcc 0.9261	ValidAcc 0.9263	TestAcc 0.9241	BestValid 0.9316
	Epoch 2250:	Loss 0.4026	TrainAcc 0.9278	ValidAcc 0.9285	TestAcc 0.9253	BestValid 0.9316
	Epoch 2275:	Loss 0.4014	TrainAcc 0.9286	ValidAcc 0.9287	TestAcc 0.9264	BestValid 0.9316
	Epoch 2300:	Loss 0.4071	TrainAcc 0.9231	ValidAcc 0.9227	TestAcc 0.9213	BestValid 0.9316
	Epoch 2325:	Loss 0.4022	TrainAcc 0.9171	ValidAcc 0.9169	TestAcc 0.9142	BestValid 0.9316
	Epoch 2350:	Loss 0.4044	TrainAcc 0.9306	ValidAcc 0.9308	TestAcc 0.9278	BestValid 0.9316
	Epoch 2375:	Loss 0.4030	TrainAcc 0.9237	ValidAcc 0.9230	TestAcc 0.9216	BestValid 0.9316
	Epoch 2400:	Loss 0.3982	TrainAcc 0.9262	ValidAcc 0.9259	TestAcc 0.9243	BestValid 0.9316
	Epoch 2425:	Loss 0.3986	TrainAcc 0.9297	ValidAcc 0.9295	TestAcc 0.9277	BestValid 0.9316
	Epoch 2450:	Loss 0.3964	TrainAcc 0.9248	ValidAcc 0.9242	TestAcc 0.9227	BestValid 0.9316
	Epoch 2475:	Loss 0.3986	TrainAcc 0.9279	ValidAcc 0.9280	TestAcc 0.9256	BestValid 0.9316
	Epoch 2500:	Loss 0.4077	TrainAcc 0.9197	ValidAcc 0.9190	TestAcc 0.9168	BestValid 0.9316
	Epoch 2525:	Loss 0.3962	TrainAcc 0.9296	ValidAcc 0.9295	TestAcc 0.9272	BestValid 0.9316
	Epoch 2550:	Loss 0.3954	TrainAcc 0.9270	ValidAcc 0.9267	TestAcc 0.9247	BestValid 0.9316
	Epoch 2575:	Loss 0.3965	TrainAcc 0.9222	ValidAcc 0.9222	TestAcc 0.9202	BestValid 0.9316
	Epoch 2600:	Loss 0.4019	TrainAcc 0.9303	ValidAcc 0.9300	TestAcc 0.9273	BestValid 0.9316
	Epoch 2625:	Loss 0.3960	TrainAcc 0.9289	ValidAcc 0.9290	TestAcc 0.9265	BestValid 0.9316
	Epoch 2650:	Loss 0.3953	TrainAcc 0.9229	ValidAcc 0.9219	TestAcc 0.9200	BestValid 0.9316
	Epoch 2675:	Loss 0.3946	TrainAcc 0.9277	ValidAcc 0.9272	TestAcc 0.9249	BestValid 0.9316
	Epoch 2700:	Loss 0.3934	TrainAcc 0.9243	ValidAcc 0.9235	TestAcc 0.9219	BestValid 0.9316
	Epoch 2725:	Loss 0.3944	TrainAcc 0.9316	ValidAcc 0.9319	TestAcc 0.9287	BestValid 0.9319
	Epoch 2750:	Loss 0.3919	TrainAcc 0.9213	ValidAcc 0.9209	TestAcc 0.9184	BestValid 0.9319
	Epoch 2775:	Loss 0.3897	TrainAcc 0.9274	ValidAcc 0.9266	TestAcc 0.9247	BestValid 0.9319
	Epoch 2800:	Loss 0.3912	TrainAcc 0.9253	ValidAcc 0.9246	TestAcc 0.9228	BestValid 0.9319
	Epoch 2825:	Loss 0.3925	TrainAcc 0.9207	ValidAcc 0.9205	TestAcc 0.9177	BestValid 0.9319
	Epoch 2850:	Loss 0.3948	TrainAcc 0.9213	ValidAcc 0.9206	TestAcc 0.9180	BestValid 0.9319
	Epoch 2875:	Loss 0.3894	TrainAcc 0.9291	ValidAcc 0.9288	TestAcc 0.9266	BestValid 0.9319
	Epoch 2900:	Loss 0.3859	TrainAcc 0.9273	ValidAcc 0.9263	TestAcc 0.9245	BestValid 0.9319
	Epoch 2925:	Loss 0.3922	TrainAcc 0.9280	ValidAcc 0.9272	TestAcc 0.9252	BestValid 0.9319
	Epoch 2950:	Loss 0.3878	TrainAcc 0.9308	ValidAcc 0.9308	TestAcc 0.9279	BestValid 0.9319
	Epoch 2975:	Loss 0.3914	TrainAcc 0.9189	ValidAcc 0.9179	TestAcc 0.9150	BestValid 0.9319
	Epoch 3000:	Loss 0.3871	TrainAcc 0.9329	ValidAcc 0.9328	TestAcc 0.9300	BestValid 0.9328
	Epoch 3025:	Loss 0.3923	TrainAcc 0.9359	ValidAcc 0.9363	TestAcc 0.9325	BestValid 0.9363
	Epoch 3050:	Loss 0.3892	TrainAcc 0.9323	ValidAcc 0.9321	TestAcc 0.9290	BestValid 0.9363
	Epoch 3075:	Loss 0.3875	TrainAcc 0.9296	ValidAcc 0.9294	TestAcc 0.9268	BestValid 0.9363
	Epoch 3100:	Loss 0.3847	TrainAcc 0.9316	ValidAcc 0.9315	TestAcc 0.9285	BestValid 0.9363
	Epoch 3125:	Loss 0.3846	TrainAcc 0.9283	ValidAcc 0.9277	TestAcc 0.9253	BestValid 0.9363
	Epoch 3150:	Loss 0.3841	TrainAcc 0.9226	ValidAcc 0.9221	TestAcc 0.9191	BestValid 0.9363
	Epoch 3175:	Loss 0.3804	TrainAcc 0.9259	ValidAcc 0.9245	TestAcc 0.9228	BestValid 0.9363
	Epoch 3200:	Loss 0.3831	TrainAcc 0.9193	ValidAcc 0.9182	TestAcc 0.9153	BestValid 0.9363
	Epoch 3225:	Loss 0.3805	TrainAcc 0.9302	ValidAcc 0.9295	TestAcc 0.9271	BestValid 0.9363
	Epoch 3250:	Loss 0.3822	TrainAcc 0.9293	ValidAcc 0.9285	TestAcc 0.9261	BestValid 0.9363
	Epoch 3275:	Loss 0.3815	TrainAcc 0.9267	ValidAcc 0.9256	TestAcc 0.9235	BestValid 0.9363
	Epoch 3300:	Loss 0.3791	TrainAcc 0.9229	ValidAcc 0.9223	TestAcc 0.9193	BestValid 0.9363
	Epoch 3325:	Loss 0.3884	TrainAcc 0.9307	ValidAcc 0.9299	TestAcc 0.9270	BestValid 0.9363
	Epoch 3350:	Loss 0.3769	TrainAcc 0.9247	ValidAcc 0.9236	TestAcc 0.9209	BestValid 0.9363
	Epoch 3375:	Loss 0.3819	TrainAcc 0.9231	ValidAcc 0.9223	TestAcc 0.9195	BestValid 0.9363
	Epoch 3400:	Loss 0.3861	TrainAcc 0.9204	ValidAcc 0.9187	TestAcc 0.9157	BestValid 0.9363
	Epoch 3425:	Loss 0.3850	TrainAcc 0.9312	ValidAcc 0.9310	TestAcc 0.9278	BestValid 0.9363
	Epoch 3450:	Loss 0.3803	TrainAcc 0.9222	ValidAcc 0.9203	TestAcc 0.9174	BestValid 0.9363
	Epoch 3475:	Loss 0.3769	TrainAcc 0.9265	ValidAcc 0.9256	TestAcc 0.9232	BestValid 0.9363
	Epoch 3500:	Loss 0.3791	TrainAcc 0.9240	ValidAcc 0.9234	TestAcc 0.9201	BestValid 0.9363
	Epoch 3525:	Loss 0.3796	TrainAcc 0.9286	ValidAcc 0.9272	TestAcc 0.9251	BestValid 0.9363
	Epoch 3550:	Loss 0.3773	TrainAcc 0.9249	ValidAcc 0.9239	TestAcc 0.9214	BestValid 0.9363
	Epoch 3575:	Loss 0.3881	TrainAcc 0.9216	ValidAcc 0.9210	TestAcc 0.9179	BestValid 0.9363
	Epoch 3600:	Loss 0.3791	TrainAcc 0.9347	ValidAcc 0.9345	TestAcc 0.9312	BestValid 0.9363
	Epoch 3625:	Loss 0.3750	TrainAcc 0.9213	ValidAcc 0.9205	TestAcc 0.9175	BestValid 0.9363
	Epoch 3650:	Loss 0.3747	TrainAcc 0.9279	ValidAcc 0.9266	TestAcc 0.9242	BestValid 0.9363
	Epoch 3675:	Loss 0.3782	TrainAcc 0.9319	ValidAcc 0.9308	TestAcc 0.9289	BestValid 0.9363
	Epoch 3700:	Loss 0.3764	TrainAcc 0.9288	ValidAcc 0.9278	TestAcc 0.9253	BestValid 0.9363
	Epoch 3725:	Loss 0.3712	TrainAcc 0.9254	ValidAcc 0.9235	TestAcc 0.9215	BestValid 0.9363
	Epoch 3750:	Loss 0.3794	TrainAcc 0.9235	ValidAcc 0.9222	TestAcc 0.9192	BestValid 0.9363
	Epoch 3775:	Loss 0.3756	TrainAcc 0.9205	ValidAcc 0.9186	TestAcc 0.9159	BestValid 0.9363
	Epoch 3800:	Loss 0.3727	TrainAcc 0.9323	ValidAcc 0.9308	TestAcc 0.9283	BestValid 0.9363
	Epoch 3825:	Loss 0.3753	TrainAcc 0.9255	ValidAcc 0.9237	TestAcc 0.9216	BestValid 0.9363
	Epoch 3850:	Loss 0.3843	TrainAcc 0.9200	ValidAcc 0.9186	TestAcc 0.9153	BestValid 0.9363
	Epoch 3875:	Loss 0.3746	TrainAcc 0.9295	ValidAcc 0.9278	TestAcc 0.9260	BestValid 0.9363
	Epoch 3900:	Loss 0.3698	TrainAcc 0.9282	ValidAcc 0.9262	TestAcc 0.9244	BestValid 0.9363
	Epoch 3925:	Loss 0.3757	TrainAcc 0.9307	ValidAcc 0.9293	TestAcc 0.9271	BestValid 0.9363
	Epoch 3950:	Loss 0.3838	TrainAcc 0.9196	ValidAcc 0.9172	TestAcc 0.9144	BestValid 0.9363
	Epoch 3975:	Loss 0.3688	TrainAcc 0.9337	ValidAcc 0.9329	TestAcc 0.9299	BestValid 0.9363
	Epoch 4000:	Loss 0.3683	TrainAcc 0.9242	ValidAcc 0.9231	TestAcc 0.9204	BestValid 0.9363
	Epoch 4025:	Loss 0.3745	TrainAcc 0.9234	ValidAcc 0.9215	TestAcc 0.9194	BestValid 0.9363
	Epoch 4050:	Loss 0.3714	TrainAcc 0.9366	ValidAcc 0.9366	TestAcc 0.9323	BestValid 0.9366
	Epoch 4075:	Loss 0.3691	TrainAcc 0.9302	ValidAcc 0.9284	TestAcc 0.9263	BestValid 0.9366
	Epoch 4100:	Loss 0.3733	TrainAcc 0.9331	ValidAcc 0.9319	TestAcc 0.9297	BestValid 0.9366
	Epoch 4125:	Loss 0.3727	TrainAcc 0.9226	ValidAcc 0.9206	TestAcc 0.9178	BestValid 0.9366
	Epoch 4150:	Loss 0.3750	TrainAcc 0.9206	ValidAcc 0.9186	TestAcc 0.9159	BestValid 0.9366
	Epoch 4175:	Loss 0.3682	TrainAcc 0.9243	ValidAcc 0.9225	TestAcc 0.9198	BestValid 0.9366
	Epoch 4200:	Loss 0.3673	TrainAcc 0.9252	ValidAcc 0.9235	TestAcc 0.9209	BestValid 0.9366
	Epoch 4225:	Loss 0.3727	TrainAcc 0.9329	ValidAcc 0.9314	TestAcc 0.9291	BestValid 0.9366
	Epoch 4250:	Loss 0.3710	TrainAcc 0.9270	ValidAcc 0.9241	TestAcc 0.9228	BestValid 0.9366
	Epoch 4275:	Loss 0.3679	TrainAcc 0.9311	ValidAcc 0.9294	TestAcc 0.9270	BestValid 0.9366
	Epoch 4300:	Loss 0.3678	TrainAcc 0.9157	ValidAcc 0.9138	TestAcc 0.9106	BestValid 0.9366
	Epoch 4325:	Loss 0.3665	TrainAcc 0.9302	ValidAcc 0.9277	TestAcc 0.9263	BestValid 0.9366
	Epoch 4350:	Loss 0.3696	TrainAcc 0.9268	ValidAcc 0.9250	TestAcc 0.9221	BestValid 0.9366
	Epoch 4375:	Loss 0.3695	TrainAcc 0.9371	ValidAcc 0.9371	TestAcc 0.9330	BestValid 0.9371
	Epoch 4400:	Loss 0.3643	TrainAcc 0.9373	ValidAcc 0.9364	TestAcc 0.9334	BestValid 0.9371
	Epoch 4425:	Loss 0.3674	TrainAcc 0.9354	ValidAcc 0.9343	TestAcc 0.9312	BestValid 0.9371
	Epoch 4450:	Loss 0.3646	TrainAcc 0.9350	ValidAcc 0.9338	TestAcc 0.9311	BestValid 0.9371
	Epoch 4475:	Loss 0.3729	TrainAcc 0.9321	ValidAcc 0.9304	TestAcc 0.9281	BestValid 0.9371
	Epoch 4500:	Loss 0.3686	TrainAcc 0.9328	ValidAcc 0.9315	TestAcc 0.9290	BestValid 0.9371
	Epoch 4525:	Loss 0.3653	TrainAcc 0.9319	ValidAcc 0.9301	TestAcc 0.9279	BestValid 0.9371
	Epoch 4550:	Loss 0.3679	TrainAcc 0.9301	ValidAcc 0.9280	TestAcc 0.9260	BestValid 0.9371
	Epoch 4575:	Loss 0.3675	TrainAcc 0.9360	ValidAcc 0.9353	TestAcc 0.9317	BestValid 0.9371
	Epoch 4600:	Loss 0.3638	TrainAcc 0.9272	ValidAcc 0.9256	TestAcc 0.9232	BestValid 0.9371
	Epoch 4625:	Loss 0.3601	TrainAcc 0.9235	ValidAcc 0.9212	TestAcc 0.9185	BestValid 0.9371
	Epoch 4650:	Loss 0.3668	TrainAcc 0.9271	ValidAcc 0.9243	TestAcc 0.9225	BestValid 0.9371
	Epoch 4675:	Loss 0.3608	TrainAcc 0.9334	ValidAcc 0.9320	TestAcc 0.9294	BestValid 0.9371
	Epoch 4700:	Loss 0.3612	TrainAcc 0.9323	ValidAcc 0.9307	TestAcc 0.9281	BestValid 0.9371
	Epoch 4725:	Loss 0.3613	TrainAcc 0.9254	ValidAcc 0.9228	TestAcc 0.9209	BestValid 0.9371
	Epoch 4750:	Loss 0.3663	TrainAcc 0.9299	ValidAcc 0.9277	TestAcc 0.9258	BestValid 0.9371
	Epoch 4775:	Loss 0.3651	TrainAcc 0.9350	ValidAcc 0.9342	TestAcc 0.9312	BestValid 0.9371
	Epoch 4800:	Loss 0.3674	TrainAcc 0.9233	ValidAcc 0.9208	TestAcc 0.9184	BestValid 0.9371
	Epoch 4825:	Loss 0.3606	TrainAcc 0.9254	ValidAcc 0.9230	TestAcc 0.9208	BestValid 0.9371
	Epoch 4850:	Loss 0.3728	TrainAcc 0.9315	ValidAcc 0.9294	TestAcc 0.9268	BestValid 0.9371
	Epoch 4875:	Loss 0.3604	TrainAcc 0.9260	ValidAcc 0.9237	TestAcc 0.9213	BestValid 0.9371
	Epoch 4900:	Loss 0.3636	TrainAcc 0.9344	ValidAcc 0.9329	TestAcc 0.9303	BestValid 0.9371
	Epoch 4925:	Loss 0.3596	TrainAcc 0.9284	ValidAcc 0.9267	TestAcc 0.9237	BestValid 0.9371
	Epoch 4950:	Loss 0.3615	TrainAcc 0.9296	ValidAcc 0.9274	TestAcc 0.9253	BestValid 0.9371
	Epoch 4975:	Loss 0.3580	TrainAcc 0.9345	ValidAcc 0.9337	TestAcc 0.9307	BestValid 0.9371
	Epoch 5000:	Loss 0.3647	TrainAcc 0.9294	ValidAcc 0.9273	TestAcc 0.9249	BestValid 0.9371
Node 1, Pre/Post-Pipelining: 8.587 / 18.064 ms, Bubble: 0.953 ms, Compute: 884.551 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 8.590 / 18.058 ms, Bubble: 0.463 ms, Compute: 885.048 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 0, Pre/Post-Pipelining: 8.585 / 18.054 ms, Bubble: 1.071 ms, Compute: 884.448 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 3, Pre/Post-Pipelining: 8.593 / 18.102 ms, Bubble: 0.065 ms, Compute: 885.388 ms, Comm: 0.010 ms, Imbalance: 0.020 ms
Node 4, Pre/Post-Pipelining: 8.591 / 18.063 ms, Bubble: 1.157 ms, Compute: 884.347 ms, Comm: 0.011 ms, Imbalance: 0.018 ms
Node 5, Pre/Post-Pipelining: 8.586 / 18.038 ms, Bubble: 1.086 ms, Compute: 884.447 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 8.590 / 18.062 ms, Bubble: 0.616 ms, Compute: 884.889 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 8.584 / 18.049 ms, Bubble: 1.083 ms, Compute: 884.439 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.585 ms
Cluster-Wide Average, Post-Pipelining Overhead: 18.054 ms
Cluster-Wide Average, Bubble: 1.071 ms
Cluster-Wide Average, Compute: 884.448 ms
Cluster-Wide Average, Communication: 0.010 ms
Cluster-Wide Average, Imbalance: 0.018 ms
Node 0, GPU memory consumption: 22.329 GB
Node 2, GPU memory consumption: 21.526 GB
Node 1, GPU memory consumption: 21.518 GB
Node 6, GPU memory consumption: 21.522 GB
Node 3, GPU memory consumption: 21.505 GB
Node 7, GPU memory consumption: 21.499 GB
Node 4, GPU memory consumption: 21.495 GB
Node 5, GPU memory consumption: 21.518 GB
Node 0, Graph-Level Communication Throughput: 25.428 Gbps, Time: 627.856 ms
Node 4, Graph-Level Communication Throughput: 11.485 Gbps, Time: 732.681 ms
Node 1, Graph-Level Communication Throughput: 23.715 Gbps, Time: 716.059 ms
Node 5, Graph-Level Communication Throughput: 19.021 Gbps, Time: 642.078 ms
Node 2, Graph-Level Communication Throughput: 36.013 Gbps, Time: 491.110 ms
Node 6, Graph-Level Communication Throughput: 26.371 Gbps, Time: 621.183 ms
Node 3, Graph-Level Communication Throughput: 49.363 Gbps, Time: 468.836 ms
Node 7, Graph-Level Communication Throughput: 21.408 Gbps, Time: 635.986 ms
------------------------node id 0,  per-epoch time: 1.403861 s---------------
------------------------node id 1,  per-epoch time: 1.403861 s---------------
------------------------node id 2,  per-epoch time: 1.403861 s---------------
------------------------node id 3,  per-epoch time: 1.403861 s---------------
------------------------node id 4,  per-epoch time: 1.403861 s---------------
------------------------node id 5,  per-epoch time: 1.403861 s---------------
------------------------node id 6,  per-epoch time: 1.403861 s---------------
------------------------node id 7,  per-epoch time: 1.403861 s---------------
************ Profiling Results ************
	Bubble: 518.956272 (ms) (36.98 percentage)
	Compute: 228.508068 (ms) (16.28 percentage)
	GraphCommComputeOverhead: 19.132961 (ms) (1.36 percentage)
	GraphCommNetwork: 616.972300 (ms) (43.96 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.820507 (ms) (1.41 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 14.493 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.9371
Target test_acc: 0.9330
Epoch to reach the target acc: 4374
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
