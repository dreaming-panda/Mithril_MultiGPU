Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.916 seconds.
Building the CSC structure...
        It takes 1.925 seconds.
Building the CSC structure...
        It takes 2.031 seconds.
Building the CSC structure...
        It takes 2.192 seconds.
Building the CSC structure...
        It takes 2.242 seconds.
Building the CSC structure...
        It takes 2.259 seconds.
Building the CSC structure...
        It takes 2.559 seconds.
Building the CSC structure...
        It takes 2.653 seconds.
Building the CSC structure...
        It takes 1.839 seconds.
        It takes 1.834 seconds.
        It takes 1.866 seconds.
        It takes 1.830 seconds.
        It takes 2.250 seconds.
        It takes 2.403 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.292 seconds.
Building the Feature Vector...
        It takes 2.383 seconds.
        It takes 0.306 seconds.
Building the Label Vector...
        It takes 0.302 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.034 seconds.
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.245 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
        It takes 0.287 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.040 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Building the Feature Vector...
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.108 Gbps (per GPU), 504.867 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.070 Gbps (per GPU), 504.563 Gbps (aggregated)
The layer-level communication performance: 63.070 Gbps (per GPU), 504.558 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.035 Gbps (per GPU), 504.283 Gbps (aggregated)
The layer-level communication performance: 63.030 Gbps (per GPU), 504.237 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.004 Gbps (per GPU), 504.029 Gbps (aggregated)
The layer-level communication performance: 62.996 Gbps (per GPU), 503.969 Gbps (aggregated)
The layer-level communication performance: 62.993 Gbps (per GPU), 503.941 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.866 Gbps (per GPU), 1310.931 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.870 Gbps (per GPU), 1310.963 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.868 Gbps (per GPU), 1310.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.860 Gbps (per GPU), 1310.877 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.865 Gbps (per GPU), 1310.922 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.867 Gbps (per GPU), 1310.934 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.862 Gbps (per GPU), 1310.896 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.866 Gbps (per GPU), 1310.926 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.021 Gbps (per GPU), 912.168 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.022 Gbps (per GPU), 912.173 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.022 Gbps (per GPU), 912.173 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.022 Gbps (per GPU), 912.173 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.021 Gbps (per GPU), 912.170 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.019 Gbps (per GPU), 912.152 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.021 Gbps (per GPU), 912.167 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.021 Gbps (per GPU), 912.169 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.951 Gbps (per GPU), 359.606 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.951 Gbps (per GPU), 359.606 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.951 Gbps (per GPU), 359.604 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.950 Gbps (per GPU), 359.598 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.951 Gbps (per GPU), 359.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.951 Gbps (per GPU), 359.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.950 Gbps (per GPU), 359.604 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.951 Gbps (per GPU), 359.605 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.45ms  2.78ms  3.10  8.38K  3.53M
 chk_1  0.76ms  2.82ms  2.97ms  3.92  6.74K  3.60M
 chk_2  0.80ms  2.68ms  2.85ms  3.55  7.27K  3.53M
 chk_3  0.81ms  2.74ms  2.90ms  3.59  7.92K  3.61M
 chk_4  0.64ms  2.65ms  2.82ms  4.42  5.33K  3.68M
 chk_5  1.02ms  2.66ms  2.82ms  2.78 10.07K  3.45M
 chk_6  0.97ms  2.81ms  2.99ms  3.08  9.41K  3.48M
 chk_7  0.83ms  2.66ms  2.81ms  3.40  8.12K  3.60M
 chk_8  0.69ms  2.77ms  2.91ms  4.24  6.09K  3.64M
 chk_9  1.11ms  2.56ms  2.78ms  2.51 11.10K  3.38M
chk_10  0.66ms  2.80ms  2.95ms  4.49  5.67K  3.63M
chk_11  0.83ms  2.64ms  2.82ms  3.40  8.16K  3.54M
chk_12  0.80ms  2.87ms  3.01ms  3.76  7.24K  3.55M
chk_13  0.64ms  2.70ms  2.84ms  4.41  5.41K  3.68M
chk_14  0.79ms  2.91ms  3.09ms  3.92  7.14K  3.53M
chk_15  0.96ms  2.78ms  2.97ms  3.09  9.25K  3.49M
chk_16  0.60ms  2.61ms  2.76ms  4.58  4.78K  3.77M
chk_17  0.77ms  2.73ms  2.91ms  3.78  6.85K  3.60M
chk_18  0.82ms  2.55ms  2.70ms  3.31  7.47K  3.57M
chk_19  0.61ms  2.61ms  2.77ms  4.53  4.88K  3.75M
chk_20  0.78ms  2.61ms  2.79ms  3.59  7.00K  3.63M
chk_21  0.64ms  2.61ms  2.77ms  4.34  5.41K  3.68M
chk_22  1.11ms  2.82ms  3.30ms  2.98 11.07K  3.39M
chk_23  0.80ms  2.72ms  2.87ms  3.59  7.23K  3.64M
chk_24  1.02ms  2.79ms  2.99ms  2.93 10.13K  3.43M
chk_25  0.74ms  2.58ms  2.75ms  3.73  6.40K  3.57M
chk_26  0.67ms  2.80ms  2.94ms  4.41  5.78K  3.55M
chk_27  0.97ms  2.67ms  2.91ms  3.01  9.34K  3.48M
chk_28  0.73ms  2.97ms  3.13ms  4.27  6.37K  3.57M
chk_29  0.64ms  2.78ms  2.92ms  4.60  5.16K  3.78M
chk_30  0.64ms  2.65ms  2.83ms  4.42  5.44K  3.67M
chk_31  0.73ms  2.80ms  2.99ms  4.11  6.33K  3.63M
   Avg  0.80  2.71  2.90
   Max  1.11  2.97  3.30
   Min  0.60  2.45  2.70
 Ratio  1.83  1.21  1.22
   Var  0.02  0.01  0.02
Profiling takes 2.455 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 372.569 ms
Partition 0 [0, 5) has cost: 372.569 ms
Partition 1 [5, 9) has cost: 347.122 ms
Partition 2 [9, 13) has cost: 347.122 ms
Partition 3 [13, 17) has cost: 347.122 ms
Partition 4 [17, 21) has cost: 347.122 ms
Partition 5 [21, 25) has cost: 347.122 ms
Partition 6 [25, 29) has cost: 347.122 ms
Partition 7 [29, 33) has cost: 353.043 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 166.223 ms
GPU 0, Compute+Comm Time: 134.235 ms, Bubble Time: 29.330 ms, Imbalance Overhead: 2.658 ms
GPU 1, Compute+Comm Time: 127.092 ms, Bubble Time: 28.979 ms, Imbalance Overhead: 10.151 ms
GPU 2, Compute+Comm Time: 127.092 ms, Bubble Time: 28.998 ms, Imbalance Overhead: 10.132 ms
GPU 3, Compute+Comm Time: 127.092 ms, Bubble Time: 28.860 ms, Imbalance Overhead: 10.270 ms
GPU 4, Compute+Comm Time: 127.092 ms, Bubble Time: 28.793 ms, Imbalance Overhead: 10.337 ms
GPU 5, Compute+Comm Time: 127.092 ms, Bubble Time: 28.839 ms, Imbalance Overhead: 10.292 ms
GPU 6, Compute+Comm Time: 127.092 ms, Bubble Time: 29.090 ms, Imbalance Overhead: 10.041 ms
GPU 7, Compute+Comm Time: 128.321 ms, Bubble Time: 29.455 ms, Imbalance Overhead: 8.447 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 322.476 ms
GPU 0, Compute+Comm Time: 248.348 ms, Bubble Time: 57.448 ms, Imbalance Overhead: 16.681 ms
GPU 1, Compute+Comm Time: 243.656 ms, Bubble Time: 56.668 ms, Imbalance Overhead: 22.153 ms
GPU 2, Compute+Comm Time: 243.656 ms, Bubble Time: 56.072 ms, Imbalance Overhead: 22.748 ms
GPU 3, Compute+Comm Time: 243.656 ms, Bubble Time: 55.991 ms, Imbalance Overhead: 22.829 ms
GPU 4, Compute+Comm Time: 243.656 ms, Bubble Time: 56.046 ms, Imbalance Overhead: 22.775 ms
GPU 5, Compute+Comm Time: 243.656 ms, Bubble Time: 56.195 ms, Imbalance Overhead: 22.625 ms
GPU 6, Compute+Comm Time: 243.656 ms, Bubble Time: 56.016 ms, Imbalance Overhead: 22.804 ms
GPU 7, Compute+Comm Time: 261.960 ms, Bubble Time: 56.649 ms, Imbalance Overhead: 3.868 ms
The estimated cost of the whole pipeline: 513.134 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 719.692 ms
Partition 0 [0, 9) has cost: 719.692 ms
Partition 1 [9, 17) has cost: 694.245 ms
Partition 2 [17, 25) has cost: 694.245 ms
Partition 3 [25, 33) has cost: 700.165 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 174.246 ms
GPU 0, Compute+Comm Time: 144.646 ms, Bubble Time: 26.613 ms, Imbalance Overhead: 2.987 ms
GPU 1, Compute+Comm Time: 140.691 ms, Bubble Time: 26.769 ms, Imbalance Overhead: 6.786 ms
GPU 2, Compute+Comm Time: 140.691 ms, Bubble Time: 26.642 ms, Imbalance Overhead: 6.912 ms
GPU 3, Compute+Comm Time: 141.268 ms, Bubble Time: 26.977 ms, Imbalance Overhead: 6.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 325.023 ms
GPU 0, Compute+Comm Time: 262.758 ms, Bubble Time: 50.369 ms, Imbalance Overhead: 11.895 ms
GPU 1, Compute+Comm Time: 260.351 ms, Bubble Time: 49.972 ms, Imbalance Overhead: 14.699 ms
GPU 2, Compute+Comm Time: 260.351 ms, Bubble Time: 50.394 ms, Imbalance Overhead: 14.278 ms
GPU 3, Compute+Comm Time: 270.715 ms, Bubble Time: 50.171 ms, Imbalance Overhead: 4.136 ms
    The estimated cost with 2 DP ways is 524.232 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1413.936 ms
Partition 0 [0, 17) has cost: 1413.936 ms
Partition 1 [17, 33) has cost: 1394.410 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 210.317 ms
GPU 0, Compute+Comm Time: 184.088 ms, Bubble Time: 21.347 ms, Imbalance Overhead: 4.882 ms
GPU 1, Compute+Comm Time: 182.281 ms, Bubble Time: 22.408 ms, Imbalance Overhead: 5.628 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 350.945 ms
GPU 0, Compute+Comm Time: 305.109 ms, Bubble Time: 37.984 ms, Imbalance Overhead: 7.851 ms
GPU 1, Compute+Comm Time: 309.284 ms, Bubble Time: 37.284 ms, Imbalance Overhead: 4.376 ms
    The estimated cost with 4 DP ways is 589.325 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2808.346 ms
Partition 0 [0, 33) has cost: 2808.346 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 461.444 ms
GPU 0, Compute+Comm Time: 461.444 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 588.205 ms
GPU 0, Compute+Comm Time: 588.205 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1102.131 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5524	TrainAcc 0.4433	ValidAcc 0.4620	TestAcc 0.4610	BestValid 0.4620
	Epoch 50:	Loss 1.8738	TrainAcc 0.7017	ValidAcc 0.7173	TestAcc 0.7136	BestValid 0.7173
	Epoch 75:	Loss 1.5056	TrainAcc 0.7530	ValidAcc 0.7647	TestAcc 0.7610	BestValid 0.7647
	Epoch 100:	Loss 1.2748	TrainAcc 0.7934	ValidAcc 0.8057	TestAcc 0.8007	BestValid 0.8057
	Epoch 125:	Loss 1.0974	TrainAcc 0.8271	ValidAcc 0.8369	TestAcc 0.8309	BestValid 0.8369
	Epoch 150:	Loss 1.0045	TrainAcc 0.8478	ValidAcc 0.8555	TestAcc 0.8516	BestValid 0.8555
	Epoch 175:	Loss 0.9325	TrainAcc 0.8608	ValidAcc 0.8669	TestAcc 0.8629	BestValid 0.8669
	Epoch 200:	Loss 0.8770	TrainAcc 0.8702	ValidAcc 0.8763	TestAcc 0.8706	BestValid 0.8763
	Epoch 225:	Loss 0.8287	TrainAcc 0.8759	ValidAcc 0.8824	TestAcc 0.8760	BestValid 0.8824
	Epoch 250:	Loss 0.7966	TrainAcc 0.8803	ValidAcc 0.8861	TestAcc 0.8806	BestValid 0.8861
	Epoch 275:	Loss 0.7685	TrainAcc 0.8824	ValidAcc 0.8882	TestAcc 0.8837	BestValid 0.8882
	Epoch 300:	Loss 0.7452	TrainAcc 0.8879	ValidAcc 0.8919	TestAcc 0.8878	BestValid 0.8919
	Epoch 325:	Loss 0.7201	TrainAcc 0.8908	ValidAcc 0.8943	TestAcc 0.8908	BestValid 0.8943
	Epoch 350:	Loss 0.7005	TrainAcc 0.8943	ValidAcc 0.8978	TestAcc 0.8949	BestValid 0.8978
	Epoch 375:	Loss 0.6874	TrainAcc 0.8966	ValidAcc 0.9002	TestAcc 0.8971	BestValid 0.9002
	Epoch 400:	Loss 0.6724	TrainAcc 0.8991	ValidAcc 0.9026	TestAcc 0.8994	BestValid 0.9026
	Epoch 425:	Loss 0.6609	TrainAcc 0.9000	ValidAcc 0.9027	TestAcc 0.8996	BestValid 0.9027
	Epoch 450:	Loss 0.6495	TrainAcc 0.9030	ValidAcc 0.9058	TestAcc 0.9021	BestValid 0.9058
	Epoch 475:	Loss 0.6384	TrainAcc 0.9044	ValidAcc 0.9064	TestAcc 0.9034	BestValid 0.9064
	Epoch 500:	Loss 0.6280	TrainAcc 0.9066	ValidAcc 0.9086	TestAcc 0.9045	BestValid 0.9086
	Epoch 525:	Loss 0.6198	TrainAcc 0.9089	ValidAcc 0.9102	TestAcc 0.9065	BestValid 0.9102
	Epoch 550:	Loss 0.6143	TrainAcc 0.9092	ValidAcc 0.9103	TestAcc 0.9069	BestValid 0.9103
	Epoch 575:	Loss 0.6013	TrainAcc 0.9114	ValidAcc 0.9120	TestAcc 0.9086	BestValid 0.9120
	Epoch 600:	Loss 0.5975	TrainAcc 0.9135	ValidAcc 0.9140	TestAcc 0.9104	BestValid 0.9140
	Epoch 625:	Loss 0.5888	TrainAcc 0.9138	ValidAcc 0.9139	TestAcc 0.9105	BestValid 0.9140
	Epoch 650:	Loss 0.5809	TrainAcc 0.9142	ValidAcc 0.9141	TestAcc 0.9108	BestValid 0.9141
	Epoch 675:	Loss 0.5752	TrainAcc 0.9156	ValidAcc 0.9150	TestAcc 0.9121	BestValid 0.9150
	Epoch 700:	Loss 0.5725	TrainAcc 0.9174	ValidAcc 0.9167	TestAcc 0.9135	BestValid 0.9167
	Epoch 725:	Loss 0.5678	TrainAcc 0.9176	ValidAcc 0.9166	TestAcc 0.9135	BestValid 0.9167
	Epoch 750:	Loss 0.5606	TrainAcc 0.9187	ValidAcc 0.9179	TestAcc 0.9145	BestValid 0.9179
	Epoch 775:	Loss 0.5601	TrainAcc 0.9202	ValidAcc 0.9189	TestAcc 0.9159	BestValid 0.9189
	Epoch 800:	Loss 0.5518	TrainAcc 0.9208	ValidAcc 0.9196	TestAcc 0.9165	BestValid 0.9196
	Epoch 825:	Loss 0.5500	TrainAcc 0.9214	ValidAcc 0.9196	TestAcc 0.9171	BestValid 0.9196
	Epoch 850:	Loss 0.5465	TrainAcc 0.9220	ValidAcc 0.9206	TestAcc 0.9178	BestValid 0.9206
	Epoch 875:	Loss 0.5381	TrainAcc 0.9232	ValidAcc 0.9212	TestAcc 0.9181	BestValid 0.9212
	Epoch 900:	Loss 0.5331	TrainAcc 0.9235	ValidAcc 0.9210	TestAcc 0.9184	BestValid 0.9212
	Epoch 925:	Loss 0.5331	TrainAcc 0.9241	ValidAcc 0.9219	TestAcc 0.9194	BestValid 0.9219
	Epoch 950:	Loss 0.5316	TrainAcc 0.9248	ValidAcc 0.9226	TestAcc 0.9195	BestValid 0.9226
	Epoch 975:	Loss 0.5288	TrainAcc 0.9265	ValidAcc 0.9237	TestAcc 0.9216	BestValid 0.9237
	Epoch 1000:	Loss 0.5229	TrainAcc 0.9263	ValidAcc 0.9237	TestAcc 0.9208	BestValid 0.9237
	Epoch 1025:	Loss 0.5194	TrainAcc 0.9262	ValidAcc 0.9235	TestAcc 0.9210	BestValid 0.9237
	Epoch 1050:	Loss 0.5161	TrainAcc 0.9277	ValidAcc 0.9253	TestAcc 0.9225	BestValid 0.9253
	Epoch 1075:	Loss 0.5142	TrainAcc 0.9277	ValidAcc 0.9248	TestAcc 0.9226	BestValid 0.9253
	Epoch 1100:	Loss 0.5134	TrainAcc 0.9290	ValidAcc 0.9260	TestAcc 0.9236	BestValid 0.9260
	Epoch 1125:	Loss 0.5105	TrainAcc 0.9285	ValidAcc 0.9258	TestAcc 0.9233	BestValid 0.9260
	Epoch 1150:	Loss 0.5091	TrainAcc 0.9286	ValidAcc 0.9258	TestAcc 0.9231	BestValid 0.9260
	Epoch 1175:	Loss 0.5070	TrainAcc 0.9297	ValidAcc 0.9265	TestAcc 0.9238	BestValid 0.9265
	Epoch 1200:	Loss 0.5013	TrainAcc 0.9299	ValidAcc 0.9271	TestAcc 0.9244	BestValid 0.9271
	Epoch 1225:	Loss 0.4999	TrainAcc 0.9299	ValidAcc 0.9262	TestAcc 0.9245	BestValid 0.9271
	Epoch 1250:	Loss 0.4984	TrainAcc 0.9308	ValidAcc 0.9276	TestAcc 0.9255	BestValid 0.9276
	Epoch 1275:	Loss 0.4969	TrainAcc 0.9307	ValidAcc 0.9275	TestAcc 0.9252	BestValid 0.9276
	Epoch 1300:	Loss 0.4949	TrainAcc 0.9312	ValidAcc 0.9280	TestAcc 0.9260	BestValid 0.9280
	Epoch 1325:	Loss 0.4925	TrainAcc 0.9317	ValidAcc 0.9278	TestAcc 0.9262	BestValid 0.9280
	Epoch 1350:	Loss 0.4883	TrainAcc 0.9324	ValidAcc 0.9283	TestAcc 0.9264	BestValid 0.9283
	Epoch 1375:	Loss 0.4869	TrainAcc 0.9323	ValidAcc 0.9289	TestAcc 0.9267	BestValid 0.9289
	Epoch 1400:	Loss 0.4860	TrainAcc 0.9316	ValidAcc 0.9278	TestAcc 0.9253	BestValid 0.9289
	Epoch 1425:	Loss 0.4896	TrainAcc 0.9326	ValidAcc 0.9290	TestAcc 0.9270	BestValid 0.9290
	Epoch 1450:	Loss 0.4833	TrainAcc 0.9343	ValidAcc 0.9308	TestAcc 0.9286	BestValid 0.9308
	Epoch 1475:	Loss 0.4828	TrainAcc 0.9325	ValidAcc 0.9280	TestAcc 0.9263	BestValid 0.9308
	Epoch 1500:	Loss 0.4797	TrainAcc 0.9337	ValidAcc 0.9296	TestAcc 0.9277	BestValid 0.9308
	Epoch 1525:	Loss 0.4738	TrainAcc 0.9338	ValidAcc 0.9298	TestAcc 0.9280	BestValid 0.9308
	Epoch 1550:	Loss 0.4762	TrainAcc 0.9343	ValidAcc 0.9299	TestAcc 0.9287	BestValid 0.9308
	Epoch 1575:	Loss 0.4744	TrainAcc 0.9346	ValidAcc 0.9299	TestAcc 0.9288	BestValid 0.9308
	Epoch 1600:	Loss 0.4730	TrainAcc 0.9357	ValidAcc 0.9313	TestAcc 0.9298	BestValid 0.9313
	Epoch 1625:	Loss 0.4769	TrainAcc 0.9339	ValidAcc 0.9291	TestAcc 0.9279	BestValid 0.9313
	Epoch 1650:	Loss 0.4727	TrainAcc 0.9346	ValidAcc 0.9295	TestAcc 0.9286	BestValid 0.9313
	Epoch 1675:	Loss 0.4729	TrainAcc 0.9361	ValidAcc 0.9316	TestAcc 0.9303	BestValid 0.9316
	Epoch 1700:	Loss 0.4705	TrainAcc 0.9358	ValidAcc 0.9309	TestAcc 0.9297	BestValid 0.9316
	Epoch 1725:	Loss 0.4658	TrainAcc 0.9362	ValidAcc 0.9315	TestAcc 0.9301	BestValid 0.9316
	Epoch 1750:	Loss 0.4663	TrainAcc 0.9351	ValidAcc 0.9306	TestAcc 0.9290	BestValid 0.9316
	Epoch 1775:	Loss 0.4652	TrainAcc 0.9364	ValidAcc 0.9316	TestAcc 0.9303	BestValid 0.9316
	Epoch 1800:	Loss 0.4615	TrainAcc 0.9362	ValidAcc 0.9308	TestAcc 0.9302	BestValid 0.9316
	Epoch 1825:	Loss 0.4620	TrainAcc 0.9366	ValidAcc 0.9322	TestAcc 0.9304	BestValid 0.9322
	Epoch 1850:	Loss 0.4620	TrainAcc 0.9366	ValidAcc 0.9312	TestAcc 0.9302	BestValid 0.9322
	Epoch 1875:	Loss 0.4578	TrainAcc 0.9370	ValidAcc 0.9323	TestAcc 0.9313	BestValid 0.9323
	Epoch 1900:	Loss 0.4585	TrainAcc 0.9371	ValidAcc 0.9325	TestAcc 0.9310	BestValid 0.9325
	Epoch 1925:	Loss 0.4605	TrainAcc 0.9374	ValidAcc 0.9329	TestAcc 0.9312	BestValid 0.9329
	Epoch 1950:	Loss 0.4532	TrainAcc 0.9372	ValidAcc 0.9324	TestAcc 0.9311	BestValid 0.9329
	Epoch 1975:	Loss 0.4542	TrainAcc 0.9380	ValidAcc 0.9327	TestAcc 0.9317	BestValid 0.9329
	Epoch 2000:	Loss 0.4520	TrainAcc 0.9374	ValidAcc 0.9320	TestAcc 0.9314	BestValid 0.9329
	Epoch 2025:	Loss 0.4517	TrainAcc 0.9380	ValidAcc 0.9331	TestAcc 0.9317	BestValid 0.9331
	Epoch 2050:	Loss 0.4532	TrainAcc 0.9391	ValidAcc 0.9337	TestAcc 0.9328	BestValid 0.9337
	Epoch 2075:	Loss 0.4523	TrainAcc 0.9380	ValidAcc 0.9321	TestAcc 0.9315	BestValid 0.9337
	Epoch 2100:	Loss 0.4494	TrainAcc 0.9380	ValidAcc 0.9323	TestAcc 0.9315	BestValid 0.9337
	Epoch 2125:	Loss 0.4508	TrainAcc 0.9389	ValidAcc 0.9332	TestAcc 0.9324	BestValid 0.9337
	Epoch 2150:	Loss 0.4495	TrainAcc 0.9393	ValidAcc 0.9334	TestAcc 0.9326	BestValid 0.9337
	Epoch 2175:	Loss 0.4461	TrainAcc 0.9387	ValidAcc 0.9327	TestAcc 0.9320	BestValid 0.9337
	Epoch 2200:	Loss 0.4471	TrainAcc 0.9391	ValidAcc 0.9335	TestAcc 0.9325	BestValid 0.9337
	Epoch 2225:	Loss 0.4482	TrainAcc 0.9389	ValidAcc 0.9336	TestAcc 0.9319	BestValid 0.9337
	Epoch 2250:	Loss 0.4477	TrainAcc 0.9407	ValidAcc 0.9349	TestAcc 0.9339	BestValid 0.9349
	Epoch 2275:	Loss 0.4468	TrainAcc 0.9402	ValidAcc 0.9342	TestAcc 0.9334	BestValid 0.9349
	Epoch 2300:	Loss 0.4460	TrainAcc 0.9393	ValidAcc 0.9329	TestAcc 0.9323	BestValid 0.9349
	Epoch 2325:	Loss 0.4445	TrainAcc 0.9400	ValidAcc 0.9340	TestAcc 0.9333	BestValid 0.9349
	Epoch 2350:	Loss 0.4421	TrainAcc 0.9392	ValidAcc 0.9329	TestAcc 0.9325	BestValid 0.9349
	Epoch 2375:	Loss 0.4420	TrainAcc 0.9406	ValidAcc 0.9348	TestAcc 0.9333	BestValid 0.9349
	Epoch 2400:	Loss 0.4408	TrainAcc 0.9407	ValidAcc 0.9353	TestAcc 0.9338	BestValid 0.9353
	Epoch 2425:	Loss 0.4391	TrainAcc 0.9403	ValidAcc 0.9340	TestAcc 0.9334	BestValid 0.9353
	Epoch 2450:	Loss 0.4421	TrainAcc 0.9411	ValidAcc 0.9353	TestAcc 0.9340	BestValid 0.9353
	Epoch 2475:	Loss 0.4378	TrainAcc 0.9415	ValidAcc 0.9360	TestAcc 0.9346	BestValid 0.9360
	Epoch 2500:	Loss 0.4362	TrainAcc 0.9412	ValidAcc 0.9353	TestAcc 0.9342	BestValid 0.9360
	Epoch 2525:	Loss 0.4341	TrainAcc 0.9408	ValidAcc 0.9348	TestAcc 0.9341	BestValid 0.9360
	Epoch 2550:	Loss 0.4395	TrainAcc 0.9424	ValidAcc 0.9367	TestAcc 0.9354	BestValid 0.9367
	Epoch 2575:	Loss 0.4310	TrainAcc 0.9415	ValidAcc 0.9353	TestAcc 0.9344	BestValid 0.9367
	Epoch 2600:	Loss 0.4335	TrainAcc 0.9419	ValidAcc 0.9359	TestAcc 0.9343	BestValid 0.9367
	Epoch 2625:	Loss 0.4306	TrainAcc 0.9420	ValidAcc 0.9358	TestAcc 0.9346	BestValid 0.9367
	Epoch 2650:	Loss 0.4334	TrainAcc 0.9411	ValidAcc 0.9347	TestAcc 0.9342	BestValid 0.9367
	Epoch 2675:	Loss 0.4317	TrainAcc 0.9422	ValidAcc 0.9362	TestAcc 0.9352	BestValid 0.9367
	Epoch 2700:	Loss 0.4316	TrainAcc 0.9417	ValidAcc 0.9357	TestAcc 0.9346	BestValid 0.9367
	Epoch 2725:	Loss 0.4305	TrainAcc 0.9429	ValidAcc 0.9373	TestAcc 0.9356	BestValid 0.9373
	Epoch 2750:	Loss 0.4314	TrainAcc 0.9427	ValidAcc 0.9370	TestAcc 0.9354	BestValid 0.9373
	Epoch 2775:	Loss 0.4271	TrainAcc 0.9424	ValidAcc 0.9360	TestAcc 0.9356	BestValid 0.9373
	Epoch 2800:	Loss 0.4263	TrainAcc 0.9420	ValidAcc 0.9360	TestAcc 0.9348	BestValid 0.9373
	Epoch 2825:	Loss 0.4290	TrainAcc 0.9421	ValidAcc 0.9365	TestAcc 0.9345	BestValid 0.9373
	Epoch 2850:	Loss 0.4269	TrainAcc 0.9424	ValidAcc 0.9368	TestAcc 0.9346	BestValid 0.9373
	Epoch 2875:	Loss 0.4260	TrainAcc 0.9432	ValidAcc 0.9374	TestAcc 0.9362	BestValid 0.9374
	Epoch 2900:	Loss 0.4250	TrainAcc 0.9421	ValidAcc 0.9365	TestAcc 0.9348	BestValid 0.9374
	Epoch 2925:	Loss 0.4238	TrainAcc 0.9432	ValidAcc 0.9377	TestAcc 0.9361	BestValid 0.9377
	Epoch 2950:	Loss 0.4247	TrainAcc 0.9434	ValidAcc 0.9374	TestAcc 0.9360	BestValid 0.9377
	Epoch 2975:	Loss 0.4242	TrainAcc 0.9437	ValidAcc 0.9380	TestAcc 0.9366	BestValid 0.9380
	Epoch 3000:	Loss 0.4216	TrainAcc 0.9427	ValidAcc 0.9368	TestAcc 0.9355	BestValid 0.9380
	Epoch 3025:	Loss 0.4211	TrainAcc 0.9435	ValidAcc 0.9374	TestAcc 0.9363	BestValid 0.9380
	Epoch 3050:	Loss 0.4225	TrainAcc 0.9440	ValidAcc 0.9376	TestAcc 0.9369	BestValid 0.9380
	Epoch 3075:	Loss 0.4229	TrainAcc 0.9432	ValidAcc 0.9373	TestAcc 0.9357	BestValid 0.9380
	Epoch 3100:	Loss 0.4192	TrainAcc 0.9434	ValidAcc 0.9366	TestAcc 0.9360	BestValid 0.9380
	Epoch 3125:	Loss 0.4241	TrainAcc 0.9436	ValidAcc 0.9372	TestAcc 0.9361	BestValid 0.9380
	Epoch 3150:	Loss 0.4201	TrainAcc 0.9440	ValidAcc 0.9376	TestAcc 0.9364	BestValid 0.9380
	Epoch 3175:	Loss 0.4182	TrainAcc 0.9449	ValidAcc 0.9390	TestAcc 0.9377	BestValid 0.9390
	Epoch 3200:	Loss 0.4197	TrainAcc 0.9442	ValidAcc 0.9378	TestAcc 0.9369	BestValid 0.9390
	Epoch 3225:	Loss 0.4180	TrainAcc 0.9440	ValidAcc 0.9386	TestAcc 0.9368	BestValid 0.9390
	Epoch 3250:	Loss 0.4161	TrainAcc 0.9442	ValidAcc 0.9377	TestAcc 0.9369	BestValid 0.9390
	Epoch 3275:	Loss 0.4137	TrainAcc 0.9454	ValidAcc 0.9397	TestAcc 0.9380	BestValid 0.9397
	Epoch 3300:	Loss 0.4129	TrainAcc 0.9440	ValidAcc 0.9377	TestAcc 0.9365	BestValid 0.9397
	Epoch 3325:	Loss 0.4197	TrainAcc 0.9435	ValidAcc 0.9377	TestAcc 0.9357	BestValid 0.9397
	Epoch 3350:	Loss 0.4164	TrainAcc 0.9457	ValidAcc 0.9397	TestAcc 0.9384	BestValid 0.9397
	Epoch 3375:	Loss 0.4148	TrainAcc 0.9448	ValidAcc 0.9393	TestAcc 0.9375	BestValid 0.9397
	Epoch 3400:	Loss 0.4157	TrainAcc 0.9438	ValidAcc 0.9374	TestAcc 0.9364	BestValid 0.9397
	Epoch 3425:	Loss 0.4167	TrainAcc 0.9445	ValidAcc 0.9384	TestAcc 0.9373	BestValid 0.9397
	Epoch 3450:	Loss 0.4138	TrainAcc 0.9450	ValidAcc 0.9382	TestAcc 0.9376	BestValid 0.9397
	Epoch 3475:	Loss 0.4147	TrainAcc 0.9449	ValidAcc 0.9387	TestAcc 0.9375	BestValid 0.9397
	Epoch 3500:	Loss 0.4112	TrainAcc 0.9451	ValidAcc 0.9388	TestAcc 0.9377	BestValid 0.9397
	Epoch 3525:	Loss 0.4113	TrainAcc 0.9446	ValidAcc 0.9377	TestAcc 0.9370	BestValid 0.9397
	Epoch 3550:	Loss 0.4107	TrainAcc 0.9443	ValidAcc 0.9387	TestAcc 0.9372	BestValid 0.9397
	Epoch 3575:	Loss 0.4147	TrainAcc 0.9460	ValidAcc 0.9397	TestAcc 0.9384	BestValid 0.9397
	Epoch 3600:	Loss 0.4109	TrainAcc 0.9457	ValidAcc 0.9390	TestAcc 0.9381	BestValid 0.9397
	Epoch 3625:	Loss 0.4101	TrainAcc 0.9451	ValidAcc 0.9389	TestAcc 0.9377	BestValid 0.9397
	Epoch 3650:	Loss 0.4126	TrainAcc 0.9459	ValidAcc 0.9392	TestAcc 0.9387	BestValid 0.9397
	Epoch 3675:	Loss 0.4093	TrainAcc 0.9463	ValidAcc 0.9403	TestAcc 0.9385	BestValid 0.9403
	Epoch 3700:	Loss 0.4106	TrainAcc 0.9460	ValidAcc 0.9397	TestAcc 0.9387	BestValid 0.9403
	Epoch 3725:	Loss 0.4096	TrainAcc 0.9462	ValidAcc 0.9400	TestAcc 0.9391	BestValid 0.9403
	Epoch 3750:	Loss 0.4070	TrainAcc 0.9461	ValidAcc 0.9403	TestAcc 0.9386	BestValid 0.9403
	Epoch 3775:	Loss 0.4051	TrainAcc 0.9455	ValidAcc 0.9394	TestAcc 0.9382	BestValid 0.9403
	Epoch 3800:	Loss 0.4074	TrainAcc 0.9456	ValidAcc 0.9394	TestAcc 0.9384	BestValid 0.9403
	Epoch 3825:	Loss 0.4078	TrainAcc 0.9460	ValidAcc 0.9394	TestAcc 0.9385	BestValid 0.9403
	Epoch 3850:	Loss 0.4070	TrainAcc 0.9473	ValidAcc 0.9414	TestAcc 0.9398	BestValid 0.9414
	Epoch 3875:	Loss 0.4075	TrainAcc 0.9469	ValidAcc 0.9410	TestAcc 0.9395	BestValid 0.9414
	Epoch 3900:	Loss 0.4051	TrainAcc 0.9465	ValidAcc 0.9402	TestAcc 0.9390	BestValid 0.9414
	Epoch 3925:	Loss 0.4060	TrainAcc 0.9465	ValidAcc 0.9399	TestAcc 0.9390	BestValid 0.9414
	Epoch 3950:	Loss 0.4051	TrainAcc 0.9471	ValidAcc 0.9410	TestAcc 0.9393	BestValid 0.9414
	Epoch 3975:	Loss 0.4042	TrainAcc 0.9459	ValidAcc 0.9402	TestAcc 0.9382	BestValid 0.9414
	Epoch 4000:	Loss 0.4010	TrainAcc 0.9465	ValidAcc 0.9405	TestAcc 0.9391	BestValid 0.9414
	Epoch 4025:	Loss 0.4069	TrainAcc 0.9470	ValidAcc 0.9404	TestAcc 0.9389	BestValid 0.9414
	Epoch 4050:	Loss 0.4060	TrainAcc 0.9480	ValidAcc 0.9423	TestAcc 0.9403	BestValid 0.9423
	Epoch 4075:	Loss 0.4049	TrainAcc 0.9478	ValidAcc 0.9419	TestAcc 0.9400	BestValid 0.9423
	Epoch 4100:	Loss 0.4002	TrainAcc 0.9465	ValidAcc 0.9403	TestAcc 0.9385	BestValid 0.9423
	Epoch 4125:	Loss 0.4041	TrainAcc 0.9467	ValidAcc 0.9406	TestAcc 0.9389	BestValid 0.9423
	Epoch 4150:	Loss 0.3985	TrainAcc 0.9483	ValidAcc 0.9434	TestAcc 0.9406	BestValid 0.9434
	Epoch 4175:	Loss 0.3994	TrainAcc 0.9469	ValidAcc 0.9404	TestAcc 0.9391	BestValid 0.9434
	Epoch 4200:	Loss 0.4004	TrainAcc 0.9468	ValidAcc 0.9412	TestAcc 0.9395	BestValid 0.9434
	Epoch 4225:	Loss 0.3985	TrainAcc 0.9465	ValidAcc 0.9400	TestAcc 0.9392	BestValid 0.9434
	Epoch 4250:	Loss 0.3977	TrainAcc 0.9470	ValidAcc 0.9405	TestAcc 0.9393	BestValid 0.9434
	Epoch 4275:	Loss 0.3966	TrainAcc 0.9471	ValidAcc 0.9411	TestAcc 0.9393	BestValid 0.9434
	Epoch 4300:	Loss 0.3963	TrainAcc 0.9471	ValidAcc 0.9407	TestAcc 0.9393	BestValid 0.9434
	Epoch 4325:	Loss 0.3999	TrainAcc 0.9475	ValidAcc 0.9415	TestAcc 0.9399	BestValid 0.9434
	Epoch 4350:	Loss 0.3984	TrainAcc 0.9484	ValidAcc 0.9424	TestAcc 0.9406	BestValid 0.9434
	Epoch 4375:	Loss 0.3937	TrainAcc 0.9473	ValidAcc 0.9414	TestAcc 0.9397	BestValid 0.9434
	Epoch 4400:	Loss 0.3952	TrainAcc 0.9487	ValidAcc 0.9425	TestAcc 0.9407	BestValid 0.9434
	Epoch 4425:	Loss 0.3959	TrainAcc 0.9477	ValidAcc 0.9417	TestAcc 0.9399	BestValid 0.9434
	Epoch 4450:	Loss 0.3930	TrainAcc 0.9485	ValidAcc 0.9421	TestAcc 0.9405	BestValid 0.9434
	Epoch 4475:	Loss 0.3963	TrainAcc 0.9481	ValidAcc 0.9421	TestAcc 0.9406	BestValid 0.9434
	Epoch 4500:	Loss 0.3927	TrainAcc 0.9478	ValidAcc 0.9417	TestAcc 0.9402	BestValid 0.9434
	Epoch 4525:	Loss 0.3927	TrainAcc 0.9479	ValidAcc 0.9424	TestAcc 0.9406	BestValid 0.9434
	Epoch 4550:	Loss 0.3934	TrainAcc 0.9479	ValidAcc 0.9418	TestAcc 0.9399	BestValid 0.9434
	Epoch 4575:	Loss 0.3919	TrainAcc 0.9483	ValidAcc 0.9420	TestAcc 0.9408	BestValid 0.9434
	Epoch 4600:	Loss 0.3912	TrainAcc 0.9490	ValidAcc 0.9432	TestAcc 0.9414	BestValid 0.9434
	Epoch 4625:	Loss 0.3923	TrainAcc 0.9484	ValidAcc 0.9417	TestAcc 0.9410	BestValid 0.9434
	Epoch 4650:	Loss 0.3919	TrainAcc 0.9490	ValidAcc 0.9433	TestAcc 0.9412	BestValid 0.9434
	Epoch 4675:	Loss 0.3932	TrainAcc 0.9490	ValidAcc 0.9432	TestAcc 0.9411	BestValid 0.9434
	Epoch 4700:	Loss 0.3905	TrainAcc 0.9481	ValidAcc 0.9425	TestAcc 0.9406	BestValid 0.9434
	Epoch 4725:	Loss 0.3923	TrainAcc 0.9485	ValidAcc 0.9428	TestAcc 0.9407	BestValid 0.9434
	Epoch 4750:	Loss 0.3926	TrainAcc 0.9488	ValidAcc 0.9430	TestAcc 0.9411	BestValid 0.9434
	Epoch 4775:	Loss 0.3915	TrainAcc 0.9490	ValidAcc 0.9428	TestAcc 0.9415	BestValid 0.9434
	Epoch 4800:	Loss 0.3914	TrainAcc 0.9480	ValidAcc 0.9423	TestAcc 0.9406	BestValid 0.9434
	Epoch 4825:	Loss 0.3892	TrainAcc 0.9488	ValidAcc 0.9431	TestAcc 0.9413	BestValid 0.9434
	Epoch 4850:	Loss 0.3888	TrainAcc 0.9490	ValidAcc 0.9431	TestAcc 0.9415	BestValid 0.9434
	Epoch 4875:	Loss 0.3865	TrainAcc 0.9492	ValidAcc 0.9432	TestAcc 0.9414	BestValid 0.9434
	Epoch 4900:	Loss 0.3869	TrainAcc 0.9493	ValidAcc 0.9438	TestAcc 0.9416	BestValid 0.9438
	Epoch 4925:	Loss 0.3879	TrainAcc 0.9491	ValidAcc 0.9434	TestAcc 0.9416	BestValid 0.9438
	Epoch 4950:	Loss 0.3883	TrainAcc 0.9495	ValidAcc 0.9440	TestAcc 0.9416	BestValid 0.9440
	Epoch 4975:	Loss 0.3852	TrainAcc 0.9492	ValidAcc 0.9434	TestAcc 0.9417	BestValid 0.9440
	Epoch 5000:	Loss 0.3898	TrainAcc 0.9493	ValidAcc 0.9436	TestAcc 0.9416	BestValid 0.9440
Node 0, Pre/Post-Pipelining: 2.162 / 25.589 ms, Bubble: 67.694 ms, Compute: 339.328 ms, Comm: 30.229 ms, Imbalance: 6.487 ms
Node 1, Pre/Post-Pipelining: 2.161 / 25.586 ms, Bubble: 67.743 ms, Compute: 338.587 ms, Comm: 31.197 ms, Imbalance: 6.218 ms
Node 2, Pre/Post-Pipelining: 2.158 / 25.499 ms, Bubble: 67.410 ms, Compute: 317.940 ms, Comm: 35.330 ms, Imbalance: 24.090 ms
Node 3, Pre/Post-Pipelining: 2.155 / 25.549 ms, Bubble: 67.338 ms, Compute: 317.610 ms, Comm: 36.434 ms, Imbalance: 23.332 ms
Node 5, Pre/Post-Pipelining: 2.157 / 25.482 ms, Bubble: 69.372 ms, Compute: 316.340 ms, Comm: 33.926 ms, Imbalance: 26.465 ms
Node 7, Pre/Post-Pipelining: 2.159 / 33.217 ms, Bubble: 63.336 ms, Compute: 330.717 ms, Comm: 27.450 ms, Imbalance: 16.076 ms
Node 4, Pre/Post-Pipelining: 2.158 / 25.532 ms, Bubble: 69.279 ms, Compute: 316.166 ms, Comm: 33.846 ms, Imbalance: 26.691 ms
Node 6, Pre/Post-Pipelining: 2.162 / 33.348 ms, Bubble: 63.204 ms, Compute: 329.473 ms, Comm: 28.489 ms, Imbalance: 16.334 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 2.162 ms
Cluster-Wide Average, Post-Pipelining Overhead: 25.589 ms
Cluster-Wide Average, Bubble: 67.694 ms
Cluster-Wide Average, Compute: 339.328 ms
Cluster-Wide Average, Communication: 30.229 ms
Cluster-Wide Average, Imbalance: 6.487 ms
Node 0, GPU memory consumption: 10.143 GB
Node 2, GPU memory consumption: 7.915 GB
Node 3, GPU memory consumption: 7.891 GB
Node 6, GPU memory consumption: 8.067 GB
Node 7, GPU memory consumption: 8.044 GB
Node 5, GPU memory consumption: 7.915 GB
Node 4, GPU memory consumption: 7.891 GB
Node 1, GPU memory consumption: 8.962 GB
Node 0, Graph-Level Communication Throughput: 100.415 Gbps, Time: 55.803 ms
Node 1, Graph-Level Communication Throughput: 100.479 Gbps, Time: 57.567 ms
Node 2, Graph-Level Communication Throughput: 99.320 Gbps, Time: 56.418 ms
Node 3, Graph-Level Communication Throughput: 110.741 Gbps, Time: 52.233 ms
Node 4, Graph-Level Communication Throughput: 105.842 Gbps, Time: 52.942 ms
Node 5, Graph-Level Communication Throughput: 102.230 Gbps, Time: 56.581 ms
Node 6, Graph-Level Communication Throughput: 107.499 Gbps, Time: 52.126 ms
Node 7, Graph-Level Communication Throughput: 103.637 Gbps, Time: 55.813 ms
------------------------node id 0,  per-epoch time: 0.955299 s---------------
------------------------node id 1,  per-epoch time: 0.955299 s---------------
------------------------node id 2,  per-epoch time: 0.955299 s---------------
------------------------node id 3,  per-epoch time: 0.955299 s---------------
------------------------node id 4,  per-epoch time: 0.955299 s---------------
------------------------node id 5,  per-epoch time: 0.955299 s---------------
------------------------node id 6,  per-epoch time: 0.955299 s---------------
------------------------node id 7,  per-epoch time: 0.955299 s---------------
************ Profiling Results ************
	Bubble: 630.312392 (ms) (66.01 percentage)
	Compute: 253.636535 (ms) (26.56 percentage)
	GraphCommComputeOverhead: 9.711733 (ms) (1.02 percentage)
	GraphCommNetwork: 54.940219 (ms) (5.75 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.292558 (ms) (0.66 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.041 GB
	Graph-level communication (cluster-wide, per-epoch): 5.303 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 6.356 GB
	Aggregated layer-level communication throughput: 278.576 Gbps
Highest valid_acc: 0.9440
Target test_acc: 0.9416
Epoch to reach the target acc: 4949
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
