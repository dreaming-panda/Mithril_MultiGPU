Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.851 seconds.
Building the CSC structure...
        It takes 1.921 seconds.
Building the CSC structure...
        It takes 2.034 seconds.
Building the CSC structure...
        It takes 2.199 seconds.
Building the CSC structure...
        It takes 2.316 seconds.
Building the CSC structure...
        It takes 2.421 seconds.
Building the CSC structure...
        It takes 2.663 seconds.
Building the CSC structure...
        It takes 2.704 seconds.
Building the CSC structure...
        It takes 1.808 seconds.
        It takes 1.841 seconds.
        It takes 1.842 seconds.
        It takes 2.249 seconds.
        It takes 2.149 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.406 seconds.
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 2.332 seconds.
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 2.382 seconds.
        It takes 0.042 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.251 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.254 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.031 seconds.
        It takes 0.270 seconds.
Building the Label Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.039 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.281 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.040 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
        It takes 0.275 seconds.
Building the Label Vector...
Number of vertices per chunk: 7281
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.271 seconds.
Building the Label Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.034 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.879 Gbps (per GPU), 511.028 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.840 Gbps (per GPU), 510.721 Gbps (aggregated)
The layer-level communication performance: 63.839 Gbps (per GPU), 510.712 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.803 Gbps (per GPU), 510.426 Gbps (aggregated)
The layer-level communication performance: 63.798 Gbps (per GPU), 510.381 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.771 Gbps (per GPU), 510.164 Gbps (aggregated)
The layer-level communication performance: 63.764 Gbps (per GPU), 510.112 Gbps (aggregated)
The layer-level communication performance: 63.759 Gbps (per GPU), 510.068 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.106 Gbps (per GPU), 1328.846 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.109 Gbps (per GPU), 1328.869 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.106 Gbps (per GPU), 1328.849 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.104 Gbps (per GPU), 1328.836 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.100 Gbps (per GPU), 1328.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.104 Gbps (per GPU), 1328.836 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.102 Gbps (per GPU), 1328.813 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.104 Gbps (per GPU), 1328.829 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.982 Gbps (per GPU), 919.855 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.983 Gbps (per GPU), 919.860 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.982 Gbps (per GPU), 919.855 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.981 Gbps (per GPU), 919.851 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.982 Gbps (per GPU), 919.853 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.982 Gbps (per GPU), 919.853 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.979 Gbps (per GPU), 919.835 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.981 Gbps (per GPU), 919.846 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.559 Gbps (per GPU), 364.469 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.559 Gbps (per GPU), 364.469 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.559 Gbps (per GPU), 364.470 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.559 Gbps (per GPU), 364.468 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.558 Gbps (per GPU), 364.468 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.559 Gbps (per GPU), 364.469 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.558 Gbps (per GPU), 364.467 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.558 Gbps (per GPU), 364.462 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.45ms  2.74ms  3.09  8.38K  3.53M
 chk_1  0.75ms  2.77ms  2.97ms  3.97  6.74K  3.60M
 chk_2  0.79ms  2.69ms  2.83ms  3.57  7.27K  3.53M
 chk_3  0.80ms  2.72ms  2.87ms  3.59  7.92K  3.61M
 chk_4  0.63ms  2.65ms  2.79ms  4.45  5.33K  3.68M
 chk_5  1.00ms  2.63ms  2.82ms  2.82 10.07K  3.45M
 chk_6  0.96ms  2.80ms  2.97ms  3.10  9.41K  3.48M
 chk_7  0.82ms  2.64ms  2.79ms  3.42  8.12K  3.60M
 chk_8  0.67ms  2.74ms  2.89ms  4.28  6.09K  3.64M
 chk_9  1.10ms  2.57ms  2.76ms  2.52 11.10K  3.38M
chk_10  0.65ms  2.79ms  2.94ms  4.53  5.67K  3.63M
chk_11  0.82ms  2.64ms  2.81ms  3.42  8.16K  3.54M
chk_12  0.79ms  2.82ms  3.00ms  3.77  7.24K  3.55M
chk_13  0.63ms  2.68ms  2.83ms  4.47  5.41K  3.68M
chk_14  0.78ms  2.90ms  3.07ms  3.94  7.14K  3.53M
chk_15  0.95ms  2.77ms  2.95ms  3.11  9.25K  3.49M
chk_16  0.59ms  2.62ms  2.74ms  4.64  4.78K  3.77M
chk_17  0.76ms  2.71ms  2.87ms  3.78  6.85K  3.60M
chk_18  0.81ms  2.53ms  2.68ms  3.33  7.47K  3.57M
chk_19  0.60ms  2.60ms  2.74ms  4.55  4.88K  3.75M
chk_20  0.77ms  2.63ms  2.77ms  3.61  7.00K  3.63M
chk_21  0.63ms  2.60ms  2.75ms  4.37  5.41K  3.68M
chk_22  1.09ms  2.82ms  3.00ms  2.74 11.07K  3.39M
chk_23  0.79ms  2.71ms  2.84ms  3.60  7.23K  3.64M
chk_24  1.01ms  2.77ms  2.95ms  2.93 10.13K  3.43M
chk_25  0.72ms  2.58ms  2.72ms  3.75  6.40K  3.57M
chk_26  0.66ms  2.77ms  2.92ms  4.45  5.78K  3.55M
chk_27  0.95ms  2.65ms  2.88ms  3.01  9.34K  3.48M
chk_28  0.72ms  2.94ms  3.11ms  4.31  6.37K  3.57M
chk_29  0.63ms  2.76ms  2.91ms  4.64  5.16K  3.78M
chk_30  0.63ms  2.64ms  2.79ms  4.41  5.44K  3.67M
chk_31  0.72ms  2.79ms  2.93ms  4.08  6.33K  3.63M
   Avg  0.78  2.70  2.86
   Max  1.10  2.94  3.11
   Min  0.59  2.45  2.68
 Ratio  1.85  1.20  1.16
   Var  0.02  0.01  0.01
Profiling takes 2.433 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 370.463 ms
Partition 0 [0, 5) has cost: 370.463 ms
Partition 1 [5, 9) has cost: 345.358 ms
Partition 2 [9, 13) has cost: 345.358 ms
Partition 3 [13, 17) has cost: 345.358 ms
Partition 4 [17, 21) has cost: 345.358 ms
Partition 5 [21, 25) has cost: 345.358 ms
Partition 6 [25, 29) has cost: 345.358 ms
Partition 7 [29, 33) has cost: 350.659 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.940 ms
GPU 0, Compute+Comm Time: 133.233 ms, Bubble Time: 29.028 ms, Imbalance Overhead: 2.678 ms
GPU 1, Compute+Comm Time: 126.224 ms, Bubble Time: 28.673 ms, Imbalance Overhead: 10.042 ms
GPU 2, Compute+Comm Time: 126.224 ms, Bubble Time: 28.614 ms, Imbalance Overhead: 10.101 ms
GPU 3, Compute+Comm Time: 126.224 ms, Bubble Time: 28.547 ms, Imbalance Overhead: 10.169 ms
GPU 4, Compute+Comm Time: 126.224 ms, Bubble Time: 28.476 ms, Imbalance Overhead: 10.239 ms
GPU 5, Compute+Comm Time: 126.224 ms, Bubble Time: 28.448 ms, Imbalance Overhead: 10.267 ms
GPU 6, Compute+Comm Time: 126.224 ms, Bubble Time: 28.668 ms, Imbalance Overhead: 10.047 ms
GPU 7, Compute+Comm Time: 127.383 ms, Bubble Time: 29.044 ms, Imbalance Overhead: 8.513 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 320.191 ms
GPU 0, Compute+Comm Time: 246.617 ms, Bubble Time: 57.113 ms, Imbalance Overhead: 16.462 ms
GPU 1, Compute+Comm Time: 242.474 ms, Bubble Time: 56.324 ms, Imbalance Overhead: 21.393 ms
GPU 2, Compute+Comm Time: 242.474 ms, Bubble Time: 55.786 ms, Imbalance Overhead: 21.931 ms
GPU 3, Compute+Comm Time: 242.474 ms, Bubble Time: 55.754 ms, Imbalance Overhead: 21.963 ms
GPU 4, Compute+Comm Time: 242.474 ms, Bubble Time: 55.786 ms, Imbalance Overhead: 21.931 ms
GPU 5, Compute+Comm Time: 242.474 ms, Bubble Time: 55.821 ms, Imbalance Overhead: 21.896 ms
GPU 6, Compute+Comm Time: 242.474 ms, Bubble Time: 55.652 ms, Imbalance Overhead: 22.065 ms
GPU 7, Compute+Comm Time: 260.571 ms, Bubble Time: 56.158 ms, Imbalance Overhead: 3.462 ms
The estimated cost of the whole pipeline: 509.387 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 715.821 ms
Partition 0 [0, 9) has cost: 715.821 ms
Partition 1 [9, 17) has cost: 690.716 ms
Partition 2 [17, 25) has cost: 690.716 ms
Partition 3 [25, 33) has cost: 696.017 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 172.893 ms
GPU 0, Compute+Comm Time: 143.369 ms, Bubble Time: 26.352 ms, Imbalance Overhead: 3.173 ms
GPU 1, Compute+Comm Time: 139.480 ms, Bubble Time: 26.319 ms, Imbalance Overhead: 7.095 ms
GPU 2, Compute+Comm Time: 139.480 ms, Bubble Time: 26.158 ms, Imbalance Overhead: 7.255 ms
GPU 3, Compute+Comm Time: 140.024 ms, Bubble Time: 26.549 ms, Imbalance Overhead: 6.321 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 322.412 ms
GPU 0, Compute+Comm Time: 260.513 ms, Bubble Time: 49.884 ms, Imbalance Overhead: 12.014 ms
GPU 1, Compute+Comm Time: 258.430 ms, Bubble Time: 49.541 ms, Imbalance Overhead: 14.442 ms
GPU 2, Compute+Comm Time: 258.430 ms, Bubble Time: 49.908 ms, Imbalance Overhead: 14.074 ms
GPU 3, Compute+Comm Time: 268.683 ms, Bubble Time: 49.704 ms, Imbalance Overhead: 4.025 ms
    The estimated cost with 2 DP ways is 520.071 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1406.537 ms
Partition 0 [0, 17) has cost: 1406.537 ms
Partition 1 [17, 33) has cost: 1386.733 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 208.741 ms
GPU 0, Compute+Comm Time: 182.377 ms, Bubble Time: 21.097 ms, Imbalance Overhead: 5.268 ms
GPU 1, Compute+Comm Time: 180.608 ms, Bubble Time: 21.841 ms, Imbalance Overhead: 6.292 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 348.600 ms
GPU 0, Compute+Comm Time: 302.807 ms, Bubble Time: 37.569 ms, Imbalance Overhead: 8.224 ms
GPU 1, Compute+Comm Time: 307.229 ms, Bubble Time: 36.912 ms, Imbalance Overhead: 4.459 ms
    The estimated cost with 4 DP ways is 585.208 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2793.270 ms
Partition 0 [0, 33) has cost: 2793.270 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 456.010 ms
GPU 0, Compute+Comm Time: 456.010 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 582.164 ms
GPU 0, Compute+Comm Time: 582.164 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1090.083 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5756	TrainAcc 0.4447	ValidAcc 0.4627	TestAcc 0.4624	BestValid 0.4627
	Epoch 50:	Loss 1.9190	TrainAcc 0.6914	ValidAcc 0.7083	TestAcc 0.7027	BestValid 0.7083
	Epoch 75:	Loss 1.5452	TrainAcc 0.7449	ValidAcc 0.7592	TestAcc 0.7543	BestValid 0.7592
	Epoch 100:	Loss 1.3218	TrainAcc 0.7836	ValidAcc 0.7974	TestAcc 0.7907	BestValid 0.7974
	Epoch 125:	Loss 1.1337	TrainAcc 0.8162	ValidAcc 0.8280	TestAcc 0.8214	BestValid 0.8280
	Epoch 150:	Loss 1.0415	TrainAcc 0.8365	ValidAcc 0.8458	TestAcc 0.8411	BestValid 0.8458
	Epoch 175:	Loss 0.9707	TrainAcc 0.8497	ValidAcc 0.8574	TestAcc 0.8532	BestValid 0.8574
	Epoch 200:	Loss 0.9154	TrainAcc 0.8598	ValidAcc 0.8674	TestAcc 0.8616	BestValid 0.8674
	Epoch 225:	Loss 0.8706	TrainAcc 0.8655	ValidAcc 0.8729	TestAcc 0.8670	BestValid 0.8729
	Epoch 250:	Loss 0.8333	TrainAcc 0.8728	ValidAcc 0.8799	TestAcc 0.8738	BestValid 0.8799
	Epoch 275:	Loss 0.8036	TrainAcc 0.8773	ValidAcc 0.8841	TestAcc 0.8784	BestValid 0.8841
	Epoch 300:	Loss 0.7825	TrainAcc 0.8794	ValidAcc 0.8846	TestAcc 0.8805	BestValid 0.8846
	Epoch 325:	Loss 0.7592	TrainAcc 0.8845	ValidAcc 0.8885	TestAcc 0.8855	BestValid 0.8885
	Epoch 350:	Loss 0.7458	TrainAcc 0.8884	ValidAcc 0.8922	TestAcc 0.8887	BestValid 0.8922
	Epoch 375:	Loss 0.7239	TrainAcc 0.8903	ValidAcc 0.8940	TestAcc 0.8909	BestValid 0.8940
	Epoch 400:	Loss 0.7086	TrainAcc 0.8943	ValidAcc 0.8975	TestAcc 0.8945	BestValid 0.8975
	Epoch 425:	Loss 0.6948	TrainAcc 0.8983	ValidAcc 0.9018	TestAcc 0.8980	BestValid 0.9018
	Epoch 450:	Loss 0.6822	TrainAcc 0.8986	ValidAcc 0.9020	TestAcc 0.8984	BestValid 0.9020
	Epoch 475:	Loss 0.6687	TrainAcc 0.9007	ValidAcc 0.9038	TestAcc 0.8999	BestValid 0.9038
	Epoch 500:	Loss 0.6649	TrainAcc 0.9030	ValidAcc 0.9054	TestAcc 0.9019	BestValid 0.9054
	Epoch 525:	Loss 0.6513	TrainAcc 0.9043	ValidAcc 0.9067	TestAcc 0.9031	BestValid 0.9067
	Epoch 550:	Loss 0.6421	TrainAcc 0.9059	ValidAcc 0.9073	TestAcc 0.9038	BestValid 0.9073
	Epoch 575:	Loss 0.6365	TrainAcc 0.9092	ValidAcc 0.9097	TestAcc 0.9066	BestValid 0.9097
	Epoch 600:	Loss 0.6297	TrainAcc 0.9102	ValidAcc 0.9111	TestAcc 0.9071	BestValid 0.9111
	Epoch 625:	Loss 0.6256	TrainAcc 0.9110	ValidAcc 0.9112	TestAcc 0.9081	BestValid 0.9112
	Epoch 650:	Loss 0.6138	TrainAcc 0.9120	ValidAcc 0.9122	TestAcc 0.9084	BestValid 0.9122
	Epoch 675:	Loss 0.6059	TrainAcc 0.9139	ValidAcc 0.9139	TestAcc 0.9101	BestValid 0.9139
	Epoch 700:	Loss 0.6059	TrainAcc 0.9142	ValidAcc 0.9139	TestAcc 0.9103	BestValid 0.9139
	Epoch 725:	Loss 0.5979	TrainAcc 0.9157	ValidAcc 0.9153	TestAcc 0.9116	BestValid 0.9153
	Epoch 750:	Loss 0.5952	TrainAcc 0.9165	ValidAcc 0.9155	TestAcc 0.9124	BestValid 0.9155
	Epoch 775:	Loss 0.5896	TrainAcc 0.9180	ValidAcc 0.9168	TestAcc 0.9138	BestValid 0.9168
	Epoch 800:	Loss 0.5858	TrainAcc 0.9190	ValidAcc 0.9174	TestAcc 0.9147	BestValid 0.9174
	Epoch 825:	Loss 0.5796	TrainAcc 0.9188	ValidAcc 0.9180	TestAcc 0.9147	BestValid 0.9180
	Epoch 850:	Loss 0.5738	TrainAcc 0.9195	ValidAcc 0.9181	TestAcc 0.9148	BestValid 0.9181
	Epoch 875:	Loss 0.5723	TrainAcc 0.9200	ValidAcc 0.9186	TestAcc 0.9152	BestValid 0.9186
	Epoch 900:	Loss 0.5708	TrainAcc 0.9215	ValidAcc 0.9207	TestAcc 0.9171	BestValid 0.9207
	Epoch 925:	Loss 0.5715	TrainAcc 0.9220	ValidAcc 0.9205	TestAcc 0.9173	BestValid 0.9207
	Epoch 950:	Loss 0.5599	TrainAcc 0.9226	ValidAcc 0.9209	TestAcc 0.9180	BestValid 0.9209
	Epoch 975:	Loss 0.5589	TrainAcc 0.9237	ValidAcc 0.9220	TestAcc 0.9189	BestValid 0.9220
	Epoch 1000:	Loss 0.5543	TrainAcc 0.9234	ValidAcc 0.9217	TestAcc 0.9183	BestValid 0.9220
	Epoch 1025:	Loss 0.5528	TrainAcc 0.9248	ValidAcc 0.9227	TestAcc 0.9202	BestValid 0.9227
	Epoch 1050:	Loss 0.5517	TrainAcc 0.9253	ValidAcc 0.9231	TestAcc 0.9206	BestValid 0.9231
	Epoch 1075:	Loss 0.5472	TrainAcc 0.9256	ValidAcc 0.9234	TestAcc 0.9208	BestValid 0.9234
	Epoch 1100:	Loss 0.5447	TrainAcc 0.9257	ValidAcc 0.9236	TestAcc 0.9202	BestValid 0.9236
	Epoch 1125:	Loss 0.5481	TrainAcc 0.9263	ValidAcc 0.9239	TestAcc 0.9214	BestValid 0.9239
	Epoch 1150:	Loss 0.5350	TrainAcc 0.9271	ValidAcc 0.9243	TestAcc 0.9221	BestValid 0.9243
	Epoch 1175:	Loss 0.5370	TrainAcc 0.9269	ValidAcc 0.9246	TestAcc 0.9218	BestValid 0.9246
	Epoch 1200:	Loss 0.5313	TrainAcc 0.9280	ValidAcc 0.9250	TestAcc 0.9227	BestValid 0.9250
	Epoch 1225:	Loss 0.5338	TrainAcc 0.9283	ValidAcc 0.9251	TestAcc 0.9235	BestValid 0.9251
	Epoch 1250:	Loss 0.5276	TrainAcc 0.9283	ValidAcc 0.9253	TestAcc 0.9233	BestValid 0.9253
	Epoch 1275:	Loss 0.5325	TrainAcc 0.9286	ValidAcc 0.9259	TestAcc 0.9231	BestValid 0.9259
	Epoch 1300:	Loss 0.5274	TrainAcc 0.9288	ValidAcc 0.9262	TestAcc 0.9237	BestValid 0.9262
	Epoch 1325:	Loss 0.5201	TrainAcc 0.9294	ValidAcc 0.9257	TestAcc 0.9239	BestValid 0.9262
	Epoch 1350:	Loss 0.5266	TrainAcc 0.9299	ValidAcc 0.9271	TestAcc 0.9247	BestValid 0.9271
	Epoch 1375:	Loss 0.5193	TrainAcc 0.9307	ValidAcc 0.9280	TestAcc 0.9261	BestValid 0.9280
	Epoch 1400:	Loss 0.5194	TrainAcc 0.9307	ValidAcc 0.9278	TestAcc 0.9256	BestValid 0.9280
	Epoch 1425:	Loss 0.5264	TrainAcc 0.9310	ValidAcc 0.9280	TestAcc 0.9258	BestValid 0.9280
	Epoch 1450:	Loss 0.5128	TrainAcc 0.9312	ValidAcc 0.9280	TestAcc 0.9262	BestValid 0.9280
	Epoch 1475:	Loss 0.5061	TrainAcc 0.9319	ValidAcc 0.9285	TestAcc 0.9267	BestValid 0.9285
	Epoch 1500:	Loss 0.5092	TrainAcc 0.9321	ValidAcc 0.9285	TestAcc 0.9271	BestValid 0.9285
	Epoch 1525:	Loss 0.5057	TrainAcc 0.9329	ValidAcc 0.9296	TestAcc 0.9273	BestValid 0.9296
	Epoch 1550:	Loss 0.5029	TrainAcc 0.9329	ValidAcc 0.9295	TestAcc 0.9276	BestValid 0.9296
	Epoch 1575:	Loss 0.5041	TrainAcc 0.9328	ValidAcc 0.9290	TestAcc 0.9276	BestValid 0.9296
	Epoch 1600:	Loss 0.5026	TrainAcc 0.9338	ValidAcc 0.9298	TestAcc 0.9282	BestValid 0.9298
	Epoch 1625:	Loss 0.5035	TrainAcc 0.9332	ValidAcc 0.9291	TestAcc 0.9275	BestValid 0.9298
	Epoch 1650:	Loss 0.5045	TrainAcc 0.9338	ValidAcc 0.9303	TestAcc 0.9280	BestValid 0.9303
	Epoch 1675:	Loss 0.4992	TrainAcc 0.9337	ValidAcc 0.9300	TestAcc 0.9282	BestValid 0.9303
	Epoch 1700:	Loss 0.4974	TrainAcc 0.9338	ValidAcc 0.9299	TestAcc 0.9281	BestValid 0.9303
	Epoch 1725:	Loss 0.4982	TrainAcc 0.9340	ValidAcc 0.9302	TestAcc 0.9286	BestValid 0.9303
	Epoch 1750:	Loss 0.4917	TrainAcc 0.9346	ValidAcc 0.9309	TestAcc 0.9287	BestValid 0.9309
	Epoch 1775:	Loss 0.4965	TrainAcc 0.9344	ValidAcc 0.9305	TestAcc 0.9288	BestValid 0.9309
	Epoch 1800:	Loss 0.4935	TrainAcc 0.9351	ValidAcc 0.9315	TestAcc 0.9293	BestValid 0.9315
	Epoch 1825:	Loss 0.4903	TrainAcc 0.9349	ValidAcc 0.9313	TestAcc 0.9294	BestValid 0.9315
	Epoch 1850:	Loss 0.4941	TrainAcc 0.9354	ValidAcc 0.9315	TestAcc 0.9296	BestValid 0.9315
	Epoch 1875:	Loss 0.4895	TrainAcc 0.9352	ValidAcc 0.9313	TestAcc 0.9297	BestValid 0.9315
	Epoch 1900:	Loss 0.4863	TrainAcc 0.9357	ValidAcc 0.9312	TestAcc 0.9300	BestValid 0.9315
	Epoch 1925:	Loss 0.4866	TrainAcc 0.9353	ValidAcc 0.9307	TestAcc 0.9293	BestValid 0.9315
	Epoch 1950:	Loss 0.4846	TrainAcc 0.9361	ValidAcc 0.9321	TestAcc 0.9300	BestValid 0.9321
	Epoch 1975:	Loss 0.4820	TrainAcc 0.9362	ValidAcc 0.9321	TestAcc 0.9305	BestValid 0.9321
	Epoch 2000:	Loss 0.4830	TrainAcc 0.9366	ValidAcc 0.9322	TestAcc 0.9308	BestValid 0.9322
	Epoch 2025:	Loss 0.4802	TrainAcc 0.9361	ValidAcc 0.9317	TestAcc 0.9302	BestValid 0.9322
	Epoch 2050:	Loss 0.4834	TrainAcc 0.9372	ValidAcc 0.9330	TestAcc 0.9314	BestValid 0.9330
	Epoch 2075:	Loss 0.4827	TrainAcc 0.9369	ValidAcc 0.9324	TestAcc 0.9308	BestValid 0.9330
	Epoch 2100:	Loss 0.4784	TrainAcc 0.9375	ValidAcc 0.9334	TestAcc 0.9314	BestValid 0.9334
	Epoch 2125:	Loss 0.4791	TrainAcc 0.9373	ValidAcc 0.9327	TestAcc 0.9314	BestValid 0.9334
	Epoch 2150:	Loss 0.4759	TrainAcc 0.9375	ValidAcc 0.9337	TestAcc 0.9318	BestValid 0.9337
	Epoch 2175:	Loss 0.4753	TrainAcc 0.9374	ValidAcc 0.9335	TestAcc 0.9314	BestValid 0.9337
	Epoch 2200:	Loss 0.4738	TrainAcc 0.9379	ValidAcc 0.9338	TestAcc 0.9318	BestValid 0.9338
	Epoch 2225:	Loss 0.4749	TrainAcc 0.9378	ValidAcc 0.9339	TestAcc 0.9318	BestValid 0.9339
	Epoch 2250:	Loss 0.4738	TrainAcc 0.9382	ValidAcc 0.9338	TestAcc 0.9321	BestValid 0.9339
	Epoch 2275:	Loss 0.4763	TrainAcc 0.9383	ValidAcc 0.9343	TestAcc 0.9326	BestValid 0.9343
	Epoch 2300:	Loss 0.4721	TrainAcc 0.9386	ValidAcc 0.9343	TestAcc 0.9324	BestValid 0.9343
	Epoch 2325:	Loss 0.4751	TrainAcc 0.9381	ValidAcc 0.9339	TestAcc 0.9320	BestValid 0.9343
	Epoch 2350:	Loss 0.4718	TrainAcc 0.9382	ValidAcc 0.9342	TestAcc 0.9319	BestValid 0.9343
	Epoch 2375:	Loss 0.4700	TrainAcc 0.9398	ValidAcc 0.9357	TestAcc 0.9332	BestValid 0.9357
	Epoch 2400:	Loss 0.4645	TrainAcc 0.9383	ValidAcc 0.9344	TestAcc 0.9322	BestValid 0.9357
	Epoch 2425:	Loss 0.4674	TrainAcc 0.9394	ValidAcc 0.9351	TestAcc 0.9328	BestValid 0.9357
	Epoch 2450:	Loss 0.4656	TrainAcc 0.9392	ValidAcc 0.9350	TestAcc 0.9330	BestValid 0.9357
	Epoch 2475:	Loss 0.4664	TrainAcc 0.9400	ValidAcc 0.9360	TestAcc 0.9334	BestValid 0.9360
	Epoch 2500:	Loss 0.4606	TrainAcc 0.9394	ValidAcc 0.9352	TestAcc 0.9328	BestValid 0.9360
	Epoch 2525:	Loss 0.4663	TrainAcc 0.9395	ValidAcc 0.9351	TestAcc 0.9330	BestValid 0.9360
	Epoch 2550:	Loss 0.4630	TrainAcc 0.9400	ValidAcc 0.9358	TestAcc 0.9335	BestValid 0.9360
	Epoch 2575:	Loss 0.4622	TrainAcc 0.9389	ValidAcc 0.9347	TestAcc 0.9326	BestValid 0.9360
	Epoch 2600:	Loss 0.4607	TrainAcc 0.9404	ValidAcc 0.9357	TestAcc 0.9338	BestValid 0.9360
	Epoch 2625:	Loss 0.4629	TrainAcc 0.9404	ValidAcc 0.9359	TestAcc 0.9339	BestValid 0.9360
	Epoch 2650:	Loss 0.4635	TrainAcc 0.9396	ValidAcc 0.9350	TestAcc 0.9331	BestValid 0.9360
	Epoch 2675:	Loss 0.4594	TrainAcc 0.9401	ValidAcc 0.9356	TestAcc 0.9337	BestValid 0.9360
	Epoch 2700:	Loss 0.4595	TrainAcc 0.9404	ValidAcc 0.9355	TestAcc 0.9335	BestValid 0.9360
	Epoch 2725:	Loss 0.4614	TrainAcc 0.9403	ValidAcc 0.9355	TestAcc 0.9339	BestValid 0.9360
	Epoch 2750:	Loss 0.4544	TrainAcc 0.9412	ValidAcc 0.9365	TestAcc 0.9346	BestValid 0.9365
	Epoch 2775:	Loss 0.4579	TrainAcc 0.9412	ValidAcc 0.9363	TestAcc 0.9345	BestValid 0.9365
	Epoch 2800:	Loss 0.4575	TrainAcc 0.9408	ValidAcc 0.9362	TestAcc 0.9342	BestValid 0.9365
	Epoch 2825:	Loss 0.4569	TrainAcc 0.9410	ValidAcc 0.9366	TestAcc 0.9346	BestValid 0.9366
	Epoch 2850:	Loss 0.4574	TrainAcc 0.9413	ValidAcc 0.9364	TestAcc 0.9346	BestValid 0.9366
	Epoch 2875:	Loss 0.4553	TrainAcc 0.9415	ValidAcc 0.9366	TestAcc 0.9348	BestValid 0.9366
	Epoch 2900:	Loss 0.4535	TrainAcc 0.9409	ValidAcc 0.9364	TestAcc 0.9342	BestValid 0.9366
	Epoch 2925:	Loss 0.4527	TrainAcc 0.9414	ValidAcc 0.9366	TestAcc 0.9349	BestValid 0.9366
	Epoch 2950:	Loss 0.4549	TrainAcc 0.9419	ValidAcc 0.9370	TestAcc 0.9351	BestValid 0.9370
	Epoch 2975:	Loss 0.4532	TrainAcc 0.9410	ValidAcc 0.9364	TestAcc 0.9344	BestValid 0.9370
	Epoch 3000:	Loss 0.4558	TrainAcc 0.9419	ValidAcc 0.9374	TestAcc 0.9355	BestValid 0.9374
	Epoch 3025:	Loss 0.4533	TrainAcc 0.9418	ValidAcc 0.9369	TestAcc 0.9351	BestValid 0.9374
	Epoch 3050:	Loss 0.4514	TrainAcc 0.9421	ValidAcc 0.9374	TestAcc 0.9353	BestValid 0.9374
	Epoch 3075:	Loss 0.4452	TrainAcc 0.9427	ValidAcc 0.9379	TestAcc 0.9359	BestValid 0.9379
	Epoch 3100:	Loss 0.4482	TrainAcc 0.9423	ValidAcc 0.9380	TestAcc 0.9358	BestValid 0.9380
	Epoch 3125:	Loss 0.4501	TrainAcc 0.9427	ValidAcc 0.9377	TestAcc 0.9365	BestValid 0.9380
	Epoch 3150:	Loss 0.4471	TrainAcc 0.9427	ValidAcc 0.9377	TestAcc 0.9359	BestValid 0.9380
	Epoch 3175:	Loss 0.4458	TrainAcc 0.9422	ValidAcc 0.9373	TestAcc 0.9355	BestValid 0.9380
	Epoch 3200:	Loss 0.4496	TrainAcc 0.9426	ValidAcc 0.9381	TestAcc 0.9358	BestValid 0.9381
	Epoch 3225:	Loss 0.4497	TrainAcc 0.9429	ValidAcc 0.9382	TestAcc 0.9362	BestValid 0.9382
	Epoch 3250:	Loss 0.4434	TrainAcc 0.9430	ValidAcc 0.9378	TestAcc 0.9362	BestValid 0.9382
	Epoch 3275:	Loss 0.4439	TrainAcc 0.9435	ValidAcc 0.9387	TestAcc 0.9365	BestValid 0.9387
	Epoch 3300:	Loss 0.4458	TrainAcc 0.9432	ValidAcc 0.9381	TestAcc 0.9363	BestValid 0.9387
	Epoch 3325:	Loss 0.4460	TrainAcc 0.9434	ValidAcc 0.9385	TestAcc 0.9365	BestValid 0.9387
	Epoch 3350:	Loss 0.4397	TrainAcc 0.9432	ValidAcc 0.9383	TestAcc 0.9361	BestValid 0.9387
	Epoch 3375:	Loss 0.4431	TrainAcc 0.9432	ValidAcc 0.9383	TestAcc 0.9363	BestValid 0.9387
	Epoch 3400:	Loss 0.4382	TrainAcc 0.9435	ValidAcc 0.9382	TestAcc 0.9366	BestValid 0.9387
	Epoch 3425:	Loss 0.4441	TrainAcc 0.9442	ValidAcc 0.9394	TestAcc 0.9368	BestValid 0.9394
	Epoch 3450:	Loss 0.4424	TrainAcc 0.9439	ValidAcc 0.9387	TestAcc 0.9369	BestValid 0.9394
	Epoch 3475:	Loss 0.4413	TrainAcc 0.9429	ValidAcc 0.9376	TestAcc 0.9361	BestValid 0.9394
	Epoch 3500:	Loss 0.4387	TrainAcc 0.9437	ValidAcc 0.9388	TestAcc 0.9369	BestValid 0.9394
	Epoch 3525:	Loss 0.4384	TrainAcc 0.9436	ValidAcc 0.9385	TestAcc 0.9370	BestValid 0.9394
	Epoch 3550:	Loss 0.4342	TrainAcc 0.9436	ValidAcc 0.9385	TestAcc 0.9365	BestValid 0.9394
	Epoch 3575:	Loss 0.4359	TrainAcc 0.9436	ValidAcc 0.9386	TestAcc 0.9366	BestValid 0.9394
	Epoch 3600:	Loss 0.4366	TrainAcc 0.9438	ValidAcc 0.9387	TestAcc 0.9368	BestValid 0.9394
	Epoch 3625:	Loss 0.4404	TrainAcc 0.9441	ValidAcc 0.9389	TestAcc 0.9370	BestValid 0.9394
	Epoch 3650:	Loss 0.4360	TrainAcc 0.9436	ValidAcc 0.9382	TestAcc 0.9365	BestValid 0.9394
	Epoch 3675:	Loss 0.4373	TrainAcc 0.9444	ValidAcc 0.9395	TestAcc 0.9374	BestValid 0.9395
	Epoch 3700:	Loss 0.4445	TrainAcc 0.9441	ValidAcc 0.9389	TestAcc 0.9373	BestValid 0.9395
	Epoch 3725:	Loss 0.4362	TrainAcc 0.9443	ValidAcc 0.9394	TestAcc 0.9373	BestValid 0.9395
	Epoch 3750:	Loss 0.4363	TrainAcc 0.9443	ValidAcc 0.9389	TestAcc 0.9373	BestValid 0.9395
	Epoch 3775:	Loss 0.4343	TrainAcc 0.9439	ValidAcc 0.9390	TestAcc 0.9371	BestValid 0.9395
	Epoch 3800:	Loss 0.4348	TrainAcc 0.9446	ValidAcc 0.9393	TestAcc 0.9376	BestValid 0.9395
	Epoch 3825:	Loss 0.4322	TrainAcc 0.9443	ValidAcc 0.9396	TestAcc 0.9374	BestValid 0.9396
	Epoch 3850:	Loss 0.4315	TrainAcc 0.9444	ValidAcc 0.9392	TestAcc 0.9373	BestValid 0.9396
	Epoch 3875:	Loss 0.4342	TrainAcc 0.9451	ValidAcc 0.9400	TestAcc 0.9380	BestValid 0.9400
	Epoch 3900:	Loss 0.4311	TrainAcc 0.9451	ValidAcc 0.9399	TestAcc 0.9382	BestValid 0.9400
	Epoch 3925:	Loss 0.4326	TrainAcc 0.9453	ValidAcc 0.9404	TestAcc 0.9381	BestValid 0.9404
	Epoch 3950:	Loss 0.4328	TrainAcc 0.9451	ValidAcc 0.9400	TestAcc 0.9379	BestValid 0.9404
	Epoch 3975:	Loss 0.4296	TrainAcc 0.9445	ValidAcc 0.9390	TestAcc 0.9376	BestValid 0.9404
	Epoch 4000:	Loss 0.4286	TrainAcc 0.9445	ValidAcc 0.9395	TestAcc 0.9376	BestValid 0.9404
	Epoch 4025:	Loss 0.4309	TrainAcc 0.9449	ValidAcc 0.9395	TestAcc 0.9376	BestValid 0.9404
	Epoch 4050:	Loss 0.4236	TrainAcc 0.9456	ValidAcc 0.9404	TestAcc 0.9384	BestValid 0.9404
	Epoch 4075:	Loss 0.4271	TrainAcc 0.9457	ValidAcc 0.9408	TestAcc 0.9385	BestValid 0.9408
	Epoch 4100:	Loss 0.4258	TrainAcc 0.9455	ValidAcc 0.9402	TestAcc 0.9383	BestValid 0.9408
	Epoch 4125:	Loss 0.4280	TrainAcc 0.9450	ValidAcc 0.9400	TestAcc 0.9380	BestValid 0.9408
	Epoch 4150:	Loss 0.4291	TrainAcc 0.9453	ValidAcc 0.9399	TestAcc 0.9383	BestValid 0.9408
	Epoch 4175:	Loss 0.4291	TrainAcc 0.9453	ValidAcc 0.9403	TestAcc 0.9383	BestValid 0.9408
	Epoch 4200:	Loss 0.4258	TrainAcc 0.9455	ValidAcc 0.9403	TestAcc 0.9383	BestValid 0.9408
	Epoch 4225:	Loss 0.4266	TrainAcc 0.9452	ValidAcc 0.9398	TestAcc 0.9380	BestValid 0.9408
	Epoch 4250:	Loss 0.4269	TrainAcc 0.9463	ValidAcc 0.9411	TestAcc 0.9390	BestValid 0.9411
	Epoch 4275:	Loss 0.4251	TrainAcc 0.9456	ValidAcc 0.9405	TestAcc 0.9381	BestValid 0.9411
	Epoch 4300:	Loss 0.4276	TrainAcc 0.9457	ValidAcc 0.9411	TestAcc 0.9383	BestValid 0.9411
	Epoch 4325:	Loss 0.4267	TrainAcc 0.9462	ValidAcc 0.9413	TestAcc 0.9390	BestValid 0.9413
	Epoch 4350:	Loss 0.4248	TrainAcc 0.9459	ValidAcc 0.9411	TestAcc 0.9387	BestValid 0.9413
	Epoch 4375:	Loss 0.4270	TrainAcc 0.9456	ValidAcc 0.9404	TestAcc 0.9381	BestValid 0.9413
	Epoch 4400:	Loss 0.4243	TrainAcc 0.9454	ValidAcc 0.9406	TestAcc 0.9379	BestValid 0.9413
	Epoch 4425:	Loss 0.4227	TrainAcc 0.9460	ValidAcc 0.9410	TestAcc 0.9389	BestValid 0.9413
	Epoch 4450:	Loss 0.4286	TrainAcc 0.9456	ValidAcc 0.9407	TestAcc 0.9382	BestValid 0.9413
	Epoch 4475:	Loss 0.4222	TrainAcc 0.9461	ValidAcc 0.9409	TestAcc 0.9390	BestValid 0.9413
	Epoch 4500:	Loss 0.4217	TrainAcc 0.9458	ValidAcc 0.9407	TestAcc 0.9388	BestValid 0.9413
	Epoch 4525:	Loss 0.4189	TrainAcc 0.9464	ValidAcc 0.9413	TestAcc 0.9391	BestValid 0.9413
	Epoch 4550:	Loss 0.4251	TrainAcc 0.9460	ValidAcc 0.9411	TestAcc 0.9388	BestValid 0.9413
	Epoch 4575:	Loss 0.4222	TrainAcc 0.9469	ValidAcc 0.9422	TestAcc 0.9398	BestValid 0.9422
	Epoch 4600:	Loss 0.4192	TrainAcc 0.9468	ValidAcc 0.9422	TestAcc 0.9399	BestValid 0.9422
	Epoch 4625:	Loss 0.4207	TrainAcc 0.9461	ValidAcc 0.9412	TestAcc 0.9392	BestValid 0.9422
	Epoch 4650:	Loss 0.4188	TrainAcc 0.9459	ValidAcc 0.9405	TestAcc 0.9387	BestValid 0.9422
	Epoch 4675:	Loss 0.4214	TrainAcc 0.9465	ValidAcc 0.9415	TestAcc 0.9393	BestValid 0.9422
	Epoch 4700:	Loss 0.4165	TrainAcc 0.9467	ValidAcc 0.9414	TestAcc 0.9396	BestValid 0.9422
	Epoch 4725:	Loss 0.4206	TrainAcc 0.9461	ValidAcc 0.9410	TestAcc 0.9392	BestValid 0.9422
	Epoch 4750:	Loss 0.4150	TrainAcc 0.9468	ValidAcc 0.9416	TestAcc 0.9400	BestValid 0.9422
	Epoch 4775:	Loss 0.4144	TrainAcc 0.9474	ValidAcc 0.9423	TestAcc 0.9404	BestValid 0.9423
	Epoch 4800:	Loss 0.4147	TrainAcc 0.9462	ValidAcc 0.9412	TestAcc 0.9391	BestValid 0.9423
	Epoch 4825:	Loss 0.4149	TrainAcc 0.9471	ValidAcc 0.9421	TestAcc 0.9402	BestValid 0.9423
	Epoch 4850:	Loss 0.4289	TrainAcc 0.9472	ValidAcc 0.9422	TestAcc 0.9402	BestValid 0.9423
	Epoch 4875:	Loss 0.4178	TrainAcc 0.9470	ValidAcc 0.9418	TestAcc 0.9402	BestValid 0.9423
	Epoch 4900:	Loss 0.4186	TrainAcc 0.9464	ValidAcc 0.9411	TestAcc 0.9390	BestValid 0.9423
	Epoch 4925:	Loss 0.4170	TrainAcc 0.9469	ValidAcc 0.9414	TestAcc 0.9398	BestValid 0.9423
	Epoch 4950:	Loss 0.4128	TrainAcc 0.9469	ValidAcc 0.9421	TestAcc 0.9400	BestValid 0.9423
	Epoch 4975:	Loss 0.4235	TrainAcc 0.9474	ValidAcc 0.9421	TestAcc 0.9400	BestValid 0.9423
	Epoch 5000:	Loss 0.4129	TrainAcc 0.9473	ValidAcc 0.9419	TestAcc 0.9403	BestValid 0.9423
Node 3, Pre/Post-Pipelining: 1.089 / 12.427 ms, Bubble: 80.831 ms, Compute: 255.278 ms, Comm: 46.804 ms, Imbalance: 29.130 ms
Node 0, Pre/Post-Pipelining: 1.091 / 12.477 ms, Bubble: 81.408 ms, Compute: 284.026 ms, Comm: 32.655 ms, Imbalance: 12.973 ms
Node 1, Pre/Post-Pipelining: 1.089 / 12.450 ms, Bubble: 81.317 ms, Compute: 255.368 ms, Comm: 42.634 ms, Imbalance: 32.405 ms
Node 6, Pre/Post-Pipelining: 1.087 / 12.463 ms, Bubble: 81.605 ms, Compute: 256.229 ms, Comm: 40.677 ms, Imbalance: 33.770 ms
Node 2, Pre/Post-Pipelining: 1.090 / 12.427 ms, Bubble: 81.996 ms, Compute: 248.815 ms, Comm: 47.177 ms, Imbalance: 34.387 ms
Node 7, Pre/Post-Pipelining: 1.092 / 27.855 ms, Bubble: 66.415 ms, Compute: 282.982 ms, Comm: 32.209 ms, Imbalance: 14.805 ms
Node 4, Pre/Post-Pipelining: 1.087 / 12.477 ms, Bubble: 81.293 ms, Compute: 251.770 ms, Comm: 46.210 ms, Imbalance: 33.252 ms
Node 5, Pre/Post-Pipelining: 1.091 / 12.421 ms, Bubble: 81.600 ms, Compute: 253.876 ms, Comm: 44.994 ms, Imbalance: 31.835 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.091 ms
Cluster-Wide Average, Post-Pipelining Overhead: 12.477 ms
Cluster-Wide Average, Bubble: 81.408 ms
Cluster-Wide Average, Compute: 284.026 ms
Cluster-Wide Average, Communication: 32.655 ms
Cluster-Wide Average, Imbalance: 12.973 ms
Node 0, GPU memory consumption: 8.059 GB
Node 3, GPU memory consumption: 6.018 GB
Node 1, GPU memory consumption: 6.042 GB
Node 2, GPU memory consumption: 6.042 GB
Node 6, GPU memory consumption: 6.042 GB
Node 7, GPU memory consumption: 6.171 GB
Node 5, GPU memory consumption: 6.042 GB
Node 4, GPU memory consumption: 6.018 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.907972 s---------------
------------------------node id 1,  per-epoch time: 0.907972 s---------------
------------------------node id 2,  per-epoch time: 0.907972 s---------------
------------------------node id 3,  per-epoch time: 0.907972 s---------------
------------------------node id 4,  per-epoch time: 0.907971 s---------------
------------------------node id 5,  per-epoch time: 0.907971 s---------------
------------------------node id 6,  per-epoch time: 0.907971 s---------------
------------------------node id 7,  per-epoch time: 0.907971 s---------------
************ Profiling Results ************
	Bubble: 647.237633 (ms) (71.31 percentage)
	Compute: 256.890735 (ms) (28.30 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.471526 (ms) (0.38 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 2.441 GB
	Aggregated layer-level communication throughput: 500.927 Gbps
Highest valid_acc: 0.9423
Target test_acc: 0.9404
Epoch to reach the target acc: 4774
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
