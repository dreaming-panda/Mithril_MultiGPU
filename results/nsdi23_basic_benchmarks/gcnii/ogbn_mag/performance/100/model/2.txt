Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.205 seconds.
Building the CSC structure...
        It takes 0.210 seconds.
Building the CSC structure...
        It takes 0.230 seconds.
Building the CSC structure...
        It takes 0.244 seconds.
Building the CSC structure...
        It takes 0.245 seconds.
Building the CSC structure...
        It takes 0.240 seconds.
Building the CSC structure...
        It takes 0.241 seconds.
Building the CSC structure...
        It takes 0.262 seconds.
Building the CSC structure...
        It takes 0.207 seconds.
        It takes 0.211 seconds.
        It takes 0.229 seconds.
        It takes 0.240 seconds.
        It takes 0.238 seconds.
        It takes 0.246 seconds.
        It takes 0.242 seconds.
        It takes 0.264 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.243 seconds.
Building the Label Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.247 seconds.
Building the Label Vector...
        It takes 0.250 seconds.
Building the Label Vector...
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.247 seconds.
Building the Label Vector...
        It takes 0.543 seconds.
        It takes 0.599 seconds.
        It takes 0.565 seconds.
        It takes 0.591 seconds.
        It takes 0.590 seconds.
        It takes 0.588 seconds.
        It takes 0.600 seconds.
        It takes 0.582 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_mag/32_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 22122) 1-[22122, 45392) 2-[45392, 67023) 3-[67023, 86386) 4-[86386, 111284) 5-[111284, 132574) 6-[132574, 154815) 7-[154815, 176781) 8-[176781, 200751) ... 31-[712285, 736389)
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 64.031 Gbps (per GPU), 512.249 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.991 Gbps (per GPU), 511.927 Gbps (aggregated)
The layer-level communication performance: 63.990 Gbps (per GPU), 511.921 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.955 Gbps (per GPU), 511.638 Gbps (aggregated)
The layer-level communication performance: 63.951 Gbps (per GPU), 511.606 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.920 Gbps (per GPU), 511.356 Gbps (aggregated)
The layer-level communication performance: 63.912 Gbps (per GPU), 511.300 Gbps (aggregated)
The layer-level communication performance: 63.908 Gbps (per GPU), 511.261 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.514 Gbps (per GPU), 1316.108 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.509 Gbps (per GPU), 1316.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.516 Gbps (per GPU), 1316.125 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.510 Gbps (per GPU), 1316.079 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.511 Gbps (per GPU), 1316.086 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.507 Gbps (per GPU), 1316.057 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.496 Gbps (per GPU), 1315.970 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.511 Gbps (per GPU), 1316.089 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.301 Gbps (per GPU), 914.411 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.301 Gbps (per GPU), 914.405 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.302 Gbps (per GPU), 914.418 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.301 Gbps (per GPU), 914.405 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.301 Gbps (per GPU), 914.406 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.298 Gbps (per GPU), 914.380 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.301 Gbps (per GPU), 914.410 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.301 Gbps (per GPU), 914.410 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.968 Gbps (per GPU), 367.747 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.969 Gbps (per GPU), 367.749 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.968 Gbps (per GPU), 367.746 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.968 Gbps (per GPU), 367.745 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.968 Gbps (per GPU), 367.748 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.968 Gbps (per GPU), 367.746 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.968 Gbps (per GPU), 367.746 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.968 Gbps (per GPU), 367.744 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.73ms  1.31ms  2.30ms  3.16 22.12K  0.37M
 chk_1  1.01ms  1.35ms  2.38ms  2.36 23.27K  0.33M
 chk_2  0.72ms  1.32ms  2.29ms  3.19 21.63K  0.36M
 chk_3  0.66ms  1.27ms  2.16ms  3.27 19.36K  0.49M
 chk_4  0.80ms  1.42ms  2.52ms  3.14 24.90K  0.27M
 chk_5  0.71ms  1.32ms  2.30ms  3.23 21.29K  0.42M
 chk_6  0.73ms  1.33ms  2.33ms  3.19 22.24K  0.36M
 chk_7  0.72ms  1.34ms  2.34ms  3.23 21.97K  0.39M
 chk_8  0.77ms  1.42ms  2.47ms  3.22 23.97K  0.31M
 chk_9  0.72ms  1.31ms  2.29ms  3.19 21.69K  0.38M
chk_10  0.77ms  1.39ms  2.45ms  3.16 24.29K  0.27M
chk_11  0.76ms  1.35ms  2.40ms  3.16 23.61K  0.33M
chk_12  0.75ms  1.38ms  2.41ms  3.21 23.29K  0.32M
chk_13  0.76ms  1.35ms  2.40ms  3.14 23.85K  0.30M
chk_14  0.74ms  1.34ms  2.35ms  3.19 22.48K  0.36M
chk_15  0.82ms  1.43ms  2.55ms  3.12 25.69K  0.21M
chk_16  0.80ms  1.42ms  2.52ms  3.14 24.83K  0.29M
chk_17  0.74ms  1.36ms  2.37ms  3.21 22.55K  0.36M
chk_18  0.76ms  1.38ms  2.42ms  3.19 23.46K  0.31M
chk_19  0.67ms  1.33ms  2.22ms  3.31 19.76K  0.47M
chk_20  0.74ms  1.36ms  2.37ms  3.19 22.71K  0.35M
chk_21  0.77ms  1.38ms  2.44ms  3.16 24.18K  0.28M
chk_22  0.77ms  1.64ms  2.46ms  3.18 24.27K  0.29M
chk_23  0.74ms  1.38ms  2.39ms  3.23 22.76K  0.35M
chk_24  0.75ms  1.37ms  2.40ms  3.20 23.18K  0.30M
chk_25  0.76ms  1.36ms  2.41ms  3.17 23.75K  0.31M
chk_26  0.73ms  1.36ms  2.37ms  3.24 22.29K  0.39M
chk_27  0.75ms  1.38ms  2.42ms  3.22 23.17K  0.35M
chk_28  0.75ms  1.37ms  2.41ms  3.20 23.35K  0.32M
chk_29  0.73ms  1.36ms  2.37ms  3.25 22.21K  0.37M
chk_30  0.77ms  1.40ms  2.46ms  3.18 24.16K  0.27M
chk_31  0.77ms  1.40ms  2.45ms  3.18 24.10K  0.31M
   Avg  0.76  1.37  2.39
   Max  1.01  1.64  2.55
   Min  0.66  1.27  2.16
 Ratio  1.52  1.29  1.18
   Var  0.00  0.00  0.01
Profiling takes 1.814 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 76.417 ms
Partition 0 [0, 2) has cost: 68.068 ms
Partition 1 [2, 3) has cost: 43.876 ms
Partition 2 [3, 4) has cost: 43.876 ms
Partition 3 [4, 5) has cost: 43.876 ms
Partition 4 [5, 6) has cost: 43.876 ms
Partition 5 [6, 7) has cost: 43.876 ms
Partition 6 [7, 8) has cost: 43.876 ms
Partition 7 [8, 9) has cost: 76.417 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 71.460 ms
GPU 0, Compute+Comm Time: 57.251 ms, Bubble Time: 12.789 ms, Imbalance Overhead: 1.420 ms
GPU 1, Compute+Comm Time: 50.499 ms, Bubble Time: 12.658 ms, Imbalance Overhead: 8.302 ms
GPU 2, Compute+Comm Time: 50.499 ms, Bubble Time: 12.680 ms, Imbalance Overhead: 8.280 ms
GPU 3, Compute+Comm Time: 50.499 ms, Bubble Time: 12.552 ms, Imbalance Overhead: 8.409 ms
GPU 4, Compute+Comm Time: 50.499 ms, Bubble Time: 12.298 ms, Imbalance Overhead: 8.663 ms
GPU 5, Compute+Comm Time: 50.499 ms, Bubble Time: 12.441 ms, Imbalance Overhead: 8.520 ms
GPU 6, Compute+Comm Time: 50.499 ms, Bubble Time: 12.239 ms, Imbalance Overhead: 8.721 ms
GPU 7, Compute+Comm Time: 57.792 ms, Bubble Time: 12.096 ms, Imbalance Overhead: 1.572 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 110.575 ms
GPU 0, Compute+Comm Time: 92.228 ms, Bubble Time: 18.087 ms, Imbalance Overhead: 0.260 ms
GPU 1, Compute+Comm Time: 66.980 ms, Bubble Time: 18.520 ms, Imbalance Overhead: 25.075 ms
GPU 2, Compute+Comm Time: 66.980 ms, Bubble Time: 19.028 ms, Imbalance Overhead: 24.567 ms
GPU 3, Compute+Comm Time: 66.980 ms, Bubble Time: 19.052 ms, Imbalance Overhead: 24.543 ms
GPU 4, Compute+Comm Time: 66.980 ms, Bubble Time: 19.657 ms, Imbalance Overhead: 23.938 ms
GPU 5, Compute+Comm Time: 66.980 ms, Bubble Time: 20.064 ms, Imbalance Overhead: 23.531 ms
GPU 6, Compute+Comm Time: 66.980 ms, Bubble Time: 19.999 ms, Imbalance Overhead: 23.596 ms
GPU 7, Compute+Comm Time: 84.420 ms, Bubble Time: 20.397 ms, Imbalance Overhead: 5.758 ms
The estimated cost of the whole pipeline: 191.136 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 120.293 ms
Partition 0 [0, 3) has cost: 111.943 ms
Partition 1 [3, 5) has cost: 87.752 ms
Partition 2 [5, 7) has cost: 87.752 ms
Partition 3 [7, 9) has cost: 120.293 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 75.001 ms
GPU 0, Compute+Comm Time: 61.759 ms, Bubble Time: 11.621 ms, Imbalance Overhead: 1.621 ms
GPU 1, Compute+Comm Time: 58.314 ms, Bubble Time: 11.566 ms, Imbalance Overhead: 5.122 ms
GPU 2, Compute+Comm Time: 58.314 ms, Bubble Time: 11.296 ms, Imbalance Overhead: 5.391 ms
GPU 3, Compute+Comm Time: 61.937 ms, Bubble Time: 11.199 ms, Imbalance Overhead: 1.865 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 103.579 ms
GPU 0, Compute+Comm Time: 87.811 ms, Bubble Time: 15.528 ms, Imbalance Overhead: 0.240 ms
GPU 1, Compute+Comm Time: 74.865 ms, Bubble Time: 15.832 ms, Imbalance Overhead: 12.882 ms
GPU 2, Compute+Comm Time: 74.865 ms, Bubble Time: 16.464 ms, Imbalance Overhead: 12.251 ms
GPU 3, Compute+Comm Time: 83.945 ms, Bubble Time: 16.498 ms, Imbalance Overhead: 3.136 ms
    The estimated cost with 2 DP ways is 187.509 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 208.044 ms
Partition 0 [0, 5) has cost: 199.695 ms
Partition 1 [5, 9) has cost: 208.044 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 98.155 ms
GPU 0, Compute+Comm Time: 86.088 ms, Bubble Time: 10.716 ms, Imbalance Overhead: 1.352 ms
GPU 1, Compute+Comm Time: 86.057 ms, Bubble Time: 10.092 ms, Imbalance Overhead: 2.006 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 120.616 ms
GPU 0, Compute+Comm Time: 107.269 ms, Bubble Time: 12.782 ms, Imbalance Overhead: 0.565 ms
GPU 1, Compute+Comm Time: 105.435 ms, Bubble Time: 13.444 ms, Imbalance Overhead: 1.737 ms
    The estimated cost with 4 DP ways is 229.710 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 407.739 ms
Partition 0 [0, 9) has cost: 407.739 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 211.565 ms
GPU 0, Compute+Comm Time: 211.565 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 231.387 ms
GPU 0, Compute+Comm Time: 231.387 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 465.100 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 13)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [13, 20)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [20, 27)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [27, 34)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [34, 41)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [41, 48)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [48, 55)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [55, 65)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 13)...
+++++++++ Node 1 initializing the weights for op[13, 20)...
+++++++++ Node 4 initializing the weights for op[34, 41)...
+++++++++ Node 2 initializing the weights for op[20, 27)...
+++++++++ Node 5 initializing the weights for op[41, 48)...
+++++++++ Node 3 initializing the weights for op[27, 34)...
+++++++++ Node 6 initializing the weights for op[48, 55)...
+++++++++ Node 7 initializing the weights for op[55, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.2651
	Epoch 50:	Loss 4.9561
	Epoch 75:	Loss 4.6038
	Epoch 100:	Loss 4.2959
Node 0, Pre/Post-Pipelining: 0.876 / 6.374 ms, Bubble: 181.753 ms, Compute: 71.034 ms, Comm: 68.017 ms, Imbalance: 139.197 ms
Node 1, Pre/Post-Pipelining: 0.872 / 6.398 ms, Bubble: 177.249 ms, Compute: 47.497 ms, Comm: 94.321 ms, Imbalance: 141.405 ms
Node 4, Pre/Post-Pipelining: 0.848 / 6.365 ms, Bubble: 164.222 ms, Compute: 45.908 ms, Comm: 147.007 ms, Imbalance: 105.257 ms
Node 3, Pre/Post-Pipelining: 0.872 / 6.370 ms, Bubble: 166.533 ms, Compute: 46.816 ms, Comm: 146.705 ms, Imbalance: 100.925 ms
Node 7, Pre/Post-Pipelining: 0.875 / 126.405 ms, Bubble: 37.043 ms, Compute: 207.899 ms, Comm: 67.656 ms, Imbalance: 28.597 ms
Node 5, Pre/Post-Pipelining: 0.872 / 6.368 ms, Bubble: 163.209 ms, Compute: 46.168 ms, Comm: 117.096 ms, Imbalance: 135.906 ms
Node 6, Pre/Post-Pipelining: 0.872 / 6.372 ms, Bubble: 159.215 ms, Compute: 46.611 ms, Comm: 94.203 ms, Imbalance: 161.971 ms
Node 2, Pre/Post-Pipelining: 0.871 / 6.373 ms, Bubble: 174.012 ms, Compute: 46.804 ms, Comm: 118.160 ms, Imbalance: 122.475 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.876 ms
Cluster-Wide Average, Post-Pipelining Overhead: 6.374 ms
Cluster-Wide Average, Bubble: 181.753 ms
Cluster-Wide Average, Compute: 71.034 ms
Cluster-Wide Average, Communication: 68.017 ms
Cluster-Wide Average, Imbalance: 139.197 ms
Node 0, GPU memory consumption: 7.335 GB
Node 1, GPU memory consumption: 3.770 GB
Node 4, GPU memory consumption: 3.747 GB
Node 2, GPU memory consumption: 3.770 GB
Node 5, GPU memory consumption: 3.770 GB
Node 3, GPU memory consumption: 3.747 GB
Node 6, GPU memory consumption: 3.770 GB
Node 7, GPU memory consumption: 7.653 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.467928 s---------------
------------------------node id 1,  per-epoch time: 0.467929 s---------------
------------------------node id 2,  per-epoch time: 0.467932 s---------------
------------------------node id 3,  per-epoch time: 0.467929 s---------------
------------------------node id 4,  per-epoch time: 0.467929 s---------------
------------------------node id 5,  per-epoch time: 0.467929 s---------------
------------------------node id 6,  per-epoch time: 0.467929 s---------------
------------------------node id 7,  per-epoch time: 0.467929 s---------------
************ Profiling Results ************
	Bubble: 401.989272 (ms) (85.12 percentage)
	Compute: 68.251520 (ms) (14.45 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.033454 (ms) (0.43 percentage)
	Layer-level communication (cluster-wide, per-epoch): 7.681 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 7.685 GB
	Aggregated layer-level communication throughput: 618.690 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0019
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
