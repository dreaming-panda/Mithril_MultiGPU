Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.193 seconds.
Building the CSC structure...
        It takes 0.200 seconds.
Building the CSC structure...
        It takes 0.213 seconds.
Building the CSC structure...
        It takes 0.241 seconds.
Building the CSC structure...
        It takes 0.242 seconds.
Building the CSC structure...
        It takes 0.257 seconds.
Building the CSC structure...
        It takes 0.254 seconds.
Building the CSC structure...
        It takes 0.265 seconds.
Building the CSC structure...
        It takes 0.185 seconds.
        It takes 0.195 seconds.
        It takes 0.204 seconds.
Building the Feature Vector...
        It takes 0.238 seconds.
        It takes 0.245 seconds.
Building the Feature Vector...
        It takes 0.259 seconds.
        It takes 0.266 seconds.
        It takes 0.265 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.247 seconds.
Building the Label Vector...
        It takes 0.246 seconds.
Building the Label Vector...
        It takes 0.243 seconds.
Building the Label Vector...
        It takes 0.238 seconds.
Building the Label Vector...
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.234 seconds.
Building the Label Vector...
        It takes 0.249 seconds.
Building the Label Vector...
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.553 seconds.
        It takes 0.596 seconds.
        It takes 0.566 seconds.
        It takes 0.595 seconds.
        It takes 0.537 seconds.
        It takes 0.590 seconds.
        It takes 0.586 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_mag/32_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
        It takes 0.584 seconds.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 23013
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 22122) 1-[22122, 45392) 2-[45392, 67023) 3-[67023, 86386) 4-[86386, 111284) 5-[111284, 132574) 6-[132574, 154815) 7-[154815, 176781) 8-[176781, 200751) ... 31-[712285, 736389)
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.494 Gbps (per GPU), 507.956 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.455 Gbps (per GPU), 507.639 Gbps (aggregated)
The layer-level communication performance: 63.454 Gbps (per GPU), 507.628 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.419 Gbps (per GPU), 507.350 Gbps (aggregated)
The layer-level communication performance: 63.415 Gbps (per GPU), 507.320 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.384 Gbps (per GPU), 507.076 Gbps (aggregated)
The layer-level communication performance: 63.378 Gbps (per GPU), 507.024 Gbps (aggregated)
The layer-level communication performance: 63.373 Gbps (per GPU), 506.986 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.661 Gbps (per GPU), 1309.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.665 Gbps (per GPU), 1309.320 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.659 Gbps (per GPU), 1309.275 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.659 Gbps (per GPU), 1309.275 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.661 Gbps (per GPU), 1309.285 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.646 Gbps (per GPU), 1309.167 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.657 Gbps (per GPU), 1309.259 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.661 Gbps (per GPU), 1309.288 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.640 Gbps (per GPU), 909.120 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.640 Gbps (per GPU), 909.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.641 Gbps (per GPU), 909.127 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.640 Gbps (per GPU), 909.120 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.640 Gbps (per GPU), 909.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.638 Gbps (per GPU), 909.105 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.640 Gbps (per GPU), 909.119 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.640 Gbps (per GPU), 909.120 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.457 Gbps (per GPU), 363.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.458 Gbps (per GPU), 363.661 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.457 Gbps (per GPU), 363.655 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.457 Gbps (per GPU), 363.657 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.457 Gbps (per GPU), 363.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.457 Gbps (per GPU), 363.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.457 Gbps (per GPU), 363.656 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.457 Gbps (per GPU), 363.658 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.73ms  1.32ms  2.31ms  3.16 22.12K  0.37M
 chk_1  0.75ms  1.36ms  2.39ms  3.17 23.27K  0.33M
 chk_2  0.72ms  1.32ms  2.29ms  3.20 21.63K  0.36M
 chk_3  0.66ms  1.27ms  2.17ms  3.26 19.36K  0.49M
 chk_4  0.81ms  1.43ms  2.53ms  3.14 24.90K  0.27M
 chk_5  0.71ms  1.33ms  2.30ms  3.23 21.29K  0.42M
 chk_6  0.73ms  1.34ms  2.34ms  3.20 22.24K  0.36M
 chk_7  0.73ms  1.35ms  2.35ms  3.23 21.97K  0.39M
 chk_8  0.77ms  1.43ms  2.48ms  3.22 23.97K  0.31M
 chk_9  0.72ms  1.32ms  2.30ms  3.19 21.69K  0.38M
chk_10  0.78ms  1.40ms  2.46ms  3.16 24.29K  0.27M
chk_11  0.76ms  1.36ms  2.41ms  3.17 23.61K  0.33M
chk_12  0.75ms  1.38ms  2.42ms  3.21 23.29K  0.32M
chk_13  0.77ms  1.36ms  2.41ms  3.15 23.85K  0.30M
chk_14  0.74ms  1.34ms  2.35ms  3.19 22.48K  0.36M
chk_15  0.82ms  1.43ms  2.56ms  3.12 25.69K  0.21M
chk_16  0.80ms  1.42ms  2.53ms  3.14 24.83K  0.29M
chk_17  0.74ms  1.37ms  2.37ms  3.21 22.55K  0.36M
chk_18  0.76ms  1.38ms  2.43ms  3.20 23.46K  0.31M
chk_19  0.67ms  1.33ms  2.24ms  3.32 19.76K  0.47M
chk_20  0.74ms  1.36ms  2.38ms  3.20 22.71K  0.35M
chk_21  0.77ms  1.39ms  2.45ms  3.17 24.18K  0.28M
chk_22  0.78ms  1.40ms  2.47ms  3.18 24.27K  0.29M
chk_23  0.74ms  1.39ms  2.41ms  3.23 22.76K  0.35M
chk_24  0.75ms  1.37ms  2.41ms  3.20 23.18K  0.30M
chk_25  0.76ms  1.36ms  2.42ms  3.17 23.75K  0.31M
chk_26  0.73ms  1.37ms  2.38ms  3.24 22.29K  0.39M
chk_27  0.75ms  1.39ms  2.43ms  3.22 23.17K  0.35M
chk_28  0.76ms  1.38ms  2.42ms  3.19 23.35K  0.32M
chk_29  0.73ms  1.37ms  2.38ms  3.25 22.21K  0.37M
chk_30  0.78ms  1.41ms  2.47ms  3.18 24.16K  0.27M
chk_31  0.77ms  1.40ms  2.47ms  3.19 24.10K  0.31M
   Avg  0.75  1.37  2.40
   Max  0.82  1.43  2.56
   Min  0.66  1.27  2.17
 Ratio  1.24  1.13  1.18
   Var  0.00  0.00  0.01
Profiling takes 1.829 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 76.682 ms
Partition 0 [0, 2) has cost: 67.829 ms
Partition 1 [2, 3) has cost: 43.824 ms
Partition 2 [3, 4) has cost: 43.824 ms
Partition 3 [4, 5) has cost: 43.824 ms
Partition 4 [5, 6) has cost: 43.824 ms
Partition 5 [6, 7) has cost: 43.824 ms
Partition 6 [7, 8) has cost: 43.824 ms
Partition 7 [8, 9) has cost: 76.682 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 71.478 ms
GPU 0, Compute+Comm Time: 57.423 ms, Bubble Time: 12.881 ms, Imbalance Overhead: 1.174 ms
GPU 1, Compute+Comm Time: 50.637 ms, Bubble Time: 12.750 ms, Imbalance Overhead: 8.091 ms
GPU 2, Compute+Comm Time: 50.637 ms, Bubble Time: 12.774 ms, Imbalance Overhead: 8.067 ms
GPU 3, Compute+Comm Time: 50.637 ms, Bubble Time: 12.643 ms, Imbalance Overhead: 8.199 ms
GPU 4, Compute+Comm Time: 50.637 ms, Bubble Time: 12.390 ms, Imbalance Overhead: 8.451 ms
GPU 5, Compute+Comm Time: 50.637 ms, Bubble Time: 12.536 ms, Imbalance Overhead: 8.306 ms
GPU 6, Compute+Comm Time: 50.637 ms, Bubble Time: 12.334 ms, Imbalance Overhead: 8.508 ms
GPU 7, Compute+Comm Time: 58.191 ms, Bubble Time: 12.190 ms, Imbalance Overhead: 1.097 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 110.909 ms
GPU 0, Compute+Comm Time: 92.716 ms, Bubble Time: 17.927 ms, Imbalance Overhead: 0.266 ms
GPU 1, Compute+Comm Time: 67.412 ms, Bubble Time: 18.367 ms, Imbalance Overhead: 25.130 ms
GPU 2, Compute+Comm Time: 67.412 ms, Bubble Time: 18.874 ms, Imbalance Overhead: 24.623 ms
GPU 3, Compute+Comm Time: 67.412 ms, Bubble Time: 18.895 ms, Imbalance Overhead: 24.602 ms
GPU 4, Compute+Comm Time: 67.412 ms, Bubble Time: 19.498 ms, Imbalance Overhead: 23.999 ms
GPU 5, Compute+Comm Time: 67.412 ms, Bubble Time: 19.909 ms, Imbalance Overhead: 23.588 ms
GPU 6, Compute+Comm Time: 67.412 ms, Bubble Time: 20.099 ms, Imbalance Overhead: 23.398 ms
GPU 7, Compute+Comm Time: 84.631 ms, Bubble Time: 20.501 ms, Imbalance Overhead: 5.777 ms
The estimated cost of the whole pipeline: 191.506 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 120.505 ms
Partition 0 [0, 3) has cost: 111.653 ms
Partition 1 [3, 5) has cost: 87.648 ms
Partition 2 [5, 7) has cost: 87.648 ms
Partition 3 [7, 9) has cost: 120.505 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 74.327 ms
GPU 0, Compute+Comm Time: 61.718 ms, Bubble Time: 11.703 ms, Imbalance Overhead: 0.906 ms
GPU 1, Compute+Comm Time: 58.253 ms, Bubble Time: 11.650 ms, Imbalance Overhead: 4.424 ms
GPU 2, Compute+Comm Time: 58.253 ms, Bubble Time: 11.383 ms, Imbalance Overhead: 4.690 ms
GPU 3, Compute+Comm Time: 62.129 ms, Bubble Time: 11.281 ms, Imbalance Overhead: 0.916 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 103.928 ms
GPU 0, Compute+Comm Time: 88.309 ms, Bubble Time: 15.365 ms, Imbalance Overhead: 0.254 ms
GPU 1, Compute+Comm Time: 75.347 ms, Bubble Time: 15.665 ms, Imbalance Overhead: 12.916 ms
GPU 2, Compute+Comm Time: 75.347 ms, Bubble Time: 16.294 ms, Imbalance Overhead: 12.287 ms
GPU 3, Compute+Comm Time: 84.188 ms, Bubble Time: 16.587 ms, Imbalance Overhead: 3.154 ms
    The estimated cost with 2 DP ways is 187.167 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 208.153 ms
Partition 0 [0, 5) has cost: 199.301 ms
Partition 1 [5, 9) has cost: 208.153 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 97.116 ms
GPU 0, Compute+Comm Time: 85.687 ms, Bubble Time: 10.790 ms, Imbalance Overhead: 0.638 ms
GPU 1, Compute+Comm Time: 85.903 ms, Bubble Time: 10.164 ms, Imbalance Overhead: 1.048 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 121.099 ms
GPU 0, Compute+Comm Time: 107.922 ms, Bubble Time: 12.599 ms, Imbalance Overhead: 0.578 ms
GPU 1, Compute+Comm Time: 105.835 ms, Bubble Time: 13.525 ms, Imbalance Overhead: 1.738 ms
    The estimated cost with 4 DP ways is 229.125 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 407.454 ms
Partition 0 [0, 9) has cost: 407.454 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 212.160 ms
GPU 0, Compute+Comm Time: 212.160 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 233.514 ms
GPU 0, Compute+Comm Time: 233.514 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 467.957 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 13)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [13, 20)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [20, 27)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [27, 34)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [34, 41)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [41, 48)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [48, 55)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [55, 65)
*** Node 7, constructing the helper classes...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 13)...
+++++++++ Node 1 initializing the weights for op[13, 20)...
+++++++++ Node 4 initializing the weights for op[34, 41)...
+++++++++ Node 2 initializing the weights for op[20, 27)...
+++++++++ Node 5 initializing the weights for op[41, 48)...
+++++++++ Node 3 initializing the weights for op[27, 34)...
+++++++++ Node 6 initializing the weights for op[48, 55)...
+++++++++ Node 7 initializing the weights for op[55, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.2538
	Epoch 50:	Loss 4.9404
	Epoch 75:	Loss 4.5924
	Epoch 100:	Loss 4.2755
Node 0, Pre/Post-Pipelining: 0.858 / 6.449 ms, Bubble: 182.235 ms, Compute: 71.423 ms, Comm: 68.265 ms, Imbalance: 139.320 ms
Node 5, Pre/Post-Pipelining: 0.860 / 6.381 ms, Bubble: 163.603 ms, Compute: 46.027 ms, Comm: 117.169 ms, Imbalance: 136.898 ms
Node 1, Pre/Post-Pipelining: 0.853 / 6.434 ms, Bubble: 177.754 ms, Compute: 47.223 ms, Comm: 94.234 ms, Imbalance: 142.546 ms
Node 2, Pre/Post-Pipelining: 0.856 / 6.411 ms, Bubble: 174.488 ms, Compute: 46.544 ms, Comm: 118.057 ms, Imbalance: 123.667 ms
Node 4, Pre/Post-Pipelining: 0.856 / 6.401 ms, Bubble: 164.557 ms, Compute: 45.963 ms, Comm: 146.973 ms, Imbalance: 106.175 ms
Node 6, Pre/Post-Pipelining: 0.847 / 6.432 ms, Bubble: 159.394 ms, Compute: 46.714 ms, Comm: 93.937 ms, Imbalance: 163.088 ms
Node 7, Pre/Post-Pipelining: 0.860 / 126.650 ms, Bubble: 37.107 ms, Compute: 208.889 ms, Comm: 67.615 ms, Imbalance: 28.770 ms
Node 3, Pre/Post-Pipelining: 0.853 / 6.421 ms, Bubble: 166.902 ms, Compute: 46.917 ms, Comm: 146.574 ms, Imbalance: 101.858 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.858 ms
Cluster-Wide Average, Post-Pipelining Overhead: 6.449 ms
Cluster-Wide Average, Bubble: 182.235 ms
Cluster-Wide Average, Compute: 71.423 ms
Cluster-Wide Average, Communication: 68.265 ms
Cluster-Wide Average, Imbalance: 139.320 ms
Node 0, GPU memory consumption: 7.329 GB
Node 4, GPU memory consumption: 3.747 GB
Node 2, GPU memory consumption: 3.770 GB
Node 5, GPU memory consumption: 3.770 GB
Node 1, GPU memory consumption: 3.770 GB
Node 3, GPU memory consumption: 3.747 GB
Node 7, GPU memory consumption: 7.653 GB
Node 6, GPU memory consumption: 3.770 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.469234 s---------------
------------------------node id 4,  per-epoch time: 0.469234 s---------------
------------------------node id 1,  per-epoch time: 0.469234 s---------------
------------------------node id 5,  per-epoch time: 0.469233 s---------------
------------------------node id 2,  per-epoch time: 0.469234 s---------------
------------------------node id 6,  per-epoch time: 0.469235 s---------------
------------------------node id 3,  per-epoch time: 0.469235 s---------------
------------------------node id 7,  per-epoch time: 0.469235 s---------------
************ Profiling Results ************
	Bubble: 403.030134 (ms) (85.13 percentage)
	Compute: 68.381787 (ms) (14.44 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.038520 (ms) (0.43 percentage)
	Layer-level communication (cluster-wide, per-epoch): 7.681 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 7.685 GB
	Aggregated layer-level communication throughput: 618.936 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0019
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
