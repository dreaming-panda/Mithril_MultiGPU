Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.200 seconds.
Building the CSC structure...
        It takes 0.229 seconds.
Building the CSC structure...
        It takes 0.241 seconds.
Building the CSC structure...
        It takes 0.238 seconds.
Building the CSC structure...
        It takes 0.243 seconds.
Building the CSC structure...
        It takes 0.264 seconds.
Building the CSC structure...
        It takes 0.264 seconds.
Building the CSC structure...
        It takes 0.266 seconds.
Building the CSC structure...
        It takes 0.196 seconds.
        It takes 0.220 seconds.
        It takes 0.231 seconds.
        It takes 0.237 seconds.
        It takes 0.246 seconds.
Building the Feature Vector...
        It takes 0.262 seconds.
        It takes 0.263 seconds.
        It takes 0.268 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.239 seconds.
Building the Label Vector...
        It takes 0.245 seconds.
Building the Label Vector...
        It takes 0.253 seconds.
Building the Label Vector...
        It takes 0.248 seconds.
Building the Label Vector...
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.239 seconds.
Building the Label Vector...
        It takes 0.252 seconds.
Building the Label Vector...
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.555 seconds.
        It takes 0.574 seconds.
        It takes 0.598 seconds.
        It takes 0.599 seconds.
        It takes 0.556 seconds.
        It takes 0.588 seconds.
        It takes 0.592 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_mag/32_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
        It takes 0.587 seconds.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 22122) 1-[22122, 45392) 2-[45392, 67023) 3-[67023, 86386) 4-[86386, 111284) 5-[111284, 132574) 6-[132574, 154815) 7-[154815, 176781) 8-[176781, 200751) ... 31-[712285, 736389)
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 62.414 Gbps (per GPU), 499.314 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.374 Gbps (per GPU), 498.994 Gbps (aggregated)
The layer-level communication performance: 62.374 Gbps (per GPU), 498.989 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.338 Gbps (per GPU), 498.705 Gbps (aggregated)
The layer-level communication performance: 62.334 Gbps (per GPU), 498.669 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.306 Gbps (per GPU), 498.447 Gbps (aggregated)
The layer-level communication performance: 62.300 Gbps (per GPU), 498.397 Gbps (aggregated)
The layer-level communication performance: 62.295 Gbps (per GPU), 498.364 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.766 Gbps (per GPU), 1326.128 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.749 Gbps (per GPU), 1325.990 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.759 Gbps (per GPU), 1326.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.765 Gbps (per GPU), 1326.121 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.748 Gbps (per GPU), 1325.980 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.766 Gbps (per GPU), 1326.128 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.764 Gbps (per GPU), 1326.108 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.766 Gbps (per GPU), 1326.125 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.733 Gbps (per GPU), 909.865 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.733 Gbps (per GPU), 909.865 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.733 Gbps (per GPU), 909.867 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.732 Gbps (per GPU), 909.853 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.732 Gbps (per GPU), 909.858 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.733 Gbps (per GPU), 909.864 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.732 Gbps (per GPU), 909.859 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.733 Gbps (per GPU), 909.861 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.519 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.520 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.519 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.517 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.565 Gbps (per GPU), 364.520 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.562 Gbps (per GPU), 364.492 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.72ms  1.30ms  2.29ms  3.17 22.12K  0.37M
 chk_1  0.75ms  1.35ms  2.37ms  3.18 23.27K  0.33M
 chk_2  0.71ms  1.31ms  2.28ms  3.21 21.63K  0.36M
 chk_3  0.66ms  1.26ms  2.15ms  3.28 19.36K  0.49M
 chk_4  0.80ms  1.41ms  2.51ms  3.15 24.90K  0.27M
 chk_5  0.71ms  1.32ms  2.29ms  3.23 21.29K  0.42M
 chk_6  0.72ms  1.31ms  2.32ms  3.20 22.24K  0.36M
 chk_7  0.72ms  1.34ms  2.33ms  3.22 21.97K  0.39M
 chk_8  0.76ms  1.41ms  2.46ms  3.23 23.97K  0.31M
 chk_9  0.72ms  1.30ms  2.28ms  3.19 21.69K  0.38M
chk_10  0.77ms  1.38ms  2.44ms  3.17 24.29K  0.27M
chk_11  0.75ms  1.35ms  2.40ms  3.18 23.61K  0.33M
chk_12  0.75ms  1.37ms  2.41ms  3.21 23.29K  0.32M
chk_13  0.76ms  1.35ms  2.39ms  3.16 23.85K  0.30M
chk_14  0.73ms  1.33ms  2.34ms  3.20 22.48K  0.36M
chk_15  0.81ms  1.42ms  2.54ms  3.12 25.69K  0.21M
chk_16  0.80ms  1.41ms  2.51ms  3.15 24.83K  0.29M
chk_17  0.73ms  1.35ms  2.36ms  3.22 22.55K  0.36M
chk_18  0.75ms  1.37ms  2.41ms  3.20 23.46K  0.31M
chk_19  0.67ms  1.32ms  2.22ms  3.33 19.76K  0.47M
chk_20  0.74ms  1.35ms  2.36ms  3.21 22.71K  0.35M
chk_21  0.77ms  1.38ms  2.43ms  3.17 24.18K  0.28M
chk_22  0.77ms  1.39ms  2.45ms  3.19 24.27K  0.29M
chk_23  0.74ms  1.38ms  2.39ms  3.24 22.76K  0.35M
chk_24  0.74ms  1.36ms  2.39ms  3.21 23.18K  0.30M
chk_25  0.76ms  1.35ms  2.40ms  3.18 23.75K  0.31M
chk_26  0.73ms  1.36ms  2.36ms  3.25 22.29K  0.39M
chk_27  0.75ms  1.38ms  2.41ms  3.23 23.17K  0.35M
chk_28  0.75ms  1.36ms  2.40ms  3.20 23.35K  0.32M
chk_29  0.73ms  1.36ms  2.36ms  3.26 22.21K  0.37M
chk_30  0.77ms  1.39ms  2.45ms  3.20 24.16K  0.27M
chk_31  0.76ms  1.39ms  2.44ms  3.20 24.10K  0.31M
   Avg  0.74  1.36  2.38
   Max  0.81  1.42  2.54
   Min  0.66  1.26  2.15
 Ratio  1.24  1.13  1.18
   Var  0.00  0.00  0.01
Profiling takes 1.803 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 76.134 ms
Partition 0 [0, 2) has cost: 67.184 ms
Partition 1 [2, 3) has cost: 43.412 ms
Partition 2 [3, 4) has cost: 43.412 ms
Partition 3 [4, 5) has cost: 43.412 ms
Partition 4 [5, 6) has cost: 43.412 ms
Partition 5 [6, 7) has cost: 43.412 ms
Partition 6 [7, 8) has cost: 43.412 ms
Partition 7 [8, 9) has cost: 76.134 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 72.008 ms
GPU 0, Compute+Comm Time: 57.821 ms, Bubble Time: 12.976 ms, Imbalance Overhead: 1.211 ms
GPU 1, Compute+Comm Time: 51.125 ms, Bubble Time: 12.843 ms, Imbalance Overhead: 8.041 ms
GPU 2, Compute+Comm Time: 51.125 ms, Bubble Time: 12.864 ms, Imbalance Overhead: 8.019 ms
GPU 3, Compute+Comm Time: 51.125 ms, Bubble Time: 12.729 ms, Imbalance Overhead: 8.154 ms
GPU 4, Compute+Comm Time: 51.125 ms, Bubble Time: 12.477 ms, Imbalance Overhead: 8.406 ms
GPU 5, Compute+Comm Time: 51.125 ms, Bubble Time: 12.622 ms, Imbalance Overhead: 8.262 ms
GPU 6, Compute+Comm Time: 51.125 ms, Bubble Time: 12.422 ms, Imbalance Overhead: 8.462 ms
GPU 7, Compute+Comm Time: 58.639 ms, Bubble Time: 12.269 ms, Imbalance Overhead: 1.100 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 111.248 ms
GPU 0, Compute+Comm Time: 93.005 ms, Bubble Time: 17.972 ms, Imbalance Overhead: 0.272 ms
GPU 1, Compute+Comm Time: 67.797 ms, Bubble Time: 18.417 ms, Imbalance Overhead: 25.034 ms
GPU 2, Compute+Comm Time: 67.797 ms, Bubble Time: 18.930 ms, Imbalance Overhead: 24.521 ms
GPU 3, Compute+Comm Time: 67.797 ms, Bubble Time: 18.954 ms, Imbalance Overhead: 24.497 ms
GPU 4, Compute+Comm Time: 67.797 ms, Bubble Time: 19.567 ms, Imbalance Overhead: 23.884 ms
GPU 5, Compute+Comm Time: 67.797 ms, Bubble Time: 19.978 ms, Imbalance Overhead: 23.473 ms
GPU 6, Compute+Comm Time: 67.797 ms, Bubble Time: 20.168 ms, Imbalance Overhead: 23.284 ms
GPU 7, Compute+Comm Time: 84.873 ms, Bubble Time: 20.572 ms, Imbalance Overhead: 5.803 ms
The estimated cost of the whole pipeline: 192.419 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 119.546 ms
Partition 0 [0, 3) has cost: 110.596 ms
Partition 1 [3, 5) has cost: 86.824 ms
Partition 2 [5, 7) has cost: 86.824 ms
Partition 3 [7, 9) has cost: 119.546 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 74.767 ms
GPU 0, Compute+Comm Time: 62.061 ms, Bubble Time: 11.771 ms, Imbalance Overhead: 0.935 ms
GPU 1, Compute+Comm Time: 58.643 ms, Bubble Time: 11.714 ms, Imbalance Overhead: 4.410 ms
GPU 2, Compute+Comm Time: 58.643 ms, Bubble Time: 11.447 ms, Imbalance Overhead: 4.677 ms
GPU 3, Compute+Comm Time: 62.498 ms, Bubble Time: 11.346 ms, Imbalance Overhead: 0.924 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 104.192 ms
GPU 0, Compute+Comm Time: 88.556 ms, Bubble Time: 15.399 ms, Imbalance Overhead: 0.237 ms
GPU 1, Compute+Comm Time: 75.636 ms, Bubble Time: 15.712 ms, Imbalance Overhead: 12.844 ms
GPU 2, Compute+Comm Time: 75.636 ms, Bubble Time: 16.348 ms, Imbalance Overhead: 12.208 ms
GPU 3, Compute+Comm Time: 84.404 ms, Bubble Time: 16.640 ms, Imbalance Overhead: 3.148 ms
    The estimated cost with 2 DP ways is 187.907 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 206.370 ms
Partition 0 [0, 5) has cost: 197.420 ms
Partition 1 [5, 9) has cost: 206.370 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 97.619 ms
GPU 0, Compute+Comm Time: 86.136 ms, Bubble Time: 10.845 ms, Imbalance Overhead: 0.637 ms
GPU 1, Compute+Comm Time: 86.357 ms, Bubble Time: 10.211 ms, Imbalance Overhead: 1.051 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 121.446 ms
GPU 0, Compute+Comm Time: 108.230 ms, Bubble Time: 12.637 ms, Imbalance Overhead: 0.579 ms
GPU 1, Compute+Comm Time: 106.120 ms, Bubble Time: 13.570 ms, Imbalance Overhead: 1.756 ms
    The estimated cost with 4 DP ways is 230.018 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 403.791 ms
Partition 0 [0, 9) has cost: 403.791 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 211.535 ms
GPU 0, Compute+Comm Time: 211.535 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 232.733 ms
GPU 0, Compute+Comm Time: 232.733 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 466.481 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 13)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [13, 20)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [20, 27)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [27, 34)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [34, 41)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [41, 48)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [48, 55)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [55, 65)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 13)...
+++++++++ Node 1 initializing the weights for op[13, 20)...
+++++++++ Node 4 initializing the weights for op[34, 41)...
+++++++++ Node 2 initializing the weights for op[20, 27)...
+++++++++ Node 3 initializing the weights for op[27, 34)...
+++++++++ Node 5 initializing the weights for op[41, 48)...
+++++++++ Node 6 initializing the weights for op[48, 55)...
+++++++++ Node 7 initializing the weights for op[55, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.2624
	Epoch 50:	Loss 4.9630
	Epoch 75:	Loss 4.6058
	Epoch 100:	Loss 4.2556
Node 0, Pre/Post-Pipelining: 0.850 / 6.570 ms, Bubble: 181.457 ms, Compute: 70.433 ms, Comm: 67.624 ms, Imbalance: 140.547 ms
Node 1, Pre/Post-Pipelining: 0.847 / 6.569 ms, Bubble: 177.011 ms, Compute: 46.902 ms, Comm: 93.718 ms, Imbalance: 142.947 ms
Node 5, Pre/Post-Pipelining: 0.851 / 6.543 ms, Bubble: 162.787 ms, Compute: 45.923 ms, Comm: 117.281 ms, Imbalance: 136.491 ms
Node 6, Pre/Post-Pipelining: 0.845 / 6.576 ms, Bubble: 158.630 ms, Compute: 46.636 ms, Comm: 94.233 ms, Imbalance: 162.474 ms
Node 7, Pre/Post-Pipelining: 0.850 / 126.084 ms, Bubble: 36.979 ms, Compute: 207.992 ms, Comm: 67.482 ms, Imbalance: 29.329 ms
Node 2, Pre/Post-Pipelining: 0.848 / 6.566 ms, Bubble: 173.627 ms, Compute: 46.657 ms, Comm: 117.952 ms, Imbalance: 123.259 ms
Node 4, Pre/Post-Pipelining: 0.847 / 6.584 ms, Bubble: 163.720 ms, Compute: 46.337 ms, Comm: 147.086 ms, Imbalance: 105.299 ms
Node 3, Pre/Post-Pipelining: 0.847 / 6.571 ms, Bubble: 166.062 ms, Compute: 46.676 ms, Comm: 146.718 ms, Imbalance: 101.614 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.850 ms
Cluster-Wide Average, Post-Pipelining Overhead: 6.570 ms
Cluster-Wide Average, Bubble: 181.457 ms
Cluster-Wide Average, Compute: 70.433 ms
Cluster-Wide Average, Communication: 67.624 ms
Cluster-Wide Average, Imbalance: 140.547 ms
Node 0, GPU memory consumption: 7.327 GB
Node 1, GPU memory consumption: 3.770 GB
Node 2, GPU memory consumption: 3.770 GB
Node 3, GPU memory consumption: 3.747 GB
Node 5, GPU memory consumption: 3.770 GB
Node 4, GPU memory consumption: 3.747 GB
Node 6, GPU memory consumption: 3.770 GB
Node 7, GPU memory consumption: 7.653 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.468170 s---------------
------------------------node id 1,  per-epoch time: 0.468171 s---------------
------------------------node id 2,  per-epoch time: 0.468173 s---------------
------------------------node id 3,  per-epoch time: 0.468172 s---------------
------------------------node id 4,  per-epoch time: 0.468173 s---------------
------------------------node id 5,  per-epoch time: 0.468171 s---------------
------------------------node id 6,  per-epoch time: 0.468172 s---------------
------------------------node id 7,  per-epoch time: 0.468172 s---------------
************ Profiling Results ************
	Bubble: 402.582566 (ms) (85.15 percentage)
	Compute: 68.161060 (ms) (14.42 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.032377 (ms) (0.43 percentage)
	Layer-level communication (cluster-wide, per-epoch): 7.681 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 7.685 GB
	Aggregated layer-level communication throughput: 619.465 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0019
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
