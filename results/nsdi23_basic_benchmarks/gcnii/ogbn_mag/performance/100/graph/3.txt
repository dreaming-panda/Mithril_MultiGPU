Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.191 seconds.
Building the CSC structure...
        It takes 0.213 seconds.
Building the CSC structure...
        It takes 0.221 seconds.
Building the CSC structure...
        It takes 0.244 seconds.
Building the CSC structure...
        It takes 0.248 seconds.
Building the CSC structure...
        It takes 0.243 seconds.
Building the CSC structure...
        It takes 0.257 seconds.
Building the CSC structure...
        It takes 0.271 seconds.
Building the CSC structure...
        It takes 0.190 seconds.
        It takes 0.207 seconds.
        It takes 0.210 seconds.
        It takes 0.233 seconds.
        It takes 0.240 seconds.
Building the Feature Vector...
        It takes 0.257 seconds.
        It takes 0.273 seconds.
        It takes 0.264 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.231 seconds.
Building the Label Vector...
        It takes 0.253 seconds.
Building the Label Vector...
        It takes 0.243 seconds.
Building the Label Vector...
        It takes 0.250 seconds.
Building the Label Vector...
        It takes 0.251 seconds.
Building the Label Vector...
        It takes 0.233 seconds.
Building the Label Vector...
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 0.528 seconds.
        It takes 0.587 seconds.
        It takes 0.594 seconds.
        It takes 0.584 seconds.
        It takes 0.548 seconds.
        It takes 0.587 seconds.
        It takes 0.595 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_mag/8_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
        It takes 0.599 seconds.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
736389, 11529061, 11529061
Number of vertices per chunk: 92049
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 92048) 1-[92048, 184097) 2-[184097, 276145) 3-[276145, 368194) 4-[368194, 460242) 5-[460242, 552291) 6-[552291, 644340) 7-[644340, 736389)
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
csr in-out ready !Start Cost Model Initialization...
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.478 Gbps (per GPU), 507.827 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.438 Gbps (per GPU), 507.502 Gbps (aggregated)
The layer-level communication performance: 63.438 Gbps (per GPU), 507.503 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.397 Gbps (per GPU), 507.177 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.402 Gbps (per GPU), 507.213 Gbps (aggregated)
The layer-level communication performance: 63.366 Gbps (per GPU), 506.930 Gbps (aggregated)
The layer-level communication performance: 63.359 Gbps (per GPU), 506.873 Gbps (aggregated)
The layer-level communication performance: 63.355 Gbps (per GPU), 506.837 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.515 Gbps (per GPU), 1316.121 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.508 Gbps (per GPU), 1316.063 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.515 Gbps (per GPU), 1316.121 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.508 Gbps (per GPU), 1316.063 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.516 Gbps (per GPU), 1316.125 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.512 Gbps (per GPU), 1316.099 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.514 Gbps (per GPU), 1316.108 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.490 Gbps (per GPU), 1315.918 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.267 Gbps (per GPU), 906.135 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.267 Gbps (per GPU), 906.136 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.267 Gbps (per GPU), 906.137 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.267 Gbps (per GPU), 906.136 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.266 Gbps (per GPU), 906.132 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.267 Gbps (per GPU), 906.133 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.266 Gbps (per GPU), 906.128 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.266 Gbps (per GPU), 906.127 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.040 Gbps (per GPU), 360.318 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.040 Gbps (per GPU), 360.318 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.039 Gbps (per GPU), 360.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.039 Gbps (per GPU), 360.314 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.040 Gbps (per GPU), 360.317 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.040 Gbps (per GPU), 360.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.039 Gbps (per GPU), 360.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.038 Gbps (per GPU), 360.305 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.39ms  5.45ms  9.23ms  3.86 92.05K  2.94M
 chk_1  2.40ms  4.69ms  8.46ms  3.52 92.05K  1.61M
 chk_2  2.41ms  4.57ms  8.34ms  3.46 92.05K  1.55M
 chk_3  2.41ms  4.37ms  8.12ms  3.37 92.05K  0.83M
 chk_4  2.41ms  4.60ms  8.32ms  3.45 92.05K  1.15M
 chk_5  2.41ms  4.38ms  8.10ms  3.36 92.05K  0.77M
 chk_6  2.41ms  4.62ms  8.35ms  3.46 92.05K  1.04M
 chk_7  2.41ms  4.41ms  8.14ms  3.37 92.05K  0.91M
   Avg  2.41  4.64  8.38
   Max  2.41  5.45  9.23
   Min  2.39  4.37  8.10
 Ratio  1.01  1.25  1.14
   Var  0.00  0.11  0.12
Profiling takes 1.517 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 67.065 ms
Partition 0 [0, 2) has cost: 56.351 ms
Partition 1 [2, 3) has cost: 37.085 ms
Partition 2 [3, 4) has cost: 37.085 ms
Partition 3 [4, 5) has cost: 37.085 ms
Partition 4 [5, 6) has cost: 37.085 ms
Partition 5 [6, 7) has cost: 37.085 ms
Partition 6 [7, 8) has cost: 37.085 ms
Partition 7 [8, 9) has cost: 67.065 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 102.156 ms
GPU 0, Compute+Comm Time: 53.435 ms, Bubble Time: 48.134 ms, Imbalance Overhead: 0.586 ms
GPU 1, Compute+Comm Time: 48.470 ms, Bubble Time: 48.130 ms, Imbalance Overhead: 5.557 ms
GPU 2, Compute+Comm Time: 48.470 ms, Bubble Time: 47.930 ms, Imbalance Overhead: 5.756 ms
GPU 3, Compute+Comm Time: 48.470 ms, Bubble Time: 47.759 ms, Imbalance Overhead: 5.928 ms
GPU 4, Compute+Comm Time: 48.470 ms, Bubble Time: 47.454 ms, Imbalance Overhead: 6.232 ms
GPU 5, Compute+Comm Time: 48.470 ms, Bubble Time: 47.311 ms, Imbalance Overhead: 6.375 ms
GPU 6, Compute+Comm Time: 48.470 ms, Bubble Time: 46.999 ms, Imbalance Overhead: 6.687 ms
GPU 7, Compute+Comm Time: 55.322 ms, Bubble Time: 46.834 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 153.641 ms
GPU 0, Compute+Comm Time: 85.986 ms, Bubble Time: 67.655 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 62.860 ms, Bubble Time: 68.601 ms, Imbalance Overhead: 22.181 ms
GPU 2, Compute+Comm Time: 62.860 ms, Bubble Time: 69.849 ms, Imbalance Overhead: 20.933 ms
GPU 3, Compute+Comm Time: 62.860 ms, Bubble Time: 70.796 ms, Imbalance Overhead: 19.986 ms
GPU 4, Compute+Comm Time: 62.860 ms, Bubble Time: 72.034 ms, Imbalance Overhead: 18.748 ms
GPU 5, Compute+Comm Time: 62.860 ms, Bubble Time: 73.000 ms, Imbalance Overhead: 17.782 ms
GPU 6, Compute+Comm Time: 62.860 ms, Bubble Time: 74.049 ms, Imbalance Overhead: 16.732 ms
GPU 7, Compute+Comm Time: 77.159 ms, Bubble Time: 74.662 ms, Imbalance Overhead: 1.820 ms
The estimated cost of the whole pipeline: 268.587 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 104.150 ms
Partition 0 [0, 3) has cost: 93.436 ms
Partition 1 [3, 5) has cost: 74.171 ms
Partition 2 [5, 7) has cost: 74.171 ms
Partition 3 [7, 9) has cost: 104.150 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 103.588 ms
GPU 0, Compute+Comm Time: 58.579 ms, Bubble Time: 44.214 ms, Imbalance Overhead: 0.795 ms
GPU 1, Compute+Comm Time: 56.094 ms, Bubble Time: 44.543 ms, Imbalance Overhead: 2.952 ms
GPU 2, Compute+Comm Time: 56.094 ms, Bubble Time: 44.289 ms, Imbalance Overhead: 3.205 ms
GPU 3, Compute+Comm Time: 59.518 ms, Bubble Time: 44.070 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 141.353 ms
GPU 0, Compute+Comm Time: 82.531 ms, Bubble Time: 58.822 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 70.950 ms, Bubble Time: 59.962 ms, Imbalance Overhead: 10.441 ms
GPU 2, Compute+Comm Time: 70.950 ms, Bubble Time: 61.079 ms, Imbalance Overhead: 9.325 ms
GPU 3, Compute+Comm Time: 78.103 ms, Bubble Time: 61.009 ms, Imbalance Overhead: 2.242 ms
    The estimated cost with 2 DP ways is 257.188 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 178.321 ms
Partition 0 [0, 5) has cost: 167.607 ms
Partition 1 [5, 9) has cost: 178.321 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 123.964 ms
GPU 0, Compute+Comm Time: 81.971 ms, Bubble Time: 40.671 ms, Imbalance Overhead: 1.322 ms
GPU 1, Compute+Comm Time: 82.439 ms, Bubble Time: 41.526 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 153.049 ms
GPU 0, Compute+Comm Time: 102.029 ms, Bubble Time: 51.020 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 99.813 ms, Bubble Time: 49.879 ms, Imbalance Overhead: 3.357 ms
    The estimated cost with 4 DP ways is 290.864 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 345.927 ms
Partition 0 [0, 9) has cost: 345.927 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 198.085 ms
GPU 0, Compute+Comm Time: 198.085 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 217.964 ms
GPU 0, Compute+Comm Time: 217.964 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 436.851 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 65)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 65)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 65)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 65)
*** Node 4, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 65)
*** Node 3, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 65)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 65)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 65)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 65)...
+++++++++ Node 1 initializing the weights for op[0, 65)...
+++++++++ Node 2 initializing the weights for op[0, 65)...
+++++++++ Node 3 initializing the weights for op[0, 65)...
+++++++++ Node 4 initializing the weights for op[0, 65)...
+++++++++ Node 5 initializing the weights for op[0, 65)...
+++++++++ Node 6 initializing the weights for op[0, 65)...
+++++++++ Node 7 initializing the weights for op[0, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.1420
	Epoch 50:	Loss 5.0234
	Epoch 75:	Loss 4.8017
	Epoch 100:	Loss 4.4796
Node 0, Pre/Post-Pipelining: 6.685 / 13.063 ms, Bubble: 1.489 ms, Compute: 112.527 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 1, Pre/Post-Pipelining: 6.683 / 14.320 ms, Bubble: 0.172 ms, Compute: 112.593 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 4, Pre/Post-Pipelining: 6.683 / 13.661 ms, Bubble: 1.046 ms, Compute: 112.382 ms, Comm: 0.008 ms, Imbalance: 0.013 ms
Node 5, Pre/Post-Pipelining: 6.681 / 13.642 ms, Bubble: 1.076 ms, Compute: 112.373 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 7, Pre/Post-Pipelining: 6.683 / 13.657 ms, Bubble: 0.979 ms, Compute: 112.446 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 6.683 / 13.715 ms, Bubble: 0.954 ms, Compute: 112.412 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 3, Pre/Post-Pipelining: 6.684 / 13.891 ms, Bubble: 0.538 ms, Compute: 112.653 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 6.683 / 13.893 ms, Bubble: 0.496 ms, Compute: 112.699 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.685 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.063 ms
Cluster-Wide Average, Bubble: 1.489 ms
Cluster-Wide Average, Compute: 112.527 ms
Cluster-Wide Average, Communication: 0.010 ms
Cluster-Wide Average, Imbalance: 0.017 ms
Node 1, GPU memory consumption: 20.702 GB
Node 4, GPU memory consumption: 20.678 GB
Node 0, GPU memory consumption: 21.210 GB
Node 5, GPU memory consumption: 20.702 GB
Node 2, GPU memory consumption: 20.702 GB
Node 3, GPU memory consumption: 20.678 GB
Node 6, GPU memory consumption: 20.702 GB
Node 7, GPU memory consumption: 20.678 GB
Node 0, Graph-Level Communication Throughput: 48.015 Gbps, Time: 39.294 ms
Node 1, Graph-Level Communication Throughput: 41.341 Gbps, Time: 41.945 ms
Node 4, Graph-Level Communication Throughput: 47.004 Gbps, Time: 44.111 ms
Node 2, Graph-Level Communication Throughput: 32.886 Gbps, Time: 42.822 ms
Node 5, Graph-Level Communication Throughput: 29.583 Gbps, Time: 44.004 ms
Node 3, Graph-Level Communication Throughput: 23.831 Gbps, Time: 43.551 ms
Node 6, Graph-Level Communication Throughput: 48.775 Gbps, Time: 41.776 ms
Node 7, Graph-Level Communication Throughput: 45.584 Gbps, Time: 42.998 ms
------------------------node id 0,  per-epoch time: 0.133840 s---------------
------------------------node id 1,  per-epoch time: 0.133840 s---------------
------------------------node id 2,  per-epoch time: 0.133841 s---------------
------------------------node id 3,  per-epoch time: 0.133841 s---------------
------------------------node id 4,  per-epoch time: 0.133840 s---------------
------------------------node id 5,  per-epoch time: 0.133840 s---------------
------------------------node id 6,  per-epoch time: 0.133843 s---------------
------------------------node id 7,  per-epoch time: 0.133841 s---------------
************ Profiling Results ************
	Bubble: 21.875924 (ms) (16.17 percentage)
	Compute: 55.431302 (ms) (40.97 percentage)
	GraphCommComputeOverhead: 2.422733 (ms) (1.79 percentage)
	GraphCommNetwork: 42.562448 (ms) (31.46 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.994833 (ms) (9.61 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 1.565 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 1.568 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0019
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
