Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 4 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.218 seconds.
Building the CSC structure...
        It takes 0.228 seconds.
Building the CSC structure...
        It takes 0.229 seconds.
Building the CSC structure...
        It takes 0.237 seconds.
Building the CSC structure...
        It takes 0.245 seconds.
Building the CSC structure...
        It takes 0.244 seconds.
Building the CSC structure...
        It takes 0.270 seconds.
Building the CSC structure...
        It takes 0.270 seconds.
Building the CSC structure...
        It takes 0.222 seconds.
        It takes 0.226 seconds.
        It takes 0.237 seconds.
        It takes 0.235 seconds.
        It takes 0.240 seconds.
        It takes 0.240 seconds.
        It takes 0.260 seconds.
        It takes 0.271 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 0.248 seconds.
Building the Label Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.245 seconds.
Building the Label Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.236 seconds.
Building the Label Vector...
        It takes 0.250 seconds.
Building the Label Vector...
        It takes 0.584 seconds.
        It takes 0.562 seconds.
        It takes 0.594 seconds.
        It takes 0.602 seconds.
        It takes 0.593 seconds.
        It takes 0.556 seconds.
        It takes 0.610 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_mag/8_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
        It takes 0.590 seconds.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 92049
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 92048) 1-[92048, 184097) 2-[184097, 276145) 3-[276145, 368194) 4-[368194, 460242) 5-[460242, 552291) 6-[552291, 644340) 7-[644340, 736389)
736389, 11529061, 11529061
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
736389, 11529061, 11529061
Number of vertices per chunk: 92049
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 62.580 Gbps (per GPU), 500.642 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.541 Gbps (per GPU), 500.331 Gbps (aggregated)
The layer-level communication performance: 62.540 Gbps (per GPU), 500.322 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.507 Gbps (per GPU), 500.058 Gbps (aggregated)
The layer-level communication performance: 62.502 Gbps (per GPU), 500.019 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.475 Gbps (per GPU), 499.798 Gbps (aggregated)
The layer-level communication performance: 62.467 Gbps (per GPU), 499.736 Gbps (aggregated)
The layer-level communication performance: 62.463 Gbps (per GPU), 499.708 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.685 Gbps (per GPU), 1309.483 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.676 Gbps (per GPU), 1309.406 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.664 Gbps (per GPU), 1309.310 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.686 Gbps (per GPU), 1309.486 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.687 Gbps (per GPU), 1309.496 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.649 Gbps (per GPU), 1309.195 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.687 Gbps (per GPU), 1309.495 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.681 Gbps (per GPU), 1309.451 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.113 Gbps (per GPU), 912.902 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.113 Gbps (per GPU), 912.901 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.113 Gbps (per GPU), 912.904 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.112 Gbps (per GPU), 912.898 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.111 Gbps (per GPU), 912.892 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.112 Gbps (per GPU), 912.897 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.112 Gbps (per GPU), 912.899 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.112 Gbps (per GPU), 912.897 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.474 Gbps (per GPU), 363.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.474 Gbps (per GPU), 363.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.473 Gbps (per GPU), 363.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.473 Gbps (per GPU), 363.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.474 Gbps (per GPU), 363.788 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.474 Gbps (per GPU), 363.788 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.473 Gbps (per GPU), 363.786 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.473 Gbps (per GPU), 363.788 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.39ms  5.45ms  9.23ms  3.86 92.05K  2.94M
 chk_1  2.40ms  4.68ms  8.44ms  3.52 92.05K  1.61M
 chk_2  2.41ms  4.57ms  8.33ms  3.46 92.05K  1.55M
 chk_3  2.41ms  4.38ms  8.11ms  3.37 92.05K  0.83M
 chk_4  2.41ms  4.58ms  8.32ms  3.46 92.05K  1.15M
 chk_5  2.41ms  4.38ms  8.10ms  3.36 92.05K  0.77M
 chk_6  2.41ms  4.61ms  8.35ms  3.46 92.05K  1.04M
 chk_7  2.41ms  4.42ms  8.15ms  3.38 92.05K  0.91M
   Avg  2.41  4.63  8.38
   Max  2.41  5.45  9.23
   Min  2.39  4.38  8.10
 Ratio  1.01  1.24  1.14
   Var  0.00  0.11  0.12
Profiling takes 1.519 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 67.020 ms
Partition 0 [0, 2) has cost: 56.311 ms
Partition 1 [2, 3) has cost: 37.064 ms
Partition 2 [3, 4) has cost: 37.064 ms
Partition 3 [4, 5) has cost: 37.064 ms
Partition 4 [5, 6) has cost: 37.064 ms
Partition 5 [6, 7) has cost: 37.064 ms
Partition 6 [7, 8) has cost: 37.064 ms
Partition 7 [8, 9) has cost: 67.020 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 103.113 ms
GPU 0, Compute+Comm Time: 53.933 ms, Bubble Time: 48.593 ms, Imbalance Overhead: 0.588 ms
GPU 1, Compute+Comm Time: 48.975 ms, Bubble Time: 48.593 ms, Imbalance Overhead: 5.545 ms
GPU 2, Compute+Comm Time: 48.975 ms, Bubble Time: 48.387 ms, Imbalance Overhead: 5.751 ms
GPU 3, Compute+Comm Time: 48.975 ms, Bubble Time: 48.219 ms, Imbalance Overhead: 5.919 ms
GPU 4, Compute+Comm Time: 48.975 ms, Bubble Time: 47.913 ms, Imbalance Overhead: 6.225 ms
GPU 5, Compute+Comm Time: 48.975 ms, Bubble Time: 47.748 ms, Imbalance Overhead: 6.389 ms
GPU 6, Compute+Comm Time: 48.975 ms, Bubble Time: 47.435 ms, Imbalance Overhead: 6.703 ms
GPU 7, Compute+Comm Time: 55.847 ms, Bubble Time: 47.266 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 154.594 ms
GPU 0, Compute+Comm Time: 86.483 ms, Bubble Time: 68.111 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 63.398 ms, Bubble Time: 69.065 ms, Imbalance Overhead: 22.131 ms
GPU 2, Compute+Comm Time: 63.398 ms, Bubble Time: 70.311 ms, Imbalance Overhead: 20.884 ms
GPU 3, Compute+Comm Time: 63.398 ms, Bubble Time: 71.259 ms, Imbalance Overhead: 19.936 ms
GPU 4, Compute+Comm Time: 63.398 ms, Bubble Time: 72.483 ms, Imbalance Overhead: 18.713 ms
GPU 5, Compute+Comm Time: 63.398 ms, Bubble Time: 73.448 ms, Imbalance Overhead: 17.747 ms
GPU 6, Compute+Comm Time: 63.398 ms, Bubble Time: 74.491 ms, Imbalance Overhead: 16.704 ms
GPU 7, Compute+Comm Time: 77.688 ms, Bubble Time: 75.098 ms, Imbalance Overhead: 1.809 ms
The estimated cost of the whole pipeline: 270.592 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 104.084 ms
Partition 0 [0, 3) has cost: 93.375 ms
Partition 1 [3, 5) has cost: 74.128 ms
Partition 2 [5, 7) has cost: 74.128 ms
Partition 3 [7, 9) has cost: 104.084 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 104.524 ms
GPU 0, Compute+Comm Time: 59.106 ms, Bubble Time: 44.618 ms, Imbalance Overhead: 0.801 ms
GPU 1, Compute+Comm Time: 56.625 ms, Bubble Time: 44.953 ms, Imbalance Overhead: 2.946 ms
GPU 2, Compute+Comm Time: 56.625 ms, Bubble Time: 44.716 ms, Imbalance Overhead: 3.183 ms
GPU 3, Compute+Comm Time: 60.066 ms, Bubble Time: 44.459 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 142.292 ms
GPU 0, Compute+Comm Time: 83.068 ms, Bubble Time: 59.224 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 71.495 ms, Bubble Time: 60.360 ms, Imbalance Overhead: 10.437 ms
GPU 2, Compute+Comm Time: 71.495 ms, Bubble Time: 61.475 ms, Imbalance Overhead: 9.322 ms
GPU 3, Compute+Comm Time: 78.644 ms, Bubble Time: 61.417 ms, Imbalance Overhead: 2.231 ms
    The estimated cost with 2 DP ways is 259.157 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 178.213 ms
Partition 0 [0, 5) has cost: 167.503 ms
Partition 1 [5, 9) has cost: 178.213 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 124.405 ms
GPU 0, Compute+Comm Time: 82.245 ms, Bubble Time: 40.796 ms, Imbalance Overhead: 1.364 ms
GPU 1, Compute+Comm Time: 82.720 ms, Bubble Time: 41.685 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 153.430 ms
GPU 0, Compute+Comm Time: 102.291 ms, Bubble Time: 51.140 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 100.076 ms, Bubble Time: 50.020 ms, Imbalance Overhead: 3.335 ms
    The estimated cost with 4 DP ways is 291.727 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 345.715 ms
Partition 0 [0, 9) has cost: 345.715 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 196.351 ms
GPU 0, Compute+Comm Time: 196.351 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 216.155 ms
GPU 0, Compute+Comm Time: 216.155 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 433.130 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 65)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 65)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 65)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 65)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 65)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 65)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 65)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 65)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 65)...
+++++++++ Node 1 initializing the weights for op[0, 65)...
+++++++++ Node 2 initializing the weights for op[0, 65)...
+++++++++ Node 3 initializing the weights for op[0, 65)...
+++++++++ Node 4 initializing the weights for op[0, 65)...
+++++++++ Node 5 initializing the weights for op[0, 65)...
+++++++++ Node 6 initializing the weights for op[0, 65)...
+++++++++ Node 7 initializing the weights for op[0, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.1332
	Epoch 50:	Loss 4.9912
	Epoch 75:	Loss 4.6885
	Epoch 100:	Loss 4.4571
Node 0, Pre/Post-Pipelining: 6.686 / 13.287 ms, Bubble: 1.529 ms, Compute: 112.658 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 2, Pre/Post-Pipelining: 6.682 / 13.905 ms, Bubble: 1.085 ms, Compute: 112.494 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 3, Pre/Post-Pipelining: 6.684 / 14.124 ms, Bubble: 0.560 ms, Compute: 112.796 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 1, Pre/Post-Pipelining: 6.689 / 14.634 ms, Bubble: 0.136 ms, Compute: 112.702 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 6.683 / 14.110 ms, Bubble: 0.551 ms, Compute: 112.820 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 5, Pre/Post-Pipelining: 6.681 / 13.904 ms, Bubble: 1.125 ms, Compute: 112.453 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 4, Pre/Post-Pipelining: 6.684 / 13.937 ms, Bubble: 1.063 ms, Compute: 112.477 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 6.683 / 13.889 ms, Bubble: 1.056 ms, Compute: 112.535 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.686 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.287 ms
Cluster-Wide Average, Bubble: 1.529 ms
Cluster-Wide Average, Compute: 112.658 ms
Cluster-Wide Average, Communication: 0.009 ms
Cluster-Wide Average, Imbalance: 0.016 ms
Node 0, GPU memory consumption: 21.208 GB
Node 2, GPU memory consumption: 20.702 GB
Node 4, GPU memory consumption: 20.678 GB
Node 1, GPU memory consumption: 20.702 GB
Node 5, GPU memory consumption: 20.702 GB
Node 3, GPU memory consumption: 20.678 GB
Node 7, GPU memory consumption: 20.678 GB
Node 6, GPU memory consumption: 20.702 GB
Node 0, Graph-Level Communication Throughput: 47.830 Gbps, Time: 39.446 ms
Node 1, Graph-Level Communication Throughput: 41.295 Gbps, Time: 41.992 ms
Node 2, Graph-Level Communication Throughput: 32.596 Gbps, Time: 43.203 ms
Node 3, Graph-Level Communication Throughput: 23.643 Gbps, Time: 43.898 ms
Node 4, Graph-Level Communication Throughput: 47.243 Gbps, Time: 43.888 ms
Node 5, Graph-Level Communication Throughput: 29.524 Gbps, Time: 44.092 ms
Node 6, Graph-Level Communication Throughput: 48.674 Gbps, Time: 41.862 ms
Node 7, Graph-Level Communication Throughput: 45.485 Gbps, Time: 43.091 ms
------------------------node id 0,  per-epoch time: 0.134235 s---------------
------------------------node id 1,  per-epoch time: 0.134236 s---------------
------------------------node id 2,  per-epoch time: 0.134235 s---------------
------------------------node id 3,  per-epoch time: 0.134235 s---------------
------------------------node id 4,  per-epoch time: 0.134237 s---------------
------------------------node id 5,  per-epoch time: 0.134236 s---------------
------------------------node id 6,  per-epoch time: 0.134235 s---------------
------------------------node id 7,  per-epoch time: 0.134237 s---------------
************ Profiling Results ************
	Bubble: 22.134683 (ms) (16.32 percentage)
	Compute: 55.389563 (ms) (40.85 percentage)
	GraphCommComputeOverhead: 2.418118 (ms) (1.78 percentage)
	GraphCommNetwork: 42.683885 (ms) (31.48 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.973137 (ms) (9.57 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 1.565 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 1.568 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0019
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
