Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.203 seconds.
Building the CSC structure...
        It takes 0.205 seconds.
Building the CSC structure...
        It takes 0.210 seconds.
Building the CSC structure...
        It takes 0.214 seconds.
Building the CSC structure...
        It takes 0.238 seconds.
Building the CSC structure...
        It takes 0.242 seconds.
Building the CSC structure...
        It takes 0.248 seconds.
Building the CSC structure...
        It takes 0.273 seconds.
Building the CSC structure...
        It takes 0.189 seconds.
        It takes 0.189 seconds.
        It takes 0.207 seconds.
        It takes 0.207 seconds.
        It takes 0.240 seconds.
        It takes 0.246 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.273 seconds.
        It takes 0.263 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.243 seconds.
Building the Label Vector...
        It takes 0.242 seconds.
Building the Label Vector...
        It takes 0.248 seconds.
Building the Label Vector...
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.534 seconds.
        It takes 0.586 seconds.
        It takes 0.560 seconds.
        It takes 0.587 seconds.
        It takes 0.585 seconds.
        It takes 0.597 seconds.
        It takes 0.598 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_mag/8_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
        It takes 0.592 seconds.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 92048) 1-[92048, 184097) 2-[184097, 276145) 3-[276145, 368194) 4-[368194, 460242) 5-[460242, 552291) 6-[552291, 644340) 7-[644340, 736389)
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
csr in-out ready !Start Cost Model Initialization...
736389, 11529061, 11529061
Number of vertices per chunk: 92049
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 62.601 Gbps (per GPU), 500.808 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.566 Gbps (per GPU), 500.528 Gbps (aggregated)
The layer-level communication performance: 62.565 Gbps (per GPU), 500.518 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.534 Gbps (per GPU), 500.270 Gbps (aggregated)
The layer-level communication performance: 62.530 Gbps (per GPU), 500.241 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.500 Gbps (per GPU), 499.997 Gbps (aggregated)
The layer-level communication performance: 62.493 Gbps (per GPU), 499.942 Gbps (aggregated)
The layer-level communication performance: 62.489 Gbps (per GPU), 499.909 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.723 Gbps (per GPU), 1325.784 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.726 Gbps (per GPU), 1325.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.710 Gbps (per GPU), 1325.683 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.712 Gbps (per GPU), 1325.699 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.720 Gbps (per GPU), 1325.761 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.721 Gbps (per GPU), 1325.768 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.718 Gbps (per GPU), 1325.745 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.721 Gbps (per GPU), 1325.768 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 112.255 Gbps (per GPU), 898.042 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.255 Gbps (per GPU), 898.044 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.255 Gbps (per GPU), 898.039 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.255 Gbps (per GPU), 898.041 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.255 Gbps (per GPU), 898.040 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.254 Gbps (per GPU), 898.036 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.254 Gbps (per GPU), 898.034 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 112.254 Gbps (per GPU), 898.034 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.915 Gbps (per GPU), 359.321 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.915 Gbps (per GPU), 359.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.915 Gbps (per GPU), 359.317 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.915 Gbps (per GPU), 359.320 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.915 Gbps (per GPU), 359.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.915 Gbps (per GPU), 359.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.915 Gbps (per GPU), 359.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.908 Gbps (per GPU), 359.261 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.38ms  5.44ms  9.18ms  3.86 92.05K  2.94M
 chk_1  2.40ms  4.66ms  8.42ms  3.52 92.05K  1.61M
 chk_2  2.39ms  4.55ms  8.30ms  3.47 92.05K  1.55M
 chk_3  2.40ms  4.36ms  8.08ms  3.37 92.05K  0.83M
 chk_4  2.40ms  4.57ms  8.29ms  3.45 92.05K  1.15M
 chk_5  2.40ms  4.36ms  8.08ms  3.37 92.05K  0.77M
 chk_6  2.40ms  4.60ms  8.33ms  3.47 92.05K  1.04M
 chk_7  2.40ms  4.40ms  8.12ms  3.39 92.05K  0.91M
   Avg  2.40  4.62  8.35
   Max  2.40  5.44  9.18
   Min  2.38  4.36  8.08
 Ratio  1.01  1.25  1.14
   Var  0.00  0.11  0.11
Profiling takes 1.508 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 66.806 ms
Partition 0 [0, 2) has cost: 56.113 ms
Partition 1 [2, 3) has cost: 36.942 ms
Partition 2 [3, 4) has cost: 36.942 ms
Partition 3 [4, 5) has cost: 36.942 ms
Partition 4 [5, 6) has cost: 36.942 ms
Partition 5 [6, 7) has cost: 36.942 ms
Partition 6 [7, 8) has cost: 36.942 ms
Partition 7 [8, 9) has cost: 66.806 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 102.971 ms
GPU 0, Compute+Comm Time: 53.861 ms, Bubble Time: 48.528 ms, Imbalance Overhead: 0.582 ms
GPU 1, Compute+Comm Time: 48.929 ms, Bubble Time: 48.532 ms, Imbalance Overhead: 5.510 ms
GPU 2, Compute+Comm Time: 48.929 ms, Bubble Time: 48.326 ms, Imbalance Overhead: 5.716 ms
GPU 3, Compute+Comm Time: 48.929 ms, Bubble Time: 48.158 ms, Imbalance Overhead: 5.884 ms
GPU 4, Compute+Comm Time: 48.929 ms, Bubble Time: 47.855 ms, Imbalance Overhead: 6.187 ms
GPU 5, Compute+Comm Time: 48.929 ms, Bubble Time: 47.689 ms, Imbalance Overhead: 6.353 ms
GPU 6, Compute+Comm Time: 48.929 ms, Bubble Time: 47.373 ms, Imbalance Overhead: 6.669 ms
GPU 7, Compute+Comm Time: 55.766 ms, Bubble Time: 47.205 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 154.314 ms
GPU 0, Compute+Comm Time: 86.325 ms, Bubble Time: 67.988 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 63.297 ms, Bubble Time: 68.943 ms, Imbalance Overhead: 22.073 ms
GPU 2, Compute+Comm Time: 63.297 ms, Bubble Time: 70.192 ms, Imbalance Overhead: 20.824 ms
GPU 3, Compute+Comm Time: 63.297 ms, Bubble Time: 71.141 ms, Imbalance Overhead: 19.876 ms
GPU 4, Compute+Comm Time: 63.297 ms, Bubble Time: 72.366 ms, Imbalance Overhead: 18.650 ms
GPU 5, Compute+Comm Time: 63.297 ms, Bubble Time: 73.332 ms, Imbalance Overhead: 17.685 ms
GPU 6, Compute+Comm Time: 63.297 ms, Bubble Time: 74.365 ms, Imbalance Overhead: 16.651 ms
GPU 7, Compute+Comm Time: 77.537 ms, Bubble Time: 74.969 ms, Imbalance Overhead: 1.808 ms
The estimated cost of the whole pipeline: 270.148 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 103.748 ms
Partition 0 [0, 3) has cost: 93.055 ms
Partition 1 [3, 5) has cost: 73.884 ms
Partition 2 [5, 7) has cost: 73.884 ms
Partition 3 [7, 9) has cost: 103.748 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 104.270 ms
GPU 0, Compute+Comm Time: 58.968 ms, Bubble Time: 44.508 ms, Imbalance Overhead: 0.795 ms
GPU 1, Compute+Comm Time: 56.497 ms, Bubble Time: 44.848 ms, Imbalance Overhead: 2.925 ms
GPU 2, Compute+Comm Time: 56.497 ms, Bubble Time: 44.614 ms, Imbalance Overhead: 3.159 ms
GPU 3, Compute+Comm Time: 59.913 ms, Bubble Time: 44.357 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 141.937 ms
GPU 0, Compute+Comm Time: 82.850 ms, Bubble Time: 59.088 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 71.327 ms, Bubble Time: 60.228 ms, Imbalance Overhead: 10.382 ms
GPU 2, Compute+Comm Time: 71.327 ms, Bubble Time: 61.341 ms, Imbalance Overhead: 9.269 ms
GPU 3, Compute+Comm Time: 78.455 ms, Bubble Time: 61.254 ms, Imbalance Overhead: 2.228 ms
    The estimated cost with 2 DP ways is 258.518 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 177.632 ms
Partition 0 [0, 5) has cost: 166.939 ms
Partition 1 [5, 9) has cost: 177.632 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 125.104 ms
GPU 0, Compute+Comm Time: 82.715 ms, Bubble Time: 41.028 ms, Imbalance Overhead: 1.362 ms
GPU 1, Compute+Comm Time: 83.181 ms, Bubble Time: 41.924 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 154.092 ms
GPU 0, Compute+Comm Time: 102.718 ms, Bubble Time: 51.374 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 100.517 ms, Bubble Time: 50.232 ms, Imbalance Overhead: 3.344 ms
    The estimated cost with 4 DP ways is 293.156 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 344.570 ms
Partition 0 [0, 9) has cost: 344.570 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 198.573 ms
GPU 0, Compute+Comm Time: 198.573 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 218.356 ms
GPU 0, Compute+Comm Time: 218.356 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 437.776 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 65)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 65)
*** Node 1, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 65)
*** Node 4, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 65)
*** Node 2, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 65)
*** Node 5, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 65)
*** Node 3, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 65)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 65)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 65)...
+++++++++ Node 1 initializing the weights for op[0, 65)...
+++++++++ Node 3 initializing the weights for op[0, 65)...
+++++++++ Node 2 initializing the weights for op[0, 65)...
+++++++++ Node 5 initializing the weights for op[0, 65)...
+++++++++ Node 6 initializing the weights for op[0, 65)...
+++++++++ Node 7 initializing the weights for op[0, 65)...
+++++++++ Node 4 initializing the weights for op[0, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 7, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.1331
	Epoch 50:	Loss 5.0140
	Epoch 75:	Loss 4.7856
	Epoch 100:	Loss 4.4257
Node 0, Pre/Post-Pipelining: 6.683 / 13.079 ms, Bubble: 1.529 ms, Compute: 112.240 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 1, Pre/Post-Pipelining: 6.682 / 14.244 ms, Bubble: 0.304 ms, Compute: 112.299 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 3, Pre/Post-Pipelining: 6.685 / 14.004 ms, Bubble: 0.322 ms, Compute: 112.518 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 6.682 / 13.928 ms, Bubble: 0.466 ms, Compute: 112.457 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 7, Pre/Post-Pipelining: 6.682 / 13.673 ms, Bubble: 1.042 ms, Compute: 112.135 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 4, Pre/Post-Pipelining: 6.685 / 13.794 ms, Bubble: 0.861 ms, Compute: 112.189 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 5, Pre/Post-Pipelining: 6.682 / 13.723 ms, Bubble: 1.037 ms, Compute: 112.089 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 2, Pre/Post-Pipelining: 6.682 / 13.770 ms, Bubble: 0.934 ms, Compute: 112.143 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.683 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.079 ms
Cluster-Wide Average, Bubble: 1.529 ms
Cluster-Wide Average, Compute: 112.240 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.014 ms
Node 3, GPU memory consumption: 20.678 GB
Node 1, GPU memory consumption: 20.702 GB
Node 0, GPU memory consumption: 21.210 GB
Node 4, GPU memory consumption: 20.678 GB
Node 2, GPU memory consumption: 20.702 GB
Node 6, GPU memory consumption: 20.702 GB
Node 5, GPU memory consumption: 20.702 GB
Node 7, GPU memory consumption: 20.678 GB
Node 0, Graph-Level Communication Throughput: 48.048 Gbps, Time: 39.267 ms
Node 1, Graph-Level Communication Throughput: 41.441 Gbps, Time: 41.844 ms
Node 4, Graph-Level Communication Throughput: 47.635 Gbps, Time: 43.526 ms
Node 2, Graph-Level Communication Throughput: 32.973 Gbps, Time: 42.709 ms
Node 5, Graph-Level Communication Throughput: 29.805 Gbps, Time: 43.676 ms
Node 3, Graph-Level Communication Throughput: 23.902 Gbps, Time: 43.422 ms
Node 6, Graph-Level Communication Throughput: 48.948 Gbps, Time: 41.628 ms
Node 7, Graph-Level Communication Throughput: 45.715 Gbps, Time: 42.874 ms
------------------------node id 0,  per-epoch time: 0.133600 s---------------
------------------------node id 1,  per-epoch time: 0.133601 s---------------
------------------------node id 2,  per-epoch time: 0.133604 s---------------
------------------------node id 3,  per-epoch time: 0.133602 s---------------
------------------------node id 4,  per-epoch time: 0.133602 s---------------
------------------------node id 5,  per-epoch time: 0.133602 s---------------
------------------------node id 6,  per-epoch time: 0.133601 s---------------
------------------------node id 7,  per-epoch time: 0.133601 s---------------
************ Profiling Results ************
	Bubble: 21.761669 (ms) (16.14 percentage)
	Compute: 55.352171 (ms) (41.05 percentage)
	GraphCommComputeOverhead: 2.413213 (ms) (1.79 percentage)
	GraphCommNetwork: 42.368069 (ms) (31.42 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.955423 (ms) (9.61 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 1.565 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 1.568 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0019
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
