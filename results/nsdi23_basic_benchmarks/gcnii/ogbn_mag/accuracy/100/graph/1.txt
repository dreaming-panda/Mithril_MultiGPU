Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.192 seconds.
Building the CSC structure...
        It takes 0.231 seconds.
Building the CSC structure...
        It takes 0.240 seconds.
Building the CSC structure...
        It takes 0.242 seconds.
Building the CSC structure...
        It takes 0.252 seconds.
Building the CSC structure...
        It takes 0.261 seconds.
Building the CSC structure...
        It takes 0.256 seconds.
Building the CSC structure...
        It takes 0.269 seconds.
Building the CSC structure...
        It takes 0.190 seconds.
        It takes 0.233 seconds.
        It takes 0.239 seconds.
Building the Feature Vector...
        It takes 0.243 seconds.
        It takes 0.249 seconds.
        It takes 0.261 seconds.
        It takes 0.265 seconds.
        It takes 0.280 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.227 seconds.
Building the Label Vector...
        It takes 0.240 seconds.
Building the Label Vector...
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.255 seconds.
Building the Label Vector...
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.237 seconds.
Building the Label Vector...
        It takes 0.250 seconds.
Building the Label Vector...
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.518 seconds.
        It takes 0.579 seconds.
        It takes 0.590 seconds.
        It takes 0.595 seconds.
        It takes 0.585 seconds.
        It takes 0.567 seconds.
        It takes 0.591 seconds.
        It takes 0.599 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_mag/8_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
736389, 11529061, 11529061
Number of vertices per chunk: 92049
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 92048) 1-[92048, 184097) 2-[184097, 276145) 3-[276145, 368194) 4-[368194, 460242) 5-[460242, 552291) 6-[552291, 644340) 7-[644340, 736389)
csr in-out ready !Start Cost Model Initialization...
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.720 Gbps (per GPU), 509.762 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.679 Gbps (per GPU), 509.436 Gbps (aggregated)
The layer-level communication performance: 63.679 Gbps (per GPU), 509.429 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.639 Gbps (per GPU), 509.114 Gbps (aggregated)
The layer-level communication performance: 63.635 Gbps (per GPU), 509.082 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.606 Gbps (per GPU), 508.852 Gbps (aggregated)
The layer-level communication performance: 63.599 Gbps (per GPU), 508.795 Gbps (aggregated)
The layer-level communication performance: 63.595 Gbps (per GPU), 508.759 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.676 Gbps (per GPU), 1309.409 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.679 Gbps (per GPU), 1309.435 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.678 Gbps (per GPU), 1309.422 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.675 Gbps (per GPU), 1309.397 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.673 Gbps (per GPU), 1309.387 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.674 Gbps (per GPU), 1309.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.671 Gbps (per GPU), 1309.371 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.674 Gbps (per GPU), 1309.394 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.168 Gbps (per GPU), 913.343 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.169 Gbps (per GPU), 913.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.169 Gbps (per GPU), 913.350 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.168 Gbps (per GPU), 913.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.168 Gbps (per GPU), 913.344 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.169 Gbps (per GPU), 913.349 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.168 Gbps (per GPU), 913.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.168 Gbps (per GPU), 913.345 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.867 Gbps (per GPU), 358.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.867 Gbps (per GPU), 358.939 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.867 Gbps (per GPU), 358.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.867 Gbps (per GPU), 358.935 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.867 Gbps (per GPU), 358.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.867 Gbps (per GPU), 358.934 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.867 Gbps (per GPU), 358.934 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.867 Gbps (per GPU), 358.936 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.39ms  5.44ms  9.36ms  3.92 92.05K  2.94M
 chk_1  2.40ms  4.66ms  8.39ms  3.50 92.05K  1.61M
 chk_2  2.65ms  4.55ms  8.29ms  3.13 92.05K  1.55M
 chk_3  2.40ms  4.37ms  8.09ms  3.37 92.05K  0.83M
 chk_4  2.40ms  4.57ms  8.29ms  3.45 92.05K  1.15M
 chk_5  2.40ms  4.37ms  8.08ms  3.36 92.05K  0.77M
 chk_6  2.40ms  4.61ms  8.32ms  3.46 92.05K  1.04M
 chk_7  2.40ms  4.41ms  8.28ms  3.45 92.05K  0.91M
   Avg  2.43  4.63  8.39
   Max  2.65  5.44  9.36
   Min  2.39  4.37  8.08
 Ratio  1.11  1.25  1.16
   Var  0.01  0.11  0.15
Profiling takes 1.518 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 67.107 ms
Partition 0 [0, 2) has cost: 56.449 ms
Partition 1 [2, 3) has cost: 37.001 ms
Partition 2 [3, 4) has cost: 37.001 ms
Partition 3 [4, 5) has cost: 37.001 ms
Partition 4 [5, 6) has cost: 37.001 ms
Partition 5 [6, 7) has cost: 37.001 ms
Partition 6 [7, 8) has cost: 37.001 ms
Partition 7 [8, 9) has cost: 67.107 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 102.208 ms
GPU 0, Compute+Comm Time: 53.485 ms, Bubble Time: 47.953 ms, Imbalance Overhead: 0.770 ms
GPU 1, Compute+Comm Time: 48.285 ms, Bubble Time: 47.965 ms, Imbalance Overhead: 5.957 ms
GPU 2, Compute+Comm Time: 48.285 ms, Bubble Time: 47.762 ms, Imbalance Overhead: 6.161 ms
GPU 3, Compute+Comm Time: 48.285 ms, Bubble Time: 47.847 ms, Imbalance Overhead: 6.076 ms
GPU 4, Compute+Comm Time: 48.285 ms, Bubble Time: 47.543 ms, Imbalance Overhead: 6.380 ms
GPU 5, Compute+Comm Time: 48.285 ms, Bubble Time: 47.381 ms, Imbalance Overhead: 6.542 ms
GPU 6, Compute+Comm Time: 48.285 ms, Bubble Time: 47.069 ms, Imbalance Overhead: 6.853 ms
GPU 7, Compute+Comm Time: 55.302 ms, Bubble Time: 46.905 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 153.214 ms
GPU 0, Compute+Comm Time: 85.767 ms, Bubble Time: 67.447 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 62.678 ms, Bubble Time: 68.547 ms, Imbalance Overhead: 21.989 ms
GPU 2, Compute+Comm Time: 62.678 ms, Bubble Time: 69.783 ms, Imbalance Overhead: 20.752 ms
GPU 3, Compute+Comm Time: 62.678 ms, Bubble Time: 70.724 ms, Imbalance Overhead: 19.812 ms
GPU 4, Compute+Comm Time: 62.678 ms, Bubble Time: 71.940 ms, Imbalance Overhead: 18.596 ms
GPU 5, Compute+Comm Time: 62.678 ms, Bubble Time: 72.910 ms, Imbalance Overhead: 17.626 ms
GPU 6, Compute+Comm Time: 62.678 ms, Bubble Time: 73.932 ms, Imbalance Overhead: 16.603 ms
GPU 7, Compute+Comm Time: 76.926 ms, Bubble Time: 74.510 ms, Imbalance Overhead: 1.777 ms
The estimated cost of the whole pipeline: 268.192 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 104.108 ms
Partition 0 [0, 3) has cost: 93.450 ms
Partition 1 [3, 5) has cost: 74.002 ms
Partition 2 [5, 7) has cost: 74.002 ms
Partition 3 [7, 9) has cost: 104.108 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 103.741 ms
GPU 0, Compute+Comm Time: 58.672 ms, Bubble Time: 44.090 ms, Imbalance Overhead: 0.979 ms
GPU 1, Compute+Comm Time: 55.942 ms, Bubble Time: 44.431 ms, Imbalance Overhead: 3.368 ms
GPU 2, Compute+Comm Time: 55.942 ms, Bubble Time: 44.449 ms, Imbalance Overhead: 3.350 ms
GPU 3, Compute+Comm Time: 59.547 ms, Bubble Time: 44.194 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 140.952 ms
GPU 0, Compute+Comm Time: 82.283 ms, Bubble Time: 58.669 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 70.782 ms, Bubble Time: 59.825 ms, Imbalance Overhead: 10.345 ms
GPU 2, Compute+Comm Time: 70.782 ms, Bubble Time: 60.942 ms, Imbalance Overhead: 9.227 ms
GPU 3, Compute+Comm Time: 77.912 ms, Bubble Time: 60.850 ms, Imbalance Overhead: 2.190 ms
    The estimated cost with 2 DP ways is 256.928 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 178.109 ms
Partition 0 [0, 5) has cost: 167.451 ms
Partition 1 [5, 9) has cost: 178.109 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 123.782 ms
GPU 0, Compute+Comm Time: 81.798 ms, Bubble Time: 40.448 ms, Imbalance Overhead: 1.536 ms
GPU 1, Compute+Comm Time: 82.200 ms, Bubble Time: 41.582 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 152.323 ms
GPU 0, Compute+Comm Time: 101.552 ms, Bubble Time: 50.771 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 99.354 ms, Bubble Time: 49.683 ms, Imbalance Overhead: 3.286 ms
    The estimated cost with 4 DP ways is 289.910 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 345.561 ms
Partition 0 [0, 9) has cost: 345.561 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 199.215 ms
GPU 0, Compute+Comm Time: 199.215 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 218.523 ms
GPU 0, Compute+Comm Time: 218.523 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 438.625 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 65)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 65)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 65)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 65)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 65)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 65)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 65)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 65)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 65)...
+++++++++ Node 1 initializing the weights for op[0, 65)...
+++++++++ Node 2 initializing the weights for op[0, 65)...
+++++++++ Node 3 initializing the weights for op[0, 65)...
+++++++++ Node 4 initializing the weights for op[0, 65)...
+++++++++ Node 5 initializing the weights for op[0, 65)...
+++++++++ Node 7 initializing the weights for op[0, 65)...
+++++++++ Node 6 initializing the weights for op[0, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.1331	TrainAcc 0.0448	ValidAcc 0.0348	TestAcc 0.0048	BestValid 0.0348
	Epoch 50:	Loss 5.0140	TrainAcc 0.1048	ValidAcc 0.1011	TestAcc 0.0811	BestValid 0.1011
	Epoch 75:	Loss 4.7856	TrainAcc 0.1027	ValidAcc 0.1023	TestAcc 0.0822	BestValid 0.1023
	Epoch 100:	Loss 4.4257	TrainAcc 0.1336	ValidAcc 0.1159	TestAcc 0.1991	BestValid 0.1159
Node 1, Pre/Post-Pipelining: 6.683 / 14.222 ms, Bubble: 0.500 ms, Compute: 112.817 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 0, Pre/Post-Pipelining: 6.685 / 13.350 ms, Bubble: 1.425 ms, Compute: 112.758 ms, Comm: 0.010 ms, Imbalance: 0.016 ms
Node 4, Pre/Post-Pipelining: 6.684 / 14.092 ms, Bubble: 0.640 ms, Compute: 112.810 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 2, Pre/Post-Pipelining: 6.684 / 14.046 ms, Bubble: 0.806 ms, Compute: 112.679 ms, Comm: 0.009 ms, Imbalance: 0.015 ms
Node 7, Pre/Post-Pipelining: 6.682 / 13.933 ms, Bubble: 0.951 ms, Compute: 112.659 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 3, Pre/Post-Pipelining: 6.684 / 14.286 ms, Bubble: 0.180 ms, Compute: 113.078 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 5, Pre/Post-Pipelining: 6.682 / 13.987 ms, Bubble: 0.888 ms, Compute: 112.666 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 6, Pre/Post-Pipelining: 6.684 / 14.201 ms, Bubble: 0.394 ms, Compute: 112.923 ms, Comm: 0.009 ms, Imbalance: 0.015 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.685 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.350 ms
Cluster-Wide Average, Bubble: 1.425 ms
Cluster-Wide Average, Compute: 112.758 ms
Cluster-Wide Average, Communication: 0.010 ms
Cluster-Wide Average, Imbalance: 0.016 ms
Node 0, GPU memory consumption: 21.210 GB
Node 4, GPU memory consumption: 20.678 GB
Node 3, GPU memory consumption: 20.678 GB
Node 5, GPU memory consumption: 20.702 GB
Node 2, GPU memory consumption: 20.702 GB
Node 6, GPU memory consumption: 20.702 GB
Node 1, GPU memory consumption: 20.702 GB
Node 7, GPU memory consumption: 20.678 GB
Node 0, Graph-Level Communication Throughput: 47.609 Gbps, Time: 39.630 ms
Node 1, Graph-Level Communication Throughput: 41.137 Gbps, Time: 42.153 ms
Node 4, Graph-Level Communication Throughput: 47.001 Gbps, Time: 44.114 ms
Node 2, Graph-Level Communication Throughput: 32.596 Gbps, Time: 43.204 ms
Node 5, Graph-Level Communication Throughput: 29.499 Gbps, Time: 44.129 ms
Node 3, Graph-Level Communication Throughput: 23.561 Gbps, Time: 44.050 ms
Node 6, Graph-Level Communication Throughput: 48.594 Gbps, Time: 41.931 ms
Node 7, Graph-Level Communication Throughput: 45.120 Gbps, Time: 43.439 ms
------------------------node id 0,  per-epoch time: 0.566984 s---------------
------------------------node id 1,  per-epoch time: 0.566982 s---------------
------------------------node id 2,  per-epoch time: 0.566984 s---------------
------------------------node id 3,  per-epoch time: 0.566984 s---------------
------------------------node id 4,  per-epoch time: 0.566982 s---------------
------------------------node id 5,  per-epoch time: 0.566984 s---------------
------------------------node id 6,  per-epoch time: 0.566984 s---------------
------------------------node id 7,  per-epoch time: 0.566983 s---------------
************ Profiling Results ************
	Bubble: 432.943646 (ms) (79.21 percentage)
	Compute: 55.377896 (ms) (10.13 percentage)
	GraphCommComputeOverhead: 2.414788 (ms) (0.44 percentage)
	GraphCommNetwork: 42.831014 (ms) (7.84 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.980575 (ms) (2.38 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 1.565 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 1.568 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.1159
Target test_acc: 0.1991
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
