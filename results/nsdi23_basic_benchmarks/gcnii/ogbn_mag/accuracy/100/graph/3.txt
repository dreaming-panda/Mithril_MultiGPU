Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.193 seconds.
Building the CSC structure...
        It takes 0.209 seconds.
Building the CSC structure...
        It takes 0.233 seconds.
Building the CSC structure...
        It takes 0.236 seconds.
Building the CSC structure...
        It takes 0.242 seconds.
Building the CSC structure...
        It takes 0.246 seconds.
Building the CSC structure...
        It takes 0.242 seconds.
Building the CSC structure...
        It takes 0.267 seconds.
Building the CSC structure...
        It takes 0.185 seconds.
        It takes 0.208 seconds.
        It takes 0.227 seconds.
        It takes 0.238 seconds.
Building the Feature Vector...
        It takes 0.243 seconds.
        It takes 0.245 seconds.
        It takes 0.257 seconds.
        It takes 0.263 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.239 seconds.
Building the Label Vector...
        It takes 0.245 seconds.
Building the Label Vector...
        It takes 0.229 seconds.
Building the Label Vector...
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.231 seconds.
Building the Label Vector...
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.582 seconds.
        It takes 0.563 seconds.
        It takes 0.595 seconds.
        It takes 0.554 seconds.
        It takes 0.603 seconds.
        It takes 0.601 seconds.
        It takes 0.601 seconds.
        It takes 0.601 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_mag/8_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 92048) 1-[92048, 184097) 2-[184097, 276145) 3-[276145, 368194) 4-[368194, 460242) 5-[460242, 552291) 6-[552291, 644340) 7-[644340, 736389)
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 63.752 Gbps (per GPU), 510.015 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.710 Gbps (per GPU), 509.678 Gbps (aggregated)
The layer-level communication performance: 63.709 Gbps (per GPU), 509.670 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.668 Gbps (per GPU), 509.344 Gbps (aggregated)
The layer-level communication performance: 63.672 Gbps (per GPU), 509.379 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.638 Gbps (per GPU), 509.104 Gbps (aggregated)
The layer-level communication performance: 63.632 Gbps (per GPU), 509.053 Gbps (aggregated)
The layer-level communication performance: 63.628 Gbps (per GPU), 509.024 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.649 Gbps (per GPU), 1325.189 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.651 Gbps (per GPU), 1325.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.648 Gbps (per GPU), 1325.185 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.636 Gbps (per GPU), 1325.087 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.649 Gbps (per GPU), 1325.188 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.648 Gbps (per GPU), 1325.185 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.645 Gbps (per GPU), 1325.162 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.649 Gbps (per GPU), 1325.192 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.495 Gbps (per GPU), 915.961 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.496 Gbps (per GPU), 915.965 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.495 Gbps (per GPU), 915.962 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.494 Gbps (per GPU), 915.956 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.496 Gbps (per GPU), 915.965 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.495 Gbps (per GPU), 915.958 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.495 Gbps (per GPU), 915.959 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.495 Gbps (per GPU), 915.959 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.875 Gbps (per GPU), 359.002 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.875 Gbps (per GPU), 359.000 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.875 Gbps (per GPU), 358.999 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.875 Gbps (per GPU), 359.002 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.875 Gbps (per GPU), 359.002 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.875 Gbps (per GPU), 359.002 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.875 Gbps (per GPU), 359.000 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.875 Gbps (per GPU), 359.001 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.38ms  5.44ms  9.18ms  3.86 92.05K  2.94M
 chk_1  2.40ms  4.66ms  8.42ms  3.51 92.05K  1.61M
 chk_2  2.40ms  4.56ms  8.29ms  3.46 92.05K  1.55M
 chk_3  2.40ms  4.36ms  8.07ms  3.36 92.05K  0.83M
 chk_4  2.40ms  4.56ms  8.29ms  3.45 92.05K  1.15M
 chk_5  2.40ms  4.36ms  8.08ms  3.36 92.05K  0.77M
 chk_6  2.40ms  4.60ms  8.32ms  3.47 92.05K  1.04M
 chk_7  2.40ms  4.40ms  8.12ms  3.38 92.05K  0.91M
   Avg  2.40  4.62  8.35
   Max  2.40  5.44  9.18
   Min  2.38  4.36  8.07
 Ratio  1.01  1.25  1.14
   Var  0.00  0.11  0.11
Profiling takes 1.508 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 66.780 ms
Partition 0 [0, 2) has cost: 56.115 ms
Partition 1 [2, 3) has cost: 36.937 ms
Partition 2 [3, 4) has cost: 36.937 ms
Partition 3 [4, 5) has cost: 36.937 ms
Partition 4 [5, 6) has cost: 36.937 ms
Partition 5 [6, 7) has cost: 36.937 ms
Partition 6 [7, 8) has cost: 36.937 ms
Partition 7 [8, 9) has cost: 66.780 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 101.681 ms
GPU 0, Compute+Comm Time: 53.178 ms, Bubble Time: 47.922 ms, Imbalance Overhead: 0.582 ms
GPU 1, Compute+Comm Time: 48.246 ms, Bubble Time: 47.923 ms, Imbalance Overhead: 5.512 ms
GPU 2, Compute+Comm Time: 48.246 ms, Bubble Time: 47.719 ms, Imbalance Overhead: 5.716 ms
GPU 3, Compute+Comm Time: 48.246 ms, Bubble Time: 47.557 ms, Imbalance Overhead: 5.879 ms
GPU 4, Compute+Comm Time: 48.246 ms, Bubble Time: 47.250 ms, Imbalance Overhead: 6.185 ms
GPU 5, Compute+Comm Time: 48.246 ms, Bubble Time: 47.086 ms, Imbalance Overhead: 6.349 ms
GPU 6, Compute+Comm Time: 48.246 ms, Bubble Time: 46.772 ms, Imbalance Overhead: 6.663 ms
GPU 7, Compute+Comm Time: 55.076 ms, Bubble Time: 46.605 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 153.025 ms
GPU 0, Compute+Comm Time: 85.629 ms, Bubble Time: 67.396 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 62.616 ms, Bubble Time: 68.347 ms, Imbalance Overhead: 22.062 ms
GPU 2, Compute+Comm Time: 62.616 ms, Bubble Time: 69.591 ms, Imbalance Overhead: 20.818 ms
GPU 3, Compute+Comm Time: 62.616 ms, Bubble Time: 70.541 ms, Imbalance Overhead: 19.868 ms
GPU 4, Compute+Comm Time: 62.616 ms, Bubble Time: 71.768 ms, Imbalance Overhead: 18.641 ms
GPU 5, Compute+Comm Time: 62.616 ms, Bubble Time: 72.728 ms, Imbalance Overhead: 17.682 ms
GPU 6, Compute+Comm Time: 62.616 ms, Bubble Time: 73.758 ms, Imbalance Overhead: 16.651 ms
GPU 7, Compute+Comm Time: 76.863 ms, Bubble Time: 74.359 ms, Imbalance Overhead: 1.803 ms
The estimated cost of the whole pipeline: 267.442 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 103.717 ms
Partition 0 [0, 3) has cost: 93.052 ms
Partition 1 [3, 5) has cost: 73.873 ms
Partition 2 [5, 7) has cost: 73.873 ms
Partition 3 [7, 9) has cost: 103.717 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 103.082 ms
GPU 0, Compute+Comm Time: 58.289 ms, Bubble Time: 43.999 ms, Imbalance Overhead: 0.794 ms
GPU 1, Compute+Comm Time: 55.820 ms, Bubble Time: 44.333 ms, Imbalance Overhead: 2.930 ms
GPU 2, Compute+Comm Time: 55.820 ms, Bubble Time: 44.105 ms, Imbalance Overhead: 3.157 ms
GPU 3, Compute+Comm Time: 59.234 ms, Bubble Time: 43.848 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 140.736 ms
GPU 0, Compute+Comm Time: 82.164 ms, Bubble Time: 58.572 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 70.641 ms, Bubble Time: 59.712 ms, Imbalance Overhead: 10.383 ms
GPU 2, Compute+Comm Time: 70.641 ms, Bubble Time: 60.827 ms, Imbalance Overhead: 9.268 ms
GPU 3, Compute+Comm Time: 77.772 ms, Bubble Time: 60.740 ms, Imbalance Overhead: 2.225 ms
    The estimated cost with 2 DP ways is 256.010 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 177.590 ms
Partition 0 [0, 5) has cost: 166.925 ms
Partition 1 [5, 9) has cost: 177.590 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 123.141 ms
GPU 0, Compute+Comm Time: 81.408 ms, Bubble Time: 40.379 ms, Imbalance Overhead: 1.354 ms
GPU 1, Compute+Comm Time: 81.877 ms, Bubble Time: 41.264 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 152.134 ms
GPU 0, Compute+Comm Time: 101.415 ms, Bubble Time: 50.719 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 99.219 ms, Bubble Time: 49.582 ms, Imbalance Overhead: 3.333 ms
    The estimated cost with 4 DP ways is 289.039 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 344.515 ms
Partition 0 [0, 9) has cost: 344.515 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 198.717 ms
GPU 0, Compute+Comm Time: 198.717 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 218.512 ms
GPU 0, Compute+Comm Time: 218.512 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 438.090 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 65)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 65)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 65)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 65)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 65)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 65)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 65)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 65)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 65)...
+++++++++ Node 1 initializing the weights for op[0, 65)...
+++++++++ Node 3 initializing the weights for op[0, 65)...
+++++++++ Node 2 initializing the weights for op[0, 65)...
+++++++++ Node 5 initializing the weights for op[0, 65)...
+++++++++ Node 6 initializing the weights for op[0, 65)...
+++++++++ Node 7 initializing the weights for op[0, 65)...
+++++++++ Node 4 initializing the weights for op[0, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.1420	TrainAcc 0.0775	ValidAcc 0.0936	TestAcc 0.0793	BestValid 0.0936
	Epoch 50:	Loss 5.0234	TrainAcc 0.0705	ValidAcc 0.0852	TestAcc 0.0592	BestValid 0.0936
	Epoch 75:	Loss 4.8017	TrainAcc 0.0872	ValidAcc 0.0969	TestAcc 0.0808	BestValid 0.0969
	Epoch 100:	Loss 4.4796	TrainAcc 0.1264	ValidAcc 0.1140	TestAcc 0.1147	BestValid 0.1140
Node 1, Pre/Post-Pipelining: 6.684 / 14.070 ms, Bubble: 0.566 ms, Compute: 112.621 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 6.684 / 14.008 ms, Bubble: 0.701 ms, Compute: 112.549 ms, Comm: 0.010 ms, Imbalance: 0.016 ms
Node 3, Pre/Post-Pipelining: 6.685 / 14.225 ms, Bubble: 0.106 ms, Compute: 112.929 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 0, Pre/Post-Pipelining: 6.684 / 13.280 ms, Bubble: 1.385 ms, Compute: 112.591 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 4, Pre/Post-Pipelining: 6.683 / 14.032 ms, Bubble: 0.566 ms, Compute: 112.666 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 5, Pre/Post-Pipelining: 6.682 / 13.926 ms, Bubble: 0.833 ms, Compute: 112.507 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 7, Pre/Post-Pipelining: 6.683 / 13.898 ms, Bubble: 0.850 ms, Compute: 112.512 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 6, Pre/Post-Pipelining: 6.685 / 14.157 ms, Bubble: 0.281 ms, Compute: 112.812 ms, Comm: 0.009 ms, Imbalance: 0.015 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.684 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.280 ms
Cluster-Wide Average, Bubble: 1.385 ms
Cluster-Wide Average, Compute: 112.591 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.014 ms
Node 2, GPU memory consumption: 20.702 GB
Node 0, GPU memory consumption: 21.206 GB
Node 4, GPU memory consumption: 20.678 GB
Node 1, GPU memory consumption: 20.702 GB
Node 5, GPU memory consumption: 20.702 GB
Node 6, GPU memory consumption: 20.702 GB
Node 3, GPU memory consumption: 20.678 GB
Node 7, GPU memory consumption: 20.678 GB
Node 0, Graph-Level Communication Throughput: 47.569 Gbps, Time: 39.662 ms
Node 4, Graph-Level Communication Throughput: 47.092 Gbps, Time: 44.029 ms
Node 1, Graph-Level Communication Throughput: 41.178 Gbps, Time: 42.111 ms
Node 5, Graph-Level Communication Throughput: 29.433 Gbps, Time: 44.228 ms
Node 2, Graph-Level Communication Throughput: 32.701 Gbps, Time: 43.064 ms
Node 6, Graph-Level Communication Throughput: 48.725 Gbps, Time: 41.819 ms
Node 3, Graph-Level Communication Throughput: 23.633 Gbps, Time: 43.917 ms
Node 7, Graph-Level Communication Throughput: 45.425 Gbps, Time: 43.148 ms
------------------------node id 0,  per-epoch time: 0.566278 s---------------
------------------------node id 4,  per-epoch time: 0.566277 s---------------
------------------------node id 1,  per-epoch time: 0.566277 s---------------
------------------------node id 5,  per-epoch time: 0.566278 s---------------
------------------------node id 2,  per-epoch time: 0.566277 s---------------
------------------------node id 6,  per-epoch time: 0.566279 s---------------
------------------------node id 3,  per-epoch time: 0.566277 s---------------
------------------------node id 7,  per-epoch time: 0.566277 s---------------
************ Profiling Results ************
	Bubble: 432.438407 (ms) (79.21 percentage)
	Compute: 55.359997 (ms) (10.14 percentage)
	GraphCommComputeOverhead: 2.412472 (ms) (0.44 percentage)
	GraphCommNetwork: 42.747115 (ms) (7.83 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.965533 (ms) (2.37 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 1.565 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 1.568 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.1140
Target test_acc: 0.1147
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
