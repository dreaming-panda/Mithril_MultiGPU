Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.201 seconds.
Building the CSC structure...
        It takes 0.214 seconds.
Building the CSC structure...
        It takes 0.232 seconds.
Building the CSC structure...
        It takes 0.235 seconds.
Building the CSC structure...
        It takes 0.238 seconds.
Building the CSC structure...
        It takes 0.247 seconds.
Building the CSC structure...
        It takes 0.264 seconds.
Building the CSC structure...
        It takes 0.273 seconds.
Building the CSC structure...
        It takes 0.209 seconds.
        It takes 0.208 seconds.
        It takes 0.237 seconds.
        It takes 0.242 seconds.
        It takes 0.241 seconds.
        It takes 0.245 seconds.
        It takes 0.263 seconds.
Building the Feature Vector...
        It takes 0.265 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.240 seconds.
Building the Label Vector...
        It takes 0.249 seconds.
Building the Label Vector...
        It takes 0.256 seconds.
Building the Label Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.550 seconds.
        It takes 0.589 seconds.
        It takes 0.554 seconds.
        It takes 0.583 seconds.
        It takes 0.590 seconds.
        It takes 0.600 seconds.
        It takes 0.603 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_mag/8_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
        It takes 0.609 seconds.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 92048) 1-[92048, 184097) 2-[184097, 276145) 3-[276145, 368194) 4-[368194, 460242) 5-[460242, 552291) 6-[552291, 644340) 7-[644340, 736389)
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.224 Gbps (per GPU), 505.791 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.186 Gbps (per GPU), 505.486 Gbps (aggregated)
The layer-level communication performance: 63.185 Gbps (per GPU), 505.478 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.150 Gbps (per GPU), 505.198 Gbps (aggregated)
The layer-level communication performance: 63.145 Gbps (per GPU), 505.162 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.116 Gbps (per GPU), 504.927 Gbps (aggregated)
The layer-level communication performance: 63.109 Gbps (per GPU), 504.871 Gbps (aggregated)
The layer-level communication performance: 63.104 Gbps (per GPU), 504.835 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.788 Gbps (per GPU), 1310.307 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.793 Gbps (per GPU), 1310.345 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.789 Gbps (per GPU), 1310.314 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.790 Gbps (per GPU), 1310.317 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.787 Gbps (per GPU), 1310.298 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.788 Gbps (per GPU), 1310.304 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.786 Gbps (per GPU), 1310.285 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.786 Gbps (per GPU), 1310.288 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.770 Gbps (per GPU), 910.157 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.770 Gbps (per GPU), 910.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.769 Gbps (per GPU), 910.153 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.769 Gbps (per GPU), 910.154 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.770 Gbps (per GPU), 910.157 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.769 Gbps (per GPU), 910.148 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.768 Gbps (per GPU), 910.147 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.768 Gbps (per GPU), 910.142 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.560 Gbps (per GPU), 364.479 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.560 Gbps (per GPU), 364.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.560 Gbps (per GPU), 364.477 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.560 Gbps (per GPU), 364.477 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.560 Gbps (per GPU), 364.477 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.560 Gbps (per GPU), 364.477 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.558 Gbps (per GPU), 364.465 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.558 Gbps (per GPU), 364.465 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.39ms  5.44ms  9.18ms  3.85 92.05K  2.94M
 chk_1  2.40ms  4.67ms  8.42ms  3.51 92.05K  1.61M
 chk_2  2.40ms  4.56ms  8.30ms  3.46 92.05K  1.55M
 chk_3  2.40ms  4.37ms  8.10ms  3.37 92.05K  0.83M
 chk_4  2.40ms  4.58ms  8.30ms  3.45 92.05K  1.15M
 chk_5  2.40ms  4.37ms  8.10ms  3.37 92.05K  0.77M
 chk_6  2.40ms  4.61ms  8.33ms  3.46 92.05K  1.04M
 chk_7  2.40ms  4.41ms  8.12ms  3.38 92.05K  0.91M
   Avg  2.40  4.63  8.36
   Max  2.40  5.44  9.18
   Min  2.39  4.37  8.10
 Ratio  1.01  1.25  1.13
   Var  0.00  0.11  0.11
Profiling takes 1.513 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 66.845 ms
Partition 0 [0, 2) has cost: 56.206 ms
Partition 1 [2, 3) has cost: 37.006 ms
Partition 2 [3, 4) has cost: 37.006 ms
Partition 3 [4, 5) has cost: 37.006 ms
Partition 4 [5, 6) has cost: 37.006 ms
Partition 5 [6, 7) has cost: 37.006 ms
Partition 6 [7, 8) has cost: 37.006 ms
Partition 7 [8, 9) has cost: 66.845 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 102.320 ms
GPU 0, Compute+Comm Time: 53.519 ms, Bubble Time: 48.223 ms, Imbalance Overhead: 0.579 ms
GPU 1, Compute+Comm Time: 48.572 ms, Bubble Time: 48.225 ms, Imbalance Overhead: 5.523 ms
GPU 2, Compute+Comm Time: 48.572 ms, Bubble Time: 48.022 ms, Imbalance Overhead: 5.726 ms
GPU 3, Compute+Comm Time: 48.572 ms, Bubble Time: 47.848 ms, Imbalance Overhead: 5.901 ms
GPU 4, Compute+Comm Time: 48.572 ms, Bubble Time: 47.545 ms, Imbalance Overhead: 6.204 ms
GPU 5, Compute+Comm Time: 48.572 ms, Bubble Time: 47.378 ms, Imbalance Overhead: 6.371 ms
GPU 6, Compute+Comm Time: 48.572 ms, Bubble Time: 47.065 ms, Imbalance Overhead: 6.683 ms
GPU 7, Compute+Comm Time: 55.418 ms, Bubble Time: 46.903 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 153.687 ms
GPU 0, Compute+Comm Time: 85.970 ms, Bubble Time: 67.716 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 62.977 ms, Bubble Time: 68.663 ms, Imbalance Overhead: 22.047 ms
GPU 2, Compute+Comm Time: 62.977 ms, Bubble Time: 69.899 ms, Imbalance Overhead: 20.811 ms
GPU 3, Compute+Comm Time: 62.977 ms, Bubble Time: 70.851 ms, Imbalance Overhead: 19.859 ms
GPU 4, Compute+Comm Time: 62.977 ms, Bubble Time: 72.076 ms, Imbalance Overhead: 18.634 ms
GPU 5, Compute+Comm Time: 62.977 ms, Bubble Time: 73.042 ms, Imbalance Overhead: 17.668 ms
GPU 6, Compute+Comm Time: 62.977 ms, Bubble Time: 74.073 ms, Imbalance Overhead: 16.637 ms
GPU 7, Compute+Comm Time: 77.230 ms, Bubble Time: 74.666 ms, Imbalance Overhead: 1.791 ms
The estimated cost of the whole pipeline: 268.807 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 103.851 ms
Partition 0 [0, 3) has cost: 93.211 ms
Partition 1 [3, 5) has cost: 74.011 ms
Partition 2 [5, 7) has cost: 74.011 ms
Partition 3 [7, 9) has cost: 103.851 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 103.781 ms
GPU 0, Compute+Comm Time: 58.694 ms, Bubble Time: 44.303 ms, Imbalance Overhead: 0.784 ms
GPU 1, Compute+Comm Time: 56.218 ms, Bubble Time: 44.636 ms, Imbalance Overhead: 2.926 ms
GPU 2, Compute+Comm Time: 56.218 ms, Bubble Time: 44.398 ms, Imbalance Overhead: 3.164 ms
GPU 3, Compute+Comm Time: 59.636 ms, Bubble Time: 44.145 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 141.492 ms
GPU 0, Compute+Comm Time: 82.584 ms, Bubble Time: 58.908 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 71.078 ms, Bubble Time: 60.026 ms, Imbalance Overhead: 10.388 ms
GPU 2, Compute+Comm Time: 71.078 ms, Bubble Time: 61.149 ms, Imbalance Overhead: 9.265 ms
GPU 3, Compute+Comm Time: 78.210 ms, Bubble Time: 61.061 ms, Imbalance Overhead: 2.222 ms
    The estimated cost with 2 DP ways is 257.536 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 177.862 ms
Partition 0 [0, 5) has cost: 167.223 ms
Partition 1 [5, 9) has cost: 177.862 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 123.905 ms
GPU 0, Compute+Comm Time: 81.925 ms, Bubble Time: 40.646 ms, Imbalance Overhead: 1.334 ms
GPU 1, Compute+Comm Time: 82.394 ms, Bubble Time: 41.511 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 152.935 ms
GPU 0, Compute+Comm Time: 101.937 ms, Bubble Time: 50.998 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 99.755 ms, Bubble Time: 49.838 ms, Imbalance Overhead: 3.342 ms
    The estimated cost with 4 DP ways is 290.682 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 345.085 ms
Partition 0 [0, 9) has cost: 345.085 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 195.948 ms
GPU 0, Compute+Comm Time: 195.948 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 215.787 ms
GPU 0, Compute+Comm Time: 215.787 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 432.323 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 65)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 65)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 65)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 65)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 65)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 65)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 65)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 65)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 65)...
+++++++++ Node 2 initializing the weights for op[0, 65)...
+++++++++ Node 3 initializing the weights for op[0, 65)...
+++++++++ Node 5 initializing the weights for op[0, 65)...
+++++++++ Node 0 initializing the weights for op[0, 65)...
+++++++++ Node 7 initializing the weights for op[0, 65)...
+++++++++ Node 4 initializing the weights for op[0, 65)...
+++++++++ Node 6 initializing the weights for op[0, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.1332	TrainAcc 0.0757	ValidAcc 0.0937	TestAcc 0.0797	BestValid 0.0937
	Epoch 50:	Loss 4.9912	TrainAcc 0.0982	ValidAcc 0.1009	TestAcc 0.0812	BestValid 0.1009
	Epoch 75:	Loss 4.6885	TrainAcc 0.0908	ValidAcc 0.0999	TestAcc 0.0810	BestValid 0.1009
	Epoch 100:	Loss 4.4571	TrainAcc 0.1295	ValidAcc 0.1156	TestAcc 0.0965	BestValid 0.1156
Node 2, Pre/Post-Pipelining: 6.682 / 14.054 ms, Bubble: 0.742 ms, Compute: 112.391 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 4, Pre/Post-Pipelining: 6.684 / 14.087 ms, Bubble: 0.596 ms, Compute: 112.506 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 3, Pre/Post-Pipelining: 6.684 / 14.268 ms, Bubble: 0.108 ms, Compute: 112.806 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 5, Pre/Post-Pipelining: 6.682 / 14.019 ms, Bubble: 0.810 ms, Compute: 112.357 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 0, Pre/Post-Pipelining: 6.684 / 13.346 ms, Bubble: 1.347 ms, Compute: 112.483 ms, Comm: 0.010 ms, Imbalance: 0.016 ms
Node 1, Pre/Post-Pipelining: 6.683 / 14.133 ms, Bubble: 0.542 ms, Compute: 112.510 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 6, Pre/Post-Pipelining: 6.686 / 14.193 ms, Bubble: 0.313 ms, Compute: 112.668 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 6.682 / 13.956 ms, Bubble: 0.863 ms, Compute: 112.371 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 6.684 ms
Cluster-Wide Average, Post-Pipelining Overhead: 13.346 ms
Cluster-Wide Average, Bubble: 1.347 ms
Cluster-Wide Average, Compute: 112.483 ms
Cluster-Wide Average, Communication: 0.010 ms
Cluster-Wide Average, Imbalance: 0.016 ms
Node 0, GPU memory consumption: 21.208 GB
Node 4, GPU memory consumption: 20.678 GB
Node 2, GPU memory consumption: 20.702 GB
Node 5, GPU memory consumption: 20.702 GB
Node 3, GPU memory consumption: 20.678 GB
Node 7, GPU memory consumption: 20.678 GB
Node 1, GPU memory consumption: 20.702 GB
Node 6, GPU memory consumption: 20.702 GB
Node 0, Graph-Level Communication Throughput: 47.966 Gbps, Time: 39.335 ms
Node 1, Graph-Level Communication Throughput: 41.388 Gbps, Time: 41.897 ms
Node 2, Graph-Level Communication Throughput: 32.717 Gbps, Time: 43.044 ms
Node 3, Graph-Level Communication Throughput: 23.707 Gbps, Time: 43.780 ms
Node 4, Graph-Level Communication Throughput: 47.175 Gbps, Time: 43.951 ms
Node 5, Graph-Level Communication Throughput: 29.675 Gbps, Time: 43.868 ms
Node 6, Graph-Level Communication Throughput: 48.915 Gbps, Time: 41.656 ms
Node 7, Graph-Level Communication Throughput: 45.487 Gbps, Time: 43.089 ms
------------------------node id 0,  per-epoch time: 0.565053 s---------------
------------------------node id 1,  per-epoch time: 0.565053 s---------------
------------------------node id 2,  per-epoch time: 0.565052 s---------------
------------------------node id 3,  per-epoch time: 0.565053 s---------------
------------------------node id 4,  per-epoch time: 0.565052 s---------------
------------------------node id 5,  per-epoch time: 0.565051 s---------------
------------------------node id 6,  per-epoch time: 0.565053 s---------------
------------------------node id 7,  per-epoch time: 0.565061 s---------------
************ Profiling Results ************
	Bubble: 431.464437 (ms) (79.19 percentage)
	Compute: 55.390927 (ms) (10.17 percentage)
	GraphCommComputeOverhead: 2.419178 (ms) (0.44 percentage)
	GraphCommNetwork: 42.577263 (ms) (7.81 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 12.969514 (ms) (2.38 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 1.565 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 1.568 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.1156
Target test_acc: 0.0965
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 4] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
