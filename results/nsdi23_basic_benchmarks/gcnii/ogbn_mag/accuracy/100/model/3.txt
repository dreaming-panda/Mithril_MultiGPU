Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.192 seconds.
Building the CSC structure...
        It takes 0.196 seconds.
Building the CSC structure...
        It takes 0.204 seconds.
Building the CSC structure...
        It takes 0.237 seconds.
Building the CSC structure...
        It takes 0.237 seconds.
Building the CSC structure...
        It takes 0.257 seconds.
Building the CSC structure...
        It takes 0.260 seconds.
Building the CSC structure...
        It takes 0.271 seconds.
Building the CSC structure...
        It takes 0.186 seconds.
        It takes 0.187 seconds.
        It takes 0.197 seconds.
Building the Feature Vector...
        It takes 0.241 seconds.
Building the Feature Vector...
        It takes 0.255 seconds.
        It takes 0.243 seconds.
Building the Feature Vector...
        It takes 0.264 seconds.
        It takes 0.269 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.240 seconds.
Building the Label Vector...
        It takes 0.243 seconds.
Building the Label Vector...
        It takes 0.247 seconds.
Building the Label Vector...
        It takes 0.246 seconds.
Building the Label Vector...
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 0.240 seconds.
Building the Label Vector...
        It takes 0.251 seconds.
Building the Label Vector...
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.580 seconds.
        It takes 0.546 seconds.
        It takes 0.593 seconds.
        It takes 0.544 seconds.
        It takes 0.571 seconds.
        It takes 0.591 seconds.
        It takes 0.588 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_mag/32_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
        It takes 0.582 seconds.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 22122) 1-[22122, 45392) 2-[45392, 67023) 3-[67023, 86386) 4-[86386, 111284) 5-[111284, 132574) 6-[132574, 154815) 7-[154815, 176781) 8-[176781, 200751) ... 31-[712285, 736389)
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.029 Gbps (per GPU), 504.229 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.989 Gbps (per GPU), 503.912 Gbps (aggregated)
The layer-level communication performance: 62.988 Gbps (per GPU), 503.904 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.953 Gbps (per GPU), 503.624 Gbps (aggregated)
The layer-level communication performance: 62.949 Gbps (per GPU), 503.593 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.921 Gbps (per GPU), 503.365 Gbps (aggregated)
The layer-level communication performance: 62.913 Gbps (per GPU), 503.307 Gbps (aggregated)
The layer-level communication performance: 62.909 Gbps (per GPU), 503.271 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.858 Gbps (per GPU), 1326.862 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.862 Gbps (per GPU), 1326.895 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.858 Gbps (per GPU), 1326.865 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.859 Gbps (per GPU), 1326.869 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.856 Gbps (per GPU), 1326.846 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.858 Gbps (per GPU), 1326.862 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.851 Gbps (per GPU), 1326.810 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.856 Gbps (per GPU), 1326.846 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.329 Gbps (per GPU), 914.631 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.330 Gbps (per GPU), 914.639 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.329 Gbps (per GPU), 914.629 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.329 Gbps (per GPU), 914.628 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.329 Gbps (per GPU), 914.629 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.332 Gbps (per GPU), 914.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.326 Gbps (per GPU), 914.607 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.332 Gbps (per GPU), 914.653 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.658 Gbps (per GPU), 357.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.657 Gbps (per GPU), 357.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.657 Gbps (per GPU), 357.259 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.658 Gbps (per GPU), 357.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.657 Gbps (per GPU), 357.260 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.657 Gbps (per GPU), 357.260 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.657 Gbps (per GPU), 357.257 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.657 Gbps (per GPU), 357.258 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.72ms  1.31ms  2.28ms  3.15 22.12K  0.37M
 chk_1  0.76ms  1.35ms  2.36ms  3.12 23.27K  0.33M
 chk_2  0.71ms  1.31ms  2.27ms  3.19 21.63K  0.36M
 chk_3  0.66ms  1.26ms  2.15ms  3.28 19.36K  0.49M
 chk_4  0.80ms  1.41ms  2.51ms  3.14 24.90K  0.27M
 chk_5  0.71ms  1.32ms  2.28ms  3.23 21.29K  0.42M
 chk_6  0.72ms  1.31ms  2.31ms  3.19 22.24K  0.36M
 chk_7  0.72ms  1.33ms  2.33ms  3.23 21.97K  0.39M
 chk_8  0.76ms  1.41ms  2.45ms  3.23 23.97K  0.31M
 chk_9  0.71ms  1.30ms  2.28ms  3.19 21.69K  0.38M
chk_10  0.77ms  1.38ms  2.44ms  3.16 24.29K  0.27M
chk_11  0.75ms  1.35ms  2.39ms  3.17 23.61K  0.33M
chk_12  0.75ms  1.37ms  2.40ms  3.21 23.29K  0.32M
chk_13  0.76ms  1.35ms  2.39ms  3.15 23.85K  0.30M
chk_14  0.73ms  1.33ms  2.34ms  3.20 22.48K  0.36M
chk_15  0.81ms  1.42ms  2.54ms  3.12 25.69K  0.21M
chk_16  0.80ms  1.41ms  2.51ms  3.14 24.83K  0.29M
chk_17  0.73ms  1.35ms  2.36ms  3.22 22.55K  0.36M
chk_18  0.75ms  1.37ms  2.41ms  3.19 23.46K  0.31M
chk_19  0.67ms  1.32ms  2.22ms  3.33 19.76K  0.47M
chk_20  0.74ms  1.35ms  2.36ms  3.20 22.71K  0.35M
chk_21  0.77ms  1.37ms  2.43ms  3.17 24.18K  0.28M
chk_22  0.77ms  1.39ms  2.45ms  3.19 24.27K  0.29M
chk_23  0.74ms  1.38ms  2.39ms  3.24 22.76K  0.35M
chk_24  0.75ms  1.36ms  2.38ms  3.20 23.18K  0.30M
chk_25  0.76ms  1.35ms  2.39ms  3.17 23.75K  0.31M
chk_26  0.73ms  1.36ms  2.36ms  3.25 22.29K  0.39M
chk_27  0.75ms  1.38ms  2.41ms  3.23 23.17K  0.35M
chk_28  0.75ms  1.36ms  2.40ms  3.20 23.35K  0.32M
chk_29  0.73ms  1.35ms  2.36ms  3.25 22.21K  0.37M
chk_30  0.77ms  1.39ms  2.45ms  3.19 24.16K  0.27M
chk_31  0.76ms  1.39ms  2.44ms  3.20 24.10K  0.31M
   Avg  0.74  1.36  2.38
   Max  0.81  1.42  2.54
   Min  0.66  1.26  2.15
 Ratio  1.24  1.13  1.18
   Var  0.00  0.00  0.01
Profiling takes 1.806 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 76.048 ms
Partition 0 [0, 2) has cost: 67.173 ms
Partition 1 [2, 3) has cost: 43.378 ms
Partition 2 [3, 4) has cost: 43.378 ms
Partition 3 [4, 5) has cost: 43.378 ms
Partition 4 [5, 6) has cost: 43.378 ms
Partition 5 [6, 7) has cost: 43.378 ms
Partition 6 [7, 8) has cost: 43.378 ms
Partition 7 [8, 9) has cost: 76.048 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 71.555 ms
GPU 0, Compute+Comm Time: 57.459 ms, Bubble Time: 12.896 ms, Imbalance Overhead: 1.200 ms
GPU 1, Compute+Comm Time: 50.752 ms, Bubble Time: 12.767 ms, Imbalance Overhead: 8.037 ms
GPU 2, Compute+Comm Time: 50.752 ms, Bubble Time: 12.799 ms, Imbalance Overhead: 8.004 ms
GPU 3, Compute+Comm Time: 50.752 ms, Bubble Time: 12.664 ms, Imbalance Overhead: 8.140 ms
GPU 4, Compute+Comm Time: 50.752 ms, Bubble Time: 12.412 ms, Imbalance Overhead: 8.391 ms
GPU 5, Compute+Comm Time: 50.752 ms, Bubble Time: 12.557 ms, Imbalance Overhead: 8.246 ms
GPU 6, Compute+Comm Time: 50.752 ms, Bubble Time: 12.358 ms, Imbalance Overhead: 8.446 ms
GPU 7, Compute+Comm Time: 58.251 ms, Bubble Time: 12.204 ms, Imbalance Overhead: 1.101 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 110.743 ms
GPU 0, Compute+Comm Time: 92.571 ms, Bubble Time: 17.906 ms, Imbalance Overhead: 0.265 ms
GPU 1, Compute+Comm Time: 67.400 ms, Bubble Time: 18.347 ms, Imbalance Overhead: 24.996 ms
GPU 2, Compute+Comm Time: 67.400 ms, Bubble Time: 18.855 ms, Imbalance Overhead: 24.488 ms
GPU 3, Compute+Comm Time: 67.400 ms, Bubble Time: 18.877 ms, Imbalance Overhead: 24.466 ms
GPU 4, Compute+Comm Time: 67.400 ms, Bubble Time: 19.484 ms, Imbalance Overhead: 23.859 ms
GPU 5, Compute+Comm Time: 67.400 ms, Bubble Time: 19.897 ms, Imbalance Overhead: 23.446 ms
GPU 6, Compute+Comm Time: 67.400 ms, Bubble Time: 20.086 ms, Imbalance Overhead: 23.257 ms
GPU 7, Compute+Comm Time: 84.488 ms, Bubble Time: 20.481 ms, Imbalance Overhead: 5.775 ms
The estimated cost of the whole pipeline: 191.413 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 119.426 ms
Partition 0 [0, 3) has cost: 110.551 ms
Partition 1 [3, 5) has cost: 86.756 ms
Partition 2 [5, 7) has cost: 86.756 ms
Partition 3 [7, 9) has cost: 119.426 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 74.326 ms
GPU 0, Compute+Comm Time: 61.699 ms, Bubble Time: 11.702 ms, Imbalance Overhead: 0.926 ms
GPU 1, Compute+Comm Time: 58.269 ms, Bubble Time: 11.656 ms, Imbalance Overhead: 4.401 ms
GPU 2, Compute+Comm Time: 58.269 ms, Bubble Time: 11.390 ms, Imbalance Overhead: 4.667 ms
GPU 3, Compute+Comm Time: 62.113 ms, Bubble Time: 11.293 ms, Imbalance Overhead: 0.919 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 103.719 ms
GPU 0, Compute+Comm Time: 88.132 ms, Bubble Time: 15.338 ms, Imbalance Overhead: 0.249 ms
GPU 1, Compute+Comm Time: 75.232 ms, Bubble Time: 15.641 ms, Imbalance Overhead: 12.845 ms
GPU 2, Compute+Comm Time: 75.232 ms, Bubble Time: 16.275 ms, Imbalance Overhead: 12.212 ms
GPU 3, Compute+Comm Time: 84.007 ms, Bubble Time: 16.565 ms, Imbalance Overhead: 3.148 ms
    The estimated cost with 2 DP ways is 186.947 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 206.182 ms
Partition 0 [0, 5) has cost: 197.307 ms
Partition 1 [5, 9) has cost: 206.182 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 97.025 ms
GPU 0, Compute+Comm Time: 85.610 ms, Bubble Time: 10.775 ms, Imbalance Overhead: 0.640 ms
GPU 1, Compute+Comm Time: 85.810 ms, Bubble Time: 10.162 ms, Imbalance Overhead: 1.054 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 120.838 ms
GPU 0, Compute+Comm Time: 107.678 ms, Bubble Time: 12.580 ms, Imbalance Overhead: 0.581 ms
GPU 1, Compute+Comm Time: 105.586 ms, Bubble Time: 13.500 ms, Imbalance Overhead: 1.752 ms
    The estimated cost with 4 DP ways is 228.756 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 403.489 ms
Partition 0 [0, 9) has cost: 403.489 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 215.512 ms
GPU 0, Compute+Comm Time: 215.512 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 236.702 ms
GPU 0, Compute+Comm Time: 236.702 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 474.824 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 13)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [13, 20)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [20, 27)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [27, 34)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [34, 41)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [41, 48)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [48, 55)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [55, 65)
*** Node 7, constructing the helper classes...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 13)...
+++++++++ Node 1 initializing the weights for op[13, 20)...
+++++++++ Node 4 initializing the weights for op[34, 41)...
+++++++++ Node 2 initializing the weights for op[20, 27)...
+++++++++ Node 5 initializing the weights for op[41, 48)...
+++++++++ Node 3 initializing the weights for op[27, 34)...
+++++++++ Node 6 initializing the weights for op[48, 55)...
+++++++++ Node 7 initializing the weights for op[55, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.2538	TrainAcc 0.0746	ValidAcc 0.0927	TestAcc 0.0754	BestValid 0.0927
	Epoch 50:	Loss 4.9404	TrainAcc 0.1027	ValidAcc 0.1013	TestAcc 0.0811	BestValid 0.1013
	Epoch 75:	Loss 4.5924	TrainAcc 0.1244	ValidAcc 0.1085	TestAcc 0.0855	BestValid 0.1085
	Epoch 100:	Loss 4.2755	TrainAcc 0.1469	ValidAcc 0.1315	TestAcc 0.1998	BestValid 0.1315
Node 1, Pre/Post-Pipelining: 0.859 / 6.302 ms, Bubble: 176.071 ms, Compute: 47.066 ms, Comm: 93.929 ms, Imbalance: 141.925 ms
Node 2, Pre/Post-Pipelining: 0.856 / 6.311 ms, Bubble: 172.657 ms, Compute: 46.709 ms, Comm: 118.703 ms, Imbalance: 121.847 ms
Node 3, Pre/Post-Pipelining: 0.859 / 6.298 ms, Bubble: 165.179 ms, Compute: 46.708 ms, Comm: 147.690 ms, Imbalance: 99.918 ms
Node 5, Pre/Post-Pipelining: 0.857 / 6.318 ms, Bubble: 161.908 ms, Compute: 46.328 ms, Comm: 117.871 ms, Imbalance: 134.795 ms
Node 0, Pre/Post-Pipelining: 0.859 / 6.301 ms, Bubble: 180.531 ms, Compute: 70.550 ms, Comm: 67.517 ms, Imbalance: 139.878 ms
Node 6, Pre/Post-Pipelining: 0.859 / 6.291 ms, Bubble: 158.032 ms, Compute: 46.156 ms, Comm: 94.011 ms, Imbalance: 162.225 ms
Node 4, Pre/Post-Pipelining: 0.845 / 6.325 ms, Bubble: 162.806 ms, Compute: 46.492 ms, Comm: 147.970 ms, Imbalance: 103.574 ms
Node 7, Pre/Post-Pipelining: 0.862 / 125.115 ms, Bubble: 37.138 ms, Compute: 206.818 ms, Comm: 67.192 ms, Imbalance: 29.840 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.859 ms
Cluster-Wide Average, Post-Pipelining Overhead: 6.301 ms
Cluster-Wide Average, Bubble: 180.531 ms
Cluster-Wide Average, Compute: 70.550 ms
Cluster-Wide Average, Communication: 67.517 ms
Cluster-Wide Average, Imbalance: 139.878 ms
Node 2, GPU memory consumption: 3.770 GB
Node 0, GPU memory consumption: 7.333 GB
Node 3, GPU memory consumption: 3.747 GB
Node 1, GPU memory consumption: 3.770 GB
Node 7, GPU memory consumption: 7.653 GB
Node 5, GPU memory consumption: 3.770 GB
Node 6, GPU memory consumption: 3.770 GB
Node 4, GPU memory consumption: 3.747 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.897685 s---------------
------------------------node id 1,  per-epoch time: 0.897682 s---------------
------------------------node id 2,  per-epoch time: 0.897683 s---------------
------------------------node id 3,  per-epoch time: 0.897684 s---------------
------------------------node id 4,  per-epoch time: 0.897684 s---------------
------------------------node id 5,  per-epoch time: 0.897683 s---------------
------------------------node id 6,  per-epoch time: 0.897684 s---------------
------------------------node id 7,  per-epoch time: 0.897685 s---------------
************ Profiling Results ************
	Bubble: 810.305939 (ms) (92.04 percentage)
	Compute: 68.055328 (ms) (7.73 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.028049 (ms) (0.23 percentage)
	Layer-level communication (cluster-wide, per-epoch): 7.681 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 7.685 GB
	Aggregated layer-level communication throughput: 617.446 Gbps
Highest valid_acc: 0.1315
Target test_acc: 0.1998
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
