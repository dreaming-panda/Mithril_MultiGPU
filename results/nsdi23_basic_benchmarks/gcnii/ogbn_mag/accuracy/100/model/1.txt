Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.234 seconds.
Building the CSC structure...
        It takes 0.235 seconds.
Building the CSC structure...
        It takes 0.251 seconds.
Building the CSC structure...
        It takes 0.268 seconds.
Building the CSC structure...
        It takes 0.296 seconds.
Building the CSC structure...
        It takes 0.294 seconds.
Building the CSC structure...
        It takes 0.297 seconds.
Building the CSC structure...
        It takes 0.297 seconds.
Building the CSC structure...
        It takes 0.220 seconds.
        It takes 0.220 seconds.
        It takes 0.239 seconds.
        It takes 0.261 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.307 seconds.
        It takes 0.307 seconds.
        It takes 0.308 seconds.
        It takes 0.310 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.814 seconds.
        It takes 0.881 seconds.
        It takes 0.902 seconds.
        It takes 0.915 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.754 seconds.
        It takes 0.774 seconds.
        It takes 0.749 seconds.
        It takes 0.779 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 2.160 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_mag/32_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
        It takes 2.160 seconds.
        It takes 2.160 seconds.
        It takes 2.160 seconds.
        It takes 2.773 seconds.
        It takes 2.773 seconds.
        It takes 2.773 seconds.
        It takes 2.773 seconds.
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 22122) 1-[22122, 45392) 2-[45392, 67023) 3-[67023, 86386) 4-[86386, 111284) 5-[111284, 132574) 6-[132574, 154815) 7-[154815, 176781) 8-[176781, 200751) ... 31-[712285, 736389)
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.098 Gbps (per GPU), 504.781 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.058 Gbps (per GPU), 504.465 Gbps (aggregated)
The layer-level communication performance: 63.054 Gbps (per GPU), 504.435 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.023 Gbps (per GPU), 504.187 Gbps (aggregated)
The layer-level communication performance: 63.018 Gbps (per GPU), 504.143 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.989 Gbps (per GPU), 503.910 Gbps (aggregated)
The layer-level communication performance: 62.983 Gbps (per GPU), 503.867 Gbps (aggregated)
The layer-level communication performance: 62.977 Gbps (per GPU), 503.818 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.600 Gbps (per GPU), 1324.803 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.595 Gbps (per GPU), 1324.760 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.595 Gbps (per GPU), 1324.760 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.594 Gbps (per GPU), 1324.753 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.603 Gbps (per GPU), 1324.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.596 Gbps (per GPU), 1324.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.592 Gbps (per GPU), 1324.737 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.596 Gbps (per GPU), 1324.767 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.476 Gbps (per GPU), 915.810 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.477 Gbps (per GPU), 915.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.476 Gbps (per GPU), 915.807 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.476 Gbps (per GPU), 915.805 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.476 Gbps (per GPU), 915.806 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.476 Gbps (per GPU), 915.810 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.475 Gbps (per GPU), 915.802 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.476 Gbps (per GPU), 915.809 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.362 Gbps (per GPU), 362.893 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.362 Gbps (per GPU), 362.894 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.361 Gbps (per GPU), 362.891 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.361 Gbps (per GPU), 362.888 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.361 Gbps (per GPU), 362.890 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.361 Gbps (per GPU), 362.889 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.361 Gbps (per GPU), 362.891 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.355 Gbps (per GPU), 362.838 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.73ms  1.30ms  2.29ms  3.16 22.12K  0.37M
 chk_1  0.75ms  1.34ms  2.39ms  3.19 23.27K  0.33M
 chk_2  0.71ms  1.31ms  2.29ms  3.22 21.63K  0.36M
 chk_3  0.66ms  1.26ms  2.16ms  3.29 19.36K  0.49M
 chk_4  0.80ms  1.41ms  2.52ms  3.16 24.90K  0.27M
 chk_5  0.71ms  1.32ms  2.30ms  3.24 21.29K  0.42M
 chk_6  0.72ms  1.32ms  2.33ms  3.21 22.24K  0.36M
 chk_7  0.72ms  1.33ms  2.34ms  3.26 21.97K  0.39M
 chk_8  0.76ms  1.41ms  2.47ms  3.24 23.97K  0.31M
 chk_9  0.71ms  1.30ms  2.29ms  3.21 21.69K  0.38M
chk_10  0.77ms  1.38ms  2.45ms  3.18 24.29K  0.27M
chk_11  0.76ms  1.35ms  2.40ms  3.18 23.61K  0.33M
chk_12  0.75ms  1.37ms  2.41ms  3.23 23.29K  0.32M
chk_13  0.76ms  1.35ms  2.41ms  3.17 23.85K  0.30M
chk_14  0.73ms  1.33ms  2.35ms  3.21 22.48K  0.36M
chk_15  0.82ms  1.42ms  2.55ms  3.13 25.69K  0.21M
chk_16  0.80ms  1.41ms  2.52ms  3.15 24.83K  0.29M
chk_17  0.73ms  1.35ms  2.37ms  3.23 22.55K  0.36M
chk_18  0.75ms  1.37ms  2.42ms  3.22 23.46K  0.31M
chk_19  0.67ms  1.32ms  2.23ms  3.35 19.76K  0.47M
chk_20  0.74ms  1.35ms  2.37ms  3.22 22.71K  0.35M
chk_21  0.77ms  1.38ms  2.44ms  3.18 24.18K  0.28M
chk_22  0.77ms  1.39ms  2.46ms  3.20 24.27K  0.29M
chk_23  0.74ms  1.38ms  2.40ms  3.25 22.76K  0.35M
chk_24  0.75ms  1.36ms  2.49ms  3.34 23.18K  0.30M
chk_25  0.76ms  1.35ms  2.41ms  3.18 23.75K  0.31M
chk_26  0.73ms  1.36ms  2.37ms  3.26 22.29K  0.39M
chk_27  0.75ms  1.38ms  2.42ms  3.24 23.17K  0.35M
chk_28  0.75ms  1.37ms  2.41ms  3.21 23.35K  0.32M
chk_29  0.73ms  1.36ms  2.37ms  3.26 22.21K  0.37M
chk_30  0.77ms  1.40ms  2.46ms  3.20 24.16K  0.27M
chk_31  0.77ms  1.39ms  2.46ms  3.22 24.10K  0.31M
   Avg  0.74  1.36  2.39
   Max  0.82  1.42  2.55
   Min  0.66  1.26  2.16
 Ratio  1.24  1.13  1.18
   Var  0.00  0.00  0.01
Profiling takes 1.810 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 76.546 ms
Partition 0 [0, 2) has cost: 67.218 ms
Partition 1 [2, 3) has cost: 43.419 ms
Partition 2 [3, 4) has cost: 43.419 ms
Partition 3 [4, 5) has cost: 43.419 ms
Partition 4 [5, 6) has cost: 43.419 ms
Partition 5 [6, 7) has cost: 43.419 ms
Partition 6 [7, 8) has cost: 43.419 ms
Partition 7 [8, 9) has cost: 76.546 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 71.573 ms
GPU 0, Compute+Comm Time: 57.465 ms, Bubble Time: 12.903 ms, Imbalance Overhead: 1.205 ms
GPU 1, Compute+Comm Time: 50.744 ms, Bubble Time: 12.770 ms, Imbalance Overhead: 8.059 ms
GPU 2, Compute+Comm Time: 50.744 ms, Bubble Time: 12.789 ms, Imbalance Overhead: 8.040 ms
GPU 3, Compute+Comm Time: 50.744 ms, Bubble Time: 12.654 ms, Imbalance Overhead: 8.174 ms
GPU 4, Compute+Comm Time: 50.744 ms, Bubble Time: 12.401 ms, Imbalance Overhead: 8.428 ms
GPU 5, Compute+Comm Time: 50.744 ms, Bubble Time: 12.544 ms, Imbalance Overhead: 8.285 ms
GPU 6, Compute+Comm Time: 50.744 ms, Bubble Time: 12.342 ms, Imbalance Overhead: 8.486 ms
GPU 7, Compute+Comm Time: 58.312 ms, Bubble Time: 12.188 ms, Imbalance Overhead: 1.072 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 111.052 ms
GPU 0, Compute+Comm Time: 92.926 ms, Bubble Time: 17.880 ms, Imbalance Overhead: 0.246 ms
GPU 1, Compute+Comm Time: 67.366 ms, Bubble Time: 18.337 ms, Imbalance Overhead: 25.348 ms
GPU 2, Compute+Comm Time: 67.366 ms, Bubble Time: 18.853 ms, Imbalance Overhead: 24.832 ms
GPU 3, Compute+Comm Time: 67.366 ms, Bubble Time: 18.886 ms, Imbalance Overhead: 24.799 ms
GPU 4, Compute+Comm Time: 67.366 ms, Bubble Time: 19.499 ms, Imbalance Overhead: 24.186 ms
GPU 5, Compute+Comm Time: 67.366 ms, Bubble Time: 19.918 ms, Imbalance Overhead: 23.767 ms
GPU 6, Compute+Comm Time: 67.366 ms, Bubble Time: 20.122 ms, Imbalance Overhead: 23.563 ms
GPU 7, Compute+Comm Time: 84.445 ms, Bubble Time: 20.532 ms, Imbalance Overhead: 6.075 ms
The estimated cost of the whole pipeline: 191.756 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 119.965 ms
Partition 0 [0, 3) has cost: 110.637 ms
Partition 1 [3, 5) has cost: 86.838 ms
Partition 2 [5, 7) has cost: 86.838 ms
Partition 3 [7, 9) has cost: 119.965 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 74.328 ms
GPU 0, Compute+Comm Time: 61.696 ms, Bubble Time: 11.706 ms, Imbalance Overhead: 0.926 ms
GPU 1, Compute+Comm Time: 58.268 ms, Bubble Time: 11.647 ms, Imbalance Overhead: 4.413 ms
GPU 2, Compute+Comm Time: 58.268 ms, Bubble Time: 11.376 ms, Imbalance Overhead: 4.684 ms
GPU 3, Compute+Comm Time: 62.150 ms, Bubble Time: 11.276 ms, Imbalance Overhead: 0.902 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 103.874 ms
GPU 0, Compute+Comm Time: 88.337 ms, Bubble Time: 15.311 ms, Imbalance Overhead: 0.226 ms
GPU 1, Compute+Comm Time: 75.198 ms, Bubble Time: 15.631 ms, Imbalance Overhead: 13.046 ms
GPU 2, Compute+Comm Time: 75.198 ms, Bubble Time: 16.271 ms, Imbalance Overhead: 12.405 ms
GPU 3, Compute+Comm Time: 83.968 ms, Bubble Time: 16.582 ms, Imbalance Overhead: 3.324 ms
    The estimated cost with 2 DP ways is 187.112 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 206.803 ms
Partition 0 [0, 5) has cost: 197.474 ms
Partition 1 [5, 9) has cost: 206.803 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 96.949 ms
GPU 0, Compute+Comm Time: 85.541 ms, Bubble Time: 10.772 ms, Imbalance Overhead: 0.637 ms
GPU 1, Compute+Comm Time: 85.772 ms, Bubble Time: 10.140 ms, Imbalance Overhead: 1.038 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 120.831 ms
GPU 0, Compute+Comm Time: 107.718 ms, Bubble Time: 12.539 ms, Imbalance Overhead: 0.573 ms
GPU 1, Compute+Comm Time: 105.463 ms, Bubble Time: 13.499 ms, Imbalance Overhead: 1.868 ms
    The estimated cost with 4 DP ways is 228.669 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 404.277 ms
Partition 0 [0, 9) has cost: 404.277 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 212.449 ms
GPU 0, Compute+Comm Time: 212.449 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 233.666 ms
GPU 0, Compute+Comm Time: 233.666 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 468.421 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 13)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [13, 20)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [20, 27)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [27, 34)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [34, 41)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [41, 48)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [55, 65)
*** Node 7, constructing the helper classes...
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [48, 55)
*** Node 6, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 13)...
+++++++++ Node 1 initializing the weights for op[13, 20)...
+++++++++ Node 2 initializing the weights for op[20, 27)...
+++++++++ Node 3 initializing the weights for op[27, 34)...
+++++++++ Node 4 initializing the weights for op[34, 41)...
+++++++++ Node 5 initializing the weights for op[41, 48)...
+++++++++ Node 6 initializing the weights for op[48, 55)...
+++++++++ Node 7 initializing the weights for op[55, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.2624	TrainAcc 0.0672	ValidAcc 0.0418	TestAcc 0.0058	BestValid 0.0418
	Epoch 50:	Loss 4.9630	TrainAcc 0.1057	ValidAcc 0.1017	TestAcc 0.0815	BestValid 0.1017
	Epoch 75:	Loss 4.6058	TrainAcc 0.1305	ValidAcc 0.1099	TestAcc 0.0973	BestValid 0.1099
	Epoch 100:	Loss 4.2556	TrainAcc 0.1505	ValidAcc 0.1351	TestAcc 0.2008	BestValid 0.1351
Node 2, Pre/Post-Pipelining: 0.846 / 6.347 ms, Bubble: 172.822 ms, Compute: 46.404 ms, Comm: 117.056 ms, Imbalance: 122.779 ms
Node 1, Pre/Post-Pipelining: 0.845 / 6.362 ms, Bubble: 176.129 ms, Compute: 47.021 ms, Comm: 93.238 ms, Imbalance: 141.740 ms
Node 5, Pre/Post-Pipelining: 0.846 / 6.331 ms, Bubble: 161.907 ms, Compute: 45.962 ms, Comm: 116.817 ms, Imbalance: 135.305 ms
Node 3, Pre/Post-Pipelining: 0.846 / 6.340 ms, Bubble: 165.313 ms, Compute: 46.201 ms, Comm: 145.695 ms, Imbalance: 101.433 ms
Node 6, Pre/Post-Pipelining: 0.846 / 6.366 ms, Bubble: 157.716 ms, Compute: 46.506 ms, Comm: 93.897 ms, Imbalance: 161.369 ms
Node 4, Pre/Post-Pipelining: 0.844 / 6.347 ms, Bubble: 162.903 ms, Compute: 46.062 ms, Comm: 146.031 ms, Imbalance: 104.997 ms
Node 7, Pre/Post-Pipelining: 0.852 / 125.183 ms, Bubble: 36.835 ms, Compute: 206.893 ms, Comm: 67.494 ms, Imbalance: 28.938 ms
Node 0, Pre/Post-Pipelining: 0.849 / 6.361 ms, Bubble: 180.563 ms, Compute: 70.446 ms, Comm: 67.363 ms, Imbalance: 139.255 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.849 ms
Cluster-Wide Average, Post-Pipelining Overhead: 6.361 ms
Cluster-Wide Average, Bubble: 180.563 ms
Cluster-Wide Average, Compute: 70.446 ms
Cluster-Wide Average, Communication: 67.363 ms
Cluster-Wide Average, Imbalance: 139.255 ms
Node 0, GPU memory consumption: 7.329 GB
Node 6, GPU memory consumption: 3.770 GB
Node 1, GPU memory consumption: 3.770 GB
Node 7, GPU memory consumption: 7.653 GB
Node 2, GPU memory consumption: 3.770 GB
Node 5, GPU memory consumption: 3.770 GB
Node 3, GPU memory consumption: 3.747 GB
Node 4, GPU memory consumption: 3.747 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.893469 s---------------
------------------------node id 1,  per-epoch time: 0.893468 s---------------
------------------------node id 4,  per-epoch time: 0.893469 s---------------
------------------------node id 2,  per-epoch time: 0.893467 s---------------
------------------------node id 5,  per-epoch time: 0.893468 s---------------
------------------------node id 3,  per-epoch time: 0.893468 s---------------
------------------------node id 6,  per-epoch time: 0.893468 s---------------
------------------------node id 7,  per-epoch time: 0.893469 s---------------
************ Profiling Results ************
	Bubble: 806.753669 (ms) (92.02 percentage)
	Compute: 67.928379 (ms) (7.75 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.020169 (ms) (0.23 percentage)
	Layer-level communication (cluster-wide, per-epoch): 7.681 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 7.685 GB
	Aggregated layer-level communication throughput: 622.757 Gbps
Highest valid_acc: 0.1351
Target test_acc: 0.2008
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
