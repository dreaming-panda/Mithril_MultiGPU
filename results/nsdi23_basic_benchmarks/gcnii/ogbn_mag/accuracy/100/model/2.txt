Initialized node 2 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.190 seconds.
Building the CSC structure...
        It takes 0.199 seconds.
Building the CSC structure...
        It takes 0.211 seconds.
Building the CSC structure...
        It takes 0.220 seconds.
Building the CSC structure...
        It takes 0.230 seconds.
Building the CSC structure...
        It takes 0.240 seconds.
Building the CSC structure...
        It takes 0.258 seconds.
Building the CSC structure...
        It takes 0.262 seconds.
Building the CSC structure...
        It takes 0.183 seconds.
        It takes 0.191 seconds.
        It takes 0.207 seconds.
        It takes 0.224 seconds.
        It takes 0.243 seconds.
Building the Feature Vector...
        It takes 0.245 seconds.
Building the Feature Vector...
        It takes 0.261 seconds.
        It takes 0.261 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.243 seconds.
Building the Label Vector...
        It takes 0.247 seconds.
Building the Label Vector...
        It takes 0.241 seconds.
Building the Label Vector...
        It takes 0.246 seconds.
Building the Label Vector...
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.234 seconds.
Building the Label Vector...
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.583 seconds.
        It takes 0.557 seconds.
        It takes 0.576 seconds.
        It takes 0.587 seconds.
        It takes 0.538 seconds.
        It takes 0.600 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_mag/32_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
        It takes 0.601 seconds.
        It takes 0.586 seconds.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 22122) 1-[22122, 45392) 2-[45392, 67023) 3-[67023, 86386) 4-[86386, 111284) 5-[111284, 132574) 6-[132574, 154815) 7-[154815, 176781) 8-[176781, 200751) ... 31-[712285, 736389)
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
GPU 3, layer [4, 5)
GPU 4, layer [5, 6)
GPU 5, layer [6, 7)
GPU 6, layer [7, 8)
GPU 7, layer [8, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
736389, 11529061, 11529061
Number of vertices per chunk: 23013
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.081 Gbps (per GPU), 504.646 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.039 Gbps (per GPU), 504.309 Gbps (aggregated)
The layer-level communication performance: 63.038 Gbps (per GPU), 504.303 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.004 Gbps (per GPU), 504.029 Gbps (aggregated)
The layer-level communication performance: 62.999 Gbps (per GPU), 503.996 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.969 Gbps (per GPU), 503.753 Gbps (aggregated)
The layer-level communication performance: 62.962 Gbps (per GPU), 503.696 Gbps (aggregated)
The layer-level communication performance: 62.958 Gbps (per GPU), 503.667 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 165.955 Gbps (per GPU), 1327.637 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.954 Gbps (per GPU), 1327.633 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.953 Gbps (per GPU), 1327.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.939 Gbps (per GPU), 1327.515 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.952 Gbps (per GPU), 1327.617 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.939 Gbps (per GPU), 1327.512 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.954 Gbps (per GPU), 1327.630 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 165.950 Gbps (per GPU), 1327.597 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.241 Gbps (per GPU), 913.928 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.241 Gbps (per GPU), 913.924 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.239 Gbps (per GPU), 913.913 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.239 Gbps (per GPU), 913.913 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.241 Gbps (per GPU), 913.924 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.240 Gbps (per GPU), 913.920 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.236 Gbps (per GPU), 913.885 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.240 Gbps (per GPU), 913.920 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.478 Gbps (per GPU), 355.822 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.477 Gbps (per GPU), 355.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.477 Gbps (per GPU), 355.818 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.477 Gbps (per GPU), 355.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.478 Gbps (per GPU), 355.821 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.477 Gbps (per GPU), 355.820 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.477 Gbps (per GPU), 355.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.478 Gbps (per GPU), 355.821 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.73ms  1.31ms  2.29ms  3.15 22.12K  0.37M
 chk_1  0.75ms  1.35ms  2.38ms  3.18 23.27K  0.33M
 chk_2  0.71ms  1.31ms  2.28ms  3.21 21.63K  0.36M
 chk_3  0.66ms  1.26ms  2.15ms  3.28 19.36K  0.49M
 chk_4  0.80ms  1.41ms  2.51ms  3.15 24.90K  0.27M
 chk_5  0.71ms  1.32ms  2.29ms  3.23 21.29K  0.42M
 chk_6  0.73ms  1.31ms  2.32ms  3.19 22.24K  0.36M
 chk_7  0.72ms  1.33ms  2.33ms  3.24 21.97K  0.39M
 chk_8  0.76ms  1.41ms  2.46ms  3.23 23.97K  0.31M
 chk_9  0.71ms  1.30ms  2.29ms  3.20 21.69K  0.38M
chk_10  0.77ms  1.38ms  2.44ms  3.17 24.29K  0.27M
chk_11  0.75ms  1.35ms  2.40ms  3.18 23.61K  0.33M
chk_12  0.75ms  1.37ms  2.41ms  3.22 23.29K  0.32M
chk_13  0.76ms  1.35ms  2.40ms  3.16 23.85K  0.30M
chk_14  0.73ms  1.33ms  2.34ms  3.20 22.48K  0.36M
chk_15  0.82ms  1.42ms  2.55ms  3.13 25.69K  0.21M
chk_16  0.80ms  1.41ms  2.51ms  3.15 24.83K  0.29M
chk_17  0.73ms  1.35ms  2.36ms  3.23 22.55K  0.36M
chk_18  0.75ms  1.37ms  2.41ms  3.20 23.46K  0.31M
chk_19  0.67ms  1.32ms  2.22ms  3.33 19.76K  0.47M
chk_20  0.74ms  1.35ms  2.37ms  3.21 22.71K  0.35M
chk_21  0.77ms  1.37ms  2.44ms  3.18 24.18K  0.28M
chk_22  0.77ms  1.39ms  2.45ms  3.19 24.27K  0.29M
chk_23  0.74ms  1.38ms  2.39ms  3.24 22.76K  0.35M
chk_24  0.75ms  1.36ms  2.39ms  3.20 23.18K  0.30M
chk_25  0.76ms  1.35ms  2.40ms  3.17 23.75K  0.31M
chk_26  0.73ms  1.36ms  2.36ms  3.24 22.29K  0.39M
chk_27  0.75ms  1.38ms  2.41ms  3.22 23.17K  0.35M
chk_28  0.75ms  1.36ms  2.40ms  3.20 23.35K  0.32M
chk_29  0.72ms  1.36ms  2.36ms  3.27 22.21K  0.37M
chk_30  0.77ms  1.39ms  2.45ms  3.20 24.16K  0.27M
chk_31  0.76ms  1.39ms  2.45ms  3.20 24.10K  0.31M
   Avg  0.74  1.36  2.38
   Max  0.82  1.42  2.55
   Min  0.66  1.26  2.15
 Ratio  1.24  1.13  1.18
   Var  0.00  0.00  0.01
Profiling takes 1.810 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 76.215 ms
Partition 0 [0, 2) has cost: 67.174 ms
Partition 1 [2, 3) has cost: 43.390 ms
Partition 2 [3, 4) has cost: 43.390 ms
Partition 3 [4, 5) has cost: 43.390 ms
Partition 4 [5, 6) has cost: 43.390 ms
Partition 5 [6, 7) has cost: 43.390 ms
Partition 6 [7, 8) has cost: 43.390 ms
Partition 7 [8, 9) has cost: 76.215 ms
The optimal partitioning:
[0, 2)
[2, 3)
[3, 4)
[4, 5)
[5, 6)
[6, 7)
[7, 8)
[8, 9)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 71.526 ms
GPU 0, Compute+Comm Time: 57.426 ms, Bubble Time: 12.892 ms, Imbalance Overhead: 1.208 ms
GPU 1, Compute+Comm Time: 50.730 ms, Bubble Time: 12.760 ms, Imbalance Overhead: 8.036 ms
GPU 2, Compute+Comm Time: 50.730 ms, Bubble Time: 12.782 ms, Imbalance Overhead: 8.014 ms
GPU 3, Compute+Comm Time: 50.730 ms, Bubble Time: 12.647 ms, Imbalance Overhead: 8.149 ms
GPU 4, Compute+Comm Time: 50.730 ms, Bubble Time: 12.394 ms, Imbalance Overhead: 8.402 ms
GPU 5, Compute+Comm Time: 50.730 ms, Bubble Time: 12.537 ms, Imbalance Overhead: 8.259 ms
GPU 6, Compute+Comm Time: 50.730 ms, Bubble Time: 12.336 ms, Imbalance Overhead: 8.460 ms
GPU 7, Compute+Comm Time: 58.264 ms, Bubble Time: 12.183 ms, Imbalance Overhead: 1.078 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 110.817 ms
GPU 0, Compute+Comm Time: 92.662 ms, Bubble Time: 17.892 ms, Imbalance Overhead: 0.263 ms
GPU 1, Compute+Comm Time: 67.372 ms, Bubble Time: 18.340 ms, Imbalance Overhead: 25.106 ms
GPU 2, Compute+Comm Time: 67.372 ms, Bubble Time: 18.851 ms, Imbalance Overhead: 24.594 ms
GPU 3, Compute+Comm Time: 67.372 ms, Bubble Time: 18.879 ms, Imbalance Overhead: 24.566 ms
GPU 4, Compute+Comm Time: 67.372 ms, Bubble Time: 19.489 ms, Imbalance Overhead: 23.956 ms
GPU 5, Compute+Comm Time: 67.372 ms, Bubble Time: 19.901 ms, Imbalance Overhead: 23.544 ms
GPU 6, Compute+Comm Time: 67.372 ms, Bubble Time: 20.092 ms, Imbalance Overhead: 23.353 ms
GPU 7, Compute+Comm Time: 84.460 ms, Bubble Time: 20.490 ms, Imbalance Overhead: 5.868 ms
The estimated cost of the whole pipeline: 191.460 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 119.605 ms
Partition 0 [0, 3) has cost: 110.564 ms
Partition 1 [3, 5) has cost: 86.780 ms
Partition 2 [5, 7) has cost: 86.780 ms
Partition 3 [7, 9) has cost: 119.605 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 74.289 ms
GPU 0, Compute+Comm Time: 61.666 ms, Bubble Time: 11.698 ms, Imbalance Overhead: 0.926 ms
GPU 1, Compute+Comm Time: 58.248 ms, Bubble Time: 11.641 ms, Imbalance Overhead: 4.401 ms
GPU 2, Compute+Comm Time: 58.248 ms, Bubble Time: 11.372 ms, Imbalance Overhead: 4.669 ms
GPU 3, Compute+Comm Time: 62.110 ms, Bubble Time: 11.271 ms, Imbalance Overhead: 0.909 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 103.712 ms
GPU 0, Compute+Comm Time: 88.158 ms, Bubble Time: 15.324 ms, Imbalance Overhead: 0.229 ms
GPU 1, Compute+Comm Time: 75.189 ms, Bubble Time: 15.637 ms, Imbalance Overhead: 12.885 ms
GPU 2, Compute+Comm Time: 75.189 ms, Bubble Time: 16.270 ms, Imbalance Overhead: 12.253 ms
GPU 3, Compute+Comm Time: 83.966 ms, Bubble Time: 16.563 ms, Imbalance Overhead: 3.183 ms
    The estimated cost with 2 DP ways is 186.901 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 206.385 ms
Partition 0 [0, 5) has cost: 197.343 ms
Partition 1 [5, 9) has cost: 206.385 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 97.015 ms
GPU 0, Compute+Comm Time: 85.600 ms, Bubble Time: 10.777 ms, Imbalance Overhead: 0.639 ms
GPU 1, Compute+Comm Time: 85.828 ms, Bubble Time: 10.149 ms, Imbalance Overhead: 1.039 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 120.826 ms
GPU 0, Compute+Comm Time: 107.685 ms, Bubble Time: 12.574 ms, Imbalance Overhead: 0.568 ms
GPU 1, Compute+Comm Time: 105.561 ms, Bubble Time: 13.499 ms, Imbalance Overhead: 1.766 ms
    The estimated cost with 4 DP ways is 228.734 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 403.728 ms
Partition 0 [0, 9) has cost: 403.728 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 216.324 ms
GPU 0, Compute+Comm Time: 216.324 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 237.511 ms
GPU 0, Compute+Comm Time: 237.511 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 476.527 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 13)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [13, 20)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [20, 27)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [27, 34)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [34, 41)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [41, 48)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [48, 55)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [55, 65)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[13, 20)...
+++++++++ Node 2 initializing the weights for op[20, 27)...
+++++++++ Node 3 initializing the weights for op[27, 34)...
+++++++++ Node 0 initializing the weights for op[0, 13)...
+++++++++ Node 4 initializing the weights for op[34, 41)...
+++++++++ Node 5 initializing the weights for op[41, 48)...
+++++++++ Node 6 initializing the weights for op[48, 55)...
+++++++++ Node 7 initializing the weights for op[55, 65)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.2651	TrainAcc 0.0835	ValidAcc 0.0946	TestAcc 0.0805	BestValid 0.0946
	Epoch 50:	Loss 4.9561	TrainAcc 0.1020	ValidAcc 0.1029	TestAcc 0.0818	BestValid 0.1029
	Epoch 75:	Loss 4.6038	TrainAcc 0.1175	ValidAcc 0.1042	TestAcc 0.0824	BestValid 0.1042
	Epoch 100:	Loss 4.2959	TrainAcc 0.1426	ValidAcc 0.1178	TestAcc 0.0908	BestValid 0.1178
Node 1, Pre/Post-Pipelining: 0.845 / 6.606 ms, Bubble: 176.157 ms, Compute: 47.063 ms, Comm: 93.877 ms, Imbalance: 142.297 ms
Node 2, Pre/Post-Pipelining: 0.847 / 6.608 ms, Bubble: 172.795 ms, Compute: 46.657 ms, Comm: 118.192 ms, Imbalance: 122.697 ms
Node 3, Pre/Post-Pipelining: 0.846 / 6.609 ms, Bubble: 165.245 ms, Compute: 46.779 ms, Comm: 147.103 ms, Imbalance: 100.792 ms
Node 5, Pre/Post-Pipelining: 0.849 / 6.611 ms, Bubble: 162.005 ms, Compute: 46.367 ms, Comm: 117.739 ms, Imbalance: 135.201 ms
Node 6, Pre/Post-Pipelining: 0.845 / 6.603 ms, Bubble: 157.938 ms, Compute: 46.614 ms, Comm: 94.629 ms, Imbalance: 161.643 ms
Node 0, Pre/Post-Pipelining: 0.849 / 6.606 ms, Bubble: 180.654 ms, Compute: 70.440 ms, Comm: 67.711 ms, Imbalance: 140.108 ms
Node 4, Pre/Post-Pipelining: 0.842 / 6.626 ms, Bubble: 162.886 ms, Compute: 46.437 ms, Comm: 147.388 ms, Imbalance: 104.539 ms
Node 7, Pre/Post-Pipelining: 0.849 / 125.483 ms, Bubble: 36.992 ms, Compute: 207.215 ms, Comm: 67.868 ms, Imbalance: 29.269 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.849 ms
Cluster-Wide Average, Post-Pipelining Overhead: 6.606 ms
Cluster-Wide Average, Bubble: 180.654 ms
Cluster-Wide Average, Compute: 70.440 ms
Cluster-Wide Average, Communication: 67.711 ms
Cluster-Wide Average, Imbalance: 140.108 ms
Node 0, GPU memory consumption: 7.331 GB
Node 2, GPU memory consumption: 3.770 GB
Node 1, GPU memory consumption: 3.770 GB
Node 4, GPU memory consumption: 3.747 GB
Node 3, GPU memory consumption: 3.747 GB
Node 5, GPU memory consumption: 3.770 GB
Node 7, GPU memory consumption: 7.653 GB
Node 6, GPU memory consumption: 3.770 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.897573 s---------------
------------------------node id 1,  per-epoch time: 0.897572 s---------------
------------------------node id 2,  per-epoch time: 0.897572 s---------------
------------------------node id 3,  per-epoch time: 0.897572 s---------------
------------------------node id 4,  per-epoch time: 0.897574 s---------------
------------------------node id 5,  per-epoch time: 0.897572 s---------------
------------------------node id 6,  per-epoch time: 0.897572 s---------------
------------------------node id 7,  per-epoch time: 0.897574 s---------------
************ Profiling Results ************
	Bubble: 810.591710 (ms) (92.03 percentage)
	Compute: 68.125595 (ms) (7.73 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 2.036029 (ms) (0.23 percentage)
	Layer-level communication (cluster-wide, per-epoch): 7.681 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 7.685 GB
	Aggregated layer-level communication throughput: 617.717 Gbps
Highest valid_acc: 0.1178
Target test_acc: 0.0908
Epoch to reach the target acc: 99
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
