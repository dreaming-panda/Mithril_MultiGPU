Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.192 seconds.
Building the CSC structure...
        It takes 0.239 seconds.
Building the CSC structure...
        It takes 0.235 seconds.
Building the CSC structure...
        It takes 0.248 seconds.
Building the CSC structure...
        It takes 0.258 seconds.
Building the CSC structure...
        It takes 0.252 seconds.
Building the CSC structure...
        It takes 0.263 seconds.
Building the CSC structure...
        It takes 0.274 seconds.
Building the CSC structure...
        It takes 0.184 seconds.
        It takes 0.240 seconds.
        It takes 0.241 seconds.
Building the Feature Vector...
        It takes 0.244 seconds.
        It takes 0.260 seconds.
        It takes 0.261 seconds.
        It takes 0.259 seconds.
        It takes 0.264 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.238 seconds.
Building the Label Vector...
        It takes 0.232 seconds.
Building the Label Vector...
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.236 seconds.
Building the Label Vector...
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.246 seconds.
Building the Label Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.578 seconds.
        It takes 0.523 seconds.
        It takes 0.586 seconds.
        It takes 0.546 seconds.
        It takes 0.596 seconds.
        It takes 0.578 seconds.
        It takes 0.594 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_mag/8_parts
The number of GCNII layers: 32
The number of hidden units: 16
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
        It takes 0.594 seconds.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 92048) 1-[92048, 184097) 2-[184097, 276145) 3-[276145, 368194) 4-[368194, 460242) 5-[460242, 552291) 6-[552291, 644340) 7-[644340, 736389)
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.834 Gbps (per GPU), 510.676 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.794 Gbps (per GPU), 510.349 Gbps (aggregated)
The layer-level communication performance: 63.793 Gbps (per GPU), 510.344 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.757 Gbps (per GPU), 510.055 Gbps (aggregated)
The layer-level communication performance: 63.753 Gbps (per GPU), 510.022 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.721 Gbps (per GPU), 509.767 Gbps (aggregated)
The layer-level communication performance: 63.714 Gbps (per GPU), 509.715 Gbps (aggregated)
The layer-level communication performance: 63.710 Gbps (per GPU), 509.679 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.770 Gbps (per GPU), 1310.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.764 Gbps (per GPU), 1310.112 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.768 Gbps (per GPU), 1310.141 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.760 Gbps (per GPU), 1310.083 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.768 Gbps (per GPU), 1310.147 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.754 Gbps (per GPU), 1310.030 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.765 Gbps (per GPU), 1310.122 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.770 Gbps (per GPU), 1310.160 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.530 Gbps (per GPU), 908.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.530 Gbps (per GPU), 908.243 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.530 Gbps (per GPU), 908.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.529 Gbps (per GPU), 908.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.529 Gbps (per GPU), 908.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.530 Gbps (per GPU), 908.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.529 Gbps (per GPU), 908.230 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.529 Gbps (per GPU), 908.233 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 44.595 Gbps (per GPU), 356.762 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.596 Gbps (per GPU), 356.764 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.595 Gbps (per GPU), 356.759 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.595 Gbps (per GPU), 356.761 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.595 Gbps (per GPU), 356.762 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.595 Gbps (per GPU), 356.761 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.595 Gbps (per GPU), 356.761 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 44.594 Gbps (per GPU), 356.755 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.37ms  1.85ms  4.34ms  3.18 92.05K  2.94M
 chk_1  1.37ms  1.40ms  3.91ms  2.86 92.05K  1.61M
 chk_2  1.37ms  1.38ms  3.88ms  2.84 92.05K  1.55M
 chk_3  1.37ms  1.15ms  3.65ms  3.17 92.05K  0.83M
 chk_4  1.37ms  1.26ms  3.75ms  2.98 92.05K  1.15M
 chk_5  1.37ms  1.13ms  3.62ms  3.21 92.05K  0.77M
 chk_6  1.37ms  1.22ms  3.72ms  3.04 92.05K  1.04M
 chk_7  1.37ms  1.18ms  3.67ms  3.12 92.05K  0.91M
   Avg  1.37  1.32  3.82
   Max  1.37  1.85  4.34
   Min  1.37  1.13  3.62
 Ratio  1.00  1.64  1.20
   Var  0.00  0.05  0.05
Profiling takes 0.696 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 52.820 ms
Partition 0 [0, 4) has cost: 42.624 ms
Partition 1 [4, 8) has cost: 42.256 ms
Partition 2 [8, 12) has cost: 42.256 ms
Partition 3 [12, 16) has cost: 42.256 ms
Partition 4 [16, 20) has cost: 42.256 ms
Partition 5 [20, 25) has cost: 52.820 ms
Partition 6 [25, 30) has cost: 52.820 ms
Partition 7 [30, 33) has cost: 51.680 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 25)
[25, 30)
[30, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 45.714 ms
GPU 0, Compute+Comm Time: 19.124 ms, Bubble Time: 19.298 ms, Imbalance Overhead: 7.292 ms
GPU 1, Compute+Comm Time: 19.815 ms, Bubble Time: 19.204 ms, Imbalance Overhead: 6.695 ms
GPU 2, Compute+Comm Time: 19.815 ms, Bubble Time: 19.579 ms, Imbalance Overhead: 6.320 ms
GPU 3, Compute+Comm Time: 19.815 ms, Bubble Time: 19.954 ms, Imbalance Overhead: 5.945 ms
GPU 4, Compute+Comm Time: 19.815 ms, Bubble Time: 20.392 ms, Imbalance Overhead: 5.507 ms
GPU 5, Compute+Comm Time: 23.293 ms, Bubble Time: 20.830 ms, Imbalance Overhead: 1.592 ms
GPU 6, Compute+Comm Time: 23.293 ms, Bubble Time: 21.952 ms, Imbalance Overhead: 0.470 ms
GPU 7, Compute+Comm Time: 21.188 ms, Bubble Time: 23.241 ms, Imbalance Overhead: 1.285 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.404 ms
GPU 0, Compute+Comm Time: 42.305 ms, Bubble Time: 42.095 ms, Imbalance Overhead: 0.004 ms
GPU 1, Compute+Comm Time: 41.341 ms, Bubble Time: 40.163 ms, Imbalance Overhead: 2.900 ms
GPU 2, Compute+Comm Time: 41.341 ms, Bubble Time: 38.330 ms, Imbalance Overhead: 4.734 ms
GPU 3, Compute+Comm Time: 34.254 ms, Bubble Time: 37.540 ms, Imbalance Overhead: 12.611 ms
GPU 4, Compute+Comm Time: 34.254 ms, Bubble Time: 37.006 ms, Imbalance Overhead: 13.144 ms
GPU 5, Compute+Comm Time: 34.254 ms, Bubble Time: 36.270 ms, Imbalance Overhead: 13.881 ms
GPU 6, Compute+Comm Time: 34.254 ms, Bubble Time: 35.984 ms, Imbalance Overhead: 14.166 ms
GPU 7, Compute+Comm Time: 35.313 ms, Bubble Time: 35.968 ms, Imbalance Overhead: 13.123 ms
The estimated cost of the whole pipeline: 136.624 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 95.077 ms
Partition 0 [0, 8) has cost: 84.880 ms
Partition 1 [8, 17) has cost: 95.077 ms
Partition 2 [17, 26) has cost: 95.077 ms
Partition 3 [26, 33) has cost: 93.937 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 49.802 ms
GPU 0, Compute+Comm Time: 24.501 ms, Bubble Time: 18.782 ms, Imbalance Overhead: 6.519 ms
GPU 1, Compute+Comm Time: 28.022 ms, Bubble Time: 19.395 ms, Imbalance Overhead: 2.385 ms
GPU 2, Compute+Comm Time: 28.022 ms, Bubble Time: 21.283 ms, Imbalance Overhead: 0.497 ms
GPU 3, Compute+Comm Time: 25.528 ms, Bubble Time: 23.656 ms, Imbalance Overhead: 0.619 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 84.301 ms
GPU 0, Compute+Comm Time: 44.299 ms, Bubble Time: 40.002 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 45.546 ms, Bubble Time: 36.220 ms, Imbalance Overhead: 2.535 ms
GPU 2, Compute+Comm Time: 45.546 ms, Bubble Time: 32.580 ms, Imbalance Overhead: 6.174 ms
GPU 3, Compute+Comm Time: 40.812 ms, Bubble Time: 31.259 ms, Imbalance Overhead: 12.230 ms
    The estimated cost with 2 DP ways is 140.808 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 189.013 ms
Partition 0 [0, 17) has cost: 179.957 ms
Partition 1 [17, 33) has cost: 189.013 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 66.486 ms
GPU 0, Compute+Comm Time: 42.932 ms, Bubble Time: 20.143 ms, Imbalance Overhead: 3.410 ms
GPU 1, Compute+Comm Time: 43.442 ms, Bubble Time: 23.044 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 96.662 ms
GPU 0, Compute+Comm Time: 62.915 ms, Bubble Time: 33.747 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 61.175 ms, Bubble Time: 28.298 ms, Imbalance Overhead: 7.189 ms
    The estimated cost with 4 DP ways is 171.305 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 368.970 ms
Partition 0 [0, 33) has cost: 368.970 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 138.874 ms
GPU 0, Compute+Comm Time: 138.874 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 160.896 ms
GPU 0, Compute+Comm Time: 160.896 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 314.759 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 5.5740	TrainAcc 0.0111	ValidAcc 0.0036	TestAcc 0.0022	BestValid 0.0036
	Epoch 50:	Loss 5.1917	TrainAcc 0.0448	ValidAcc 0.0348	TestAcc 0.0048	BestValid 0.0348
	Epoch 75:	Loss 5.1345	TrainAcc 0.0448	ValidAcc 0.0348	TestAcc 0.0048	BestValid 0.0348
	Epoch 100:	Loss 5.1084	TrainAcc 0.0448	ValidAcc 0.0348	TestAcc 0.0048	BestValid 0.0348
	Epoch 125:	Loss 5.0863	TrainAcc 0.0453	ValidAcc 0.0362	TestAcc 0.0062	BestValid 0.0362
	Epoch 150:	Loss 5.0606	TrainAcc 0.0689	ValidAcc 0.0908	TestAcc 0.0693	BestValid 0.0908
	Epoch 175:	Loss 5.0343	TrainAcc 0.0797	ValidAcc 0.0937	TestAcc 0.0797	BestValid 0.0937
	Epoch 200:	Loss 4.9924	TrainAcc 0.0822	ValidAcc 0.0940	TestAcc 0.0803	BestValid 0.0940
	Epoch 225:	Loss 4.9439	TrainAcc 0.0835	ValidAcc 0.0946	TestAcc 0.0805	BestValid 0.0946
	Epoch 250:	Loss 4.8898	TrainAcc 0.0874	ValidAcc 0.0959	TestAcc 0.0807	BestValid 0.0959
	Epoch 275:	Loss 4.8341	TrainAcc 0.0960	ValidAcc 0.0989	TestAcc 0.0818	BestValid 0.0989
	Epoch 300:	Loss 4.7803	TrainAcc 0.1018	ValidAcc 0.1016	TestAcc 0.0834	BestValid 0.1016
	Epoch 325:	Loss 4.7257	TrainAcc 0.1055	ValidAcc 0.1031	TestAcc 0.0857	BestValid 0.1031
	Epoch 350:	Loss 4.6782	TrainAcc 0.1080	ValidAcc 0.1038	TestAcc 0.0888	BestValid 0.1038
	Epoch 375:	Loss 4.6325	TrainAcc 0.1102	ValidAcc 0.1046	TestAcc 0.0928	BestValid 0.1046
	Epoch 400:	Loss 4.5977	TrainAcc 0.1125	ValidAcc 0.1051	TestAcc 0.0978	BestValid 0.1051
	Epoch 425:	Loss 4.5629	TrainAcc 0.1168	ValidAcc 0.1056	TestAcc 0.1035	BestValid 0.1056
	Epoch 450:	Loss 4.5340	TrainAcc 0.1237	ValidAcc 0.1064	TestAcc 0.1100	BestValid 0.1064
	Epoch 475:	Loss 4.5078	TrainAcc 0.1303	ValidAcc 0.1073	TestAcc 0.1183	BestValid 0.1073
	Epoch 500:	Loss 4.4799	TrainAcc 0.1355	ValidAcc 0.1090	TestAcc 0.1289	BestValid 0.1090
	Epoch 525:	Loss 4.4589	TrainAcc 0.1391	ValidAcc 0.1108	TestAcc 0.1406	BestValid 0.1108
	Epoch 550:	Loss 4.4336	TrainAcc 0.1420	ValidAcc 0.1125	TestAcc 0.1552	BestValid 0.1125
	Epoch 575:	Loss 4.4147	TrainAcc 0.1439	ValidAcc 0.1148	TestAcc 0.1685	BestValid 0.1148
	Epoch 600:	Loss 4.3953	TrainAcc 0.1458	ValidAcc 0.1170	TestAcc 0.1753	BestValid 0.1170
	Epoch 625:	Loss 4.3768	TrainAcc 0.1473	ValidAcc 0.1186	TestAcc 0.1832	BestValid 0.1186
	Epoch 650:	Loss 4.3598	TrainAcc 0.1487	ValidAcc 0.1202	TestAcc 0.1900	BestValid 0.1202
	Epoch 675:	Loss 4.3439	TrainAcc 0.1498	ValidAcc 0.1213	TestAcc 0.1957	BestValid 0.1213
	Epoch 700:	Loss 4.3297	TrainAcc 0.1511	ValidAcc 0.1232	TestAcc 0.2009	BestValid 0.1232
	Epoch 725:	Loss 4.3172	TrainAcc 0.1524	ValidAcc 0.1262	TestAcc 0.2066	BestValid 0.1262
	Epoch 750:	Loss 4.3017	TrainAcc 0.1537	ValidAcc 0.1297	TestAcc 0.2099	BestValid 0.1297
	Epoch 775:	Loss 4.2902	TrainAcc 0.1555	ValidAcc 0.1344	TestAcc 0.2127	BestValid 0.1344
	Epoch 800:	Loss 4.2767	TrainAcc 0.1571	ValidAcc 0.1382	TestAcc 0.2136	BestValid 0.1382
	Epoch 825:	Loss 4.2663	TrainAcc 0.1580	ValidAcc 0.1416	TestAcc 0.2154	BestValid 0.1416
	Epoch 850:	Loss 4.2582	TrainAcc 0.1603	ValidAcc 0.1461	TestAcc 0.2145	BestValid 0.1461
	Epoch 875:	Loss 4.2496	TrainAcc 0.1616	ValidAcc 0.1490	TestAcc 0.2130	BestValid 0.1490
	Epoch 900:	Loss 4.2389	TrainAcc 0.1635	ValidAcc 0.1536	TestAcc 0.2156	BestValid 0.1536
	Epoch 925:	Loss 4.2299	TrainAcc 0.1648	ValidAcc 0.1565	TestAcc 0.2113	BestValid 0.1565
	Epoch 950:	Loss 4.2236	TrainAcc 0.1657	ValidAcc 0.1597	TestAcc 0.2138	BestValid 0.1597
	Epoch 975:	Loss 4.2155	TrainAcc 0.1672	ValidAcc 0.1628	TestAcc 0.2121	BestValid 0.1628
	Epoch 1000:	Loss 4.2073	TrainAcc 0.1686	ValidAcc 0.1660	TestAcc 0.2104	BestValid 0.1660
	Epoch 1025:	Loss 4.2002	TrainAcc 0.1691	ValidAcc 0.1675	TestAcc 0.2089	BestValid 0.1675
	Epoch 1050:	Loss 4.1958	TrainAcc 0.1698	ValidAcc 0.1695	TestAcc 0.2115	BestValid 0.1695
	Epoch 1075:	Loss 4.1853	TrainAcc 0.1703	ValidAcc 0.1710	TestAcc 0.2103	BestValid 0.1710
	Epoch 1100:	Loss 4.1797	TrainAcc 0.1713	ValidAcc 0.1732	TestAcc 0.2087	BestValid 0.1732
	Epoch 1125:	Loss 4.1738	TrainAcc 0.1721	ValidAcc 0.1744	TestAcc 0.2062	BestValid 0.1744
	Epoch 1150:	Loss 4.1670	TrainAcc 0.1724	ValidAcc 0.1762	TestAcc 0.2090	BestValid 0.1762
	Epoch 1175:	Loss 4.1637	TrainAcc 0.1729	ValidAcc 0.1772	TestAcc 0.2066	BestValid 0.1772
	Epoch 1200:	Loss 4.1595	TrainAcc 0.1731	ValidAcc 0.1778	TestAcc 0.2089	BestValid 0.1778
	Epoch 1225:	Loss 4.1550	TrainAcc 0.1739	ValidAcc 0.1794	TestAcc 0.2066	BestValid 0.1794
	Epoch 1250:	Loss 4.1512	TrainAcc 0.1747	ValidAcc 0.1810	TestAcc 0.2047	BestValid 0.1810
	Epoch 1275:	Loss 4.1455	TrainAcc 0.1752	ValidAcc 0.1819	TestAcc 0.2058	BestValid 0.1819
	Epoch 1300:	Loss 4.1436	TrainAcc 0.1754	ValidAcc 0.1821	TestAcc 0.2073	BestValid 0.1821
	Epoch 1325:	Loss 4.1393	TrainAcc 0.1757	ValidAcc 0.1831	TestAcc 0.2076	BestValid 0.1831
	Epoch 1350:	Loss 4.1332	TrainAcc 0.1761	ValidAcc 0.1837	TestAcc 0.2077	BestValid 0.1837
	Epoch 1375:	Loss 4.1288	TrainAcc 0.1766	ValidAcc 0.1847	TestAcc 0.2037	BestValid 0.1847
	Epoch 1400:	Loss 4.1306	TrainAcc 0.1771	ValidAcc 0.1849	TestAcc 0.2032	BestValid 0.1849
	Epoch 1425:	Loss 4.1202	TrainAcc 0.1778	ValidAcc 0.1870	TestAcc 0.2012	BestValid 0.1870
	Epoch 1450:	Loss 4.1244	TrainAcc 0.1782	ValidAcc 0.1877	TestAcc 0.2058	BestValid 0.1877
	Epoch 1475:	Loss 4.1170	TrainAcc 0.1788	ValidAcc 0.1885	TestAcc 0.2051	BestValid 0.1885
	Epoch 1500:	Loss 4.1126	TrainAcc 0.1791	ValidAcc 0.1890	TestAcc 0.2049	BestValid 0.1890
	Epoch 1525:	Loss 4.1121	TrainAcc 0.1793	ValidAcc 0.1892	TestAcc 0.2041	BestValid 0.1892
	Epoch 1550:	Loss 4.1111	TrainAcc 0.1799	ValidAcc 0.1898	TestAcc 0.2050	BestValid 0.1898
	Epoch 1575:	Loss 4.1041	TrainAcc 0.1803	ValidAcc 0.1902	TestAcc 0.2065	BestValid 0.1902
	Epoch 1600:	Loss 4.1015	TrainAcc 0.1807	ValidAcc 0.1904	TestAcc 0.2069	BestValid 0.1904
	Epoch 1625:	Loss 4.0996	TrainAcc 0.1817	ValidAcc 0.1906	TestAcc 0.2030	BestValid 0.1906
	Epoch 1650:	Loss 4.0988	TrainAcc 0.1821	ValidAcc 0.1908	TestAcc 0.2053	BestValid 0.1908
	Epoch 1675:	Loss 4.0911	TrainAcc 0.1820	ValidAcc 0.1907	TestAcc 0.2059	BestValid 0.1908
	Epoch 1700:	Loss 4.0936	TrainAcc 0.1825	ValidAcc 0.1910	TestAcc 0.2056	BestValid 0.1910
	Epoch 1725:	Loss 4.0915	TrainAcc 0.1832	ValidAcc 0.1909	TestAcc 0.2070	BestValid 0.1910
	Epoch 1750:	Loss 4.0853	TrainAcc 0.1831	ValidAcc 0.1910	TestAcc 0.2080	BestValid 0.1910
	Epoch 1775:	Loss 4.0840	TrainAcc 0.1838	ValidAcc 0.1907	TestAcc 0.2078	BestValid 0.1910
	Epoch 1800:	Loss 4.0815	TrainAcc 0.1841	ValidAcc 0.1915	TestAcc 0.2084	BestValid 0.1915
	Epoch 1825:	Loss 4.0804	TrainAcc 0.1844	ValidAcc 0.1910	TestAcc 0.2088	BestValid 0.1915
	Epoch 1850:	Loss 4.0778	TrainAcc 0.1844	ValidAcc 0.1912	TestAcc 0.2115	BestValid 0.1915
	Epoch 1875:	Loss 4.0750	TrainAcc 0.1853	ValidAcc 0.1916	TestAcc 0.2098	BestValid 0.1916
	Epoch 1900:	Loss 4.0736	TrainAcc 0.1855	ValidAcc 0.1920	TestAcc 0.2112	BestValid 0.1920
	Epoch 1925:	Loss 4.0714	TrainAcc 0.1860	ValidAcc 0.1916	TestAcc 0.2134	BestValid 0.1920
	Epoch 1950:	Loss 4.0664	TrainAcc 0.1859	ValidAcc 0.1917	TestAcc 0.2149	BestValid 0.1920
	Epoch 1975:	Loss 4.0665	TrainAcc 0.1866	ValidAcc 0.1919	TestAcc 0.2199	BestValid 0.1920
	Epoch 2000:	Loss 4.0634	TrainAcc 0.1868	ValidAcc 0.1917	TestAcc 0.2191	BestValid 0.1920
	Epoch 2025:	Loss 4.0632	TrainAcc 0.1874	ValidAcc 0.1923	TestAcc 0.2206	BestValid 0.1923
	Epoch 2050:	Loss 4.0606	TrainAcc 0.1879	ValidAcc 0.1919	TestAcc 0.2180	BestValid 0.1923
	Epoch 2075:	Loss 4.0564	TrainAcc 0.1880	ValidAcc 0.1918	TestAcc 0.2194	BestValid 0.1923
	Epoch 2100:	Loss 4.0548	TrainAcc 0.1883	ValidAcc 0.1918	TestAcc 0.2185	BestValid 0.1923
	Epoch 2125:	Loss 4.0543	TrainAcc 0.1884	ValidAcc 0.1919	TestAcc 0.2262	BestValid 0.1923
	Epoch 2150:	Loss 4.0531	TrainAcc 0.1890	ValidAcc 0.1921	TestAcc 0.2260	BestValid 0.1923
	Epoch 2175:	Loss 4.0499	TrainAcc 0.1894	ValidAcc 0.1920	TestAcc 0.2248	BestValid 0.1923
	Epoch 2200:	Loss 4.0521	TrainAcc 0.1898	ValidAcc 0.1924	TestAcc 0.2313	BestValid 0.1924
	Epoch 2225:	Loss 4.0506	TrainAcc 0.1902	ValidAcc 0.1926	TestAcc 0.2272	BestValid 0.1926
	Epoch 2250:	Loss 4.0478	TrainAcc 0.1910	ValidAcc 0.1922	TestAcc 0.2286	BestValid 0.1926
	Epoch 2275:	Loss 4.0443	TrainAcc 0.1910	ValidAcc 0.1922	TestAcc 0.2315	BestValid 0.1926
	Epoch 2300:	Loss 4.0441	TrainAcc 0.1915	ValidAcc 0.1923	TestAcc 0.2340	BestValid 0.1926
	Epoch 2325:	Loss 4.0411	TrainAcc 0.1917	ValidAcc 0.1924	TestAcc 0.2360	BestValid 0.1926
	Epoch 2350:	Loss 4.0410	TrainAcc 0.1920	ValidAcc 0.1921	TestAcc 0.2374	BestValid 0.1926
	Epoch 2375:	Loss 4.0360	TrainAcc 0.1925	ValidAcc 0.1924	TestAcc 0.2398	BestValid 0.1926
	Epoch 2400:	Loss 4.0373	TrainAcc 0.1930	ValidAcc 0.1924	TestAcc 0.2419	BestValid 0.1926
	Epoch 2425:	Loss 4.0337	TrainAcc 0.1935	ValidAcc 0.1925	TestAcc 0.2418	BestValid 0.1926
	Epoch 2450:	Loss 4.0307	TrainAcc 0.1937	ValidAcc 0.1922	TestAcc 0.2431	BestValid 0.1926
	Epoch 2475:	Loss 4.0353	TrainAcc 0.1938	ValidAcc 0.1929	TestAcc 0.2469	BestValid 0.1929
	Epoch 2500:	Loss 4.0290	TrainAcc 0.1941	ValidAcc 0.1926	TestAcc 0.2452	BestValid 0.1929
	Epoch 2525:	Loss 4.0265	TrainAcc 0.1947	ValidAcc 0.1928	TestAcc 0.2470	BestValid 0.1929
	Epoch 2550:	Loss 4.0251	TrainAcc 0.1947	ValidAcc 0.1927	TestAcc 0.2495	BestValid 0.1929
	Epoch 2575:	Loss 4.0269	TrainAcc 0.1955	ValidAcc 0.1918	TestAcc 0.2467	BestValid 0.1929
	Epoch 2600:	Loss 4.0272	TrainAcc 0.1960	ValidAcc 0.1921	TestAcc 0.2486	BestValid 0.1929
	Epoch 2625:	Loss 4.0241	TrainAcc 0.1954	ValidAcc 0.1929	TestAcc 0.2504	BestValid 0.1929
	Epoch 2650:	Loss 4.0238	TrainAcc 0.1964	ValidAcc 0.1926	TestAcc 0.2505	BestValid 0.1929
	Epoch 2675:	Loss 4.0206	TrainAcc 0.1963	ValidAcc 0.1929	TestAcc 0.2528	BestValid 0.1929
	Epoch 2700:	Loss 4.0190	TrainAcc 0.1964	ValidAcc 0.1927	TestAcc 0.2511	BestValid 0.1929
	Epoch 2725:	Loss 4.0145	TrainAcc 0.1969	ValidAcc 0.1922	TestAcc 0.2501	BestValid 0.1929
	Epoch 2750:	Loss 4.0139	TrainAcc 0.1968	ValidAcc 0.1924	TestAcc 0.2522	BestValid 0.1929
	Epoch 2775:	Loss 4.0130	TrainAcc 0.1969	ValidAcc 0.1924	TestAcc 0.2527	BestValid 0.1929
	Epoch 2800:	Loss 4.0113	TrainAcc 0.1973	ValidAcc 0.1920	TestAcc 0.2508	BestValid 0.1929
	Epoch 2825:	Loss 4.0144	TrainAcc 0.1977	ValidAcc 0.1928	TestAcc 0.2515	BestValid 0.1929
	Epoch 2850:	Loss 4.0089	TrainAcc 0.1980	ValidAcc 0.1929	TestAcc 0.2516	BestValid 0.1929
	Epoch 2875:	Loss 4.0057	TrainAcc 0.1983	ValidAcc 0.1916	TestAcc 0.2521	BestValid 0.1929
	Epoch 2900:	Loss 4.0047	TrainAcc 0.1983	ValidAcc 0.1921	TestAcc 0.2517	BestValid 0.1929
	Epoch 2925:	Loss 4.0084	TrainAcc 0.1983	ValidAcc 0.1916	TestAcc 0.2521	BestValid 0.1929
	Epoch 2950:	Loss 4.0102	TrainAcc 0.1991	ValidAcc 0.1913	TestAcc 0.2515	BestValid 0.1929
	Epoch 2975:	Loss 4.0046	TrainAcc 0.1987	ValidAcc 0.1916	TestAcc 0.2511	BestValid 0.1929
	Epoch 3000:	Loss 4.0056	TrainAcc 0.1993	ValidAcc 0.1916	TestAcc 0.2523	BestValid 0.1929
	Epoch 3025:	Loss 4.0058	TrainAcc 0.1995	ValidAcc 0.1922	TestAcc 0.2525	BestValid 0.1929
	Epoch 3050:	Loss 4.0014	TrainAcc 0.1997	ValidAcc 0.1906	TestAcc 0.2508	BestValid 0.1929
	Epoch 3075:	Loss 4.0008	TrainAcc 0.1999	ValidAcc 0.1922	TestAcc 0.2523	BestValid 0.1929
	Epoch 3100:	Loss 3.9987	TrainAcc 0.2002	ValidAcc 0.1908	TestAcc 0.2510	BestValid 0.1929
	Epoch 3125:	Loss 4.0016	TrainAcc 0.2010	ValidAcc 0.1911	TestAcc 0.2518	BestValid 0.1929
	Epoch 3150:	Loss 4.0012	TrainAcc 0.2014	ValidAcc 0.1916	TestAcc 0.2499	BestValid 0.1929
	Epoch 3175:	Loss 3.9960	TrainAcc 0.2013	ValidAcc 0.1913	TestAcc 0.2516	BestValid 0.1929
	Epoch 3200:	Loss 3.9981	TrainAcc 0.2019	ValidAcc 0.1910	TestAcc 0.2510	BestValid 0.1929
	Epoch 3225:	Loss 3.9984	TrainAcc 0.2014	ValidAcc 0.1903	TestAcc 0.2513	BestValid 0.1929
	Epoch 3250:	Loss 3.9939	TrainAcc 0.2020	ValidAcc 0.1906	TestAcc 0.2505	BestValid 0.1929
	Epoch 3275:	Loss 3.9937	TrainAcc 0.2025	ValidAcc 0.1910	TestAcc 0.2515	BestValid 0.1929
	Epoch 3300:	Loss 3.9950	TrainAcc 0.2028	ValidAcc 0.1912	TestAcc 0.2506	BestValid 0.1929
	Epoch 3325:	Loss 3.9953	TrainAcc 0.2028	ValidAcc 0.1909	TestAcc 0.2508	BestValid 0.1929
	Epoch 3350:	Loss 3.9915	TrainAcc 0.2034	ValidAcc 0.1913	TestAcc 0.2520	BestValid 0.1929
	Epoch 3375:	Loss 3.9881	TrainAcc 0.2039	ValidAcc 0.1913	TestAcc 0.2514	BestValid 0.1929
	Epoch 3400:	Loss 3.9864	TrainAcc 0.2041	ValidAcc 0.1918	TestAcc 0.2513	BestValid 0.1929
	Epoch 3425:	Loss 3.9914	TrainAcc 0.2040	ValidAcc 0.1904	TestAcc 0.2503	BestValid 0.1929
	Epoch 3450:	Loss 3.9885	TrainAcc 0.2048	ValidAcc 0.1909	TestAcc 0.2507	BestValid 0.1929
	Epoch 3475:	Loss 3.9889	TrainAcc 0.2047	ValidAcc 0.1902	TestAcc 0.2512	BestValid 0.1929
	Epoch 3500:	Loss 3.9876	TrainAcc 0.2051	ValidAcc 0.1902	TestAcc 0.2524	BestValid 0.1929
	Epoch 3525:	Loss 3.9868	TrainAcc 0.2055	ValidAcc 0.1908	TestAcc 0.2508	BestValid 0.1929
	Epoch 3550:	Loss 3.9826	TrainAcc 0.2057	ValidAcc 0.1901	TestAcc 0.2518	BestValid 0.1929
	Epoch 3575:	Loss 3.9822	TrainAcc 0.2063	ValidAcc 0.1900	TestAcc 0.2502	BestValid 0.1929
	Epoch 3600:	Loss 3.9834	TrainAcc 0.2061	ValidAcc 0.1899	TestAcc 0.2506	BestValid 0.1929
	Epoch 3625:	Loss 3.9816	TrainAcc 0.2064	ValidAcc 0.1897	TestAcc 0.2509	BestValid 0.1929
	Epoch 3650:	Loss 3.9816	TrainAcc 0.2060	ValidAcc 0.1896	TestAcc 0.2519	BestValid 0.1929
	Epoch 3675:	Loss 3.9814	TrainAcc 0.2073	ValidAcc 0.1902	TestAcc 0.2506	BestValid 0.1929
	Epoch 3700:	Loss 3.9771	TrainAcc 0.2072	ValidAcc 0.1901	TestAcc 0.2508	BestValid 0.1929
	Epoch 3725:	Loss 3.9789	TrainAcc 0.2069	ValidAcc 0.1898	TestAcc 0.2518	BestValid 0.1929
	Epoch 3750:	Loss 3.9787	TrainAcc 0.2075	ValidAcc 0.1900	TestAcc 0.2512	BestValid 0.1929
	Epoch 3775:	Loss 3.9767	TrainAcc 0.2076	ValidAcc 0.1900	TestAcc 0.2502	BestValid 0.1929
	Epoch 3800:	Loss 3.9760	TrainAcc 0.2087	ValidAcc 0.1905	TestAcc 0.2510	BestValid 0.1929
	Epoch 3825:	Loss 3.9785	TrainAcc 0.2082	ValidAcc 0.1907	TestAcc 0.2543	BestValid 0.1929
	Epoch 3850:	Loss 3.9780	TrainAcc 0.2083	ValidAcc 0.1898	TestAcc 0.2501	BestValid 0.1929
	Epoch 3875:	Loss 3.9754	TrainAcc 0.2088	ValidAcc 0.1899	TestAcc 0.2513	BestValid 0.1929
	Epoch 3900:	Loss 3.9768	TrainAcc 0.2091	ValidAcc 0.1903	TestAcc 0.2527	BestValid 0.1929
	Epoch 3925:	Loss 3.9728	TrainAcc 0.2084	ValidAcc 0.1889	TestAcc 0.2501	BestValid 0.1929
	Epoch 3950:	Loss 3.9746	TrainAcc 0.2094	ValidAcc 0.1892	TestAcc 0.2520	BestValid 0.1929
	Epoch 3975:	Loss 3.9710	TrainAcc 0.2096	ValidAcc 0.1899	TestAcc 0.2514	BestValid 0.1929
	Epoch 4000:	Loss 3.9715	TrainAcc 0.2100	ValidAcc 0.1896	TestAcc 0.2531	BestValid 0.1929
	Epoch 4025:	Loss 3.9712	TrainAcc 0.2102	ValidAcc 0.1904	TestAcc 0.2532	BestValid 0.1929
	Epoch 4050:	Loss 3.9710	TrainAcc 0.2103	ValidAcc 0.1906	TestAcc 0.2559	BestValid 0.1929
	Epoch 4075:	Loss 3.9671	TrainAcc 0.2102	ValidAcc 0.1895	TestAcc 0.2529	BestValid 0.1929
	Epoch 4100:	Loss 3.9643	TrainAcc 0.2106	ValidAcc 0.1887	TestAcc 0.2537	BestValid 0.1929
	Epoch 4125:	Loss 3.9662	TrainAcc 0.2108	ValidAcc 0.1899	TestAcc 0.2553	BestValid 0.1929
	Epoch 4150:	Loss 3.9675	TrainAcc 0.2110	ValidAcc 0.1893	TestAcc 0.2514	BestValid 0.1929
	Epoch 4175:	Loss 3.9621	TrainAcc 0.2115	ValidAcc 0.1900	TestAcc 0.2544	BestValid 0.1929
	Epoch 4200:	Loss 3.9617	TrainAcc 0.2112	ValidAcc 0.1896	TestAcc 0.2527	BestValid 0.1929
	Epoch 4225:	Loss 3.9644	TrainAcc 0.2112	ValidAcc 0.1890	TestAcc 0.2533	BestValid 0.1929
	Epoch 4250:	Loss 3.9641	TrainAcc 0.2115	ValidAcc 0.1887	TestAcc 0.2540	BestValid 0.1929
	Epoch 4275:	Loss 3.9642	TrainAcc 0.2120	ValidAcc 0.1897	TestAcc 0.2546	BestValid 0.1929
	Epoch 4300:	Loss 3.9617	TrainAcc 0.2122	ValidAcc 0.1894	TestAcc 0.2527	BestValid 0.1929
	Epoch 4325:	Loss 3.9607	TrainAcc 0.2121	ValidAcc 0.1896	TestAcc 0.2551	BestValid 0.1929
	Epoch 4350:	Loss 3.9612	TrainAcc 0.2130	ValidAcc 0.1897	TestAcc 0.2547	BestValid 0.1929
	Epoch 4375:	Loss 3.9591	TrainAcc 0.2128	ValidAcc 0.1891	TestAcc 0.2531	BestValid 0.1929
	Epoch 4400:	Loss 3.9572	TrainAcc 0.2133	ValidAcc 0.1888	TestAcc 0.2544	BestValid 0.1929
	Epoch 4425:	Loss 3.9605	TrainAcc 0.2132	ValidAcc 0.1895	TestAcc 0.2534	BestValid 0.1929
	Epoch 4450:	Loss 3.9614	TrainAcc 0.2135	ValidAcc 0.1891	TestAcc 0.2533	BestValid 0.1929
	Epoch 4475:	Loss 3.9608	TrainAcc 0.2139	ValidAcc 0.1900	TestAcc 0.2542	BestValid 0.1929
	Epoch 4500:	Loss 3.9610	TrainAcc 0.2137	ValidAcc 0.1893	TestAcc 0.2549	BestValid 0.1929
	Epoch 4525:	Loss 3.9564	TrainAcc 0.2143	ValidAcc 0.1898	TestAcc 0.2552	BestValid 0.1929
	Epoch 4550:	Loss 3.9561	TrainAcc 0.2141	ValidAcc 0.1893	TestAcc 0.2550	BestValid 0.1929
	Epoch 4575:	Loss 3.9554	TrainAcc 0.2148	ValidAcc 0.1897	TestAcc 0.2541	BestValid 0.1929
	Epoch 4600:	Loss 3.9555	TrainAcc 0.2145	ValidAcc 0.1884	TestAcc 0.2533	BestValid 0.1929
	Epoch 4625:	Loss 3.9537	TrainAcc 0.2149	ValidAcc 0.1898	TestAcc 0.2564	BestValid 0.1929
	Epoch 4650:	Loss 3.9542	TrainAcc 0.2147	ValidAcc 0.1885	TestAcc 0.2543	BestValid 0.1929
	Epoch 4675:	Loss 3.9536	TrainAcc 0.2151	ValidAcc 0.1893	TestAcc 0.2557	BestValid 0.1929
	Epoch 4700:	Loss 3.9548	TrainAcc 0.2153	ValidAcc 0.1893	TestAcc 0.2556	BestValid 0.1929
	Epoch 4725:	Loss 3.9561	TrainAcc 0.2153	ValidAcc 0.1890	TestAcc 0.2560	BestValid 0.1929
	Epoch 4750:	Loss 3.9525	TrainAcc 0.2155	ValidAcc 0.1887	TestAcc 0.2541	BestValid 0.1929
	Epoch 4775:	Loss 3.9495	TrainAcc 0.2160	ValidAcc 0.1896	TestAcc 0.2562	BestValid 0.1929
	Epoch 4800:	Loss 3.9543	TrainAcc 0.2155	ValidAcc 0.1884	TestAcc 0.2541	BestValid 0.1929
	Epoch 4825:	Loss 3.9498	TrainAcc 0.2157	ValidAcc 0.1893	TestAcc 0.2574	BestValid 0.1929
	Epoch 4850:	Loss 3.9501	TrainAcc 0.2160	ValidAcc 0.1888	TestAcc 0.2539	BestValid 0.1929
	Epoch 4875:	Loss 3.9518	TrainAcc 0.2163	ValidAcc 0.1899	TestAcc 0.2568	BestValid 0.1929
[mpiexec@gnerv1] Sending Ctrl-C to processes as requested
[mpiexec@gnerv1] Press Ctrl-C again to force abort
Ctrl-C caught... cleaning up processes
[proxy:0:0@gnerv1] HYDU_sock_write (../../../../src/pm/hydra/lib/utils/sock.c:250): write error (Broken pipe)
[proxy:0:0@gnerv1] PMIP_send_hdr_upstream (../../../../src/pm/hydra/proxy/pmip_cb.c:34): sock write error
[proxy:0:1@gnerv2] HYD_pmcd_pmip_control_cmd_cb (../../../../src/pm/hydra/proxy/pmip_cb.c:480): assert (!closed) failed
[proxy:0:1@gnerv2] HYDT_dmxu_poll_wait_for_event (../../../../src/pm/hydra/lib/tools/demux/demux_poll.c:76): callback returned error status
[proxy:0:1@gnerv2] main (../../../../src/pm/hydra/proxy/pmip.c:190): demux engine error waiting for event
[mpiexec@gnerv1] HYDT_bscu_wait_for_completion (../../../../src/pm/hydra/lib/tools/bootstrap/utils/bscu_wait.c:109): one of the processes terminated badly; aborting
[mpiexec@gnerv1] HYDT_bsci_wait_for_completion (../../../../src/pm/hydra/lib/tools/bootstrap/src/bsci_wait.c:21): launcher returned error waiting for completion
[mpiexec@gnerv1] HYD_pmci_wait_for_completion (../../../../src/pm/hydra/mpiexec/pmiserv_pmci.c:197): launcher returned error waiting for completion
[mpiexec@gnerv1] main (../../../../src/pm/hydra/mpiexec/mpiexec.c:252): process manager error waiting for completion
