gnerv1
Mon Jul 31 22:13:25 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   33C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   33C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   32C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   30C    P8    23W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 11%] Built target context
[ 36%] Built target core
Scanning dependencies of target cudahelp
[ 38%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 41%] Linking CXX static library libcudahelp.a
[ 77%] Built target cudahelp
[ 80%] Linking CXX executable gcnii
[ 88%] Linking CXX executable estimate_comm_volume
[ 88%] Linking CXX executable gcn
[ 88%] Linking CXX executable graphsage
[ 91%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_graphsage
[ 97%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target OSDI2023_MULTI_NODES_gcn
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.914 seconds.
Building the CSC structure...
        It takes 1.937 seconds.
Building the CSC structure...
        It takes 2.054 seconds.
Building the CSC structure...
        It takes 2.283 seconds.
Building the CSC structure...
        It takes 2.444 seconds.
Building the CSC structure...
        It takes 2.486 seconds.
Building the CSC structure...
        It takes 2.646 seconds.
Building the CSC structure...
        It takes 2.692 seconds.
Building the CSC structure...
        It takes 1.843 seconds.
        It takes 1.845 seconds.
        It takes 1.901 seconds.
        It takes 2.239 seconds.
Building the Feature Vector...
        It takes 2.361 seconds.
        It takes 2.361 seconds.
Building the Feature Vector...
        It takes 2.330 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 2.389 seconds.
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.295 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.034 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Building the Feature Vector...
        It takes 0.309 seconds.
Building the Label Vector...
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.044 seconds.
        It takes 0.035 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.714 Gbps (per GPU), 445.715 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.471 Gbps (per GPU), 443.770 Gbps (aggregated)
The layer-level communication performance: 55.457 Gbps (per GPU), 443.653 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.263 Gbps (per GPU), 442.107 Gbps (aggregated)
The layer-level communication performance: 55.232 Gbps (per GPU), 441.859 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.056 Gbps (per GPU), 440.446 Gbps (aggregated)
The layer-level communication performance: 55.012 Gbps (per GPU), 440.094 Gbps (aggregated)
The layer-level communication performance: 54.984 Gbps (per GPU), 439.874 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 161.254 Gbps (per GPU), 1290.034 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.233 Gbps (per GPU), 1289.864 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.254 Gbps (per GPU), 1290.034 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.229 Gbps (per GPU), 1289.836 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.233 Gbps (per GPU), 1289.861 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.229 Gbps (per GPU), 1289.836 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.223 Gbps (per GPU), 1289.786 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.223 Gbps (per GPU), 1289.786 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.409 Gbps (per GPU), 835.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.409 Gbps (per GPU), 835.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.409 Gbps (per GPU), 835.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.408 Gbps (per GPU), 835.262 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.410 Gbps (per GPU), 835.276 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.425 Gbps (per GPU), 835.401 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.408 Gbps (per GPU), 835.262 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.423 Gbps (per GPU), 835.387 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.087 Gbps (per GPU), 296.698 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.086 Gbps (per GPU), 296.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.087 Gbps (per GPU), 296.694 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.086 Gbps (per GPU), 296.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.085 Gbps (per GPU), 296.684 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.086 Gbps (per GPU), 296.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.083 Gbps (per GPU), 296.666 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.087 Gbps (per GPU), 296.694 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.42ms  2.72ms  3.05  8.38K  3.53M
 chk_1  0.76ms  2.75ms  2.91ms  3.86  6.74K  3.60M
 chk_2  0.80ms  2.63ms  2.80ms  3.51  7.27K  3.53M
 chk_3  0.80ms  2.66ms  2.83ms  3.53  7.92K  3.61M
 chk_4  0.63ms  2.59ms  2.74ms  4.35  5.33K  3.68M
 chk_5  1.00ms  2.59ms  2.78ms  2.77 10.07K  3.45M
 chk_6  0.96ms  2.75ms  2.94ms  3.07  9.41K  3.48M
 chk_7  0.82ms  2.60ms  2.75ms  3.37  8.12K  3.60M
 chk_8  0.68ms  2.69ms  2.86ms  4.20  6.09K  3.64M
 chk_9  1.09ms  2.52ms  2.71ms  2.48 11.10K  3.38M
chk_10  0.65ms  2.73ms  2.91ms  4.47  5.67K  3.63M
chk_11  0.82ms  2.60ms  2.78ms  3.38  8.16K  3.54M
chk_12  0.79ms  2.77ms  2.98ms  3.77  7.24K  3.55M
chk_13  0.64ms  2.64ms  2.80ms  4.39  5.41K  3.68M
chk_14  0.78ms  2.87ms  3.04ms  3.91  7.14K  3.53M
chk_15  0.95ms  2.74ms  2.92ms  3.07  9.25K  3.49M
chk_16  0.60ms  2.56ms  2.70ms  4.50  4.78K  3.77M
chk_17  0.76ms  2.67ms  2.84ms  3.72  6.85K  3.60M
chk_18  0.81ms  2.48ms  2.65ms  3.27  7.47K  3.57M
chk_19  0.61ms  2.53ms  2.69ms  4.44  4.88K  3.75M
chk_20  0.77ms  2.57ms  2.74ms  3.56  7.00K  3.63M
chk_21  0.63ms  2.54ms  2.70ms  4.25  5.41K  3.68M
chk_22  1.10ms  2.81ms  2.99ms  2.73 11.07K  3.39M
chk_23  0.79ms  2.68ms  2.82ms  3.55  7.23K  3.64M
chk_24  1.01ms  2.76ms  2.92ms  2.89 10.13K  3.43M
chk_25  0.73ms  2.53ms  2.70ms  3.70  6.40K  3.57M
chk_26  0.66ms  2.75ms  2.89ms  4.35  5.78K  3.55M
chk_27  0.96ms  2.63ms  2.83ms  2.95  9.34K  3.48M
chk_28  0.73ms  2.92ms  3.09ms  4.24  6.37K  3.57M
chk_29  0.63ms  2.73ms  2.88ms  4.56  5.16K  3.78M
chk_30  0.64ms  2.61ms  2.78ms  4.34  5.44K  3.67M
chk_31  0.73ms  2.74ms  2.93ms  4.03  6.33K  3.63M
   Avg  0.79  2.66  2.83
   Max  1.10  2.92  3.09
   Min  0.60  2.42  2.65
 Ratio  1.82  1.21  1.17
   Var  0.02  0.01  0.01
Profiling takes 2.416 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 365.502 ms
Partition 0 [0, 5) has cost: 365.502 ms
Partition 1 [5, 9) has cost: 340.277 ms
Partition 2 [9, 13) has cost: 340.277 ms
Partition 3 [13, 17) has cost: 340.277 ms
Partition 4 [17, 21) has cost: 340.277 ms
Partition 5 [21, 25) has cost: 340.277 ms
Partition 6 [25, 29) has cost: 340.277 ms
Partition 7 [29, 33) has cost: 345.845 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.820 ms
GPU 0, Compute+Comm Time: 133.439 ms, Bubble Time: 29.103 ms, Imbalance Overhead: 3.278 ms
GPU 1, Compute+Comm Time: 126.370 ms, Bubble Time: 28.796 ms, Imbalance Overhead: 10.655 ms
GPU 2, Compute+Comm Time: 126.370 ms, Bubble Time: 28.789 ms, Imbalance Overhead: 10.661 ms
GPU 3, Compute+Comm Time: 126.370 ms, Bubble Time: 28.692 ms, Imbalance Overhead: 10.758 ms
GPU 4, Compute+Comm Time: 126.370 ms, Bubble Time: 28.585 ms, Imbalance Overhead: 10.865 ms
GPU 5, Compute+Comm Time: 126.370 ms, Bubble Time: 28.604 ms, Imbalance Overhead: 10.847 ms
GPU 6, Compute+Comm Time: 126.370 ms, Bubble Time: 28.846 ms, Imbalance Overhead: 10.604 ms
GPU 7, Compute+Comm Time: 127.566 ms, Bubble Time: 29.227 ms, Imbalance Overhead: 9.028 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 319.173 ms
GPU 0, Compute+Comm Time: 245.040 ms, Bubble Time: 56.664 ms, Imbalance Overhead: 17.469 ms
GPU 1, Compute+Comm Time: 240.668 ms, Bubble Time: 55.895 ms, Imbalance Overhead: 22.610 ms
GPU 2, Compute+Comm Time: 240.668 ms, Bubble Time: 55.334 ms, Imbalance Overhead: 23.171 ms
GPU 3, Compute+Comm Time: 240.668 ms, Bubble Time: 55.281 ms, Imbalance Overhead: 23.224 ms
GPU 4, Compute+Comm Time: 240.668 ms, Bubble Time: 55.424 ms, Imbalance Overhead: 23.081 ms
GPU 5, Compute+Comm Time: 240.668 ms, Bubble Time: 55.583 ms, Imbalance Overhead: 22.922 ms
GPU 6, Compute+Comm Time: 240.668 ms, Bubble Time: 55.474 ms, Imbalance Overhead: 23.031 ms
GPU 7, Compute+Comm Time: 258.824 ms, Bubble Time: 56.059 ms, Imbalance Overhead: 4.290 ms
The estimated cost of the whole pipeline: 509.243 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 705.778 ms
Partition 0 [0, 9) has cost: 705.778 ms
Partition 1 [9, 17) has cost: 680.553 ms
Partition 2 [17, 25) has cost: 680.553 ms
Partition 3 [25, 33) has cost: 686.122 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 171.935 ms
GPU 0, Compute+Comm Time: 142.373 ms, Bubble Time: 26.821 ms, Imbalance Overhead: 2.741 ms
GPU 1, Compute+Comm Time: 138.552 ms, Bubble Time: 26.418 ms, Imbalance Overhead: 6.965 ms
GPU 2, Compute+Comm Time: 138.552 ms, Bubble Time: 26.300 ms, Imbalance Overhead: 7.084 ms
GPU 3, Compute+Comm Time: 139.107 ms, Bubble Time: 26.025 ms, Imbalance Overhead: 6.803 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 318.422 ms
GPU 0, Compute+Comm Time: 257.036 ms, Bubble Time: 48.376 ms, Imbalance Overhead: 13.011 ms
GPU 1, Compute+Comm Time: 254.876 ms, Bubble Time: 48.723 ms, Imbalance Overhead: 14.823 ms
GPU 2, Compute+Comm Time: 254.876 ms, Bubble Time: 48.916 ms, Imbalance Overhead: 14.630 ms
GPU 3, Compute+Comm Time: 264.898 ms, Bubble Time: 49.857 ms, Imbalance Overhead: 3.667 ms
    The estimated cost with 2 DP ways is 514.875 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1386.332 ms
Partition 0 [0, 17) has cost: 1386.332 ms
Partition 1 [17, 33) has cost: 1366.675 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 216.787 ms
GPU 0, Compute+Comm Time: 188.966 ms, Bubble Time: 23.200 ms, Imbalance Overhead: 4.621 ms
GPU 1, Compute+Comm Time: 187.146 ms, Bubble Time: 24.047 ms, Imbalance Overhead: 5.594 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 354.785 ms
GPU 0, Compute+Comm Time: 307.302 ms, Bubble Time: 39.443 ms, Imbalance Overhead: 8.040 ms
GPU 1, Compute+Comm Time: 311.859 ms, Bubble Time: 38.036 ms, Imbalance Overhead: 4.891 ms
    The estimated cost with 4 DP ways is 600.151 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2753.007 ms
Partition 0 [0, 33) has cost: 2753.007 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 523.600 ms
GPU 0, Compute+Comm Time: 523.600 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 647.520 ms
GPU 0, Compute+Comm Time: 647.520 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1229.676 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ************ Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******

****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.4673	TrainAcc 0.4929	ValidAcc 0.5153	TestAcc 0.5111	BestValid 0.5153
	Epoch 50:	Loss 1.8037	TrainAcc 0.6695	ValidAcc 0.6909	TestAcc 0.6856	BestValid 0.6909
	Epoch 75:	Loss 1.4603	TrainAcc 0.7448	ValidAcc 0.7621	TestAcc 0.7559	BestValid 0.7621
	Epoch 100:	Loss 1.2563	TrainAcc 0.7804	ValidAcc 0.7945	TestAcc 0.7893	BestValid 0.7945
	Epoch 125:	Loss 1.1178	TrainAcc 0.8044	ValidAcc 0.8169	TestAcc 0.8116	BestValid 0.8169
	Epoch 150:	Loss 1.0252	TrainAcc 0.8335	ValidAcc 0.8423	TestAcc 0.8379	BestValid 0.8423
	Epoch 175:	Loss 0.9601	TrainAcc 0.8373	ValidAcc 0.8487	TestAcc 0.8423	BestValid 0.8487
	Epoch 200:	Loss 0.9030	TrainAcc 0.8577	ValidAcc 0.8674	TestAcc 0.8605	BestValid 0.8674
	Epoch 225:	Loss 0.8585	TrainAcc 0.8561	ValidAcc 0.8637	TestAcc 0.8597	BestValid 0.8674
	Epoch 250:	Loss 0.8250	TrainAcc 0.8628	ValidAcc 0.8696	TestAcc 0.8648	BestValid 0.8696
	Epoch 275:	Loss 0.8114	TrainAcc 0.8615	ValidAcc 0.8701	TestAcc 0.8627	BestValid 0.8701
	Epoch 300:	Loss 0.8088	TrainAcc 0.8722	ValidAcc 0.8778	TestAcc 0.8723	BestValid 0.8778
	Epoch 325:	Loss 0.7731	TrainAcc 0.8712	ValidAcc 0.8772	TestAcc 0.8715	BestValid 0.8778
	Epoch 350:	Loss 0.7530	TrainAcc 0.8696	ValidAcc 0.8759	TestAcc 0.8711	BestValid 0.8778
	Epoch 375:	Loss 0.7295	TrainAcc 0.8763	ValidAcc 0.8816	TestAcc 0.8752	BestValid 0.8816
	Epoch 400:	Loss 0.7157	TrainAcc 0.8803	ValidAcc 0.8862	TestAcc 0.8805	BestValid 0.8862
	Epoch 425:	Loss 0.7011	TrainAcc 0.8871	ValidAcc 0.8910	TestAcc 0.8863	BestValid 0.8910
	Epoch 450:	Loss 0.6870	TrainAcc 0.8940	ValidAcc 0.9003	TestAcc 0.8947	BestValid 0.9003
	Epoch 475:	Loss 0.6739	TrainAcc 0.8964	ValidAcc 0.9009	TestAcc 0.8951	BestValid 0.9009
	Epoch 500:	Loss 0.6626	TrainAcc 0.8971	ValidAcc 0.9032	TestAcc 0.8977	BestValid 0.9032
	Epoch 525:	Loss 0.6542	TrainAcc 0.8991	ValidAcc 0.9041	TestAcc 0.8983	BestValid 0.9041
	Epoch 550:	Loss 0.6475	TrainAcc 0.8973	ValidAcc 0.9026	TestAcc 0.8972	BestValid 0.9041
	Epoch 575:	Loss 0.6432	TrainAcc 0.8942	ValidAcc 0.8988	TestAcc 0.8938	BestValid 0.9041
	Epoch 600:	Loss 0.6370	TrainAcc 0.8986	ValidAcc 0.9028	TestAcc 0.8973	BestValid 0.9041
	Epoch 625:	Loss 0.6323	TrainAcc 0.9005	ValidAcc 0.9060	TestAcc 0.9005	BestValid 0.9060
	Epoch 650:	Loss 0.6298	TrainAcc 0.9039	ValidAcc 0.9073	TestAcc 0.9018	BestValid 0.9073
	Epoch 675:	Loss 0.6234	TrainAcc 0.8951	ValidAcc 0.8995	TestAcc 0.8960	BestValid 0.9073
	Epoch 700:	Loss 0.6344	TrainAcc 0.9025	ValidAcc 0.9064	TestAcc 0.9012	BestValid 0.9073
	Epoch 725:	Loss 0.6190	TrainAcc 0.9011	ValidAcc 0.9043	TestAcc 0.8996	BestValid 0.9073
	Epoch 750:	Loss 0.6068	TrainAcc 0.9057	ValidAcc 0.9097	TestAcc 0.9054	BestValid 0.9097
	Epoch 775:	Loss 0.5941	TrainAcc 0.9122	ValidAcc 0.9165	TestAcc 0.9106	BestValid 0.9165
	Epoch 800:	Loss 0.6001	TrainAcc 0.8985	ValidAcc 0.9027	TestAcc 0.8989	BestValid 0.9165
	Epoch 825:	Loss 0.6051	TrainAcc 0.9109	ValidAcc 0.9166	TestAcc 0.9092	BestValid 0.9166
	Epoch 850:	Loss 0.5999	TrainAcc 0.9054	ValidAcc 0.9087	TestAcc 0.9036	BestValid 0.9166
	Epoch 875:	Loss 0.5993	TrainAcc 0.9141	ValidAcc 0.9185	TestAcc 0.9128	BestValid 0.9185
	Epoch 900:	Loss 0.5854	TrainAcc 0.9152	ValidAcc 0.9177	TestAcc 0.9129	BestValid 0.9185
	Epoch 925:	Loss 0.5749	TrainAcc 0.9104	ValidAcc 0.9146	TestAcc 0.9097	BestValid 0.9185
	Epoch 950:	Loss 0.5748	TrainAcc 0.9146	ValidAcc 0.9178	TestAcc 0.9127	BestValid 0.9185
	Epoch 975:	Loss 0.5712	TrainAcc 0.9113	ValidAcc 0.9150	TestAcc 0.9102	BestValid 0.9185
	Epoch 1000:	Loss 0.5689	TrainAcc 0.9166	ValidAcc 0.9196	TestAcc 0.9146	BestValid 0.9196
	Epoch 1025:	Loss 0.5645	TrainAcc 0.9144	ValidAcc 0.9186	TestAcc 0.9132	BestValid 0.9196
	Epoch 1050:	Loss 0.5588	TrainAcc 0.9186	ValidAcc 0.9216	TestAcc 0.9167	BestValid 0.9216
	Epoch 1075:	Loss 0.5534	TrainAcc 0.9171	ValidAcc 0.9209	TestAcc 0.9161	BestValid 0.9216
	Epoch 1100:	Loss 0.5535	TrainAcc 0.9191	ValidAcc 0.9219	TestAcc 0.9176	BestValid 0.9219
	Epoch 1125:	Loss 0.5432	TrainAcc 0.9203	ValidAcc 0.9237	TestAcc 0.9189	BestValid 0.9237
	Epoch 1150:	Loss 0.5437	TrainAcc 0.9187	ValidAcc 0.9218	TestAcc 0.9179	BestValid 0.9237
	Epoch 1175:	Loss 0.5396	TrainAcc 0.9219	ValidAcc 0.9253	TestAcc 0.9204	BestValid 0.9253
	Epoch 1200:	Loss 0.5361	TrainAcc 0.9199	ValidAcc 0.9227	TestAcc 0.9189	BestValid 0.9253
	Epoch 1225:	Loss 0.5367	TrainAcc 0.9214	ValidAcc 0.9248	TestAcc 0.9203	BestValid 0.9253
	Epoch 1250:	Loss 0.5353	TrainAcc 0.9224	ValidAcc 0.9240	TestAcc 0.9204	BestValid 0.9253
	Epoch 1275:	Loss 0.5311	TrainAcc 0.9183	ValidAcc 0.9220	TestAcc 0.9179	BestValid 0.9253
	Epoch 1300:	Loss 0.5332	TrainAcc 0.9225	ValidAcc 0.9235	TestAcc 0.9198	BestValid 0.9253
	Epoch 1325:	Loss 0.5310	TrainAcc 0.9179	ValidAcc 0.9212	TestAcc 0.9170	BestValid 0.9253
	Epoch 1350:	Loss 0.5301	TrainAcc 0.9205	ValidAcc 0.9214	TestAcc 0.9184	BestValid 0.9253
	Epoch 1375:	Loss 0.5293	TrainAcc 0.9201	ValidAcc 0.9227	TestAcc 0.9177	BestValid 0.9253
	Epoch 1400:	Loss 0.5228	TrainAcc 0.9204	ValidAcc 0.9214	TestAcc 0.9185	BestValid 0.9253
	Epoch 1425:	Loss 0.5220	TrainAcc 0.9225	ValidAcc 0.9253	TestAcc 0.9204	BestValid 0.9253
	Epoch 1450:	Loss 0.5217	TrainAcc 0.9218	ValidAcc 0.9232	TestAcc 0.9205	BestValid 0.9253
	Epoch 1475:	Loss 0.5212	TrainAcc 0.9233	ValidAcc 0.9259	TestAcc 0.9220	BestValid 0.9259
	Epoch 1500:	Loss 0.5112	TrainAcc 0.9234	ValidAcc 0.9252	TestAcc 0.9226	BestValid 0.9259
	Epoch 1525:	Loss 0.5076	TrainAcc 0.9251	ValidAcc 0.9275	TestAcc 0.9236	BestValid 0.9275
	Epoch 1550:	Loss 0.5036	TrainAcc 0.9268	ValidAcc 0.9285	TestAcc 0.9250	BestValid 0.9285
	Epoch 1575:	Loss 0.5036	TrainAcc 0.9212	ValidAcc 0.9228	TestAcc 0.9188	BestValid 0.9285
	Epoch 1600:	Loss 0.5142	TrainAcc 0.9217	ValidAcc 0.9238	TestAcc 0.9193	BestValid 0.9285
	Epoch 1625:	Loss 0.5278	TrainAcc 0.9137	ValidAcc 0.9147	TestAcc 0.9114	BestValid 0.9285
	Epoch 1650:	Loss 0.5275	TrainAcc 0.9209	ValidAcc 0.9238	TestAcc 0.9190	BestValid 0.9285
	Epoch 1675:	Loss 0.5248	TrainAcc 0.9192	ValidAcc 0.9201	TestAcc 0.9157	BestValid 0.9285
	Epoch 1700:	Loss 0.5168	TrainAcc 0.9216	ValidAcc 0.9249	TestAcc 0.9211	BestValid 0.9285
	Epoch 1725:	Loss 0.5039	TrainAcc 0.9229	ValidAcc 0.9239	TestAcc 0.9194	BestValid 0.9285
	Epoch 1750:	Loss 0.5046	TrainAcc 0.9223	ValidAcc 0.9253	TestAcc 0.9219	BestValid 0.9285
	Epoch 1775:	Loss 0.5071	TrainAcc 0.9253	ValidAcc 0.9259	TestAcc 0.9221	BestValid 0.9285
	Epoch 1800:	Loss 0.4960	TrainAcc 0.9247	ValidAcc 0.9285	TestAcc 0.9245	BestValid 0.9285
	Epoch 1825:	Loss 0.4967	TrainAcc 0.9266	ValidAcc 0.9273	TestAcc 0.9239	BestValid 0.9285
	Epoch 1850:	Loss 0.4914	TrainAcc 0.9273	ValidAcc 0.9300	TestAcc 0.9266	BestValid 0.9300
	Epoch 1875:	Loss 0.4959	TrainAcc 0.9268	ValidAcc 0.9270	TestAcc 0.9242	BestValid 0.9300
	Epoch 1900:	Loss 0.4945	TrainAcc 0.9288	ValidAcc 0.9314	TestAcc 0.9271	BestValid 0.9314
	Epoch 1925:	Loss 0.4955	TrainAcc 0.9276	ValidAcc 0.9284	TestAcc 0.9254	BestValid 0.9314
	Epoch 1950:	Loss 0.4978	TrainAcc 0.9281	ValidAcc 0.9295	TestAcc 0.9263	BestValid 0.9314
	Epoch 1975:	Loss 0.4913	TrainAcc 0.9291	ValidAcc 0.9302	TestAcc 0.9273	BestValid 0.9314
	Epoch 2000:	Loss 0.4921	TrainAcc 0.9280	ValidAcc 0.9293	TestAcc 0.9266	BestValid 0.9314
	Epoch 2025:	Loss 0.4847	TrainAcc 0.9305	ValidAcc 0.9315	TestAcc 0.9281	BestValid 0.9315
	Epoch 2050:	Loss 0.4802	TrainAcc 0.9295	ValidAcc 0.9302	TestAcc 0.9267	BestValid 0.9315
	Epoch 2075:	Loss 0.4820	TrainAcc 0.9310	ValidAcc 0.9326	TestAcc 0.9287	BestValid 0.9326
	Epoch 2100:	Loss 0.4780	TrainAcc 0.9294	ValidAcc 0.9298	TestAcc 0.9264	BestValid 0.9326
	Epoch 2125:	Loss 0.4759	TrainAcc 0.9311	ValidAcc 0.9320	TestAcc 0.9283	BestValid 0.9326
	Epoch 2150:	Loss 0.4754	TrainAcc 0.9295	ValidAcc 0.9294	TestAcc 0.9268	BestValid 0.9326
	Epoch 2175:	Loss 0.4761	TrainAcc 0.9308	ValidAcc 0.9304	TestAcc 0.9280	BestValid 0.9326
	Epoch 2200:	Loss 0.4826	TrainAcc 0.9293	ValidAcc 0.9304	TestAcc 0.9273	BestValid 0.9326
	Epoch 2225:	Loss 0.4727	TrainAcc 0.9296	ValidAcc 0.9298	TestAcc 0.9265	BestValid 0.9326
	Epoch 2250:	Loss 0.4723	TrainAcc 0.9302	ValidAcc 0.9319	TestAcc 0.9283	BestValid 0.9326
	Epoch 2275:	Loss 0.4758	TrainAcc 0.9297	ValidAcc 0.9300	TestAcc 0.9271	BestValid 0.9326
	Epoch 2300:	Loss 0.4735	TrainAcc 0.9316	ValidAcc 0.9330	TestAcc 0.9296	BestValid 0.9330
	Epoch 2325:	Loss 0.4718	TrainAcc 0.9315	ValidAcc 0.9307	TestAcc 0.9282	BestValid 0.9330
	Epoch 2350:	Loss 0.4721	TrainAcc 0.9276	ValidAcc 0.9287	TestAcc 0.9257	BestValid 0.9330
	Epoch 2375:	Loss 0.4852	TrainAcc 0.9285	ValidAcc 0.9277	TestAcc 0.9241	BestValid 0.9330
	Epoch 2400:	Loss 0.4834	TrainAcc 0.9240	ValidAcc 0.9245	TestAcc 0.9222	BestValid 0.9330
	Epoch 2425:	Loss 0.4839	TrainAcc 0.9275	ValidAcc 0.9264	TestAcc 0.9232	BestValid 0.9330
	Epoch 2450:	Loss 0.4696	TrainAcc 0.9266	ValidAcc 0.9287	TestAcc 0.9246	BestValid 0.9330
	Epoch 2475:	Loss 0.4751	TrainAcc 0.9226	ValidAcc 0.9224	TestAcc 0.9199	BestValid 0.9330
	Epoch 2500:	Loss 0.4678	TrainAcc 0.9306	ValidAcc 0.9327	TestAcc 0.9279	BestValid 0.9330
	Epoch 2525:	Loss 0.4749	TrainAcc 0.9244	ValidAcc 0.9249	TestAcc 0.9225	BestValid 0.9330
	Epoch 2550:	Loss 0.4779	TrainAcc 0.9339	ValidAcc 0.9354	TestAcc 0.9303	BestValid 0.9354
	Epoch 2575:	Loss 0.4800	TrainAcc 0.9302	ValidAcc 0.9317	TestAcc 0.9281	BestValid 0.9354
	Epoch 2600:	Loss 0.4790	TrainAcc 0.9332	ValidAcc 0.9323	TestAcc 0.9293	BestValid 0.9354
	Epoch 2625:	Loss 0.4794	TrainAcc 0.9318	ValidAcc 0.9339	TestAcc 0.9302	BestValid 0.9354
	Epoch 2650:	Loss 0.4733	TrainAcc 0.9301	ValidAcc 0.9297	TestAcc 0.9261	BestValid 0.9354
	Epoch 2675:	Loss 0.4594	TrainAcc 0.9343	ValidAcc 0.9367	TestAcc 0.9320	BestValid 0.9367
	Epoch 2700:	Loss 0.4574	TrainAcc 0.9307	ValidAcc 0.9312	TestAcc 0.9273	BestValid 0.9367
	Epoch 2725:	Loss 0.4620	TrainAcc 0.9355	ValidAcc 0.9356	TestAcc 0.9317	BestValid 0.9367
	Epoch 2750:	Loss 0.4604	TrainAcc 0.9306	ValidAcc 0.9316	TestAcc 0.9275	BestValid 0.9367
	Epoch 2775:	Loss 0.4666	TrainAcc 0.9316	ValidAcc 0.9310	TestAcc 0.9276	BestValid 0.9367
	Epoch 2800:	Loss 0.4669	TrainAcc 0.9284	ValidAcc 0.9286	TestAcc 0.9255	BestValid 0.9367
	Epoch 2825:	Loss 0.4672	TrainAcc 0.9291	ValidAcc 0.9283	TestAcc 0.9252	BestValid 0.9367
	Epoch 2850:	Loss 0.4616	TrainAcc 0.9302	ValidAcc 0.9300	TestAcc 0.9267	BestValid 0.9367
	Epoch 2875:	Loss 0.4584	TrainAcc 0.9313	ValidAcc 0.9312	TestAcc 0.9282	BestValid 0.9367
	Epoch 2900:	Loss 0.4501	TrainAcc 0.9338	ValidAcc 0.9342	TestAcc 0.9306	BestValid 0.9367
	Epoch 2925:	Loss 0.4488	TrainAcc 0.9341	ValidAcc 0.9340	TestAcc 0.9314	BestValid 0.9367
	Epoch 2950:	Loss 0.4512	TrainAcc 0.9366	ValidAcc 0.9370	TestAcc 0.9334	BestValid 0.9370
	Epoch 2975:	Loss 0.4458	TrainAcc 0.9353	ValidAcc 0.9357	TestAcc 0.9329	BestValid 0.9370
	Epoch 3000:	Loss 0.4475	TrainAcc 0.9372	ValidAcc 0.9374	TestAcc 0.9338	BestValid 0.9374
	Epoch 3025:	Loss 0.4513	TrainAcc 0.9363	ValidAcc 0.9360	TestAcc 0.9339	BestValid 0.9374
	Epoch 3050:	Loss 0.4543	TrainAcc 0.9367	ValidAcc 0.9378	TestAcc 0.9329	BestValid 0.9378
	Epoch 3075:	Loss 0.4592	TrainAcc 0.9357	ValidAcc 0.9363	TestAcc 0.9326	BestValid 0.9378
	Epoch 3100:	Loss 0.4541	TrainAcc 0.9325	ValidAcc 0.9322	TestAcc 0.9290	BestValid 0.9378
	Epoch 3125:	Loss 0.4588	TrainAcc 0.9317	ValidAcc 0.9323	TestAcc 0.9285	BestValid 0.9378
	Epoch 3150:	Loss 0.4571	TrainAcc 0.9299	ValidAcc 0.9303	TestAcc 0.9272	BestValid 0.9378
	Epoch 3175:	Loss 0.4520	TrainAcc 0.9343	ValidAcc 0.9351	TestAcc 0.9299	BestValid 0.9378
	Epoch 3200:	Loss 0.4559	TrainAcc 0.9342	ValidAcc 0.9350	TestAcc 0.9319	BestValid 0.9378
	Epoch 3225:	Loss 0.4484	TrainAcc 0.9367	ValidAcc 0.9371	TestAcc 0.9331	BestValid 0.9378
	Epoch 3250:	Loss 0.4403	TrainAcc 0.9368	ValidAcc 0.9368	TestAcc 0.9346	BestValid 0.9378
	Epoch 3275:	Loss 0.4349	TrainAcc 0.9378	ValidAcc 0.9386	TestAcc 0.9343	BestValid 0.9386
	Epoch 3300:	Loss 0.4309	TrainAcc 0.9386	ValidAcc 0.9385	TestAcc 0.9357	BestValid 0.9386
	Epoch 3325:	Loss 0.4360	TrainAcc 0.9384	ValidAcc 0.9387	TestAcc 0.9356	BestValid 0.9387
	Epoch 3350:	Loss 0.4310	TrainAcc 0.9388	ValidAcc 0.9391	TestAcc 0.9356	BestValid 0.9391
	Epoch 3375:	Loss 0.4367	TrainAcc 0.9371	ValidAcc 0.9375	TestAcc 0.9348	BestValid 0.9391
	Epoch 3400:	Loss 0.4452	TrainAcc 0.9382	ValidAcc 0.9381	TestAcc 0.9341	BestValid 0.9391
	Epoch 3425:	Loss 0.4436	TrainAcc 0.9349	ValidAcc 0.9355	TestAcc 0.9334	BestValid 0.9391
	Epoch 3450:	Loss 0.4489	TrainAcc 0.9366	ValidAcc 0.9354	TestAcc 0.9318	BestValid 0.9391
	Epoch 3475:	Loss 0.4476	TrainAcc 0.9348	ValidAcc 0.9359	TestAcc 0.9328	BestValid 0.9391
	Epoch 3500:	Loss 0.4478	TrainAcc 0.9359	ValidAcc 0.9346	TestAcc 0.9315	BestValid 0.9391
	Epoch 3525:	Loss 0.4341	TrainAcc 0.9381	ValidAcc 0.9382	TestAcc 0.9351	BestValid 0.9391
	Epoch 3550:	Loss 0.4389	TrainAcc 0.9358	ValidAcc 0.9355	TestAcc 0.9320	BestValid 0.9391
	Epoch 3575:	Loss 0.4412	TrainAcc 0.9400	ValidAcc 0.9399	TestAcc 0.9356	BestValid 0.9399
	Epoch 3600:	Loss 0.4434	TrainAcc 0.9372	ValidAcc 0.9370	TestAcc 0.9342	BestValid 0.9399
	Epoch 3625:	Loss 0.4449	TrainAcc 0.9384	ValidAcc 0.9371	TestAcc 0.9343	BestValid 0.9399
	Epoch 3650:	Loss 0.4485	TrainAcc 0.9363	ValidAcc 0.9363	TestAcc 0.9333	BestValid 0.9399
	Epoch 3675:	Loss 0.4395	TrainAcc 0.9375	ValidAcc 0.9371	TestAcc 0.9335	BestValid 0.9399
	Epoch 3700:	Loss 0.4380	TrainAcc 0.9356	ValidAcc 0.9351	TestAcc 0.9319	BestValid 0.9399
	Epoch 3725:	Loss 0.4335	TrainAcc 0.9383	ValidAcc 0.9373	TestAcc 0.9340	BestValid 0.9399
	Epoch 3750:	Loss 0.4318	TrainAcc 0.9358	ValidAcc 0.9353	TestAcc 0.9321	BestValid 0.9399
	Epoch 3775:	Loss 0.4330	TrainAcc 0.9375	ValidAcc 0.9379	TestAcc 0.9347	BestValid 0.9399
	Epoch 3800:	Loss 0.4328	TrainAcc 0.9396	ValidAcc 0.9390	TestAcc 0.9349	BestValid 0.9399
	Epoch 3825:	Loss 0.4311	TrainAcc 0.9383	ValidAcc 0.9383	TestAcc 0.9355	BestValid 0.9399
	Epoch 3850:	Loss 0.4247	TrainAcc 0.9403	ValidAcc 0.9399	TestAcc 0.9359	BestValid 0.9399
	Epoch 3875:	Loss 0.4247	TrainAcc 0.9409	ValidAcc 0.9407	TestAcc 0.9376	BestValid 0.9407
	Epoch 3900:	Loss 0.4290	TrainAcc 0.9341	ValidAcc 0.9338	TestAcc 0.9292	BestValid 0.9407
	Epoch 3925:	Loss 0.4352	TrainAcc 0.9363	ValidAcc 0.9357	TestAcc 0.9318	BestValid 0.9407
	Epoch 3950:	Loss 0.4567	TrainAcc 0.9224	ValidAcc 0.9214	TestAcc 0.9178	BestValid 0.9407
	Epoch 3975:	Loss 0.4570	TrainAcc 0.9330	ValidAcc 0.9332	TestAcc 0.9279	BestValid 0.9407
	Epoch 4000:	Loss 0.4460	TrainAcc 0.9318	ValidAcc 0.9321	TestAcc 0.9275	BestValid 0.9407
	Epoch 4025:	Loss 0.4435	TrainAcc 0.9335	ValidAcc 0.9319	TestAcc 0.9283	BestValid 0.9407
	Epoch 4050:	Loss 0.4286	TrainAcc 0.9368	ValidAcc 0.9365	TestAcc 0.9341	BestValid 0.9407
	Epoch 4075:	Loss 0.4229	TrainAcc 0.9368	ValidAcc 0.9357	TestAcc 0.9326	BestValid 0.9407
	Epoch 4100:	Loss 0.4222	TrainAcc 0.9407	ValidAcc 0.9399	TestAcc 0.9376	BestValid 0.9407
	Epoch 4125:	Loss 0.4151	TrainAcc 0.9377	ValidAcc 0.9369	TestAcc 0.9343	BestValid 0.9407
	Epoch 4150:	Loss 0.4227	TrainAcc 0.9421	ValidAcc 0.9414	TestAcc 0.9377	BestValid 0.9414
	Epoch 4175:	Loss 0.4244	TrainAcc 0.9360	ValidAcc 0.9358	TestAcc 0.9328	BestValid 0.9414
	Epoch 4200:	Loss 0.4242	TrainAcc 0.9407	ValidAcc 0.9405	TestAcc 0.9357	BestValid 0.9414
	Epoch 4225:	Loss 0.4274	TrainAcc 0.9335	ValidAcc 0.9335	TestAcc 0.9298	BestValid 0.9414
	Epoch 4250:	Loss 0.4285	TrainAcc 0.9391	ValidAcc 0.9387	TestAcc 0.9346	BestValid 0.9414
	Epoch 4275:	Loss 0.4300	TrainAcc 0.9338	ValidAcc 0.9329	TestAcc 0.9289	BestValid 0.9414
	Epoch 4300:	Loss 0.4292	TrainAcc 0.9386	ValidAcc 0.9383	TestAcc 0.9347	BestValid 0.9414
	Epoch 4325:	Loss 0.4215	TrainAcc 0.9383	ValidAcc 0.9381	TestAcc 0.9331	BestValid 0.9414
	Epoch 4350:	Loss 0.4253	TrainAcc 0.9398	ValidAcc 0.9393	TestAcc 0.9361	BestValid 0.9414
	Epoch 4375:	Loss 0.4241	TrainAcc 0.9411	ValidAcc 0.9399	TestAcc 0.9368	BestValid 0.9414
	Epoch 4400:	Loss 0.4162	TrainAcc 0.9389	ValidAcc 0.9371	TestAcc 0.9354	BestValid 0.9414
	Epoch 4425:	Loss 0.4170	TrainAcc 0.9415	ValidAcc 0.9413	TestAcc 0.9375	BestValid 0.9414
	Epoch 4450:	Loss 0.4235	TrainAcc 0.9346	ValidAcc 0.9332	TestAcc 0.9298	BestValid 0.9414
	Epoch 4475:	Loss 0.4178	TrainAcc 0.9414	ValidAcc 0.9408	TestAcc 0.9368	BestValid 0.9414
	Epoch 4500:	Loss 0.4299	TrainAcc 0.9328	ValidAcc 0.9322	TestAcc 0.9282	BestValid 0.9414
	Epoch 4525:	Loss 0.4275	TrainAcc 0.9407	ValidAcc 0.9400	TestAcc 0.9363	BestValid 0.9414
	Epoch 4550:	Loss 0.4281	TrainAcc 0.9386	ValidAcc 0.9381	TestAcc 0.9341	BestValid 0.9414
	Epoch 4575:	Loss 0.4221	TrainAcc 0.9402	ValidAcc 0.9397	TestAcc 0.9361	BestValid 0.9414
	Epoch 4600:	Loss 0.4164	TrainAcc 0.9411	ValidAcc 0.9408	TestAcc 0.9377	BestValid 0.9414
	Epoch 4625:	Loss 0.4071	TrainAcc 0.9403	ValidAcc 0.9385	TestAcc 0.9356	BestValid 0.9414
	Epoch 4650:	Loss 0.4071	TrainAcc 0.9415	ValidAcc 0.9413	TestAcc 0.9381	BestValid 0.9414
	Epoch 4675:	Loss 0.4120	TrainAcc 0.9404	ValidAcc 0.9387	TestAcc 0.9360	BestValid 0.9414
	Epoch 4700:	Loss 0.4081	TrainAcc 0.9421	ValidAcc 0.9416	TestAcc 0.9384	BestValid 0.9416
	Epoch 4725:	Loss 0.4084	TrainAcc 0.9402	ValidAcc 0.9392	TestAcc 0.9362	BestValid 0.9416
	Epoch 4750:	Loss 0.4157	TrainAcc 0.9437	ValidAcc 0.9428	TestAcc 0.9394	BestValid 0.9428
	Epoch 4775:	Loss 0.4089	TrainAcc 0.9394	ValidAcc 0.9390	TestAcc 0.9354	BestValid 0.9428
	Epoch 4800:	Loss 0.4144	TrainAcc 0.9435	ValidAcc 0.9423	TestAcc 0.9388	BestValid 0.9428
	Epoch 4825:	Loss 0.4227	TrainAcc 0.9360	ValidAcc 0.9358	TestAcc 0.9313	BestValid 0.9428
	Epoch 4850:	Loss 0.4350	TrainAcc 0.9370	ValidAcc 0.9356	TestAcc 0.9317	BestValid 0.9428
	Epoch 4875:	Loss 0.4423	TrainAcc 0.9334	ValidAcc 0.9333	TestAcc 0.9284	BestValid 0.9428
	Epoch 4900:	Loss 0.4286	TrainAcc 0.9333	ValidAcc 0.9322	TestAcc 0.9282	BestValid 0.9428
	Epoch 4925:	Loss 0.4312	TrainAcc 0.9331	ValidAcc 0.9327	TestAcc 0.9280	BestValid 0.9428
	Epoch 4950:	Loss 0.4319	TrainAcc 0.9386	ValidAcc 0.9387	TestAcc 0.9355	BestValid 0.9428
	Epoch 4975:	Loss 0.4182	TrainAcc 0.9363	ValidAcc 0.9347	TestAcc 0.9309	BestValid 0.9428
	Epoch 5000:	Loss 0.4133	TrainAcc 0.9412	ValidAcc 0.9410	TestAcc 0.9386	BestValid 0.9428
Node 1, Pre/Post-Pipelining: 0.265 / 1.389 ms, Bubble: 81.218 ms, Compute: 254.610 ms, Comm: 42.570 ms, Imbalance: 35.278 ms
Node 0, Pre/Post-Pipelining: 0.145 / 1.502 ms, Bubble: 81.089 ms, Compute: 288.171 ms, Comm: 32.699 ms, Imbalance: 11.005 ms
Node 2, Pre/Post-Pipelining: 0.232 / 1.406 ms, Bubble: 81.953 ms, Compute: 249.227 ms, Comm: 47.535 ms, Imbalance: 35.538 ms
Node 5, Pre/Post-Pipelining: 0.270 / 1.413 ms, Bubble: 82.012 ms, Compute: 254.693 ms, Comm: 45.349 ms, Imbalance: 32.082 ms
Node 6, Pre/Post-Pipelining: 0.258 / 1.422 ms, Bubble: 82.240 ms, Compute: 255.762 ms, Comm: 40.639 ms, Imbalance: 35.581 ms
Node 7, Pre/Post-Pipelining: 0.215 / 16.867 ms, Bubble: 67.143 ms, Compute: 281.488 ms, Comm: 32.152 ms, Imbalance: 17.488 ms
Node 3, Pre/Post-Pipelining: 0.219 / 1.414 ms, Bubble: 80.913 ms, Compute: 256.458 ms, Comm: 47.390 ms, Imbalance: 29.140 ms
Node 4, Pre/Post-Pipelining: 0.216 / 1.408 ms, Bubble: 81.670 ms, Compute: 250.274 ms, Comm: 46.740 ms, Imbalance: 35.796 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.145 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.502 ms
Cluster-Wide Average, Bubble: 81.089 ms
Cluster-Wide Average, Compute: 288.171 ms
Cluster-Wide Average, Communication: 32.699 ms
Cluster-Wide Average, Imbalance: 11.005 ms
Node 0, GPU memory consumption: 8.059 GB
Node 1, GPU memory consumption: 6.042 GB
Node 2, GPU memory consumption: 6.042 GB
Node 3, GPU memory consumption: 6.018 GB
Node 5, GPU memory consumption: 6.042 GB
Node 4, GPU memory consumption: 6.018 GB
Node 6, GPU memory consumption: 6.042 GB
Node 7, GPU memory consumption: 6.171 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.911499 s---------------
------------------------node id 1,  per-epoch time: 0.911499 s---------------
------------------------node id 2,  per-epoch time: 0.911499 s---------------
------------------------node id 3,  per-epoch time: 0.911499 s---------------
------------------------node id 4,  per-epoch time: 0.911498 s---------------
------------------------node id 5,  per-epoch time: 0.911498 s---------------
------------------------node id 6,  per-epoch time: 0.911498 s---------------
------------------------node id 7,  per-epoch time: 0.911498 s---------------
************ Profiling Results ************
	Bubble: 650.510372 (ms) (71.40 percentage)
	Compute: 259.511815 (ms) (28.48 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 1.100011 (ms) (0.12 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 2.431 GB
	Aggregated layer-level communication throughput: 498.363 Gbps
Highest valid_acc: 0.9428
Target test_acc: 0.9394
Epoch to reach the target acc: 4749
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
