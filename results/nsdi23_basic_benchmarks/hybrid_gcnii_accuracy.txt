gnerv1
Tue Aug  1 14:16:59 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   34C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   37C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   35C    P8    21W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   33C    P8    23W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 22%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 91%] Built target estimate_comm_volume
[ 91%] Built target OSDI2023_MULTI_NODES_gcn
[ 91%] Built target OSDI2023_MULTI_NODES_gcnii
[ 91%] Built target OSDI2023_MULTI_NODES_graphsage
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.870 seconds.
Building the CSC structure...
        It takes 2.034 seconds.
Building the CSC structure...
        It takes 2.065 seconds.
Building the CSC structure...
        It takes 2.244 seconds.
Building the CSC structure...
        It takes 2.447 seconds.
Building the CSC structure...
        It takes 2.452 seconds.
Building the CSC structure...
        It takes 2.651 seconds.
Building the CSC structure...
        It takes 2.715 seconds.
Building the CSC structure...
        It takes 1.814 seconds.
        It takes 1.846 seconds.
        It takes 1.838 seconds.
        It takes 2.398 seconds.
Building the Feature Vector...
        It takes 2.322 seconds.
        It takes 2.345 seconds.
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 2.295 seconds.
        It takes 0.037 seconds.
        It takes 2.458 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.309 seconds.
Building the Label Vector...
        It takes 0.278 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.042 seconds.
        It takes 0.298 seconds.
Building the Label Vector...
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.038 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.292 seconds.
Building the Label Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.032 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.556 Gbps (per GPU), 444.445 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.310 Gbps (per GPU), 442.481 Gbps (aggregated)
The layer-level communication performance: 55.307 Gbps (per GPU), 442.457 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.082 Gbps (per GPU), 440.657 Gbps (aggregated)
The layer-level communication performance: 55.058 Gbps (per GPU), 440.461 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.891 Gbps (per GPU), 439.131 Gbps (aggregated)
The layer-level communication performance: 54.846 Gbps (per GPU), 438.764 Gbps (aggregated)
The layer-level communication performance: 54.823 Gbps (per GPU), 438.588 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.842 Gbps (per GPU), 1270.736 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.909 Gbps (per GPU), 1271.269 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.842 Gbps (per GPU), 1270.736 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.914 Gbps (per GPU), 1271.314 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.848 Gbps (per GPU), 1270.785 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.854 Gbps (per GPU), 1270.833 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.843 Gbps (per GPU), 1270.741 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.851 Gbps (per GPU), 1270.810 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.442 Gbps (per GPU), 827.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.442 Gbps (per GPU), 827.537 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.444 Gbps (per GPU), 827.551 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.440 Gbps (per GPU), 827.517 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.442 Gbps (per GPU), 827.537 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.474 Gbps (per GPU), 827.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.440 Gbps (per GPU), 827.517 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.470 Gbps (per GPU), 827.761 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.317 Gbps (per GPU), 258.533 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.317 Gbps (per GPU), 258.537 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.316 Gbps (per GPU), 258.528 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.316 Gbps (per GPU), 258.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.316 Gbps (per GPU), 258.529 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.316 Gbps (per GPU), 258.531 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.317 Gbps (per GPU), 258.534 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.316 Gbps (per GPU), 258.531 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.40ms  2.75ms  3.09  8.38K  3.53M
 chk_1  0.76ms  2.78ms  2.93ms  3.85  6.74K  3.60M
 chk_2  0.79ms  2.62ms  2.80ms  3.53  7.27K  3.53M
 chk_3  0.80ms  2.68ms  2.84ms  3.54  7.92K  3.61M
 chk_4  0.63ms  2.60ms  3.01ms  4.79  5.33K  3.68M
 chk_5  1.00ms  2.60ms  2.75ms  2.75 10.07K  3.45M
 chk_6  0.98ms  2.76ms  2.95ms  3.01  9.41K  3.48M
 chk_7  0.82ms  2.60ms  2.77ms  3.40  8.12K  3.60M
 chk_8  0.68ms  2.71ms  2.84ms  4.19  6.09K  3.64M
 chk_9  1.09ms  2.52ms  2.71ms  2.48 11.10K  3.38M
chk_10  0.65ms  2.76ms  2.88ms  4.44  5.67K  3.63M
chk_11  0.82ms  2.60ms  2.99ms  3.65  8.16K  3.54M
chk_12  0.79ms  2.81ms  2.96ms  3.73  7.24K  3.55M
chk_13  0.63ms  2.63ms  2.78ms  4.39  5.41K  3.68M
chk_14  0.78ms  2.89ms  3.28ms  4.21  7.14K  3.53M
chk_15  0.95ms  2.73ms  2.90ms  3.06  9.25K  3.49M
chk_16  0.62ms  2.55ms  2.69ms  4.35  4.78K  3.77M
chk_17  0.76ms  2.69ms  2.85ms  3.74  6.85K  3.60M
chk_18  0.81ms  2.49ms  2.66ms  3.29  7.47K  3.57M
chk_19  0.61ms  2.55ms  2.69ms  4.44  4.88K  3.75M
chk_20  0.77ms  2.56ms  2.72ms  3.54  7.00K  3.63M
chk_21  0.63ms  2.56ms  2.93ms  4.63  5.41K  3.68M
chk_22  1.09ms  2.78ms  2.95ms  2.70 11.07K  3.39M
chk_23  0.79ms  2.67ms  2.80ms  3.54  7.23K  3.64M
chk_24  1.01ms  2.72ms  3.12ms  3.10 10.13K  3.43M
chk_25  0.74ms  2.56ms  2.66ms  3.57  6.40K  3.57M
chk_26  0.66ms  2.75ms  2.87ms  4.36  5.78K  3.55M
chk_27  0.95ms  2.64ms  2.83ms  2.98  9.34K  3.48M
chk_28  0.72ms  2.92ms  3.07ms  4.23  6.37K  3.57M
chk_29  0.63ms  2.72ms  2.84ms  4.52  5.16K  3.78M
chk_30  0.64ms  2.59ms  2.78ms  4.38  5.44K  3.67M
chk_31  0.72ms  2.74ms  2.91ms  4.03  6.33K  3.63M
   Avg  0.79  2.66  2.86
   Max  1.09  2.92  3.28
   Min  0.61  2.40  2.66
 Ratio  1.80  1.21  1.23
   Var  0.02  0.01  0.02
Profiling takes 2.437 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 365.887 ms
Partition 0 [0, 5) has cost: 365.887 ms
Partition 1 [5, 9) has cost: 340.670 ms
Partition 2 [9, 13) has cost: 340.670 ms
Partition 3 [13, 17) has cost: 340.670 ms
Partition 4 [17, 21) has cost: 340.670 ms
Partition 5 [21, 25) has cost: 340.670 ms
Partition 6 [25, 29) has cost: 340.670 ms
Partition 7 [29, 33) has cost: 347.029 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.992 ms
GPU 0, Compute+Comm Time: 133.571 ms, Bubble Time: 29.056 ms, Imbalance Overhead: 3.366 ms
GPU 1, Compute+Comm Time: 126.516 ms, Bubble Time: 28.741 ms, Imbalance Overhead: 10.735 ms
GPU 2, Compute+Comm Time: 126.516 ms, Bubble Time: 28.793 ms, Imbalance Overhead: 10.683 ms
GPU 3, Compute+Comm Time: 126.516 ms, Bubble Time: 28.668 ms, Imbalance Overhead: 10.807 ms
GPU 4, Compute+Comm Time: 126.516 ms, Bubble Time: 28.618 ms, Imbalance Overhead: 10.857 ms
GPU 5, Compute+Comm Time: 126.516 ms, Bubble Time: 28.695 ms, Imbalance Overhead: 10.781 ms
GPU 6, Compute+Comm Time: 126.516 ms, Bubble Time: 28.961 ms, Imbalance Overhead: 10.514 ms
GPU 7, Compute+Comm Time: 127.623 ms, Bubble Time: 29.361 ms, Imbalance Overhead: 9.008 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 319.683 ms
GPU 0, Compute+Comm Time: 246.243 ms, Bubble Time: 56.876 ms, Imbalance Overhead: 16.565 ms
GPU 1, Compute+Comm Time: 240.991 ms, Bubble Time: 56.038 ms, Imbalance Overhead: 22.654 ms
GPU 2, Compute+Comm Time: 240.991 ms, Bubble Time: 55.430 ms, Imbalance Overhead: 23.262 ms
GPU 3, Compute+Comm Time: 240.991 ms, Bubble Time: 55.307 ms, Imbalance Overhead: 23.385 ms
GPU 4, Compute+Comm Time: 240.991 ms, Bubble Time: 55.379 ms, Imbalance Overhead: 23.313 ms
GPU 5, Compute+Comm Time: 240.991 ms, Bubble Time: 55.544 ms, Imbalance Overhead: 23.148 ms
GPU 6, Compute+Comm Time: 240.991 ms, Bubble Time: 55.358 ms, Imbalance Overhead: 23.334 ms
GPU 7, Compute+Comm Time: 259.154 ms, Bubble Time: 55.983 ms, Imbalance Overhead: 4.547 ms
The estimated cost of the whole pipeline: 509.959 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 706.557 ms
Partition 0 [0, 9) has cost: 706.557 ms
Partition 1 [9, 17) has cost: 681.340 ms
Partition 2 [17, 25) has cost: 681.340 ms
Partition 3 [25, 33) has cost: 687.699 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 172.198 ms
GPU 0, Compute+Comm Time: 142.673 ms, Bubble Time: 26.976 ms, Imbalance Overhead: 2.549 ms
GPU 1, Compute+Comm Time: 138.866 ms, Bubble Time: 26.502 ms, Imbalance Overhead: 6.829 ms
GPU 2, Compute+Comm Time: 138.866 ms, Bubble Time: 26.417 ms, Imbalance Overhead: 6.914 ms
GPU 3, Compute+Comm Time: 139.391 ms, Bubble Time: 26.179 ms, Imbalance Overhead: 6.629 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 318.708 ms
GPU 0, Compute+Comm Time: 258.218 ms, Bubble Time: 48.550 ms, Imbalance Overhead: 11.940 ms
GPU 1, Compute+Comm Time: 255.135 ms, Bubble Time: 48.796 ms, Imbalance Overhead: 14.777 ms
GPU 2, Compute+Comm Time: 255.135 ms, Bubble Time: 49.148 ms, Imbalance Overhead: 14.425 ms
GPU 3, Compute+Comm Time: 265.137 ms, Bubble Time: 50.211 ms, Imbalance Overhead: 3.360 ms
    The estimated cost with 2 DP ways is 515.451 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1387.897 ms
Partition 0 [0, 17) has cost: 1387.897 ms
Partition 1 [17, 33) has cost: 1369.039 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 218.336 ms
GPU 0, Compute+Comm Time: 189.753 ms, Bubble Time: 23.244 ms, Imbalance Overhead: 5.339 ms
GPU 1, Compute+Comm Time: 187.898 ms, Bubble Time: 23.862 ms, Imbalance Overhead: 6.576 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 355.910 ms
GPU 0, Compute+Comm Time: 308.684 ms, Bubble Time: 39.163 ms, Imbalance Overhead: 8.063 ms
GPU 1, Compute+Comm Time: 312.503 ms, Bubble Time: 38.064 ms, Imbalance Overhead: 5.343 ms
    The estimated cost with 4 DP ways is 602.958 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2756.935 ms
Partition 0 [0, 33) has cost: 2756.935 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 583.633 ms
GPU 0, Compute+Comm Time: 583.633 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 708.030 ms
GPU 0, Compute+Comm Time: 708.030 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1356.247 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5702	TrainAcc 0.4330	ValidAcc 0.4514	TestAcc 0.4512	BestValid 0.4514
	Epoch 50:	Loss 1.8936	TrainAcc 0.6935	ValidAcc 0.7087	TestAcc 0.7037	BestValid 0.7087
	Epoch 75:	Loss 1.5142	TrainAcc 0.7618	ValidAcc 0.7760	TestAcc 0.7693	BestValid 0.7760
	Epoch 100:	Loss 1.2888	TrainAcc 0.7908	ValidAcc 0.8023	TestAcc 0.7958	BestValid 0.8023
	Epoch 125:	Loss 1.1026	TrainAcc 0.8250	ValidAcc 0.8350	TestAcc 0.8280	BestValid 0.8350
	Epoch 150:	Loss 1.0063	TrainAcc 0.8463	ValidAcc 0.8561	TestAcc 0.8489	BestValid 0.8561
	Epoch 175:	Loss 0.9341	TrainAcc 0.8598	ValidAcc 0.8687	TestAcc 0.8612	BestValid 0.8687
	Epoch 200:	Loss 0.8841	TrainAcc 0.8690	ValidAcc 0.8773	TestAcc 0.8697	BestValid 0.8773
	Epoch 225:	Loss 0.8320	TrainAcc 0.8762	ValidAcc 0.8831	TestAcc 0.8768	BestValid 0.8831
	Epoch 250:	Loss 0.7996	TrainAcc 0.8808	ValidAcc 0.8875	TestAcc 0.8814	BestValid 0.8875
	Epoch 275:	Loss 0.7705	TrainAcc 0.8847	ValidAcc 0.8907	TestAcc 0.8851	BestValid 0.8907
	Epoch 300:	Loss 0.7447	TrainAcc 0.8886	ValidAcc 0.8939	TestAcc 0.8889	BestValid 0.8939
	Epoch 325:	Loss 0.7260	TrainAcc 0.8923	ValidAcc 0.8966	TestAcc 0.8919	BestValid 0.8966
	Epoch 350:	Loss 0.7084	TrainAcc 0.8950	ValidAcc 0.8991	TestAcc 0.8952	BestValid 0.8991
	Epoch 375:	Loss 0.6956	TrainAcc 0.8967	ValidAcc 0.9009	TestAcc 0.8965	BestValid 0.9009
	Epoch 400:	Loss 0.6767	TrainAcc 0.8999	ValidAcc 0.9044	TestAcc 0.8989	BestValid 0.9044
	Epoch 425:	Loss 0.6619	TrainAcc 0.9015	ValidAcc 0.9053	TestAcc 0.9004	BestValid 0.9053
	Epoch 450:	Loss 0.6489	TrainAcc 0.9031	ValidAcc 0.9063	TestAcc 0.9017	BestValid 0.9063
	Epoch 475:	Loss 0.6384	TrainAcc 0.9058	ValidAcc 0.9085	TestAcc 0.9040	BestValid 0.9085
	Epoch 500:	Loss 0.6275	TrainAcc 0.9076	ValidAcc 0.9096	TestAcc 0.9058	BestValid 0.9096
	Epoch 525:	Loss 0.6199	TrainAcc 0.9090	ValidAcc 0.9114	TestAcc 0.9072	BestValid 0.9114
	Epoch 550:	Loss 0.6112	TrainAcc 0.9104	ValidAcc 0.9128	TestAcc 0.9086	BestValid 0.9128
	Epoch 575:	Loss 0.6051	TrainAcc 0.9118	ValidAcc 0.9137	TestAcc 0.9096	BestValid 0.9137
	Epoch 600:	Loss 0.5989	TrainAcc 0.9144	ValidAcc 0.9167	TestAcc 0.9123	BestValid 0.9167
	Epoch 625:	Loss 0.5886	TrainAcc 0.9146	ValidAcc 0.9164	TestAcc 0.9125	BestValid 0.9167
	Epoch 650:	Loss 0.5801	TrainAcc 0.9152	ValidAcc 0.9162	TestAcc 0.9130	BestValid 0.9167
	Epoch 675:	Loss 0.5801	TrainAcc 0.9172	ValidAcc 0.9185	TestAcc 0.9153	BestValid 0.9185
	Epoch 700:	Loss 0.5701	TrainAcc 0.9185	ValidAcc 0.9198	TestAcc 0.9162	BestValid 0.9198
	Epoch 725:	Loss 0.5675	TrainAcc 0.9183	ValidAcc 0.9193	TestAcc 0.9158	BestValid 0.9198
	Epoch 750:	Loss 0.5609	TrainAcc 0.9187	ValidAcc 0.9197	TestAcc 0.9162	BestValid 0.9198
	Epoch 775:	Loss 0.5554	TrainAcc 0.9203	ValidAcc 0.9210	TestAcc 0.9174	BestValid 0.9210
	Epoch 800:	Loss 0.5530	TrainAcc 0.9210	ValidAcc 0.9210	TestAcc 0.9175	BestValid 0.9210
	Epoch 825:	Loss 0.5460	TrainAcc 0.9226	ValidAcc 0.9227	TestAcc 0.9195	BestValid 0.9227
	Epoch 850:	Loss 0.5410	TrainAcc 0.9221	ValidAcc 0.9225	TestAcc 0.9193	BestValid 0.9227
	Epoch 875:	Loss 0.5402	TrainAcc 0.9230	ValidAcc 0.9234	TestAcc 0.9203	BestValid 0.9234
	Epoch 900:	Loss 0.5352	TrainAcc 0.9238	ValidAcc 0.9233	TestAcc 0.9203	BestValid 0.9234
	Epoch 925:	Loss 0.5303	TrainAcc 0.9246	ValidAcc 0.9241	TestAcc 0.9212	BestValid 0.9241
	Epoch 950:	Loss 0.5290	TrainAcc 0.9256	ValidAcc 0.9252	TestAcc 0.9221	BestValid 0.9252
	Epoch 975:	Loss 0.5241	TrainAcc 0.9266	ValidAcc 0.9261	TestAcc 0.9229	BestValid 0.9261
	Epoch 1000:	Loss 0.5214	TrainAcc 0.9254	ValidAcc 0.9249	TestAcc 0.9216	BestValid 0.9261
	Epoch 1025:	Loss 0.5182	TrainAcc 0.9267	ValidAcc 0.9259	TestAcc 0.9229	BestValid 0.9261
	Epoch 1050:	Loss 0.5214	TrainAcc 0.9282	ValidAcc 0.9274	TestAcc 0.9247	BestValid 0.9274
	Epoch 1075:	Loss 0.5155	TrainAcc 0.9274	ValidAcc 0.9259	TestAcc 0.9229	BestValid 0.9274
	Epoch 1100:	Loss 0.5080	TrainAcc 0.9289	ValidAcc 0.9269	TestAcc 0.9250	BestValid 0.9274
	Epoch 1125:	Loss 0.5058	TrainAcc 0.9289	ValidAcc 0.9280	TestAcc 0.9246	BestValid 0.9280
	Epoch 1150:	Loss 0.5077	TrainAcc 0.9299	ValidAcc 0.9289	TestAcc 0.9255	BestValid 0.9289
	Epoch 1175:	Loss 0.5030	TrainAcc 0.9290	ValidAcc 0.9273	TestAcc 0.9242	BestValid 0.9289
	Epoch 1200:	Loss 0.5027	TrainAcc 0.9295	ValidAcc 0.9277	TestAcc 0.9246	BestValid 0.9289
	Epoch 1225:	Loss 0.4956	TrainAcc 0.9310	ValidAcc 0.9293	TestAcc 0.9260	BestValid 0.9293
	Epoch 1250:	Loss 0.4944	TrainAcc 0.9312	ValidAcc 0.9298	TestAcc 0.9264	BestValid 0.9298
	Epoch 1275:	Loss 0.4919	TrainAcc 0.9315	ValidAcc 0.9300	TestAcc 0.9266	BestValid 0.9300
	Epoch 1300:	Loss 0.4887	TrainAcc 0.9315	ValidAcc 0.9298	TestAcc 0.9262	BestValid 0.9300
	Epoch 1325:	Loss 0.4876	TrainAcc 0.9317	ValidAcc 0.9296	TestAcc 0.9262	BestValid 0.9300
	Epoch 1350:	Loss 0.4855	TrainAcc 0.9324	ValidAcc 0.9303	TestAcc 0.9274	BestValid 0.9303
	Epoch 1375:	Loss 0.4843	TrainAcc 0.9316	ValidAcc 0.9295	TestAcc 0.9269	BestValid 0.9303
	Epoch 1400:	Loss 0.4813	TrainAcc 0.9333	ValidAcc 0.9313	TestAcc 0.9285	BestValid 0.9313
	Epoch 1425:	Loss 0.4829	TrainAcc 0.9333	ValidAcc 0.9305	TestAcc 0.9279	BestValid 0.9313
	Epoch 1450:	Loss 0.4829	TrainAcc 0.9334	ValidAcc 0.9305	TestAcc 0.9285	BestValid 0.9313
	Epoch 1475:	Loss 0.4779	TrainAcc 0.9345	ValidAcc 0.9318	TestAcc 0.9290	BestValid 0.9318
	Epoch 1500:	Loss 0.4743	TrainAcc 0.9337	ValidAcc 0.9310	TestAcc 0.9283	BestValid 0.9318
	Epoch 1525:	Loss 0.4745	TrainAcc 0.9348	ValidAcc 0.9319	TestAcc 0.9294	BestValid 0.9319
	Epoch 1550:	Loss 0.4740	TrainAcc 0.9349	ValidAcc 0.9320	TestAcc 0.9294	BestValid 0.9320
	Epoch 1575:	Loss 0.4749	TrainAcc 0.9360	ValidAcc 0.9326	TestAcc 0.9306	BestValid 0.9326
	Epoch 1600:	Loss 0.4702	TrainAcc 0.9352	ValidAcc 0.9322	TestAcc 0.9299	BestValid 0.9326
	Epoch 1625:	Loss 0.4675	TrainAcc 0.9356	ValidAcc 0.9326	TestAcc 0.9300	BestValid 0.9326
	Epoch 1650:	Loss 0.4666	TrainAcc 0.9365	ValidAcc 0.9335	TestAcc 0.9309	BestValid 0.9335
	Epoch 1675:	Loss 0.4654	TrainAcc 0.9361	ValidAcc 0.9326	TestAcc 0.9301	BestValid 0.9335
	Epoch 1700:	Loss 0.4661	TrainAcc 0.9354	ValidAcc 0.9324	TestAcc 0.9298	BestValid 0.9335
	Epoch 1725:	Loss 0.4660	TrainAcc 0.9355	ValidAcc 0.9326	TestAcc 0.9299	BestValid 0.9335
	Epoch 1750:	Loss 0.4623	TrainAcc 0.9379	ValidAcc 0.9347	TestAcc 0.9325	BestValid 0.9347
	Epoch 1775:	Loss 0.4613	TrainAcc 0.9370	ValidAcc 0.9333	TestAcc 0.9311	BestValid 0.9347
	Epoch 1800:	Loss 0.4609	TrainAcc 0.9377	ValidAcc 0.9342	TestAcc 0.9318	BestValid 0.9347
	Epoch 1825:	Loss 0.4599	TrainAcc 0.9380	ValidAcc 0.9342	TestAcc 0.9319	BestValid 0.9347
	Epoch 1850:	Loss 0.4614	TrainAcc 0.9371	ValidAcc 0.9340	TestAcc 0.9309	BestValid 0.9347
	Epoch 1875:	Loss 0.4546	TrainAcc 0.9384	ValidAcc 0.9348	TestAcc 0.9327	BestValid 0.9348
	Epoch 1900:	Loss 0.4530	TrainAcc 0.9383	ValidAcc 0.9353	TestAcc 0.9324	BestValid 0.9353
	Epoch 1925:	Loss 0.4548	TrainAcc 0.9383	ValidAcc 0.9343	TestAcc 0.9322	BestValid 0.9353
	Epoch 1950:	Loss 0.4523	TrainAcc 0.9390	ValidAcc 0.9350	TestAcc 0.9334	BestValid 0.9353
	Epoch 1975:	Loss 0.4504	TrainAcc 0.9397	ValidAcc 0.9354	TestAcc 0.9335	BestValid 0.9354
	Epoch 2000:	Loss 0.4502	TrainAcc 0.9387	ValidAcc 0.9353	TestAcc 0.9328	BestValid 0.9354
	Epoch 2025:	Loss 0.4493	TrainAcc 0.9391	ValidAcc 0.9350	TestAcc 0.9331	BestValid 0.9354
	Epoch 2050:	Loss 0.4461	TrainAcc 0.9388	ValidAcc 0.9352	TestAcc 0.9330	BestValid 0.9354
	Epoch 2075:	Loss 0.4473	TrainAcc 0.9405	ValidAcc 0.9362	TestAcc 0.9343	BestValid 0.9362
	Epoch 2100:	Loss 0.4474	TrainAcc 0.9393	ValidAcc 0.9350	TestAcc 0.9328	BestValid 0.9362
	Epoch 2125:	Loss 0.4431	TrainAcc 0.9395	ValidAcc 0.9356	TestAcc 0.9330	BestValid 0.9362
	Epoch 2150:	Loss 0.4426	TrainAcc 0.9397	ValidAcc 0.9363	TestAcc 0.9330	BestValid 0.9363
	Epoch 2175:	Loss 0.4422	TrainAcc 0.9408	ValidAcc 0.9368	TestAcc 0.9344	BestValid 0.9368
	Epoch 2200:	Loss 0.4416	TrainAcc 0.9408	ValidAcc 0.9367	TestAcc 0.9342	BestValid 0.9368
	Epoch 2225:	Loss 0.4423	TrainAcc 0.9405	ValidAcc 0.9365	TestAcc 0.9335	BestValid 0.9368
	Epoch 2250:	Loss 0.4411	TrainAcc 0.9418	ValidAcc 0.9367	TestAcc 0.9354	BestValid 0.9368
	Epoch 2275:	Loss 0.4389	TrainAcc 0.9417	ValidAcc 0.9370	TestAcc 0.9352	BestValid 0.9370
	Epoch 2300:	Loss 0.4395	TrainAcc 0.9416	ValidAcc 0.9376	TestAcc 0.9348	BestValid 0.9376
	Epoch 2325:	Loss 0.4379	TrainAcc 0.9410	ValidAcc 0.9368	TestAcc 0.9339	BestValid 0.9376
	Epoch 2350:	Loss 0.4372	TrainAcc 0.9413	ValidAcc 0.9373	TestAcc 0.9348	BestValid 0.9376
	Epoch 2375:	Loss 0.4352	TrainAcc 0.9417	ValidAcc 0.9373	TestAcc 0.9345	BestValid 0.9376
	Epoch 2400:	Loss 0.4349	TrainAcc 0.9418	ValidAcc 0.9373	TestAcc 0.9350	BestValid 0.9376
	Epoch 2425:	Loss 0.4338	TrainAcc 0.9414	ValidAcc 0.9368	TestAcc 0.9343	BestValid 0.9376
	Epoch 2450:	Loss 0.4338	TrainAcc 0.9425	ValidAcc 0.9377	TestAcc 0.9356	BestValid 0.9377
	Epoch 2475:	Loss 0.4326	TrainAcc 0.9431	ValidAcc 0.9380	TestAcc 0.9359	BestValid 0.9380
	Epoch 2500:	Loss 0.4334	TrainAcc 0.9412	ValidAcc 0.9373	TestAcc 0.9342	BestValid 0.9380
	Epoch 2525:	Loss 0.4321	TrainAcc 0.9426	ValidAcc 0.9378	TestAcc 0.9356	BestValid 0.9380
	Epoch 2550:	Loss 0.4300	TrainAcc 0.9422	ValidAcc 0.9377	TestAcc 0.9349	BestValid 0.9380
	Epoch 2575:	Loss 0.4307	TrainAcc 0.9433	ValidAcc 0.9384	TestAcc 0.9361	BestValid 0.9384
	Epoch 2600:	Loss 0.4303	TrainAcc 0.9428	ValidAcc 0.9379	TestAcc 0.9355	BestValid 0.9384
	Epoch 2625:	Loss 0.4289	TrainAcc 0.9416	ValidAcc 0.9369	TestAcc 0.9346	BestValid 0.9384
	Epoch 2650:	Loss 0.4306	TrainAcc 0.9434	ValidAcc 0.9384	TestAcc 0.9364	BestValid 0.9384
	Epoch 2675:	Loss 0.4273	TrainAcc 0.9441	ValidAcc 0.9388	TestAcc 0.9373	BestValid 0.9388
	Epoch 2700:	Loss 0.4296	TrainAcc 0.9426	ValidAcc 0.9377	TestAcc 0.9350	BestValid 0.9388
	Epoch 2725:	Loss 0.4253	TrainAcc 0.9440	ValidAcc 0.9387	TestAcc 0.9367	BestValid 0.9388
	Epoch 2750:	Loss 0.4267	TrainAcc 0.9431	ValidAcc 0.9384	TestAcc 0.9360	BestValid 0.9388
	Epoch 2775:	Loss 0.4245	TrainAcc 0.9435	ValidAcc 0.9385	TestAcc 0.9362	BestValid 0.9388
	Epoch 2800:	Loss 0.4233	TrainAcc 0.9442	ValidAcc 0.9389	TestAcc 0.9364	BestValid 0.9389
	Epoch 2825:	Loss 0.4211	TrainAcc 0.9439	ValidAcc 0.9386	TestAcc 0.9366	BestValid 0.9389
	Epoch 2850:	Loss 0.4217	TrainAcc 0.9433	ValidAcc 0.9384	TestAcc 0.9358	BestValid 0.9389
	Epoch 2875:	Loss 0.4231	TrainAcc 0.9444	ValidAcc 0.9396	TestAcc 0.9369	BestValid 0.9396
	Epoch 2900:	Loss 0.4206	TrainAcc 0.9433	ValidAcc 0.9380	TestAcc 0.9356	BestValid 0.9396
	Epoch 2925:	Loss 0.4181	TrainAcc 0.9446	ValidAcc 0.9392	TestAcc 0.9371	BestValid 0.9396
	Epoch 2950:	Loss 0.4186	TrainAcc 0.9450	ValidAcc 0.9398	TestAcc 0.9380	BestValid 0.9398
	Epoch 2975:	Loss 0.4197	TrainAcc 0.9448	ValidAcc 0.9397	TestAcc 0.9372	BestValid 0.9398
	Epoch 3000:	Loss 0.4182	TrainAcc 0.9454	ValidAcc 0.9396	TestAcc 0.9381	BestValid 0.9398
	Epoch 3025:	Loss 0.4215	TrainAcc 0.9448	ValidAcc 0.9394	TestAcc 0.9375	BestValid 0.9398
	Epoch 3050:	Loss 0.4162	TrainAcc 0.9447	ValidAcc 0.9400	TestAcc 0.9372	BestValid 0.9400
	Epoch 3075:	Loss 0.4161	TrainAcc 0.9455	ValidAcc 0.9402	TestAcc 0.9383	BestValid 0.9402
	Epoch 3100:	Loss 0.4169	TrainAcc 0.9449	ValidAcc 0.9400	TestAcc 0.9376	BestValid 0.9402
	Epoch 3125:	Loss 0.4155	TrainAcc 0.9454	ValidAcc 0.9407	TestAcc 0.9380	BestValid 0.9407
	Epoch 3150:	Loss 0.4108	TrainAcc 0.9453	ValidAcc 0.9402	TestAcc 0.9382	BestValid 0.9407
	Epoch 3175:	Loss 0.4125	TrainAcc 0.9448	ValidAcc 0.9400	TestAcc 0.9374	BestValid 0.9407
	Epoch 3200:	Loss 0.4146	TrainAcc 0.9445	ValidAcc 0.9400	TestAcc 0.9369	BestValid 0.9407
	Epoch 3225:	Loss 0.4128	TrainAcc 0.9456	ValidAcc 0.9404	TestAcc 0.9380	BestValid 0.9407
	Epoch 3250:	Loss 0.4139	TrainAcc 0.9448	ValidAcc 0.9402	TestAcc 0.9374	BestValid 0.9407
	Epoch 3275:	Loss 0.4124	TrainAcc 0.9460	ValidAcc 0.9405	TestAcc 0.9387	BestValid 0.9407
	Epoch 3300:	Loss 0.4091	TrainAcc 0.9446	ValidAcc 0.9400	TestAcc 0.9375	BestValid 0.9407
	Epoch 3325:	Loss 0.4107	TrainAcc 0.9460	ValidAcc 0.9409	TestAcc 0.9382	BestValid 0.9409
	Epoch 3350:	Loss 0.4118	TrainAcc 0.9460	ValidAcc 0.9405	TestAcc 0.9386	BestValid 0.9409
	Epoch 3375:	Loss 0.4111	TrainAcc 0.9465	ValidAcc 0.9411	TestAcc 0.9387	BestValid 0.9411
	Epoch 3400:	Loss 0.4081	TrainAcc 0.9451	ValidAcc 0.9398	TestAcc 0.9375	BestValid 0.9411
	Epoch 3425:	Loss 0.4123	TrainAcc 0.9472	ValidAcc 0.9415	TestAcc 0.9393	BestValid 0.9415
	Epoch 3450:	Loss 0.4105	TrainAcc 0.9467	ValidAcc 0.9415	TestAcc 0.9391	BestValid 0.9415
	Epoch 3475:	Loss 0.4088	TrainAcc 0.9463	ValidAcc 0.9412	TestAcc 0.9386	BestValid 0.9415
	Epoch 3500:	Loss 0.4075	TrainAcc 0.9463	ValidAcc 0.9408	TestAcc 0.9392	BestValid 0.9415
	Epoch 3525:	Loss 0.4065	TrainAcc 0.9462	ValidAcc 0.9410	TestAcc 0.9384	BestValid 0.9415
	Epoch 3550:	Loss 0.4082	TrainAcc 0.9456	ValidAcc 0.9405	TestAcc 0.9380	BestValid 0.9415
	Epoch 3575:	Loss 0.4020	TrainAcc 0.9468	ValidAcc 0.9416	TestAcc 0.9390	BestValid 0.9416
	Epoch 3600:	Loss 0.4066	TrainAcc 0.9469	ValidAcc 0.9418	TestAcc 0.9392	BestValid 0.9418
	Epoch 3625:	Loss 0.4053	TrainAcc 0.9468	ValidAcc 0.9413	TestAcc 0.9392	BestValid 0.9418
	Epoch 3650:	Loss 0.4075	TrainAcc 0.9475	ValidAcc 0.9425	TestAcc 0.9394	BestValid 0.9425
	Epoch 3675:	Loss 0.4032	TrainAcc 0.9472	ValidAcc 0.9419	TestAcc 0.9396	BestValid 0.9425
	Epoch 3700:	Loss 0.4051	TrainAcc 0.9469	ValidAcc 0.9417	TestAcc 0.9388	BestValid 0.9425
	Epoch 3725:	Loss 0.4035	TrainAcc 0.9475	ValidAcc 0.9420	TestAcc 0.9397	BestValid 0.9425
	Epoch 3750:	Loss 0.3971	TrainAcc 0.9473	ValidAcc 0.9418	TestAcc 0.9394	BestValid 0.9425
	Epoch 3775:	Loss 0.4018	TrainAcc 0.9473	ValidAcc 0.9423	TestAcc 0.9391	BestValid 0.9425
	Epoch 3800:	Loss 0.4058	TrainAcc 0.9476	ValidAcc 0.9422	TestAcc 0.9399	BestValid 0.9425
	Epoch 3825:	Loss 0.4009	TrainAcc 0.9474	ValidAcc 0.9419	TestAcc 0.9396	BestValid 0.9425
	Epoch 3850:	Loss 0.3980	TrainAcc 0.9468	ValidAcc 0.9412	TestAcc 0.9392	BestValid 0.9425
	Epoch 3875:	Loss 0.3990	TrainAcc 0.9475	ValidAcc 0.9418	TestAcc 0.9395	BestValid 0.9425
	Epoch 3900:	Loss 0.4019	TrainAcc 0.9466	ValidAcc 0.9412	TestAcc 0.9388	BestValid 0.9425
	Epoch 3925:	Loss 0.3968	TrainAcc 0.9468	ValidAcc 0.9418	TestAcc 0.9388	BestValid 0.9425
	Epoch 3950:	Loss 0.3993	TrainAcc 0.9489	ValidAcc 0.9429	TestAcc 0.9406	BestValid 0.9429
	Epoch 3975:	Loss 0.3973	TrainAcc 0.9477	ValidAcc 0.9423	TestAcc 0.9395	BestValid 0.9429
	Epoch 4000:	Loss 0.3970	TrainAcc 0.9480	ValidAcc 0.9421	TestAcc 0.9396	BestValid 0.9429
	Epoch 4025:	Loss 0.3936	TrainAcc 0.9479	ValidAcc 0.9425	TestAcc 0.9400	BestValid 0.9429
	Epoch 4050:	Loss 0.3975	TrainAcc 0.9487	ValidAcc 0.9424	TestAcc 0.9409	BestValid 0.9429
	Epoch 4075:	Loss 0.3952	TrainAcc 0.9483	ValidAcc 0.9424	TestAcc 0.9403	BestValid 0.9429
	Epoch 4100:	Loss 0.3967	TrainAcc 0.9479	ValidAcc 0.9426	TestAcc 0.9400	BestValid 0.9429
	Epoch 4125:	Loss 0.3960	TrainAcc 0.9482	ValidAcc 0.9426	TestAcc 0.9401	BestValid 0.9429
	Epoch 4150:	Loss 0.3918	TrainAcc 0.9490	ValidAcc 0.9430	TestAcc 0.9409	BestValid 0.9430
	Epoch 4175:	Loss 0.3906	TrainAcc 0.9489	ValidAcc 0.9434	TestAcc 0.9410	BestValid 0.9434
	Epoch 4200:	Loss 0.3922	TrainAcc 0.9484	ValidAcc 0.9427	TestAcc 0.9406	BestValid 0.9434
	Epoch 4225:	Loss 0.3914	TrainAcc 0.9489	ValidAcc 0.9432	TestAcc 0.9411	BestValid 0.9434
	Epoch 4250:	Loss 0.3917	TrainAcc 0.9493	ValidAcc 0.9439	TestAcc 0.9412	BestValid 0.9439
	Epoch 4275:	Loss 0.3928	TrainAcc 0.9488	ValidAcc 0.9435	TestAcc 0.9410	BestValid 0.9439
	Epoch 4300:	Loss 0.3934	TrainAcc 0.9488	ValidAcc 0.9433	TestAcc 0.9408	BestValid 0.9439
	Epoch 4325:	Loss 0.3922	TrainAcc 0.9492	ValidAcc 0.9436	TestAcc 0.9414	BestValid 0.9439
	Epoch 4350:	Loss 0.3925	TrainAcc 0.9488	ValidAcc 0.9430	TestAcc 0.9410	BestValid 0.9439
	Epoch 4375:	Loss 0.3901	TrainAcc 0.9496	ValidAcc 0.9434	TestAcc 0.9419	BestValid 0.9439
	Epoch 4400:	Loss 0.3901	TrainAcc 0.9487	ValidAcc 0.9434	TestAcc 0.9408	BestValid 0.9439
	Epoch 4425:	Loss 0.3868	TrainAcc 0.9489	ValidAcc 0.9430	TestAcc 0.9409	BestValid 0.9439
	Epoch 4450:	Loss 0.3921	TrainAcc 0.9496	ValidAcc 0.9441	TestAcc 0.9424	BestValid 0.9441
	Epoch 4475:	Loss 0.3884	TrainAcc 0.9499	ValidAcc 0.9440	TestAcc 0.9422	BestValid 0.9441
	Epoch 4500:	Loss 0.3853	TrainAcc 0.9499	ValidAcc 0.9439	TestAcc 0.9424	BestValid 0.9441
	Epoch 4525:	Loss 0.3876	TrainAcc 0.9493	ValidAcc 0.9440	TestAcc 0.9419	BestValid 0.9441
	Epoch 4550:	Loss 0.3859	TrainAcc 0.9503	ValidAcc 0.9446	TestAcc 0.9429	BestValid 0.9446
	Epoch 4575:	Loss 0.3876	TrainAcc 0.9491	ValidAcc 0.9432	TestAcc 0.9413	BestValid 0.9446
	Epoch 4600:	Loss 0.3854	TrainAcc 0.9494	ValidAcc 0.9437	TestAcc 0.9419	BestValid 0.9446
	Epoch 4625:	Loss 0.3864	TrainAcc 0.9496	ValidAcc 0.9438	TestAcc 0.9420	BestValid 0.9446
	Epoch 4650:	Loss 0.3859	TrainAcc 0.9504	ValidAcc 0.9446	TestAcc 0.9433	BestValid 0.9446
	Epoch 4675:	Loss 0.3883	TrainAcc 0.9498	ValidAcc 0.9441	TestAcc 0.9424	BestValid 0.9446
	Epoch 4700:	Loss 0.3860	TrainAcc 0.9495	ValidAcc 0.9439	TestAcc 0.9420	BestValid 0.9446
	Epoch 4725:	Loss 0.3842	TrainAcc 0.9503	ValidAcc 0.9446	TestAcc 0.9426	BestValid 0.9446
	Epoch 4750:	Loss 0.3808	TrainAcc 0.9493	ValidAcc 0.9439	TestAcc 0.9414	BestValid 0.9446
	Epoch 4775:	Loss 0.3829	TrainAcc 0.9501	ValidAcc 0.9438	TestAcc 0.9425	BestValid 0.9446
	Epoch 4800:	Loss 0.3815	TrainAcc 0.9503	ValidAcc 0.9449	TestAcc 0.9433	BestValid 0.9449
	Epoch 4825:	Loss 0.3810	TrainAcc 0.9499	ValidAcc 0.9442	TestAcc 0.9424	BestValid 0.9449
	Epoch 4850:	Loss 0.3814	TrainAcc 0.9501	ValidAcc 0.9446	TestAcc 0.9429	BestValid 0.9449
	Epoch 4875:	Loss 0.3835	TrainAcc 0.9498	ValidAcc 0.9445	TestAcc 0.9427	BestValid 0.9449
	Epoch 4900:	Loss 0.3797	TrainAcc 0.9499	ValidAcc 0.9450	TestAcc 0.9427	BestValid 0.9450
	Epoch 4925:	Loss 0.3809	TrainAcc 0.9503	ValidAcc 0.9445	TestAcc 0.9431	BestValid 0.9450
	Epoch 4950:	Loss 0.3811	TrainAcc 0.9511	ValidAcc 0.9454	TestAcc 0.9441	BestValid 0.9454
	Epoch 4975:	Loss 0.3807	TrainAcc 0.9507	ValidAcc 0.9449	TestAcc 0.9443	BestValid 0.9454
	Epoch 5000:	Loss 0.3811	TrainAcc 0.9495	ValidAcc 0.9444	TestAcc 0.9423	BestValid 0.9454
Node 0, Pre/Post-Pipelining: 2.631 / 3.588 ms, Bubble: 67.266 ms, Compute: 334.202 ms, Comm: 32.276 ms, Imbalance: 6.778 ms
Node 2, Pre/Post-Pipelining: 2.621 / 3.239 ms, Bubble: 67.016 ms, Compute: 314.511 ms, Comm: 37.357 ms, Imbalance: 22.832 ms
Node 1, Pre/Post-Pipelining: 2.632 / 3.565 ms, Bubble: 67.204 ms, Compute: 337.139 ms, Comm: 29.055 ms, Imbalance: 7.138 ms
Node 7, Pre/Post-Pipelining: 2.632 / 11.091 ms, Bubble: 62.999 ms, Compute: 330.688 ms, Comm: 25.230 ms, Imbalance: 15.417 ms
Node 3, Pre/Post-Pipelining: 2.616 / 3.270 ms, Bubble: 66.924 ms, Compute: 318.091 ms, Comm: 34.115 ms, Imbalance: 22.514 ms
Node 4, Pre/Post-Pipelining: 2.589 / 3.707 ms, Bubble: 68.689 ms, Compute: 313.246 ms, Comm: 35.743 ms, Imbalance: 24.930 ms
Node 5, Pre/Post-Pipelining: 2.588 / 3.712 ms, Bubble: 68.621 ms, Compute: 317.208 ms, Comm: 31.719 ms, Imbalance: 24.976 ms
Node 6, Pre/Post-Pipelining: 2.633 / 11.231 ms, Bubble: 63.051 ms, Compute: 326.032 ms, Comm: 30.212 ms, Imbalance: 15.084 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 2.631 ms
Cluster-Wide Average, Post-Pipelining Overhead: 3.588 ms
Cluster-Wide Average, Bubble: 67.266 ms
Cluster-Wide Average, Compute: 334.202 ms
Cluster-Wide Average, Communication: 32.276 ms
Cluster-Wide Average, Imbalance: 6.778 ms
Node 0, GPU memory consumption: 9.405 GB
Node 2, GPU memory consumption: 7.219 GB
Node 1, GPU memory consumption: 8.118 GB
Node 4, GPU memory consumption: 7.196 GB
Node 3, GPU memory consumption: 7.096 GB
Node 5, GPU memory consumption: 7.120 GB
Node 7, GPU memory consumption: 7.210 GB
Node 6, GPU memory consumption: 7.337 GB
Node 0, Graph-Level Communication Throughput: 123.920 Gbps, Time: 48.796 ms
Node 1, Graph-Level Communication Throughput: 93.414 Gbps, Time: 58.127 ms
Node 2, Graph-Level Communication Throughput: 115.932 Gbps, Time: 52.158 ms
Node 3, Graph-Level Communication Throughput: 102.587 Gbps, Time: 52.929 ms
Node 4, Graph-Level Communication Throughput: 121.558 Gbps, Time: 49.744 ms
Node 5, Graph-Level Communication Throughput: 98.011 Gbps, Time: 55.400 ms
Node 6, Graph-Level Communication Throughput: 127.440 Gbps, Time: 47.448 ms
Node 7, Graph-Level Communication Throughput: 96.202 Gbps, Time: 56.442 ms
------------------------node id 0,  per-epoch time: 0.940688 s---------------
------------------------node id 1,  per-epoch time: 0.940688 s---------------
------------------------node id 2,  per-epoch time: 0.940688 s---------------
------------------------node id 3,  per-epoch time: 0.940688 s---------------
------------------------node id 4,  per-epoch time: 0.940688 s---------------
------------------------node id 5,  per-epoch time: 0.940688 s---------------
------------------------node id 6,  per-epoch time: 0.940688 s---------------
------------------------node id 7,  per-epoch time: 0.940688 s---------------
************ Profiling Results ************
	Bubble: 617.578416 (ms) (65.68 percentage)
	Compute: 254.063918 (ms) (27.02 percentage)
	GraphCommComputeOverhead: 9.780665 (ms) (1.04 percentage)
	GraphCommNetwork: 52.635167 (ms) (5.60 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.232722 (ms) (0.66 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.041 GB
	Graph-level communication (cluster-wide, per-epoch): 5.344 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 6.388 GB
	Aggregated layer-level communication throughput: 279.878 Gbps
Highest valid_acc: 0.9454
Target test_acc: 0.9441
Epoch to reach the target acc: 4949
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
