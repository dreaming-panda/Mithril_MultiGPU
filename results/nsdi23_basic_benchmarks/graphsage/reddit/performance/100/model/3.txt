Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.875 seconds.
Building the CSC structure...
        It takes 2.064 seconds.
Building the CSC structure...
        It takes 2.079 seconds.
Building the CSC structure...
        It takes 2.082 seconds.
Building the CSC structure...
        It takes 2.325 seconds.
Building the CSC structure...
        It takes 2.430 seconds.
Building the CSC structure...
        It takes 2.575 seconds.
Building the CSC structure...
        It takes 2.646 seconds.
Building the CSC structure...
        It takes 1.824 seconds.
        It takes 1.839 seconds.
        It takes 1.890 seconds.
        It takes 2.140 seconds.
Building the Feature Vector...
        It takes 2.385 seconds.
        It takes 2.355 seconds.
        It takes 2.303 seconds.
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 2.391 seconds.
Building the Feature Vector...
        It takes 0.243 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.267 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Building the Feature Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
Building the Feature Vector...
        It takes 0.304 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.258 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.481 Gbps (per GPU), 451.851 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.185 Gbps (per GPU), 449.483 Gbps (aggregated)
The layer-level communication performance: 56.179 Gbps (per GPU), 449.435 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.966 Gbps (per GPU), 447.724 Gbps (aggregated)
The layer-level communication performance: 55.923 Gbps (per GPU), 447.385 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.741 Gbps (per GPU), 445.925 Gbps (aggregated)
The layer-level communication performance: 55.706 Gbps (per GPU), 445.645 Gbps (aggregated)
The layer-level communication performance: 55.671 Gbps (per GPU), 445.369 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.886 Gbps (per GPU), 1287.090 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.948 Gbps (per GPU), 1287.583 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.884 Gbps (per GPU), 1287.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.963 Gbps (per GPU), 1287.707 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.886 Gbps (per GPU), 1287.090 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.895 Gbps (per GPU), 1287.164 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.886 Gbps (per GPU), 1287.091 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.707 Gbps (per GPU), 1269.655 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.194 Gbps (per GPU), 833.553 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.194 Gbps (per GPU), 833.548 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.193 Gbps (per GPU), 833.540 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.194 Gbps (per GPU), 833.554 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.193 Gbps (per GPU), 833.547 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.193 Gbps (per GPU), 833.547 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.193 Gbps (per GPU), 833.547 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.194 Gbps (per GPU), 833.554 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.184 Gbps (per GPU), 241.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.183 Gbps (per GPU), 241.466 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.183 Gbps (per GPU), 241.467 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.184 Gbps (per GPU), 241.471 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.183 Gbps (per GPU), 241.467 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.183 Gbps (per GPU), 241.462 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.184 Gbps (per GPU), 241.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.183 Gbps (per GPU), 241.463 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.28ms  2.73ms  2.46ms  1.34  8.38K  3.53M
 chk_1  3.58ms  2.94ms  2.75ms  1.30  6.74K  3.60M
 chk_2  3.50ms  2.82ms  2.63ms  1.33  7.27K  3.53M
 chk_3  3.54ms  2.88ms  2.65ms  1.34  7.92K  3.61M
 chk_4  3.26ms  2.80ms  2.64ms  1.24  5.33K  3.68M
 chk_5  3.65ms  2.83ms  2.49ms  1.46 10.07K  3.45M
 chk_6  3.76ms  2.98ms  2.66ms  1.41  9.41K  3.48M
 chk_7  3.46ms  2.81ms  2.58ms  1.34  8.12K  3.60M
 chk_8  3.43ms  2.89ms  2.74ms  1.25  6.09K  3.64M
 chk_9  3.68ms  2.75ms  2.40ms  1.53 11.10K  3.38M
chk_10  3.42ms  2.92ms  2.75ms  1.24  5.67K  3.63M
chk_11  3.46ms  2.80ms  2.56ms  1.35  8.16K  3.54M
chk_12  3.64ms  2.97ms  2.79ms  1.30  7.24K  3.55M
chk_13  3.31ms  2.81ms  2.68ms  1.23  5.41K  3.68M
chk_14  3.72ms  3.04ms  2.85ms  1.30  7.14K  3.53M
chk_15  3.74ms  2.96ms  2.67ms  1.40  9.25K  3.49M
chk_16  3.20ms  2.76ms  2.64ms  1.21  4.78K  3.77M
chk_17  3.49ms  2.88ms  2.68ms  1.30  6.85K  3.60M
chk_18  3.34ms  2.69ms  2.49ms  1.34  7.47K  3.57M
chk_19  3.21ms  2.97ms  2.61ms  1.23  4.88K  3.75M
chk_20  3.44ms  2.75ms  2.59ms  1.33  7.00K  3.63M
chk_21  3.26ms  2.73ms  2.58ms  1.27  5.41K  3.68M
chk_22  3.94ms  2.98ms  2.64ms  1.49 11.07K  3.39M
chk_23  3.52ms  2.84ms  2.68ms  1.32  7.23K  3.64M
chk_24  3.78ms  2.98ms  2.66ms  1.42 10.13K  3.43M
chk_25  3.27ms  2.72ms  2.55ms  1.28  6.40K  3.57M
chk_26  3.45ms  2.92ms  2.75ms  1.25  5.78K  3.55M
chk_27  3.62ms  2.89ms  2.57ms  1.41  9.34K  3.48M
chk_28  3.68ms  3.10ms  2.91ms  1.27  6.37K  3.57M
chk_29  3.42ms  2.92ms  2.77ms  1.23  5.16K  3.78M
chk_30  3.27ms  2.80ms  2.70ms  1.21  5.44K  3.67M
chk_31  3.53ms  2.96ms  2.79ms  1.27  6.33K  3.63M
   Avg  3.50  2.87  2.65
   Max  3.94  3.10  2.91
   Min  3.20  2.69  2.40
 Ratio  1.23  1.15  1.21
   Var  0.03  0.01  0.01
Profiling takes 3.355 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 387.342 ms
Partition 0 [0, 4) has cost: 387.342 ms
Partition 1 [4, 8) has cost: 367.279 ms
Partition 2 [8, 12) has cost: 367.279 ms
Partition 3 [12, 16) has cost: 367.279 ms
Partition 4 [16, 20) has cost: 367.279 ms
Partition 5 [20, 24) has cost: 367.279 ms
Partition 6 [24, 28) has cost: 367.279 ms
Partition 7 [28, 32) has cost: 360.379 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 175.362 ms
GPU 0, Compute+Comm Time: 140.227 ms, Bubble Time: 30.482 ms, Imbalance Overhead: 4.654 ms
GPU 1, Compute+Comm Time: 134.480 ms, Bubble Time: 30.249 ms, Imbalance Overhead: 10.633 ms
GPU 2, Compute+Comm Time: 134.480 ms, Bubble Time: 30.220 ms, Imbalance Overhead: 10.662 ms
GPU 3, Compute+Comm Time: 134.480 ms, Bubble Time: 30.078 ms, Imbalance Overhead: 10.804 ms
GPU 4, Compute+Comm Time: 134.480 ms, Bubble Time: 30.078 ms, Imbalance Overhead: 10.804 ms
GPU 5, Compute+Comm Time: 134.480 ms, Bubble Time: 30.052 ms, Imbalance Overhead: 10.830 ms
GPU 6, Compute+Comm Time: 134.480 ms, Bubble Time: 30.286 ms, Imbalance Overhead: 10.596 ms
GPU 7, Compute+Comm Time: 132.022 ms, Bubble Time: 30.735 ms, Imbalance Overhead: 12.605 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 337.671 ms
GPU 0, Compute+Comm Time: 254.755 ms, Bubble Time: 60.268 ms, Imbalance Overhead: 22.648 ms
GPU 1, Compute+Comm Time: 259.196 ms, Bubble Time: 59.351 ms, Imbalance Overhead: 19.124 ms
GPU 2, Compute+Comm Time: 259.196 ms, Bubble Time: 58.850 ms, Imbalance Overhead: 19.625 ms
GPU 3, Compute+Comm Time: 259.196 ms, Bubble Time: 58.878 ms, Imbalance Overhead: 19.596 ms
GPU 4, Compute+Comm Time: 259.196 ms, Bubble Time: 58.739 ms, Imbalance Overhead: 19.736 ms
GPU 5, Compute+Comm Time: 259.196 ms, Bubble Time: 58.930 ms, Imbalance Overhead: 19.545 ms
GPU 6, Compute+Comm Time: 259.196 ms, Bubble Time: 58.883 ms, Imbalance Overhead: 19.592 ms
GPU 7, Compute+Comm Time: 273.513 ms, Bubble Time: 59.378 ms, Imbalance Overhead: 4.780 ms
The estimated cost of the whole pipeline: 538.685 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 754.621 ms
Partition 0 [0, 8) has cost: 754.621 ms
Partition 1 [8, 16) has cost: 734.558 ms
Partition 2 [16, 24) has cost: 734.558 ms
Partition 3 [24, 32) has cost: 727.658 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 183.567 ms
GPU 0, Compute+Comm Time: 150.158 ms, Bubble Time: 28.164 ms, Imbalance Overhead: 5.246 ms
GPU 1, Compute+Comm Time: 147.081 ms, Bubble Time: 27.601 ms, Imbalance Overhead: 8.885 ms
GPU 2, Compute+Comm Time: 147.081 ms, Bubble Time: 27.406 ms, Imbalance Overhead: 9.080 ms
GPU 3, Compute+Comm Time: 145.817 ms, Bubble Time: 27.095 ms, Imbalance Overhead: 10.655 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 336.970 ms
GPU 0, Compute+Comm Time: 271.038 ms, Bubble Time: 51.222 ms, Imbalance Overhead: 14.711 ms
GPU 1, Compute+Comm Time: 273.218 ms, Bubble Time: 51.536 ms, Imbalance Overhead: 12.216 ms
GPU 2, Compute+Comm Time: 273.218 ms, Bubble Time: 51.524 ms, Imbalance Overhead: 12.229 ms
GPU 3, Compute+Comm Time: 281.104 ms, Bubble Time: 52.560 ms, Imbalance Overhead: 3.306 ms
    The estimated cost with 2 DP ways is 546.564 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1489.180 ms
Partition 0 [0, 16) has cost: 1489.180 ms
Partition 1 [16, 32) has cost: 1462.216 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 229.681 ms
GPU 0, Compute+Comm Time: 198.266 ms, Bubble Time: 24.088 ms, Imbalance Overhead: 7.327 ms
GPU 1, Compute+Comm Time: 196.118 ms, Bubble Time: 24.901 ms, Imbalance Overhead: 8.662 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 372.930 ms
GPU 0, Compute+Comm Time: 323.062 ms, Bubble Time: 41.856 ms, Imbalance Overhead: 8.012 ms
GPU 1, Compute+Comm Time: 328.319 ms, Bubble Time: 40.295 ms, Imbalance Overhead: 4.316 ms
    The estimated cost with 4 DP ways is 632.742 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2951.396 ms
Partition 0 [0, 32) has cost: 2951.396 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 626.664 ms
GPU 0, Compute+Comm Time: 626.664 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 755.004 ms
GPU 0, Compute+Comm Time: 755.004 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1450.752 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ************ Start Scheduling the Tasks in a Pipelined Fashion ******

****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2465
	Epoch 50:	Loss 3.0756
	Epoch 75:	Loss 2.8707
	Epoch 100:	Loss 2.7572
	Epoch 125:	Loss 2.6239
	Epoch 150:	Loss 2.4507
	Epoch 175:	Loss 2.2965
	Epoch 200:	Loss 2.1869
	Epoch 225:	Loss 2.0433
	Epoch 250:	Loss 1.8593
	Epoch 275:	Loss 1.6794
	Epoch 300:	Loss 1.5504
	Epoch 325:	Loss 1.4604
	Epoch 350:	Loss 1.3538
	Epoch 375:	Loss 1.2809
	Epoch 400:	Loss 1.2179
	Epoch 425:	Loss 1.1387
	Epoch 450:	Loss 1.0813
	Epoch 475:	Loss 1.0305
	Epoch 500:	Loss 0.9900
	Epoch 525:	Loss 0.9505
	Epoch 550:	Loss 0.9142
	Epoch 575:	Loss 0.8863
	Epoch 600:	Loss 0.8567
	Epoch 625:	Loss 0.8321
	Epoch 650:	Loss 0.7881
	Epoch 675:	Loss 0.7703
	Epoch 700:	Loss 0.7433
	Epoch 725:	Loss 0.7275
	Epoch 750:	Loss 0.7070
	Epoch 775:	Loss 0.6873
	Epoch 800:	Loss 0.6733
	Epoch 825:	Loss 0.6587
	Epoch 850:	Loss 0.6388
	Epoch 875:	Loss 0.6296
	Epoch 900:	Loss 0.6217
	Epoch 925:	Loss 0.6289
	Epoch 950:	Loss 0.5994
	Epoch 975:	Loss 0.5898
	Epoch 1000:	Loss 0.5856
	Epoch 1025:	Loss 0.5784
	Epoch 1050:	Loss 0.5717
	Epoch 1075:	Loss 0.5629
	Epoch 1100:	Loss 0.5518
	Epoch 1125:	Loss 0.5617
	Epoch 1150:	Loss 0.5530
	Epoch 1175:	Loss 0.5453
	Epoch 1200:	Loss 0.5369
	Epoch 1225:	Loss 0.5306
	Epoch 1250:	Loss 0.5264
	Epoch 1275:	Loss 0.5218
	Epoch 1300:	Loss 0.5187
	Epoch 1325:	Loss 0.5172
	Epoch 1350:	Loss 0.5108
	Epoch 1375:	Loss 0.5067
	Epoch 1400:	Loss 0.4990
	Epoch 1425:	Loss 0.5008
	Epoch 1450:	Loss 0.4988
	Epoch 1475:	Loss 0.4917
	Epoch 1500:	Loss 0.4845
	Epoch 1525:	Loss 0.4953
	Epoch 1550:	Loss 0.4935
	Epoch 1575:	Loss 0.4829
	Epoch 1600:	Loss 0.4785
	Epoch 1625:	Loss 0.4748
	Epoch 1650:	Loss 0.4676
	Epoch 1675:	Loss 0.4689
	Epoch 1700:	Loss 0.4677
	Epoch 1725:	Loss 0.4677
	Epoch 1750:	Loss 0.4640
	Epoch 1775:	Loss 0.4581
	Epoch 1800:	Loss 0.4608
	Epoch 1825:	Loss 0.4595
	Epoch 1850:	Loss 0.4568
	Epoch 1875:	Loss 0.4579
	Epoch 1900:	Loss 0.4522
	Epoch 1925:	Loss 0.4561
	Epoch 1950:	Loss 0.4553
	Epoch 1975:	Loss 0.4474
	Epoch 2000:	Loss 0.4443
	Epoch 2025:	Loss 0.4464
	Epoch 2050:	Loss 0.4433
	Epoch 2075:	Loss 0.4411
	Epoch 2100:	Loss 0.4419
	Epoch 2125:	Loss 0.4418
	Epoch 2150:	Loss 0.4392
	Epoch 2175:	Loss 0.4332
	Epoch 2200:	Loss 0.4310
	Epoch 2225:	Loss 0.4349
	Epoch 2250:	Loss 0.4259
	Epoch 2275:	Loss 0.4257
	Epoch 2300:	Loss 0.4218
	Epoch 2325:	Loss 0.4337
	Epoch 2350:	Loss 0.4255
	Epoch 2375:	Loss 0.4224
	Epoch 2400:	Loss 0.4192
	Epoch 2425:	Loss 0.4264
	Epoch 2450:	Loss 0.4261
	Epoch 2475:	Loss 0.4235
	Epoch 2500:	Loss 0.4277
	Epoch 2525:	Loss 0.4468
	Epoch 2550:	Loss 0.4404
	Epoch 2575:	Loss 0.4371
	Epoch 2600:	Loss 0.4313
	Epoch 2625:	Loss 0.4903
	Epoch 2650:	Loss 0.4648
	Epoch 2675:	Loss 0.4484
	Epoch 2700:	Loss 0.4339
	Epoch 2725:	Loss 0.4272
	Epoch 2750:	Loss 0.4167
	Epoch 2775:	Loss 0.4116
	Epoch 2800:	Loss 0.4096
	Epoch 2825:	Loss 0.4071
	Epoch 2850:	Loss 0.4042
	Epoch 2875:	Loss 0.4041
	Epoch 2900:	Loss 0.4028
	Epoch 2925:	Loss 0.4023
	Epoch 2950:	Loss 0.4029
	Epoch 2975:	Loss 0.4021
	Epoch 3000:	Loss 0.3984
	Epoch 3025:	Loss 0.3971
	Epoch 3050:	Loss 0.3964
	Epoch 3075:	Loss 0.3954
	Epoch 3100:	Loss 0.3981
	Epoch 3125:	Loss 0.3971
	Epoch 3150:	Loss 0.3925
	Epoch 3175:	Loss 0.3902
	Epoch 3200:	Loss 0.3897
	Epoch 3225:	Loss 0.3918
	Epoch 3250:	Loss 0.3887
	Epoch 3275:	Loss 0.3884
	Epoch 3300:	Loss 0.3862
	Epoch 3325:	Loss 0.3885
	Epoch 3350:	Loss 0.3895
	Epoch 3375:	Loss 0.3911
	Epoch 3400:	Loss 0.3862
	Epoch 3425:	Loss 0.3886
	Epoch 3450:	Loss 0.3831
	Epoch 3475:	Loss 0.3843
	Epoch 3500:	Loss 0.3831
	Epoch 3525:	Loss 0.3808
	Epoch 3550:	Loss 0.3785
	Epoch 3575:	Loss 0.3771
	Epoch 3600:	Loss 0.3790
	Epoch 3625:	Loss 0.3795
	Epoch 3650:	Loss 0.3768
	Epoch 3675:	Loss 0.3771
	Epoch 3700:	Loss 0.3784
	Epoch 3725:	Loss 0.3763
	Epoch 3750:	Loss 0.3730
	Epoch 3775:	Loss 0.3733
	Epoch 3800:	Loss 0.3723
	Epoch 3825:	Loss 0.3752
	Epoch 3850:	Loss 0.3716
	Epoch 3875:	Loss 0.3724
	Epoch 3900:	Loss 0.3706
	Epoch 3925:	Loss 0.3755
	Epoch 3950:	Loss 0.3811
	Epoch 3975:	Loss 0.3731
	Epoch 4000:	Loss 0.3729
	Epoch 4025:	Loss 0.3742
	Epoch 4050:	Loss 0.3672
	Epoch 4075:	Loss 0.3685
	Epoch 4100:	Loss 0.3681
	Epoch 4125:	Loss 0.3769
	Epoch 4150:	Loss 0.3675
	Epoch 4175:	Loss 0.3713
	Epoch 4200:	Loss 0.3676
	Epoch 4225:	Loss 0.3889
	Epoch 4250:	Loss 0.3757
	Epoch 4275:	Loss 0.3735
	Epoch 4300:	Loss 0.3695
	Epoch 4325:	Loss 0.3753
	Epoch 4350:	Loss 0.3679
	Epoch 4375:	Loss 0.3663
	Epoch 4400:	Loss 0.3617
	Epoch 4425:	Loss 0.3623
	Epoch 4450:	Loss 0.3596
	Epoch 4475:	Loss 0.3589
	Epoch 4500:	Loss 0.3600
	Epoch 4525:	Loss 0.3575
	Epoch 4550:	Loss 0.3586
	Epoch 4575:	Loss 0.3560
	Epoch 4600:	Loss 0.3550
	Epoch 4625:	Loss 0.3537
	Epoch 4650:	Loss 0.3552
	Epoch 4675:	Loss 0.3584
	Epoch 4700:	Loss 0.3529
	Epoch 4725:	Loss 0.3599
	Epoch 4750:	Loss 0.3592
	Epoch 4775:	Loss 0.3534
	Epoch 4800:	Loss 0.3519
	Epoch 4825:	Loss 0.3524
	Epoch 4850:	Loss 0.3543
	Epoch 4875:	Loss 0.3500
	Epoch 4900:	Loss 0.3500
	Epoch 4925:	Loss 0.3502
	Epoch 4950:	Loss 0.3504
	Epoch 4975:	Loss 0.3479
	Epoch 5000:	Loss 0.3453
Node 2, Pre/Post-Pipelining: 1.104 / 1.099 ms, Bubble: 103.363 ms, Compute: 380.003 ms, Comm: 30.138 ms, Imbalance: 40.781 ms
Node 0, Pre/Post-Pipelining: 1.107 / 1.163 ms, Bubble: 101.533 ms, Compute: 425.210 ms, Comm: 16.804 ms, Imbalance: 9.714 ms
Node 7, Pre/Post-Pipelining: 1.102 / 16.467 ms, Bubble: 89.694 ms, Compute: 408.858 ms, Comm: 17.346 ms, Imbalance: 22.572 ms
Node 1, Pre/Post-Pipelining: 1.107 / 1.112 ms, Bubble: 102.190 ms, Compute: 399.091 ms, Comm: 27.933 ms, Imbalance: 24.565 ms
Node 4, Pre/Post-Pipelining: 1.099 / 1.183 ms, Bubble: 103.987 ms, Compute: 378.891 ms, Comm: 27.098 ms, Imbalance: 44.506 ms
Node 3, Pre/Post-Pipelining: 1.100 / 1.168 ms, Bubble: 103.049 ms, Compute: 388.255 ms, Comm: 26.971 ms, Imbalance: 35.621 ms
Node 5, Pre/Post-Pipelining: 1.099 / 1.166 ms, Bubble: 104.139 ms, Compute: 389.485 ms, Comm: 31.191 ms, Imbalance: 29.250 ms
Node 6, Pre/Post-Pipelining: 1.101 / 1.141 ms, Bubble: 104.545 ms, Compute: 388.847 ms, Comm: 29.593 ms, Imbalance: 31.113 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.107 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.163 ms
Cluster-Wide Average, Bubble: 101.533 ms
Cluster-Wide Average, Compute: 425.210 ms
Cluster-Wide Average, Communication: 16.804 ms
Cluster-Wide Average, Imbalance: 9.714 ms
Node 0, GPU memory consumption: 6.616 GB
Node 2, GPU memory consumption: 5.260 GB
Node 1, GPU memory consumption: 5.260 GB
Node 3, GPU memory consumption: 5.237 GB
Node 5, GPU memory consumption: 5.260 GB
Node 7, GPU memory consumption: 5.042 GB
Node 4, GPU memory consumption: 5.237 GB
Node 6, GPU memory consumption: 5.260 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.556203 s---------------
------------------------node id 1,  per-epoch time: 0.556203 s---------------
------------------------node id 4,  per-epoch time: 0.556203 s---------------
------------------------node id 2,  per-epoch time: 0.556203 s---------------
------------------------node id 5,  per-epoch time: 0.556203 s---------------
------------------------node id 3,  per-epoch time: 0.556203 s---------------
------------------------node id 6,  per-epoch time: 0.556203 s---------------
------------------------node id 7,  per-epoch time: 0.556203 s---------------
************ Profiling Results ************
	Bubble: 162.193603 (ms) (29.16 percentage)
	Compute: 389.468627 (ms) (70.01 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.638166 (ms) (0.83 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 403.209 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
