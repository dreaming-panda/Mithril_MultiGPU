Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.062 seconds.
Building the CSC structure...
        It takes 2.084 seconds.
Building the CSC structure...
        It takes 2.368 seconds.
Building the CSC structure...
        It takes 2.413 seconds.
Building the CSC structure...
        It takes 2.417 seconds.
Building the CSC structure...
        It takes 2.452 seconds.
Building the CSC structure...
        It takes 2.615 seconds.
Building the CSC structure...
        It takes 2.650 seconds.
Building the CSC structure...
        It takes 1.852 seconds.
        It takes 1.896 seconds.
        It takes 2.290 seconds.
        It takes 2.333 seconds.
        It takes 2.328 seconds.
        It takes 2.395 seconds.
        It takes 2.254 seconds.
        It takes 2.304 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.248 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.298 seconds.
Building the Label Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.037 seconds.
        It takes 0.308 seconds.
Building the Label Vector...
        It takes 0.287 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.037 seconds.
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.031 seconds.
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.026 Gbps (per GPU), 456.212 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.759 Gbps (per GPU), 454.070 Gbps (aggregated)
The layer-level communication performance: 56.740 Gbps (per GPU), 453.921 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.552 Gbps (per GPU), 452.420 Gbps (aggregated)
The layer-level communication performance: 56.512 Gbps (per GPU), 452.094 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.320 Gbps (per GPU), 450.563 Gbps (aggregated)
The layer-level communication performance: 56.280 Gbps (per GPU), 450.243 Gbps (aggregated)
The layer-level communication performance: 56.247 Gbps (per GPU), 449.976 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.951 Gbps (per GPU), 1279.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.926 Gbps (per GPU), 1279.407 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.944 Gbps (per GPU), 1279.556 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.929 Gbps (per GPU), 1279.434 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.941 Gbps (per GPU), 1279.531 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.938 Gbps (per GPU), 1279.507 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.941 Gbps (per GPU), 1279.531 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.941 Gbps (per GPU), 1279.530 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.011 Gbps (per GPU), 832.086 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.011 Gbps (per GPU), 832.087 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.011 Gbps (per GPU), 832.086 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.012 Gbps (per GPU), 832.093 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.010 Gbps (per GPU), 832.079 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.008 Gbps (per GPU), 832.066 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.008 Gbps (per GPU), 832.066 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.007 Gbps (per GPU), 832.059 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.768 Gbps (per GPU), 262.144 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.768 Gbps (per GPU), 262.142 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.767 Gbps (per GPU), 262.139 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.767 Gbps (per GPU), 262.137 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.767 Gbps (per GPU), 262.138 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.767 Gbps (per GPU), 262.135 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.768 Gbps (per GPU), 262.143 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.767 Gbps (per GPU), 262.139 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.26ms  2.67ms  2.41ms  1.35  8.38K  3.53M
 chk_1  3.54ms  2.93ms  2.75ms  1.29  6.74K  3.60M
 chk_2  3.48ms  2.79ms  2.63ms  1.32  7.27K  3.53M
 chk_3  3.51ms  2.86ms  2.65ms  1.33  7.92K  3.61M
 chk_4  3.23ms  2.76ms  2.63ms  1.23  5.33K  3.68M
 chk_5  3.62ms  2.79ms  2.49ms  1.45 10.07K  3.45M
 chk_6  3.74ms  2.96ms  2.66ms  1.41  9.41K  3.48M
 chk_7  3.44ms  2.79ms  2.56ms  1.34  8.12K  3.60M
 chk_8  3.41ms  2.87ms  2.70ms  1.26  6.09K  3.64M
 chk_9  3.66ms  2.74ms  2.39ms  1.54 11.10K  3.38M
chk_10  3.39ms  2.90ms  2.75ms  1.23  5.67K  3.63M
chk_11  3.45ms  2.80ms  2.56ms  1.35  8.16K  3.54M
chk_12  3.63ms  2.97ms  2.77ms  1.31  7.24K  3.55M
chk_13  3.27ms  2.81ms  2.65ms  1.23  5.41K  3.68M
chk_14  3.69ms  3.05ms  2.83ms  1.30  7.14K  3.53M
chk_15  3.72ms  2.96ms  2.65ms  1.40  9.25K  3.49M
chk_16  3.19ms  2.75ms  2.59ms  1.23  4.78K  3.77M
chk_17  3.46ms  2.87ms  2.67ms  1.29  6.85K  3.60M
chk_18  3.33ms  2.79ms  2.49ms  1.34  7.47K  3.57M
chk_19  3.17ms  2.73ms  2.60ms  1.22  4.88K  3.75M
chk_20  3.38ms  2.75ms  2.57ms  1.32  7.00K  3.63M
chk_21  3.23ms  2.73ms  2.56ms  1.26  5.41K  3.68M
chk_22  3.95ms  2.98ms  2.61ms  1.51 11.07K  3.39M
chk_23  3.53ms  2.83ms  2.63ms  1.34  7.23K  3.64M
chk_24  3.75ms  2.93ms  2.61ms  1.44 10.13K  3.43M
chk_25  3.26ms  2.69ms  2.53ms  1.29  6.40K  3.57M
chk_26  3.41ms  2.89ms  2.73ms  1.25  5.78K  3.55M
chk_27  3.57ms  2.88ms  2.54ms  1.40  9.34K  3.48M
chk_28  3.65ms  3.10ms  2.90ms  1.26  6.37K  3.57M
chk_29  3.38ms  2.91ms  2.74ms  1.23  5.16K  3.78M
chk_30  3.25ms  2.80ms  2.68ms  1.21  5.44K  3.67M
chk_31  3.52ms  2.94ms  2.76ms  1.28  6.33K  3.63M
   Avg  3.47  2.85  2.63
   Max  3.95  3.10  2.90
   Min  3.17  2.67  2.39
 Ratio  1.24  1.16  1.21
   Var  0.04  0.01  0.01
Profiling takes 3.335 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 384.638 ms
Partition 0 [0, 4) has cost: 384.638 ms
Partition 1 [4, 8) has cost: 364.769 ms
Partition 2 [8, 12) has cost: 364.769 ms
Partition 3 [12, 16) has cost: 364.769 ms
Partition 4 [16, 20) has cost: 364.769 ms
Partition 5 [20, 24) has cost: 364.769 ms
Partition 6 [24, 28) has cost: 364.769 ms
Partition 7 [28, 32) has cost: 357.875 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 172.661 ms
GPU 0, Compute+Comm Time: 138.923 ms, Bubble Time: 30.317 ms, Imbalance Overhead: 3.422 ms
GPU 1, Compute+Comm Time: 133.202 ms, Bubble Time: 30.036 ms, Imbalance Overhead: 9.423 ms
GPU 2, Compute+Comm Time: 133.202 ms, Bubble Time: 30.014 ms, Imbalance Overhead: 9.445 ms
GPU 3, Compute+Comm Time: 133.202 ms, Bubble Time: 29.839 ms, Imbalance Overhead: 9.620 ms
GPU 4, Compute+Comm Time: 133.202 ms, Bubble Time: 29.830 ms, Imbalance Overhead: 9.629 ms
GPU 5, Compute+Comm Time: 133.202 ms, Bubble Time: 29.831 ms, Imbalance Overhead: 9.628 ms
GPU 6, Compute+Comm Time: 133.202 ms, Bubble Time: 30.037 ms, Imbalance Overhead: 9.423 ms
GPU 7, Compute+Comm Time: 130.829 ms, Bubble Time: 30.488 ms, Imbalance Overhead: 11.345 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 336.383 ms
GPU 0, Compute+Comm Time: 253.192 ms, Bubble Time: 59.679 ms, Imbalance Overhead: 23.513 ms
GPU 1, Compute+Comm Time: 257.713 ms, Bubble Time: 58.740 ms, Imbalance Overhead: 19.931 ms
GPU 2, Compute+Comm Time: 257.713 ms, Bubble Time: 58.290 ms, Imbalance Overhead: 20.380 ms
GPU 3, Compute+Comm Time: 257.713 ms, Bubble Time: 58.294 ms, Imbalance Overhead: 20.376 ms
GPU 4, Compute+Comm Time: 257.713 ms, Bubble Time: 58.224 ms, Imbalance Overhead: 20.447 ms
GPU 5, Compute+Comm Time: 257.713 ms, Bubble Time: 58.498 ms, Imbalance Overhead: 20.172 ms
GPU 6, Compute+Comm Time: 257.713 ms, Bubble Time: 58.510 ms, Imbalance Overhead: 20.160 ms
GPU 7, Compute+Comm Time: 271.861 ms, Bubble Time: 59.136 ms, Imbalance Overhead: 5.386 ms
The estimated cost of the whole pipeline: 534.496 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 749.407 ms
Partition 0 [0, 8) has cost: 749.407 ms
Partition 1 [8, 16) has cost: 729.538 ms
Partition 2 [16, 24) has cost: 729.538 ms
Partition 3 [24, 32) has cost: 722.644 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 179.782 ms
GPU 0, Compute+Comm Time: 148.479 ms, Bubble Time: 28.034 ms, Imbalance Overhead: 3.269 ms
GPU 1, Compute+Comm Time: 145.377 ms, Bubble Time: 27.492 ms, Imbalance Overhead: 6.913 ms
GPU 2, Compute+Comm Time: 145.377 ms, Bubble Time: 27.331 ms, Imbalance Overhead: 7.075 ms
GPU 3, Compute+Comm Time: 144.227 ms, Bubble Time: 27.467 ms, Imbalance Overhead: 8.088 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 335.553 ms
GPU 0, Compute+Comm Time: 269.502 ms, Bubble Time: 50.998 ms, Imbalance Overhead: 15.054 ms
GPU 1, Compute+Comm Time: 271.713 ms, Bubble Time: 51.357 ms, Imbalance Overhead: 12.484 ms
GPU 2, Compute+Comm Time: 271.713 ms, Bubble Time: 51.440 ms, Imbalance Overhead: 12.400 ms
GPU 3, Compute+Comm Time: 279.480 ms, Bubble Time: 52.563 ms, Imbalance Overhead: 3.511 ms
    The estimated cost with 2 DP ways is 541.103 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1478.946 ms
Partition 0 [0, 16) has cost: 1478.946 ms
Partition 1 [16, 32) has cost: 1452.183 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 223.579 ms
GPU 0, Compute+Comm Time: 195.207 ms, Bubble Time: 24.004 ms, Imbalance Overhead: 4.369 ms
GPU 1, Compute+Comm Time: 193.109 ms, Bubble Time: 24.653 ms, Imbalance Overhead: 5.818 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 371.977 ms
GPU 0, Compute+Comm Time: 321.968 ms, Bubble Time: 41.324 ms, Imbalance Overhead: 8.685 ms
GPU 1, Compute+Comm Time: 327.176 ms, Bubble Time: 40.253 ms, Imbalance Overhead: 4.547 ms
    The estimated cost with 4 DP ways is 625.334 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2931.128 ms
Partition 0 [0, 32) has cost: 2931.128 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 582.659 ms
GPU 0, Compute+Comm Time: 582.659 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 715.764 ms
GPU 0, Compute+Comm Time: 715.764 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1363.344 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
