Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 7 on machine gnerv2
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.915 seconds.
Building the CSC structure...
        It takes 2.070 seconds.
Building the CSC structure...
        It takes 2.138 seconds.
Building the CSC structure...
        It takes 2.167 seconds.
Building the CSC structure...
        It takes 2.420 seconds.
Building the CSC structure...
        It takes 2.434 seconds.
Building the CSC structure...
        It takes 2.637 seconds.
Building the CSC structure...
        It takes 2.646 seconds.
Building the CSC structure...
        It takes 1.844 seconds.
        It takes 1.892 seconds.
        It takes 1.833 seconds.
        It takes 2.260 seconds.
        It takes 2.287 seconds.
Building the Feature Vector...
        It takes 2.363 seconds.
        It takes 2.381 seconds.
        It takes 2.373 seconds.
        It takes 0.287 seconds.
Building the Label Vector...
        It takes 0.043 seconds.
Building the Feature Vector...
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.274 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Building the Feature Vector...
        It takes 0.033 seconds.
        It takes 0.290 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Building the Feature Vector...
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.934 Gbps (per GPU), 447.471 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.682 Gbps (per GPU), 445.458 Gbps (aggregated)
The layer-level communication performance: 55.677 Gbps (per GPU), 445.415 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.453 Gbps (per GPU), 443.628 Gbps (aggregated)
The layer-level communication performance: 55.424 Gbps (per GPU), 443.389 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.254 Gbps (per GPU), 442.029 Gbps (aggregated)
The layer-level communication performance: 55.209 Gbps (per GPU), 441.669 Gbps (aggregated)
The layer-level communication performance: 55.181 Gbps (per GPU), 441.448 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.896 Gbps (per GPU), 1279.166 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.957 Gbps (per GPU), 1279.653 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.899 Gbps (per GPU), 1279.190 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.975 Gbps (per GPU), 1279.800 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.896 Gbps (per GPU), 1279.166 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.884 Gbps (per GPU), 1279.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.902 Gbps (per GPU), 1279.214 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.902 Gbps (per GPU), 1279.214 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.581 Gbps (per GPU), 828.648 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.584 Gbps (per GPU), 828.674 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.582 Gbps (per GPU), 828.655 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.580 Gbps (per GPU), 828.641 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.583 Gbps (per GPU), 828.661 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.579 Gbps (per GPU), 828.634 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.580 Gbps (per GPU), 828.641 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.580 Gbps (per GPU), 828.641 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.375 Gbps (per GPU), 258.997 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.375 Gbps (per GPU), 258.998 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.375 Gbps (per GPU), 258.998 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.374 Gbps (per GPU), 258.992 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.374 Gbps (per GPU), 258.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.374 Gbps (per GPU), 258.991 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.375 Gbps (per GPU), 258.996 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.374 Gbps (per GPU), 258.993 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.28ms  2.68ms  2.44ms  1.34  8.38K  3.53M
 chk_1  3.55ms  2.92ms  2.72ms  1.31  6.74K  3.60M
 chk_2  3.48ms  2.82ms  2.62ms  1.33  7.27K  3.53M
 chk_3  3.51ms  2.87ms  2.64ms  1.33  7.92K  3.61M
 chk_4  3.23ms  2.77ms  2.63ms  1.23  5.33K  3.68M
 chk_5  3.61ms  2.81ms  2.49ms  1.45 10.07K  3.45M
 chk_6  3.76ms  2.97ms  2.67ms  1.41  9.41K  3.48M
 chk_7  3.43ms  2.79ms  2.58ms  1.33  8.12K  3.60M
 chk_8  3.42ms  2.87ms  2.72ms  1.26  6.09K  3.64M
 chk_9  3.65ms  2.75ms  2.40ms  1.52 11.10K  3.38M
chk_10  3.39ms  2.92ms  2.77ms  1.22  5.67K  3.63M
chk_11  3.47ms  2.83ms  2.58ms  1.35  8.16K  3.54M
chk_12  3.65ms  2.99ms  2.79ms  1.31  7.24K  3.55M
chk_13  3.29ms  2.80ms  2.67ms  1.23  5.41K  3.68M
chk_14  3.70ms  3.05ms  2.85ms  1.30  7.14K  3.53M
chk_15  3.72ms  2.95ms  2.66ms  1.40  9.25K  3.49M
chk_16  3.18ms  2.75ms  2.61ms  1.22  4.78K  3.77M
chk_17  3.46ms  2.87ms  2.67ms  1.30  6.85K  3.60M
chk_18  3.34ms  2.68ms  2.48ms  1.35  7.47K  3.57M
chk_19  3.19ms  2.73ms  2.60ms  1.23  4.88K  3.75M
chk_20  3.38ms  2.78ms  2.56ms  1.32  7.00K  3.63M
chk_21  3.26ms  2.73ms  2.57ms  1.27  5.41K  3.68M
chk_22  3.96ms  3.00ms  2.63ms  1.51 11.07K  3.39M
chk_23  3.54ms  2.83ms  2.64ms  1.34  7.23K  3.64M
chk_24  3.76ms  2.97ms  2.60ms  1.44 10.13K  3.43M
chk_25  3.26ms  2.73ms  2.52ms  1.30  6.40K  3.57M
chk_26  3.45ms  2.90ms  2.73ms  1.27  5.78K  3.55M
chk_27  3.61ms  2.91ms  2.54ms  1.42  9.34K  3.48M
chk_28  3.68ms  3.11ms  2.88ms  1.28  6.37K  3.57M
chk_29  3.40ms  2.91ms  2.76ms  1.23  5.16K  3.78M
chk_30  3.27ms  2.78ms  2.69ms  1.22  5.44K  3.67M
chk_31  3.50ms  2.92ms  2.77ms  1.26  6.33K  3.63M
   Avg  3.48  2.86  2.64
   Max  3.96  3.11  2.88
   Min  3.18  2.68  2.40
 Ratio  1.25  1.16  1.20
   Var  0.04  0.01  0.01
Profiling takes 3.344 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 385.557 ms
Partition 0 [0, 4) has cost: 385.557 ms
Partition 1 [4, 8) has cost: 365.590 ms
Partition 2 [8, 12) has cost: 365.590 ms
Partition 3 [12, 16) has cost: 365.590 ms
Partition 4 [16, 20) has cost: 365.590 ms
Partition 5 [20, 24) has cost: 365.590 ms
Partition 6 [24, 28) has cost: 365.590 ms
Partition 7 [28, 32) has cost: 358.647 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.414 ms
GPU 0, Compute+Comm Time: 139.382 ms, Bubble Time: 30.306 ms, Imbalance Overhead: 3.726 ms
GPU 1, Compute+Comm Time: 133.488 ms, Bubble Time: 29.989 ms, Imbalance Overhead: 9.937 ms
GPU 2, Compute+Comm Time: 133.488 ms, Bubble Time: 29.948 ms, Imbalance Overhead: 9.978 ms
GPU 3, Compute+Comm Time: 133.488 ms, Bubble Time: 29.806 ms, Imbalance Overhead: 10.120 ms
GPU 4, Compute+Comm Time: 133.488 ms, Bubble Time: 29.819 ms, Imbalance Overhead: 10.107 ms
GPU 5, Compute+Comm Time: 133.488 ms, Bubble Time: 29.831 ms, Imbalance Overhead: 10.095 ms
GPU 6, Compute+Comm Time: 133.488 ms, Bubble Time: 30.090 ms, Imbalance Overhead: 9.836 ms
GPU 7, Compute+Comm Time: 131.121 ms, Bubble Time: 30.591 ms, Imbalance Overhead: 11.702 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 337.680 ms
GPU 0, Compute+Comm Time: 254.182 ms, Bubble Time: 59.968 ms, Imbalance Overhead: 23.530 ms
GPU 1, Compute+Comm Time: 258.758 ms, Bubble Time: 58.981 ms, Imbalance Overhead: 19.940 ms
GPU 2, Compute+Comm Time: 258.758 ms, Bubble Time: 58.449 ms, Imbalance Overhead: 20.473 ms
GPU 3, Compute+Comm Time: 258.758 ms, Bubble Time: 58.417 ms, Imbalance Overhead: 20.505 ms
GPU 4, Compute+Comm Time: 258.758 ms, Bubble Time: 58.337 ms, Imbalance Overhead: 20.584 ms
GPU 5, Compute+Comm Time: 258.758 ms, Bubble Time: 58.599 ms, Imbalance Overhead: 20.322 ms
GPU 6, Compute+Comm Time: 258.758 ms, Bubble Time: 58.642 ms, Imbalance Overhead: 20.279 ms
GPU 7, Compute+Comm Time: 272.830 ms, Bubble Time: 59.227 ms, Imbalance Overhead: 5.622 ms
The estimated cost of the whole pipeline: 536.648 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 751.147 ms
Partition 0 [0, 8) has cost: 751.147 ms
Partition 1 [8, 16) has cost: 731.180 ms
Partition 2 [16, 24) has cost: 731.180 ms
Partition 3 [24, 32) has cost: 724.237 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 179.614 ms
GPU 0, Compute+Comm Time: 148.816 ms, Bubble Time: 28.049 ms, Imbalance Overhead: 2.748 ms
GPU 1, Compute+Comm Time: 145.605 ms, Bubble Time: 27.514 ms, Imbalance Overhead: 6.495 ms
GPU 2, Compute+Comm Time: 145.605 ms, Bubble Time: 27.329 ms, Imbalance Overhead: 6.679 ms
GPU 3, Compute+Comm Time: 144.466 ms, Bubble Time: 27.053 ms, Imbalance Overhead: 8.094 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 337.030 ms
GPU 0, Compute+Comm Time: 270.839 ms, Bubble Time: 51.073 ms, Imbalance Overhead: 15.118 ms
GPU 1, Compute+Comm Time: 273.109 ms, Bubble Time: 51.414 ms, Imbalance Overhead: 12.507 ms
GPU 2, Compute+Comm Time: 273.109 ms, Bubble Time: 51.476 ms, Imbalance Overhead: 12.445 ms
GPU 3, Compute+Comm Time: 280.819 ms, Bubble Time: 52.577 ms, Imbalance Overhead: 3.634 ms
    The estimated cost with 2 DP ways is 542.476 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1482.327 ms
Partition 0 [0, 16) has cost: 1482.327 ms
Partition 1 [16, 32) has cost: 1455.417 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 224.647 ms
GPU 0, Compute+Comm Time: 195.793 ms, Bubble Time: 24.042 ms, Imbalance Overhead: 4.812 ms
GPU 1, Compute+Comm Time: 193.622 ms, Bubble Time: 24.907 ms, Imbalance Overhead: 6.117 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 373.162 ms
GPU 0, Compute+Comm Time: 323.338 ms, Bubble Time: 41.765 ms, Imbalance Overhead: 8.059 ms
GPU 1, Compute+Comm Time: 328.536 ms, Bubble Time: 40.293 ms, Imbalance Overhead: 4.333 ms
    The estimated cost with 4 DP ways is 627.699 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2937.744 ms
Partition 0 [0, 32) has cost: 2937.744 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 588.204 ms
GPU 0, Compute+Comm Time: 588.204 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 721.480 ms
GPU 0, Compute+Comm Time: 721.480 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1375.168 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.3317
	Epoch 50:	Loss 3.1250
	Epoch 75:	Loss 2.8948
	Epoch 100:	Loss 2.7621
	Epoch 125:	Loss 2.6828
	Epoch 150:	Loss 2.6271
	Epoch 175:	Loss 2.5666
	Epoch 200:	Loss 2.3845
	Epoch 225:	Loss 2.2092
	Epoch 250:	Loss 2.1049
	Epoch 275:	Loss 1.9748
	Epoch 300:	Loss 1.8380
	Epoch 325:	Loss 1.6363
	Epoch 350:	Loss 1.5274
	Epoch 375:	Loss 1.4393
	Epoch 400:	Loss 1.3427
	Epoch 425:	Loss 1.2423
	Epoch 450:	Loss 1.1769
	Epoch 475:	Loss 1.1012
	Epoch 500:	Loss 1.0433
	Epoch 525:	Loss 0.9796
	Epoch 550:	Loss 0.9486
	Epoch 575:	Loss 0.9060
	Epoch 600:	Loss 0.8772
	Epoch 625:	Loss 0.8532
	Epoch 650:	Loss 0.8117
	Epoch 675:	Loss 0.7830
	Epoch 700:	Loss 0.7562
	Epoch 725:	Loss 0.7217
	Epoch 750:	Loss 0.6987
	Epoch 775:	Loss 0.6769
	Epoch 800:	Loss 0.6601
	Epoch 825:	Loss 0.6463
	Epoch 850:	Loss 0.6370
	Epoch 875:	Loss 0.6194
	Epoch 900:	Loss 0.6080
	Epoch 925:	Loss 0.5945
	Epoch 950:	Loss 0.5875
	Epoch 975:	Loss 0.5738
	Epoch 1000:	Loss 0.5669
	Epoch 1025:	Loss 0.5703
	Epoch 1050:	Loss 0.5587
	Epoch 1075:	Loss 0.5390
	Epoch 1100:	Loss 0.5310
	Epoch 1125:	Loss 0.5262
	Epoch 1150:	Loss 0.5198
	Epoch 1175:	Loss 0.5154
	Epoch 1200:	Loss 0.5081
	Epoch 1225:	Loss 0.4927
	Epoch 1250:	Loss 0.4914
	Epoch 1275:	Loss 0.4847
	Epoch 1300:	Loss 0.4771
	Epoch 1325:	Loss 0.4748
	Epoch 1350:	Loss 0.4661
	Epoch 1375:	Loss 0.4616
	Epoch 1400:	Loss 0.4595
	Epoch 1425:	Loss 0.4572
	Epoch 1450:	Loss 0.4501
	Epoch 1475:	Loss 0.4487
	Epoch 1500:	Loss 0.4416
	Epoch 1525:	Loss 0.4471
	Epoch 1550:	Loss 0.4332
	Epoch 1575:	Loss 0.4327
	Epoch 1600:	Loss 0.4284
	Epoch 1625:	Loss 0.4306
	Epoch 1650:	Loss 0.4242
	Epoch 1675:	Loss 0.4214
	Epoch 1700:	Loss 0.4189
	Epoch 1725:	Loss 0.4172
	Epoch 1750:	Loss 0.4173
	Epoch 1775:	Loss 0.4146
	Epoch 1800:	Loss 0.4103
	Epoch 1825:	Loss 0.4097
	Epoch 1850:	Loss 0.4049
	Epoch 1875:	Loss 0.4037
	Epoch 1900:	Loss 0.4059
	Epoch 1925:	Loss 0.4089
	Epoch 1950:	Loss 0.4058
	Epoch 1975:	Loss 0.4032
	Epoch 2000:	Loss 0.4002
	Epoch 2025:	Loss 0.4039
	Epoch 2050:	Loss 0.3966
	Epoch 2075:	Loss 0.3945
	Epoch 2100:	Loss 0.3924
	Epoch 2125:	Loss 0.3931
	Epoch 2150:	Loss 0.3881
	Epoch 2175:	Loss 0.3879
	Epoch 2200:	Loss 0.3834
	Epoch 2225:	Loss 0.3902
	Epoch 2250:	Loss 0.3849
	Epoch 2275:	Loss 0.3827
	Epoch 2300:	Loss 0.3831
	Epoch 2325:	Loss 0.3844
	Epoch 2350:	Loss 0.3801
	Epoch 2375:	Loss 0.3785
	Epoch 2400:	Loss 0.3777
	Epoch 2425:	Loss 0.3790
	Epoch 2450:	Loss 0.3760
	Epoch 2475:	Loss 0.3775
	Epoch 2500:	Loss 0.3721
	Epoch 2525:	Loss 0.3704
	Epoch 2550:	Loss 0.3701
	Epoch 2575:	Loss 0.3708
	Epoch 2600:	Loss 0.3662
	Epoch 2625:	Loss 0.3686
	Epoch 2650:	Loss 0.3683
	Epoch 2675:	Loss 0.3694
	Epoch 2700:	Loss 0.3676
	Epoch 2725:	Loss 0.3695
	Epoch 2750:	Loss 0.3654
	Epoch 2775:	Loss 0.3664
	Epoch 2800:	Loss 0.3664
	Epoch 2825:	Loss 0.3701
	Epoch 2850:	Loss 0.3649
	Epoch 2875:	Loss 0.3617
	Epoch 2900:	Loss 0.3644
	Epoch 2925:	Loss 0.3668
	Epoch 2950:	Loss 0.3649
	Epoch 2975:	Loss 0.3635
	Epoch 3000:	Loss 0.3607
	Epoch 3025:	Loss 0.3610
	Epoch 3050:	Loss 0.3624
	Epoch 3075:	Loss 0.3577
	Epoch 3100:	Loss 0.3570
	Epoch 3125:	Loss 0.3598
	Epoch 3150:	Loss 0.3557
	Epoch 3175:	Loss 0.3595
	Epoch 3200:	Loss 0.3537
	Epoch 3225:	Loss 0.3562
	Epoch 3250:	Loss 0.3522
	Epoch 3275:	Loss 0.3511
	Epoch 3300:	Loss 0.3538
	Epoch 3325:	Loss 0.3542
	Epoch 3350:	Loss 0.3532
	Epoch 3375:	Loss 0.3576
	Epoch 3400:	Loss 0.3545
	Epoch 3425:	Loss 0.3586
	Epoch 3450:	Loss 0.3547
	Epoch 3475:	Loss 0.3541
	Epoch 3500:	Loss 0.3516
	Epoch 3525:	Loss 0.3540
	Epoch 3550:	Loss 0.3494
	Epoch 3575:	Loss 0.3496
	Epoch 3600:	Loss 0.3467
	Epoch 3625:	Loss 0.3498
	Epoch 3650:	Loss 0.3484
	Epoch 3675:	Loss 0.3501
	Epoch 3700:	Loss 0.3460
	Epoch 3725:	Loss 0.3538
	Epoch 3750:	Loss 0.3529
	Epoch 3775:	Loss 0.3502
	Epoch 3800:	Loss 0.3559
	Epoch 3825:	Loss 0.3896
	Epoch 3850:	Loss 0.3773
	Epoch 3875:	Loss 0.3653
	Epoch 3900:	Loss 0.3612
	Epoch 3925:	Loss 0.3593
	Epoch 3950:	Loss 0.3478
	Epoch 3975:	Loss 0.3491
	Epoch 4000:	Loss 0.3485
	Epoch 4025:	Loss 0.3644
	Epoch 4050:	Loss 0.3588
	Epoch 4075:	Loss 0.3613
	Epoch 4100:	Loss 0.3535
	Epoch 4125:	Loss 0.3619
	Epoch 4150:	Loss 0.3581
	Epoch 4175:	Loss 0.3519
	Epoch 4200:	Loss 0.3467
	Epoch 4225:	Loss 0.3462
	Epoch 4250:	Loss 0.3433
	Epoch 4275:	Loss 0.3433
	Epoch 4300:	Loss 0.3405
	Epoch 4325:	Loss 0.3386
	Epoch 4350:	Loss 0.3378
	Epoch 4375:	Loss 0.3395
	Epoch 4400:	Loss 0.3402
	Epoch 4425:	Loss 0.3381
	Epoch 4450:	Loss 0.3377
	Epoch 4475:	Loss 0.3375
	Epoch 4500:	Loss 0.3377
	Epoch 4525:	Loss 0.3373
	Epoch 4550:	Loss 0.3355
	Epoch 4575:	Loss 0.3373
	Epoch 4600:	Loss 0.3349
	Epoch 4625:	Loss 0.3353
	Epoch 4650:	Loss 0.3357
	Epoch 4675:	Loss 0.3328
	Epoch 4700:	Loss 0.3357
	Epoch 4725:	Loss 0.3334
	Epoch 4750:	Loss 0.3337
	Epoch 4775:	Loss 0.3326
	Epoch 4800:	Loss 0.3322
	Epoch 4825:	Loss 0.3354
	Epoch 4850:	Loss 0.3356
	Epoch 4875:	Loss 0.3370
	Epoch 4900:	Loss 0.3349
	Epoch 4925:	Loss 0.3437
	Epoch 4950:	Loss 0.3377
	Epoch 4975:	Loss 0.3362
	Epoch 5000:	Loss 0.3310
Node 1, Pre/Post-Pipelining: 1.110 / 1.151 ms, Bubble: 102.353 ms, Compute: 399.584 ms, Comm: 28.116 ms, Imbalance: 25.410 ms
Node 2, Pre/Post-Pipelining: 1.106 / 1.171 ms, Bubble: 103.511 ms, Compute: 382.388 ms, Comm: 30.226 ms, Imbalance: 39.776 ms
Node 3, Pre/Post-Pipelining: 1.107 / 1.137 ms, Bubble: 103.455 ms, Compute: 387.640 ms, Comm: 26.872 ms, Imbalance: 37.705 ms
Node 0, Pre/Post-Pipelining: 1.107 / 1.283 ms, Bubble: 101.483 ms, Compute: 427.820 ms, Comm: 16.894 ms, Imbalance: 8.624 ms
Node 6, Pre/Post-Pipelining: 1.107 / 1.158 ms, Bubble: 105.073 ms, Compute: 389.083 ms, Comm: 29.574 ms, Imbalance: 32.108 ms
Node 7, Pre/Post-Pipelining: 1.106 / 16.469 ms, Bubble: 90.265 ms, Compute: 408.459 ms, Comm: 17.373 ms, Imbalance: 24.087 ms
Node 4, Pre/Post-Pipelining: 1.103 / 1.170 ms, Bubble: 104.419 ms, Compute: 377.634 ms, Comm: 27.237 ms, Imbalance: 46.955 ms
Node 5, Pre/Post-Pipelining: 1.103 / 1.170 ms, Bubble: 104.571 ms, Compute: 390.105 ms, Comm: 31.210 ms, Imbalance: 29.881 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.107 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.283 ms
Cluster-Wide Average, Bubble: 101.483 ms
Cluster-Wide Average, Compute: 427.820 ms
Cluster-Wide Average, Communication: 16.894 ms
Cluster-Wide Average, Imbalance: 8.624 ms
Node 0, GPU memory consumption: 6.616 GB
Node 3, GPU memory consumption: 5.237 GB
Node 2, GPU memory consumption: 5.260 GB
Node 1, GPU memory consumption: 5.260 GB
Node 4, GPU memory consumption: 5.237 GB
Node 6, GPU memory consumption: 5.260 GB
Node 7, GPU memory consumption: 5.042 GB
Node 5, GPU memory consumption: 5.260 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.557887 s---------------
------------------------node id 1,  per-epoch time: 0.557887 s---------------
------------------------node id 2,  per-epoch time: 0.557887 s---------------
------------------------node id 3,  per-epoch time: 0.557887 s---------------
------------------------node id 4,  per-epoch time: 0.557887 s---------------
------------------------node id 5,  per-epoch time: 0.557887 s---------------
------------------------node id 6,  per-epoch time: 0.557887 s---------------
------------------------node id 7,  per-epoch time: 0.557887 s---------------
************ Profiling Results ************
	Bubble: 163.393356 (ms) (29.28 percentage)
	Compute: 389.907767 (ms) (69.88 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.686057 (ms) (0.84 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 402.379 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
