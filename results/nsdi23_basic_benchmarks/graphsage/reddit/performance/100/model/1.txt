Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 0 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.842 seconds.
Building the CSC structure...
        It takes 1.921 seconds.
Building the CSC structure...
        It takes 2.058 seconds.
Building the CSC structure...
        It takes 2.075 seconds.
Building the CSC structure...
        It takes 2.351 seconds.
Building the CSC structure...
        It takes 2.457 seconds.
Building the CSC structure...
        It takes 2.469 seconds.
Building the CSC structure...
        It takes 2.617 seconds.
Building the CSC structure...
        It takes 1.802 seconds.
        It takes 1.838 seconds.
        It takes 1.876 seconds.
        It takes 1.871 seconds.
Building the Feature Vector...
        It takes 2.281 seconds.
Building the Feature Vector...
        It takes 2.365 seconds.
        It takes 2.354 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
        It takes 2.373 seconds.
        It takes 0.286 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
        It takes 0.305 seconds.
Building the Label Vector...
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.033 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.261 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.030 seconds.
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 59.121 Gbps (per GPU), 472.969 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.832 Gbps (per GPU), 470.657 Gbps (aggregated)
The layer-level communication performance: 58.822 Gbps (per GPU), 470.575 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.580 Gbps (per GPU), 468.639 Gbps (aggregated)
The layer-level communication performance: 58.543 Gbps (per GPU), 468.347 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.365 Gbps (per GPU), 466.918 Gbps (aggregated)
The layer-level communication performance: 58.312 Gbps (per GPU), 466.496 Gbps (aggregated)
The layer-level communication performance: 58.285 Gbps (per GPU), 466.276 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 161.468 Gbps (per GPU), 1291.747 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.466 Gbps (per GPU), 1291.728 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.465 Gbps (per GPU), 1291.723 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.472 Gbps (per GPU), 1291.772 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.450 Gbps (per GPU), 1291.598 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.450 Gbps (per GPU), 1291.598 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.447 Gbps (per GPU), 1291.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.444 Gbps (per GPU), 1291.549 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 105.127 Gbps (per GPU), 841.012 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.127 Gbps (per GPU), 841.012 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.127 Gbps (per GPU), 841.012 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.127 Gbps (per GPU), 841.019 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.127 Gbps (per GPU), 841.012 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.126 Gbps (per GPU), 841.005 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.125 Gbps (per GPU), 840.998 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.127 Gbps (per GPU), 841.012 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 37.445 Gbps (per GPU), 299.562 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.446 Gbps (per GPU), 299.569 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.445 Gbps (per GPU), 299.560 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.446 Gbps (per GPU), 299.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.445 Gbps (per GPU), 299.563 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.446 Gbps (per GPU), 299.568 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.444 Gbps (per GPU), 299.553 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 37.445 Gbps (per GPU), 299.561 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.32ms  2.67ms  2.46ms  1.35  8.38K  3.53M
 chk_1  3.54ms  2.91ms  2.71ms  1.31  6.74K  3.60M
 chk_2  3.46ms  2.79ms  2.59ms  1.33  7.27K  3.53M
 chk_3  3.49ms  2.84ms  2.70ms  1.29  7.92K  3.61M
 chk_4  3.21ms  2.75ms  2.58ms  1.24  5.33K  3.68M
 chk_5  3.61ms  2.79ms  2.45ms  1.47 10.07K  3.45M
 chk_6  3.74ms  2.94ms  2.66ms  1.41  9.41K  3.48M
 chk_7  3.43ms  2.77ms  2.56ms  1.34  8.12K  3.60M
 chk_8  3.38ms  2.85ms  2.70ms  1.25  6.09K  3.64M
 chk_9  3.63ms  2.72ms  2.38ms  1.53 11.10K  3.38M
chk_10  3.38ms  2.88ms  2.73ms  1.24  5.67K  3.63M
chk_11  3.42ms  2.79ms  2.54ms  1.34  8.16K  3.54M
chk_12  3.59ms  2.95ms  2.76ms  1.30  7.24K  3.55M
chk_13  3.27ms  2.79ms  2.63ms  1.24  5.41K  3.68M
chk_14  3.69ms  3.02ms  2.82ms  1.31  7.14K  3.53M
chk_15  3.72ms  2.93ms  2.65ms  1.40  9.25K  3.49M
chk_16  3.17ms  2.71ms  2.60ms  1.22  4.78K  3.77M
chk_17  3.45ms  2.84ms  2.66ms  1.30  6.85K  3.60M
chk_18  3.33ms  2.66ms  2.46ms  1.35  7.47K  3.57M
chk_19  3.18ms  2.72ms  2.58ms  1.24  4.88K  3.75M
chk_20  3.37ms  2.73ms  2.54ms  1.33  7.00K  3.63M
chk_21  3.20ms  2.71ms  2.56ms  1.25  5.41K  3.68M
chk_22  3.92ms  2.96ms  2.61ms  1.50 11.07K  3.39M
chk_23  3.51ms  2.81ms  2.63ms  1.33  7.23K  3.64M
chk_24  3.74ms  2.91ms  2.61ms  1.43 10.13K  3.43M
chk_25  3.25ms  2.68ms  2.53ms  1.29  6.40K  3.57M
chk_26  3.42ms  2.88ms  2.73ms  1.25  5.78K  3.55M
chk_27  3.56ms  2.84ms  2.53ms  1.41  9.34K  3.48M
chk_28  3.66ms  3.06ms  2.87ms  1.28  6.37K  3.57M
chk_29  3.38ms  2.89ms  2.72ms  1.24  5.16K  3.78M
chk_30  3.25ms  2.78ms  2.67ms  1.22  5.44K  3.67M
chk_31  3.48ms  2.92ms  2.77ms  1.26  6.33K  3.63M
   Avg  3.46  2.83  2.62
   Max  3.92  3.06  2.87
   Min  3.17  2.66  2.38
 Ratio  1.23  1.15  1.20
   Var  0.03  0.01  0.01
Profiling takes 3.319 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 382.175 ms
Partition 0 [0, 4) has cost: 382.175 ms
Partition 1 [4, 8) has cost: 361.910 ms
Partition 2 [8, 12) has cost: 361.910 ms
Partition 3 [12, 16) has cost: 361.910 ms
Partition 4 [16, 20) has cost: 361.910 ms
Partition 5 [20, 24) has cost: 361.910 ms
Partition 6 [24, 28) has cost: 361.910 ms
Partition 7 [28, 32) has cost: 355.420 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.812 ms
GPU 0, Compute+Comm Time: 137.404 ms, Bubble Time: 29.994 ms, Imbalance Overhead: 3.413 ms
GPU 1, Compute+Comm Time: 131.453 ms, Bubble Time: 29.700 ms, Imbalance Overhead: 9.658 ms
GPU 2, Compute+Comm Time: 131.453 ms, Bubble Time: 29.674 ms, Imbalance Overhead: 9.685 ms
GPU 3, Compute+Comm Time: 131.453 ms, Bubble Time: 29.528 ms, Imbalance Overhead: 9.831 ms
GPU 4, Compute+Comm Time: 131.453 ms, Bubble Time: 29.519 ms, Imbalance Overhead: 9.840 ms
GPU 5, Compute+Comm Time: 131.453 ms, Bubble Time: 29.508 ms, Imbalance Overhead: 9.851 ms
GPU 6, Compute+Comm Time: 131.453 ms, Bubble Time: 29.730 ms, Imbalance Overhead: 9.629 ms
GPU 7, Compute+Comm Time: 129.283 ms, Bubble Time: 30.172 ms, Imbalance Overhead: 11.356 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 333.536 ms
GPU 0, Compute+Comm Time: 251.356 ms, Bubble Time: 59.534 ms, Imbalance Overhead: 22.646 ms
GPU 1, Compute+Comm Time: 255.676 ms, Bubble Time: 58.609 ms, Imbalance Overhead: 19.251 ms
GPU 2, Compute+Comm Time: 255.676 ms, Bubble Time: 58.124 ms, Imbalance Overhead: 19.736 ms
GPU 3, Compute+Comm Time: 255.676 ms, Bubble Time: 58.111 ms, Imbalance Overhead: 19.749 ms
GPU 4, Compute+Comm Time: 255.676 ms, Bubble Time: 57.992 ms, Imbalance Overhead: 19.869 ms
GPU 5, Compute+Comm Time: 255.676 ms, Bubble Time: 58.188 ms, Imbalance Overhead: 19.673 ms
GPU 6, Compute+Comm Time: 255.676 ms, Bubble Time: 58.115 ms, Imbalance Overhead: 19.745 ms
GPU 7, Compute+Comm Time: 269.990 ms, Bubble Time: 58.543 ms, Imbalance Overhead: 5.004 ms
The estimated cost of the whole pipeline: 529.565 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 744.085 ms
Partition 0 [0, 8) has cost: 744.085 ms
Partition 1 [8, 16) has cost: 723.820 ms
Partition 2 [16, 24) has cost: 723.820 ms
Partition 3 [24, 32) has cost: 717.329 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 176.776 ms
GPU 0, Compute+Comm Time: 146.581 ms, Bubble Time: 27.707 ms, Imbalance Overhead: 2.488 ms
GPU 1, Compute+Comm Time: 143.331 ms, Bubble Time: 27.158 ms, Imbalance Overhead: 6.286 ms
GPU 2, Compute+Comm Time: 143.331 ms, Bubble Time: 26.992 ms, Imbalance Overhead: 6.453 ms
GPU 3, Compute+Comm Time: 142.289 ms, Bubble Time: 26.681 ms, Imbalance Overhead: 7.805 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 332.669 ms
GPU 0, Compute+Comm Time: 267.270 ms, Bubble Time: 50.688 ms, Imbalance Overhead: 14.712 ms
GPU 1, Compute+Comm Time: 269.327 ms, Bubble Time: 50.948 ms, Imbalance Overhead: 12.394 ms
GPU 2, Compute+Comm Time: 269.327 ms, Bubble Time: 50.907 ms, Imbalance Overhead: 12.435 ms
GPU 3, Compute+Comm Time: 277.285 ms, Bubble Time: 52.002 ms, Imbalance Overhead: 3.382 ms
    The estimated cost with 2 DP ways is 534.917 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1467.904 ms
Partition 0 [0, 16) has cost: 1467.904 ms
Partition 1 [16, 32) has cost: 1441.149 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 220.941 ms
GPU 0, Compute+Comm Time: 192.719 ms, Bubble Time: 23.732 ms, Imbalance Overhead: 4.490 ms
GPU 1, Compute+Comm Time: 190.598 ms, Bubble Time: 24.433 ms, Imbalance Overhead: 5.910 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 368.287 ms
GPU 0, Compute+Comm Time: 318.788 ms, Bubble Time: 40.916 ms, Imbalance Overhead: 8.584 ms
GPU 1, Compute+Comm Time: 323.941 ms, Bubble Time: 39.794 ms, Imbalance Overhead: 4.553 ms
    The estimated cost with 4 DP ways is 618.690 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2909.053 ms
Partition 0 [0, 32) has cost: 2909.053 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 524.599 ms
GPU 0, Compute+Comm Time: 524.599 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 656.604 ms
GPU 0, Compute+Comm Time: 656.604 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1240.263 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.3318
	Epoch 50:	Loss 3.1254
Node 1, Pre/Post-Pipelining: 1.109 / 1.119 ms, Bubble: 98.153 ms, Compute: 381.767 ms, Comm: 28.140 ms, Imbalance: 30.363 ms
Node 0, Pre/Post-Pipelining: 1.110 / 1.161 ms, Bubble: 96.945 ms, Compute: 416.526 ms, Comm: 16.759 ms, Imbalance: 7.513 ms
Node 2, Pre/Post-Pipelining: 1.104 / 1.148 ms, Bubble: 99.141 ms, Compute: 372.084 ms, Comm: 29.971 ms, Imbalance: 37.444 ms
Node 4, Pre/Post-Pipelining: 1.106 / 1.089 ms, Bubble: 100.461 ms, Compute: 368.261 ms, Comm: 26.698 ms, Imbalance: 43.664 ms
Node 3, Pre/Post-Pipelining: 1.105 / 1.134 ms, Bubble: 99.180 ms, Compute: 380.996 ms, Comm: 26.509 ms, Imbalance: 31.654 ms
Node 6, Pre/Post-Pipelining: 1.107 / 1.080 ms, Bubble: 101.654 ms, Compute: 375.413 ms, Comm: 29.286 ms, Imbalance: 32.328 ms
Node 7, Pre/Post-Pipelining: 1.106 / 16.055 ms, Bubble: 87.918 ms, Compute: 385.694 ms, Comm: 17.413 ms, Imbalance: 32.570 ms
Node 5, Pre/Post-Pipelining: 1.105 / 1.153 ms, Bubble: 100.843 ms, Compute: 376.864 ms, Comm: 30.894 ms, Imbalance: 30.017 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.110 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.161 ms
Cluster-Wide Average, Bubble: 96.945 ms
Cluster-Wide Average, Compute: 416.526 ms
Cluster-Wide Average, Communication: 16.759 ms
Cluster-Wide Average, Imbalance: 7.513 ms
Node 0, GPU memory consumption: 6.614 GB
Node 1, GPU memory consumption: 5.260 GB
Node 6, GPU memory consumption: 5.260 GB
Node 2, GPU memory consumption: 5.260 GB
Node 4, GPU memory consumption: 5.237 GB
Node 3, GPU memory consumption: 5.237 GB
Node 7, GPU memory consumption: 5.042 GB
Node 5, GPU memory consumption: 5.260 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.540711 s---------------
------------------------node id 1,  per-epoch time: 0.540710 s---------------
------------------------node id 2,  per-epoch time: 0.540712 s---------------
------------------------node id 3,  per-epoch time: 0.540712 s---------------
------------------------node id 4,  per-epoch time: 0.540708 s---------------
------------------------node id 5,  per-epoch time: 0.540708 s---------------
------------------------node id 6,  per-epoch time: 0.540708 s---------------
------------------------node id 7,  per-epoch time: 0.540707 s---------------
************ Profiling Results ************
	Bubble: 166.077100 (ms) (30.11 percentage)
	Compute: 380.926941 (ms) (69.07 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.482887 (ms) (0.81 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 405.965 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
