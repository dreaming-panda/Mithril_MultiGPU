Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.916 seconds.
Building the CSC structure...
        It takes 2.063 seconds.
Building the CSC structure...
        It takes 2.170 seconds.
Building the CSC structure...
        It takes 2.383 seconds.
Building the CSC structure...
        It takes 2.424 seconds.
Building the CSC structure...
        It takes 2.452 seconds.
Building the CSC structure...
        It takes 2.463 seconds.
Building the CSC structure...
        It takes 2.640 seconds.
Building the CSC structure...
        It takes 1.842 seconds.
        It takes 1.842 seconds.
        It takes 1.882 seconds.
        It takes 2.292 seconds.
Building the Feature Vector...
        It takes 2.328 seconds.
        It takes 2.387 seconds.
        It takes 2.398 seconds.
        It takes 2.365 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
Building the Feature Vector...
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.245 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
Building the Feature Vector...
        It takes 0.303 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.258 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.035 seconds.
        It takes 0.292 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.278 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.156 Gbps (per GPU), 441.249 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.914 Gbps (per GPU), 439.313 Gbps (aggregated)
The layer-level communication performance: 54.907 Gbps (per GPU), 439.259 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.691 Gbps (per GPU), 437.529 Gbps (aggregated)
The layer-level communication performance: 54.670 Gbps (per GPU), 437.361 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.487 Gbps (per GPU), 435.894 Gbps (aggregated)
The layer-level communication performance: 54.438 Gbps (per GPU), 435.502 Gbps (aggregated)
The layer-level communication performance: 54.416 Gbps (per GPU), 435.326 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 161.434 Gbps (per GPU), 1291.474 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.416 Gbps (per GPU), 1291.325 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.391 Gbps (per GPU), 1291.130 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.413 Gbps (per GPU), 1291.302 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.431 Gbps (per GPU), 1291.449 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.415 Gbps (per GPU), 1291.323 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.397 Gbps (per GPU), 1291.176 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.416 Gbps (per GPU), 1291.325 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.032 Gbps (per GPU), 832.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.033 Gbps (per GPU), 832.265 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.033 Gbps (per GPU), 832.265 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.034 Gbps (per GPU), 832.272 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.034 Gbps (per GPU), 832.272 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.032 Gbps (per GPU), 832.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.032 Gbps (per GPU), 832.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.033 Gbps (per GPU), 832.265 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.748 Gbps (per GPU), 253.986 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.749 Gbps (per GPU), 253.990 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.749 Gbps (per GPU), 253.989 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.745 Gbps (per GPU), 253.962 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.749 Gbps (per GPU), 253.989 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.748 Gbps (per GPU), 253.987 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.748 Gbps (per GPU), 253.984 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.748 Gbps (per GPU), 253.984 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.27ms  2.71ms  2.48ms  1.32  8.38K  3.53M
 chk_1  3.57ms  2.93ms  2.74ms  1.30  6.74K  3.60M
 chk_2  3.50ms  2.84ms  2.63ms  1.33  7.27K  3.53M
 chk_3  3.55ms  2.88ms  2.65ms  1.34  7.92K  3.61M
 chk_4  3.26ms  2.80ms  2.64ms  1.23  5.33K  3.68M
 chk_5  3.62ms  2.83ms  2.49ms  1.45 10.07K  3.45M
 chk_6  3.76ms  2.97ms  2.67ms  1.41  9.41K  3.48M
 chk_7  3.46ms  2.81ms  2.58ms  1.34  8.12K  3.60M
 chk_8  3.42ms  2.89ms  2.73ms  1.26  6.09K  3.64M
 chk_9  3.68ms  2.75ms  2.41ms  1.53 11.10K  3.38M
chk_10  3.41ms  2.92ms  2.76ms  1.24  5.67K  3.63M
chk_11  3.47ms  2.80ms  2.59ms  1.34  8.16K  3.54M
chk_12  3.65ms  2.99ms  2.80ms  1.31  7.24K  3.55M
chk_13  3.30ms  2.83ms  2.68ms  1.23  5.41K  3.68M
chk_14  3.71ms  3.05ms  2.85ms  1.30  7.14K  3.53M
chk_15  3.74ms  2.97ms  2.67ms  1.40  9.25K  3.49M
chk_16  3.20ms  2.76ms  2.62ms  1.22  4.78K  3.77M
chk_17  3.49ms  2.88ms  2.70ms  1.29  6.85K  3.60M
chk_18  3.37ms  2.70ms  2.49ms  1.36  7.47K  3.57M
chk_19  3.21ms  2.76ms  2.61ms  1.23  4.88K  3.75M
chk_20  3.40ms  2.78ms  2.59ms  1.31  7.00K  3.63M
chk_21  3.27ms  2.75ms  2.58ms  1.27  5.41K  3.68M
chk_22  3.94ms  3.00ms  2.65ms  1.49 11.07K  3.39M
chk_23  3.53ms  2.85ms  2.67ms  1.32  7.23K  3.64M
chk_24  3.78ms  2.94ms  2.65ms  1.43 10.13K  3.43M
chk_25  3.27ms  2.72ms  2.57ms  1.28  6.40K  3.57M
chk_26  3.47ms  2.92ms  2.77ms  1.25  5.78K  3.55M
chk_27  3.62ms  2.88ms  2.57ms  1.41  9.34K  3.48M
chk_28  3.69ms  3.10ms  2.90ms  1.27  6.37K  3.57M
chk_29  3.41ms  2.91ms  2.77ms  1.23  5.16K  3.78M
chk_30  3.29ms  2.80ms  2.71ms  1.21  5.44K  3.67M
chk_31  3.53ms  2.96ms  2.80ms  1.26  6.33K  3.63M
   Avg  3.50  2.87  2.66
   Max  3.94  3.10  2.90
   Min  3.20  2.70  2.41
 Ratio  1.23  1.15  1.21
   Var  0.03  0.01  0.01
Profiling takes 3.358 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 386.951 ms
Partition 0 [0, 4) has cost: 386.951 ms
Partition 1 [4, 8) has cost: 366.773 ms
Partition 2 [8, 12) has cost: 366.773 ms
Partition 3 [12, 16) has cost: 366.773 ms
Partition 4 [16, 20) has cost: 366.773 ms
Partition 5 [20, 24) has cost: 366.773 ms
Partition 6 [24, 28) has cost: 366.773 ms
Partition 7 [28, 32) has cost: 360.104 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 174.196 ms
GPU 0, Compute+Comm Time: 139.970 ms, Bubble Time: 30.566 ms, Imbalance Overhead: 3.660 ms
GPU 1, Compute+Comm Time: 134.010 ms, Bubble Time: 30.300 ms, Imbalance Overhead: 9.886 ms
GPU 2, Compute+Comm Time: 134.010 ms, Bubble Time: 30.263 ms, Imbalance Overhead: 9.922 ms
GPU 3, Compute+Comm Time: 134.010 ms, Bubble Time: 30.146 ms, Imbalance Overhead: 10.040 ms
GPU 4, Compute+Comm Time: 134.010 ms, Bubble Time: 30.154 ms, Imbalance Overhead: 10.032 ms
GPU 5, Compute+Comm Time: 134.010 ms, Bubble Time: 30.119 ms, Imbalance Overhead: 10.067 ms
GPU 6, Compute+Comm Time: 134.010 ms, Bubble Time: 30.344 ms, Imbalance Overhead: 9.841 ms
GPU 7, Compute+Comm Time: 131.803 ms, Bubble Time: 30.792 ms, Imbalance Overhead: 11.600 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 338.465 ms
GPU 0, Compute+Comm Time: 255.333 ms, Bubble Time: 60.263 ms, Imbalance Overhead: 22.869 ms
GPU 1, Compute+Comm Time: 259.795 ms, Bubble Time: 59.357 ms, Imbalance Overhead: 19.313 ms
GPU 2, Compute+Comm Time: 259.795 ms, Bubble Time: 58.888 ms, Imbalance Overhead: 19.782 ms
GPU 3, Compute+Comm Time: 259.795 ms, Bubble Time: 58.936 ms, Imbalance Overhead: 19.735 ms
GPU 4, Compute+Comm Time: 259.795 ms, Bubble Time: 58.798 ms, Imbalance Overhead: 19.872 ms
GPU 5, Compute+Comm Time: 259.795 ms, Bubble Time: 58.981 ms, Imbalance Overhead: 19.689 ms
GPU 6, Compute+Comm Time: 259.795 ms, Bubble Time: 58.955 ms, Imbalance Overhead: 19.716 ms
GPU 7, Compute+Comm Time: 274.013 ms, Bubble Time: 59.460 ms, Imbalance Overhead: 4.992 ms
The estimated cost of the whole pipeline: 538.294 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 753.724 ms
Partition 0 [0, 8) has cost: 753.724 ms
Partition 1 [8, 16) has cost: 733.546 ms
Partition 2 [16, 24) has cost: 733.546 ms
Partition 3 [24, 32) has cost: 726.877 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 180.049 ms
GPU 0, Compute+Comm Time: 149.126 ms, Bubble Time: 28.306 ms, Imbalance Overhead: 2.617 ms
GPU 1, Compute+Comm Time: 145.872 ms, Bubble Time: 27.728 ms, Imbalance Overhead: 6.449 ms
GPU 2, Compute+Comm Time: 145.872 ms, Bubble Time: 27.489 ms, Imbalance Overhead: 6.688 ms
GPU 3, Compute+Comm Time: 144.811 ms, Bubble Time: 27.204 ms, Imbalance Overhead: 8.034 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 337.333 ms
GPU 0, Compute+Comm Time: 271.320 ms, Bubble Time: 51.267 ms, Imbalance Overhead: 14.747 ms
GPU 1, Compute+Comm Time: 273.496 ms, Bubble Time: 51.596 ms, Imbalance Overhead: 12.241 ms
GPU 2, Compute+Comm Time: 273.496 ms, Bubble Time: 51.640 ms, Imbalance Overhead: 12.197 ms
GPU 3, Compute+Comm Time: 281.320 ms, Bubble Time: 52.745 ms, Imbalance Overhead: 3.268 ms
    The estimated cost with 2 DP ways is 543.251 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1487.270 ms
Partition 0 [0, 16) has cost: 1487.270 ms
Partition 1 [16, 32) has cost: 1460.423 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 224.333 ms
GPU 0, Compute+Comm Time: 195.689 ms, Bubble Time: 24.198 ms, Imbalance Overhead: 4.445 ms
GPU 1, Compute+Comm Time: 193.549 ms, Bubble Time: 24.739 ms, Imbalance Overhead: 6.045 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 373.164 ms
GPU 0, Compute+Comm Time: 323.154 ms, Bubble Time: 41.461 ms, Imbalance Overhead: 8.549 ms
GPU 1, Compute+Comm Time: 328.399 ms, Bubble Time: 40.390 ms, Imbalance Overhead: 4.375 ms
    The estimated cost with 4 DP ways is 627.371 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2947.692 ms
Partition 0 [0, 32) has cost: 2947.692 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 597.746 ms
GPU 0, Compute+Comm Time: 597.746 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 730.985 ms
GPU 0, Compute+Comm Time: 730.985 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1395.168 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2452
	Epoch 50:	Loss 3.0810
	Epoch 75:	Loss 2.8579
	Epoch 100:	Loss 2.6935
	Epoch 125:	Loss 2.4493
	Epoch 150:	Loss 2.3539
	Epoch 175:	Loss 2.2433
	Epoch 200:	Loss 2.1485
	Epoch 225:	Loss 1.9851
	Epoch 250:	Loss 1.8136
	Epoch 275:	Loss 1.6575
	Epoch 300:	Loss 1.5190
	Epoch 325:	Loss 1.3920
	Epoch 350:	Loss 1.3036
	Epoch 375:	Loss 1.2331
	Epoch 400:	Loss 1.1814
	Epoch 425:	Loss 1.1257
	Epoch 450:	Loss 1.0856
	Epoch 475:	Loss 1.0238
	Epoch 500:	Loss 0.9664
	Epoch 525:	Loss 0.9159
	Epoch 550:	Loss 0.8744
	Epoch 575:	Loss 0.8433
	Epoch 600:	Loss 0.8142
	Epoch 625:	Loss 0.7927
	Epoch 650:	Loss 0.7581
	Epoch 675:	Loss 0.7391
	Epoch 700:	Loss 0.7221
	Epoch 725:	Loss 0.7134
	Epoch 750:	Loss 0.7025
	Epoch 775:	Loss 0.6973
	Epoch 800:	Loss 0.6722
	Epoch 825:	Loss 0.6632
	Epoch 850:	Loss 0.6360
	Epoch 875:	Loss 0.6245
	Epoch 900:	Loss 0.6065
	Epoch 925:	Loss 0.6014
	Epoch 950:	Loss 0.6072
	Epoch 975:	Loss 0.5998
	Epoch 1000:	Loss 0.5773
	Epoch 1025:	Loss 0.5734
	Epoch 1050:	Loss 0.5595
	Epoch 1075:	Loss 0.5481
	Epoch 1100:	Loss 0.5470
	Epoch 1125:	Loss 0.5517
	Epoch 1150:	Loss 0.5306
	Epoch 1175:	Loss 0.5313
	Epoch 1200:	Loss 0.5226
	Epoch 1225:	Loss 0.5185
	Epoch 1250:	Loss 0.5182
	Epoch 1275:	Loss 0.5144
	Epoch 1300:	Loss 0.5065
	Epoch 1325:	Loss 0.5070
	Epoch 1350:	Loss 0.5062
	Epoch 1375:	Loss 0.4940
	Epoch 1400:	Loss 0.4929
	Epoch 1425:	Loss 0.4929
	Epoch 1450:	Loss 0.4883
	Epoch 1475:	Loss 0.4837
	Epoch 1500:	Loss 0.4775
	Epoch 1525:	Loss 0.4777
	Epoch 1550:	Loss 0.4728
	Epoch 1575:	Loss 0.4735
	Epoch 1600:	Loss 0.4686
	Epoch 1625:	Loss 0.4724
	Epoch 1650:	Loss 0.4682
	Epoch 1675:	Loss 0.4572
	Epoch 1700:	Loss 0.4584
	Epoch 1725:	Loss 0.4523
	Epoch 1750:	Loss 0.4544
	Epoch 1775:	Loss 0.4490
	Epoch 1800:	Loss 0.4463
	Epoch 1825:	Loss 0.4465
	Epoch 1850:	Loss 0.4407
	Epoch 1875:	Loss 0.4350
	Epoch 1900:	Loss 0.4356
	Epoch 1925:	Loss 0.4392
	Epoch 1950:	Loss 0.4309
	Epoch 1975:	Loss 0.4287
	Epoch 2000:	Loss 0.4305
	Epoch 2025:	Loss 0.4399
	Epoch 2050:	Loss 0.4274
	Epoch 2075:	Loss 0.4302
	Epoch 2100:	Loss 0.4223
	Epoch 2125:	Loss 0.4238
	Epoch 2150:	Loss 0.4167
	Epoch 2175:	Loss 0.4154
	Epoch 2200:	Loss 0.4147
	Epoch 2225:	Loss 0.4159
	Epoch 2250:	Loss 0.4106
	Epoch 2275:	Loss 0.4126
	Epoch 2300:	Loss 0.4110
	Epoch 2325:	Loss 0.4046
	Epoch 2350:	Loss 0.4059
	Epoch 2375:	Loss 0.4059
	Epoch 2400:	Loss 0.3990
	Epoch 2425:	Loss 0.4072
	Epoch 2450:	Loss 0.3971
	Epoch 2475:	Loss 0.4014
	Epoch 2500:	Loss 0.3977
	Epoch 2525:	Loss 0.3974
	Epoch 2550:	Loss 0.3951
	Epoch 2575:	Loss 0.3942
	Epoch 2600:	Loss 0.3926
	Epoch 2625:	Loss 0.3921
	Epoch 2650:	Loss 0.3921
	Epoch 2675:	Loss 0.3894
	Epoch 2700:	Loss 0.3862
	Epoch 2725:	Loss 0.3885
	Epoch 2750:	Loss 0.3900
	Epoch 2775:	Loss 0.3893
	Epoch 2800:	Loss 0.3889
	Epoch 2825:	Loss 0.3963
	Epoch 2850:	Loss 0.3836
	Epoch 2875:	Loss 0.3856
	Epoch 2900:	Loss 0.3846
	Epoch 2925:	Loss 0.3825
	Epoch 2950:	Loss 0.3805
	Epoch 2975:	Loss 0.3771
	Epoch 3000:	Loss 0.3758
	Epoch 3025:	Loss 0.3792
	Epoch 3050:	Loss 0.3755
	Epoch 3075:	Loss 0.3756
	Epoch 3100:	Loss 0.3741
	Epoch 3125:	Loss 0.3739
	Epoch 3150:	Loss 0.3762
	Epoch 3175:	Loss 0.3781
	Epoch 3200:	Loss 0.3716
	Epoch 3225:	Loss 0.3787
	Epoch 3250:	Loss 0.3829
	Epoch 3275:	Loss 0.3806
	Epoch 3300:	Loss 0.3798
	Epoch 3325:	Loss 0.3948
	Epoch 3350:	Loss 0.3957
	Epoch 3375:	Loss 0.3857
	Epoch 3400:	Loss 0.3780
	Epoch 3425:	Loss 0.3730
	Epoch 3450:	Loss 0.3697
	Epoch 3475:	Loss 0.3719
	Epoch 3500:	Loss 0.3693
	Epoch 3525:	Loss 0.3743
	Epoch 3550:	Loss 0.3731
	Epoch 3575:	Loss 0.3653
	Epoch 3600:	Loss 0.3686
	Epoch 3625:	Loss 0.3671
	Epoch 3650:	Loss 0.3645
	Epoch 3675:	Loss 0.3658
	Epoch 3700:	Loss 0.3666
	Epoch 3725:	Loss 0.3846
	Epoch 3750:	Loss 0.3704
	Epoch 3775:	Loss 0.3647
	Epoch 3800:	Loss 0.3634
	Epoch 3825:	Loss 0.3630
	Epoch 3850:	Loss 0.3645
	Epoch 3875:	Loss 0.3662
	Epoch 3900:	Loss 0.3659
	Epoch 3925:	Loss 0.3730
	Epoch 3950:	Loss 0.3710
	Epoch 3975:	Loss 0.3678
	Epoch 4000:	Loss 0.3636
	Epoch 4025:	Loss 0.3609
	Epoch 4050:	Loss 0.3603
	Epoch 4075:	Loss 0.3588
	Epoch 4100:	Loss 0.3574
	Epoch 4125:	Loss 0.3541
	Epoch 4150:	Loss 0.3545
	Epoch 4175:	Loss 0.3562
	Epoch 4200:	Loss 0.3538
	Epoch 4225:	Loss 0.3528
	Epoch 4250:	Loss 0.3539
	Epoch 4275:	Loss 0.3533
	Epoch 4300:	Loss 0.3523
	Epoch 4325:	Loss 0.3661
	Epoch 4350:	Loss 0.3591
	Epoch 4375:	Loss 0.3775
	Epoch 4400:	Loss 0.3692
	Epoch 4425:	Loss 0.3762
	Epoch 4450:	Loss 0.3655
	Epoch 4475:	Loss 0.3579
	Epoch 4500:	Loss 0.3536
	Epoch 4525:	Loss 0.3531
	Epoch 4550:	Loss 0.3506
	Epoch 4575:	Loss 0.3497
	Epoch 4600:	Loss 0.3503
	Epoch 4625:	Loss 0.3505
	Epoch 4650:	Loss 0.3474
	Epoch 4675:	Loss 0.3446
	Epoch 4700:	Loss 0.3446
	Epoch 4725:	Loss 0.3455
	Epoch 4750:	Loss 0.3468
	Epoch 4775:	Loss 0.3443
	Epoch 4800:	Loss 0.3468
	Epoch 4825:	Loss 0.3472
	Epoch 4850:	Loss 0.3452
	Epoch 4875:	Loss 0.3454
	Epoch 4900:	Loss 0.3455
	Epoch 4925:	Loss 0.3539
	Epoch 4950:	Loss 0.3469
	Epoch 4975:	Loss 0.3434
	Epoch 5000:	Loss 0.3433
Node 1, Pre/Post-Pipelining: 1.112 / 1.123 ms, Bubble: 102.211 ms, Compute: 399.438 ms, Comm: 28.126 ms, Imbalance: 25.583 ms
Node 2, Pre/Post-Pipelining: 1.106 / 1.160 ms, Bubble: 103.350 ms, Compute: 382.318 ms, Comm: 30.218 ms, Imbalance: 39.864 ms
Node 6, Pre/Post-Pipelining: 1.108 / 1.097 ms, Bubble: 105.083 ms, Compute: 388.022 ms, Comm: 29.550 ms, Imbalance: 33.095 ms
Node 3, Pre/Post-Pipelining: 1.105 / 1.170 ms, Bubble: 103.255 ms, Compute: 388.457 ms, Comm: 27.126 ms, Imbalance: 36.635 ms
Node 7, Pre/Post-Pipelining: 1.106 / 16.475 ms, Bubble: 90.245 ms, Compute: 408.270 ms, Comm: 17.290 ms, Imbalance: 24.236 ms
Node 0, Pre/Post-Pipelining: 1.109 / 1.227 ms, Bubble: 101.326 ms, Compute: 428.028 ms, Comm: 16.891 ms, Imbalance: 8.487 ms
Node 4, Pre/Post-Pipelining: 1.105 / 1.144 ms, Bubble: 104.317 ms, Compute: 378.315 ms, Comm: 27.181 ms, Imbalance: 46.290 ms
Node 5, Pre/Post-Pipelining: 1.104 / 1.154 ms, Bubble: 104.491 ms, Compute: 389.801 ms, Comm: 31.118 ms, Imbalance: 30.220 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.109 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.227 ms
Cluster-Wide Average, Bubble: 101.326 ms
Cluster-Wide Average, Compute: 428.028 ms
Cluster-Wide Average, Communication: 16.891 ms
Cluster-Wide Average, Imbalance: 8.487 ms
Node 0, GPU memory consumption: 6.614 GB
Node 1, GPU memory consumption: 5.260 GB
Node 2, GPU memory consumption: 5.260 GB
Node 3, GPU memory consumption: 5.237 GB
Node 4, GPU memory consumption: 5.237 GB
Node 6, GPU memory consumption: 5.260 GB
Node 5, GPU memory consumption: 5.260 GB
Node 7, GPU memory consumption: 5.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.557740 s---------------
------------------------node id 1,  per-epoch time: 0.557740 s---------------
------------------------node id 2,  per-epoch time: 0.557740 s---------------
------------------------node id 3,  per-epoch time: 0.557740 s---------------
------------------------node id 4,  per-epoch time: 0.557741 s---------------
------------------------node id 5,  per-epoch time: 0.557741 s---------------
------------------------node id 6,  per-epoch time: 0.557741 s---------------
------------------------node id 7,  per-epoch time: 0.557741 s---------------
************ Profiling Results ************
	Bubble: 163.236372 (ms) (29.26 percentage)
	Compute: 389.930300 (ms) (69.90 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.670024 (ms) (0.84 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 402.382 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
