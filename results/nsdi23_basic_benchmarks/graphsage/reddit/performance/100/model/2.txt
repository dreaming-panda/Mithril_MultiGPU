Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.868 seconds.
Building the CSC structure...
        It takes 2.057 seconds.
Building the CSC structure...
        It takes 2.316 seconds.
Building the CSC structure...
        It takes 2.422 seconds.
Building the CSC structure...
        It takes 2.435 seconds.
Building the CSC structure...
        It takes 2.632 seconds.
Building the CSC structure...
        It takes 2.658 seconds.
Building the CSC structure...
        It takes 2.665 seconds.
Building the CSC structure...
        It takes 1.803 seconds.
        It takes 1.879 seconds.
        It takes 2.302 seconds.
Building the Feature Vector...
        It takes 2.322 seconds.
        It takes 2.387 seconds.
        It takes 0.243 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 2.311 seconds.
        It takes 2.322 seconds.
        It takes 2.391 seconds.
Building the Feature Vector...
        It takes 0.273 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.246 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
        It takes 0.271 seconds.
Building the Label Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.036 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.264 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.031 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 59.526 Gbps (per GPU), 476.209 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.226 Gbps (per GPU), 473.811 Gbps (aggregated)
The layer-level communication performance: 59.235 Gbps (per GPU), 473.882 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.960 Gbps (per GPU), 471.676 Gbps (aggregated)
The layer-level communication performance: 58.936 Gbps (per GPU), 471.485 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.708 Gbps (per GPU), 469.667 Gbps (aggregated)
The layer-level communication performance: 58.661 Gbps (per GPU), 469.288 Gbps (aggregated)
The layer-level communication performance: 58.633 Gbps (per GPU), 469.061 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.674 Gbps (per GPU), 1277.388 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.826 Gbps (per GPU), 1278.604 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.670 Gbps (per GPU), 1277.364 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.844 Gbps (per GPU), 1278.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.674 Gbps (per GPU), 1277.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.680 Gbps (per GPU), 1277.440 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.677 Gbps (per GPU), 1277.412 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.683 Gbps (per GPU), 1277.461 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.824 Gbps (per GPU), 838.595 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.826 Gbps (per GPU), 838.608 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.825 Gbps (per GPU), 838.602 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.824 Gbps (per GPU), 838.595 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.824 Gbps (per GPU), 838.595 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.825 Gbps (per GPU), 838.602 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.823 Gbps (per GPU), 838.581 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.825 Gbps (per GPU), 838.602 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.439 Gbps (per GPU), 283.513 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.439 Gbps (per GPU), 283.515 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.439 Gbps (per GPU), 283.514 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.439 Gbps (per GPU), 283.513 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.439 Gbps (per GPU), 283.513 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.439 Gbps (per GPU), 283.514 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.437 Gbps (per GPU), 283.497 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.437 Gbps (per GPU), 283.497 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.32ms  2.68ms  2.44ms  1.36  8.38K  3.53M
 chk_1  3.54ms  2.93ms  2.74ms  1.29  6.74K  3.60M
 chk_2  3.48ms  2.79ms  2.62ms  1.33  7.27K  3.53M
 chk_3  3.53ms  2.87ms  2.64ms  1.34  7.92K  3.61M
 chk_4  3.22ms  2.78ms  2.63ms  1.22  5.33K  3.68M
 chk_5  3.60ms  2.80ms  2.51ms  1.44 10.07K  3.45M
 chk_6  3.73ms  2.95ms  2.67ms  1.40  9.41K  3.48M
 chk_7  3.45ms  2.78ms  2.57ms  1.34  8.12K  3.60M
 chk_8  3.41ms  2.86ms  2.71ms  1.26  6.09K  3.64M
 chk_9  3.65ms  2.73ms  2.39ms  1.53 11.10K  3.38M
chk_10  3.39ms  2.90ms  2.74ms  1.24  5.67K  3.63M
chk_11  3.46ms  2.81ms  2.56ms  1.35  8.16K  3.54M
chk_12  3.66ms  2.98ms  2.78ms  1.32  7.24K  3.55M
chk_13  3.29ms  2.82ms  2.65ms  1.24  5.41K  3.68M
chk_14  3.71ms  3.04ms  2.83ms  1.31  7.14K  3.53M
chk_15  3.72ms  2.95ms  2.65ms  1.40  9.25K  3.49M
chk_16  3.19ms  2.74ms  2.61ms  1.22  4.78K  3.77M
chk_17  3.48ms  2.87ms  2.68ms  1.30  6.85K  3.60M
chk_18  3.33ms  2.67ms  2.49ms  1.34  7.47K  3.57M
chk_19  3.18ms  2.73ms  2.61ms  1.22  4.88K  3.75M
chk_20  3.40ms  2.74ms  2.58ms  1.32  7.00K  3.63M
chk_21  3.26ms  2.72ms  2.57ms  1.27  5.41K  3.68M
chk_22  3.94ms  2.98ms  2.62ms  1.50 11.07K  3.39M
chk_23  3.52ms  2.86ms  2.63ms  1.34  7.23K  3.64M
chk_24  3.76ms  2.94ms  2.64ms  1.43 10.13K  3.43M
chk_25  3.27ms  2.70ms  2.53ms  1.29  6.40K  3.57M
chk_26  3.44ms  2.90ms  2.74ms  1.25  5.78K  3.55M
chk_27  3.60ms  2.89ms  2.56ms  1.41  9.34K  3.48M
chk_28  3.67ms  3.11ms  2.89ms  1.27  6.37K  3.57M
chk_29  3.41ms  2.91ms  2.75ms  1.24  5.16K  3.78M
chk_30  3.26ms  2.79ms  2.69ms  1.21  5.44K  3.67M
chk_31  3.52ms  2.94ms  2.78ms  1.27  6.33K  3.63M
   Avg  3.48  2.85  2.64
   Max  3.94  3.11  2.89
   Min  3.18  2.67  2.39
 Ratio  1.24  1.16  1.21
   Var  0.03  0.01  0.01
Profiling takes 3.339 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 384.870 ms
Partition 0 [0, 4) has cost: 384.870 ms
Partition 1 [4, 8) has cost: 364.627 ms
Partition 2 [8, 12) has cost: 364.627 ms
Partition 3 [12, 16) has cost: 364.627 ms
Partition 4 [16, 20) has cost: 364.627 ms
Partition 5 [20, 24) has cost: 364.627 ms
Partition 6 [24, 28) has cost: 364.627 ms
Partition 7 [28, 32) has cost: 357.960 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 172.133 ms
GPU 0, Compute+Comm Time: 138.310 ms, Bubble Time: 30.338 ms, Imbalance Overhead: 3.485 ms
GPU 1, Compute+Comm Time: 132.377 ms, Bubble Time: 30.080 ms, Imbalance Overhead: 9.676 ms
GPU 2, Compute+Comm Time: 132.377 ms, Bubble Time: 30.003 ms, Imbalance Overhead: 9.754 ms
GPU 3, Compute+Comm Time: 132.377 ms, Bubble Time: 29.806 ms, Imbalance Overhead: 9.951 ms
GPU 4, Compute+Comm Time: 132.377 ms, Bubble Time: 29.785 ms, Imbalance Overhead: 9.971 ms
GPU 5, Compute+Comm Time: 132.377 ms, Bubble Time: 29.758 ms, Imbalance Overhead: 9.998 ms
GPU 6, Compute+Comm Time: 132.377 ms, Bubble Time: 29.959 ms, Imbalance Overhead: 9.797 ms
GPU 7, Compute+Comm Time: 130.161 ms, Bubble Time: 30.388 ms, Imbalance Overhead: 11.584 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 335.769 ms
GPU 0, Compute+Comm Time: 252.846 ms, Bubble Time: 59.659 ms, Imbalance Overhead: 23.264 ms
GPU 1, Compute+Comm Time: 257.298 ms, Bubble Time: 58.762 ms, Imbalance Overhead: 19.710 ms
GPU 2, Compute+Comm Time: 257.298 ms, Bubble Time: 58.308 ms, Imbalance Overhead: 20.164 ms
GPU 3, Compute+Comm Time: 257.298 ms, Bubble Time: 58.282 ms, Imbalance Overhead: 20.190 ms
GPU 4, Compute+Comm Time: 257.298 ms, Bubble Time: 58.191 ms, Imbalance Overhead: 20.280 ms
GPU 5, Compute+Comm Time: 257.298 ms, Bubble Time: 58.483 ms, Imbalance Overhead: 19.989 ms
GPU 6, Compute+Comm Time: 257.298 ms, Bubble Time: 58.498 ms, Imbalance Overhead: 19.974 ms
GPU 7, Compute+Comm Time: 271.607 ms, Bubble Time: 59.096 ms, Imbalance Overhead: 5.066 ms
The estimated cost of the whole pipeline: 533.298 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 749.497 ms
Partition 0 [0, 8) has cost: 749.497 ms
Partition 1 [8, 16) has cost: 729.254 ms
Partition 2 [16, 24) has cost: 729.254 ms
Partition 3 [24, 32) has cost: 722.587 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 178.169 ms
GPU 0, Compute+Comm Time: 147.626 ms, Bubble Time: 27.936 ms, Imbalance Overhead: 2.606 ms
GPU 1, Compute+Comm Time: 144.420 ms, Bubble Time: 27.385 ms, Imbalance Overhead: 6.363 ms
GPU 2, Compute+Comm Time: 144.420 ms, Bubble Time: 27.178 ms, Imbalance Overhead: 6.570 ms
GPU 3, Compute+Comm Time: 143.346 ms, Bubble Time: 26.835 ms, Imbalance Overhead: 7.988 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 335.154 ms
GPU 0, Compute+Comm Time: 269.407 ms, Bubble Time: 50.995 ms, Imbalance Overhead: 14.753 ms
GPU 1, Compute+Comm Time: 271.660 ms, Bubble Time: 51.295 ms, Imbalance Overhead: 12.199 ms
GPU 2, Compute+Comm Time: 271.660 ms, Bubble Time: 51.287 ms, Imbalance Overhead: 12.207 ms
GPU 3, Compute+Comm Time: 279.520 ms, Bubble Time: 52.360 ms, Imbalance Overhead: 3.274 ms
    The estimated cost with 2 DP ways is 538.989 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1478.751 ms
Partition 0 [0, 16) has cost: 1478.751 ms
Partition 1 [16, 32) has cost: 1451.841 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 222.464 ms
GPU 0, Compute+Comm Time: 194.107 ms, Bubble Time: 23.886 ms, Imbalance Overhead: 4.471 ms
GPU 1, Compute+Comm Time: 191.997 ms, Bubble Time: 24.603 ms, Imbalance Overhead: 5.863 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 370.938 ms
GPU 0, Compute+Comm Time: 321.159 ms, Bubble Time: 41.250 ms, Imbalance Overhead: 8.529 ms
GPU 1, Compute+Comm Time: 326.406 ms, Bubble Time: 40.056 ms, Imbalance Overhead: 4.476 ms
    The estimated cost with 4 DP ways is 623.072 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2930.592 ms
Partition 0 [0, 32) has cost: 2930.592 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 548.529 ms
GPU 0, Compute+Comm Time: 548.529 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 681.426 ms
GPU 0, Compute+Comm Time: 681.426 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1291.452 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2452
	Epoch 50:	Loss 3.0810
Node 1, Pre/Post-Pipelining: 1.109 / 1.107 ms, Bubble: 99.470 ms, Compute: 383.759 ms, Comm: 28.031 ms, Imbalance: 30.274 ms
Node 2, Pre/Post-Pipelining: 1.107 / 1.102 ms, Bubble: 100.733 ms, Compute: 370.661 ms, Comm: 30.215 ms, Imbalance: 40.418 ms
Node 4, Pre/Post-Pipelining: 1.107 / 1.081 ms, Bubble: 101.719 ms, Compute: 369.260 ms, Comm: 26.590 ms, Imbalance: 44.671 ms
Node 3, Pre/Post-Pipelining: 1.104 / 1.145 ms, Bubble: 100.417 ms, Compute: 382.389 ms, Comm: 26.532 ms, Imbalance: 32.095 ms
Node 5, Pre/Post-Pipelining: 1.107 / 1.072 ms, Bubble: 102.167 ms, Compute: 375.483 ms, Comm: 31.002 ms, Imbalance: 33.285 ms
Node 0, Pre/Post-Pipelining: 1.111 / 1.195 ms, Bubble: 98.341 ms, Compute: 418.574 ms, Comm: 16.809 ms, Imbalance: 7.117 ms
Node 6, Pre/Post-Pipelining: 1.109 / 1.079 ms, Bubble: 102.649 ms, Compute: 376.558 ms, Comm: 29.302 ms, Imbalance: 33.352 ms
Node 7, Pre/Post-Pipelining: 1.102 / 16.302 ms, Bubble: 88.143 ms, Compute: 391.793 ms, Comm: 17.362 ms, Imbalance: 29.062 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.111 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.195 ms
Cluster-Wide Average, Bubble: 98.341 ms
Cluster-Wide Average, Compute: 418.574 ms
Cluster-Wide Average, Communication: 16.809 ms
Cluster-Wide Average, Imbalance: 7.117 ms
Node 0, GPU memory consumption: 6.614 GB
Node 2, GPU memory consumption: 5.260 GB
Node 3, GPU memory consumption: 5.237 GB
Node 6, GPU memory consumption: 5.260 GB
Node 1, GPU memory consumption: 5.260 GB
Node 4, GPU memory consumption: 5.237 GB
Node 5, GPU memory consumption: 5.260 GB
Node 7, GPU memory consumption: 5.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.543822 s---------------
------------------------node id 1,  per-epoch time: 0.543822 s---------------
------------------------node id 2,  per-epoch time: 0.543821 s---------------
------------------------node id 3,  per-epoch time: 0.543822 s---------------
------------------------node id 4,  per-epoch time: 0.543820 s---------------
------------------------node id 5,  per-epoch time: 0.543820 s---------------
------------------------node id 6,  per-epoch time: 0.543820 s---------------
------------------------node id 7,  per-epoch time: 0.543821 s---------------
************ Profiling Results ************
	Bubble: 167.833766 (ms) (30.27 percentage)
	Compute: 382.164823 (ms) (68.92 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.511186 (ms) (0.81 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 405.625 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
