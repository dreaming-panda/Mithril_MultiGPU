Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.866 seconds.
Building the CSC structure...
        It takes 1.883 seconds.
Building the CSC structure...
        It takes 2.406 seconds.
Building the CSC structure...
        It takes 2.466 seconds.
Building the CSC structure...
        It takes 2.470 seconds.
Building the CSC structure...
        It takes 2.472 seconds.
Building the CSC structure...
        It takes 2.614 seconds.
Building the CSC structure...
        It takes 2.668 seconds.
Building the CSC structure...
        It takes 1.828 seconds.
        It takes 1.828 seconds.
        It takes 2.192 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.329 seconds.
        It takes 2.384 seconds.
        It takes 2.406 seconds.
        It takes 2.327 seconds.
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 2.298 seconds.
        It takes 0.030 seconds.
        It takes 0.314 seconds.
Building the Label Vector...
        It takes 0.045 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.317 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
        It takes 0.278 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
        It takes 0.262 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.032 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.032 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 54.928 Gbps (per GPU), 439.421 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.683 Gbps (per GPU), 437.463 Gbps (aggregated)
The layer-level communication performance: 54.679 Gbps (per GPU), 437.432 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.470 Gbps (per GPU), 435.762 Gbps (aggregated)
The layer-level communication performance: 54.440 Gbps (per GPU), 435.518 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.257 Gbps (per GPU), 434.059 Gbps (aggregated)
The layer-level communication performance: 54.221 Gbps (per GPU), 433.768 Gbps (aggregated)
The layer-level communication performance: 54.187 Gbps (per GPU), 433.498 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.431 Gbps (per GPU), 1251.450 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.469 Gbps (per GPU), 1251.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.457 Gbps (per GPU), 1251.657 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.437 Gbps (per GPU), 1251.494 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.454 Gbps (per GPU), 1251.634 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.483 Gbps (per GPU), 1251.868 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.451 Gbps (per GPU), 1251.611 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.457 Gbps (per GPU), 1251.657 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.741 Gbps (per GPU), 829.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.741 Gbps (per GPU), 829.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.742 Gbps (per GPU), 829.932 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.739 Gbps (per GPU), 829.912 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.741 Gbps (per GPU), 829.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.742 Gbps (per GPU), 829.932 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.741 Gbps (per GPU), 829.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.741 Gbps (per GPU), 829.925 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.636 Gbps (per GPU), 277.086 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.636 Gbps (per GPU), 277.085 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.635 Gbps (per GPU), 277.080 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.634 Gbps (per GPU), 277.075 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.635 Gbps (per GPU), 277.079 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.636 Gbps (per GPU), 277.086 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.633 Gbps (per GPU), 277.064 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.633 Gbps (per GPU), 277.062 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.43ms 10.07ms  9.27ms  1.34 29.12K 14.23M
 chk_1  7.88ms  5.58ms  4.74ms  1.66 29.12K  6.56M
 chk_2 19.73ms 17.57ms 16.63ms  1.19 29.12K 24.68M
 chk_3 19.72ms 17.47ms 16.63ms  1.19 29.12K 22.95M
 chk_4  7.74ms  5.42ms  4.61ms  1.68 29.12K  6.33M
 chk_5 11.98ms  9.48ms  8.68ms  1.38 29.12K 12.05M
 chk_6 13.11ms 10.66ms  9.91ms  1.32 29.12K 14.60M
 chk_7 12.24ms 10.14ms  9.08ms  1.35 29.12K 13.21M
   Avg 13.10 10.80  9.95
   Max 19.73 17.57 16.63
   Min  7.74  5.42  4.61
 Ratio  2.55  3.24  3.61
   Var 18.29 18.65 18.47
Profiling takes 3.087 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 364.009 ms
Partition 0 [0, 4) has cost: 364.009 ms
Partition 1 [4, 8) has cost: 345.574 ms
Partition 2 [8, 12) has cost: 345.574 ms
Partition 3 [12, 16) has cost: 345.574 ms
Partition 4 [16, 20) has cost: 345.574 ms
Partition 5 [20, 24) has cost: 345.574 ms
Partition 6 [24, 28) has cost: 345.574 ms
Partition 7 [28, 32) has cost: 338.745 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 315.861 ms
GPU 0, Compute+Comm Time: 131.284 ms, Bubble Time: 135.739 ms, Imbalance Overhead: 48.837 ms
GPU 1, Compute+Comm Time: 126.516 ms, Bubble Time: 126.156 ms, Imbalance Overhead: 63.188 ms
GPU 2, Compute+Comm Time: 126.516 ms, Bubble Time: 116.071 ms, Imbalance Overhead: 73.274 ms
GPU 3, Compute+Comm Time: 126.516 ms, Bubble Time: 116.982 ms, Imbalance Overhead: 72.362 ms
GPU 4, Compute+Comm Time: 126.516 ms, Bubble Time: 126.590 ms, Imbalance Overhead: 62.754 ms
GPU 5, Compute+Comm Time: 126.516 ms, Bubble Time: 135.713 ms, Imbalance Overhead: 53.631 ms
GPU 6, Compute+Comm Time: 126.516 ms, Bubble Time: 145.051 ms, Imbalance Overhead: 44.294 ms
GPU 7, Compute+Comm Time: 124.518 ms, Bubble Time: 155.266 ms, Imbalance Overhead: 36.076 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 619.523 ms
GPU 0, Compute+Comm Time: 241.371 ms, Bubble Time: 306.770 ms, Imbalance Overhead: 71.383 ms
GPU 1, Compute+Comm Time: 246.202 ms, Bubble Time: 286.199 ms, Imbalance Overhead: 87.122 ms
GPU 2, Compute+Comm Time: 246.202 ms, Bubble Time: 267.170 ms, Imbalance Overhead: 106.150 ms
GPU 3, Compute+Comm Time: 246.202 ms, Bubble Time: 248.681 ms, Imbalance Overhead: 124.640 ms
GPU 4, Compute+Comm Time: 246.202 ms, Bubble Time: 228.809 ms, Imbalance Overhead: 144.511 ms
GPU 5, Compute+Comm Time: 246.202 ms, Bubble Time: 226.337 ms, Imbalance Overhead: 146.984 ms
GPU 6, Compute+Comm Time: 246.202 ms, Bubble Time: 245.882 ms, Imbalance Overhead: 127.439 ms
GPU 7, Compute+Comm Time: 259.869 ms, Bubble Time: 263.944 ms, Imbalance Overhead: 95.710 ms
The estimated cost of the whole pipeline: 982.153 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 709.583 ms
Partition 0 [0, 8) has cost: 709.583 ms
Partition 1 [8, 16) has cost: 691.149 ms
Partition 2 [16, 24) has cost: 691.149 ms
Partition 3 [24, 32) has cost: 684.320 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 324.808 ms
GPU 0, Compute+Comm Time: 168.387 ms, Bubble Time: 155.629 ms, Imbalance Overhead: 0.792 ms
GPU 1, Compute+Comm Time: 165.982 ms, Bubble Time: 135.860 ms, Imbalance Overhead: 22.966 ms
GPU 2, Compute+Comm Time: 165.982 ms, Bubble Time: 115.689 ms, Imbalance Overhead: 43.137 ms
GPU 3, Compute+Comm Time: 165.050 ms, Bubble Time: 116.701 ms, Imbalance Overhead: 43.057 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 623.493 ms
GPU 0, Compute+Comm Time: 314.478 ms, Bubble Time: 224.081 ms, Imbalance Overhead: 84.935 ms
GPU 1, Compute+Comm Time: 316.910 ms, Bubble Time: 221.331 ms, Imbalance Overhead: 85.253 ms
GPU 2, Compute+Comm Time: 316.910 ms, Bubble Time: 260.421 ms, Imbalance Overhead: 46.163 ms
GPU 3, Compute+Comm Time: 323.782 ms, Bubble Time: 298.305 ms, Imbalance Overhead: 1.406 ms
    The estimated cost with 2 DP ways is 995.717 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1400.732 ms
Partition 0 [0, 16) has cost: 1400.732 ms
Partition 1 [16, 32) has cost: 1375.468 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 363.034 ms
GPU 0, Compute+Comm Time: 242.705 ms, Bubble Time: 120.329 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 241.041 ms, Bubble Time: 121.543 ms, Imbalance Overhead: 0.450 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 650.844 ms
GPU 0, Compute+Comm Time: 431.227 ms, Bubble Time: 218.401 ms, Imbalance Overhead: 1.216 ms
GPU 1, Compute+Comm Time: 435.748 ms, Bubble Time: 215.096 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1064.572 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2776.200 ms
Partition 0 [0, 32) has cost: 2776.200 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 486.901 ms
GPU 0, Compute+Comm Time: 486.901 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 679.183 ms
GPU 0, Compute+Comm Time: 679.183 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1224.388 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 287)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 287)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 287)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 287)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 287)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 287)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 287)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 287)...
+++++++++ Node 6 initializing the weights for op[0, 287)...
+++++++++ Node 4 initializing the weights for op[0, 287)...
+++++++++ Node 2 initializing the weights for op[0, 287)...
+++++++++ Node 0 initializing the weights for op[0, 287)...
+++++++++ Node 3 initializing the weights for op[0, 287)...
+++++++++ Node 5 initializing the weights for op[0, 287)...
+++++++++ Node 7 initializing the weights for op[0, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 5, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 6, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2003
	Epoch 50:	Loss 3.0599
Node 0, Pre/Post-Pipelining: 8.743 / 17.976 ms, Bubble: 0.978 ms, Compute: 1068.831 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 1, Pre/Post-Pipelining: 8.748 / 18.043 ms, Bubble: 0.851 ms, Compute: 1068.873 ms, Comm: 0.009 ms, Imbalance: 0.018 ms
Node 5, Pre/Post-Pipelining: 8.741 / 17.967 ms, Bubble: 1.041 ms, Compute: 1068.783 ms, Comm: 0.007 ms, Imbalance: 0.014 ms
Node 2, Pre/Post-Pipelining: 8.755 / 18.096 ms, Bubble: 0.355 ms, Compute: 1069.315 ms, Comm: 0.010 ms, Imbalance: 0.019 ms
Node 6, Pre/Post-Pipelining: 8.752 / 18.019 ms, Bubble: 0.548 ms, Compute: 1069.214 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 3, Pre/Post-Pipelining: 8.757 / 18.050 ms, Bubble: 0.055 ms, Compute: 1069.671 ms, Comm: 0.008 ms, Imbalance: 0.014 ms
Node 4, Pre/Post-Pipelining: 8.750 / 18.111 ms, Bubble: 1.006 ms, Compute: 1068.646 ms, Comm: 0.011 ms, Imbalance: 0.018 ms
Node 7, Pre/Post-Pipelining: 8.743 / 18.067 ms, Bubble: 1.023 ms, Compute: 1068.669 ms, Comm: 0.010 ms, Imbalance: 0.019 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.743 ms
Cluster-Wide Average, Post-Pipelining Overhead: 17.976 ms
Cluster-Wide Average, Bubble: 0.978 ms
Cluster-Wide Average, Compute: 1068.831 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.015 ms
Node 0, GPU memory consumption: 17.618 GB
Node 3, GPU memory consumption: 16.870 GB
Node 1, GPU memory consumption: 16.885 GB
Node 2, GPU memory consumption: 16.893 GB
Node 6, GPU memory consumption: 16.889 GB
Node 4, GPU memory consumption: 16.860 GB
Node 5, GPU memory consumption: 16.883 GB
Node 7, GPU memory consumption: 16.864 GB
Node 0, Graph-Level Communication Throughput: 22.486 Gbps, Time: 710.017 ms
Node 1, Graph-Level Communication Throughput: 20.048 Gbps, Time: 847.040 ms
Node 2, Graph-Level Communication Throughput: 35.977 Gbps, Time: 491.598 ms
Node 3, Graph-Level Communication Throughput: 49.701 Gbps, Time: 465.649 ms
Node 4, Graph-Level Communication Throughput: 9.703 Gbps, Time: 867.209 ms
Node 5, Graph-Level Communication Throughput: 16.707 Gbps, Time: 731.008 ms
Node 6, Graph-Level Communication Throughput: 23.478 Gbps, Time: 697.711 ms
Node 7, Graph-Level Communication Throughput: 18.875 Gbps, Time: 721.318 ms
------------------------node id 0,  per-epoch time: 1.096601 s---------------
------------------------node id 1,  per-epoch time: 1.096600 s---------------
------------------------node id 2,  per-epoch time: 1.096601 s---------------
------------------------node id 3,  per-epoch time: 1.096601 s---------------
------------------------node id 4,  per-epoch time: 1.096600 s---------------
------------------------node id 5,  per-epoch time: 1.096601 s---------------
------------------------node id 6,  per-epoch time: 1.096600 s---------------
------------------------node id 7,  per-epoch time: 1.096593 s---------------
************ Profiling Results ************
	Bubble: 28.458995 (ms) (2.59 percentage)
	Compute: 340.043840 (ms) (30.94 percentage)
	GraphCommComputeOverhead: 19.129157 (ms) (1.74 percentage)
	GraphCommNetwork: 691.443924 (ms) (62.92 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.909683 (ms) (1.81 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.022 GB
	Total communication (cluster-wide, per-epoch): 14.504 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
