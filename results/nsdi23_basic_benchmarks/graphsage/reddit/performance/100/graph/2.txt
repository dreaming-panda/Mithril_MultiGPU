Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.868 seconds.
Building the CSC structure...
        It takes 2.252 seconds.
Building the CSC structure...
        It takes 2.364 seconds.
Building the CSC structure...
        It takes 2.399 seconds.
Building the CSC structure...
        It takes 2.421 seconds.
Building the CSC structure...
        It takes 2.618 seconds.
Building the CSC structure...
        It takes 2.637 seconds.
Building the CSC structure...
        It takes 2.699 seconds.
Building the CSC structure...
        It takes 1.826 seconds.
        It takes 2.185 seconds.
        It takes 2.345 seconds.
Building the Feature Vector...
        It takes 2.300 seconds.
        It takes 2.350 seconds.
        It takes 0.252 seconds.
Building the Label Vector...
        It takes 2.313 seconds.
        It takes 0.035 seconds.
        It takes 2.347 seconds.
        It takes 2.325 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.310 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.300 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.035 seconds.
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.242 seconds.
Building the Label Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.031 seconds.
Building the Feature Vector...
        It takes 0.239 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.268 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.032 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.330 Gbps (per GPU), 458.644 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.068 Gbps (per GPU), 456.542 Gbps (aggregated)
The layer-level communication performance: 57.066 Gbps (per GPU), 456.526 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.849 Gbps (per GPU), 454.792 Gbps (aggregated)
The layer-level communication performance: 56.824 Gbps (per GPU), 454.595 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.645 Gbps (per GPU), 453.158 Gbps (aggregated)
The layer-level communication performance: 56.597 Gbps (per GPU), 452.777 Gbps (aggregated)
The layer-level communication performance: 56.569 Gbps (per GPU), 452.555 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.935 Gbps (per GPU), 1279.483 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.936 Gbps (per GPU), 1279.486 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.911 Gbps (per GPU), 1279.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.835 Gbps (per GPU), 1278.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.911 Gbps (per GPU), 1279.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.841 Gbps (per GPU), 1278.727 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.911 Gbps (per GPU), 1279.288 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.911 Gbps (per GPU), 1279.288 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.308 Gbps (per GPU), 834.466 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.308 Gbps (per GPU), 834.466 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.307 Gbps (per GPU), 834.459 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.308 Gbps (per GPU), 834.466 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.310 Gbps (per GPU), 834.480 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.308 Gbps (per GPU), 834.466 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.307 Gbps (per GPU), 834.460 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.308 Gbps (per GPU), 834.466 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.577 Gbps (per GPU), 268.615 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.577 Gbps (per GPU), 268.618 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.576 Gbps (per GPU), 268.612 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.576 Gbps (per GPU), 268.610 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.576 Gbps (per GPU), 268.608 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.577 Gbps (per GPU), 268.618 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.577 Gbps (per GPU), 268.615 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.575 Gbps (per GPU), 268.604 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.70ms 10.17ms  9.29ms  1.37 29.12K 14.23M
 chk_1  7.96ms  5.65ms  4.79ms  1.66 29.12K  6.56M
 chk_2 19.87ms 17.58ms 16.79ms  1.18 29.12K 24.68M
 chk_3 19.84ms 17.46ms 16.75ms  1.18 29.12K 22.95M
 chk_4  7.75ms  5.61ms  4.63ms  1.67 29.12K  6.33M
 chk_5 12.02ms  9.71ms  8.80ms  1.37 29.12K 12.05M
 chk_6 13.23ms 10.83ms 10.03ms  1.32 29.12K 14.60M
 chk_7 12.32ms 10.13ms  9.22ms  1.34 29.12K 13.21M
   Avg 13.21 10.89 10.04
   Max 19.87 17.58 16.79
   Min  7.75  5.61  4.63
 Ratio  2.56  3.13  3.62
   Var 18.51 18.21 18.78
Profiling takes 3.116 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 367.108 ms
Partition 0 [0, 4) has cost: 367.108 ms
Partition 1 [4, 8) has cost: 348.554 ms
Partition 2 [8, 12) has cost: 348.554 ms
Partition 3 [12, 16) has cost: 348.554 ms
Partition 4 [16, 20) has cost: 348.554 ms
Partition 5 [20, 24) has cost: 348.554 ms
Partition 6 [24, 28) has cost: 348.554 ms
Partition 7 [28, 32) has cost: 341.723 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 316.360 ms
GPU 0, Compute+Comm Time: 131.584 ms, Bubble Time: 136.093 ms, Imbalance Overhead: 48.682 ms
GPU 1, Compute+Comm Time: 126.742 ms, Bubble Time: 126.708 ms, Imbalance Overhead: 62.910 ms
GPU 2, Compute+Comm Time: 126.742 ms, Bubble Time: 116.809 ms, Imbalance Overhead: 72.809 ms
GPU 3, Compute+Comm Time: 126.742 ms, Bubble Time: 117.758 ms, Imbalance Overhead: 71.860 ms
GPU 4, Compute+Comm Time: 126.742 ms, Bubble Time: 127.118 ms, Imbalance Overhead: 62.500 ms
GPU 5, Compute+Comm Time: 126.742 ms, Bubble Time: 136.030 ms, Imbalance Overhead: 53.588 ms
GPU 6, Compute+Comm Time: 126.742 ms, Bubble Time: 145.180 ms, Imbalance Overhead: 44.438 ms
GPU 7, Compute+Comm Time: 124.717 ms, Bubble Time: 155.426 ms, Imbalance Overhead: 36.216 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 620.474 ms
GPU 0, Compute+Comm Time: 243.013 ms, Bubble Time: 307.012 ms, Imbalance Overhead: 70.449 ms
GPU 1, Compute+Comm Time: 247.819 ms, Bubble Time: 286.537 ms, Imbalance Overhead: 86.118 ms
GPU 2, Compute+Comm Time: 247.819 ms, Bubble Time: 267.889 ms, Imbalance Overhead: 104.765 ms
GPU 3, Compute+Comm Time: 247.819 ms, Bubble Time: 249.802 ms, Imbalance Overhead: 122.853 ms
GPU 4, Compute+Comm Time: 247.819 ms, Bubble Time: 230.258 ms, Imbalance Overhead: 142.396 ms
GPU 5, Compute+Comm Time: 247.819 ms, Bubble Time: 227.736 ms, Imbalance Overhead: 144.919 ms
GPU 6, Compute+Comm Time: 247.819 ms, Bubble Time: 247.002 ms, Imbalance Overhead: 125.652 ms
GPU 7, Compute+Comm Time: 261.530 ms, Bubble Time: 264.730 ms, Imbalance Overhead: 94.213 ms
The estimated cost of the whole pipeline: 983.675 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 715.662 ms
Partition 0 [0, 8) has cost: 715.662 ms
Partition 1 [8, 16) has cost: 697.108 ms
Partition 2 [16, 24) has cost: 697.108 ms
Partition 3 [24, 32) has cost: 690.278 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 324.477 ms
GPU 0, Compute+Comm Time: 168.488 ms, Bubble Time: 155.232 ms, Imbalance Overhead: 0.757 ms
GPU 1, Compute+Comm Time: 165.973 ms, Bubble Time: 135.766 ms, Imbalance Overhead: 22.738 ms
GPU 2, Compute+Comm Time: 165.973 ms, Bubble Time: 115.837 ms, Imbalance Overhead: 42.668 ms
GPU 3, Compute+Comm Time: 165.014 ms, Bubble Time: 116.968 ms, Imbalance Overhead: 42.494 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 623.382 ms
GPU 0, Compute+Comm Time: 315.683 ms, Bubble Time: 224.696 ms, Imbalance Overhead: 83.004 ms
GPU 1, Compute+Comm Time: 318.011 ms, Bubble Time: 221.879 ms, Imbalance Overhead: 83.492 ms
GPU 2, Compute+Comm Time: 318.011 ms, Bubble Time: 260.443 ms, Imbalance Overhead: 44.929 ms
GPU 3, Compute+Comm Time: 325.011 ms, Bubble Time: 297.731 ms, Imbalance Overhead: 0.640 ms
    The estimated cost with 2 DP ways is 995.252 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1412.771 ms
Partition 0 [0, 16) has cost: 1412.771 ms
Partition 1 [16, 32) has cost: 1387.386 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 361.941 ms
GPU 0, Compute+Comm Time: 242.070 ms, Bubble Time: 119.871 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 240.439 ms, Bubble Time: 121.366 ms, Imbalance Overhead: 0.136 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 649.720 ms
GPU 0, Compute+Comm Time: 430.496 ms, Bubble Time: 218.091 ms, Imbalance Overhead: 1.132 ms
GPU 1, Compute+Comm Time: 435.033 ms, Bubble Time: 214.687 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1062.243 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2800.157 ms
Partition 0 [0, 32) has cost: 2800.157 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 496.905 ms
GPU 0, Compute+Comm Time: 496.905 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 688.852 ms
GPU 0, Compute+Comm Time: 688.852 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1245.045 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 287)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 287)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 287)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 287)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 287)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 287)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 287)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 287)...
+++++++++ Node 1 initializing the weights for op[0, 287)...
+++++++++ Node 4 initializing the weights for op[0, 287)...
+++++++++ Node 2 initializing the weights for op[0, 287)...
+++++++++ Node 5 initializing the weights for op[0, 287)...
+++++++++ Node 3 initializing the weights for op[0, 287)...
+++++++++ Node 6 initializing the weights for op[0, 287)...
+++++++++ Node 7 initializing the weights for op[0, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2004
	Epoch 50:	Loss 3.0598
	Epoch 75:	Loss 2.9902
	Epoch 100:	Loss 2.9399
	Epoch 125:	Loss 2.9184
	Epoch 150:	Loss 2.9006
	Epoch 175:	Loss 2.8901
	Epoch 200:	Loss 2.8794
	Epoch 225:	Loss 2.8610
	Epoch 250:	Loss 2.8364
	Epoch 275:	Loss 2.8128
	Epoch 300:	Loss 2.7748
	Epoch 325:	Loss 2.6996
	Epoch 350:	Loss 2.6569
	Epoch 375:	Loss 2.6130
	Epoch 400:	Loss 2.6052
	Epoch 425:	Loss 2.5037
	Epoch 450:	Loss 2.4700
	Epoch 475:	Loss 2.4123
	Epoch 500:	Loss 2.4363
	Epoch 525:	Loss 2.2839
	Epoch 550:	Loss 2.2103
	Epoch 575:	Loss 2.1323
	Epoch 600:	Loss 2.1349
	Epoch 625:	Loss 2.1073
	Epoch 650:	Loss 2.1056
	Epoch 675:	Loss 2.0819
	Epoch 700:	Loss 2.0224
	Epoch 725:	Loss 1.9099
	Epoch 750:	Loss 1.8659
	Epoch 775:	Loss 1.8716
	Epoch 800:	Loss 1.9021
	Epoch 825:	Loss 1.7853
	Epoch 850:	Loss 1.8580
	Epoch 875:	Loss 1.7547
	Epoch 900:	Loss 1.7148
	Epoch 925:	Loss 1.8875
	Epoch 950:	Loss 1.6417
	Epoch 975:	Loss 1.6150
	Epoch 1000:	Loss 1.6042
	Epoch 1025:	Loss 1.5796
	Epoch 1050:	Loss 1.5805
	Epoch 1075:	Loss 1.5590
	Epoch 1100:	Loss 1.6227
	Epoch 1125:	Loss 1.4858
	Epoch 1150:	Loss 1.4805
	Epoch 1175:	Loss 1.4797
	Epoch 1200:	Loss 1.5008
	Epoch 1225:	Loss 1.4076
	Epoch 1250:	Loss 1.4006
	Epoch 1275:	Loss 1.4568
	Epoch 1300:	Loss 1.4765
	Epoch 1325:	Loss 1.3326
	Epoch 1350:	Loss 1.4038
	Epoch 1375:	Loss 1.3022
	Epoch 1400:	Loss 1.3227
	Epoch 1425:	Loss 1.2991
	Epoch 1450:	Loss 1.4317
	Epoch 1475:	Loss 1.3871
	Epoch 1500:	Loss 1.3653
	Epoch 1525:	Loss 1.2420
	Epoch 1550:	Loss 1.2351
	Epoch 1575:	Loss 1.2226
	Epoch 1600:	Loss 1.2033
	Epoch 1625:	Loss 1.1822
	Epoch 1650:	Loss 1.1983
	Epoch 1675:	Loss 1.1735
	Epoch 1700:	Loss 1.2184
	Epoch 1725:	Loss 1.1819
	Epoch 1750:	Loss 1.3201
	Epoch 1775:	Loss 1.2763
	Epoch 1800:	Loss 1.2042
	Epoch 1825:	Loss 1.0903
	Epoch 1850:	Loss 1.1118
	Epoch 1875:	Loss 1.7054
	Epoch 1900:	Loss 1.1263
	Epoch 1925:	Loss 1.1185
	Epoch 1950:	Loss 1.0708
	Epoch 1975:	Loss 1.1588
	Epoch 2000:	Loss 1.1070
	Epoch 2025:	Loss 1.0481
	Epoch 2050:	Loss 1.0410
	Epoch 2075:	Loss 1.0409
	Epoch 2100:	Loss 1.2719
	Epoch 2125:	Loss 1.1299
	Epoch 2150:	Loss 1.0472
	Epoch 2175:	Loss 1.0060
	Epoch 2200:	Loss 0.9761
	Epoch 2225:	Loss 0.9478
	Epoch 2250:	Loss 1.2364
	Epoch 2275:	Loss 1.0321
	Epoch 2300:	Loss 0.9531
	Epoch 2325:	Loss 0.9150
	Epoch 2350:	Loss 1.0380
	Epoch 2375:	Loss 0.9298
	Epoch 2400:	Loss 1.0332
	Epoch 2425:	Loss 0.9030
	Epoch 2450:	Loss 0.9145
	Epoch 2475:	Loss 0.9498
	Epoch 2500:	Loss 0.8779
	Epoch 2525:	Loss 0.9631
	Epoch 2550:	Loss 0.8922
	Epoch 2575:	Loss 0.8556
	Epoch 2600:	Loss 1.1096
	Epoch 2625:	Loss 0.8652
	Epoch 2650:	Loss 0.8874
	Epoch 2675:	Loss 0.8871
	Epoch 2700:	Loss 0.8585
	Epoch 2725:	Loss 0.8594
	Epoch 2750:	Loss 1.0951
	Epoch 2775:	Loss 0.9564
	Epoch 2800:	Loss 0.8544
	Epoch 2825:	Loss 0.9910
	Epoch 2850:	Loss 0.8239
	Epoch 2875:	Loss 0.8658
	Epoch 2900:	Loss 0.8116
	Epoch 2925:	Loss 0.8088
	Epoch 2950:	Loss 0.8306
	Epoch 2975:	Loss 0.8021
	Epoch 3000:	Loss 0.7991
	Epoch 3025:	Loss 0.9486
	Epoch 3050:	Loss 0.7904
	Epoch 3075:	Loss 0.8265
	Epoch 3100:	Loss 0.8497
	Epoch 3125:	Loss 0.8989
	Epoch 3150:	Loss 0.7760
	Epoch 3175:	Loss 0.7786
	Epoch 3200:	Loss 0.7664
	Epoch 3225:	Loss 0.7759
	Epoch 3250:	Loss 0.7728
	Epoch 3275:	Loss 0.7511
	Epoch 3300:	Loss 1.0682
	Epoch 3325:	Loss 0.7680
	Epoch 3350:	Loss 0.7551
	Epoch 3375:	Loss 0.7687
	Epoch 3400:	Loss 0.8180
	Epoch 3425:	Loss 0.7636
	Epoch 3450:	Loss 0.7522
	Epoch 3475:	Loss 0.8367
	Epoch 3500:	Loss 0.7486
	Epoch 3525:	Loss 0.8193
	Epoch 3550:	Loss 0.7259
	Epoch 3575:	Loss 0.7626
	Epoch 3600:	Loss 0.7448
	Epoch 3625:	Loss 0.7659
	Epoch 3650:	Loss 0.7362
	Epoch 3675:	Loss 0.7083
	Epoch 3700:	Loss 0.7070
	Epoch 3725:	Loss 0.7119
	Epoch 3750:	Loss 0.7222
	Epoch 3775:	Loss 0.6980
	Epoch 3800:	Loss 0.7041
	Epoch 3825:	Loss 0.6917
	Epoch 3850:	Loss 0.7392
	Epoch 3875:	Loss 0.8301
	Epoch 3900:	Loss 0.7047
	Epoch 3925:	Loss 0.6922
	Epoch 3950:	Loss 0.6884
	Epoch 3975:	Loss 0.6579
	Epoch 4000:	Loss 0.7186
	Epoch 4025:	Loss 0.7325
	Epoch 4050:	Loss 0.6748
	Epoch 4075:	Loss 0.6830
	Epoch 4100:	Loss 0.6784
	Epoch 4125:	Loss 0.6649
	Epoch 4150:	Loss 0.6618
	Epoch 4175:	Loss 0.6507
	Epoch 4200:	Loss 0.6591
	Epoch 4225:	Loss 0.6414
	Epoch 4250:	Loss 0.6501
	Epoch 4275:	Loss 0.6739
	Epoch 4300:	Loss 0.6461
	Epoch 4325:	Loss 0.9480
	Epoch 4350:	Loss 0.7102
	Epoch 4375:	Loss 0.6301
	Epoch 4400:	Loss 0.6402
	Epoch 4425:	Loss 0.6403
	Epoch 4450:	Loss 0.6209
	Epoch 4475:	Loss 0.6209
	Epoch 4500:	Loss 0.6379
	Epoch 4525:	Loss 0.6069
	Epoch 4550:	Loss 0.9919
	Epoch 4575:	Loss 0.8379
	Epoch 4600:	Loss 0.6356
	Epoch 4625:	Loss 0.6179
	Epoch 4650:	Loss 0.6074
	Epoch 4675:	Loss 0.6335
	Epoch 4700:	Loss 0.6171
	Epoch 4725:	Loss 0.6429
	Epoch 4750:	Loss 0.5992
	Epoch 4775:	Loss 0.6364
	Epoch 4800:	Loss 0.6124
	Epoch 4825:	Loss 0.7036
	Epoch 4850:	Loss 0.6176
	Epoch 4875:	Loss 0.6202
	Epoch 4900:	Loss 0.6000
	Epoch 4925:	Loss 0.6109
	Epoch 4950:	Loss 0.6480
	Epoch 4975:	Loss 0.5770
Node 1, Pre/Post-Pipelining: 8.700 / 25.518 ms, Bubble: 1.134 ms, Compute: 1081.741 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 5, Pre/Post-Pipelining: 8.700 / 25.548 ms, Bubble: 1.237 ms, Compute: 1081.616 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 8.698 / 25.492 ms, Bubble: 1.239 ms, Compute: 1081.673 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 4, Pre/Post-Pipelining: 8.705 / 25.564 ms, Bubble: 1.302 ms, Compute: 1081.525 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
	Epoch 5000:	Loss 0.5860
Node 6, Pre/Post-Pipelining: 8.712 / 25.595 ms, Bubble: 0.759 ms, Compute: 1082.036 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 8.713 / 25.615 ms, Bubble: 0.386 ms, Compute: 1082.387 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 3, Pre/Post-Pipelining: 8.723 / 25.720 ms, Bubble: 0.056 ms, Compute: 1082.591 ms, Comm: 0.011 ms, Imbalance: 0.022 ms
Node 0, Pre/Post-Pipelining: 8.706 / 25.618 ms, Bubble: 1.157 ms, Compute: 1081.611 ms, Comm: 0.011 ms, Imbalance: 0.019 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.706 ms
Cluster-Wide Average, Post-Pipelining Overhead: 25.618 ms
Cluster-Wide Average, Bubble: 1.157 ms
Cluster-Wide Average, Compute: 1081.611 ms
Cluster-Wide Average, Communication: 0.011 ms
Cluster-Wide Average, Imbalance: 0.019 ms
Node 0, GPU memory consumption: 17.618 GB
Node 1, GPU memory consumption: 16.885 GB
Node 2, GPU memory consumption: 16.893 GB
Node 4, GPU memory consumption: 16.860 GB
Node 3, GPU memory consumption: 16.870 GB
Node 5, GPU memory consumption: 16.883 GB
Node 6, GPU memory consumption: 16.889 GB
Node 7, GPU memory consumption: 16.864 GB
Node 0, Graph-Level Communication Throughput: 22.298 Gbps, Time: 715.982 ms
Node 1, Graph-Level Communication Throughput: 19.811 Gbps, Time: 857.188 ms
Node 2, Graph-Level Communication Throughput: 35.694 Gbps, Time: 495.488 ms
Node 4, Graph-Level Communication Throughput: 9.573 Gbps, Time: 879.025 ms
Node 3, Graph-Level Communication Throughput: 49.114 Gbps, Time: 471.209 ms
Node 5, Graph-Level Communication Throughput: 16.557 Gbps, Time: 737.641 ms
Node 6, Graph-Level Communication Throughput: 23.209 Gbps, Time: 705.811 ms
Node 7, Graph-Level Communication Throughput: 18.701 Gbps, Time: 728.036 ms
------------------------node id 0,  per-epoch time: 1.117174 s---------------
------------------------node id 1,  per-epoch time: 1.117174 s---------------
------------------------node id 2,  per-epoch time: 1.117174 s---------------
------------------------node id 3,  per-epoch time: 1.117174 s---------------
------------------------node id 4,  per-epoch time: 1.117174 s---------------
------------------------node id 5,  per-epoch time: 1.117174 s---------------
------------------------node id 6,  per-epoch time: 1.117174 s---------------
------------------------node id 7,  per-epoch time: 1.117174 s---------------
************ Profiling Results ************
	Bubble: 35.559415 (ms) (3.18 percentage)
	Compute: 343.378829 (ms) (30.74 percentage)
	GraphCommComputeOverhead: 19.281985 (ms) (1.73 percentage)
	GraphCommNetwork: 698.795683 (ms) (62.55 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 20.168745 (ms) (1.81 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.022 GB
	Total communication (cluster-wide, per-epoch): 14.504 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
