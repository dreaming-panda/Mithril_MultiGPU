Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.011 seconds.
Building the CSC structure...
        It takes 2.060 seconds.
Building the CSC structure...
        It takes 2.262 seconds.
Building the CSC structure...
        It takes 2.408 seconds.
Building the CSC structure...
        It takes 2.426 seconds.
Building the CSC structure...
        It takes 2.443 seconds.
Building the CSC structure...
        It takes 2.608 seconds.
Building the CSC structure...
        It takes 2.683 seconds.
Building the CSC structure...
        It takes 1.869 seconds.
        It takes 1.867 seconds.
        It takes 2.200 seconds.
        It takes 2.340 seconds.
        It takes 2.345 seconds.
        It takes 2.363 seconds.
        It takes 2.335 seconds.
Building the Feature Vector...
        It takes 2.354 seconds.
Building the Feature Vector...
        It takes 0.253 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.241 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.334 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.043 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
        It takes 0.254 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.256 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.747 Gbps (per GPU), 445.972 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.504 Gbps (per GPU), 444.030 Gbps (aggregated)
The layer-level communication performance: 55.499 Gbps (per GPU), 443.991 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.292 Gbps (per GPU), 442.339 Gbps (aggregated)
The layer-level communication performance: 55.271 Gbps (per GPU), 442.164 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.085 Gbps (per GPU), 440.681 Gbps (aggregated)
The layer-level communication performance: 55.126 Gbps (per GPU), 441.008 Gbps (aggregated)
The layer-level communication performance: 55.013 Gbps (per GPU), 440.102 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.920 Gbps (per GPU), 1287.361 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.908 Gbps (per GPU), 1287.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.887 Gbps (per GPU), 1287.092 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.902 Gbps (per GPU), 1287.213 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.920 Gbps (per GPU), 1287.362 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.908 Gbps (per GPU), 1287.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.899 Gbps (per GPU), 1287.192 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.911 Gbps (per GPU), 1287.287 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.613 Gbps (per GPU), 836.908 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.614 Gbps (per GPU), 836.908 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.614 Gbps (per GPU), 836.915 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.614 Gbps (per GPU), 836.908 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.611 Gbps (per GPU), 836.887 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.612 Gbps (per GPU), 836.894 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.610 Gbps (per GPU), 836.880 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.607 Gbps (per GPU), 836.859 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.931 Gbps (per GPU), 271.445 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.931 Gbps (per GPU), 271.450 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.929 Gbps (per GPU), 271.436 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.929 Gbps (per GPU), 271.429 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.931 Gbps (per GPU), 271.448 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.931 Gbps (per GPU), 271.448 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.930 Gbps (per GPU), 271.442 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.930 Gbps (per GPU), 271.442 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.28ms 10.01ms  9.25ms  1.33 29.12K 14.23M
 chk_1  7.87ms  5.56ms  4.72ms  1.67 29.12K  6.56M
 chk_2 19.55ms 17.32ms 16.50ms  1.18 29.12K 24.68M
 chk_3 19.58ms 17.35ms 16.53ms  1.18 29.12K 22.95M
 chk_4  7.68ms  5.42ms  4.57ms  1.68 29.12K  6.33M
 chk_5 11.85ms  9.50ms  8.64ms  1.37 29.12K 12.05M
 chk_6 13.07ms 10.60ms  9.89ms  1.32 29.12K 14.60M
 chk_7 12.18ms 10.13ms  9.04ms  1.35 29.12K 13.21M
   Avg 13.01 10.74  9.89
   Max 19.58 17.35 16.53
   Min  7.68  5.42  4.57
 Ratio  2.55  3.20  3.61
   Var 17.92 18.07 18.18
Profiling takes 3.067 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 361.723 ms
Partition 0 [0, 4) has cost: 361.723 ms
Partition 1 [4, 8) has cost: 343.540 ms
Partition 2 [8, 12) has cost: 343.540 ms
Partition 3 [12, 16) has cost: 343.540 ms
Partition 4 [16, 20) has cost: 343.540 ms
Partition 5 [20, 24) has cost: 343.540 ms
Partition 6 [24, 28) has cost: 343.540 ms
Partition 7 [28, 32) has cost: 336.795 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 313.004 ms
GPU 0, Compute+Comm Time: 130.348 ms, Bubble Time: 134.730 ms, Imbalance Overhead: 47.926 ms
GPU 1, Compute+Comm Time: 125.793 ms, Bubble Time: 125.380 ms, Imbalance Overhead: 61.831 ms
GPU 2, Compute+Comm Time: 125.793 ms, Bubble Time: 115.531 ms, Imbalance Overhead: 71.680 ms
GPU 3, Compute+Comm Time: 125.793 ms, Bubble Time: 116.255 ms, Imbalance Overhead: 70.956 ms
GPU 4, Compute+Comm Time: 125.793 ms, Bubble Time: 125.736 ms, Imbalance Overhead: 61.475 ms
GPU 5, Compute+Comm Time: 125.793 ms, Bubble Time: 134.636 ms, Imbalance Overhead: 52.575 ms
GPU 6, Compute+Comm Time: 125.793 ms, Bubble Time: 143.733 ms, Imbalance Overhead: 43.478 ms
GPU 7, Compute+Comm Time: 123.749 ms, Bubble Time: 153.723 ms, Imbalance Overhead: 35.532 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 613.509 ms
GPU 0, Compute+Comm Time: 239.792 ms, Bubble Time: 303.355 ms, Imbalance Overhead: 70.362 ms
GPU 1, Compute+Comm Time: 244.492 ms, Bubble Time: 283.372 ms, Imbalance Overhead: 85.645 ms
GPU 2, Compute+Comm Time: 244.492 ms, Bubble Time: 264.783 ms, Imbalance Overhead: 104.234 ms
GPU 3, Compute+Comm Time: 244.492 ms, Bubble Time: 246.716 ms, Imbalance Overhead: 122.301 ms
GPU 4, Compute+Comm Time: 244.492 ms, Bubble Time: 226.993 ms, Imbalance Overhead: 142.024 ms
GPU 5, Compute+Comm Time: 244.492 ms, Bubble Time: 224.780 ms, Imbalance Overhead: 144.237 ms
GPU 6, Compute+Comm Time: 244.492 ms, Bubble Time: 244.261 ms, Imbalance Overhead: 124.756 ms
GPU 7, Compute+Comm Time: 258.120 ms, Bubble Time: 261.974 ms, Imbalance Overhead: 93.415 ms
The estimated cost of the whole pipeline: 972.839 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 705.263 ms
Partition 0 [0, 8) has cost: 705.263 ms
Partition 1 [8, 16) has cost: 687.079 ms
Partition 2 [16, 24) has cost: 687.079 ms
Partition 3 [24, 32) has cost: 680.335 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 321.796 ms
GPU 0, Compute+Comm Time: 166.923 ms, Bubble Time: 154.016 ms, Imbalance Overhead: 0.857 ms
GPU 1, Compute+Comm Time: 164.701 ms, Bubble Time: 134.817 ms, Imbalance Overhead: 22.278 ms
GPU 2, Compute+Comm Time: 164.701 ms, Bubble Time: 115.119 ms, Imbalance Overhead: 41.976 ms
GPU 3, Compute+Comm Time: 163.734 ms, Bubble Time: 115.782 ms, Imbalance Overhead: 42.280 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 617.235 ms
GPU 0, Compute+Comm Time: 311.708 ms, Bubble Time: 221.453 ms, Imbalance Overhead: 84.074 ms
GPU 1, Compute+Comm Time: 313.999 ms, Bubble Time: 219.295 ms, Imbalance Overhead: 83.941 ms
GPU 2, Compute+Comm Time: 313.999 ms, Bubble Time: 258.256 ms, Imbalance Overhead: 44.980 ms
GPU 3, Compute+Comm Time: 320.868 ms, Bubble Time: 295.449 ms, Imbalance Overhead: 0.918 ms
    The estimated cost with 2 DP ways is 985.983 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1392.342 ms
Partition 0 [0, 16) has cost: 1392.342 ms
Partition 1 [16, 32) has cost: 1367.414 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 359.390 ms
GPU 0, Compute+Comm Time: 240.046 ms, Bubble Time: 119.344 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 238.450 ms, Bubble Time: 119.884 ms, Imbalance Overhead: 1.056 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 644.843 ms
GPU 0, Compute+Comm Time: 426.816 ms, Bubble Time: 215.570 ms, Imbalance Overhead: 2.458 ms
GPU 1, Compute+Comm Time: 431.325 ms, Bubble Time: 213.518 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1054.445 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2759.756 ms
Partition 0 [0, 32) has cost: 2759.756 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 490.974 ms
GPU 0, Compute+Comm Time: 490.974 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 680.757 ms
GPU 0, Compute+Comm Time: 680.757 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1230.318 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 287)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 287)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 287)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 287)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 287)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 287)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 287)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 287)...
+++++++++ Node 4 initializing the weights for op[0, 287)...
+++++++++ Node 0 initializing the weights for op[0, 287)...
+++++++++ Node 5 initializing the weights for op[0, 287)...
+++++++++ Node 2 initializing the weights for op[0, 287)...
+++++++++ Node 6 initializing the weights for op[0, 287)...
+++++++++ Node 3 initializing the weights for op[0, 287)...
+++++++++ Node 7 initializing the weights for op[0, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.7121
	Epoch 50:	Loss 3.7119
Node 0, Pre/Post-Pipelining: 8.742 / 19.613 ms, Bubble: 1.007 ms, Compute: 1068.186 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 1, Pre/Post-Pipelining: 8.752 / 19.686 ms, Bubble: 0.885 ms, Compute: 1068.210 ms, Comm: 0.010 ms, Imbalance: 0.019 ms
Node 4, Pre/Post-Pipelining: 8.745 / 19.639 ms, Bubble: 0.996 ms, Compute: 1068.165 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 8.755 / 19.725 ms, Bubble: 0.425 ms, Compute: 1068.637 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 7, Pre/Post-Pipelining: 8.740 / 19.597 ms, Bubble: 1.120 ms, Compute: 1068.088 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 3, Pre/Post-Pipelining: 8.759 / 19.755 ms, Bubble: 0.048 ms, Compute: 1068.983 ms, Comm: 0.009 ms, Imbalance: 0.018 ms
Node 5, Pre/Post-Pipelining: 8.740 / 19.607 ms, Bubble: 1.055 ms, Compute: 1068.147 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 6, Pre/Post-Pipelining: 8.758 / 19.717 ms, Bubble: 0.531 ms, Compute: 1068.540 ms, Comm: 0.010 ms, Imbalance: 0.019 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.742 ms
Cluster-Wide Average, Post-Pipelining Overhead: 19.613 ms
Cluster-Wide Average, Bubble: 1.007 ms
Cluster-Wide Average, Compute: 1068.186 ms
Cluster-Wide Average, Communication: 0.008 ms
Cluster-Wide Average, Imbalance: 0.015 ms
Node 0, GPU memory consumption: 17.618 GB
Node 1, GPU memory consumption: 16.885 GB
Node 3, GPU memory consumption: 16.870 GB
Node 2, GPU memory consumption: 16.893 GB
Node 4, GPU memory consumption: 16.860 GB
Node 5, GPU memory consumption: 16.883 GB
Node 6, GPU memory consumption: 16.889 GB
Node 7, GPU memory consumption: 16.864 GB
Node 0, Graph-Level Communication Throughput: 22.436 Gbps, Time: 711.587 ms
Node 1, Graph-Level Communication Throughput: 20.036 Gbps, Time: 847.562 ms
Node 2, Graph-Level Communication Throughput: 35.705 Gbps, Time: 495.335 ms
Node 3, Graph-Level Communication Throughput: 49.494 Gbps, Time: 467.597 ms
Node 4, Graph-Level Communication Throughput: 9.700 Gbps, Time: 867.551 ms
Node 5, Graph-Level Communication Throughput: 16.666 Gbps, Time: 732.806 ms
Node 6, Graph-Level Communication Throughput: 23.452 Gbps, Time: 698.499 ms
Node 7, Graph-Level Communication Throughput: 18.783 Gbps, Time: 724.861 ms
------------------------node id 0,  per-epoch time: 1.097619 s---------------
------------------------node id 1,  per-epoch time: 1.097620 s---------------
------------------------node id 2,  per-epoch time: 1.097619 s---------------
------------------------node id 3,  per-epoch time: 1.097619 s---------------
------------------------node id 4,  per-epoch time: 1.097618 s---------------
------------------------node id 5,  per-epoch time: 1.097621 s---------------
------------------------node id 6,  per-epoch time: 1.097619 s---------------
------------------------node id 7,  per-epoch time: 1.097619 s---------------
************ Profiling Results ************
	Bubble: 29.810720 (ms) (2.71 percentage)
	Compute: 337.805873 (ms) (30.71 percentage)
	GraphCommComputeOverhead: 19.080731 (ms) (1.73 percentage)
	GraphCommNetwork: 693.224105 (ms) (63.03 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.896616 (ms) (1.81 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.022 GB
	Total communication (cluster-wide, per-epoch): 14.504 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
