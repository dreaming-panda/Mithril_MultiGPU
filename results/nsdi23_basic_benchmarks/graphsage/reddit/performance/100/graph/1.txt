Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.872 seconds.
Building the CSC structure...
        It takes 2.059 seconds.
Building the CSC structure...
        It takes 2.201 seconds.
Building the CSC structure...
        It takes 2.403 seconds.
Building the CSC structure...
        It takes 2.436 seconds.
Building the CSC structure...
        It takes 2.612 seconds.
Building the CSC structure...
        It takes 2.629 seconds.
Building the CSC structure...
        It takes 2.657 seconds.
Building the CSC structure...
        It takes 1.831 seconds.
        It takes 1.860 seconds.
        It takes 2.317 seconds.
Building the Feature Vector...
        It takes 2.318 seconds.
        It takes 2.448 seconds.
        It takes 0.248 seconds.
Building the Label Vector...
        It takes 2.334 seconds.
        It takes 2.352 seconds.
        It takes 0.040 seconds.
        It takes 2.349 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
        It takes 0.313 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.289 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
        It takes 0.236 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.168 Gbps (per GPU), 449.341 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.920 Gbps (per GPU), 447.362 Gbps (aggregated)
The layer-level communication performance: 55.896 Gbps (per GPU), 447.171 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.695 Gbps (per GPU), 445.563 Gbps (aggregated)
The layer-level communication performance: 55.671 Gbps (per GPU), 445.367 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.486 Gbps (per GPU), 443.889 Gbps (aggregated)
The layer-level communication performance: 55.456 Gbps (per GPU), 443.649 Gbps (aggregated)
The layer-level communication performance: 55.412 Gbps (per GPU), 443.300 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.796 Gbps (per GPU), 1278.365 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.743 Gbps (per GPU), 1277.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.801 Gbps (per GPU), 1278.410 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.759 Gbps (per GPU), 1278.069 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.780 Gbps (per GPU), 1278.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.780 Gbps (per GPU), 1278.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.780 Gbps (per GPU), 1278.240 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.780 Gbps (per GPU), 1278.240 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.712 Gbps (per GPU), 837.695 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.706 Gbps (per GPU), 837.646 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.714 Gbps (per GPU), 837.715 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.697 Gbps (per GPU), 837.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.714 Gbps (per GPU), 837.716 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.714 Gbps (per GPU), 837.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.714 Gbps (per GPU), 837.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.714 Gbps (per GPU), 837.716 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.221 Gbps (per GPU), 249.765 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.220 Gbps (per GPU), 249.762 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.221 Gbps (per GPU), 249.766 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.220 Gbps (per GPU), 249.759 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.220 Gbps (per GPU), 249.758 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.220 Gbps (per GPU), 249.760 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.219 Gbps (per GPU), 249.753 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.220 Gbps (per GPU), 249.759 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.05ms  9.72ms  8.91ms  1.35 29.12K 14.23M
 chk_1  7.62ms  5.42ms  4.57ms  1.67 29.12K  6.56M
 chk_2 18.87ms 16.79ms 15.93ms  1.18 29.12K 24.68M
 chk_3 19.04ms 16.87ms 16.05ms  1.19 29.12K 22.95M
 chk_4  7.45ms  5.23ms  4.40ms  1.69 29.12K  6.33M
 chk_5 11.43ms  9.19ms  8.34ms  1.37 29.12K 12.05M
 chk_6 12.64ms 10.22ms  9.50ms  1.33 29.12K 14.60M
 chk_7 11.87ms  9.63ms  8.59ms  1.38 29.12K 13.21M
   Avg 12.62 10.38  9.54
   Max 19.04 16.87 16.05
   Min  7.45  5.23  4.40
 Ratio  2.56  3.23  3.64
   Var 16.79 17.09 17.13
Profiling takes 2.974 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 350.166 ms
Partition 0 [0, 4) has cost: 350.166 ms
Partition 1 [4, 8) has cost: 332.270 ms
Partition 2 [8, 12) has cost: 332.270 ms
Partition 3 [12, 16) has cost: 332.270 ms
Partition 4 [16, 20) has cost: 332.270 ms
Partition 5 [20, 24) has cost: 332.270 ms
Partition 6 [24, 28) has cost: 332.270 ms
Partition 7 [28, 32) has cost: 325.488 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 303.653 ms
GPU 0, Compute+Comm Time: 126.283 ms, Bubble Time: 130.073 ms, Imbalance Overhead: 47.297 ms
GPU 1, Compute+Comm Time: 121.588 ms, Bubble Time: 121.034 ms, Imbalance Overhead: 61.031 ms
GPU 2, Compute+Comm Time: 121.588 ms, Bubble Time: 111.426 ms, Imbalance Overhead: 70.639 ms
GPU 3, Compute+Comm Time: 121.588 ms, Bubble Time: 112.135 ms, Imbalance Overhead: 69.931 ms
GPU 4, Compute+Comm Time: 121.588 ms, Bubble Time: 121.534 ms, Imbalance Overhead: 60.532 ms
GPU 5, Compute+Comm Time: 121.588 ms, Bubble Time: 130.366 ms, Imbalance Overhead: 51.699 ms
GPU 6, Compute+Comm Time: 121.588 ms, Bubble Time: 139.398 ms, Imbalance Overhead: 42.668 ms
GPU 7, Compute+Comm Time: 119.649 ms, Bubble Time: 149.682 ms, Imbalance Overhead: 34.322 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 595.923 ms
GPU 0, Compute+Comm Time: 232.384 ms, Bubble Time: 294.872 ms, Imbalance Overhead: 68.667 ms
GPU 1, Compute+Comm Time: 237.227 ms, Bubble Time: 275.185 ms, Imbalance Overhead: 83.511 ms
GPU 2, Compute+Comm Time: 237.227 ms, Bubble Time: 256.923 ms, Imbalance Overhead: 101.773 ms
GPU 3, Compute+Comm Time: 237.227 ms, Bubble Time: 239.185 ms, Imbalance Overhead: 119.511 ms
GPU 4, Compute+Comm Time: 237.227 ms, Bubble Time: 219.838 ms, Imbalance Overhead: 138.858 ms
GPU 5, Compute+Comm Time: 237.227 ms, Bubble Time: 217.963 ms, Imbalance Overhead: 140.733 ms
GPU 6, Compute+Comm Time: 237.227 ms, Bubble Time: 236.947 ms, Imbalance Overhead: 121.749 ms
GPU 7, Compute+Comm Time: 250.428 ms, Bubble Time: 254.169 ms, Imbalance Overhead: 91.326 ms
The estimated cost of the whole pipeline: 944.555 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 682.435 ms
Partition 0 [0, 8) has cost: 682.435 ms
Partition 1 [8, 16) has cost: 664.539 ms
Partition 2 [16, 24) has cost: 664.539 ms
Partition 3 [24, 32) has cost: 657.757 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 313.825 ms
GPU 0, Compute+Comm Time: 162.871 ms, Bubble Time: 150.152 ms, Imbalance Overhead: 0.801 ms
GPU 1, Compute+Comm Time: 160.656 ms, Bubble Time: 131.506 ms, Imbalance Overhead: 21.663 ms
GPU 2, Compute+Comm Time: 160.656 ms, Bubble Time: 112.291 ms, Imbalance Overhead: 40.878 ms
GPU 3, Compute+Comm Time: 159.634 ms, Bubble Time: 112.976 ms, Imbalance Overhead: 41.215 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 600.538 ms
GPU 0, Compute+Comm Time: 302.753 ms, Bubble Time: 215.122 ms, Imbalance Overhead: 82.663 ms
GPU 1, Compute+Comm Time: 305.073 ms, Bubble Time: 213.535 ms, Imbalance Overhead: 81.931 ms
GPU 2, Compute+Comm Time: 305.073 ms, Bubble Time: 251.503 ms, Imbalance Overhead: 43.962 ms
GPU 3, Compute+Comm Time: 311.680 ms, Bubble Time: 287.710 ms, Imbalance Overhead: 1.148 ms
    The estimated cost with 2 DP ways is 960.081 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1346.975 ms
Partition 0 [0, 16) has cost: 1346.975 ms
Partition 1 [16, 32) has cost: 1322.296 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 351.591 ms
GPU 0, Compute+Comm Time: 234.874 ms, Bubble Time: 116.716 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 233.304 ms, Bubble Time: 117.356 ms, Imbalance Overhead: 0.931 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 628.120 ms
GPU 0, Compute+Comm Time: 415.454 ms, Bubble Time: 209.318 ms, Imbalance Overhead: 3.348 ms
GPU 1, Compute+Comm Time: 419.814 ms, Bubble Time: 208.306 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1028.697 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2669.271 ms
Partition 0 [0, 32) has cost: 2669.271 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 512.550 ms
GPU 0, Compute+Comm Time: 512.550 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 697.117 ms
GPU 0, Compute+Comm Time: 697.117 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1270.151 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 287)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 287)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 287)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 287)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 287)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 287)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 287)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 287)...
+++++++++ Node 5 initializing the weights for op[0, 287)...
+++++++++ Node 1 initializing the weights for op[0, 287)...
+++++++++ Node 4 initializing the weights for op[0, 287)...
+++++++++ Node 2 initializing the weights for op[0, 287)...
+++++++++ Node 6 initializing the weights for op[0, 287)...
+++++++++ Node 7 initializing the weights for op[0, 287)...
+++++++++ Node 3 initializing the weights for op[0, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.7121
	Epoch 50:	Loss 3.7119
	Epoch 75:	Loss 3.6924
	Epoch 100:	Loss 3.3570
	Epoch 125:	Loss 3.1720
	Epoch 150:	Loss 3.0779
	Epoch 175:	Loss 3.0059
	Epoch 200:	Loss 2.9540
	Epoch 225:	Loss 2.9291
	Epoch 250:	Loss 2.9155
	Epoch 275:	Loss 2.9024
	Epoch 300:	Loss 2.8914
	Epoch 325:	Loss 2.8849
	Epoch 350:	Loss 2.8707
	Epoch 375:	Loss 2.8540
	Epoch 400:	Loss 2.8308
	Epoch 425:	Loss 2.8031
	Epoch 450:	Loss 2.7656
	Epoch 475:	Loss 2.7448
	Epoch 500:	Loss 2.6881
	Epoch 525:	Loss 2.6374
	Epoch 550:	Loss 2.6184
	Epoch 575:	Loss 2.5578
	Epoch 600:	Loss 2.6318
	Epoch 625:	Loss 2.4330
	Epoch 650:	Loss 2.5198
	Epoch 675:	Loss 2.2882
	Epoch 700:	Loss 2.2430
	Epoch 725:	Loss 2.2077
	Epoch 750:	Loss 2.1766
	Epoch 775:	Loss 2.1072
	Epoch 800:	Loss 2.0496
	Epoch 825:	Loss 2.0870
	Epoch 850:	Loss 1.9828
	Epoch 875:	Loss 1.9515
	Epoch 900:	Loss 2.0305
	Epoch 925:	Loss 1.8822
	Epoch 950:	Loss 1.8697
	Epoch 975:	Loss 1.8207
	Epoch 1000:	Loss 1.7723
	Epoch 1025:	Loss 1.7721
	Epoch 1050:	Loss 1.8945
	Epoch 1075:	Loss 1.6958
	Epoch 1100:	Loss 1.7047
	Epoch 1125:	Loss 1.6604
	Epoch 1150:	Loss 1.6863
	Epoch 1175:	Loss 1.7169
	Epoch 1200:	Loss 1.6588
	Epoch 1225:	Loss 1.7507
	Epoch 1250:	Loss 1.6682
	Epoch 1275:	Loss 1.6016
	Epoch 1300:	Loss 1.5865
	Epoch 1325:	Loss 1.5271
	Epoch 1350:	Loss 1.5343
	Epoch 1375:	Loss 1.5397
	Epoch 1400:	Loss 1.4926
	Epoch 1425:	Loss 1.5841
	Epoch 1450:	Loss 1.4255
	Epoch 1475:	Loss 1.4572
	Epoch 1500:	Loss 1.6200
	Epoch 1525:	Loss 1.3994
	Epoch 1550:	Loss 1.4071
	Epoch 1575:	Loss 1.4342
	Epoch 1600:	Loss 1.3274
	Epoch 1625:	Loss 1.3635
	Epoch 1650:	Loss 1.3548
	Epoch 1675:	Loss 1.4298
	Epoch 1700:	Loss 1.3353
	Epoch 1725:	Loss 1.2646
	Epoch 1750:	Loss 1.2858
	Epoch 1775:	Loss 1.2623
	Epoch 1800:	Loss 1.2520
	Epoch 1825:	Loss 1.2081
	Epoch 1850:	Loss 1.1837
	Epoch 1875:	Loss 1.2257
	Epoch 1900:	Loss 1.2353
	Epoch 1925:	Loss 1.2846
	Epoch 1950:	Loss 1.3834
	Epoch 1975:	Loss 1.1873
	Epoch 2000:	Loss 1.1787
	Epoch 2025:	Loss 1.1294
	Epoch 2050:	Loss 1.1882
	Epoch 2075:	Loss 1.1435
	Epoch 2100:	Loss 1.1105
	Epoch 2125:	Loss 1.1946
	Epoch 2150:	Loss 1.1683
	Epoch 2175:	Loss 1.1471
	Epoch 2200:	Loss 1.0899
	Epoch 2225:	Loss 1.2073
	Epoch 2250:	Loss 1.1418
	Epoch 2275:	Loss 1.0716
	Epoch 2300:	Loss 1.2771
	Epoch 2325:	Loss 1.0957
	Epoch 2350:	Loss 1.0457
	Epoch 2375:	Loss 1.0357
	Epoch 2400:	Loss 1.0400
	Epoch 2425:	Loss 1.0700
	Epoch 2450:	Loss 1.0336
	Epoch 2475:	Loss 1.0045
	Epoch 2500:	Loss 1.0525
	Epoch 2525:	Loss 1.0089
	Epoch 2550:	Loss 1.0238
	Epoch 2575:	Loss 1.0267
	Epoch 2600:	Loss 1.0887
	Epoch 2625:	Loss 0.9921
	Epoch 2650:	Loss 1.0658
	Epoch 2675:	Loss 1.0293
	Epoch 2700:	Loss 1.0011
	Epoch 2725:	Loss 1.0247
	Epoch 2750:	Loss 1.0027
	Epoch 2775:	Loss 1.0309
	Epoch 2800:	Loss 0.9974
	Epoch 2825:	Loss 0.9605
	Epoch 2850:	Loss 0.9452
	Epoch 2875:	Loss 0.9600
	Epoch 2900:	Loss 1.0136
	Epoch 2925:	Loss 0.9600
	Epoch 2950:	Loss 0.9426
	Epoch 2975:	Loss 0.9112
	Epoch 3000:	Loss 1.2579
	Epoch 3025:	Loss 1.0382
	Epoch 3050:	Loss 0.9973
	Epoch 3075:	Loss 0.9191
	Epoch 3100:	Loss 0.9316
	Epoch 3125:	Loss 1.0658
	Epoch 3150:	Loss 0.9048
	Epoch 3175:	Loss 0.8646
	Epoch 3200:	Loss 0.8793
	Epoch 3225:	Loss 0.9510
	Epoch 3250:	Loss 0.9306
	Epoch 3275:	Loss 0.8608
	Epoch 3300:	Loss 1.0317
	Epoch 3325:	Loss 1.0059
	Epoch 3350:	Loss 0.8958
	Epoch 3375:	Loss 0.8558
	Epoch 3400:	Loss 0.9154
	Epoch 3425:	Loss 0.8974
	Epoch 3450:	Loss 0.8699
	Epoch 3475:	Loss 0.8505
	Epoch 3500:	Loss 1.0718
	Epoch 3525:	Loss 0.9069
	Epoch 3550:	Loss 0.9043
	Epoch 3575:	Loss 0.8323
	Epoch 3600:	Loss 0.8659
	Epoch 3625:	Loss 0.8217
	Epoch 3650:	Loss 0.8297
	Epoch 3675:	Loss 0.8063
	Epoch 3700:	Loss 0.8176
	Epoch 3725:	Loss 0.8431
	Epoch 3750:	Loss 1.5535
	Epoch 3775:	Loss 0.8323
	Epoch 3800:	Loss 0.8026
	Epoch 3825:	Loss 0.7967
	Epoch 3850:	Loss 0.9297
	Epoch 3875:	Loss 0.8258
	Epoch 3900:	Loss 0.8841
	Epoch 3925:	Loss 0.8340
	Epoch 3950:	Loss 0.8458
	Epoch 3975:	Loss 0.7623
	Epoch 4000:	Loss 0.7835
	Epoch 4025:	Loss 0.7687
	Epoch 4050:	Loss 0.9566
	Epoch 4075:	Loss 0.7972
	Epoch 4100:	Loss 0.7594
	Epoch 4125:	Loss 0.7978
	Epoch 4150:	Loss 0.7402
	Epoch 4175:	Loss 0.7587
	Epoch 4200:	Loss 0.8409
	Epoch 4225:	Loss 0.7602
	Epoch 4250:	Loss 0.7449
	Epoch 4275:	Loss 0.8496
	Epoch 4300:	Loss 0.7744
	Epoch 4325:	Loss 0.7684
	Epoch 4350:	Loss 0.7074
	Epoch 4375:	Loss 0.7463
	Epoch 4400:	Loss 0.7124
	Epoch 4425:	Loss 0.8107
	Epoch 4450:	Loss 0.7577
	Epoch 4475:	Loss 0.7151
	Epoch 4500:	Loss 0.7798
	Epoch 4525:	Loss 0.7747
	Epoch 4550:	Loss 0.7318
	Epoch 4575:	Loss 0.7072
	Epoch 4600:	Loss 0.6930
	Epoch 4625:	Loss 0.6798
	Epoch 4650:	Loss 0.7047
	Epoch 4675:	Loss 0.7374
	Epoch 4700:	Loss 0.7247
	Epoch 4725:	Loss 0.6649
	Epoch 4750:	Loss 0.6783
	Epoch 4775:	Loss 1.0757
	Epoch 4800:	Loss 0.7351
	Epoch 4825:	Loss 0.6938
	Epoch 4850:	Loss 0.6809
	Epoch 4875:	Loss 0.7150
	Epoch 4900:	Loss 0.6893
	Epoch 4925:	Loss 0.6639
	Epoch 4950:	Loss 0.7165
	Epoch 4975:	Loss 0.6780
	Epoch 5000:	Loss 0.7363
Node 2, Pre/Post-Pipelining: 8.712 / 25.020 ms, Bubble: 0.369 ms, Compute: 1075.864 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 0, Pre/Post-Pipelining: 8.707 / 25.039 ms, Bubble: 1.141 ms, Compute: 1075.071 ms, Comm: 0.011 ms, Imbalance: 0.019 ms
Node 1, Pre/Post-Pipelining: 8.701 / 24.951 ms, Bubble: 1.095 ms, Compute: 1075.210 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 3, Pre/Post-Pipelining: 8.722 / 25.071 ms, Bubble: 0.047 ms, Compute: 1076.119 ms, Comm: 0.009 ms, Imbalance: 0.021 ms
Node 4, Pre/Post-Pipelining: 8.705 / 25.003 ms, Bubble: 1.270 ms, Compute: 1074.984 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 5, Pre/Post-Pipelining: 8.704 / 24.993 ms, Bubble: 1.208 ms, Compute: 1075.059 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 8.714 / 25.032 ms, Bubble: 0.729 ms, Compute: 1075.494 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 8.703 / 24.996 ms, Bubble: 1.172 ms, Compute: 1075.092 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.707 ms
Cluster-Wide Average, Post-Pipelining Overhead: 25.039 ms
Cluster-Wide Average, Bubble: 1.141 ms
Cluster-Wide Average, Compute: 1075.071 ms
Cluster-Wide Average, Communication: 0.011 ms
Cluster-Wide Average, Imbalance: 0.019 ms
Node 0, GPU memory consumption: 17.618 GB
Node 1, GPU memory consumption: 16.885 GB
Node 2, GPU memory consumption: 16.893 GB
Node 3, GPU memory consumption: 16.870 GB
Node 4, GPU memory consumption: 16.860 GB
Node 5, GPU memory consumption: 16.883 GB
Node 6, GPU memory consumption: 16.889 GB
Node 7, GPU memory consumption: 16.864 GB
Node 0, Graph-Level Communication Throughput: 22.448 Gbps, Time: 711.222 ms
Node 1, Graph-Level Communication Throughput: 19.947 Gbps, Time: 851.342 ms
Node 4, Graph-Level Communication Throughput: 9.641 Gbps, Time: 872.804 ms
Node 2, Graph-Level Communication Throughput: 35.950 Gbps, Time: 491.964 ms
Node 5, Graph-Level Communication Throughput: 16.685 Gbps, Time: 731.982 ms
Node 3, Graph-Level Communication Throughput: 49.495 Gbps, Time: 467.586 ms
Node 6, Graph-Level Communication Throughput: 23.383 Gbps, Time: 700.558 ms
Node 7, Graph-Level Communication Throughput: 18.837 Gbps, Time: 722.781 ms
------------------------node id 0,  per-epoch time: 1.110040 s---------------
------------------------node id 1,  per-epoch time: 1.110040 s---------------
------------------------node id 2,  per-epoch time: 1.110040 s---------------
------------------------node id 3,  per-epoch time: 1.110040 s---------------
------------------------node id 4,  per-epoch time: 1.110040 s---------------
------------------------node id 5,  per-epoch time: 1.110040 s---------------
------------------------node id 6,  per-epoch time: 1.110040 s---------------
------------------------node id 7,  per-epoch time: 1.110040 s---------------
************ Profiling Results ************
	Bubble: 34.962113 (ms) (3.15 percentage)
	Compute: 341.892473 (ms) (30.80 percentage)
	GraphCommComputeOverhead: 19.277094 (ms) (1.74 percentage)
	GraphCommNetwork: 693.778247 (ms) (62.50 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 20.141516 (ms) (1.81 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.022 GB
	Total communication (cluster-wide, per-epoch): 14.504 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
