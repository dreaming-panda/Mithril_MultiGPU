Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.865 seconds.
Building the CSC structure...
        It takes 1.880 seconds.
Building the CSC structure...
        It takes 2.143 seconds.
Building the CSC structure...
        It takes 2.381 seconds.
Building the CSC structure...
        It takes 2.417 seconds.
Building the CSC structure...
        It takes 2.447 seconds.
Building the CSC structure...
        It takes 2.605 seconds.
Building the CSC structure...
        It takes 2.716 seconds.
Building the CSC structure...
        It takes 1.827 seconds.
        It takes 1.834 seconds.
        It takes 2.151 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.346 seconds.
        It takes 2.361 seconds.
        It takes 2.343 seconds.
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 2.345 seconds.
        It takes 0.036 seconds.
        It takes 0.318 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 2.465 seconds.
Building the Feature Vector...
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.282 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.037 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.033 seconds.
        It takes 0.252 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.872 Gbps (per GPU), 446.980 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.608 Gbps (per GPU), 444.861 Gbps (aggregated)
The layer-level communication performance: 55.613 Gbps (per GPU), 444.908 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.405 Gbps (per GPU), 443.244 Gbps (aggregated)
The layer-level communication performance: 55.380 Gbps (per GPU), 443.037 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.198 Gbps (per GPU), 441.582 Gbps (aggregated)
The layer-level communication performance: 55.155 Gbps (per GPU), 441.242 Gbps (aggregated)
The layer-level communication performance: 55.126 Gbps (per GPU), 441.006 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.747 Gbps (per GPU), 1277.975 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.865 Gbps (per GPU), 1278.922 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.750 Gbps (per GPU), 1277.996 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.753 Gbps (per GPU), 1278.021 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.753 Gbps (per GPU), 1278.021 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.795 Gbps (per GPU), 1278.361 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.750 Gbps (per GPU), 1277.996 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.746 Gbps (per GPU), 1277.972 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.620 Gbps (per GPU), 836.963 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.620 Gbps (per GPU), 836.963 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.620 Gbps (per GPU), 836.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.620 Gbps (per GPU), 836.957 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.620 Gbps (per GPU), 836.957 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.588 Gbps (per GPU), 836.705 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.620 Gbps (per GPU), 836.957 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.620 Gbps (per GPU), 836.957 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.600 Gbps (per GPU), 276.796 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.599 Gbps (per GPU), 276.796 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.599 Gbps (per GPU), 276.792 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.598 Gbps (per GPU), 276.787 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.599 Gbps (per GPU), 276.795 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.599 Gbps (per GPU), 276.794 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.600 Gbps (per GPU), 276.799 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.600 Gbps (per GPU), 276.798 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.55ms 10.17ms  9.60ms  1.31 29.12K 14.23M
 chk_1  7.91ms  5.61ms  4.78ms  1.66 29.12K  6.56M
 chk_2 19.77ms 17.76ms 16.70ms  1.18 29.12K 24.68M
 chk_3 19.71ms 17.46ms 16.76ms  1.18 29.12K 22.95M
 chk_4  7.75ms  5.64ms  4.65ms  1.67 29.12K  6.33M
 chk_5 12.03ms  9.66ms  8.70ms  1.38 29.12K 12.05M
 chk_6 13.20ms 10.72ms  9.93ms  1.33 29.12K 14.60M
 chk_7 12.32ms 10.15ms  9.11ms  1.35 29.12K 13.21M
   Avg 13.16 10.90 10.03
   Max 19.77 17.76 16.76
   Min  7.75  5.61  4.65
 Ratio  2.55  3.16  3.60
   Var 18.21 18.56 18.65
Profiling takes 3.108 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 366.767 ms
Partition 0 [0, 4) has cost: 366.767 ms
Partition 1 [4, 8) has cost: 348.698 ms
Partition 2 [8, 12) has cost: 348.698 ms
Partition 3 [12, 16) has cost: 348.698 ms
Partition 4 [16, 20) has cost: 348.698 ms
Partition 5 [20, 24) has cost: 348.698 ms
Partition 6 [24, 28) has cost: 348.698 ms
Partition 7 [28, 32) has cost: 341.749 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 322.207 ms
GPU 0, Compute+Comm Time: 131.931 ms, Bubble Time: 136.801 ms, Imbalance Overhead: 53.475 ms
GPU 1, Compute+Comm Time: 127.251 ms, Bubble Time: 126.439 ms, Imbalance Overhead: 68.516 ms
GPU 2, Compute+Comm Time: 127.251 ms, Bubble Time: 115.946 ms, Imbalance Overhead: 79.009 ms
GPU 3, Compute+Comm Time: 127.251 ms, Bubble Time: 117.619 ms, Imbalance Overhead: 77.336 ms
GPU 4, Compute+Comm Time: 127.251 ms, Bubble Time: 127.775 ms, Imbalance Overhead: 67.181 ms
GPU 5, Compute+Comm Time: 127.251 ms, Bubble Time: 137.930 ms, Imbalance Overhead: 57.025 ms
GPU 6, Compute+Comm Time: 127.251 ms, Bubble Time: 148.293 ms, Imbalance Overhead: 46.663 ms
GPU 7, Compute+Comm Time: 125.379 ms, Bubble Time: 159.600 ms, Imbalance Overhead: 37.227 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 619.583 ms
GPU 0, Compute+Comm Time: 243.056 ms, Bubble Time: 306.365 ms, Imbalance Overhead: 70.162 ms
GPU 1, Compute+Comm Time: 248.132 ms, Bubble Time: 286.184 ms, Imbalance Overhead: 85.267 ms
GPU 2, Compute+Comm Time: 248.132 ms, Bubble Time: 267.578 ms, Imbalance Overhead: 103.873 ms
GPU 3, Compute+Comm Time: 248.132 ms, Bubble Time: 249.558 ms, Imbalance Overhead: 121.893 ms
GPU 4, Compute+Comm Time: 248.132 ms, Bubble Time: 229.920 ms, Imbalance Overhead: 141.532 ms
GPU 5, Compute+Comm Time: 248.132 ms, Bubble Time: 227.686 ms, Imbalance Overhead: 143.765 ms
GPU 6, Compute+Comm Time: 248.132 ms, Bubble Time: 247.012 ms, Imbalance Overhead: 124.440 ms
GPU 7, Compute+Comm Time: 261.521 ms, Bubble Time: 264.638 ms, Imbalance Overhead: 93.424 ms
The estimated cost of the whole pipeline: 988.879 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 715.465 ms
Partition 0 [0, 8) has cost: 715.465 ms
Partition 1 [8, 16) has cost: 697.396 ms
Partition 2 [16, 24) has cost: 697.396 ms
Partition 3 [24, 32) has cost: 690.448 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 331.535 ms
GPU 0, Compute+Comm Time: 169.879 ms, Bubble Time: 158.816 ms, Imbalance Overhead: 2.840 ms
GPU 1, Compute+Comm Time: 167.691 ms, Bubble Time: 137.444 ms, Imbalance Overhead: 26.400 ms
GPU 2, Compute+Comm Time: 167.691 ms, Bubble Time: 115.941 ms, Imbalance Overhead: 47.903 ms
GPU 3, Compute+Comm Time: 166.840 ms, Bubble Time: 118.775 ms, Imbalance Overhead: 45.921 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 623.326 ms
GPU 0, Compute+Comm Time: 315.652 ms, Bubble Time: 224.441 ms, Imbalance Overhead: 83.233 ms
GPU 1, Compute+Comm Time: 318.090 ms, Bubble Time: 222.170 ms, Imbalance Overhead: 83.066 ms
GPU 2, Compute+Comm Time: 318.090 ms, Bubble Time: 260.821 ms, Imbalance Overhead: 44.415 ms
GPU 3, Compute+Comm Time: 324.912 ms, Bubble Time: 297.810 ms, Imbalance Overhead: 0.604 ms
    The estimated cost with 2 DP ways is 1002.604 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1412.862 ms
Partition 0 [0, 16) has cost: 1412.862 ms
Partition 1 [16, 32) has cost: 1387.844 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 368.812 ms
GPU 0, Compute+Comm Time: 245.489 ms, Bubble Time: 119.783 ms, Imbalance Overhead: 3.540 ms
GPU 1, Compute+Comm Time: 243.873 ms, Bubble Time: 124.939 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 649.624 ms
GPU 0, Compute+Comm Time: 430.192 ms, Bubble Time: 217.380 ms, Imbalance Overhead: 2.052 ms
GPU 1, Compute+Comm Time: 434.590 ms, Bubble Time: 215.034 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1069.357 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2800.705 ms
Partition 0 [0, 32) has cost: 2800.705 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 494.615 ms
GPU 0, Compute+Comm Time: 494.615 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 678.094 ms
GPU 0, Compute+Comm Time: 678.094 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1231.345 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 287)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 287)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 287)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 287)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 287)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 287)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 287)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 287)...
+++++++++ Node 0 initializing the weights for op[0, 287)...
+++++++++ Node 5 initializing the weights for op[0, 287)...
+++++++++ Node 6 initializing the weights for op[0, 287)...
+++++++++ Node 2 initializing the weights for op[0, 287)...
+++++++++ Node 7 initializing the weights for op[0, 287)...
+++++++++ Node 3 initializing the weights for op[0, 287)...
+++++++++ Node 4 initializing the weights for op[0, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000



*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2398
Node 1, Pre/Post-Pipelining: 8.743 / 19.436 ms, Bubble: 0.949 ms, Compute: 1072.955 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
	Epoch 50:	Loss 3.0606
Node 2, Pre/Post-Pipelining: 8.756 / 19.540 ms, Bubble: 0.464 ms, Compute: 1073.327 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 0, Pre/Post-Pipelining: 8.755 / 19.573 ms, Bubble: 0.981 ms, Compute: 1072.762 ms, Comm: 0.011 ms, Imbalance: 0.021 ms
Node 5, Pre/Post-Pipelining: 8.742 / 19.438 ms, Bubble: 1.093 ms, Compute: 1072.821 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 3, Pre/Post-Pipelining: 8.764 / 19.639 ms, Bubble: 0.047 ms, Compute: 1073.619 ms, Comm: 0.011 ms, Imbalance: 0.027 ms
Node 7, Pre/Post-Pipelining: 8.746 / 19.482 ms, Bubble: 1.071 ms, Compute: 1072.774 ms, Comm: 0.009 ms, Imbalance: 0.018 ms
Node 4, Pre/Post-Pipelining: 8.747 / 19.486 ms, Bubble: 1.091 ms, Compute: 1072.762 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 8.756 / 19.545 ms, Bubble: 0.576 ms, Compute: 1073.216 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.755 ms
Cluster-Wide Average, Post-Pipelining Overhead: 19.573 ms
Cluster-Wide Average, Bubble: 0.981 ms
Cluster-Wide Average, Compute: 1072.762 ms
Cluster-Wide Average, Communication: 0.011 ms
Cluster-Wide Average, Imbalance: 0.021 ms
Node 0, GPU memory consumption: 17.618 GB
Node 1, GPU memory consumption: 16.885 GB
Node 2, GPU memory consumption: 16.893 GB
Node 6, GPU memory consumption: 16.889 GB
Node 3, GPU memory consumption: 16.870 GB
Node 4, GPU memory consumption: 16.860 GB
Node 5, GPU memory consumption: 16.883 GB
Node 7, GPU memory consumption: 16.864 GB
Node 0, Graph-Level Communication Throughput: 22.402 Gbps, Time: 712.653 ms
Node 1, Graph-Level Communication Throughput: 19.941 Gbps, Time: 851.612 ms
Node 4, Graph-Level Communication Throughput: 9.656 Gbps, Time: 871.505 ms
Node 2, Graph-Level Communication Throughput: 35.680 Gbps, Time: 495.688 ms
Node 5, Graph-Level Communication Throughput: 16.629 Gbps, Time: 734.420 ms
Node 3, Graph-Level Communication Throughput: 49.433 Gbps, Time: 468.172 ms
Node 6, Graph-Level Communication Throughput: 23.377 Gbps, Time: 700.743 ms
Node 7, Graph-Level Communication Throughput: 18.778 Gbps, Time: 725.033 ms
------------------------node id 0,  per-epoch time: 1.102158 s---------------
------------------------node id 1,  per-epoch time: 1.102162 s---------------
------------------------node id 2,  per-epoch time: 1.102161 s---------------
------------------------node id 3,  per-epoch time: 1.102162 s---------------
------------------------node id 4,  per-epoch time: 1.102162 s---------------
------------------------node id 5,  per-epoch time: 1.102163 s---------------
------------------------node id 6,  per-epoch time: 1.102163 s---------------
------------------------node id 7,  per-epoch time: 1.102162 s---------------
************ Profiling Results ************
	Bubble: 30.215162 (ms) (2.73 percentage)
	Compute: 340.559404 (ms) (30.82 percentage)
	GraphCommComputeOverhead: 19.170997 (ms) (1.74 percentage)
	GraphCommNetwork: 694.979166 (ms) (62.90 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.941654 (ms) (1.80 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.022 GB
	Total communication (cluster-wide, per-epoch): 14.504 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
