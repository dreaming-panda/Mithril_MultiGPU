Initialized node 6 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.864 seconds.
Building the CSC structure...
        It takes 1.877 seconds.
Building the CSC structure...
        It takes 2.311 seconds.
Building the CSC structure...
        It takes 2.387 seconds.
Building the CSC structure...
        It takes 2.419 seconds.
Building the CSC structure...
        It takes 2.449 seconds.
Building the CSC structure...
        It takes 2.643 seconds.
Building the CSC structure...
        It takes 2.694 seconds.
Building the CSC structure...
        It takes 1.809 seconds.
        It takes 1.838 seconds.
        It takes 2.197 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.361 seconds.
        It takes 2.430 seconds.
        It takes 2.386 seconds.
        It takes 2.294 seconds.
        It takes 0.302 seconds.
Building the Label Vector...
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.038 seconds.
        It takes 2.333 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.263 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Building the Feature Vector...
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.320 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
Building the Feature Vector...
        It takes 0.293 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.033 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.283 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.035 seconds.
        It takes 0.258 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 52.781 Gbps (per GPU), 422.251 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 52.544 Gbps (per GPU), 420.354 Gbps (aggregated)
The layer-level communication performance: 52.538 Gbps (per GPU), 420.305 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 52.333 Gbps (per GPU), 418.665 Gbps (aggregated)
The layer-level communication performance: 52.304 Gbps (per GPU), 418.436 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 52.150 Gbps (per GPU), 417.202 Gbps (aggregated)
The layer-level communication performance: 52.115 Gbps (per GPU), 416.923 Gbps (aggregated)
The layer-level communication performance: 52.085 Gbps (per GPU), 416.682 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 161.758 Gbps (per GPU), 1294.064 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.721 Gbps (per GPU), 1293.765 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.761 Gbps (per GPU), 1294.089 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.746 Gbps (per GPU), 1293.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.724 Gbps (per GPU), 1293.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.746 Gbps (per GPU), 1293.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.746 Gbps (per GPU), 1293.964 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.752 Gbps (per GPU), 1294.014 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.241 Gbps (per GPU), 833.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.242 Gbps (per GPU), 833.934 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.241 Gbps (per GPU), 833.927 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.240 Gbps (per GPU), 833.920 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.240 Gbps (per GPU), 833.920 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.239 Gbps (per GPU), 833.913 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.241 Gbps (per GPU), 833.927 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.242 Gbps (per GPU), 833.934 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.782 Gbps (per GPU), 262.255 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.782 Gbps (per GPU), 262.255 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.782 Gbps (per GPU), 262.257 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.781 Gbps (per GPU), 262.248 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.781 Gbps (per GPU), 262.248 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.781 Gbps (per GPU), 262.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.781 Gbps (per GPU), 262.250 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.780 Gbps (per GPU), 262.243 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.01ms  9.75ms  8.94ms  1.34 29.12K 14.23M
 chk_1  7.68ms  5.45ms  4.60ms  1.67 29.12K  6.56M
 chk_2 19.10ms 16.83ms 16.06ms  1.19 29.12K 24.68M
 chk_3 19.23ms 17.07ms 16.16ms  1.19 29.12K 22.95M
 chk_4  7.53ms  5.27ms  4.44ms  1.69 29.12K  6.33M
 chk_5 11.48ms  9.24ms  8.42ms  1.36 29.12K 12.05M
 chk_6 12.64ms 10.38ms  9.64ms  1.31 29.12K 14.60M
 chk_7 11.90ms  9.70ms  8.71ms  1.37 29.12K 13.21M
   Avg 12.70 10.46  9.62
   Max 19.23 17.07 16.16
   Min  7.53  5.27  4.44
 Ratio  2.55  3.24  3.64
   Var 17.26 17.36 17.37
Profiling takes 2.994 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 352.646 ms
Partition 0 [0, 4) has cost: 352.646 ms
Partition 1 [4, 8) has cost: 334.764 ms
Partition 2 [8, 12) has cost: 334.764 ms
Partition 3 [12, 16) has cost: 334.764 ms
Partition 4 [16, 20) has cost: 334.764 ms
Partition 5 [20, 24) has cost: 334.764 ms
Partition 6 [24, 28) has cost: 334.764 ms
Partition 7 [28, 32) has cost: 328.035 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 308.228 ms
GPU 0, Compute+Comm Time: 127.802 ms, Bubble Time: 132.502 ms, Imbalance Overhead: 47.924 ms
GPU 1, Compute+Comm Time: 123.149 ms, Bubble Time: 123.099 ms, Imbalance Overhead: 61.980 ms
GPU 2, Compute+Comm Time: 123.149 ms, Bubble Time: 113.151 ms, Imbalance Overhead: 71.928 ms
GPU 3, Compute+Comm Time: 123.149 ms, Bubble Time: 113.682 ms, Imbalance Overhead: 71.397 ms
GPU 4, Compute+Comm Time: 123.149 ms, Bubble Time: 123.126 ms, Imbalance Overhead: 61.953 ms
GPU 5, Compute+Comm Time: 123.149 ms, Bubble Time: 132.025 ms, Imbalance Overhead: 53.053 ms
GPU 6, Compute+Comm Time: 123.149 ms, Bubble Time: 141.125 ms, Imbalance Overhead: 43.954 ms
GPU 7, Compute+Comm Time: 121.236 ms, Bubble Time: 151.444 ms, Imbalance Overhead: 35.548 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 603.674 ms
GPU 0, Compute+Comm Time: 235.048 ms, Bubble Time: 298.408 ms, Imbalance Overhead: 70.218 ms
GPU 1, Compute+Comm Time: 239.863 ms, Bubble Time: 278.273 ms, Imbalance Overhead: 85.538 ms
GPU 2, Compute+Comm Time: 239.863 ms, Bubble Time: 259.881 ms, Imbalance Overhead: 103.930 ms
GPU 3, Compute+Comm Time: 239.863 ms, Bubble Time: 242.029 ms, Imbalance Overhead: 121.782 ms
GPU 4, Compute+Comm Time: 239.863 ms, Bubble Time: 222.560 ms, Imbalance Overhead: 141.251 ms
GPU 5, Compute+Comm Time: 239.863 ms, Bubble Time: 220.852 ms, Imbalance Overhead: 142.958 ms
GPU 6, Compute+Comm Time: 239.863 ms, Bubble Time: 240.156 ms, Imbalance Overhead: 123.654 ms
GPU 7, Compute+Comm Time: 253.092 ms, Bubble Time: 257.747 ms, Imbalance Overhead: 92.835 ms
The estimated cost of the whole pipeline: 957.497 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 687.410 ms
Partition 0 [0, 8) has cost: 687.410 ms
Partition 1 [8, 16) has cost: 669.528 ms
Partition 2 [16, 24) has cost: 669.528 ms
Partition 3 [24, 32) has cost: 662.800 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 317.072 ms
GPU 0, Compute+Comm Time: 163.906 ms, Bubble Time: 152.348 ms, Imbalance Overhead: 0.818 ms
GPU 1, Compute+Comm Time: 161.611 ms, Bubble Time: 132.998 ms, Imbalance Overhead: 22.463 ms
GPU 2, Compute+Comm Time: 161.611 ms, Bubble Time: 113.101 ms, Imbalance Overhead: 42.360 ms
GPU 3, Compute+Comm Time: 160.674 ms, Bubble Time: 113.309 ms, Imbalance Overhead: 43.090 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 607.292 ms
GPU 0, Compute+Comm Time: 305.832 ms, Bubble Time: 216.962 ms, Imbalance Overhead: 84.498 ms
GPU 1, Compute+Comm Time: 308.205 ms, Bubble Time: 215.870 ms, Imbalance Overhead: 83.217 ms
GPU 2, Compute+Comm Time: 308.205 ms, Bubble Time: 254.478 ms, Imbalance Overhead: 44.609 ms
GPU 3, Compute+Comm Time: 314.838 ms, Bubble Time: 291.373 ms, Imbalance Overhead: 1.082 ms
    The estimated cost with 2 DP ways is 970.583 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1356.938 ms
Partition 0 [0, 16) has cost: 1356.938 ms
Partition 1 [16, 32) has cost: 1332.328 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 355.341 ms
GPU 0, Compute+Comm Time: 237.016 ms, Bubble Time: 118.325 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 235.400 ms, Bubble Time: 117.885 ms, Imbalance Overhead: 2.056 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 634.760 ms
GPU 0, Compute+Comm Time: 419.384 ms, Bubble Time: 210.737 ms, Imbalance Overhead: 4.640 ms
GPU 1, Compute+Comm Time: 423.882 ms, Bubble Time: 210.878 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1039.606 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2689.266 ms
Partition 0 [0, 32) has cost: 2689.266 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 498.798 ms
GPU 0, Compute+Comm Time: 498.798 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 685.365 ms
GPU 0, Compute+Comm Time: 685.365 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1243.371 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 287)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 287)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 287)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 287)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 287)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 287)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 287)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 287)...
+++++++++ Node 7 initializing the weights for op[0, 287)...
+++++++++ Node 5 initializing the weights for op[0, 287)...
+++++++++ Node 6 initializing the weights for op[0, 287)...
+++++++++ Node 1 initializing the weights for op[0, 287)...
+++++++++ Node 0 initializing the weights for op[0, 287)...
+++++++++ Node 2 initializing the weights for op[0, 287)...
+++++++++ Node 3 initializing the weights for op[0, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.7121	TrainAcc 0.0887	ValidAcc 0.0774	TestAcc 0.0762	BestValid 0.0774
	Epoch 50:	Loss 3.7119	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0774
Node 2, Pre/Post-Pipelining: 8.752 / 18.370 ms, Bubble: 0.380 ms, Compute: 1064.779 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 3, Pre/Post-Pipelining: 8.756 / 18.413 ms, Bubble: 0.046 ms, Compute: 1065.068 ms, Comm: 0.009 ms, Imbalance: 0.018 ms
Node 0, Pre/Post-Pipelining: 8.744 / 18.321 ms, Bubble: 0.772 ms, Compute: 1064.437 ms, Comm: 0.009 ms, Imbalance: 0.019 ms
Node 1, Pre/Post-Pipelining: 8.743 / 18.313 ms, Bubble: 0.638 ms, Compute: 1064.582 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 4, Pre/Post-Pipelining: 8.749 / 18.352 ms, Bubble: 0.700 ms, Compute: 1064.482 ms, Comm: 0.010 ms, Imbalance: 0.016 ms
Node 5, Pre/Post-Pipelining: 8.742 / 18.321 ms, Bubble: 0.744 ms, Compute: 1064.477 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 6, Pre/Post-Pipelining: 8.755 / 18.397 ms, Bubble: 0.322 ms, Compute: 1064.808 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 8.741 / 18.279 ms, Bubble: 0.920 ms, Compute: 1064.339 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.744 ms
Cluster-Wide Average, Post-Pipelining Overhead: 18.321 ms
Cluster-Wide Average, Bubble: 0.772 ms
Cluster-Wide Average, Compute: 1064.437 ms
Cluster-Wide Average, Communication: 0.009 ms
Cluster-Wide Average, Imbalance: 0.019 ms
Node 2, GPU memory consumption: 16.893 GB
Node 3, GPU memory consumption: 16.870 GB
Node 0, GPU memory consumption: 17.618 GB
Node 4, GPU memory consumption: 16.860 GB
Node 6, GPU memory consumption: 16.889 GB
Node 1, GPU memory consumption: 16.885 GB
Node 5, GPU memory consumption: 16.883 GB
Node 7, GPU memory consumption: 16.864 GB
Node 0, Graph-Level Communication Throughput: 22.426 Gbps, Time: 711.903 ms
Node 1, Graph-Level Communication Throughput: 20.085 Gbps, Time: 845.482 ms
Node 4, Graph-Level Communication Throughput: 9.738 Gbps, Time: 864.131 ms
Node 2, Graph-Level Communication Throughput: 35.471 Gbps, Time: 498.606 ms
Node 5, Graph-Level Communication Throughput: 16.708 Gbps, Time: 730.986 ms
Node 3, Graph-Level Communication Throughput: 49.399 Gbps, Time: 468.488 ms
Node 6, Graph-Level Communication Throughput: 23.499 Gbps, Time: 697.080 ms
Node 7, Graph-Level Communication Throughput: 18.759 Gbps, Time: 725.788 ms
------------------------node id 0,  per-epoch time: 1.677348 s---------------
------------------------node id 1,  per-epoch time: 1.677346 s---------------
------------------------node id 4,  per-epoch time: 1.677347 s---------------
------------------------node id 2,  per-epoch time: 1.677344 s---------------
------------------------node id 5,  per-epoch time: 1.677349 s---------------
------------------------node id 3,  per-epoch time: 1.677343 s---------------
------------------------node id 6,  per-epoch time: 1.677348 s---------------
------------------------node id 7,  per-epoch time: 1.677351 s---------------
************ Profiling Results ************
	Bubble: 555.547036 (ms) (34.24 percentage)
	Compute: 335.247501 (ms) (20.66 percentage)
	GraphCommComputeOverhead: 19.006717 (ms) (1.17 percentage)
	GraphCommNetwork: 692.808146 (ms) (42.70 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.784884 (ms) (1.22 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.022 GB
	Total communication (cluster-wide, per-epoch): 14.504 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0774
Target test_acc: 0.0762
Epoch to reach the target acc: 24
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
