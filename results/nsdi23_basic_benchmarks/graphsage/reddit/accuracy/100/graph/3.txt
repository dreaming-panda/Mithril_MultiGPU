Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.894 seconds.
Building the CSC structure...
        It takes 1.901 seconds.
Building the CSC structure...
        It takes 2.368 seconds.
Building the CSC structure...
        It takes 2.433 seconds.
Building the CSC structure...
        It takes 2.599 seconds.
Building the CSC structure...
        It takes 2.605 seconds.
Building the CSC structure...
        It takes 2.660 seconds.
Building the CSC structure...
        It takes 2.680 seconds.
Building the CSC structure...
        It takes 1.832 seconds.
        It takes 1.835 seconds.
Building the Feature Vector...
        It takes 2.345 seconds.
        It takes 2.326 seconds.
Building the Feature Vector...
        It takes 2.278 seconds.
        It takes 2.363 seconds.
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 2.303 seconds.
        It takes 0.040 seconds.
        It takes 2.355 seconds.
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.279 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.042 seconds.
Building the Feature Vector...
        It takes 0.314 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.263 seconds.
Building the Label Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
        It takes 0.032 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
Building the Feature Vector...
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.271 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.032 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 55.692 Gbps (per GPU), 445.533 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.444 Gbps (per GPU), 443.551 Gbps (aggregated)
The layer-level communication performance: 55.442 Gbps (per GPU), 443.533 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.217 Gbps (per GPU), 441.737 Gbps (aggregated)
The layer-level communication performance: 55.185 Gbps (per GPU), 441.483 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.016 Gbps (per GPU), 440.124 Gbps (aggregated)
The layer-level communication performance: 54.973 Gbps (per GPU), 439.782 Gbps (aggregated)
The layer-level communication performance: 54.943 Gbps (per GPU), 439.541 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.116 Gbps (per GPU), 1272.930 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.119 Gbps (per GPU), 1272.953 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.119 Gbps (per GPU), 1272.954 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.107 Gbps (per GPU), 1272.858 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.122 Gbps (per GPU), 1272.978 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.104 Gbps (per GPU), 1272.833 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.104 Gbps (per GPU), 1272.833 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.104 Gbps (per GPU), 1272.833 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.566 Gbps (per GPU), 828.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.566 Gbps (per GPU), 828.526 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.570 Gbps (per GPU), 828.559 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.567 Gbps (per GPU), 828.538 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.566 Gbps (per GPU), 828.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.566 Gbps (per GPU), 828.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.566 Gbps (per GPU), 828.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.566 Gbps (per GPU), 828.525 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.367 Gbps (per GPU), 266.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.368 Gbps (per GPU), 266.944 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.368 Gbps (per GPU), 266.943 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.367 Gbps (per GPU), 266.936 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.367 Gbps (per GPU), 266.933 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.367 Gbps (per GPU), 266.933 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.368 Gbps (per GPU), 266.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.367 Gbps (per GPU), 266.933 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 12.33ms 10.05ms  9.16ms  1.35 29.12K 14.23M
 chk_1  7.85ms  5.58ms  4.71ms  1.67 29.12K  6.56M
 chk_2 19.42ms 17.28ms 16.65ms  1.17 29.12K 24.68M
 chk_3 19.49ms 17.34ms 16.54ms  1.18 29.12K 22.95M
 chk_4  7.65ms  5.39ms  4.55ms  1.68 29.12K  6.33M
 chk_5 11.81ms  9.47ms  8.58ms  1.38 29.12K 12.05M
 chk_6 13.03ms 10.62ms  9.77ms  1.33 29.12K 14.60M
 chk_7 12.09ms  9.90ms  9.00ms  1.34 29.12K 13.21M
   Avg 12.96 10.70  9.87
   Max 19.49 17.34 16.65
   Min  7.65  5.39  4.55
 Ratio  2.55  3.22  3.66
   Var 17.65 18.04 18.55
Profiling takes 3.065 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 360.546 ms
Partition 0 [0, 4) has cost: 360.546 ms
Partition 1 [4, 8) has cost: 342.497 ms
Partition 2 [8, 12) has cost: 342.497 ms
Partition 3 [12, 16) has cost: 342.497 ms
Partition 4 [16, 20) has cost: 342.497 ms
Partition 5 [20, 24) has cost: 342.497 ms
Partition 6 [24, 28) has cost: 342.497 ms
Partition 7 [28, 32) has cost: 335.837 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 313.704 ms
GPU 0, Compute+Comm Time: 129.967 ms, Bubble Time: 134.665 ms, Imbalance Overhead: 49.072 ms
GPU 1, Compute+Comm Time: 125.326 ms, Bubble Time: 125.235 ms, Imbalance Overhead: 63.142 ms
GPU 2, Compute+Comm Time: 125.326 ms, Bubble Time: 115.273 ms, Imbalance Overhead: 73.105 ms
GPU 3, Compute+Comm Time: 125.326 ms, Bubble Time: 115.861 ms, Imbalance Overhead: 72.517 ms
GPU 4, Compute+Comm Time: 125.326 ms, Bubble Time: 125.468 ms, Imbalance Overhead: 62.910 ms
GPU 5, Compute+Comm Time: 125.326 ms, Bubble Time: 134.558 ms, Imbalance Overhead: 53.820 ms
GPU 6, Compute+Comm Time: 125.326 ms, Bubble Time: 143.884 ms, Imbalance Overhead: 44.493 ms
GPU 7, Compute+Comm Time: 123.358 ms, Bubble Time: 154.340 ms, Imbalance Overhead: 36.006 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 611.844 ms
GPU 0, Compute+Comm Time: 239.250 ms, Bubble Time: 302.562 ms, Imbalance Overhead: 70.031 ms
GPU 1, Compute+Comm Time: 243.943 ms, Bubble Time: 282.383 ms, Imbalance Overhead: 85.518 ms
GPU 2, Compute+Comm Time: 243.943 ms, Bubble Time: 263.989 ms, Imbalance Overhead: 103.912 ms
GPU 3, Compute+Comm Time: 243.943 ms, Bubble Time: 246.211 ms, Imbalance Overhead: 121.690 ms
GPU 4, Compute+Comm Time: 243.943 ms, Bubble Time: 226.794 ms, Imbalance Overhead: 141.108 ms
GPU 5, Compute+Comm Time: 243.943 ms, Bubble Time: 224.681 ms, Imbalance Overhead: 143.220 ms
GPU 6, Compute+Comm Time: 243.943 ms, Bubble Time: 243.856 ms, Imbalance Overhead: 124.046 ms
GPU 7, Compute+Comm Time: 257.351 ms, Bubble Time: 261.290 ms, Imbalance Overhead: 93.203 ms
The estimated cost of the whole pipeline: 971.825 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 703.043 ms
Partition 0 [0, 8) has cost: 703.043 ms
Partition 1 [8, 16) has cost: 684.994 ms
Partition 2 [16, 24) has cost: 684.994 ms
Partition 3 [24, 32) has cost: 678.333 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 323.206 ms
GPU 0, Compute+Comm Time: 167.071 ms, Bubble Time: 154.954 ms, Imbalance Overhead: 1.180 ms
GPU 1, Compute+Comm Time: 164.834 ms, Bubble Time: 135.562 ms, Imbalance Overhead: 22.809 ms
GPU 2, Compute+Comm Time: 164.834 ms, Bubble Time: 115.637 ms, Imbalance Overhead: 42.734 ms
GPU 3, Compute+Comm Time: 163.884 ms, Bubble Time: 115.992 ms, Imbalance Overhead: 43.330 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 616.025 ms
GPU 0, Compute+Comm Time: 311.534 ms, Bubble Time: 221.605 ms, Imbalance Overhead: 82.887 ms
GPU 1, Compute+Comm Time: 313.782 ms, Bubble Time: 219.497 ms, Imbalance Overhead: 82.746 ms
GPU 2, Compute+Comm Time: 313.782 ms, Bubble Time: 257.846 ms, Imbalance Overhead: 44.397 ms
GPU 3, Compute+Comm Time: 320.460 ms, Bubble Time: 294.454 ms, Imbalance Overhead: 1.111 ms
    The estimated cost with 2 DP ways is 986.192 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1388.037 ms
Partition 0 [0, 16) has cost: 1388.037 ms
Partition 1 [16, 32) has cost: 1363.327 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 361.044 ms
GPU 0, Compute+Comm Time: 240.922 ms, Bubble Time: 120.122 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 239.409 ms, Bubble Time: 120.011 ms, Imbalance Overhead: 1.624 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 643.375 ms
GPU 0, Compute+Comm Time: 426.132 ms, Bubble Time: 215.135 ms, Imbalance Overhead: 2.108 ms
GPU 1, Compute+Comm Time: 430.337 ms, Bubble Time: 213.038 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 1054.640 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2751.364 ms
Partition 0 [0, 32) has cost: 2751.364 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 497.287 ms
GPU 0, Compute+Comm Time: 497.287 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 684.531 ms
GPU 0, Compute+Comm Time: 684.531 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1240.909 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 287)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 287)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 287)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 287)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 287)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 287)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 287)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 7 initializing the weights for op[0, 287)...
+++++++++ Node 1 initializing the weights for op[0, 287)...
+++++++++ Node 5 initializing the weights for op[0, 287)...
+++++++++ Node 2 initializing the weights for op[0, 287)...
+++++++++ Node 6 initializing the weights for op[0, 287)...
+++++++++ Node 4 initializing the weights for op[0, 287)...
+++++++++ Node 0 initializing the weights for op[0, 287)...
+++++++++ Node 3 initializing the weights for op[0, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2398	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 50:	Loss 3.0606	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
Node 2, Pre/Post-Pipelining: 8.752 / 18.759 ms, Bubble: 0.548 ms, Compute: 1071.837 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 0, Pre/Post-Pipelining: 8.748 / 18.854 ms, Bubble: 0.841 ms, Compute: 1071.427 ms, Comm: 0.011 ms, Imbalance: 0.022 ms
Node 1, Pre/Post-Pipelining: 8.746 / 18.757 ms, Bubble: 0.851 ms, Compute: 1071.532 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 3, Pre/Post-Pipelining: 8.758 / 18.829 ms, Bubble: 0.105 ms, Compute: 1072.199 ms, Comm: 0.009 ms, Imbalance: 0.018 ms
Node 4, Pre/Post-Pipelining: 8.751 / 18.775 ms, Bubble: 0.911 ms, Compute: 1071.446 ms, Comm: 0.010 ms, Imbalance: 0.019 ms
Node 5, Pre/Post-Pipelining: 8.742 / 18.683 ms, Bubble: 1.049 ms, Compute: 1071.422 ms, Comm: 0.008 ms, Imbalance: 0.015 ms
Node 6, Pre/Post-Pipelining: 8.753 / 18.740 ms, Bubble: 0.576 ms, Compute: 1071.823 ms, Comm: 0.009 ms, Imbalance: 0.017 ms
Node 7, Pre/Post-Pipelining: 8.741 / 18.677 ms, Bubble: 1.103 ms, Compute: 1071.371 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.748 ms
Cluster-Wide Average, Post-Pipelining Overhead: 18.854 ms
Cluster-Wide Average, Bubble: 0.841 ms
Cluster-Wide Average, Compute: 1071.427 ms
Cluster-Wide Average, Communication: 0.011 ms
Cluster-Wide Average, Imbalance: 0.022 ms
Node 2, GPU memory consumption: 16.893 GB
Node 0, GPU memory consumption: 17.618 GB
Node 3, GPU memory consumption: 16.870 GB
Node 1, GPU memory consumption: 16.885 GB
Node 6, GPU memory consumption: 16.889 GB
Node 4, GPU memory consumption: 16.860 GB
Node 7, GPU memory consumption: 16.864 GB
Node 5, GPU memory consumption: 16.883 GB
Node 0, Graph-Level Communication Throughput: 22.377 Gbps, Time: 713.456 ms
Node 1, Graph-Level Communication Throughput: 19.967 Gbps, Time: 850.504 ms
Node 2, Graph-Level Communication Throughput: 35.466 Gbps, Time: 498.682 ms
Node 3, Graph-Level Communication Throughput: 49.235 Gbps, Time: 470.047 ms
Node 4, Graph-Level Communication Throughput: 9.680 Gbps, Time: 869.317 ms
Node 5, Graph-Level Communication Throughput: 16.629 Gbps, Time: 734.431 ms
Node 6, Graph-Level Communication Throughput: 23.359 Gbps, Time: 701.260 ms
Node 7, Graph-Level Communication Throughput: 18.720 Gbps, Time: 727.297 ms
------------------------node id 0,  per-epoch time: 1.700646 s---------------
------------------------node id 1,  per-epoch time: 1.700645 s---------------
------------------------node id 4,  per-epoch time: 1.700647 s---------------
------------------------node id 2,  per-epoch time: 1.700645 s---------------
------------------------node id 5,  per-epoch time: 1.700648 s---------------
------------------------node id 3,  per-epoch time: 1.700645 s---------------
------------------------node id 6,  per-epoch time: 1.700647 s---------------
------------------------node id 7,  per-epoch time: 1.700648 s---------------
************ Profiling Results ************
	Bubble: 569.752994 (ms) (34.68 percentage)
	Compute: 338.572955 (ms) (20.61 percentage)
	GraphCommComputeOverhead: 19.107587 (ms) (1.16 percentage)
	GraphCommNetwork: 695.624674 (ms) (42.34 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.867049 (ms) (1.21 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.022 GB
	Total communication (cluster-wide, per-epoch): 14.504 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0584
Target test_acc: 0.0574
Epoch to reach the target acc: 24
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
