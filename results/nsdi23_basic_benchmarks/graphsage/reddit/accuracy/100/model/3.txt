Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.856 seconds.
Building the CSC structure...
        It takes 1.922 seconds.
Building the CSC structure...
        It takes 2.059 seconds.
Building the CSC structure...
        It takes 2.085 seconds.
Building the CSC structure...
        It takes 2.345 seconds.
Building the CSC structure...
        It takes 2.442 seconds.
Building the CSC structure...
        It takes 2.645 seconds.
Building the CSC structure...
        It takes 2.657 seconds.
Building the CSC structure...
        It takes 1.813 seconds.
        It takes 1.842 seconds.
        It takes 1.879 seconds.
        It takes 1.901 seconds.
        It takes 2.290 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.407 seconds.
        It takes 0.311 seconds.
Building the Label Vector...
        It takes 0.046 seconds.
        It takes 2.366 seconds.
        It takes 0.304 seconds.
Building the Label Vector...
        It takes 2.420 seconds.
        It takes 0.035 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Building the Feature Vector...
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.031 seconds.
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
train nodes 153431, valid nodes 23831, test nodes 55703
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.234 Gbps (per GPU), 441.869 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.995 Gbps (per GPU), 439.963 Gbps (aggregated)
The layer-level communication performance: 54.989 Gbps (per GPU), 439.911 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.791 Gbps (per GPU), 438.327 Gbps (aggregated)
The layer-level communication performance: 54.772 Gbps (per GPU), 438.175 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.589 Gbps (per GPU), 436.714 Gbps (aggregated)
The layer-level communication performance: 54.543 Gbps (per GPU), 436.343 Gbps (aggregated)
The layer-level communication performance: 54.523 Gbps (per GPU), 436.182 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.489 Gbps (per GPU), 1283.913 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.492 Gbps (per GPU), 1283.938 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.474 Gbps (per GPU), 1283.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.397 Gbps (per GPU), 1283.177 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.474 Gbps (per GPU), 1283.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.400 Gbps (per GPU), 1283.201 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.477 Gbps (per GPU), 1283.815 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.474 Gbps (per GPU), 1283.790 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.864 Gbps (per GPU), 830.912 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.866 Gbps (per GPU), 830.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.867 Gbps (per GPU), 830.932 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.866 Gbps (per GPU), 830.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.867 Gbps (per GPU), 830.932 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.866 Gbps (per GPU), 830.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.863 Gbps (per GPU), 830.905 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.863 Gbps (per GPU), 830.905 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.382 Gbps (per GPU), 267.054 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.382 Gbps (per GPU), 267.056 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.383 Gbps (per GPU), 267.060 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.376 Gbps (per GPU), 267.012 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.383 Gbps (per GPU), 267.065 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.376 Gbps (per GPU), 267.008 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.383 Gbps (per GPU), 267.065 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.383 Gbps (per GPU), 267.062 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.31ms  2.66ms  2.41ms  1.37  8.38K  3.53M
 chk_1  3.55ms  2.93ms  2.74ms  1.29  6.74K  3.60M
 chk_2  3.47ms  2.80ms  2.63ms  1.32  7.27K  3.53M
 chk_3  3.53ms  2.85ms  2.65ms  1.34  7.92K  3.61M
 chk_4  3.23ms  2.76ms  2.62ms  1.23  5.33K  3.68M
 chk_5  3.61ms  2.81ms  2.49ms  1.45 10.07K  3.45M
 chk_6  3.74ms  2.95ms  2.65ms  1.41  9.41K  3.48M
 chk_7  3.45ms  2.78ms  2.55ms  1.35  8.12K  3.60M
 chk_8  3.42ms  2.84ms  2.70ms  1.27  6.09K  3.64M
 chk_9  3.66ms  2.73ms  2.38ms  1.54 11.10K  3.38M
chk_10  3.38ms  2.89ms  2.73ms  1.24  5.67K  3.63M
chk_11  3.47ms  2.81ms  2.56ms  1.36  8.16K  3.54M
chk_12  3.64ms  2.98ms  2.77ms  1.31  7.24K  3.55M
chk_13  3.28ms  2.82ms  2.65ms  1.24  5.41K  3.68M
chk_14  3.70ms  3.05ms  2.84ms  1.30  7.14K  3.53M
chk_15  3.71ms  2.96ms  2.66ms  1.39  9.25K  3.49M
chk_16  3.18ms  2.75ms  2.61ms  1.22  4.78K  3.77M
chk_17  3.46ms  2.87ms  2.68ms  1.29  6.85K  3.60M
chk_18  3.35ms  2.68ms  2.48ms  1.35  7.47K  3.57M
chk_19  3.18ms  2.73ms  2.59ms  1.23  4.88K  3.75M
chk_20  3.40ms  2.74ms  2.56ms  1.33  7.00K  3.63M
chk_21  3.24ms  2.72ms  2.55ms  1.27  5.41K  3.68M
chk_22  3.92ms  2.97ms  2.62ms  1.49 11.07K  3.39M
chk_23  3.51ms  2.83ms  2.66ms  1.32  7.23K  3.64M
chk_24  3.77ms  2.94ms  2.81ms  1.34 10.13K  3.43M
chk_25  3.26ms  2.71ms  2.52ms  1.29  6.40K  3.57M
chk_26  3.41ms  2.89ms  2.75ms  1.24  5.78K  3.55M
chk_27  3.58ms  2.88ms  2.54ms  1.41  9.34K  3.48M
chk_28  3.66ms  3.10ms  2.87ms  1.28  6.37K  3.57M
chk_29  3.40ms  2.90ms  2.72ms  1.25  5.16K  3.78M
chk_30  3.27ms  2.79ms  2.68ms  1.22  5.44K  3.67M
chk_31  3.52ms  2.93ms  2.95ms  1.20  6.33K  3.63M
   Avg  3.48  2.85  2.64
   Max  3.92  3.10  2.95
   Min  3.18  2.66  2.38
 Ratio  1.23  1.16  1.24
   Var  0.03  0.01  0.02
Profiling takes 3.348 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 384.497 ms
Partition 0 [0, 4) has cost: 384.497 ms
Partition 1 [4, 8) has cost: 364.304 ms
Partition 2 [8, 12) has cost: 364.304 ms
Partition 3 [12, 16) has cost: 364.304 ms
Partition 4 [16, 20) has cost: 364.304 ms
Partition 5 [20, 24) has cost: 364.304 ms
Partition 6 [24, 28) has cost: 364.304 ms
Partition 7 [28, 32) has cost: 357.832 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.285 ms
GPU 0, Compute+Comm Time: 139.148 ms, Bubble Time: 30.377 ms, Imbalance Overhead: 3.760 ms
GPU 1, Compute+Comm Time: 133.218 ms, Bubble Time: 30.099 ms, Imbalance Overhead: 9.968 ms
GPU 2, Compute+Comm Time: 133.218 ms, Bubble Time: 30.063 ms, Imbalance Overhead: 10.004 ms
GPU 3, Compute+Comm Time: 133.218 ms, Bubble Time: 29.903 ms, Imbalance Overhead: 10.164 ms
GPU 4, Compute+Comm Time: 133.218 ms, Bubble Time: 29.898 ms, Imbalance Overhead: 10.169 ms
GPU 5, Compute+Comm Time: 133.218 ms, Bubble Time: 29.907 ms, Imbalance Overhead: 10.160 ms
GPU 6, Compute+Comm Time: 133.218 ms, Bubble Time: 30.153 ms, Imbalance Overhead: 9.914 ms
GPU 7, Compute+Comm Time: 131.129 ms, Bubble Time: 30.619 ms, Imbalance Overhead: 11.537 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 337.149 ms
GPU 0, Compute+Comm Time: 253.696 ms, Bubble Time: 59.912 ms, Imbalance Overhead: 23.541 ms
GPU 1, Compute+Comm Time: 258.080 ms, Bubble Time: 59.166 ms, Imbalance Overhead: 19.903 ms
GPU 2, Compute+Comm Time: 258.080 ms, Bubble Time: 58.673 ms, Imbalance Overhead: 20.395 ms
GPU 3, Compute+Comm Time: 258.080 ms, Bubble Time: 58.632 ms, Imbalance Overhead: 20.437 ms
GPU 4, Compute+Comm Time: 258.080 ms, Bubble Time: 58.524 ms, Imbalance Overhead: 20.545 ms
GPU 5, Compute+Comm Time: 258.080 ms, Bubble Time: 58.773 ms, Imbalance Overhead: 20.296 ms
GPU 6, Compute+Comm Time: 258.080 ms, Bubble Time: 58.741 ms, Imbalance Overhead: 20.328 ms
GPU 7, Compute+Comm Time: 272.343 ms, Bubble Time: 59.326 ms, Imbalance Overhead: 5.480 ms
The estimated cost of the whole pipeline: 535.955 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 748.801 ms
Partition 0 [0, 8) has cost: 748.801 ms
Partition 1 [8, 16) has cost: 728.608 ms
Partition 2 [16, 24) has cost: 728.608 ms
Partition 3 [24, 32) has cost: 722.135 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 179.249 ms
GPU 0, Compute+Comm Time: 148.442 ms, Bubble Time: 28.099 ms, Imbalance Overhead: 2.708 ms
GPU 1, Compute+Comm Time: 145.241 ms, Bubble Time: 27.554 ms, Imbalance Overhead: 6.453 ms
GPU 2, Compute+Comm Time: 145.241 ms, Bubble Time: 27.377 ms, Imbalance Overhead: 6.630 ms
GPU 3, Compute+Comm Time: 144.305 ms, Bubble Time: 27.065 ms, Imbalance Overhead: 7.879 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 336.541 ms
GPU 0, Compute+Comm Time: 270.150 ms, Bubble Time: 51.241 ms, Imbalance Overhead: 15.149 ms
GPU 1, Compute+Comm Time: 272.222 ms, Bubble Time: 51.717 ms, Imbalance Overhead: 12.602 ms
GPU 2, Compute+Comm Time: 272.222 ms, Bubble Time: 51.721 ms, Imbalance Overhead: 12.598 ms
GPU 3, Compute+Comm Time: 280.053 ms, Bubble Time: 52.816 ms, Imbalance Overhead: 3.672 ms
    The estimated cost with 2 DP ways is 541.580 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1477.409 ms
Partition 0 [0, 16) has cost: 1477.409 ms
Partition 1 [16, 32) has cost: 1450.743 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 224.242 ms
GPU 0, Compute+Comm Time: 195.441 ms, Bubble Time: 24.062 ms, Imbalance Overhead: 4.738 ms
GPU 1, Compute+Comm Time: 193.470 ms, Bubble Time: 24.819 ms, Imbalance Overhead: 5.952 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 373.008 ms
GPU 0, Compute+Comm Time: 322.752 ms, Bubble Time: 41.478 ms, Imbalance Overhead: 8.778 ms
GPU 1, Compute+Comm Time: 327.808 ms, Bubble Time: 40.489 ms, Imbalance Overhead: 4.711 ms
    The estimated cost with 4 DP ways is 627.112 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2928.152 ms
Partition 0 [0, 32) has cost: 2928.152 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 574.345 ms
GPU 0, Compute+Comm Time: 574.345 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 707.727 ms
GPU 0, Compute+Comm Time: 707.727 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1346.176 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2465	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 50:	Loss 3.0755	TrainAcc 0.0691	ValidAcc 0.0585	TestAcc 0.0575	BestValid 0.0585
Node 2, Pre/Post-Pipelining: 1.104 / 1.191 ms, Bubble: 99.328 ms, Compute: 372.361 ms, Comm: 30.021 ms, Imbalance: 36.482 ms
Node 3, Pre/Post-Pipelining: 1.104 / 1.158 ms, Bubble: 99.484 ms, Compute: 381.421 ms, Comm: 26.952 ms, Imbalance: 30.123 ms
Node 1, Pre/Post-Pipelining: 1.108 / 1.110 ms, Bubble: 98.337 ms, Compute: 382.234 ms, Comm: 28.181 ms, Imbalance: 29.335 ms
Node 4, Pre/Post-Pipelining: 1.106 / 1.086 ms, Bubble: 101.004 ms, Compute: 367.584 ms, Comm: 26.535 ms, Imbalance: 43.642 ms
Node 0, Pre/Post-Pipelining: 1.110 / 1.173 ms, Bubble: 97.117 ms, Compute: 415.248 ms, Comm: 16.756 ms, Imbalance: 8.266 ms
Node 6, Pre/Post-Pipelining: 1.104 / 1.153 ms, Bubble: 102.021 ms, Compute: 377.720 ms, Comm: 29.327 ms, Imbalance: 29.086 ms
Node 7, Pre/Post-Pipelining: 1.103 / 16.189 ms, Bubble: 88.004 ms, Compute: 388.222 ms, Comm: 17.464 ms, Imbalance: 29.297 ms
Node 5, Pre/Post-Pipelining: 1.105 / 1.104 ms, Bubble: 101.608 ms, Compute: 372.387 ms, Comm: 31.027 ms, Imbalance: 33.356 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.110 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.173 ms
Cluster-Wide Average, Bubble: 97.117 ms
Cluster-Wide Average, Compute: 415.248 ms
Cluster-Wide Average, Communication: 16.756 ms
Cluster-Wide Average, Imbalance: 8.266 ms
Node 0, GPU memory consumption: 6.614 GB
Node 3, GPU memory consumption: 5.237 GB
Node 1, GPU memory consumption: 5.260 GB
Node 2, GPU memory consumption: 5.260 GB
Node 4, GPU memory consumption: 5.237 GB
Node 6, GPU memory consumption: 5.260 GB
Node 5, GPU memory consumption: 5.260 GB
Node 7, GPU memory consumption: 5.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 1.126334 s---------------
------------------------node id 1,  per-epoch time: 1.126332 s---------------
------------------------node id 4,  per-epoch time: 1.126329 s---------------
------------------------node id 2,  per-epoch time: 1.126329 s---------------
------------------------node id 5,  per-epoch time: 1.126329 s---------------
------------------------node id 3,  per-epoch time: 1.126329 s---------------
------------------------node id 6,  per-epoch time: 1.126327 s---------------
------------------------node id 7,  per-epoch time: 1.126328 s---------------
************ Profiling Results ************
	Bubble: 693.554063 (ms) (64.28 percentage)
	Compute: 380.932983 (ms) (35.30 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.524074 (ms) (0.42 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 404.799 Gbps
Highest valid_acc: 0.0585
Target test_acc: 0.0575
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
