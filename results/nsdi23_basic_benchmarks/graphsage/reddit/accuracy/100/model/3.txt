Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.004 seconds.
Building the CSC structure...
        It takes 2.072 seconds.
Building the CSC structure...
        It takes 2.163 seconds.
Building the CSC structure...
        It takes 2.392 seconds.
Building the CSC structure...
        It takes 2.438 seconds.
Building the CSC structure...
        It takes 2.614 seconds.
Building the CSC structure...
        It takes 2.613 seconds.
Building the CSC structure...
        It takes 2.656 seconds.
Building the CSC structure...
        It takes 1.893 seconds.
        It takes 2.273 seconds.
        It takes 2.250 seconds.
        It takes 2.329 seconds.
        It takes 2.409 seconds.
        It takes 2.353 seconds.
        It takes 2.391 seconds.
        It takes 2.385 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.300 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.046 seconds.
Building the Feature Vector...
        It takes 0.302 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.301 seconds.
Building the Label Vector...
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
        It takes 0.036 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.049 seconds.
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.146 Gbps (per GPU), 449.164 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.896 Gbps (per GPU), 447.167 Gbps (aggregated)
The layer-level communication performance: 55.893 Gbps (per GPU), 447.146 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.656 Gbps (per GPU), 445.249 Gbps (aggregated)
The layer-level communication performance: 55.626 Gbps (per GPU), 445.006 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.463 Gbps (per GPU), 443.706 Gbps (aggregated)
The layer-level communication performance: 55.422 Gbps (per GPU), 443.377 Gbps (aggregated)
The layer-level communication performance: 55.389 Gbps (per GPU), 443.114 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 139.656 Gbps (per GPU), 1117.248 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 139.657 Gbps (per GPU), 1117.255 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 139.585 Gbps (per GPU), 1116.679 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 139.589 Gbps (per GPU), 1116.713 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 139.643 Gbps (per GPU), 1117.142 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 139.647 Gbps (per GPU), 1117.178 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 139.645 Gbps (per GPU), 1117.161 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 139.647 Gbps (per GPU), 1117.178 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.059 Gbps (per GPU), 832.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.061 Gbps (per GPU), 832.485 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.061 Gbps (per GPU), 832.485 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.061 Gbps (per GPU), 832.485 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.059 Gbps (per GPU), 832.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.059 Gbps (per GPU), 832.472 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.059 Gbps (per GPU), 832.471 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.058 Gbps (per GPU), 832.466 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.635 Gbps (per GPU), 269.079 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.632 Gbps (per GPU), 269.053 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.633 Gbps (per GPU), 269.067 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.635 Gbps (per GPU), 269.077 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.632 Gbps (per GPU), 269.059 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.634 Gbps (per GPU), 269.070 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.635 Gbps (per GPU), 269.076 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.635 Gbps (per GPU), 269.077 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.28ms  2.67ms  2.40ms  1.37  8.38K  3.53M
 chk_1  3.56ms  2.92ms  2.74ms  1.30  6.74K  3.60M
 chk_2  3.47ms  2.80ms  2.62ms  1.33  7.27K  3.53M
 chk_3  3.54ms  2.86ms  2.63ms  1.34  7.92K  3.61M
 chk_4  3.22ms  2.77ms  2.63ms  1.23  5.33K  3.68M
 chk_5  3.59ms  2.81ms  2.48ms  1.45 10.07K  3.45M
 chk_6  3.74ms  2.96ms  2.65ms  1.41  9.41K  3.48M
 chk_7  3.44ms  2.79ms  2.57ms  1.34  8.12K  3.60M
 chk_8  3.41ms  2.87ms  2.71ms  1.26  6.09K  3.64M
 chk_9  3.66ms  2.74ms  2.39ms  1.53 11.10K  3.38M
chk_10  3.38ms  2.90ms  2.74ms  1.24  5.67K  3.63M
chk_11  3.43ms  2.80ms  2.55ms  1.34  8.16K  3.54M
chk_12  3.63ms  2.96ms  2.79ms  1.30  7.24K  3.55M
chk_13  3.27ms  2.79ms  2.66ms  1.23  5.41K  3.68M
chk_14  3.69ms  3.04ms  2.84ms  1.30  7.14K  3.53M
chk_15  3.71ms  2.95ms  2.67ms  1.39  9.25K  3.49M
chk_16  3.18ms  2.75ms  2.62ms  1.21  4.78K  3.77M
chk_17  3.45ms  2.87ms  2.67ms  1.29  6.85K  3.60M
chk_18  3.33ms  2.67ms  2.47ms  1.35  7.47K  3.57M
chk_19  3.16ms  2.73ms  2.58ms  1.23  4.88K  3.75M
chk_20  3.37ms  2.73ms  2.55ms  1.32  7.00K  3.63M
chk_21  3.24ms  2.71ms  2.56ms  1.27  5.41K  3.68M
chk_22  4.00ms  2.98ms  2.63ms  1.52 11.07K  3.39M
chk_23  3.54ms  2.83ms  2.65ms  1.34  7.23K  3.64M
chk_24  3.76ms  2.94ms  2.63ms  1.43 10.13K  3.43M
chk_25  3.25ms  2.70ms  2.55ms  1.28  6.40K  3.57M
chk_26  3.45ms  2.91ms  2.74ms  1.26  5.78K  3.55M
chk_27  3.59ms  2.88ms  2.53ms  1.42  9.34K  3.48M
chk_28  3.66ms  3.08ms  2.86ms  1.28  6.37K  3.57M
chk_29  3.38ms  2.89ms  2.72ms  1.24  5.16K  3.78M
chk_30  3.25ms  2.78ms  2.68ms  1.21  5.44K  3.67M
chk_31  3.51ms  2.93ms  2.77ms  1.26  6.33K  3.63M
   Avg  3.47  2.84  2.63
   Max  4.00  3.08  2.86
   Min  3.16  2.67  2.39
 Ratio  1.27  1.16  1.19
   Var  0.04  0.01  0.01
Profiling takes 3.333 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 384.081 ms
Partition 0 [0, 4) has cost: 384.081 ms
Partition 1 [4, 8) has cost: 363.932 ms
Partition 2 [8, 12) has cost: 363.932 ms
Partition 3 [12, 16) has cost: 363.932 ms
Partition 4 [16, 20) has cost: 363.932 ms
Partition 5 [20, 24) has cost: 363.932 ms
Partition 6 [24, 28) has cost: 363.932 ms
Partition 7 [28, 32) has cost: 357.219 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 172.467 ms
GPU 0, Compute+Comm Time: 138.674 ms, Bubble Time: 30.175 ms, Imbalance Overhead: 3.619 ms
GPU 1, Compute+Comm Time: 132.747 ms, Bubble Time: 29.889 ms, Imbalance Overhead: 9.832 ms
GPU 2, Compute+Comm Time: 132.747 ms, Bubble Time: 29.875 ms, Imbalance Overhead: 9.846 ms
GPU 3, Compute+Comm Time: 132.747 ms, Bubble Time: 29.739 ms, Imbalance Overhead: 9.982 ms
GPU 4, Compute+Comm Time: 132.747 ms, Bubble Time: 29.766 ms, Imbalance Overhead: 9.955 ms
GPU 5, Compute+Comm Time: 132.747 ms, Bubble Time: 29.765 ms, Imbalance Overhead: 9.956 ms
GPU 6, Compute+Comm Time: 132.747 ms, Bubble Time: 30.004 ms, Imbalance Overhead: 9.717 ms
GPU 7, Compute+Comm Time: 130.511 ms, Bubble Time: 30.472 ms, Imbalance Overhead: 11.485 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 336.144 ms
GPU 0, Compute+Comm Time: 253.263 ms, Bubble Time: 59.913 ms, Imbalance Overhead: 22.968 ms
GPU 1, Compute+Comm Time: 257.741 ms, Bubble Time: 58.964 ms, Imbalance Overhead: 19.440 ms
GPU 2, Compute+Comm Time: 257.741 ms, Bubble Time: 58.446 ms, Imbalance Overhead: 19.957 ms
GPU 3, Compute+Comm Time: 257.741 ms, Bubble Time: 58.417 ms, Imbalance Overhead: 19.986 ms
GPU 4, Compute+Comm Time: 257.741 ms, Bubble Time: 58.260 ms, Imbalance Overhead: 20.144 ms
GPU 5, Compute+Comm Time: 257.741 ms, Bubble Time: 58.462 ms, Imbalance Overhead: 19.941 ms
GPU 6, Compute+Comm Time: 257.741 ms, Bubble Time: 58.408 ms, Imbalance Overhead: 19.995 ms
GPU 7, Compute+Comm Time: 271.963 ms, Bubble Time: 58.904 ms, Imbalance Overhead: 5.277 ms
The estimated cost of the whole pipeline: 534.042 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 748.013 ms
Partition 0 [0, 8) has cost: 748.013 ms
Partition 1 [8, 16) has cost: 727.864 ms
Partition 2 [16, 24) has cost: 727.864 ms
Partition 3 [24, 32) has cost: 721.151 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 180.678 ms
GPU 0, Compute+Comm Time: 149.687 ms, Bubble Time: 28.338 ms, Imbalance Overhead: 2.653 ms
GPU 1, Compute+Comm Time: 146.452 ms, Bubble Time: 27.823 ms, Imbalance Overhead: 6.403 ms
GPU 2, Compute+Comm Time: 146.452 ms, Bubble Time: 27.605 ms, Imbalance Overhead: 6.621 ms
GPU 3, Compute+Comm Time: 145.369 ms, Bubble Time: 27.263 ms, Imbalance Overhead: 8.046 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 337.523 ms
GPU 0, Compute+Comm Time: 271.218 ms, Bubble Time: 51.277 ms, Imbalance Overhead: 15.028 ms
GPU 1, Compute+Comm Time: 273.424 ms, Bubble Time: 51.615 ms, Imbalance Overhead: 12.483 ms
GPU 2, Compute+Comm Time: 273.424 ms, Bubble Time: 51.638 ms, Imbalance Overhead: 12.460 ms
GPU 3, Compute+Comm Time: 281.296 ms, Bubble Time: 52.739 ms, Imbalance Overhead: 3.488 ms
    The estimated cost with 2 DP ways is 544.111 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1475.878 ms
Partition 0 [0, 16) has cost: 1475.878 ms
Partition 1 [16, 32) has cost: 1449.015 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 223.450 ms
GPU 0, Compute+Comm Time: 194.714 ms, Bubble Time: 23.989 ms, Imbalance Overhead: 4.747 ms
GPU 1, Compute+Comm Time: 192.551 ms, Bubble Time: 24.713 ms, Imbalance Overhead: 6.186 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 371.600 ms
GPU 0, Compute+Comm Time: 321.777 ms, Bubble Time: 41.438 ms, Imbalance Overhead: 8.385 ms
GPU 1, Compute+Comm Time: 327.047 ms, Bubble Time: 40.163 ms, Imbalance Overhead: 4.390 ms
    The estimated cost with 4 DP ways is 624.803 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2924.893 ms
Partition 0 [0, 32) has cost: 2924.893 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 570.319 ms
GPU 0, Compute+Comm Time: 570.319 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 703.270 ms
GPU 0, Compute+Comm Time: 703.270 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1337.269 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2465	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 50:	Loss 3.0755	TrainAcc 0.0691	ValidAcc 0.0585	TestAcc 0.0575	BestValid 0.0585
	Epoch 75:	Loss 2.8707	TrainAcc 0.2278	ValidAcc 0.2626	TestAcc 0.2598	BestValid 0.2626
	Epoch 100:	Loss 2.7572	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2626
	Epoch 125:	Loss 2.6231	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2626
	Epoch 150:	Loss 2.4490	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2626
	Epoch 175:	Loss 2.2958	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2626
	Epoch 200:	Loss 2.1859	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2626
	Epoch 225:	Loss 2.0408	TrainAcc 0.0861	ValidAcc 0.0744	TestAcc 0.0736	BestValid 0.2626
	Epoch 250:	Loss 1.8568	TrainAcc 0.1083	ValidAcc 0.0913	TestAcc 0.0923	BestValid 0.2626
	Epoch 275:	Loss 1.6835	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2626
	Epoch 300:	Loss 1.5502	TrainAcc 0.0437	ValidAcc 0.0352	TestAcc 0.0358	BestValid 0.2626
	Epoch 325:	Loss 1.4076	TrainAcc 0.0888	ValidAcc 0.0742	TestAcc 0.0728	BestValid 0.2626
	Epoch 350:	Loss 1.3301	TrainAcc 0.0282	ValidAcc 0.0230	TestAcc 0.0222	BestValid 0.2626
	Epoch 375:	Loss 1.2663	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 400:	Loss 1.2027	TrainAcc 0.0345	ValidAcc 0.0281	TestAcc 0.0275	BestValid 0.2626
	Epoch 425:	Loss 1.1239	TrainAcc 0.0260	ValidAcc 0.0350	TestAcc 0.0328	BestValid 0.2626
	Epoch 450:	Loss 1.0665	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 475:	Loss 1.0128	TrainAcc 0.0386	ValidAcc 0.0339	TestAcc 0.0319	BestValid 0.2626
	Epoch 500:	Loss 0.9709	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0174	BestValid 0.2626
	Epoch 525:	Loss 0.9424	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0174	BestValid 0.2626
	Epoch 550:	Loss 0.9135	TrainAcc 0.0282	ValidAcc 0.0218	TestAcc 0.0204	BestValid 0.2626
	Epoch 575:	Loss 0.8804	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0174	BestValid 0.2626
	Epoch 600:	Loss 0.8491	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 625:	Loss 0.8143	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 650:	Loss 0.7759	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 675:	Loss 0.7594	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 700:	Loss 0.7334	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 725:	Loss 0.7072	TrainAcc 0.0213	ValidAcc 0.0182	TestAcc 0.0171	BestValid 0.2626
	Epoch 750:	Loss 0.6902	TrainAcc 0.0214	ValidAcc 0.0181	TestAcc 0.0172	BestValid 0.2626
	Epoch 775:	Loss 0.6647	TrainAcc 0.0223	ValidAcc 0.0180	TestAcc 0.0173	BestValid 0.2626
	Epoch 800:	Loss 0.6580	TrainAcc 0.0223	ValidAcc 0.0182	TestAcc 0.0173	BestValid 0.2626
	Epoch 825:	Loss 0.6440	TrainAcc 0.0222	ValidAcc 0.0177	TestAcc 0.0169	BestValid 0.2626
	Epoch 850:	Loss 0.6313	TrainAcc 0.0229	ValidAcc 0.0202	TestAcc 0.0189	BestValid 0.2626
	Epoch 875:	Loss 0.6200	TrainAcc 0.0223	ValidAcc 0.0181	TestAcc 0.0173	BestValid 0.2626
	Epoch 900:	Loss 0.6113	TrainAcc 0.0214	ValidAcc 0.0181	TestAcc 0.0172	BestValid 0.2626
	Epoch 925:	Loss 0.6129	TrainAcc 0.0220	ValidAcc 0.0174	TestAcc 0.0168	BestValid 0.2626
	Epoch 950:	Loss 0.5960	TrainAcc 0.0245	ValidAcc 0.0188	TestAcc 0.0178	BestValid 0.2626
	Epoch 975:	Loss 0.5819	TrainAcc 0.0211	ValidAcc 0.0171	TestAcc 0.0166	BestValid 0.2626
	Epoch 1000:	Loss 0.5852	TrainAcc 0.0218	ValidAcc 0.0179	TestAcc 0.0171	BestValid 0.2626
	Epoch 1025:	Loss 0.5747	TrainAcc 0.0206	ValidAcc 0.0162	TestAcc 0.0157	BestValid 0.2626
	Epoch 1050:	Loss 0.5743	TrainAcc 0.0209	ValidAcc 0.0170	TestAcc 0.0163	BestValid 0.2626
	Epoch 1075:	Loss 0.5644	TrainAcc 0.0211	ValidAcc 0.0174	TestAcc 0.0166	BestValid 0.2626
	Epoch 1100:	Loss 0.5511	TrainAcc 0.0201	ValidAcc 0.0157	TestAcc 0.0151	BestValid 0.2626
	Epoch 1125:	Loss 0.5524	TrainAcc 0.0212	ValidAcc 0.0177	TestAcc 0.0168	BestValid 0.2626
	Epoch 1150:	Loss 0.5442	TrainAcc 0.0212	ValidAcc 0.0176	TestAcc 0.0168	BestValid 0.2626
	Epoch 1175:	Loss 0.5382	TrainAcc 0.0213	ValidAcc 0.0182	TestAcc 0.0171	BestValid 0.2626
	Epoch 1200:	Loss 0.5322	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 1225:	Loss 0.5271	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 1250:	Loss 0.5250	TrainAcc 0.0213	ValidAcc 0.0180	TestAcc 0.0171	BestValid 0.2626
	Epoch 1275:	Loss 0.5220	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 1300:	Loss 0.5181	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 1325:	Loss 0.5182	TrainAcc 0.0212	ValidAcc 0.0176	TestAcc 0.0167	BestValid 0.2626
	Epoch 1350:	Loss 0.5084	TrainAcc 0.0212	ValidAcc 0.0178	TestAcc 0.0169	BestValid 0.2626
	Epoch 1375:	Loss 0.5042	TrainAcc 0.0211	ValidAcc 0.0176	TestAcc 0.0168	BestValid 0.2626
	Epoch 1400:	Loss 0.4984	TrainAcc 0.0213	ValidAcc 0.0179	TestAcc 0.0170	BestValid 0.2626
	Epoch 1425:	Loss 0.5037	TrainAcc 0.0212	ValidAcc 0.0175	TestAcc 0.0168	BestValid 0.2626
	Epoch 1450:	Loss 0.5009	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 1475:	Loss 0.4949	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 1500:	Loss 0.4899	TrainAcc 0.0198	ValidAcc 0.0150	TestAcc 0.0146	BestValid 0.2626
	Epoch 1525:	Loss 0.4940	TrainAcc 0.0208	ValidAcc 0.0165	TestAcc 0.0162	BestValid 0.2626
	Epoch 1550:	Loss 0.4963	TrainAcc 0.0213	ValidAcc 0.0178	TestAcc 0.0169	BestValid 0.2626
	Epoch 1575:	Loss 0.4867	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.2626
	Epoch 1600:	Loss 0.4857	TrainAcc 0.0220	ValidAcc 0.0175	TestAcc 0.0169	BestValid 0.2626
	Epoch 1625:	Loss 0.4823	TrainAcc 0.0211	ValidAcc 0.0173	TestAcc 0.0167	BestValid 0.2626
	Epoch 1650:	Loss 0.4746	TrainAcc 0.0202	ValidAcc 0.0157	TestAcc 0.0152	BestValid 0.2626
	Epoch 1675:	Loss 0.4780	TrainAcc 0.0208	ValidAcc 0.0167	TestAcc 0.0162	BestValid 0.2626
	Epoch 1700:	Loss 0.4728	TrainAcc 0.0122	ValidAcc 0.0096	TestAcc 0.0097	BestValid 0.2626
	Epoch 1725:	Loss 0.4703	TrainAcc 0.0204	ValidAcc 0.0163	TestAcc 0.0160	BestValid 0.2626
	Epoch 1750:	Loss 0.4704	TrainAcc 0.0021	ValidAcc 0.0012	TestAcc 0.0012	BestValid 0.2626
	Epoch 1775:	Loss 0.4647	TrainAcc 0.0149	ValidAcc 0.0125	TestAcc 0.0125	BestValid 0.2626
	Epoch 1800:	Loss 0.4701	TrainAcc 0.0004	ValidAcc 0.0004	TestAcc 0.0004	BestValid 0.2626
	Epoch 1825:	Loss 0.4652	TrainAcc 0.0018	ValidAcc 0.0009	TestAcc 0.0010	BestValid 0.2626
	Epoch 1850:	Loss 0.4642	TrainAcc 0.0031	ValidAcc 0.0023	TestAcc 0.0021	BestValid 0.2626
	Epoch 1875:	Loss 0.4592	TrainAcc 0.0701	ValidAcc 0.0589	TestAcc 0.0581	BestValid 0.2626
	Epoch 1900:	Loss 0.4574	TrainAcc 0.0017	ValidAcc 0.0011	TestAcc 0.0010	BestValid 0.2626
	Epoch 1925:	Loss 0.4578	TrainAcc 0.0703	ValidAcc 0.0590	TestAcc 0.0582	BestValid 0.2626
	Epoch 1950:	Loss 0.4621	TrainAcc 0.0014	ValidAcc 0.0008	TestAcc 0.0008	BestValid 0.2626
	Epoch 1975:	Loss 0.4541	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0576	BestValid 0.2626
	Epoch 2000:	Loss 0.4516	TrainAcc 0.0035	ValidAcc 0.0028	TestAcc 0.0033	BestValid 0.2626
	Epoch 2025:	Loss 0.4524	TrainAcc 0.0065	ValidAcc 0.0027	TestAcc 0.0025	BestValid 0.2626
	Epoch 2050:	Loss 0.4437	TrainAcc 0.0038	ValidAcc 0.0021	TestAcc 0.0021	BestValid 0.2626
	Epoch 2075:	Loss 0.4458	TrainAcc 0.0005	ValidAcc 0.0002	TestAcc 0.0002	BestValid 0.2626
	Epoch 2100:	Loss 0.4434	TrainAcc 0.0692	ValidAcc 0.0585	TestAcc 0.0576	BestValid 0.2626
	Epoch 2125:	Loss 0.4468	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2626
	Epoch 2150:	Loss 0.4421	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2626
	Epoch 2175:	Loss 0.4383	TrainAcc 0.0704	ValidAcc 0.0590	TestAcc 0.0582	BestValid 0.2626
	Epoch 2200:	Loss 0.4349	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2626
	Epoch 2225:	Loss 0.4401	TrainAcc 0.0357	ValidAcc 0.0314	TestAcc 0.0298	BestValid 0.2626
	Epoch 2250:	Loss 0.4348	TrainAcc 0.0703	ValidAcc 0.0591	TestAcc 0.0581	BestValid 0.2626
	Epoch 2275:	Loss 0.4298	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2626
	Epoch 2300:	Loss 0.4263	TrainAcc 0.0547	ValidAcc 0.0522	TestAcc 0.0513	BestValid 0.2626
	Epoch 2325:	Loss 0.4310	TrainAcc 0.0712	ValidAcc 0.0593	TestAcc 0.0586	BestValid 0.2626
	Epoch 2350:	Loss 0.4290	TrainAcc 0.0702	ValidAcc 0.0588	TestAcc 0.0579	BestValid 0.2626
	Epoch 2375:	Loss 0.4243	TrainAcc 0.0884	ValidAcc 0.0739	TestAcc 0.0723	BestValid 0.2626
	Epoch 2400:	Loss 0.4218	TrainAcc 0.0225	ValidAcc 0.0190	TestAcc 0.0180	BestValid 0.2626
	Epoch 2425:	Loss 0.4317	TrainAcc 0.1014	ValidAcc 0.0958	TestAcc 0.0921	BestValid 0.2626
	Epoch 2450:	Loss 0.4231	TrainAcc 0.0030	ValidAcc 0.0021	TestAcc 0.0021	BestValid 0.2626
	Epoch 2475:	Loss 0.4187	TrainAcc 0.0918	ValidAcc 0.0843	TestAcc 0.0810	BestValid 0.2626
	Epoch 2500:	Loss 0.4188	TrainAcc 0.0896	ValidAcc 0.0857	TestAcc 0.0821	BestValid 0.2626
	Epoch 2525:	Loss 0.4198	TrainAcc 0.0791	ValidAcc 0.0721	TestAcc 0.0701	BestValid 0.2626
	Epoch 2550:	Loss 0.4139	TrainAcc 0.1066	ValidAcc 0.1032	TestAcc 0.0992	BestValid 0.2626
	Epoch 2575:	Loss 0.4162	TrainAcc 0.1006	ValidAcc 0.0922	TestAcc 0.0900	BestValid 0.2626
	Epoch 2600:	Loss 0.4095	TrainAcc 0.0946	ValidAcc 0.0903	TestAcc 0.0870	BestValid 0.2626
	Epoch 2625:	Loss 0.4107	TrainAcc 0.0890	ValidAcc 0.0853	TestAcc 0.0819	BestValid 0.2626
	Epoch 2650:	Loss 0.4132	TrainAcc 0.0944	ValidAcc 0.0895	TestAcc 0.0858	BestValid 0.2626
	Epoch 2675:	Loss 0.4114	TrainAcc 0.0706	ValidAcc 0.0595	TestAcc 0.0585	BestValid 0.2626
	Epoch 2700:	Loss 0.4114	TrainAcc 0.0693	ValidAcc 0.0584	TestAcc 0.0576	BestValid 0.2626
	Epoch 2725:	Loss 0.4149	TrainAcc 0.0690	ValidAcc 0.0583	TestAcc 0.0575	BestValid 0.2626
	Epoch 2750:	Loss 0.4041	TrainAcc 0.0694	ValidAcc 0.0586	TestAcc 0.0577	BestValid 0.2626
	Epoch 2775:	Loss 0.4044	TrainAcc 0.0693	ValidAcc 0.0585	TestAcc 0.0577	BestValid 0.2626
	Epoch 2800:	Loss 0.4019	TrainAcc 0.0981	ValidAcc 0.0951	TestAcc 0.0911	BestValid 0.2626
	Epoch 2825:	Loss 0.4059	TrainAcc 0.0794	ValidAcc 0.0666	TestAcc 0.0649	BestValid 0.2626
	Epoch 2850:	Loss 0.4050	TrainAcc 0.0832	ValidAcc 0.0775	TestAcc 0.0756	BestValid 0.2626
	Epoch 2875:	Loss 0.4032	TrainAcc 0.0713	ValidAcc 0.0600	TestAcc 0.0591	BestValid 0.2626
	Epoch 2900:	Loss 0.4006	TrainAcc 0.0696	ValidAcc 0.0587	TestAcc 0.0578	BestValid 0.2626
	Epoch 2925:	Loss 0.4180	TrainAcc 0.0827	ValidAcc 0.0686	TestAcc 0.0677	BestValid 0.2626
	Epoch 2950:	Loss 0.4062	TrainAcc 0.1015	ValidAcc 0.0914	TestAcc 0.0883	BestValid 0.2626
	Epoch 2975:	Loss 0.4101	TrainAcc 0.0699	ValidAcc 0.0591	TestAcc 0.0581	BestValid 0.2626
	Epoch 3000:	Loss 0.4075	TrainAcc 0.1384	ValidAcc 0.1263	TestAcc 0.1230	BestValid 0.2626
	Epoch 3025:	Loss 0.4853	TrainAcc 0.1254	ValidAcc 0.1138	TestAcc 0.1116	BestValid 0.2626
	Epoch 3050:	Loss 0.4326	TrainAcc 0.0707	ValidAcc 0.0594	TestAcc 0.0584	BestValid 0.2626
	Epoch 3075:	Loss 0.4358	TrainAcc 0.0829	ValidAcc 0.0777	TestAcc 0.0753	BestValid 0.2626
	Epoch 3100:	Loss 0.4240	TrainAcc 0.0715	ValidAcc 0.0600	TestAcc 0.0592	BestValid 0.2626
	Epoch 3125:	Loss 0.4329	TrainAcc 0.0989	ValidAcc 0.0892	TestAcc 0.0859	BestValid 0.2626
	Epoch 3150:	Loss 0.4127	TrainAcc 0.0243	ValidAcc 0.0221	TestAcc 0.0218	BestValid 0.2626
	Epoch 3175:	Loss 0.4055	TrainAcc 0.0913	ValidAcc 0.0786	TestAcc 0.0771	BestValid 0.2626
	Epoch 3200:	Loss 0.4035	TrainAcc 0.0435	ValidAcc 0.0454	TestAcc 0.0421	BestValid 0.2626
	Epoch 3225:	Loss 0.4246	TrainAcc 0.0784	ValidAcc 0.0670	TestAcc 0.0667	BestValid 0.2626
	Epoch 3250:	Loss 0.3940	TrainAcc 0.0756	ValidAcc 0.0636	TestAcc 0.0629	BestValid 0.2626
	Epoch 3275:	Loss 0.3940	TrainAcc 0.0761	ValidAcc 0.0630	TestAcc 0.0619	BestValid 0.2626
	Epoch 3300:	Loss 0.3896	TrainAcc 0.0099	ValidAcc 0.0080	TestAcc 0.0080	BestValid 0.2626
	Epoch 3325:	Loss 0.3874	TrainAcc 0.0840	ValidAcc 0.0735	TestAcc 0.0732	BestValid 0.2626
	Epoch 3350:	Loss 0.3904	TrainAcc 0.0099	ValidAcc 0.0086	TestAcc 0.0079	BestValid 0.2626
	Epoch 3375:	Loss 0.3901	TrainAcc 0.0841	ValidAcc 0.0730	TestAcc 0.0727	BestValid 0.2626
	Epoch 3400:	Loss 0.3859	TrainAcc 0.0770	ValidAcc 0.0650	TestAcc 0.0647	BestValid 0.2626
	Epoch 3425:	Loss 0.3872	TrainAcc 0.0990	ValidAcc 0.0887	TestAcc 0.0878	BestValid 0.2626
	Epoch 3450:	Loss 0.3813	TrainAcc 0.0764	ValidAcc 0.0645	TestAcc 0.0644	BestValid 0.2626
	Epoch 3475:	Loss 0.3820	TrainAcc 0.1010	ValidAcc 0.0961	TestAcc 0.0934	BestValid 0.2626
	Epoch 3500:	Loss 0.3811	TrainAcc 0.0803	ValidAcc 0.0680	TestAcc 0.0683	BestValid 0.2626
	Epoch 3525:	Loss 0.3807	TrainAcc 0.0894	ValidAcc 0.0799	TestAcc 0.0797	BestValid 0.2626
	Epoch 3550:	Loss 0.3796	TrainAcc 0.1312	ValidAcc 0.1246	TestAcc 0.1220	BestValid 0.2626
	Epoch 3575:	Loss 0.3776	TrainAcc 0.1193	ValidAcc 0.1149	TestAcc 0.1100	BestValid 0.2626
	Epoch 3600:	Loss 0.3773	TrainAcc 0.1238	ValidAcc 0.1189	TestAcc 0.1155	BestValid 0.2626
	Epoch 3625:	Loss 0.3756	TrainAcc 0.1285	ValidAcc 0.1221	TestAcc 0.1196	BestValid 0.2626
	Epoch 3650:	Loss 0.3759	TrainAcc 0.1297	ValidAcc 0.1236	TestAcc 0.1198	BestValid 0.2626
	Epoch 3675:	Loss 0.3753	TrainAcc 0.0591	ValidAcc 0.0640	TestAcc 0.0616	BestValid 0.2626
	Epoch 3700:	Loss 0.3751	TrainAcc 0.1296	ValidAcc 0.1235	TestAcc 0.1204	BestValid 0.2626
	Epoch 3725:	Loss 0.3737	TrainAcc 0.1441	ValidAcc 0.1358	TestAcc 0.1337	BestValid 0.2626
	Epoch 3750:	Loss 0.3711	TrainAcc 0.0891	ValidAcc 0.0773	TestAcc 0.0750	BestValid 0.2626
	Epoch 3775:	Loss 0.3748	TrainAcc 0.0861	ValidAcc 0.0776	TestAcc 0.0764	BestValid 0.2626
	Epoch 3800:	Loss 0.3731	TrainAcc 0.1572	ValidAcc 0.1543	TestAcc 0.1486	BestValid 0.2626
	Epoch 3825:	Loss 0.4019	TrainAcc 0.1263	ValidAcc 0.1207	TestAcc 0.1177	BestValid 0.2626
	Epoch 3850:	Loss 0.3773	TrainAcc 0.0990	ValidAcc 0.0936	TestAcc 0.0916	BestValid 0.2626
	Epoch 3875:	Loss 0.3759	TrainAcc 0.1216	ValidAcc 0.1111	TestAcc 0.1073	BestValid 0.2626
	Epoch 3900:	Loss 0.3766	TrainAcc 0.0730	ValidAcc 0.0621	TestAcc 0.0612	BestValid 0.2626
	Epoch 3925:	Loss 0.3968	TrainAcc 0.1246	ValidAcc 0.1193	TestAcc 0.1153	BestValid 0.2626
	Epoch 3950:	Loss 0.4033	TrainAcc 0.0528	ValidAcc 0.0519	TestAcc 0.0495	BestValid 0.2626
	Epoch 3975:	Loss 0.3856	TrainAcc 0.1622	ValidAcc 0.1578	TestAcc 0.1524	BestValid 0.2626
	Epoch 4000:	Loss 0.3805	TrainAcc 0.0778	ValidAcc 0.0687	TestAcc 0.0675	BestValid 0.2626
	Epoch 4025:	Loss 0.3794	TrainAcc 0.1251	ValidAcc 0.1191	TestAcc 0.1155	BestValid 0.2626
	Epoch 4050:	Loss 0.3724	TrainAcc 0.1255	ValidAcc 0.1191	TestAcc 0.1156	BestValid 0.2626
	Epoch 4075:	Loss 0.3716	TrainAcc 0.0737	ValidAcc 0.0664	TestAcc 0.0637	BestValid 0.2626
	Epoch 4100:	Loss 0.3694	TrainAcc 0.0807	ValidAcc 0.0760	TestAcc 0.0733	BestValid 0.2626
	Epoch 4125:	Loss 0.3689	TrainAcc 0.0867	ValidAcc 0.0743	TestAcc 0.0714	BestValid 0.2626
	Epoch 4150:	Loss 0.3653	TrainAcc 0.1331	ValidAcc 0.1234	TestAcc 0.1198	BestValid 0.2626
	Epoch 4175:	Loss 0.3650	TrainAcc 0.1260	ValidAcc 0.1191	TestAcc 0.1159	BestValid 0.2626
	Epoch 4200:	Loss 0.3632	TrainAcc 0.1259	ValidAcc 0.1193	TestAcc 0.1160	BestValid 0.2626
	Epoch 4225:	Loss 0.3647	TrainAcc 0.1262	ValidAcc 0.1193	TestAcc 0.1161	BestValid 0.2626
	Epoch 4250:	Loss 0.3614	TrainAcc 0.1271	ValidAcc 0.1197	TestAcc 0.1164	BestValid 0.2626
	Epoch 4275:	Loss 0.3647	TrainAcc 0.1257	ValidAcc 0.1192	TestAcc 0.1159	BestValid 0.2626
	Epoch 4300:	Loss 0.3641	TrainAcc 0.1252	ValidAcc 0.1192	TestAcc 0.1156	BestValid 0.2626
	Epoch 4325:	Loss 0.3619	TrainAcc 0.1262	ValidAcc 0.1194	TestAcc 0.1162	BestValid 0.2626
	Epoch 4350:	Loss 0.3617	TrainAcc 0.1259	ValidAcc 0.1194	TestAcc 0.1161	BestValid 0.2626
	Epoch 4375:	Loss 0.3610	TrainAcc 0.1261	ValidAcc 0.1198	TestAcc 0.1163	BestValid 0.2626
	Epoch 4400:	Loss 0.3577	TrainAcc 0.1261	ValidAcc 0.1194	TestAcc 0.1163	BestValid 0.2626
	Epoch 4425:	Loss 0.3590	TrainAcc 0.1239	ValidAcc 0.1096	TestAcc 0.1062	BestValid 0.2626
	Epoch 4450:	Loss 0.3578	TrainAcc 0.1247	ValidAcc 0.1186	TestAcc 0.1154	BestValid 0.2626
	Epoch 4475:	Loss 0.3581	TrainAcc 0.1268	ValidAcc 0.1196	TestAcc 0.1160	BestValid 0.2626
	Epoch 4500:	Loss 0.3593	TrainAcc 0.0730	ValidAcc 0.0682	TestAcc 0.0654	BestValid 0.2626
	Epoch 4525:	Loss 0.3648	TrainAcc 0.1250	ValidAcc 0.1191	TestAcc 0.1154	BestValid 0.2626
	Epoch 4550:	Loss 0.3594	TrainAcc 0.1305	ValidAcc 0.1250	TestAcc 0.1221	BestValid 0.2626
	Epoch 4575:	Loss 0.3596	TrainAcc 0.1284	ValidAcc 0.1230	TestAcc 0.1193	BestValid 0.2626
	Epoch 4600:	Loss 0.3592	TrainAcc 0.1246	ValidAcc 0.1186	TestAcc 0.1152	BestValid 0.2626
	Epoch 4625:	Loss 0.3699	TrainAcc 0.0728	ValidAcc 0.0613	TestAcc 0.0606	BestValid 0.2626
	Epoch 4650:	Loss 0.3651	TrainAcc 0.1250	ValidAcc 0.1192	TestAcc 0.1154	BestValid 0.2626
	Epoch 4675:	Loss 0.3606	TrainAcc 0.0877	ValidAcc 0.0775	TestAcc 0.0765	BestValid 0.2626
	Epoch 4700:	Loss 0.3548	TrainAcc 0.1234	ValidAcc 0.1181	TestAcc 0.1144	BestValid 0.2626
	Epoch 4725:	Loss 0.3576	TrainAcc 0.1250	ValidAcc 0.1190	TestAcc 0.1154	BestValid 0.2626
	Epoch 4750:	Loss 0.3576	TrainAcc 0.1238	ValidAcc 0.1184	TestAcc 0.1145	BestValid 0.2626
	Epoch 4775:	Loss 0.3533	TrainAcc 0.1247	ValidAcc 0.1190	TestAcc 0.1153	BestValid 0.2626
	Epoch 4800:	Loss 0.3522	TrainAcc 0.1248	ValidAcc 0.1190	TestAcc 0.1152	BestValid 0.2626
	Epoch 4825:	Loss 0.3519	TrainAcc 0.1248	ValidAcc 0.1190	TestAcc 0.1153	BestValid 0.2626
	Epoch 4850:	Loss 0.3523	TrainAcc 0.1249	ValidAcc 0.1191	TestAcc 0.1153	BestValid 0.2626
	Epoch 4875:	Loss 0.3477	TrainAcc 0.1236	ValidAcc 0.1183	TestAcc 0.1145	BestValid 0.2626
	Epoch 4900:	Loss 0.3484	TrainAcc 0.1249	ValidAcc 0.1190	TestAcc 0.1153	BestValid 0.2626
	Epoch 4925:	Loss 0.3492	TrainAcc 0.1248	ValidAcc 0.1189	TestAcc 0.1152	BestValid 0.2626
	Epoch 4950:	Loss 0.3502	TrainAcc 0.1247	ValidAcc 0.1188	TestAcc 0.1151	BestValid 0.2626
	Epoch 4975:	Loss 0.3491	TrainAcc 0.1249	ValidAcc 0.1190	TestAcc 0.1153	BestValid 0.2626
	Epoch 5000:	Loss 0.3466	TrainAcc 0.1251	ValidAcc 0.1193	TestAcc 0.1154	BestValid 0.2626
Node 2, Pre/Post-Pipelining: 1.106 / 1.163 ms, Bubble: 100.959 ms, Compute: 372.916 ms, Comm: 30.213 ms, Imbalance: 38.357 ms
Node 3, Pre/Post-Pipelining: 1.107 / 1.157 ms, Bubble: 100.819 ms, Compute: 381.770 ms, Comm: 27.040 ms, Imbalance: 32.518 ms
Node 4, Pre/Post-Pipelining: 1.106 / 1.098 ms, Bubble: 101.970 ms, Compute: 369.422 ms, Comm: 26.614 ms, Imbalance: 44.820 ms
Node 1, Pre/Post-Pipelining: 1.110 / 1.116 ms, Bubble: 99.954 ms, Compute: 385.197 ms, Comm: 28.162 ms, Imbalance: 28.812 ms
Node 6, Pre/Post-Pipelining: 1.108 / 1.147 ms, Bubble: 102.645 ms, Compute: 381.366 ms, Comm: 29.415 ms, Imbalance: 28.931 ms
Node 5, Pre/Post-Pipelining: 1.108 / 1.079 ms, Bubble: 102.353 ms, Compute: 376.575 ms, Comm: 31.087 ms, Imbalance: 32.485 ms
Node 7, Pre/Post-Pipelining: 1.105 / 16.389 ms, Bubble: 88.084 ms, Compute: 396.284 ms, Comm: 17.486 ms, Imbalance: 24.996 ms
Node 0, Pre/Post-Pipelining: 1.110 / 1.182 ms, Bubble: 98.991 ms, Compute: 417.115 ms, Comm: 16.806 ms, Imbalance: 8.544 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.110 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.182 ms
Cluster-Wide Average, Bubble: 98.991 ms
Cluster-Wide Average, Compute: 417.115 ms
Cluster-Wide Average, Communication: 16.806 ms
Cluster-Wide Average, Imbalance: 8.544 ms
Node 0, GPU memory consumption: 6.614 GB
Node 2, GPU memory consumption: 5.260 GB
Node 1, GPU memory consumption: 5.260 GB
Node 3, GPU memory consumption: 5.237 GB
Node 4, GPU memory consumption: 5.237 GB
Node 5, GPU memory consumption: 5.260 GB
Node 6, GPU memory consumption: 5.260 GB
Node 7, GPU memory consumption: 5.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 1.074052 s---------------
------------------------node id 1,  per-epoch time: 1.074052 s---------------
------------------------node id 2,  per-epoch time: 1.074052 s---------------
------------------------node id 4,  per-epoch time: 1.074052 s---------------
------------------------node id 3,  per-epoch time: 1.074052 s---------------
------------------------node id 5,  per-epoch time: 1.074052 s---------------
------------------------node id 6,  per-epoch time: 1.074052 s---------------
------------------------node id 7,  per-epoch time: 1.074052 s---------------
************ Profiling Results ************
	Bubble: 689.266624 (ms) (64.20 percentage)
	Compute: 379.811538 (ms) (35.38 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.547042 (ms) (0.42 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 403.702 Gbps
Highest valid_acc: 0.2626
Target test_acc: 0.2598
Epoch to reach the target acc: 74
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
