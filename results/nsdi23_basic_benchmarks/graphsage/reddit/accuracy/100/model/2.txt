Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.920 seconds.
Building the CSC structure...
        It takes 2.354 seconds.
Building the CSC structure...
        It takes 2.406 seconds.
Building the CSC structure...
        It takes 2.443 seconds.
Building the CSC structure...
        It takes 2.471 seconds.
Building the CSC structure...
        It takes 2.526 seconds.
Building the CSC structure...
        It takes 2.650 seconds.
Building the CSC structure...
        It takes 2.654 seconds.
Building the CSC structure...
        It takes 1.844 seconds.
        It takes 2.168 seconds.
        It takes 2.299 seconds.
Building the Feature Vector...
        It takes 2.402 seconds.
        It takes 2.407 seconds.
        It takes 2.416 seconds.
        It takes 2.296 seconds.
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 2.495 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.287 seconds.
Building the Label Vector...
        It takes 0.249 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.030 seconds.
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.846 Gbps (per GPU), 446.765 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.599 Gbps (per GPU), 444.793 Gbps (aggregated)
The layer-level communication performance: 55.588 Gbps (per GPU), 444.706 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.382 Gbps (per GPU), 443.059 Gbps (aggregated)
The layer-level communication performance: 55.357 Gbps (per GPU), 442.857 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.177 Gbps (per GPU), 441.416 Gbps (aggregated)
The layer-level communication performance: 55.137 Gbps (per GPU), 441.099 Gbps (aggregated)
The layer-level communication performance: 55.101 Gbps (per GPU), 440.809 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 161.465 Gbps (per GPU), 1291.723 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.465 Gbps (per GPU), 1291.723 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.468 Gbps (per GPU), 1291.747 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.468 Gbps (per GPU), 1291.747 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.456 Gbps (per GPU), 1291.648 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.456 Gbps (per GPU), 1291.648 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.453 Gbps (per GPU), 1291.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.450 Gbps (per GPU), 1291.598 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.155 Gbps (per GPU), 833.237 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.152 Gbps (per GPU), 833.216 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.155 Gbps (per GPU), 833.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.155 Gbps (per GPU), 833.243 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.155 Gbps (per GPU), 833.236 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.154 Gbps (per GPU), 833.230 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.154 Gbps (per GPU), 833.230 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.150 Gbps (per GPU), 833.203 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.496 Gbps (per GPU), 267.966 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.496 Gbps (per GPU), 267.969 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.497 Gbps (per GPU), 267.973 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.495 Gbps (per GPU), 267.959 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.496 Gbps (per GPU), 267.968 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.495 Gbps (per GPU), 267.961 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.495 Gbps (per GPU), 267.963 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.496 Gbps (per GPU), 267.969 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.32ms  2.67ms  2.38ms  1.40  8.38K  3.53M
 chk_1  3.54ms  2.90ms  2.73ms  1.30  6.74K  3.60M
 chk_2  3.46ms  2.80ms  2.61ms  1.33  7.27K  3.53M
 chk_3  3.50ms  2.84ms  2.62ms  1.33  7.92K  3.61M
 chk_4  3.19ms  2.74ms  2.60ms  1.23  5.33K  3.68M
 chk_5  3.59ms  2.79ms  2.47ms  1.45 10.07K  3.45M
 chk_6  3.73ms  2.94ms  2.64ms  1.42  9.41K  3.48M
 chk_7  3.42ms  2.78ms  2.53ms  1.35  8.12K  3.60M
 chk_8  3.38ms  2.85ms  2.68ms  1.26  6.09K  3.64M
 chk_9  3.62ms  2.71ms  2.36ms  1.53 11.10K  3.38M
chk_10  3.38ms  2.88ms  2.73ms  1.24  5.67K  3.63M
chk_11  3.44ms  2.89ms  2.54ms  1.35  8.16K  3.54M
chk_12  3.63ms  2.93ms  2.75ms  1.32  7.24K  3.55M
chk_13  3.26ms  2.77ms  2.63ms  1.24  5.41K  3.68M
chk_14  3.68ms  3.02ms  2.82ms  1.31  7.14K  3.53M
chk_15  3.69ms  2.94ms  2.64ms  1.40  9.25K  3.49M
chk_16  3.15ms  2.73ms  2.59ms  1.22  4.78K  3.77M
chk_17  3.45ms  2.86ms  2.66ms  1.29  6.85K  3.60M
chk_18  3.32ms  2.66ms  2.47ms  1.35  7.47K  3.57M
chk_19  3.16ms  2.74ms  2.59ms  1.22  4.88K  3.75M
chk_20  3.37ms  2.75ms  2.57ms  1.31  7.00K  3.63M
chk_21  3.21ms  2.73ms  2.56ms  1.25  5.41K  3.68M
chk_22  3.92ms  2.99ms  2.62ms  1.50 11.07K  3.39M
chk_23  3.50ms  2.83ms  2.79ms  1.25  7.23K  3.64M
chk_24  3.75ms  2.94ms  2.60ms  1.44 10.13K  3.43M
chk_25  3.26ms  2.70ms  2.52ms  1.29  6.40K  3.57M
chk_26  3.43ms  2.89ms  2.73ms  1.26  5.78K  3.55M
chk_27  3.57ms  2.88ms  2.55ms  1.40  9.34K  3.48M
chk_28  3.64ms  3.08ms  2.88ms  1.26  6.37K  3.57M
chk_29  3.38ms  2.87ms  2.72ms  1.24  5.16K  3.78M
chk_30  3.24ms  2.77ms  2.66ms  1.22  5.44K  3.67M
chk_31  3.49ms  2.92ms  2.76ms  1.26  6.33K  3.63M
   Avg  3.46  2.84  2.63
   Max  3.92  3.08  2.88
   Min  3.15  2.66  2.36
 Ratio  1.24  1.16  1.22
   Var  0.03  0.01  0.01
Profiling takes 3.323 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 383.068 ms
Partition 0 [0, 4) has cost: 383.068 ms
Partition 1 [4, 8) has cost: 363.181 ms
Partition 2 [8, 12) has cost: 363.181 ms
Partition 3 [12, 16) has cost: 363.181 ms
Partition 4 [16, 20) has cost: 363.181 ms
Partition 5 [20, 24) has cost: 363.181 ms
Partition 6 [24, 28) has cost: 363.181 ms
Partition 7 [28, 32) has cost: 356.400 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 173.133 ms
GPU 0, Compute+Comm Time: 138.739 ms, Bubble Time: 30.182 ms, Imbalance Overhead: 4.212 ms
GPU 1, Compute+Comm Time: 132.964 ms, Bubble Time: 29.886 ms, Imbalance Overhead: 10.283 ms
GPU 2, Compute+Comm Time: 132.964 ms, Bubble Time: 29.834 ms, Imbalance Overhead: 10.335 ms
GPU 3, Compute+Comm Time: 132.964 ms, Bubble Time: 29.695 ms, Imbalance Overhead: 10.474 ms
GPU 4, Compute+Comm Time: 132.964 ms, Bubble Time: 29.677 ms, Imbalance Overhead: 10.492 ms
GPU 5, Compute+Comm Time: 132.964 ms, Bubble Time: 29.643 ms, Imbalance Overhead: 10.526 ms
GPU 6, Compute+Comm Time: 132.964 ms, Bubble Time: 29.856 ms, Imbalance Overhead: 10.313 ms
GPU 7, Compute+Comm Time: 130.727 ms, Bubble Time: 30.309 ms, Imbalance Overhead: 12.097 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 335.268 ms
GPU 0, Compute+Comm Time: 252.371 ms, Bubble Time: 59.704 ms, Imbalance Overhead: 23.192 ms
GPU 1, Compute+Comm Time: 256.915 ms, Bubble Time: 58.776 ms, Imbalance Overhead: 19.577 ms
GPU 2, Compute+Comm Time: 256.915 ms, Bubble Time: 58.292 ms, Imbalance Overhead: 20.061 ms
GPU 3, Compute+Comm Time: 256.915 ms, Bubble Time: 58.292 ms, Imbalance Overhead: 20.060 ms
GPU 4, Compute+Comm Time: 256.915 ms, Bubble Time: 58.217 ms, Imbalance Overhead: 20.136 ms
GPU 5, Compute+Comm Time: 256.915 ms, Bubble Time: 58.431 ms, Imbalance Overhead: 19.922 ms
GPU 6, Compute+Comm Time: 256.915 ms, Bubble Time: 58.417 ms, Imbalance Overhead: 19.935 ms
GPU 7, Compute+Comm Time: 271.027 ms, Bubble Time: 58.875 ms, Imbalance Overhead: 5.365 ms
The estimated cost of the whole pipeline: 533.821 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 746.250 ms
Partition 0 [0, 8) has cost: 746.250 ms
Partition 1 [8, 16) has cost: 726.362 ms
Partition 2 [16, 24) has cost: 726.362 ms
Partition 3 [24, 32) has cost: 719.581 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 179.946 ms
GPU 0, Compute+Comm Time: 148.229 ms, Bubble Time: 27.972 ms, Imbalance Overhead: 3.744 ms
GPU 1, Compute+Comm Time: 145.121 ms, Bubble Time: 27.414 ms, Imbalance Overhead: 7.411 ms
GPU 2, Compute+Comm Time: 145.121 ms, Bubble Time: 27.161 ms, Imbalance Overhead: 7.664 ms
GPU 3, Compute+Comm Time: 144.124 ms, Bubble Time: 26.848 ms, Imbalance Overhead: 8.973 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 334.952 ms
GPU 0, Compute+Comm Time: 268.720 ms, Bubble Time: 50.888 ms, Imbalance Overhead: 15.344 ms
GPU 1, Compute+Comm Time: 270.972 ms, Bubble Time: 51.245 ms, Imbalance Overhead: 12.735 ms
GPU 2, Compute+Comm Time: 270.972 ms, Bubble Time: 51.315 ms, Imbalance Overhead: 12.665 ms
GPU 3, Compute+Comm Time: 278.781 ms, Bubble Time: 52.313 ms, Imbalance Overhead: 3.857 ms
    The estimated cost with 2 DP ways is 540.642 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1472.612 ms
Partition 0 [0, 16) has cost: 1472.612 ms
Partition 1 [16, 32) has cost: 1445.943 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 225.315 ms
GPU 0, Compute+Comm Time: 195.498 ms, Bubble Time: 24.074 ms, Imbalance Overhead: 5.743 ms
GPU 1, Compute+Comm Time: 193.503 ms, Bubble Time: 24.700 ms, Imbalance Overhead: 7.112 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 370.516 ms
GPU 0, Compute+Comm Time: 320.786 ms, Bubble Time: 41.416 ms, Imbalance Overhead: 8.314 ms
GPU 1, Compute+Comm Time: 325.986 ms, Bubble Time: 40.131 ms, Imbalance Overhead: 4.399 ms
    The estimated cost with 4 DP ways is 625.623 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2918.555 ms
Partition 0 [0, 32) has cost: 2918.555 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 573.584 ms
GPU 0, Compute+Comm Time: 573.584 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 704.277 ms
GPU 0, Compute+Comm Time: 704.277 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1341.755 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2452	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 50:	Loss 3.0810	TrainAcc 0.0690	ValidAcc 0.0585	TestAcc 0.0574	BestValid 0.0585
	Epoch 75:	Loss 2.8587	TrainAcc 0.1763	ValidAcc 0.2049	TestAcc 0.2056	BestValid 0.2049
	Epoch 100:	Loss 2.6924	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 125:	Loss 2.4486	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 150:	Loss 2.3537	TrainAcc 0.0983	ValidAcc 0.0913	TestAcc 0.0881	BestValid 0.2049
	Epoch 175:	Loss 2.2442	TrainAcc 0.0910	ValidAcc 0.0727	TestAcc 0.0716	BestValid 0.2049
	Epoch 200:	Loss 2.1471	TrainAcc 0.0938	ValidAcc 0.0740	TestAcc 0.0730	BestValid 0.2049
	Epoch 225:	Loss 1.9815	TrainAcc 0.0920	ValidAcc 0.0734	TestAcc 0.0722	BestValid 0.2049
	Epoch 250:	Loss 1.8085	TrainAcc 0.0900	ValidAcc 0.0717	TestAcc 0.0703	BestValid 0.2049
	Epoch 275:	Loss 1.6571	TrainAcc 0.0824	ValidAcc 0.0684	TestAcc 0.0665	BestValid 0.2049
	Epoch 300:	Loss 1.5094	TrainAcc 0.0241	ValidAcc 0.0158	TestAcc 0.0156	BestValid 0.2049
	Epoch 325:	Loss 1.3822	TrainAcc 0.0305	ValidAcc 0.0192	TestAcc 0.0189	BestValid 0.2049
	Epoch 350:	Loss 1.2945	TrainAcc 0.0747	ValidAcc 0.0608	TestAcc 0.0649	BestValid 0.2049
	Epoch 375:	Loss 1.2285	TrainAcc 0.1579	ValidAcc 0.1330	TestAcc 0.1351	BestValid 0.2049
	Epoch 400:	Loss 1.1727	TrainAcc 0.0544	ValidAcc 0.0448	TestAcc 0.0491	BestValid 0.2049
	Epoch 425:	Loss 1.1205	TrainAcc 0.0555	ValidAcc 0.0441	TestAcc 0.0477	BestValid 0.2049
	Epoch 450:	Loss 1.0771	TrainAcc 0.1099	ValidAcc 0.0799	TestAcc 0.0832	BestValid 0.2049
	Epoch 475:	Loss 1.0221	TrainAcc 0.0557	ValidAcc 0.0441	TestAcc 0.0481	BestValid 0.2049
	Epoch 500:	Loss 0.9655	TrainAcc 0.0863	ValidAcc 0.0590	TestAcc 0.0627	BestValid 0.2049
	Epoch 525:	Loss 0.9198	TrainAcc 0.0595	ValidAcc 0.0454	TestAcc 0.0490	BestValid 0.2049
	Epoch 550:	Loss 0.8808	TrainAcc 0.0541	ValidAcc 0.0436	TestAcc 0.0469	BestValid 0.2049
	Epoch 575:	Loss 0.8553	TrainAcc 0.0550	ValidAcc 0.0447	TestAcc 0.0481	BestValid 0.2049
	Epoch 600:	Loss 0.8316	TrainAcc 0.0575	ValidAcc 0.0488	TestAcc 0.0519	BestValid 0.2049
	Epoch 625:	Loss 0.8264	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2049
	Epoch 650:	Loss 0.7977	TrainAcc 0.0504	ValidAcc 0.0418	TestAcc 0.0455	BestValid 0.2049
	Epoch 675:	Loss 0.7663	TrainAcc 0.0702	ValidAcc 0.0628	TestAcc 0.0663	BestValid 0.2049
	Epoch 700:	Loss 0.7452	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2049
	Epoch 725:	Loss 0.7420	TrainAcc 0.0501	ValidAcc 0.0418	TestAcc 0.0454	BestValid 0.2049
	Epoch 750:	Loss 0.7164	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2049
	Epoch 775:	Loss 0.6993	TrainAcc 0.0505	ValidAcc 0.0419	TestAcc 0.0459	BestValid 0.2049
	Epoch 800:	Loss 0.6851	TrainAcc 0.0505	ValidAcc 0.0420	TestAcc 0.0459	BestValid 0.2049
	Epoch 825:	Loss 0.6787	TrainAcc 0.0589	ValidAcc 0.0496	TestAcc 0.0537	BestValid 0.2049
	Epoch 850:	Loss 0.6573	TrainAcc 0.0506	ValidAcc 0.0418	TestAcc 0.0454	BestValid 0.2049
	Epoch 875:	Loss 0.6452	TrainAcc 0.0504	ValidAcc 0.0418	TestAcc 0.0452	BestValid 0.2049
	Epoch 900:	Loss 0.6258	TrainAcc 0.0503	ValidAcc 0.0418	TestAcc 0.0452	BestValid 0.2049
	Epoch 925:	Loss 0.6226	TrainAcc 0.0508	ValidAcc 0.0420	TestAcc 0.0456	BestValid 0.2049
	Epoch 950:	Loss 0.6229	TrainAcc 0.0502	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2049
	Epoch 975:	Loss 0.6029	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0451	BestValid 0.2049
	Epoch 1000:	Loss 0.5902	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2049
	Epoch 1025:	Loss 0.5908	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2049
	Epoch 1050:	Loss 0.5762	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0452	BestValid 0.2049
	Epoch 1075:	Loss 0.5628	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2049
	Epoch 1100:	Loss 0.5698	TrainAcc 0.0502	ValidAcc 0.0417	TestAcc 0.0453	BestValid 0.2049
	Epoch 1125:	Loss 0.5525	TrainAcc 0.0507	ValidAcc 0.0418	TestAcc 0.0455	BestValid 0.2049
	Epoch 1150:	Loss 0.5396	TrainAcc 0.0517	ValidAcc 0.0424	TestAcc 0.0460	BestValid 0.2049
	Epoch 1175:	Loss 0.5431	TrainAcc 0.0547	ValidAcc 0.0458	TestAcc 0.0498	BestValid 0.2049
	Epoch 1200:	Loss 0.5471	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0453	BestValid 0.2049
	Epoch 1225:	Loss 0.5876	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2049
	Epoch 1250:	Loss 0.5508	TrainAcc 0.0203	ValidAcc 0.0210	TestAcc 0.0209	BestValid 0.2049
	Epoch 1275:	Loss 0.5351	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2049
	Epoch 1300:	Loss 0.5286	TrainAcc 0.0673	ValidAcc 0.0598	TestAcc 0.0629	BestValid 0.2049
	Epoch 1325:	Loss 0.5245	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0452	BestValid 0.2049
	Epoch 1350:	Loss 0.5152	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0452	BestValid 0.2049
	Epoch 1375:	Loss 0.5069	TrainAcc 0.0513	ValidAcc 0.0424	TestAcc 0.0463	BestValid 0.2049
	Epoch 1400:	Loss 0.5003	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0452	BestValid 0.2049
	Epoch 1425:	Loss 0.5050	TrainAcc 0.0685	ValidAcc 0.0593	TestAcc 0.0633	BestValid 0.2049
	Epoch 1450:	Loss 0.4982	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0452	BestValid 0.2049
	Epoch 1475:	Loss 0.4912	TrainAcc 0.0501	ValidAcc 0.0415	TestAcc 0.0452	BestValid 0.2049
	Epoch 1500:	Loss 0.4886	TrainAcc 0.0501	ValidAcc 0.0415	TestAcc 0.0452	BestValid 0.2049
	Epoch 1525:	Loss 0.4827	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0451	BestValid 0.2049
	Epoch 1550:	Loss 0.4815	TrainAcc 0.0501	ValidAcc 0.0417	TestAcc 0.0452	BestValid 0.2049
	Epoch 1575:	Loss 0.4749	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0451	BestValid 0.2049
	Epoch 1600:	Loss 0.4758	TrainAcc 0.0501	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.2049
	Epoch 1625:	Loss 0.4763	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0451	BestValid 0.2049
	Epoch 1650:	Loss 0.4762	TrainAcc 0.0504	ValidAcc 0.0417	TestAcc 0.0454	BestValid 0.2049
	Epoch 1675:	Loss 0.4646	TrainAcc 0.0500	ValidAcc 0.0415	TestAcc 0.0451	BestValid 0.2049
	Epoch 1700:	Loss 0.4653	TrainAcc 0.0487	ValidAcc 0.0410	TestAcc 0.0445	BestValid 0.2049
	Epoch 1725:	Loss 0.4590	TrainAcc 0.0009	ValidAcc 0.0009	TestAcc 0.0006	BestValid 0.2049
	Epoch 1750:	Loss 0.4619	TrainAcc 0.0008	ValidAcc 0.0005	TestAcc 0.0005	BestValid 0.2049
	Epoch 1775:	Loss 0.4553	TrainAcc 0.0021	ValidAcc 0.0018	TestAcc 0.0017	BestValid 0.2049
	Epoch 1800:	Loss 0.4536	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0451	BestValid 0.2049
	Epoch 1825:	Loss 0.4528	TrainAcc 0.0402	ValidAcc 0.0347	TestAcc 0.0381	BestValid 0.2049
	Epoch 1850:	Loss 0.4475	TrainAcc 0.0005	ValidAcc 0.0003	TestAcc 0.0002	BestValid 0.2049
	Epoch 1875:	Loss 0.4413	TrainAcc 0.0011	ValidAcc 0.0005	TestAcc 0.0005	BestValid 0.2049
	Epoch 1900:	Loss 0.4423	TrainAcc 0.0022	ValidAcc 0.0016	TestAcc 0.0015	BestValid 0.2049
	Epoch 1925:	Loss 0.4439	TrainAcc 0.0006	ValidAcc 0.0003	TestAcc 0.0004	BestValid 0.2049
	Epoch 1950:	Loss 0.4370	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 1975:	Loss 0.4339	TrainAcc 0.0691	ValidAcc 0.0585	TestAcc 0.0575	BestValid 0.2049
	Epoch 2000:	Loss 0.4325	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 2025:	Loss 0.4506	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2050:	Loss 0.4319	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 2075:	Loss 0.4326	TrainAcc 0.0004	ValidAcc 0.0003	TestAcc 0.0002	BestValid 0.2049
	Epoch 2100:	Loss 0.4274	TrainAcc 0.0696	ValidAcc 0.0589	TestAcc 0.0579	BestValid 0.2049
	Epoch 2125:	Loss 0.4331	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2150:	Loss 0.4203	TrainAcc 0.0692	ValidAcc 0.0586	TestAcc 0.0575	BestValid 0.2049
	Epoch 2175:	Loss 0.4199	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 2200:	Loss 0.4199	TrainAcc 0.0691	ValidAcc 0.0585	TestAcc 0.0575	BestValid 0.2049
	Epoch 2225:	Loss 0.4256	TrainAcc 0.0697	ValidAcc 0.0590	TestAcc 0.0579	BestValid 0.2049
	Epoch 2250:	Loss 0.4173	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2275:	Loss 0.4153	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 2300:	Loss 0.4132	TrainAcc 0.0738	ValidAcc 0.0613	TestAcc 0.0608	BestValid 0.2049
	Epoch 2325:	Loss 0.4093	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 2350:	Loss 0.4070	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2375:	Loss 0.4073	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2400:	Loss 0.4024	TrainAcc 0.0690	ValidAcc 0.0585	TestAcc 0.0574	BestValid 0.2049
	Epoch 2425:	Loss 0.4060	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2450:	Loss 0.3995	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 2475:	Loss 0.4025	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2500:	Loss 0.4037	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2525:	Loss 0.4009	TrainAcc 0.0690	ValidAcc 0.0585	TestAcc 0.0574	BestValid 0.2049
	Epoch 2550:	Loss 0.3979	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2575:	Loss 0.3974	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2600:	Loss 0.3950	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2625:	Loss 0.3923	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2650:	Loss 0.3933	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2675:	Loss 0.3902	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2700:	Loss 0.3896	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2725:	Loss 0.3895	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2750:	Loss 0.3871	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2775:	Loss 0.3897	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2800:	Loss 0.3874	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2825:	Loss 0.3840	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2850:	Loss 0.3832	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 2875:	Loss 0.3815	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2900:	Loss 0.3836	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2925:	Loss 0.3854	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2950:	Loss 0.3895	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 2975:	Loss 0.3799	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3000:	Loss 0.3784	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3025:	Loss 0.3859	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3050:	Loss 0.3774	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3075:	Loss 0.3779	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3100:	Loss 0.3765	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3125:	Loss 0.3993	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3150:	Loss 0.3834	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 3175:	Loss 0.3833	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3200:	Loss 0.3762	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3225:	Loss 0.3935	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3250:	Loss 0.3838	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 3275:	Loss 0.3777	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 3300:	Loss 0.3754	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3325:	Loss 0.3750	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3350:	Loss 0.3705	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3375:	Loss 0.3722	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3400:	Loss 0.3700	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3425:	Loss 0.3970	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 3450:	Loss 0.3715	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3475:	Loss 0.3699	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3500:	Loss 0.3692	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3525:	Loss 0.3677	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3550:	Loss 0.3669	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3575:	Loss 0.3620	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3600:	Loss 0.3643	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3625:	Loss 0.3640	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3650:	Loss 0.3608	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3675:	Loss 0.3627	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3700:	Loss 0.3606	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3725:	Loss 0.3616	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3750:	Loss 0.3626	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3775:	Loss 0.3596	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3800:	Loss 0.3577	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3825:	Loss 0.3571	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3850:	Loss 0.3582	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3875:	Loss 0.3568	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3900:	Loss 0.3569	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3925:	Loss 0.3559	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3950:	Loss 0.3586	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 3975:	Loss 0.3565	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4000:	Loss 0.3537	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4025:	Loss 0.3559	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4050:	Loss 0.3537	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4075:	Loss 0.3529	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4100:	Loss 0.3539	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4125:	Loss 0.3553	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4150:	Loss 0.3518	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4175:	Loss 0.3565	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4200:	Loss 0.3529	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4225:	Loss 0.3699	TrainAcc 0.0691	ValidAcc 0.0587	TestAcc 0.0576	BestValid 0.2049
	Epoch 4250:	Loss 0.3644	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 4275:	Loss 0.3677	TrainAcc 0.0845	ValidAcc 0.0719	TestAcc 0.0717	BestValid 0.2049
	Epoch 4300:	Loss 0.3693	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.2049
	Epoch 4325:	Loss 0.3924	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 4350:	Loss 0.3714	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0576	BestValid 0.2049
	Epoch 4375:	Loss 0.3620	TrainAcc 0.0701	ValidAcc 0.0590	TestAcc 0.0583	BestValid 0.2049
	Epoch 4400:	Loss 0.3568	TrainAcc 0.0706	ValidAcc 0.0602	TestAcc 0.0595	BestValid 0.2049
	Epoch 4425:	Loss 0.3538	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0576	BestValid 0.2049
	Epoch 4450:	Loss 0.3512	TrainAcc 0.0692	ValidAcc 0.0584	TestAcc 0.0576	BestValid 0.2049
	Epoch 4475:	Loss 0.3508	TrainAcc 0.0691	ValidAcc 0.0585	TestAcc 0.0575	BestValid 0.2049
	Epoch 4500:	Loss 0.3502	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 4525:	Loss 0.3516	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 4550:	Loss 0.3483	TrainAcc 0.0693	ValidAcc 0.0585	TestAcc 0.0577	BestValid 0.2049
	Epoch 4575:	Loss 0.3494	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 4600:	Loss 0.3508	TrainAcc 0.0692	ValidAcc 0.0585	TestAcc 0.0576	BestValid 0.2049
	Epoch 4625:	Loss 0.3612	TrainAcc 0.0699	ValidAcc 0.0598	TestAcc 0.0585	BestValid 0.2049
	Epoch 4650:	Loss 0.3536	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.2049
	Epoch 4675:	Loss 0.3609	TrainAcc 0.0694	ValidAcc 0.0586	TestAcc 0.0577	BestValid 0.2049
	Epoch 4700:	Loss 0.3560	TrainAcc 0.0692	ValidAcc 0.0586	TestAcc 0.0576	BestValid 0.2049
	Epoch 4725:	Loss 0.3532	TrainAcc 0.0694	ValidAcc 0.0588	TestAcc 0.0580	BestValid 0.2049
	Epoch 4750:	Loss 0.3514	TrainAcc 0.0725	ValidAcc 0.0611	TestAcc 0.0610	BestValid 0.2049
	Epoch 4775:	Loss 0.3459	TrainAcc 0.0715	ValidAcc 0.0602	TestAcc 0.0592	BestValid 0.2049
	Epoch 4800:	Loss 0.3450	TrainAcc 0.0735	ValidAcc 0.0621	TestAcc 0.0609	BestValid 0.2049
	Epoch 4825:	Loss 0.3452	TrainAcc 0.0717	ValidAcc 0.0608	TestAcc 0.0594	BestValid 0.2049
	Epoch 4850:	Loss 0.3435	TrainAcc 0.0713	ValidAcc 0.0599	TestAcc 0.0591	BestValid 0.2049
	Epoch 4875:	Loss 0.3437	TrainAcc 0.0702	ValidAcc 0.0593	TestAcc 0.0586	BestValid 0.2049
	Epoch 4900:	Loss 0.3424	TrainAcc 0.0698	ValidAcc 0.0589	TestAcc 0.0582	BestValid 0.2049
	Epoch 4925:	Loss 0.3421	TrainAcc 0.0708	ValidAcc 0.0598	TestAcc 0.0590	BestValid 0.2049
	Epoch 4950:	Loss 0.3434	TrainAcc 0.0708	ValidAcc 0.0598	TestAcc 0.0590	BestValid 0.2049
	Epoch 4975:	Loss 0.3416	TrainAcc 0.0720	ValidAcc 0.0614	TestAcc 0.0604	BestValid 0.2049
	Epoch 5000:	Loss 0.3404	TrainAcc 0.0714	ValidAcc 0.0606	TestAcc 0.0598	BestValid 0.2049
Node 2, Pre/Post-Pipelining: 1.107 / 1.156 ms, Bubble: 100.606 ms, Compute: 372.976 ms, Comm: 30.236 ms, Imbalance: 39.242 ms
Node 5, Pre/Post-Pipelining: 1.105 / 1.165 ms, Bubble: 102.208 ms, Compute: 379.098 ms, Comm: 31.164 ms, Imbalance: 30.510 ms
Node 4, Pre/Post-Pipelining: 1.104 / 1.168 ms, Bubble: 101.722 ms, Compute: 371.566 ms, Comm: 27.093 ms, Imbalance: 42.936 ms
Node 7, Pre/Post-Pipelining: 1.105 / 16.227 ms, Bubble: 88.651 ms, Compute: 393.048 ms, Comm: 17.358 ms, Imbalance: 28.620 ms
Node 1, Pre/Post-Pipelining: 1.110 / 1.114 ms, Bubble: 99.484 ms, Compute: 385.073 ms, Comm: 28.159 ms, Imbalance: 30.018 ms
Node 0, Pre/Post-Pipelining: 1.111 / 1.229 ms, Bubble: 98.309 ms, Compute: 419.327 ms, Comm: 16.875 ms, Imbalance: 7.472 ms
Node 3, Pre/Post-Pipelining: 1.107 / 1.149 ms, Bubble: 100.571 ms, Compute: 381.427 ms, Comm: 27.104 ms, Imbalance: 33.642 ms
Node 6, Pre/Post-Pipelining: 1.107 / 1.134 ms, Bubble: 102.791 ms, Compute: 380.835 ms, Comm: 29.516 ms, Imbalance: 29.810 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.111 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.229 ms
Cluster-Wide Average, Bubble: 98.309 ms
Cluster-Wide Average, Compute: 419.327 ms
Cluster-Wide Average, Communication: 16.875 ms
Cluster-Wide Average, Imbalance: 7.472 ms
Node 0, GPU memory consumption: 6.614 GB
Node 2, GPU memory consumption: 5.260 GB
Node 1, GPU memory consumption: 5.260 GB
Node 3, GPU memory consumption: 5.237 GB
Node 4, GPU memory consumption: 5.237 GB
Node 5, GPU memory consumption: 5.260 GB
Node 6, GPU memory consumption: 5.260 GB
Node 7, GPU memory consumption: 5.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 1.090309 s---------------
------------------------node id 1,  per-epoch time: 1.090309 s---------------
------------------------node id 2,  per-epoch time: 1.090309 s---------------
------------------------node id 3,  per-epoch time: 1.090309 s---------------
------------------------node id 4,  per-epoch time: 1.090309 s---------------
------------------------node id 5,  per-epoch time: 1.090309 s---------------
------------------------node id 6,  per-epoch time: 1.090309 s---------------
------------------------node id 7,  per-epoch time: 1.090309 s---------------
************ Profiling Results ************
	Bubble: 705.178681 (ms) (64.70 percentage)
	Compute: 380.107050 (ms) (34.88 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.580995 (ms) (0.42 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 402.376 Gbps
Highest valid_acc: 0.2049
Target test_acc: 0.2056
Epoch to reach the target acc: 74
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
