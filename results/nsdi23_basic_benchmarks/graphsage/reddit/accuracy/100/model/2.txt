Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.914 seconds.
Building the CSC structure...
        It takes 2.057 seconds.
Building the CSC structure...
        It takes 2.078 seconds.
Building the CSC structure...
        It takes 2.162 seconds.
Building the CSC structure...
        It takes 2.395 seconds.
Building the CSC structure...
        It takes 2.428 seconds.
Building the CSC structure...
        It takes 2.434 seconds.
Building the CSC structure...
        It takes 2.607 seconds.
Building the CSC structure...
        It takes 1.869 seconds.
        It takes 1.892 seconds.
        It takes 1.885 seconds.
        It takes 2.171 seconds.
        It takes 2.328 seconds.
Building the Feature Vector...
        It takes 2.396 seconds.
        It takes 2.418 seconds.
        It takes 2.381 seconds.
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.300 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.043 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
Building the Feature Vector...
        It takes 0.036 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.273 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.267 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.031 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.747 Gbps (per GPU), 453.973 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.470 Gbps (per GPU), 451.759 Gbps (aggregated)
The layer-level communication performance: 56.476 Gbps (per GPU), 451.809 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.249 Gbps (per GPU), 449.991 Gbps (aggregated)
The layer-level communication performance: 56.223 Gbps (per GPU), 449.781 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.045 Gbps (per GPU), 448.362 Gbps (aggregated)
The layer-level communication performance: 56.086 Gbps (per GPU), 448.688 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.965 Gbps (per GPU), 447.721 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.960 Gbps (per GPU), 1279.678 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.963 Gbps (per GPU), 1279.702 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.846 Gbps (per GPU), 1278.771 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.951 Gbps (per GPU), 1279.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.850 Gbps (per GPU), 1278.800 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.951 Gbps (per GPU), 1279.605 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.956 Gbps (per GPU), 1279.651 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.948 Gbps (per GPU), 1279.585 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.167 Gbps (per GPU), 833.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.167 Gbps (per GPU), 833.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.170 Gbps (per GPU), 833.361 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.167 Gbps (per GPU), 833.332 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.167 Gbps (per GPU), 833.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.164 Gbps (per GPU), 833.313 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.167 Gbps (per GPU), 833.333 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.162 Gbps (per GPU), 833.299 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.735 Gbps (per GPU), 277.880 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.735 Gbps (per GPU), 277.880 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.734 Gbps (per GPU), 277.871 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.728 Gbps (per GPU), 277.821 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.734 Gbps (per GPU), 277.872 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.727 Gbps (per GPU), 277.818 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.732 Gbps (per GPU), 277.855 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.733 Gbps (per GPU), 277.866 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.26ms  2.68ms  2.44ms  1.34  8.38K  3.53M
 chk_1  3.54ms  2.92ms  2.71ms  1.31  6.74K  3.60M
 chk_2  3.45ms  2.78ms  2.61ms  1.32  7.27K  3.53M
 chk_3  3.51ms  2.83ms  2.62ms  1.34  7.92K  3.61M
 chk_4  3.20ms  2.75ms  2.61ms  1.23  5.33K  3.68M
 chk_5  3.59ms  2.79ms  2.47ms  1.45 10.07K  3.45M
 chk_6  3.74ms  3.05ms  2.64ms  1.42  9.41K  3.48M
 chk_7  3.43ms  2.75ms  2.56ms  1.34  8.12K  3.60M
 chk_8  3.38ms  2.85ms  2.70ms  1.25  6.09K  3.64M
 chk_9  3.64ms  2.71ms  2.37ms  1.53 11.10K  3.38M
chk_10  3.39ms  2.90ms  2.73ms  1.24  5.67K  3.63M
chk_11  3.45ms  2.78ms  2.54ms  1.36  8.16K  3.54M
chk_12  3.62ms  2.95ms  2.75ms  1.32  7.24K  3.55M
chk_13  3.26ms  2.79ms  2.63ms  1.24  5.41K  3.68M
chk_14  3.70ms  3.02ms  2.83ms  1.31  7.14K  3.53M
chk_15  3.71ms  2.94ms  2.65ms  1.40  9.25K  3.49M
chk_16  3.17ms  2.73ms  2.60ms  1.22  4.78K  3.77M
chk_17  3.45ms  2.86ms  2.68ms  1.29  6.85K  3.60M
chk_18  3.33ms  2.68ms  2.47ms  1.35  7.47K  3.57M
chk_19  3.17ms  2.72ms  2.57ms  1.23  4.88K  3.75M
chk_20  3.38ms  2.74ms  2.54ms  1.33  7.00K  3.63M
chk_21  3.22ms  2.71ms  2.55ms  1.26  5.41K  3.68M
chk_22  3.95ms  2.96ms  2.61ms  1.51 11.07K  3.39M
chk_23  3.51ms  2.82ms  2.67ms  1.31  7.23K  3.64M
chk_24  3.75ms  2.93ms  2.63ms  1.43 10.13K  3.43M
chk_25  3.25ms  2.71ms  2.53ms  1.29  6.40K  3.57M
chk_26  3.42ms  2.89ms  2.73ms  1.25  5.78K  3.55M
chk_27  3.60ms  2.93ms  2.55ms  1.41  9.34K  3.48M
chk_28  3.66ms  3.09ms  2.88ms  1.27  6.37K  3.57M
chk_29  3.40ms  2.89ms  2.74ms  1.24  5.16K  3.78M
chk_30  3.25ms  2.76ms  2.66ms  1.22  5.44K  3.67M
chk_31  3.49ms  2.92ms  2.76ms  1.26  6.33K  3.63M
   Avg  3.46  2.84  2.63
   Max  3.95  3.09  2.88
   Min  3.17  2.68  2.37
 Ratio  1.25  1.15  1.21
   Var  0.04  0.01  0.01
Profiling takes 3.323 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 383.332 ms
Partition 0 [0, 4) has cost: 383.332 ms
Partition 1 [4, 8) has cost: 363.282 ms
Partition 2 [8, 12) has cost: 363.282 ms
Partition 3 [12, 16) has cost: 363.282 ms
Partition 4 [16, 20) has cost: 363.282 ms
Partition 5 [20, 24) has cost: 363.282 ms
Partition 6 [24, 28) has cost: 363.282 ms
Partition 7 [28, 32) has cost: 356.486 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 172.009 ms
GPU 0, Compute+Comm Time: 138.189 ms, Bubble Time: 30.216 ms, Imbalance Overhead: 3.604 ms
GPU 1, Compute+Comm Time: 132.274 ms, Bubble Time: 29.987 ms, Imbalance Overhead: 9.748 ms
GPU 2, Compute+Comm Time: 132.274 ms, Bubble Time: 29.946 ms, Imbalance Overhead: 9.789 ms
GPU 3, Compute+Comm Time: 132.274 ms, Bubble Time: 29.768 ms, Imbalance Overhead: 9.967 ms
GPU 4, Compute+Comm Time: 132.274 ms, Bubble Time: 29.740 ms, Imbalance Overhead: 9.995 ms
GPU 5, Compute+Comm Time: 132.274 ms, Bubble Time: 29.743 ms, Imbalance Overhead: 9.992 ms
GPU 6, Compute+Comm Time: 132.274 ms, Bubble Time: 29.973 ms, Imbalance Overhead: 9.762 ms
GPU 7, Compute+Comm Time: 130.054 ms, Bubble Time: 30.436 ms, Imbalance Overhead: 11.519 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 337.835 ms
GPU 0, Compute+Comm Time: 252.707 ms, Bubble Time: 59.835 ms, Imbalance Overhead: 25.293 ms
GPU 1, Compute+Comm Time: 257.282 ms, Bubble Time: 58.562 ms, Imbalance Overhead: 21.990 ms
GPU 2, Compute+Comm Time: 257.282 ms, Bubble Time: 58.080 ms, Imbalance Overhead: 22.473 ms
GPU 3, Compute+Comm Time: 257.282 ms, Bubble Time: 58.039 ms, Imbalance Overhead: 22.514 ms
GPU 4, Compute+Comm Time: 257.282 ms, Bubble Time: 57.970 ms, Imbalance Overhead: 22.583 ms
GPU 5, Compute+Comm Time: 257.282 ms, Bubble Time: 58.245 ms, Imbalance Overhead: 22.307 ms
GPU 6, Compute+Comm Time: 257.282 ms, Bubble Time: 58.235 ms, Imbalance Overhead: 22.318 ms
GPU 7, Compute+Comm Time: 271.417 ms, Bubble Time: 58.849 ms, Imbalance Overhead: 7.569 ms
The estimated cost of the whole pipeline: 535.336 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 746.614 ms
Partition 0 [0, 8) has cost: 746.614 ms
Partition 1 [8, 16) has cost: 726.564 ms
Partition 2 [16, 24) has cost: 726.564 ms
Partition 3 [24, 32) has cost: 719.768 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 178.049 ms
GPU 0, Compute+Comm Time: 147.482 ms, Bubble Time: 27.921 ms, Imbalance Overhead: 2.646 ms
GPU 1, Compute+Comm Time: 144.237 ms, Bubble Time: 27.374 ms, Imbalance Overhead: 6.437 ms
GPU 2, Compute+Comm Time: 144.237 ms, Bubble Time: 27.199 ms, Imbalance Overhead: 6.613 ms
GPU 3, Compute+Comm Time: 143.193 ms, Bubble Time: 26.874 ms, Imbalance Overhead: 7.982 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 337.763 ms
GPU 0, Compute+Comm Time: 269.699 ms, Bubble Time: 50.868 ms, Imbalance Overhead: 17.196 ms
GPU 1, Compute+Comm Time: 272.028 ms, Bubble Time: 51.172 ms, Imbalance Overhead: 14.562 ms
GPU 2, Compute+Comm Time: 272.028 ms, Bubble Time: 51.186 ms, Imbalance Overhead: 14.548 ms
GPU 3, Compute+Comm Time: 279.745 ms, Bubble Time: 52.275 ms, Imbalance Overhead: 5.743 ms
    The estimated cost with 2 DP ways is 541.602 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1473.177 ms
Partition 0 [0, 16) has cost: 1473.177 ms
Partition 1 [16, 32) has cost: 1446.332 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 222.534 ms
GPU 0, Compute+Comm Time: 194.199 ms, Bubble Time: 23.920 ms, Imbalance Overhead: 4.416 ms
GPU 1, Compute+Comm Time: 192.065 ms, Bubble Time: 24.632 ms, Imbalance Overhead: 5.838 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 373.829 ms
GPU 0, Compute+Comm Time: 323.018 ms, Bubble Time: 41.336 ms, Imbalance Overhead: 9.475 ms
GPU 1, Compute+Comm Time: 328.281 ms, Bubble Time: 40.076 ms, Imbalance Overhead: 5.471 ms
    The estimated cost with 4 DP ways is 626.181 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2919.509 ms
Partition 0 [0, 32) has cost: 2919.509 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 556.117 ms
GPU 0, Compute+Comm Time: 556.117 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 691.374 ms
GPU 0, Compute+Comm Time: 691.374 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1309.866 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2453	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 50:	Loss 3.0811	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
Node 0, Pre/Post-Pipelining: 1.109 / 1.154 ms, Bubble: 97.705 ms, Compute: 415.925 ms, Comm: 16.759 ms, Imbalance: 7.613 ms
Node 3, Pre/Post-Pipelining: 1.105 / 1.149 ms, Bubble: 99.734 ms, Compute: 382.214 ms, Comm: 26.935 ms, Imbalance: 29.716 ms
Node 4, Pre/Post-Pipelining: 1.105 / 1.083 ms, Bubble: 101.031 ms, Compute: 367.149 ms, Comm: 26.634 ms, Imbalance: 44.605 ms
Node 2, Pre/Post-Pipelining: 1.104 / 1.178 ms, Bubble: 99.861 ms, Compute: 372.390 ms, Comm: 30.203 ms, Imbalance: 36.553 ms
Node 7, Pre/Post-Pipelining: 1.101 / 16.198 ms, Bubble: 87.795 ms, Compute: 388.333 ms, Comm: 17.443 ms, Imbalance: 30.113 ms
Node 1, Pre/Post-Pipelining: 1.108 / 1.104 ms, Bubble: 98.758 ms, Compute: 381.621 ms, Comm: 28.143 ms, Imbalance: 30.160 ms
Node 6, Pre/Post-Pipelining: 1.108 / 1.084 ms, Bubble: 102.006 ms, Compute: 375.133 ms, Comm: 29.263 ms, Imbalance: 32.640 ms
Node 5, Pre/Post-Pipelining: 1.105 / 1.086 ms, Bubble: 101.472 ms, Compute: 373.343 ms, Comm: 30.959 ms, Imbalance: 33.298 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.109 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.154 ms
Cluster-Wide Average, Bubble: 97.705 ms
Cluster-Wide Average, Compute: 415.925 ms
Cluster-Wide Average, Communication: 16.759 ms
Cluster-Wide Average, Imbalance: 7.613 ms
Node 0, GPU memory consumption: 6.614 GB
Node 1, GPU memory consumption: 5.260 GB
Node 2, GPU memory consumption: 5.260 GB
Node 5, GPU memory consumption: 5.260 GB
Node 3, GPU memory consumption: 5.237 GB
Node 4, GPU memory consumption: 5.237 GB
Node 6, GPU memory consumption: 5.260 GB
Node 7, GPU memory consumption: 5.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 1.125773 s---------------
------------------------node id 1,  per-epoch time: 1.125779 s---------------
------------------------node id 4,  per-epoch time: 1.125770 s---------------
------------------------node id 2,  per-epoch time: 1.125775 s---------------
------------------------node id 5,  per-epoch time: 1.125776 s---------------
------------------------node id 3,  per-epoch time: 1.125773 s---------------
------------------------node id 6,  per-epoch time: 1.125775 s---------------
------------------------node id 7,  per-epoch time: 1.125772 s---------------
************ Profiling Results ************
	Bubble: 692.959381 (ms) (64.27 percentage)
	Compute: 380.751600 (ms) (35.31 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.506617 (ms) (0.42 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 404.646 Gbps
Highest valid_acc: 0.0584
Target test_acc: 0.0574
Epoch to reach the target acc: 24
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
