Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.053 seconds.
Building the CSC structure...
        It takes 2.083 seconds.
Building the CSC structure...
        It takes 2.253 seconds.
Building the CSC structure...
        It takes 2.357 seconds.
Building the CSC structure...
        It takes 2.402 seconds.
Building the CSC structure...
        It takes 2.438 seconds.
Building the CSC structure...
        It takes 2.661 seconds.
Building the CSC structure...
        It takes 2.675 seconds.
Building the CSC structure...
        It takes 1.879 seconds.
        It takes 1.893 seconds.
        It takes 2.402 seconds.
        It takes 2.305 seconds.
        It takes 2.327 seconds.
        It takes 2.338 seconds.
        It takes 2.382 seconds.
Building the Feature Vector...
        It takes 2.395 seconds.
Building the Feature Vector...
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.295 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.296 seconds.
Building the Label Vector...
        It takes 0.317 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.041 seconds.
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.040 seconds.
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.268 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.032 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.196 Gbps (per GPU), 473.572 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.921 Gbps (per GPU), 471.367 Gbps (aggregated)
The layer-level communication performance: 58.906 Gbps (per GPU), 471.251 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.667 Gbps (per GPU), 469.338 Gbps (aggregated)
The layer-level communication performance: 58.638 Gbps (per GPU), 469.100 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.430 Gbps (per GPU), 467.437 Gbps (aggregated)
The layer-level communication performance: 58.383 Gbps (per GPU), 467.064 Gbps (aggregated)
The layer-level communication performance: 58.355 Gbps (per GPU), 466.838 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.603 Gbps (per GPU), 1284.823 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.538 Gbps (per GPU), 1284.306 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.594 Gbps (per GPU), 1284.751 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.606 Gbps (per GPU), 1284.847 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.597 Gbps (per GPU), 1284.774 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.541 Gbps (per GPU), 1284.331 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.591 Gbps (per GPU), 1284.724 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.597 Gbps (per GPU), 1284.774 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.595 Gbps (per GPU), 828.764 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.595 Gbps (per GPU), 828.757 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.595 Gbps (per GPU), 828.764 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.595 Gbps (per GPU), 828.763 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.595 Gbps (per GPU), 828.764 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.595 Gbps (per GPU), 828.764 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.595 Gbps (per GPU), 828.757 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.595 Gbps (per GPU), 828.757 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 36.825 Gbps (per GPU), 294.598 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.824 Gbps (per GPU), 294.590 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.825 Gbps (per GPU), 294.599 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.824 Gbps (per GPU), 294.590 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.823 Gbps (per GPU), 294.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.823 Gbps (per GPU), 294.587 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.823 Gbps (per GPU), 294.581 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 36.823 Gbps (per GPU), 294.587 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.29ms  2.71ms  2.44ms  1.35  8.38K  3.53M
 chk_1  3.56ms  2.92ms  2.74ms  1.30  6.74K  3.60M
 chk_2  3.48ms  2.83ms  2.63ms  1.32  7.27K  3.53M
 chk_3  3.53ms  2.87ms  2.65ms  1.33  7.92K  3.61M
 chk_4  3.23ms  2.77ms  2.63ms  1.23  5.33K  3.68M
 chk_5  3.61ms  2.88ms  2.49ms  1.45 10.07K  3.45M
 chk_6  3.76ms  2.95ms  2.66ms  1.42  9.41K  3.48M
 chk_7  3.44ms  2.80ms  2.55ms  1.35  8.12K  3.60M
 chk_8  3.41ms  2.87ms  2.71ms  1.26  6.09K  3.64M
 chk_9  3.68ms  2.73ms  2.41ms  1.52 11.10K  3.38M
chk_10  3.41ms  2.90ms  2.76ms  1.23  5.67K  3.63M
chk_11  3.45ms  2.78ms  2.57ms  1.34  8.16K  3.54M
chk_12  3.62ms  2.98ms  2.78ms  1.30  7.24K  3.55M
chk_13  3.29ms  2.88ms  2.66ms  1.24  5.41K  3.68M
chk_14  3.69ms  3.05ms  2.85ms  1.30  7.14K  3.53M
chk_15  3.72ms  2.98ms  2.67ms  1.39  9.25K  3.49M
chk_16  3.17ms  2.78ms  2.63ms  1.21  4.78K  3.77M
chk_17  3.48ms  2.88ms  2.69ms  1.29  6.85K  3.60M
chk_18  3.35ms  2.69ms  2.49ms  1.35  7.47K  3.57M
chk_19  3.22ms  2.74ms  2.60ms  1.24  4.88K  3.75M
chk_20  3.40ms  2.78ms  2.57ms  1.32  7.00K  3.63M
chk_21  3.27ms  2.87ms  2.58ms  1.27  5.41K  3.68M
chk_22  3.95ms  2.99ms  2.63ms  1.50 11.07K  3.39M
chk_23  3.52ms  2.85ms  2.65ms  1.33  7.23K  3.64M
chk_24  3.80ms  2.94ms  2.64ms  1.44 10.13K  3.43M
chk_25  3.27ms  2.72ms  2.55ms  1.29  6.40K  3.57M
chk_26  3.45ms  2.92ms  2.75ms  1.26  5.78K  3.55M
chk_27  3.62ms  2.87ms  2.55ms  1.42  9.34K  3.48M
chk_28  3.67ms  3.09ms  2.89ms  1.27  6.37K  3.57M
chk_29  3.39ms  2.97ms  2.74ms  1.24  5.16K  3.78M
chk_30  3.26ms  2.80ms  2.69ms  1.21  5.44K  3.67M
chk_31  3.51ms  2.95ms  2.78ms  1.26  6.33K  3.63M
   Avg  3.48  2.87  2.64
   Max  3.95  3.09  2.89
   Min  3.17  2.69  2.41
 Ratio  1.24  1.15  1.20
   Var  0.04  0.01  0.01
Profiling takes 3.347 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 386.702 ms
Partition 0 [0, 4) has cost: 386.702 ms
Partition 1 [4, 8) has cost: 366.917 ms
Partition 2 [8, 12) has cost: 366.917 ms
Partition 3 [12, 16) has cost: 366.917 ms
Partition 4 [16, 20) has cost: 366.917 ms
Partition 5 [20, 24) has cost: 366.917 ms
Partition 6 [24, 28) has cost: 366.917 ms
Partition 7 [28, 32) has cost: 359.801 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 172.406 ms
GPU 0, Compute+Comm Time: 138.685 ms, Bubble Time: 30.273 ms, Imbalance Overhead: 3.448 ms
GPU 1, Compute+Comm Time: 132.789 ms, Bubble Time: 30.015 ms, Imbalance Overhead: 9.602 ms
GPU 2, Compute+Comm Time: 132.789 ms, Bubble Time: 29.970 ms, Imbalance Overhead: 9.646 ms
GPU 3, Compute+Comm Time: 132.789 ms, Bubble Time: 29.835 ms, Imbalance Overhead: 9.781 ms
GPU 4, Compute+Comm Time: 132.789 ms, Bubble Time: 29.849 ms, Imbalance Overhead: 9.767 ms
GPU 5, Compute+Comm Time: 132.789 ms, Bubble Time: 29.809 ms, Imbalance Overhead: 9.807 ms
GPU 6, Compute+Comm Time: 132.789 ms, Bubble Time: 30.004 ms, Imbalance Overhead: 9.612 ms
GPU 7, Compute+Comm Time: 130.540 ms, Bubble Time: 30.423 ms, Imbalance Overhead: 11.442 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 336.278 ms
GPU 0, Compute+Comm Time: 254.448 ms, Bubble Time: 60.067 ms, Imbalance Overhead: 21.764 ms
GPU 1, Compute+Comm Time: 259.315 ms, Bubble Time: 59.162 ms, Imbalance Overhead: 17.802 ms
GPU 2, Compute+Comm Time: 259.315 ms, Bubble Time: 58.496 ms, Imbalance Overhead: 18.468 ms
GPU 3, Compute+Comm Time: 259.315 ms, Bubble Time: 58.515 ms, Imbalance Overhead: 18.449 ms
GPU 4, Compute+Comm Time: 259.315 ms, Bubble Time: 58.369 ms, Imbalance Overhead: 18.595 ms
GPU 5, Compute+Comm Time: 259.315 ms, Bubble Time: 58.542 ms, Imbalance Overhead: 18.422 ms
GPU 6, Compute+Comm Time: 259.315 ms, Bubble Time: 58.512 ms, Imbalance Overhead: 18.452 ms
GPU 7, Compute+Comm Time: 273.204 ms, Bubble Time: 58.989 ms, Imbalance Overhead: 4.086 ms
The estimated cost of the whole pipeline: 534.118 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 753.619 ms
Partition 0 [0, 8) has cost: 753.619 ms
Partition 1 [8, 16) has cost: 733.834 ms
Partition 2 [16, 24) has cost: 733.834 ms
Partition 3 [24, 32) has cost: 726.718 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 178.641 ms
GPU 0, Compute+Comm Time: 147.943 ms, Bubble Time: 28.151 ms, Imbalance Overhead: 2.547 ms
GPU 1, Compute+Comm Time: 144.718 ms, Bubble Time: 27.618 ms, Imbalance Overhead: 6.306 ms
GPU 2, Compute+Comm Time: 144.718 ms, Bubble Time: 27.323 ms, Imbalance Overhead: 6.600 ms
GPU 3, Compute+Comm Time: 143.644 ms, Bubble Time: 26.971 ms, Imbalance Overhead: 8.026 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 336.207 ms
GPU 0, Compute+Comm Time: 270.719 ms, Bubble Time: 51.105 ms, Imbalance Overhead: 14.383 ms
GPU 1, Compute+Comm Time: 273.092 ms, Bubble Time: 51.538 ms, Imbalance Overhead: 11.577 ms
GPU 2, Compute+Comm Time: 273.092 ms, Bubble Time: 51.740 ms, Imbalance Overhead: 11.376 ms
GPU 3, Compute+Comm Time: 280.793 ms, Bubble Time: 52.754 ms, Imbalance Overhead: 2.660 ms
    The estimated cost with 2 DP ways is 540.591 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1487.453 ms
Partition 0 [0, 16) has cost: 1487.453 ms
Partition 1 [16, 32) has cost: 1460.552 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 223.271 ms
GPU 0, Compute+Comm Time: 194.761 ms, Bubble Time: 24.114 ms, Imbalance Overhead: 4.396 ms
GPU 1, Compute+Comm Time: 192.620 ms, Bubble Time: 24.681 ms, Imbalance Overhead: 5.971 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 372.793 ms
GPU 0, Compute+Comm Time: 322.972 ms, Bubble Time: 41.351 ms, Imbalance Overhead: 8.470 ms
GPU 1, Compute+Comm Time: 328.247 ms, Bubble Time: 40.491 ms, Imbalance Overhead: 4.055 ms
    The estimated cost with 4 DP ways is 625.867 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2948.006 ms
Partition 0 [0, 32) has cost: 2948.006 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 532.734 ms
GPU 0, Compute+Comm Time: 532.734 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 667.330 ms
GPU 0, Compute+Comm Time: 667.330 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1260.068 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.3316	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 50:	Loss 3.1249	TrainAcc 0.0690	ValidAcc 0.0585	TestAcc 0.0574	BestValid 0.0585
Node 6, Pre/Post-Pipelining: 1.107 / 1.130 ms, Bubble: 102.155 ms, Compute: 378.350 ms, Comm: 29.399 ms, Imbalance: 30.418 ms
Node 4, Pre/Post-Pipelining: 1.109 / 1.082 ms, Bubble: 101.345 ms, Compute: 366.558 ms, Comm: 26.661 ms, Imbalance: 46.323 ms
Node 3, Pre/Post-Pipelining: 1.106 / 1.168 ms, Bubble: 99.973 ms, Compute: 382.151 ms, Comm: 26.870 ms, Imbalance: 31.030 ms
Node 1, Pre/Post-Pipelining: 1.111 / 1.108 ms, Bubble: 98.986 ms, Compute: 382.604 ms, Comm: 28.026 ms, Imbalance: 30.519 ms
Node 7, Pre/Post-Pipelining: 1.114 / 16.277 ms, Bubble: 88.071 ms, Compute: 389.805 ms, Comm: 17.581 ms, Imbalance: 29.578 ms
Node 0, Pre/Post-Pipelining: 1.113 / 1.174 ms, Bubble: 97.803 ms, Compute: 417.133 ms, Comm: 16.785 ms, Imbalance: 7.735 ms
Node 2, Pre/Post-Pipelining: 1.105 / 1.166 ms, Bubble: 100.033 ms, Compute: 372.528 ms, Comm: 30.074 ms, Imbalance: 37.767 ms
Node 5, Pre/Post-Pipelining: 1.107 / 1.085 ms, Bubble: 101.675 ms, Compute: 374.505 ms, Comm: 31.146 ms, Imbalance: 33.109 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.113 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.174 ms
Cluster-Wide Average, Bubble: 97.803 ms
Cluster-Wide Average, Compute: 417.133 ms
Cluster-Wide Average, Communication: 16.785 ms
Cluster-Wide Average, Imbalance: 7.735 ms
Node 0, GPU memory consumption: 6.614 GB
Node 1, GPU memory consumption: 5.260 GB
Node 2, GPU memory consumption: 5.260 GB
Node 3, GPU memory consumption: 5.237 GB
Node 6, GPU memory consumption: 5.260 GB
Node 7, GPU memory consumption: 5.042 GB
Node 4, GPU memory consumption: 5.237 GB
Node 5, GPU memory consumption: 5.260 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 1.125144 s---------------
------------------------node id 1,  per-epoch time: 1.125145 s---------------
------------------------node id 2,  per-epoch time: 1.125145 s---------------
------------------------node id 3,  per-epoch time: 1.125140 s---------------
------------------------node id 4,  per-epoch time: 1.125135 s---------------
------------------------node id 5,  per-epoch time: 1.125143 s---------------
------------------------node id 6,  per-epoch time: 1.125131 s---------------
------------------------node id 7,  per-epoch time: 1.125139 s---------------
************ Profiling Results ************
	Bubble: 691.329530 (ms) (64.16 percentage)
	Compute: 381.582144 (ms) (35.42 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.540854 (ms) (0.42 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 404.249 Gbps
Highest valid_acc: 0.0585
Target test_acc: 0.0574
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
