Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.873 seconds.
Building the CSC structure...
        It takes 1.917 seconds.
Building the CSC structure...
        It takes 2.059 seconds.
Building the CSC structure...
        It takes 2.073 seconds.
Building the CSC structure...
        It takes 2.414 seconds.
Building the CSC structure...
        It takes 2.416 seconds.
Building the CSC structure...
        It takes 2.546 seconds.
Building the CSC structure...
        It takes 2.644 seconds.
Building the CSC structure...
        It takes 1.811 seconds.
        It takes 1.842 seconds.
        It takes 1.835 seconds.
        It takes 1.961 seconds.
Building the Feature Vector...
        It takes 2.312 seconds.
Building the Feature Vector...
        It takes 2.407 seconds.
        It takes 2.308 seconds.
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 2.386 seconds.
        It takes 0.305 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.278 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.046 seconds.
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.289 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.040 seconds.
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.252 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.030 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.928 Gbps (per GPU), 455.426 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.674 Gbps (per GPU), 453.392 Gbps (aggregated)
The layer-level communication performance: 56.672 Gbps (per GPU), 453.377 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.447 Gbps (per GPU), 451.580 Gbps (aggregated)
The layer-level communication performance: 56.417 Gbps (per GPU), 451.333 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.225 Gbps (per GPU), 449.803 Gbps (aggregated)
The layer-level communication performance: 56.180 Gbps (per GPU), 449.441 Gbps (aggregated)
The layer-level communication performance: 56.153 Gbps (per GPU), 449.224 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.659 Gbps (per GPU), 1245.270 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.844 Gbps (per GPU), 1246.755 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.664 Gbps (per GPU), 1245.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.708 Gbps (per GPU), 1245.663 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.665 Gbps (per GPU), 1245.319 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.665 Gbps (per GPU), 1245.317 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.682 Gbps (per GPU), 1245.454 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.668 Gbps (per GPU), 1245.343 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.304 Gbps (per GPU), 826.430 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.305 Gbps (per GPU), 826.444 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.305 Gbps (per GPU), 826.437 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.305 Gbps (per GPU), 826.443 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.305 Gbps (per GPU), 826.443 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.305 Gbps (per GPU), 826.437 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.305 Gbps (per GPU), 826.437 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.301 Gbps (per GPU), 826.410 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.902 Gbps (per GPU), 279.218 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.902 Gbps (per GPU), 279.217 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.901 Gbps (per GPU), 279.205 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.901 Gbps (per GPU), 279.209 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.902 Gbps (per GPU), 279.217 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.901 Gbps (per GPU), 279.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.901 Gbps (per GPU), 279.208 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.901 Gbps (per GPU), 279.209 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.31ms  2.72ms  2.45ms  1.35  8.38K  3.53M
 chk_1  3.58ms  2.94ms  2.76ms  1.30  6.74K  3.60M
 chk_2  3.52ms  2.84ms  2.64ms  1.33  7.27K  3.53M
 chk_3  3.55ms  2.88ms  2.66ms  1.33  7.92K  3.61M
 chk_4  3.27ms  2.78ms  2.66ms  1.23  5.33K  3.68M
 chk_5  3.64ms  2.82ms  2.53ms  1.44 10.07K  3.45M
 chk_6  3.77ms  3.08ms  2.68ms  1.41  9.41K  3.48M
 chk_7  3.46ms  2.80ms  2.59ms  1.34  8.12K  3.60M
 chk_8  3.44ms  2.90ms  2.73ms  1.26  6.09K  3.64M
 chk_9  3.66ms  2.76ms  2.41ms  1.52 11.10K  3.38M
chk_10  3.42ms  2.93ms  2.79ms  1.23  5.67K  3.63M
chk_11  3.48ms  2.82ms  2.59ms  1.34  8.16K  3.54M
chk_12  3.66ms  3.00ms  2.81ms  1.30  7.24K  3.55M
chk_13  3.32ms  2.84ms  2.69ms  1.24  5.41K  3.68M
chk_14  3.72ms  3.06ms  2.86ms  1.30  7.14K  3.53M
chk_15  3.75ms  2.97ms  2.68ms  1.40  9.25K  3.49M
chk_16  3.22ms  2.79ms  2.63ms  1.22  4.78K  3.77M
chk_17  3.49ms  2.90ms  2.69ms  1.30  6.85K  3.60M
chk_18  3.38ms  2.72ms  2.50ms  1.35  7.47K  3.57M
chk_19  3.21ms  2.77ms  2.62ms  1.23  4.88K  3.75M
chk_20  3.39ms  2.81ms  2.58ms  1.32  7.00K  3.63M
chk_21  3.26ms  2.76ms  2.59ms  1.26  5.41K  3.68M
chk_22  3.99ms  3.01ms  2.64ms  1.51 11.07K  3.39M
chk_23  3.56ms  2.86ms  2.69ms  1.32  7.23K  3.64M
chk_24  3.81ms  2.96ms  2.67ms  1.43 10.13K  3.43M
chk_25  3.33ms  2.75ms  2.58ms  1.29  6.40K  3.57M
chk_26  3.47ms  2.93ms  2.78ms  1.25  5.78K  3.55M
chk_27  3.63ms  2.89ms  2.58ms  1.41  9.34K  3.48M
chk_28  3.68ms  3.12ms  2.91ms  1.26  6.37K  3.57M
chk_29  3.40ms  2.92ms  2.79ms  1.22  5.16K  3.78M
chk_30  3.30ms  2.81ms  2.69ms  1.23  5.44K  3.67M
chk_31  3.52ms  2.97ms  2.79ms  1.26  6.33K  3.63M
   Avg  3.51  2.88  2.66
   Max  3.99  3.12  2.91
   Min  3.21  2.72  2.41
 Ratio  1.24  1.15  1.21
   Var  0.03  0.01  0.01
Profiling takes 3.367 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 388.519 ms
Partition 0 [0, 4) has cost: 388.519 ms
Partition 1 [4, 8) has cost: 368.433 ms
Partition 2 [8, 12) has cost: 368.433 ms
Partition 3 [12, 16) has cost: 368.433 ms
Partition 4 [16, 20) has cost: 368.433 ms
Partition 5 [20, 24) has cost: 368.433 ms
Partition 6 [24, 28) has cost: 368.433 ms
Partition 7 [28, 32) has cost: 361.579 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 176.843 ms
GPU 0, Compute+Comm Time: 140.190 ms, Bubble Time: 30.549 ms, Imbalance Overhead: 6.104 ms
GPU 1, Compute+Comm Time: 134.393 ms, Bubble Time: 30.275 ms, Imbalance Overhead: 12.175 ms
GPU 2, Compute+Comm Time: 134.393 ms, Bubble Time: 30.229 ms, Imbalance Overhead: 12.221 ms
GPU 3, Compute+Comm Time: 134.393 ms, Bubble Time: 30.094 ms, Imbalance Overhead: 12.356 ms
GPU 4, Compute+Comm Time: 134.393 ms, Bubble Time: 30.100 ms, Imbalance Overhead: 12.350 ms
GPU 5, Compute+Comm Time: 134.393 ms, Bubble Time: 30.062 ms, Imbalance Overhead: 12.388 ms
GPU 6, Compute+Comm Time: 134.393 ms, Bubble Time: 30.269 ms, Imbalance Overhead: 12.181 ms
GPU 7, Compute+Comm Time: 132.055 ms, Bubble Time: 31.044 ms, Imbalance Overhead: 13.744 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 338.667 ms
GPU 0, Compute+Comm Time: 255.714 ms, Bubble Time: 60.279 ms, Imbalance Overhead: 22.674 ms
GPU 1, Compute+Comm Time: 260.230 ms, Bubble Time: 59.385 ms, Imbalance Overhead: 19.052 ms
GPU 2, Compute+Comm Time: 260.230 ms, Bubble Time: 58.946 ms, Imbalance Overhead: 19.491 ms
GPU 3, Compute+Comm Time: 260.230 ms, Bubble Time: 58.989 ms, Imbalance Overhead: 19.448 ms
GPU 4, Compute+Comm Time: 260.230 ms, Bubble Time: 58.880 ms, Imbalance Overhead: 19.556 ms
GPU 5, Compute+Comm Time: 260.230 ms, Bubble Time: 59.076 ms, Imbalance Overhead: 19.361 ms
GPU 6, Compute+Comm Time: 260.230 ms, Bubble Time: 59.056 ms, Imbalance Overhead: 19.381 ms
GPU 7, Compute+Comm Time: 274.520 ms, Bubble Time: 59.553 ms, Imbalance Overhead: 4.594 ms
The estimated cost of the whole pipeline: 541.286 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 756.952 ms
Partition 0 [0, 8) has cost: 756.952 ms
Partition 1 [8, 16) has cost: 736.865 ms
Partition 2 [16, 24) has cost: 736.865 ms
Partition 3 [24, 32) has cost: 730.011 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 183.258 ms
GPU 0, Compute+Comm Time: 150.048 ms, Bubble Time: 28.283 ms, Imbalance Overhead: 4.927 ms
GPU 1, Compute+Comm Time: 146.930 ms, Bubble Time: 27.775 ms, Imbalance Overhead: 8.553 ms
GPU 2, Compute+Comm Time: 146.930 ms, Bubble Time: 27.549 ms, Imbalance Overhead: 8.779 ms
GPU 3, Compute+Comm Time: 145.787 ms, Bubble Time: 27.254 ms, Imbalance Overhead: 10.218 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 338.328 ms
GPU 0, Compute+Comm Time: 272.195 ms, Bubble Time: 51.541 ms, Imbalance Overhead: 14.592 ms
GPU 1, Compute+Comm Time: 274.368 ms, Bubble Time: 51.845 ms, Imbalance Overhead: 12.115 ms
GPU 2, Compute+Comm Time: 274.368 ms, Bubble Time: 51.867 ms, Imbalance Overhead: 12.093 ms
GPU 3, Compute+Comm Time: 282.243 ms, Bubble Time: 52.796 ms, Imbalance Overhead: 3.290 ms
    The estimated cost with 2 DP ways is 547.666 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1493.818 ms
Partition 0 [0, 16) has cost: 1493.818 ms
Partition 1 [16, 32) has cost: 1466.877 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 226.944 ms
GPU 0, Compute+Comm Time: 197.166 ms, Bubble Time: 24.175 ms, Imbalance Overhead: 5.604 ms
GPU 1, Compute+Comm Time: 195.024 ms, Bubble Time: 24.874 ms, Imbalance Overhead: 7.046 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 374.097 ms
GPU 0, Compute+Comm Time: 324.063 ms, Bubble Time: 41.755 ms, Imbalance Overhead: 8.278 ms
GPU 1, Compute+Comm Time: 329.326 ms, Bubble Time: 40.423 ms, Imbalance Overhead: 4.348 ms
    The estimated cost with 4 DP ways is 631.093 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2960.694 ms
Partition 0 [0, 32) has cost: 2960.694 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 557.767 ms
GPU 0, Compute+Comm Time: 557.767 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 689.126 ms
GPU 0, Compute+Comm Time: 689.126 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1309.238 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 0 initializing the weights for op[0, 37)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.3318	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 50:	Loss 3.1255	TrainAcc 0.0690	ValidAcc 0.0585	TestAcc 0.0574	BestValid 0.0585
	Epoch 75:	Loss 2.8954	TrainAcc 0.0747	ValidAcc 0.0651	TestAcc 0.0644	BestValid 0.0651
	Epoch 100:	Loss 2.7623	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0651
	Epoch 125:	Loss 2.6832	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0651
	Epoch 150:	Loss 2.6272	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0651
	Epoch 175:	Loss 2.5640	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0651
	Epoch 200:	Loss 2.3770	TrainAcc 0.0867	ValidAcc 0.0802	TestAcc 0.0772	BestValid 0.0802
	Epoch 225:	Loss 2.2074	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0802
	Epoch 250:	Loss 2.1048	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0802
	Epoch 275:	Loss 1.9760	TrainAcc 0.0882	ValidAcc 0.0730	TestAcc 0.0718	BestValid 0.0802
	Epoch 300:	Loss 1.8423	TrainAcc 0.0886	ValidAcc 0.0740	TestAcc 0.0725	BestValid 0.0802
	Epoch 325:	Loss 1.6361	TrainAcc 0.0717	ValidAcc 0.0592	TestAcc 0.0586	BestValid 0.0802
	Epoch 350:	Loss 1.5190	TrainAcc 0.0886	ValidAcc 0.0739	TestAcc 0.0727	BestValid 0.0802
	Epoch 375:	Loss 1.4233	TrainAcc 0.0847	ValidAcc 0.0689	TestAcc 0.0678	BestValid 0.0802
	Epoch 400:	Loss 1.3303	TrainAcc 0.0889	ValidAcc 0.0742	TestAcc 0.0725	BestValid 0.0802
	Epoch 425:	Loss 1.2313	TrainAcc 0.0861	ValidAcc 0.0705	TestAcc 0.0690	BestValid 0.0802
	Epoch 450:	Loss 1.1635	TrainAcc 0.0934	ValidAcc 0.0773	TestAcc 0.0754	BestValid 0.0802
	Epoch 475:	Loss 1.0821	TrainAcc 0.0872	ValidAcc 0.0718	TestAcc 0.0706	BestValid 0.0802
	Epoch 500:	Loss 1.0301	TrainAcc 0.1043	ValidAcc 0.0880	TestAcc 0.0845	BestValid 0.0880
	Epoch 525:	Loss 0.9633	TrainAcc 0.0990	ValidAcc 0.0826	TestAcc 0.0808	BestValid 0.0880
	Epoch 550:	Loss 0.9188	TrainAcc 0.1056	ValidAcc 0.0858	TestAcc 0.0836	BestValid 0.0880
	Epoch 575:	Loss 0.8739	TrainAcc 0.1172	ValidAcc 0.0997	TestAcc 0.0965	BestValid 0.0997
	Epoch 600:	Loss 0.8446	TrainAcc 0.1127	ValidAcc 0.0960	TestAcc 0.0915	BestValid 0.0997
	Epoch 625:	Loss 0.8154	TrainAcc 0.1017	ValidAcc 0.0861	TestAcc 0.0836	BestValid 0.0997
	Epoch 650:	Loss 0.7842	TrainAcc 0.0399	ValidAcc 0.0350	TestAcc 0.0321	BestValid 0.0997
	Epoch 675:	Loss 0.7560	TrainAcc 0.0152	ValidAcc 0.0117	TestAcc 0.0101	BestValid 0.0997
	Epoch 700:	Loss 0.7332	TrainAcc 0.0504	ValidAcc 0.0493	TestAcc 0.0483	BestValid 0.0997
	Epoch 725:	Loss 0.7043	TrainAcc 0.0425	ValidAcc 0.0413	TestAcc 0.0416	BestValid 0.0997
	Epoch 750:	Loss 0.6926	TrainAcc 0.0598	ValidAcc 0.0584	TestAcc 0.0589	BestValid 0.0997
	Epoch 775:	Loss 0.6732	TrainAcc 0.0493	ValidAcc 0.0444	TestAcc 0.0421	BestValid 0.0997
	Epoch 800:	Loss 0.6593	TrainAcc 0.0967	ValidAcc 0.0833	TestAcc 0.0820	BestValid 0.0997
	Epoch 825:	Loss 0.6476	TrainAcc 0.0859	ValidAcc 0.0702	TestAcc 0.0689	BestValid 0.0997
	Epoch 850:	Loss 0.6372	TrainAcc 0.0694	ValidAcc 0.0587	TestAcc 0.0577	BestValid 0.0997
	Epoch 875:	Loss 0.6168	TrainAcc 0.0692	ValidAcc 0.0587	TestAcc 0.0577	BestValid 0.0997
	Epoch 900:	Loss 0.6077	TrainAcc 0.0180	ValidAcc 0.0144	TestAcc 0.0142	BestValid 0.0997
	Epoch 925:	Loss 0.5979	TrainAcc 0.0181	ValidAcc 0.0144	TestAcc 0.0142	BestValid 0.0997
	Epoch 950:	Loss 0.5874	TrainAcc 0.0110	ValidAcc 0.0089	TestAcc 0.0086	BestValid 0.0997
	Epoch 975:	Loss 0.5729	TrainAcc 0.0027	ValidAcc 0.0031	TestAcc 0.0021	BestValid 0.0997
	Epoch 1000:	Loss 0.5635	TrainAcc 0.0056	ValidAcc 0.0031	TestAcc 0.0032	BestValid 0.0997
	Epoch 1025:	Loss 0.5648	TrainAcc 0.0074	ValidAcc 0.0039	TestAcc 0.0042	BestValid 0.0997
	Epoch 1050:	Loss 0.5485	TrainAcc 0.0249	ValidAcc 0.0156	TestAcc 0.0157	BestValid 0.0997
	Epoch 1075:	Loss 0.5344	TrainAcc 0.0208	ValidAcc 0.0133	TestAcc 0.0134	BestValid 0.0997
	Epoch 1100:	Loss 0.5286	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.0997
	Epoch 1125:	Loss 0.5196	TrainAcc 0.0249	ValidAcc 0.0154	TestAcc 0.0157	BestValid 0.0997
	Epoch 1150:	Loss 0.5085	TrainAcc 0.0253	ValidAcc 0.0157	TestAcc 0.0159	BestValid 0.0997
	Epoch 1175:	Loss 0.5020	TrainAcc 0.0249	ValidAcc 0.0154	TestAcc 0.0159	BestValid 0.0997
	Epoch 1200:	Loss 0.4979	TrainAcc 0.0257	ValidAcc 0.0161	TestAcc 0.0163	BestValid 0.0997
	Epoch 1225:	Loss 0.4853	TrainAcc 0.0251	ValidAcc 0.0155	TestAcc 0.0158	BestValid 0.0997
	Epoch 1250:	Loss 0.4834	TrainAcc 0.0247	ValidAcc 0.0155	TestAcc 0.0157	BestValid 0.0997
	Epoch 1275:	Loss 0.4763	TrainAcc 0.0420	ValidAcc 0.0329	TestAcc 0.0310	BestValid 0.0997
	Epoch 1300:	Loss 0.4678	TrainAcc 0.0424	ValidAcc 0.0332	TestAcc 0.0310	BestValid 0.0997
	Epoch 1325:	Loss 0.4701	TrainAcc 0.0408	ValidAcc 0.0322	TestAcc 0.0304	BestValid 0.0997
	Epoch 1350:	Loss 0.4590	TrainAcc 0.0425	ValidAcc 0.0333	TestAcc 0.0312	BestValid 0.0997
	Epoch 1375:	Loss 0.4598	TrainAcc 0.0192	ValidAcc 0.0186	TestAcc 0.0164	BestValid 0.0997
	Epoch 1400:	Loss 0.4512	TrainAcc 0.0207	ValidAcc 0.0193	TestAcc 0.0171	BestValid 0.0997
	Epoch 1425:	Loss 0.4514	TrainAcc 0.0389	ValidAcc 0.0312	TestAcc 0.0292	BestValid 0.0997
	Epoch 1450:	Loss 0.4431	TrainAcc 0.0231	ValidAcc 0.0210	TestAcc 0.0192	BestValid 0.0997
	Epoch 1475:	Loss 0.4424	TrainAcc 0.0241	ValidAcc 0.0219	TestAcc 0.0200	BestValid 0.0997
	Epoch 1500:	Loss 0.4377	TrainAcc 0.0195	ValidAcc 0.0180	TestAcc 0.0162	BestValid 0.0997
	Epoch 1525:	Loss 0.4450	TrainAcc 0.0186	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.0997
	Epoch 1550:	Loss 0.4358	TrainAcc 0.0220	ValidAcc 0.0211	TestAcc 0.0185	BestValid 0.0997
	Epoch 1575:	Loss 0.4312	TrainAcc 0.0102	ValidAcc 0.0097	TestAcc 0.0093	BestValid 0.0997
	Epoch 1600:	Loss 0.4275	TrainAcc 0.0182	ValidAcc 0.0172	TestAcc 0.0155	BestValid 0.0997
	Epoch 1625:	Loss 0.4324	TrainAcc 0.0111	ValidAcc 0.0104	TestAcc 0.0101	BestValid 0.0997
	Epoch 1650:	Loss 0.4237	TrainAcc 0.0171	ValidAcc 0.0159	TestAcc 0.0150	BestValid 0.0997
	Epoch 1675:	Loss 0.4231	TrainAcc 0.0270	ValidAcc 0.0259	TestAcc 0.0234	BestValid 0.0997
	Epoch 1700:	Loss 0.4222	TrainAcc 0.0244	ValidAcc 0.0228	TestAcc 0.0211	BestValid 0.0997
	Epoch 1725:	Loss 0.4193	TrainAcc 0.0228	ValidAcc 0.0216	TestAcc 0.0193	BestValid 0.0997
	Epoch 1750:	Loss 0.4181	TrainAcc 0.0111	ValidAcc 0.0104	TestAcc 0.0099	BestValid 0.0997
	Epoch 1775:	Loss 0.4165	TrainAcc 0.0104	ValidAcc 0.0097	TestAcc 0.0092	BestValid 0.0997
	Epoch 1800:	Loss 0.4142	TrainAcc 0.0590	ValidAcc 0.0490	TestAcc 0.0471	BestValid 0.0997
	Epoch 1825:	Loss 0.4562	TrainAcc 0.1125	ValidAcc 0.0966	TestAcc 0.0934	BestValid 0.0997
	Epoch 1850:	Loss 0.4241	TrainAcc 0.0102	ValidAcc 0.0097	TestAcc 0.0093	BestValid 0.0997
	Epoch 1875:	Loss 0.4490	TrainAcc 0.0433	ValidAcc 0.0468	TestAcc 0.0441	BestValid 0.0997
	Epoch 1900:	Loss 0.4347	TrainAcc 0.0249	ValidAcc 0.0241	TestAcc 0.0229	BestValid 0.0997
	Epoch 1925:	Loss 0.4698	TrainAcc 0.0514	ValidAcc 0.0433	TestAcc 0.0408	BestValid 0.0997
	Epoch 1950:	Loss 0.4399	TrainAcc 0.0397	ValidAcc 0.0376	TestAcc 0.0379	BestValid 0.0997
	Epoch 1975:	Loss 0.4253	TrainAcc 0.0104	ValidAcc 0.0098	TestAcc 0.0097	BestValid 0.0997
	Epoch 2000:	Loss 0.4204	TrainAcc 0.0311	ValidAcc 0.0217	TestAcc 0.0215	BestValid 0.0997
	Epoch 2025:	Loss 0.4220	TrainAcc 0.0429	ValidAcc 0.0354	TestAcc 0.0349	BestValid 0.0997
	Epoch 2050:	Loss 0.4111	TrainAcc 0.0531	ValidAcc 0.0431	TestAcc 0.0413	BestValid 0.0997
	Epoch 2075:	Loss 0.4110	TrainAcc 0.0127	ValidAcc 0.0125	TestAcc 0.0122	BestValid 0.0997
	Epoch 2100:	Loss 0.4085	TrainAcc 0.0086	ValidAcc 0.0087	TestAcc 0.0091	BestValid 0.0997
	Epoch 2125:	Loss 0.4097	TrainAcc 0.0072	ValidAcc 0.0076	TestAcc 0.0075	BestValid 0.0997
	Epoch 2150:	Loss 0.4041	TrainAcc 0.0106	ValidAcc 0.0100	TestAcc 0.0098	BestValid 0.0997
	Epoch 2175:	Loss 0.4080	TrainAcc 0.0118	ValidAcc 0.0115	TestAcc 0.0112	BestValid 0.0997
	Epoch 2200:	Loss 0.3981	TrainAcc 0.0108	ValidAcc 0.0101	TestAcc 0.0100	BestValid 0.0997
	Epoch 2225:	Loss 0.4047	TrainAcc 0.0122	ValidAcc 0.0117	TestAcc 0.0116	BestValid 0.0997
	Epoch 2250:	Loss 0.4000	TrainAcc 0.0126	ValidAcc 0.0120	TestAcc 0.0117	BestValid 0.0997
	Epoch 2275:	Loss 0.3966	TrainAcc 0.0115	ValidAcc 0.0111	TestAcc 0.0107	BestValid 0.0997
	Epoch 2300:	Loss 0.3947	TrainAcc 0.0116	ValidAcc 0.0108	TestAcc 0.0109	BestValid 0.0997
	Epoch 2325:	Loss 0.3947	TrainAcc 0.0169	ValidAcc 0.0164	TestAcc 0.0163	BestValid 0.0997
	Epoch 2350:	Loss 0.3927	TrainAcc 0.0327	ValidAcc 0.0305	TestAcc 0.0306	BestValid 0.0997
	Epoch 2375:	Loss 0.3905	TrainAcc 0.0154	ValidAcc 0.0144	TestAcc 0.0143	BestValid 0.0997
	Epoch 2400:	Loss 0.3899	TrainAcc 0.0260	ValidAcc 0.0255	TestAcc 0.0262	BestValid 0.0997
	Epoch 2425:	Loss 0.3909	TrainAcc 0.0535	ValidAcc 0.0509	TestAcc 0.0510	BestValid 0.0997
	Epoch 2450:	Loss 0.3896	TrainAcc 0.0208	ValidAcc 0.0198	TestAcc 0.0203	BestValid 0.0997
	Epoch 2475:	Loss 0.3920	TrainAcc 0.0213	ValidAcc 0.0208	TestAcc 0.0213	BestValid 0.0997
	Epoch 2500:	Loss 0.3864	TrainAcc 0.0320	ValidAcc 0.0312	TestAcc 0.0330	BestValid 0.0997
	Epoch 2525:	Loss 0.3823	TrainAcc 0.0510	ValidAcc 0.0490	TestAcc 0.0489	BestValid 0.0997
	Epoch 2550:	Loss 0.3828	TrainAcc 0.0484	ValidAcc 0.0465	TestAcc 0.0468	BestValid 0.0997
	Epoch 2575:	Loss 0.3813	TrainAcc 0.0265	ValidAcc 0.0270	TestAcc 0.0276	BestValid 0.0997
	Epoch 2600:	Loss 0.3792	TrainAcc 0.0518	ValidAcc 0.0501	TestAcc 0.0499	BestValid 0.0997
	Epoch 2625:	Loss 0.3817	TrainAcc 0.0559	ValidAcc 0.0528	TestAcc 0.0519	BestValid 0.0997
	Epoch 2650:	Loss 0.3767	TrainAcc 0.0299	ValidAcc 0.0305	TestAcc 0.0314	BestValid 0.0997
	Epoch 2675:	Loss 0.3751	TrainAcc 0.0331	ValidAcc 0.0323	TestAcc 0.0335	BestValid 0.0997
	Epoch 2700:	Loss 0.3757	TrainAcc 0.0365	ValidAcc 0.0368	TestAcc 0.0369	BestValid 0.0997
	Epoch 2725:	Loss 0.3744	TrainAcc 0.0496	ValidAcc 0.0474	TestAcc 0.0461	BestValid 0.0997
	Epoch 2750:	Loss 0.3720	TrainAcc 0.0546	ValidAcc 0.0504	TestAcc 0.0501	BestValid 0.0997
	Epoch 2775:	Loss 0.3735	TrainAcc 0.0524	ValidAcc 0.0434	TestAcc 0.0450	BestValid 0.0997
	Epoch 2800:	Loss 0.3702	TrainAcc 0.0522	ValidAcc 0.0444	TestAcc 0.0457	BestValid 0.0997
	Epoch 2825:	Loss 0.3709	TrainAcc 0.0330	ValidAcc 0.0323	TestAcc 0.0326	BestValid 0.0997
	Epoch 2850:	Loss 0.3688	TrainAcc 0.0731	ValidAcc 0.0638	TestAcc 0.0643	BestValid 0.0997
	Epoch 2875:	Loss 0.3683	TrainAcc 0.0739	ValidAcc 0.0646	TestAcc 0.0653	BestValid 0.0997
	Epoch 2900:	Loss 0.3685	TrainAcc 0.0529	ValidAcc 0.0496	TestAcc 0.0498	BestValid 0.0997
	Epoch 2925:	Loss 0.3683	TrainAcc 0.0732	ValidAcc 0.0646	TestAcc 0.0654	BestValid 0.0997
	Epoch 2950:	Loss 0.3673	TrainAcc 0.0570	ValidAcc 0.0493	TestAcc 0.0511	BestValid 0.0997
	Epoch 2975:	Loss 0.3645	TrainAcc 0.0731	ValidAcc 0.0645	TestAcc 0.0653	BestValid 0.0997
	Epoch 3000:	Loss 0.3653	TrainAcc 0.0443	ValidAcc 0.0408	TestAcc 0.0425	BestValid 0.0997
	Epoch 3025:	Loss 0.3709	TrainAcc 0.0745	ValidAcc 0.0654	TestAcc 0.0662	BestValid 0.0997
	Epoch 3050:	Loss 0.3664	TrainAcc 0.0679	ValidAcc 0.0611	TestAcc 0.0613	BestValid 0.0997
	Epoch 3075:	Loss 0.3690	TrainAcc 0.0400	ValidAcc 0.0379	TestAcc 0.0396	BestValid 0.0997
	Epoch 3100:	Loss 0.3644	TrainAcc 0.0509	ValidAcc 0.0457	TestAcc 0.0474	BestValid 0.0997
	Epoch 3125:	Loss 0.3687	TrainAcc 0.0732	ValidAcc 0.0648	TestAcc 0.0658	BestValid 0.0997
	Epoch 3150:	Loss 0.3646	TrainAcc 0.0614	ValidAcc 0.0570	TestAcc 0.0575	BestValid 0.0997
	Epoch 3175:	Loss 0.3654	TrainAcc 0.0517	ValidAcc 0.0465	TestAcc 0.0480	BestValid 0.0997
	Epoch 3200:	Loss 0.3591	TrainAcc 0.0445	ValidAcc 0.0410	TestAcc 0.0427	BestValid 0.0997
	Epoch 3225:	Loss 0.3594	TrainAcc 0.0451	ValidAcc 0.0415	TestAcc 0.0433	BestValid 0.0997
	Epoch 3250:	Loss 0.3580	TrainAcc 0.0365	ValidAcc 0.0364	TestAcc 0.0377	BestValid 0.0997
	Epoch 3275:	Loss 0.3565	TrainAcc 0.0474	ValidAcc 0.0432	TestAcc 0.0454	BestValid 0.0997
	Epoch 3300:	Loss 0.3569	TrainAcc 0.0389	ValidAcc 0.0378	TestAcc 0.0392	BestValid 0.0997
	Epoch 3325:	Loss 0.3584	TrainAcc 0.0503	ValidAcc 0.0454	TestAcc 0.0473	BestValid 0.0997
	Epoch 3350:	Loss 0.3568	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3375:	Loss 0.3557	TrainAcc 0.0399	ValidAcc 0.0381	TestAcc 0.0397	BestValid 0.0997
	Epoch 3400:	Loss 0.3558	TrainAcc 0.0389	ValidAcc 0.0378	TestAcc 0.0392	BestValid 0.0997
	Epoch 3425:	Loss 0.3551	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3450:	Loss 0.3533	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0370	BestValid 0.0997
	Epoch 3475:	Loss 0.3568	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0370	BestValid 0.0997
	Epoch 3500:	Loss 0.3540	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0370	BestValid 0.0997
	Epoch 3525:	Loss 0.3533	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3550:	Loss 0.3516	TrainAcc 0.0388	ValidAcc 0.0378	TestAcc 0.0392	BestValid 0.0997
	Epoch 3575:	Loss 0.3528	TrainAcc 0.0353	ValidAcc 0.0358	TestAcc 0.0373	BestValid 0.0997
	Epoch 3600:	Loss 0.3488	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3625:	Loss 0.3513	TrainAcc 0.0356	ValidAcc 0.0359	TestAcc 0.0374	BestValid 0.0997
	Epoch 3650:	Loss 0.3502	TrainAcc 0.0379	ValidAcc 0.0373	TestAcc 0.0386	BestValid 0.0997
	Epoch 3675:	Loss 0.3500	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3700:	Loss 0.3511	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3725:	Loss 0.3558	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3750:	Loss 0.3537	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3775:	Loss 0.3563	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3800:	Loss 0.3523	TrainAcc 0.0346	ValidAcc 0.0356	TestAcc 0.0371	BestValid 0.0997
	Epoch 3825:	Loss 0.3527	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3850:	Loss 0.3486	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3875:	Loss 0.3472	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3900:	Loss 0.3457	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 3925:	Loss 0.3520	TrainAcc 0.0394	ValidAcc 0.0383	TestAcc 0.0399	BestValid 0.0997
	Epoch 3950:	Loss 0.3452	TrainAcc 0.0599	ValidAcc 0.0650	TestAcc 0.0666	BestValid 0.0997
	Epoch 3975:	Loss 0.3476	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 4000:	Loss 0.3488	TrainAcc 0.0347	ValidAcc 0.0356	TestAcc 0.0370	BestValid 0.0997
	Epoch 4025:	Loss 0.3677	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.0997
	Epoch 4050:	Loss 0.3676	TrainAcc 0.0570	ValidAcc 0.0487	TestAcc 0.0504	BestValid 0.0997
	Epoch 4075:	Loss 0.3595	TrainAcc 0.0345	ValidAcc 0.0355	TestAcc 0.0369	BestValid 0.0997
	Epoch 4100:	Loss 0.3538	TrainAcc 0.0349	ValidAcc 0.0354	TestAcc 0.0372	BestValid 0.0997
	Epoch 4125:	Loss 0.3611	TrainAcc 0.0466	ValidAcc 0.0559	TestAcc 0.0584	BestValid 0.0997
	Epoch 4150:	Loss 0.3496	TrainAcc 0.1001	ValidAcc 0.1248	TestAcc 0.1284	BestValid 0.1248
	Epoch 4175:	Loss 0.3466	TrainAcc 0.0348	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4200:	Loss 0.3452	TrainAcc 0.0347	ValidAcc 0.0355	TestAcc 0.0369	BestValid 0.1248
	Epoch 4225:	Loss 0.3433	TrainAcc 0.0398	ValidAcc 0.0438	TestAcc 0.0448	BestValid 0.1248
	Epoch 4250:	Loss 0.3410	TrainAcc 0.0733	ValidAcc 0.0855	TestAcc 0.0876	BestValid 0.1248
	Epoch 4275:	Loss 0.3404	TrainAcc 0.0479	ValidAcc 0.0487	TestAcc 0.0508	BestValid 0.1248
	Epoch 4300:	Loss 0.3403	TrainAcc 0.0360	ValidAcc 0.0366	TestAcc 0.0380	BestValid 0.1248
	Epoch 4325:	Loss 0.3429	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4350:	Loss 0.3413	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0370	BestValid 0.1248
	Epoch 4375:	Loss 0.3397	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0370	BestValid 0.1248
	Epoch 4400:	Loss 0.3410	TrainAcc 0.0353	ValidAcc 0.0359	TestAcc 0.0375	BestValid 0.1248
	Epoch 4425:	Loss 0.3386	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4450:	Loss 0.3386	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0370	BestValid 0.1248
	Epoch 4475:	Loss 0.3377	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4500:	Loss 0.3367	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4525:	Loss 0.3401	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4550:	Loss 0.3374	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4575:	Loss 0.3376	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0370	BestValid 0.1248
	Epoch 4600:	Loss 0.3351	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4625:	Loss 0.3359	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4650:	Loss 0.3359	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4675:	Loss 0.3356	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4700:	Loss 0.3352	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4725:	Loss 0.3363	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4750:	Loss 0.3398	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4775:	Loss 0.3392	TrainAcc 0.0346	ValidAcc 0.0354	TestAcc 0.0370	BestValid 0.1248
	Epoch 4800:	Loss 0.3341	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0370	BestValid 0.1248
	Epoch 4825:	Loss 0.3369	TrainAcc 0.0941	ValidAcc 0.0904	TestAcc 0.0905	BestValid 0.1248
	Epoch 4850:	Loss 0.3323	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1248
	Epoch 4875:	Loss 0.3324	TrainAcc 0.0749	ValidAcc 0.0778	TestAcc 0.0789	BestValid 0.1248
	Epoch 4900:	Loss 0.3339	TrainAcc 0.1003	ValidAcc 0.0919	TestAcc 0.0923	BestValid 0.1248
	Epoch 4925:	Loss 0.3317	TrainAcc 0.0345	ValidAcc 0.0354	TestAcc 0.0369	BestValid 0.1248
	Epoch 4950:	Loss 0.3309	TrainAcc 0.0345	ValidAcc 0.0353	TestAcc 0.0370	BestValid 0.1248
	Epoch 4975:	Loss 0.3330	TrainAcc 0.1015	ValidAcc 0.0926	TestAcc 0.0929	BestValid 0.1248
	Epoch 5000:	Loss 0.3312	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0370	BestValid 0.1248
Node 2, Pre/Post-Pipelining: 1.109 / 1.143 ms, Bubble: 100.667 ms, Compute: 372.596 ms, Comm: 30.222 ms, Imbalance: 39.447 ms
Node 0, Pre/Post-Pipelining: 1.111 / 1.287 ms, Bubble: 98.275 ms, Compute: 419.454 ms, Comm: 16.890 ms, Imbalance: 7.158 ms
Node 4, Pre/Post-Pipelining: 1.108 / 1.159 ms, Bubble: 101.797 ms, Compute: 371.061 ms, Comm: 27.080 ms, Imbalance: 43.269 ms
Node 7, Pre/Post-Pipelining: 1.109 / 16.195 ms, Bubble: 88.702 ms, Compute: 392.960 ms, Comm: 17.311 ms, Imbalance: 28.584 ms
Node 6, Pre/Post-Pipelining: 1.111 / 1.099 ms, Bubble: 102.925 ms, Compute: 378.933 ms, Comm: 29.532 ms, Imbalance: 31.497 ms
Node 1, Pre/Post-Pipelining: 1.113 / 1.115 ms, Bubble: 99.545 ms, Compute: 384.629 ms, Comm: 28.178 ms, Imbalance: 30.255 ms
Node 3, Pre/Post-Pipelining: 1.106 / 1.190 ms, Bubble: 100.543 ms, Compute: 382.326 ms, Comm: 26.995 ms, Imbalance: 32.653 ms
Node 5, Pre/Post-Pipelining: 1.106 / 1.159 ms, Bubble: 102.225 ms, Compute: 378.589 ms, Comm: 31.174 ms, Imbalance: 30.823 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.111 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.287 ms
Cluster-Wide Average, Bubble: 98.275 ms
Cluster-Wide Average, Compute: 419.454 ms
Cluster-Wide Average, Communication: 16.890 ms
Cluster-Wide Average, Imbalance: 7.158 ms
Node 0, GPU memory consumption: 6.614 GB
Node 2, GPU memory consumption: 5.260 GB
Node 1, GPU memory consumption: 5.260 GB
Node 4, GPU memory consumption: 5.237 GB
Node 3, GPU memory consumption: 5.237 GB
Node 7, GPU memory consumption: 5.042 GB
Node 5, GPU memory consumption: 5.260 GB
Node 6, GPU memory consumption: 5.260 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 1.088440 s---------------
------------------------node id 1,  per-epoch time: 1.088441 s---------------
------------------------node id 2,  per-epoch time: 1.088440 s---------------
------------------------node id 3,  per-epoch time: 1.088441 s---------------
------------------------node id 4,  per-epoch time: 1.088440 s---------------
------------------------node id 5,  per-epoch time: 1.088441 s---------------
------------------------node id 6,  per-epoch time: 1.088441 s---------------
------------------------node id 7,  per-epoch time: 1.088441 s---------------
************ Profiling Results ************
	Bubble: 703.645471 (ms) (64.67 percentage)
	Compute: 379.809166 (ms) (34.91 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 4.548440 (ms) (0.42 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 1.218 GB
	Aggregated layer-level communication throughput: 402.614 Gbps
Highest valid_acc: 0.1248
Target test_acc: 0.1284
Epoch to reach the target acc: 4149
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 5] Success 
[MPI Rank 7] Success 
