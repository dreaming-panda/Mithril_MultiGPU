gnerv1
Mon Jul 31 19:34:05 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 34%   54C    P8    32W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   50C    P8    28W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   48C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   45C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 11%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 88%] Built target OSDI2023_MULTI_NODES_graphsage
[ 88%] Built target OSDI2023_MULTI_NODES_gcn
[ 88%] Built target estimate_comm_volume
[ 88%] Built target OSDI2023_MULTI_NODES_gcnii
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.059 seconds.
Building the CSC structure...
        It takes 2.391 seconds.
Building the CSC structure...
        It takes 2.406 seconds.
Building the CSC structure...
        It takes 2.425 seconds.
Building the CSC structure...
        It takes 2.447 seconds.
Building the CSC structure...
        It takes 2.563 seconds.
Building the CSC structure...
        It takes 2.650 seconds.
Building the CSC structure...
        It takes 2.653 seconds.
Building the CSC structure...
        It takes 1.832 seconds.
        It takes 2.256 seconds.
        It takes 2.323 seconds.
        It takes 2.337 seconds.
        It takes 2.325 seconds.
        It takes 2.332 seconds.
        It takes 2.312 seconds.
        It takes 2.418 seconds.
Building the Feature Vector...
        It takes 0.236 seconds.
Building the Label Vector...
        It takes 0.028 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.280 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.042 seconds.
        It takes 0.034 seconds.
        It takes 0.268 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.037 seconds.
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
        It takes 0.290 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.287 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 55.446 Gbps (per GPU), 443.572 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.235 Gbps (per GPU), 441.882 Gbps (aggregated)
The layer-level communication performance: 55.199 Gbps (per GPU), 441.595 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.996 Gbps (per GPU), 439.968 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.966 Gbps (per GPU), 439.729 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.794 Gbps (per GPU), 438.349 Gbps (aggregated)
The layer-level communication performance: 54.752 Gbps (per GPU), 438.013 Gbps (aggregated)
The layer-level communication performance: 54.719 Gbps (per GPU), 437.752 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.482 Gbps (per GPU), 1267.856 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.426 Gbps (per GPU), 1267.407 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.443 Gbps (per GPU), 1267.546 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.419 Gbps (per GPU), 1267.353 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.413 Gbps (per GPU), 1267.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.413 Gbps (per GPU), 1267.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.401 Gbps (per GPU), 1267.211 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.411 Gbps (per GPU), 1267.285 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.749 Gbps (per GPU), 829.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.748 Gbps (per GPU), 829.987 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.748 Gbps (per GPU), 829.987 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.748 Gbps (per GPU), 829.980 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.750 Gbps (per GPU), 830.001 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.749 Gbps (per GPU), 829.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.748 Gbps (per GPU), 829.987 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.751 Gbps (per GPU), 830.007 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.441 Gbps (per GPU), 275.524 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.441 Gbps (per GPU), 275.524 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.440 Gbps (per GPU), 275.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.441 Gbps (per GPU), 275.525 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.440 Gbps (per GPU), 275.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.439 Gbps (per GPU), 275.516 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.439 Gbps (per GPU), 275.513 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.415 Gbps (per GPU), 275.318 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.45ms  2.75ms  3.04  8.38K  3.53M
 chk_1  0.76ms  2.82ms  2.97ms  3.92  6.74K  3.60M
 chk_2  0.80ms  2.67ms  2.84ms  3.55  7.27K  3.53M
 chk_3  0.81ms  2.74ms  2.88ms  3.56  7.92K  3.61M
 chk_4  0.63ms  2.65ms  2.80ms  4.43  5.33K  3.68M
 chk_5  1.01ms  2.64ms  2.81ms  2.79 10.07K  3.45M
 chk_6  0.97ms  2.79ms  2.99ms  3.09  9.41K  3.48M
 chk_7  0.82ms  2.65ms  2.95ms  3.60  8.12K  3.60M
 chk_8  0.68ms  2.75ms  2.91ms  4.26  6.09K  3.64M
 chk_9  1.10ms  2.59ms  2.78ms  2.52 11.10K  3.38M
chk_10  0.66ms  2.79ms  2.94ms  4.49  5.67K  3.63M
chk_11  0.83ms  2.63ms  2.83ms  3.43  8.16K  3.54M
chk_12  0.80ms  2.85ms  3.02ms  3.78  7.24K  3.55M
chk_13  0.64ms  2.68ms  2.86ms  4.46  5.41K  3.68M
chk_14  0.78ms  2.92ms  3.09ms  3.95  7.14K  3.53M
chk_15  0.96ms  2.81ms  2.99ms  3.13  9.25K  3.49M
chk_16  0.60ms  2.62ms  2.77ms  4.61  4.78K  3.77M
chk_17  0.77ms  2.76ms  2.89ms  3.78  6.85K  3.60M
chk_18  0.81ms  2.55ms  2.71ms  3.33  7.47K  3.57M
chk_19  0.61ms  2.62ms  2.77ms  4.55  4.88K  3.75M
chk_20  0.77ms  2.61ms  2.78ms  3.59  7.00K  3.63M
chk_21  0.64ms  2.60ms  2.75ms  4.30  5.41K  3.68M
chk_22  1.10ms  2.80ms  3.04ms  2.76 11.07K  3.39M
chk_23  0.80ms  2.72ms  2.88ms  3.61  7.23K  3.64M
chk_24  1.02ms  2.81ms  3.01ms  2.96 10.13K  3.43M
chk_25  0.73ms  2.59ms  2.77ms  3.77  6.40K  3.57M
chk_26  0.67ms  2.81ms  2.95ms  4.42  5.78K  3.55M
chk_27  0.96ms  2.67ms  2.89ms  3.00  9.34K  3.48M
chk_28  0.73ms  2.98ms  3.13ms  4.28  6.37K  3.57M
chk_29  0.64ms  2.77ms  2.92ms  4.60  5.16K  3.78M
chk_30  0.64ms  2.64ms  2.82ms  4.39  5.44K  3.67M
chk_31  0.73ms  2.78ms  2.98ms  4.08  6.33K  3.63M
   Avg  0.79  2.71  2.89
   Max  1.10  2.98  3.13
   Min  0.60  2.45  2.71
 Ratio  1.84  1.22  1.16
   Var  0.02  0.01  0.01
Profiling takes 2.454 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 372.379 ms
Partition 0 [0, 5) has cost: 372.379 ms
Partition 1 [5, 9) has cost: 347.010 ms
Partition 2 [9, 13) has cost: 347.010 ms
Partition 3 [13, 17) has cost: 347.010 ms
Partition 4 [17, 21) has cost: 347.010 ms
Partition 5 [21, 25) has cost: 347.010 ms
Partition 6 [25, 29) has cost: 347.010 ms
Partition 7 [29, 33) has cost: 352.748 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 168.221 ms
GPU 0, Compute+Comm Time: 135.770 ms, Bubble Time: 29.594 ms, Imbalance Overhead: 2.857 ms
GPU 1, Compute+Comm Time: 128.656 ms, Bubble Time: 29.248 ms, Imbalance Overhead: 10.317 ms
GPU 2, Compute+Comm Time: 128.656 ms, Bubble Time: 29.256 ms, Imbalance Overhead: 10.309 ms
GPU 3, Compute+Comm Time: 128.656 ms, Bubble Time: 29.111 ms, Imbalance Overhead: 10.454 ms
GPU 4, Compute+Comm Time: 128.656 ms, Bubble Time: 29.045 ms, Imbalance Overhead: 10.520 ms
GPU 5, Compute+Comm Time: 128.656 ms, Bubble Time: 29.115 ms, Imbalance Overhead: 10.450 ms
GPU 6, Compute+Comm Time: 128.656 ms, Bubble Time: 29.383 ms, Imbalance Overhead: 10.182 ms
GPU 7, Compute+Comm Time: 130.013 ms, Bubble Time: 29.761 ms, Imbalance Overhead: 8.447 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 324.140 ms
GPU 0, Compute+Comm Time: 249.625 ms, Bubble Time: 57.698 ms, Imbalance Overhead: 16.817 ms
GPU 1, Compute+Comm Time: 245.245 ms, Bubble Time: 56.919 ms, Imbalance Overhead: 21.977 ms
GPU 2, Compute+Comm Time: 245.245 ms, Bubble Time: 56.311 ms, Imbalance Overhead: 22.585 ms
GPU 3, Compute+Comm Time: 245.245 ms, Bubble Time: 56.198 ms, Imbalance Overhead: 22.698 ms
GPU 4, Compute+Comm Time: 245.245 ms, Bubble Time: 56.245 ms, Imbalance Overhead: 22.651 ms
GPU 5, Compute+Comm Time: 245.245 ms, Bubble Time: 56.404 ms, Imbalance Overhead: 22.492 ms
GPU 6, Compute+Comm Time: 245.245 ms, Bubble Time: 56.246 ms, Imbalance Overhead: 22.650 ms
GPU 7, Compute+Comm Time: 263.500 ms, Bubble Time: 56.878 ms, Imbalance Overhead: 3.763 ms
The estimated cost of the whole pipeline: 516.979 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 719.389 ms
Partition 0 [0, 9) has cost: 719.389 ms
Partition 1 [9, 17) has cost: 694.020 ms
Partition 2 [17, 25) has cost: 694.020 ms
Partition 3 [25, 33) has cost: 699.758 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 174.553 ms
GPU 0, Compute+Comm Time: 144.878 ms, Bubble Time: 27.325 ms, Imbalance Overhead: 2.351 ms
GPU 1, Compute+Comm Time: 141.029 ms, Bubble Time: 26.915 ms, Imbalance Overhead: 6.608 ms
GPU 2, Compute+Comm Time: 141.029 ms, Bubble Time: 26.841 ms, Imbalance Overhead: 6.682 ms
GPU 3, Compute+Comm Time: 141.716 ms, Bubble Time: 26.528 ms, Imbalance Overhead: 6.308 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 323.223 ms
GPU 0, Compute+Comm Time: 261.695 ms, Bubble Time: 49.375 ms, Imbalance Overhead: 12.153 ms
GPU 1, Compute+Comm Time: 259.539 ms, Bubble Time: 49.794 ms, Imbalance Overhead: 13.890 ms
GPU 2, Compute+Comm Time: 259.539 ms, Bubble Time: 49.866 ms, Imbalance Overhead: 13.818 ms
GPU 3, Compute+Comm Time: 269.624 ms, Bubble Time: 50.713 ms, Imbalance Overhead: 2.886 ms
    The estimated cost with 2 DP ways is 522.665 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1413.409 ms
Partition 0 [0, 17) has cost: 1413.409 ms
Partition 1 [17, 33) has cost: 1393.778 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 220.530 ms
GPU 0, Compute+Comm Time: 192.022 ms, Bubble Time: 23.622 ms, Imbalance Overhead: 4.885 ms
GPU 1, Compute+Comm Time: 190.239 ms, Bubble Time: 24.327 ms, Imbalance Overhead: 5.964 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 360.557 ms
GPU 0, Compute+Comm Time: 312.448 ms, Bubble Time: 39.972 ms, Imbalance Overhead: 8.138 ms
GPU 1, Compute+Comm Time: 317.048 ms, Bubble Time: 38.823 ms, Imbalance Overhead: 4.686 ms
    The estimated cost with 4 DP ways is 610.141 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2807.187 ms
Partition 0 [0, 33) has cost: 2807.187 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 557.290 ms
GPU 0, Compute+Comm Time: 557.290 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 683.160 ms
GPU 0, Compute+Comm Time: 683.160 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1302.472 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5716	TrainAcc 0.4329	ValidAcc 0.4509	TestAcc 0.4511	BestValid 0.4509
	Epoch 50:	Loss 1.9229	TrainAcc 0.6916	ValidAcc 0.7074	TestAcc 0.7018	BestValid 0.7074
	Epoch 75:	Loss 1.5355	TrainAcc 0.7551	ValidAcc 0.7693	TestAcc 0.7632	BestValid 0.7693
	Epoch 100:	Loss 1.3099	TrainAcc 0.7877	ValidAcc 0.8002	TestAcc 0.7942	BestValid 0.8002
	Epoch 125:	Loss 1.1221	TrainAcc 0.8171	ValidAcc 0.8282	TestAcc 0.8213	BestValid 0.8282
	Epoch 150:	Loss 1.0293	TrainAcc 0.8382	ValidAcc 0.8478	TestAcc 0.8415	BestValid 0.8478
	Epoch 175:	Loss 0.9656	TrainAcc 0.8504	ValidAcc 0.8595	TestAcc 0.8533	BestValid 0.8595
	Epoch 200:	Loss 0.9073	TrainAcc 0.8587	ValidAcc 0.8667	TestAcc 0.8611	BestValid 0.8667
	Epoch 225:	Loss 0.8547	TrainAcc 0.8688	ValidAcc 0.8762	TestAcc 0.8706	BestValid 0.8762
	Epoch 250:	Loss 0.8235	TrainAcc 0.8767	ValidAcc 0.8833	TestAcc 0.8784	BestValid 0.8833
	Epoch 275:	Loss 0.7963	TrainAcc 0.8807	ValidAcc 0.8863	TestAcc 0.8822	BestValid 0.8863
	Epoch 300:	Loss 0.7723	TrainAcc 0.8853	ValidAcc 0.8905	TestAcc 0.8863	BestValid 0.8905
	Epoch 325:	Loss 0.7483	TrainAcc 0.8894	ValidAcc 0.8944	TestAcc 0.8902	BestValid 0.8944
	Epoch 350:	Loss 0.7289	TrainAcc 0.8913	ValidAcc 0.8953	TestAcc 0.8916	BestValid 0.8953
	Epoch 375:	Loss 0.7070	TrainAcc 0.8953	ValidAcc 0.8993	TestAcc 0.8956	BestValid 0.8993
	Epoch 400:	Loss 0.6952	TrainAcc 0.8980	ValidAcc 0.9013	TestAcc 0.8973	BestValid 0.9013
	Epoch 425:	Loss 0.6810	TrainAcc 0.8996	ValidAcc 0.9034	TestAcc 0.8988	BestValid 0.9034
	Epoch 450:	Loss 0.6696	TrainAcc 0.9025	ValidAcc 0.9055	TestAcc 0.9011	BestValid 0.9055
	Epoch 475:	Loss 0.6582	TrainAcc 0.9047	ValidAcc 0.9073	TestAcc 0.9030	BestValid 0.9073
	Epoch 500:	Loss 0.6495	TrainAcc 0.9068	ValidAcc 0.9099	TestAcc 0.9047	BestValid 0.9099
	Epoch 525:	Loss 0.6384	TrainAcc 0.9076	ValidAcc 0.9104	TestAcc 0.9058	BestValid 0.9104
	Epoch 550:	Loss 0.6318	TrainAcc 0.9094	ValidAcc 0.9120	TestAcc 0.9074	BestValid 0.9120
	Epoch 575:	Loss 0.6217	TrainAcc 0.9113	ValidAcc 0.9132	TestAcc 0.9090	BestValid 0.9132
	Epoch 600:	Loss 0.6107	TrainAcc 0.9131	ValidAcc 0.9149	TestAcc 0.9101	BestValid 0.9149
	Epoch 625:	Loss 0.5996	TrainAcc 0.9134	ValidAcc 0.9156	TestAcc 0.9113	BestValid 0.9156
	Epoch 650:	Loss 0.5957	TrainAcc 0.9150	ValidAcc 0.9163	TestAcc 0.9127	BestValid 0.9163
	Epoch 675:	Loss 0.5904	TrainAcc 0.9163	ValidAcc 0.9174	TestAcc 0.9142	BestValid 0.9174
	Epoch 700:	Loss 0.5845	TrainAcc 0.9175	ValidAcc 0.9177	TestAcc 0.9151	BestValid 0.9177
	Epoch 725:	Loss 0.5776	TrainAcc 0.9185	ValidAcc 0.9193	TestAcc 0.9162	BestValid 0.9193
	Epoch 750:	Loss 0.5751	TrainAcc 0.9194	ValidAcc 0.9203	TestAcc 0.9170	BestValid 0.9203
	Epoch 775:	Loss 0.5694	TrainAcc 0.9203	ValidAcc 0.9214	TestAcc 0.9178	BestValid 0.9214
	Epoch 800:	Loss 0.5686	TrainAcc 0.9202	ValidAcc 0.9211	TestAcc 0.9180	BestValid 0.9214
	Epoch 825:	Loss 0.5589	TrainAcc 0.9212	ValidAcc 0.9221	TestAcc 0.9187	BestValid 0.9221
	Epoch 850:	Loss 0.5523	TrainAcc 0.9225	ValidAcc 0.9229	TestAcc 0.9200	BestValid 0.9229
	Epoch 875:	Loss 0.5543	TrainAcc 0.9233	ValidAcc 0.9236	TestAcc 0.9210	BestValid 0.9236
	Epoch 900:	Loss 0.5446	TrainAcc 0.9237	ValidAcc 0.9235	TestAcc 0.9216	BestValid 0.9236
	Epoch 925:	Loss 0.5382	TrainAcc 0.9248	ValidAcc 0.9246	TestAcc 0.9223	BestValid 0.9246
	Epoch 950:	Loss 0.5342	TrainAcc 0.9252	ValidAcc 0.9248	TestAcc 0.9221	BestValid 0.9248
	Epoch 975:	Loss 0.5356	TrainAcc 0.9253	ValidAcc 0.9249	TestAcc 0.9224	BestValid 0.9249
	Epoch 1000:	Loss 0.5314	TrainAcc 0.9268	ValidAcc 0.9260	TestAcc 0.9237	BestValid 0.9260
	Epoch 1025:	Loss 0.5286	TrainAcc 0.9272	ValidAcc 0.9261	TestAcc 0.9238	BestValid 0.9261
	Epoch 1050:	Loss 0.5251	TrainAcc 0.9279	ValidAcc 0.9268	TestAcc 0.9243	BestValid 0.9268
	Epoch 1075:	Loss 0.5217	TrainAcc 0.9283	ValidAcc 0.9280	TestAcc 0.9252	BestValid 0.9280
	Epoch 1100:	Loss 0.5173	TrainAcc 0.9288	ValidAcc 0.9275	TestAcc 0.9253	BestValid 0.9280
	Epoch 1125:	Loss 0.5116	TrainAcc 0.9297	ValidAcc 0.9280	TestAcc 0.9259	BestValid 0.9280
	Epoch 1150:	Loss 0.5101	TrainAcc 0.9299	ValidAcc 0.9280	TestAcc 0.9258	BestValid 0.9280
	Epoch 1175:	Loss 0.5097	TrainAcc 0.9304	ValidAcc 0.9297	TestAcc 0.9265	BestValid 0.9297
	Epoch 1200:	Loss 0.5068	TrainAcc 0.9308	ValidAcc 0.9295	TestAcc 0.9267	BestValid 0.9297
	Epoch 1225:	Loss 0.5042	TrainAcc 0.9313	ValidAcc 0.9295	TestAcc 0.9270	BestValid 0.9297
	Epoch 1250:	Loss 0.5010	TrainAcc 0.9321	ValidAcc 0.9306	TestAcc 0.9280	BestValid 0.9306
	Epoch 1275:	Loss 0.4961	TrainAcc 0.9321	ValidAcc 0.9305	TestAcc 0.9278	BestValid 0.9306
	Epoch 1300:	Loss 0.4940	TrainAcc 0.9328	ValidAcc 0.9311	TestAcc 0.9287	BestValid 0.9311
	Epoch 1325:	Loss 0.4915	TrainAcc 0.9330	ValidAcc 0.9313	TestAcc 0.9291	BestValid 0.9313
	Epoch 1350:	Loss 0.4845	TrainAcc 0.9329	ValidAcc 0.9313	TestAcc 0.9285	BestValid 0.9313
	Epoch 1375:	Loss 0.4863	TrainAcc 0.9339	ValidAcc 0.9317	TestAcc 0.9297	BestValid 0.9317
	Epoch 1400:	Loss 0.4796	TrainAcc 0.9344	ValidAcc 0.9323	TestAcc 0.9301	BestValid 0.9323
	Epoch 1425:	Loss 0.4833	TrainAcc 0.9343	ValidAcc 0.9324	TestAcc 0.9301	BestValid 0.9324
	Epoch 1450:	Loss 0.4815	TrainAcc 0.9344	ValidAcc 0.9324	TestAcc 0.9301	BestValid 0.9324
	Epoch 1475:	Loss 0.4779	TrainAcc 0.9355	ValidAcc 0.9334	TestAcc 0.9313	BestValid 0.9334
	Epoch 1500:	Loss 0.4734	TrainAcc 0.9353	ValidAcc 0.9329	TestAcc 0.9311	BestValid 0.9334
	Epoch 1525:	Loss 0.4719	TrainAcc 0.9362	ValidAcc 0.9335	TestAcc 0.9318	BestValid 0.9335
	Epoch 1550:	Loss 0.4671	TrainAcc 0.9366	ValidAcc 0.9341	TestAcc 0.9322	BestValid 0.9341
	Epoch 1575:	Loss 0.4678	TrainAcc 0.9357	ValidAcc 0.9335	TestAcc 0.9313	BestValid 0.9341
	Epoch 1600:	Loss 0.4693	TrainAcc 0.9369	ValidAcc 0.9345	TestAcc 0.9326	BestValid 0.9345
	Epoch 1625:	Loss 0.4609	TrainAcc 0.9371	ValidAcc 0.9344	TestAcc 0.9326	BestValid 0.9345
	Epoch 1650:	Loss 0.4625	TrainAcc 0.9375	ValidAcc 0.9347	TestAcc 0.9326	BestValid 0.9347
	Epoch 1675:	Loss 0.4617	TrainAcc 0.9370	ValidAcc 0.9343	TestAcc 0.9323	BestValid 0.9347
	Epoch 1700:	Loss 0.4590	TrainAcc 0.9375	ValidAcc 0.9342	TestAcc 0.9328	BestValid 0.9347
	Epoch 1725:	Loss 0.4565	TrainAcc 0.9384	ValidAcc 0.9351	TestAcc 0.9335	BestValid 0.9351
	Epoch 1750:	Loss 0.4561	TrainAcc 0.9382	ValidAcc 0.9352	TestAcc 0.9334	BestValid 0.9352
	Epoch 1775:	Loss 0.4587	TrainAcc 0.9385	ValidAcc 0.9355	TestAcc 0.9337	BestValid 0.9355
	Epoch 1800:	Loss 0.4509	TrainAcc 0.9389	ValidAcc 0.9359	TestAcc 0.9341	BestValid 0.9359
	Epoch 1825:	Loss 0.4528	TrainAcc 0.9391	ValidAcc 0.9363	TestAcc 0.9343	BestValid 0.9363
	Epoch 1850:	Loss 0.4467	TrainAcc 0.9393	ValidAcc 0.9363	TestAcc 0.9345	BestValid 0.9363
	Epoch 1875:	Loss 0.4480	TrainAcc 0.9396	ValidAcc 0.9362	TestAcc 0.9344	BestValid 0.9363
	Epoch 1900:	Loss 0.4455	TrainAcc 0.9398	ValidAcc 0.9365	TestAcc 0.9349	BestValid 0.9365
	Epoch 1925:	Loss 0.4425	TrainAcc 0.9401	ValidAcc 0.9368	TestAcc 0.9352	BestValid 0.9368
	Epoch 1950:	Loss 0.4430	TrainAcc 0.9411	ValidAcc 0.9377	TestAcc 0.9359	BestValid 0.9377
	Epoch 1975:	Loss 0.4393	TrainAcc 0.9400	ValidAcc 0.9369	TestAcc 0.9348	BestValid 0.9377
	Epoch 2000:	Loss 0.4401	TrainAcc 0.9409	ValidAcc 0.9379	TestAcc 0.9360	BestValid 0.9379
	Epoch 2025:	Loss 0.4362	TrainAcc 0.9406	ValidAcc 0.9370	TestAcc 0.9354	BestValid 0.9379
	Epoch 2050:	Loss 0.4377	TrainAcc 0.9413	ValidAcc 0.9377	TestAcc 0.9359	BestValid 0.9379
	Epoch 2075:	Loss 0.4385	TrainAcc 0.9421	ValidAcc 0.9390	TestAcc 0.9369	BestValid 0.9390
	Epoch 2100:	Loss 0.4352	TrainAcc 0.9411	ValidAcc 0.9381	TestAcc 0.9356	BestValid 0.9390
	Epoch 2125:	Loss 0.4337	TrainAcc 0.9423	ValidAcc 0.9384	TestAcc 0.9368	BestValid 0.9390
	Epoch 2150:	Loss 0.4321	TrainAcc 0.9419	ValidAcc 0.9385	TestAcc 0.9360	BestValid 0.9390
	Epoch 2175:	Loss 0.4294	TrainAcc 0.9421	ValidAcc 0.9386	TestAcc 0.9365	BestValid 0.9390
	Epoch 2200:	Loss 0.4341	TrainAcc 0.9423	ValidAcc 0.9388	TestAcc 0.9365	BestValid 0.9390
	Epoch 2225:	Loss 0.4258	TrainAcc 0.9427	ValidAcc 0.9398	TestAcc 0.9370	BestValid 0.9398
	Epoch 2250:	Loss 0.4258	TrainAcc 0.9427	ValidAcc 0.9394	TestAcc 0.9369	BestValid 0.9398
	Epoch 2275:	Loss 0.4237	TrainAcc 0.9431	ValidAcc 0.9398	TestAcc 0.9372	BestValid 0.9398
	Epoch 2300:	Loss 0.4238	TrainAcc 0.9436	ValidAcc 0.9405	TestAcc 0.9382	BestValid 0.9405
	Epoch 2325:	Loss 0.4210	TrainAcc 0.9436	ValidAcc 0.9405	TestAcc 0.9380	BestValid 0.9405
	Epoch 2350:	Loss 0.4200	TrainAcc 0.9433	ValidAcc 0.9401	TestAcc 0.9377	BestValid 0.9405
	Epoch 2375:	Loss 0.4207	TrainAcc 0.9444	ValidAcc 0.9406	TestAcc 0.9383	BestValid 0.9406
	Epoch 2400:	Loss 0.4204	TrainAcc 0.9441	ValidAcc 0.9405	TestAcc 0.9386	BestValid 0.9406
	Epoch 2425:	Loss 0.4151	TrainAcc 0.9441	ValidAcc 0.9406	TestAcc 0.9384	BestValid 0.9406
	Epoch 2450:	Loss 0.4135	TrainAcc 0.9441	ValidAcc 0.9407	TestAcc 0.9384	BestValid 0.9407
	Epoch 2475:	Loss 0.4161	TrainAcc 0.9439	ValidAcc 0.9402	TestAcc 0.9380	BestValid 0.9407
	Epoch 2500:	Loss 0.4126	TrainAcc 0.9448	ValidAcc 0.9415	TestAcc 0.9390	BestValid 0.9415
	Epoch 2525:	Loss 0.4151	TrainAcc 0.9447	ValidAcc 0.9413	TestAcc 0.9387	BestValid 0.9415
	Epoch 2550:	Loss 0.4116	TrainAcc 0.9454	ValidAcc 0.9417	TestAcc 0.9392	BestValid 0.9417
	Epoch 2575:	Loss 0.4117	TrainAcc 0.9447	ValidAcc 0.9421	TestAcc 0.9390	BestValid 0.9421
	Epoch 2600:	Loss 0.4083	TrainAcc 0.9459	ValidAcc 0.9427	TestAcc 0.9404	BestValid 0.9427
	Epoch 2625:	Loss 0.4127	TrainAcc 0.9450	ValidAcc 0.9421	TestAcc 0.9389	BestValid 0.9427
	Epoch 2650:	Loss 0.4075	TrainAcc 0.9456	ValidAcc 0.9427	TestAcc 0.9400	BestValid 0.9427
	Epoch 2675:	Loss 0.4035	TrainAcc 0.9458	ValidAcc 0.9426	TestAcc 0.9404	BestValid 0.9427
	Epoch 2700:	Loss 0.4070	TrainAcc 0.9456	ValidAcc 0.9425	TestAcc 0.9397	BestValid 0.9427
	Epoch 2725:	Loss 0.4075	TrainAcc 0.9460	ValidAcc 0.9427	TestAcc 0.9402	BestValid 0.9427
	Epoch 2750:	Loss 0.4036	TrainAcc 0.9453	ValidAcc 0.9423	TestAcc 0.9392	BestValid 0.9427
	Epoch 2775:	Loss 0.4018	TrainAcc 0.9466	ValidAcc 0.9428	TestAcc 0.9404	BestValid 0.9428
	Epoch 2800:	Loss 0.4019	TrainAcc 0.9464	ValidAcc 0.9430	TestAcc 0.9404	BestValid 0.9430
	Epoch 2825:	Loss 0.3973	TrainAcc 0.9459	ValidAcc 0.9424	TestAcc 0.9403	BestValid 0.9430
	Epoch 2850:	Loss 0.4021	TrainAcc 0.9466	ValidAcc 0.9433	TestAcc 0.9405	BestValid 0.9433
	Epoch 2875:	Loss 0.4002	TrainAcc 0.9466	ValidAcc 0.9429	TestAcc 0.9408	BestValid 0.9433
	Epoch 2900:	Loss 0.4014	TrainAcc 0.9467	ValidAcc 0.9433	TestAcc 0.9404	BestValid 0.9433
	Epoch 2925:	Loss 0.3973	TrainAcc 0.9471	ValidAcc 0.9439	TestAcc 0.9413	BestValid 0.9439
	Epoch 2950:	Loss 0.3997	TrainAcc 0.9480	ValidAcc 0.9447	TestAcc 0.9421	BestValid 0.9447
	Epoch 2975:	Loss 0.3946	TrainAcc 0.9476	ValidAcc 0.9441	TestAcc 0.9417	BestValid 0.9447
	Epoch 3000:	Loss 0.3957	TrainAcc 0.9469	ValidAcc 0.9435	TestAcc 0.9409	BestValid 0.9447
	Epoch 3025:	Loss 0.3928	TrainAcc 0.9478	ValidAcc 0.9444	TestAcc 0.9414	BestValid 0.9447
	Epoch 3050:	Loss 0.3958	TrainAcc 0.9464	ValidAcc 0.9435	TestAcc 0.9400	BestValid 0.9447
	Epoch 3075:	Loss 0.3919	TrainAcc 0.9487	ValidAcc 0.9453	TestAcc 0.9429	BestValid 0.9453
	Epoch 3100:	Loss 0.3906	TrainAcc 0.9467	ValidAcc 0.9434	TestAcc 0.9404	BestValid 0.9453
	Epoch 3125:	Loss 0.3865	TrainAcc 0.9479	ValidAcc 0.9447	TestAcc 0.9420	BestValid 0.9453
	Epoch 3150:	Loss 0.3882	TrainAcc 0.9470	ValidAcc 0.9439	TestAcc 0.9409	BestValid 0.9453
	Epoch 3175:	Loss 0.3866	TrainAcc 0.9472	ValidAcc 0.9442	TestAcc 0.9411	BestValid 0.9453
	Epoch 3200:	Loss 0.3885	TrainAcc 0.9490	ValidAcc 0.9455	TestAcc 0.9429	BestValid 0.9455
	Epoch 3225:	Loss 0.3900	TrainAcc 0.9485	ValidAcc 0.9451	TestAcc 0.9422	BestValid 0.9455
	Epoch 3250:	Loss 0.3858	TrainAcc 0.9485	ValidAcc 0.9454	TestAcc 0.9424	BestValid 0.9455
	Epoch 3275:	Loss 0.3832	TrainAcc 0.9492	ValidAcc 0.9455	TestAcc 0.9431	BestValid 0.9455
	Epoch 3300:	Loss 0.3820	TrainAcc 0.9497	ValidAcc 0.9454	TestAcc 0.9431	BestValid 0.9455
	Epoch 3325:	Loss 0.3846	TrainAcc 0.9481	ValidAcc 0.9442	TestAcc 0.9418	BestValid 0.9455
	Epoch 3350:	Loss 0.3854	TrainAcc 0.9493	ValidAcc 0.9458	TestAcc 0.9429	BestValid 0.9458
	Epoch 3375:	Loss 0.3818	TrainAcc 0.9477	ValidAcc 0.9440	TestAcc 0.9409	BestValid 0.9458
	Epoch 3400:	Loss 0.3837	TrainAcc 0.9493	ValidAcc 0.9457	TestAcc 0.9427	BestValid 0.9458
	Epoch 3425:	Loss 0.3797	TrainAcc 0.9495	ValidAcc 0.9462	TestAcc 0.9429	BestValid 0.9462
	Epoch 3450:	Loss 0.3791	TrainAcc 0.9490	ValidAcc 0.9453	TestAcc 0.9427	BestValid 0.9462
	Epoch 3475:	Loss 0.3776	TrainAcc 0.9499	ValidAcc 0.9461	TestAcc 0.9435	BestValid 0.9462
	Epoch 3500:	Loss 0.3780	TrainAcc 0.9482	ValidAcc 0.9444	TestAcc 0.9414	BestValid 0.9462
	Epoch 3525:	Loss 0.3768	TrainAcc 0.9496	ValidAcc 0.9458	TestAcc 0.9432	BestValid 0.9462
	Epoch 3550:	Loss 0.3784	TrainAcc 0.9483	ValidAcc 0.9451	TestAcc 0.9416	BestValid 0.9462
	Epoch 3575:	Loss 0.3770	TrainAcc 0.9501	ValidAcc 0.9462	TestAcc 0.9432	BestValid 0.9462
	Epoch 3600:	Loss 0.3762	TrainAcc 0.9490	ValidAcc 0.9449	TestAcc 0.9426	BestValid 0.9462
	Epoch 3625:	Loss 0.3750	TrainAcc 0.9503	ValidAcc 0.9466	TestAcc 0.9438	BestValid 0.9466
	Epoch 3650:	Loss 0.3788	TrainAcc 0.9482	ValidAcc 0.9446	TestAcc 0.9418	BestValid 0.9466
	Epoch 3675:	Loss 0.3736	TrainAcc 0.9499	ValidAcc 0.9457	TestAcc 0.9434	BestValid 0.9466
	Epoch 3700:	Loss 0.3768	TrainAcc 0.9491	ValidAcc 0.9455	TestAcc 0.9429	BestValid 0.9466
	Epoch 3725:	Loss 0.3702	TrainAcc 0.9510	ValidAcc 0.9471	TestAcc 0.9446	BestValid 0.9471
	Epoch 3750:	Loss 0.3718	TrainAcc 0.9505	ValidAcc 0.9461	TestAcc 0.9440	BestValid 0.9471
	Epoch 3775:	Loss 0.3733	TrainAcc 0.9494	ValidAcc 0.9460	TestAcc 0.9430	BestValid 0.9471
	Epoch 3800:	Loss 0.3714	TrainAcc 0.9494	ValidAcc 0.9459	TestAcc 0.9427	BestValid 0.9471
	Epoch 3825:	Loss 0.3704	TrainAcc 0.9492	ValidAcc 0.9452	TestAcc 0.9425	BestValid 0.9471
	Epoch 3850:	Loss 0.3685	TrainAcc 0.9497	ValidAcc 0.9462	TestAcc 0.9431	BestValid 0.9471
	Epoch 3875:	Loss 0.3694	TrainAcc 0.9511	ValidAcc 0.9473	TestAcc 0.9446	BestValid 0.9473
	Epoch 3900:	Loss 0.3709	TrainAcc 0.9511	ValidAcc 0.9468	TestAcc 0.9443	BestValid 0.9473
	Epoch 3925:	Loss 0.3688	TrainAcc 0.9503	ValidAcc 0.9464	TestAcc 0.9437	BestValid 0.9473
	Epoch 3950:	Loss 0.3636	TrainAcc 0.9519	ValidAcc 0.9480	TestAcc 0.9448	BestValid 0.9480
	Epoch 3975:	Loss 0.3712	TrainAcc 0.9512	ValidAcc 0.9468	TestAcc 0.9446	BestValid 0.9480
	Epoch 4000:	Loss 0.3671	TrainAcc 0.9509	ValidAcc 0.9468	TestAcc 0.9440	BestValid 0.9480
	Epoch 4025:	Loss 0.3650	TrainAcc 0.9506	ValidAcc 0.9463	TestAcc 0.9436	BestValid 0.9480
	Epoch 4050:	Loss 0.3623	TrainAcc 0.9504	ValidAcc 0.9459	TestAcc 0.9437	BestValid 0.9480
	Epoch 4075:	Loss 0.3661	TrainAcc 0.9500	ValidAcc 0.9462	TestAcc 0.9431	BestValid 0.9480
	Epoch 4100:	Loss 0.3645	TrainAcc 0.9512	ValidAcc 0.9472	TestAcc 0.9446	BestValid 0.9480
	Epoch 4125:	Loss 0.3580	TrainAcc 0.9503	ValidAcc 0.9464	TestAcc 0.9432	BestValid 0.9480
	Epoch 4150:	Loss 0.3620	TrainAcc 0.9523	ValidAcc 0.9478	TestAcc 0.9458	BestValid 0.9480
	Epoch 4175:	Loss 0.3629	TrainAcc 0.9521	ValidAcc 0.9480	TestAcc 0.9455	BestValid 0.9480
	Epoch 4200:	Loss 0.3592	TrainAcc 0.9518	ValidAcc 0.9469	TestAcc 0.9449	BestValid 0.9480
	Epoch 4225:	Loss 0.3598	TrainAcc 0.9509	ValidAcc 0.9468	TestAcc 0.9443	BestValid 0.9480
	Epoch 4250:	Loss 0.3586	TrainAcc 0.9530	ValidAcc 0.9487	TestAcc 0.9466	BestValid 0.9487
	Epoch 4275:	Loss 0.3597	TrainAcc 0.9509	ValidAcc 0.9471	TestAcc 0.9444	BestValid 0.9487
	Epoch 4300:	Loss 0.3571	TrainAcc 0.9506	ValidAcc 0.9462	TestAcc 0.9440	BestValid 0.9487
	Epoch 4325:	Loss 0.3578	TrainAcc 0.9510	ValidAcc 0.9465	TestAcc 0.9442	BestValid 0.9487
	Epoch 4350:	Loss 0.3556	TrainAcc 0.9521	ValidAcc 0.9475	TestAcc 0.9457	BestValid 0.9487
	Epoch 4375:	Loss 0.3566	TrainAcc 0.9518	ValidAcc 0.9473	TestAcc 0.9448	BestValid 0.9487
	Epoch 4400:	Loss 0.3564	TrainAcc 0.9507	ValidAcc 0.9462	TestAcc 0.9441	BestValid 0.9487
	Epoch 4425:	Loss 0.3553	TrainAcc 0.9522	ValidAcc 0.9473	TestAcc 0.9454	BestValid 0.9487
	Epoch 4450:	Loss 0.3531	TrainAcc 0.9513	ValidAcc 0.9462	TestAcc 0.9448	BestValid 0.9487
	Epoch 4475:	Loss 0.3492	TrainAcc 0.9507	ValidAcc 0.9456	TestAcc 0.9432	BestValid 0.9487
	Epoch 4500:	Loss 0.3510	TrainAcc 0.9520	ValidAcc 0.9472	TestAcc 0.9455	BestValid 0.9487
	Epoch 4525:	Loss 0.3511	TrainAcc 0.9529	ValidAcc 0.9475	TestAcc 0.9463	BestValid 0.9487
	Epoch 4550:	Loss 0.3558	TrainAcc 0.9527	ValidAcc 0.9478	TestAcc 0.9460	BestValid 0.9487
	Epoch 4575:	Loss 0.3525	TrainAcc 0.9535	ValidAcc 0.9484	TestAcc 0.9468	BestValid 0.9487
	Epoch 4600:	Loss 0.3517	TrainAcc 0.9522	ValidAcc 0.9472	TestAcc 0.9453	BestValid 0.9487
	Epoch 4625:	Loss 0.3451	TrainAcc 0.9517	ValidAcc 0.9473	TestAcc 0.9453	BestValid 0.9487
	Epoch 4650:	Loss 0.3518	TrainAcc 0.9533	ValidAcc 0.9485	TestAcc 0.9467	BestValid 0.9487
	Epoch 4675:	Loss 0.3538	TrainAcc 0.9525	ValidAcc 0.9473	TestAcc 0.9459	BestValid 0.9487
	Epoch 4700:	Loss 0.3519	TrainAcc 0.9506	ValidAcc 0.9454	TestAcc 0.9434	BestValid 0.9487
	Epoch 4725:	Loss 0.3484	TrainAcc 0.9529	ValidAcc 0.9476	TestAcc 0.9464	BestValid 0.9487
	Epoch 4750:	Loss 0.3482	TrainAcc 0.9536	ValidAcc 0.9483	TestAcc 0.9472	BestValid 0.9487
	Epoch 4775:	Loss 0.3464	TrainAcc 0.9513	ValidAcc 0.9456	TestAcc 0.9441	BestValid 0.9487
	Epoch 4800:	Loss 0.3460	TrainAcc 0.9508	ValidAcc 0.9457	TestAcc 0.9440	BestValid 0.9487
	Epoch 4825:	Loss 0.3453	TrainAcc 0.9528	ValidAcc 0.9476	TestAcc 0.9461	BestValid 0.9487
	Epoch 4850:	Loss 0.3433	TrainAcc 0.9535	ValidAcc 0.9481	TestAcc 0.9471	BestValid 0.9487
	Epoch 4875:	Loss 0.3458	TrainAcc 0.9505	ValidAcc 0.9457	TestAcc 0.9436	BestValid 0.9487
	Epoch 4900:	Loss 0.3432	TrainAcc 0.9531	ValidAcc 0.9477	TestAcc 0.9461	BestValid 0.9487
	Epoch 4925:	Loss 0.3456	TrainAcc 0.9539	ValidAcc 0.9483	TestAcc 0.9475	BestValid 0.9487
	Epoch 4950:	Loss 0.3435	TrainAcc 0.9503	ValidAcc 0.9454	TestAcc 0.9434	BestValid 0.9487
	Epoch 4975:	Loss 0.3433	TrainAcc 0.9537	ValidAcc 0.9483	TestAcc 0.9464	BestValid 0.9487
	Epoch 5000:	Loss 0.3467	TrainAcc 0.9534	ValidAcc 0.9475	TestAcc 0.9468	BestValid 0.9487
Node 5, Pre/Post-Pipelining: 1.143 / 0.869 ms, Bubble: 84.219 ms, Compute: 254.129 ms, Comm: 46.705 ms, Imbalance: 43.832 ms
Node 7, Pre/Post-Pipelining: 1.140 / 16.169 ms, Bubble: 70.390 ms, Compute: 280.184 ms, Comm: 32.522 ms, Imbalance: 30.067 ms
Node 4, Pre/Post-Pipelining: 1.146 / 0.878 ms, Bubble: 83.402 ms, Compute: 249.203 ms, Comm: 48.361 ms, Imbalance: 48.206 ms
Node 6, Pre/Post-Pipelining: 1.142 / 0.853 ms, Bubble: 84.981 ms, Compute: 254.787 ms, Comm: 41.386 ms, Imbalance: 47.864 ms
Node 1, Pre/Post-Pipelining: 1.133 / 0.907 ms, Bubble: 82.135 ms, Compute: 265.757 ms, Comm: 43.023 ms, Imbalance: 37.186 ms
Node 3, Pre/Post-Pipelining: 1.136 / 0.888 ms, Bubble: 82.038 ms, Compute: 263.459 ms, Comm: 48.087 ms, Imbalance: 34.750 ms
Node 0, Pre/Post-Pipelining: 1.138 / 1.015 ms, Bubble: 81.779 ms, Compute: 298.715 ms, Comm: 33.177 ms, Imbalance: 13.534 ms
Node 2, Pre/Post-Pipelining: 1.143 / 0.879 ms, Bubble: 83.072 ms, Compute: 258.017 ms, Comm: 48.428 ms, Imbalance: 39.272 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.138 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.015 ms
Cluster-Wide Average, Bubble: 81.779 ms
Cluster-Wide Average, Compute: 298.715 ms
Cluster-Wide Average, Communication: 33.177 ms
Cluster-Wide Average, Imbalance: 13.534 ms
Node 0, GPU memory consumption: 8.059 GB
Node 1, GPU memory consumption: 6.042 GB
Node 3, GPU memory consumption: 6.018 GB
Node 4, GPU memory consumption: 6.018 GB
Node 6, GPU memory consumption: 6.042 GB
Node 5, GPU memory consumption: 6.042 GB
Node 7, GPU memory consumption: 6.171 GB
Node 2, GPU memory consumption: 6.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.936868 s---------------
------------------------node id 1,  per-epoch time: 0.936868 s---------------
------------------------node id 2,  per-epoch time: 0.936868 s---------------
------------------------node id 3,  per-epoch time: 0.936868 s---------------
------------------------node id 4,  per-epoch time: 0.936868 s---------------
------------------------node id 5,  per-epoch time: 0.936868 s---------------
------------------------node id 6,  per-epoch time: 0.936868 s---------------
------------------------node id 7,  per-epoch time: 0.936868 s---------------
************ Profiling Results ************
	Bubble: 671.816531 (ms) (71.74 percentage)
	Compute: 260.914148 (ms) (27.86 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.724866 (ms) (0.40 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 2.431 GB
	Aggregated layer-level communication throughput: 488.714 Gbps
Highest valid_acc: 0.9487
Target test_acc: 0.9466
Epoch to reach the target acc: 4249
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
