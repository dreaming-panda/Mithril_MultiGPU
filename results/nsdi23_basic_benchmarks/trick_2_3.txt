gnerv1
Tue Aug  1 02:42:03 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   41C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   41C    P8    26W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   39C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   37C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 11%] Built target context
[ 36%] Built target core
Scanning dependencies of target cudahelp
[ 38%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 41%] Linking CXX static library libcudahelp.a
[ 77%] Built target cudahelp
[ 86%] Linking CXX executable graphsage
[ 86%] Linking CXX executable gcn
[ 86%] Linking CXX executable estimate_comm_volume
[ 86%] Linking CXX executable gcnii
[ 91%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_gcnii
[ 97%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target OSDI2023_MULTI_NODES_gcn
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.866 seconds.
Building the CSC structure...
        It takes 1.913 seconds.
Building the CSC structure...
        It takes 2.047 seconds.
Building the CSC structure...
        It takes 2.342 seconds.
Building the CSC structure...
        It takes 2.397 seconds.
Building the CSC structure...
        It takes 2.447 seconds.
Building the CSC structure...
        It takes 2.464 seconds.
Building the CSC structure...
        It takes 2.519 seconds.
Building the CSC structure...
        It takes 1.816 seconds.
        It takes 1.873 seconds.
        It takes 1.854 seconds.
        It takes 2.243 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.397 seconds.
        It takes 2.349 seconds.
        It takes 2.335 seconds.
        It takes 2.398 seconds.
        It takes 0.259 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
        It takes 0.292 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.260 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.307 seconds.
Building the Label Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.034 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.330 seconds.
Building the Label Vector...
        It takes 0.337 seconds.
Building the Label Vector...
        It takes 0.043 seconds.
        It takes 0.045 seconds.
        It takes 0.278 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.278 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.255 Gbps (per GPU), 458.040 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.976 Gbps (per GPU), 455.809 Gbps (aggregated)
The layer-level communication performance: 56.974 Gbps (per GPU), 455.789 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.732 Gbps (per GPU), 453.856 Gbps (aggregated)
The layer-level communication performance: 56.714 Gbps (per GPU), 453.708 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.532 Gbps (per GPU), 452.259 Gbps (aggregated)
The layer-level communication performance: 56.481 Gbps (per GPU), 451.845 Gbps (aggregated)
The layer-level communication performance: 56.459 Gbps (per GPU), 451.671 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 161.512 Gbps (per GPU), 1292.096 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.493 Gbps (per GPU), 1291.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.509 Gbps (per GPU), 1292.071 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.493 Gbps (per GPU), 1291.948 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.496 Gbps (per GPU), 1291.971 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.496 Gbps (per GPU), 1291.971 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.500 Gbps (per GPU), 1291.996 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.497 Gbps (per GPU), 1291.974 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.213 Gbps (per GPU), 833.706 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.218 Gbps (per GPU), 833.747 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.216 Gbps (per GPU), 833.726 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.212 Gbps (per GPU), 833.699 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.214 Gbps (per GPU), 833.713 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.217 Gbps (per GPU), 833.733 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.217 Gbps (per GPU), 833.733 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.215 Gbps (per GPU), 833.720 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.805 Gbps (per GPU), 270.441 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.805 Gbps (per GPU), 270.438 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.805 Gbps (per GPU), 270.440 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.804 Gbps (per GPU), 270.432 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.804 Gbps (per GPU), 270.428 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.804 Gbps (per GPU), 270.432 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.803 Gbps (per GPU), 270.426 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.804 Gbps (per GPU), 270.431 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.88ms  2.42ms  2.71ms  3.06  8.38K  3.53M
 chk_1  0.75ms  2.77ms  2.94ms  3.92  6.74K  3.60M
 chk_2  0.79ms  2.66ms  2.80ms  3.54  7.27K  3.53M
 chk_3  0.80ms  2.68ms  2.85ms  3.58  7.92K  3.61M
 chk_4  0.62ms  2.60ms  2.77ms  4.44  5.33K  3.68M
 chk_5  1.00ms  2.61ms  2.80ms  2.81 10.07K  3.45M
 chk_6  0.96ms  2.76ms  2.95ms  3.09  9.41K  3.48M
 chk_7  0.81ms  2.53ms  2.78ms  3.41  8.12K  3.60M
 chk_8  0.67ms  2.71ms  2.87ms  4.27  6.09K  3.64M
 chk_9  1.09ms  2.52ms  2.72ms  2.49 11.10K  3.38M
chk_10  0.64ms  2.76ms  2.90ms  4.50  5.67K  3.63M
chk_11  0.82ms  2.61ms  2.80ms  3.43  8.16K  3.54M
chk_12  0.79ms  2.82ms  2.99ms  3.80  7.24K  3.55M
chk_13  0.63ms  2.64ms  2.82ms  4.51  5.41K  3.68M
chk_14  0.77ms  2.89ms  3.06ms  3.95  7.14K  3.53M
chk_15  0.95ms  2.74ms  2.93ms  3.10  9.25K  3.49M
chk_16  0.59ms  2.57ms  2.72ms  4.60  4.78K  3.77M
chk_17  0.76ms  2.70ms  2.86ms  3.79  6.85K  3.60M
chk_18  0.80ms  2.51ms  2.67ms  3.33  7.47K  3.57M
chk_19  0.60ms  2.58ms  2.71ms  4.50  4.88K  3.75M
chk_20  0.76ms  2.57ms  2.72ms  3.56  7.00K  3.63M
chk_21  0.63ms  2.58ms  2.71ms  4.31  5.41K  3.68M
chk_22  1.09ms  2.78ms  2.97ms  2.72 11.07K  3.39M
chk_23  0.79ms  2.67ms  2.82ms  3.58  7.23K  3.64M
chk_24  1.01ms  2.71ms  2.88ms  2.86 10.13K  3.43M
chk_25  0.72ms  2.53ms  2.68ms  3.70  6.40K  3.57M
chk_26  0.66ms  2.73ms  2.89ms  4.41  5.78K  3.55M
chk_27  0.95ms  2.64ms  2.82ms  2.96  9.34K  3.48M
chk_28  0.72ms  2.92ms  3.30ms  4.57  6.37K  3.57M
chk_29  0.62ms  2.71ms  2.86ms  4.58  5.16K  3.78M
chk_30  0.63ms  2.60ms  2.76ms  4.35  5.44K  3.67M
chk_31  0.72ms  2.75ms  2.91ms  4.05  6.33K  3.63M
   Avg  0.78  2.66  2.84
   Max  1.09  2.92  3.30
   Min  0.59  2.42  2.67
 Ratio  1.84  1.21  1.23
   Var  0.02  0.01  0.02
Profiling takes 2.417 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 366.120 ms
Partition 0 [0, 5) has cost: 366.120 ms
Partition 1 [5, 9) has cost: 341.095 ms
Partition 2 [9, 13) has cost: 341.095 ms
Partition 3 [13, 17) has cost: 341.095 ms
Partition 4 [17, 21) has cost: 341.095 ms
Partition 5 [21, 25) has cost: 341.095 ms
Partition 6 [25, 29) has cost: 341.095 ms
Partition 7 [29, 33) has cost: 346.788 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.397 ms
GPU 0, Compute+Comm Time: 133.257 ms, Bubble Time: 29.005 ms, Imbalance Overhead: 3.134 ms
GPU 1, Compute+Comm Time: 126.261 ms, Bubble Time: 28.698 ms, Imbalance Overhead: 10.438 ms
GPU 2, Compute+Comm Time: 126.261 ms, Bubble Time: 28.717 ms, Imbalance Overhead: 10.418 ms
GPU 3, Compute+Comm Time: 126.261 ms, Bubble Time: 28.645 ms, Imbalance Overhead: 10.491 ms
GPU 4, Compute+Comm Time: 126.261 ms, Bubble Time: 28.570 ms, Imbalance Overhead: 10.566 ms
GPU 5, Compute+Comm Time: 126.261 ms, Bubble Time: 28.608 ms, Imbalance Overhead: 10.527 ms
GPU 6, Compute+Comm Time: 126.261 ms, Bubble Time: 28.878 ms, Imbalance Overhead: 10.257 ms
GPU 7, Compute+Comm Time: 127.431 ms, Bubble Time: 29.268 ms, Imbalance Overhead: 8.698 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 319.299 ms
GPU 0, Compute+Comm Time: 245.398 ms, Bubble Time: 56.877 ms, Imbalance Overhead: 17.024 ms
GPU 1, Compute+Comm Time: 240.875 ms, Bubble Time: 56.087 ms, Imbalance Overhead: 22.337 ms
GPU 2, Compute+Comm Time: 240.875 ms, Bubble Time: 55.483 ms, Imbalance Overhead: 22.941 ms
GPU 3, Compute+Comm Time: 240.875 ms, Bubble Time: 55.396 ms, Imbalance Overhead: 23.029 ms
GPU 4, Compute+Comm Time: 240.875 ms, Bubble Time: 55.668 ms, Imbalance Overhead: 22.756 ms
GPU 5, Compute+Comm Time: 240.875 ms, Bubble Time: 55.755 ms, Imbalance Overhead: 22.670 ms
GPU 6, Compute+Comm Time: 240.875 ms, Bubble Time: 55.590 ms, Imbalance Overhead: 22.835 ms
GPU 7, Compute+Comm Time: 258.903 ms, Bubble Time: 56.167 ms, Imbalance Overhead: 4.229 ms
The estimated cost of the whole pipeline: 508.931 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 707.215 ms
Partition 0 [0, 9) has cost: 707.215 ms
Partition 1 [9, 17) has cost: 682.191 ms
Partition 2 [17, 25) has cost: 682.191 ms
Partition 3 [25, 33) has cost: 687.883 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 171.387 ms
GPU 0, Compute+Comm Time: 142.193 ms, Bubble Time: 26.852 ms, Imbalance Overhead: 2.342 ms
GPU 1, Compute+Comm Time: 138.404 ms, Bubble Time: 26.419 ms, Imbalance Overhead: 6.564 ms
GPU 2, Compute+Comm Time: 138.404 ms, Bubble Time: 26.282 ms, Imbalance Overhead: 6.700 ms
GPU 3, Compute+Comm Time: 138.943 ms, Bubble Time: 26.050 ms, Imbalance Overhead: 6.393 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 318.089 ms
GPU 0, Compute+Comm Time: 257.127 ms, Bubble Time: 48.611 ms, Imbalance Overhead: 12.352 ms
GPU 1, Compute+Comm Time: 254.790 ms, Bubble Time: 48.862 ms, Imbalance Overhead: 14.437 ms
GPU 2, Compute+Comm Time: 254.790 ms, Bubble Time: 49.026 ms, Imbalance Overhead: 14.273 ms
GPU 3, Compute+Comm Time: 264.752 ms, Bubble Time: 49.937 ms, Imbalance Overhead: 3.400 ms
    The estimated cost with 2 DP ways is 513.950 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1389.406 ms
Partition 0 [0, 17) has cost: 1389.406 ms
Partition 1 [17, 33) has cost: 1370.074 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 216.952 ms
GPU 0, Compute+Comm Time: 188.853 ms, Bubble Time: 23.165 ms, Imbalance Overhead: 4.934 ms
GPU 1, Compute+Comm Time: 187.049 ms, Bubble Time: 23.806 ms, Imbalance Overhead: 6.096 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 354.647 ms
GPU 0, Compute+Comm Time: 307.207 ms, Bubble Time: 38.911 ms, Imbalance Overhead: 8.529 ms
GPU 1, Compute+Comm Time: 311.540 ms, Bubble Time: 38.054 ms, Imbalance Overhead: 5.053 ms
    The estimated cost with 4 DP ways is 600.179 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2759.480 ms
Partition 0 [0, 33) has cost: 2759.480 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 563.032 ms
GPU 0, Compute+Comm Time: 563.032 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 687.398 ms
GPU 0, Compute+Comm Time: 687.398 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1312.952 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5753	TrainAcc 0.4334	ValidAcc 0.4531	TestAcc 0.4531	BestValid 0.4531
	Epoch 50:	Loss 1.9186	TrainAcc 0.6943	ValidAcc 0.7119	TestAcc 0.7053	BestValid 0.7119
	Epoch 75:	Loss 1.5302	TrainAcc 0.7658	ValidAcc 0.7790	TestAcc 0.7730	BestValid 0.7790
	Epoch 100:	Loss 1.2993	TrainAcc 0.8066	ValidAcc 0.8183	TestAcc 0.8118	BestValid 0.8183
	Epoch 125:	Loss 1.1324	TrainAcc 0.8264	ValidAcc 0.8367	TestAcc 0.8308	BestValid 0.8367
	Epoch 150:	Loss 1.0338	TrainAcc 0.8405	ValidAcc 0.8494	TestAcc 0.8445	BestValid 0.8494
	Epoch 175:	Loss 0.9666	TrainAcc 0.8528	ValidAcc 0.8621	TestAcc 0.8553	BestValid 0.8621
	Epoch 200:	Loss 0.9093	TrainAcc 0.8628	ValidAcc 0.8713	TestAcc 0.8636	BestValid 0.8713
	Epoch 225:	Loss 0.8656	TrainAcc 0.8707	ValidAcc 0.8777	TestAcc 0.8713	BestValid 0.8777
	Epoch 250:	Loss 0.8343	TrainAcc 0.8775	ValidAcc 0.8837	TestAcc 0.8777	BestValid 0.8837
	Epoch 275:	Loss 0.8036	TrainAcc 0.8825	ValidAcc 0.8878	TestAcc 0.8824	BestValid 0.8878
	Epoch 300:	Loss 0.7774	TrainAcc 0.8870	ValidAcc 0.8919	TestAcc 0.8868	BestValid 0.8919
	Epoch 325:	Loss 0.7515	TrainAcc 0.8915	ValidAcc 0.8950	TestAcc 0.8903	BestValid 0.8950
	Epoch 350:	Loss 0.7344	TrainAcc 0.8942	ValidAcc 0.8974	TestAcc 0.8929	BestValid 0.8974
	Epoch 375:	Loss 0.7176	TrainAcc 0.8967	ValidAcc 0.9008	TestAcc 0.8955	BestValid 0.9008
	Epoch 400:	Loss 0.7055	TrainAcc 0.9002	ValidAcc 0.9045	TestAcc 0.8987	BestValid 0.9045
	Epoch 425:	Loss 0.6840	TrainAcc 0.9022	ValidAcc 0.9062	TestAcc 0.9005	BestValid 0.9062
	Epoch 450:	Loss 0.6745	TrainAcc 0.9041	ValidAcc 0.9079	TestAcc 0.9019	BestValid 0.9079
	Epoch 475:	Loss 0.6618	TrainAcc 0.9060	ValidAcc 0.9096	TestAcc 0.9039	BestValid 0.9096
	Epoch 500:	Loss 0.6518	TrainAcc 0.9080	ValidAcc 0.9114	TestAcc 0.9053	BestValid 0.9114
	Epoch 525:	Loss 0.6494	TrainAcc 0.9095	ValidAcc 0.9133	TestAcc 0.9074	BestValid 0.9133
	Epoch 550:	Loss 0.6349	TrainAcc 0.9108	ValidAcc 0.9136	TestAcc 0.9083	BestValid 0.9136
	Epoch 575:	Loss 0.6283	TrainAcc 0.9127	ValidAcc 0.9152	TestAcc 0.9100	BestValid 0.9152
	Epoch 600:	Loss 0.6162	TrainAcc 0.9135	ValidAcc 0.9157	TestAcc 0.9108	BestValid 0.9157
	Epoch 625:	Loss 0.6094	TrainAcc 0.9148	ValidAcc 0.9167	TestAcc 0.9123	BestValid 0.9167
	Epoch 650:	Loss 0.5986	TrainAcc 0.9155	ValidAcc 0.9169	TestAcc 0.9132	BestValid 0.9169
	Epoch 675:	Loss 0.5975	TrainAcc 0.9169	ValidAcc 0.9178	TestAcc 0.9144	BestValid 0.9178
	Epoch 700:	Loss 0.5917	TrainAcc 0.9180	ValidAcc 0.9190	TestAcc 0.9156	BestValid 0.9190
	Epoch 725:	Loss 0.5855	TrainAcc 0.9188	ValidAcc 0.9197	TestAcc 0.9162	BestValid 0.9197
	Epoch 750:	Loss 0.5783	TrainAcc 0.9195	ValidAcc 0.9203	TestAcc 0.9168	BestValid 0.9203
	Epoch 775:	Loss 0.5741	TrainAcc 0.9203	ValidAcc 0.9214	TestAcc 0.9174	BestValid 0.9214
	Epoch 800:	Loss 0.5667	TrainAcc 0.9209	ValidAcc 0.9225	TestAcc 0.9181	BestValid 0.9225
	Epoch 825:	Loss 0.5617	TrainAcc 0.9218	ValidAcc 0.9230	TestAcc 0.9188	BestValid 0.9230
	Epoch 850:	Loss 0.5567	TrainAcc 0.9221	ValidAcc 0.9229	TestAcc 0.9195	BestValid 0.9230
	Epoch 875:	Loss 0.5555	TrainAcc 0.9239	ValidAcc 0.9254	TestAcc 0.9212	BestValid 0.9254
	Epoch 900:	Loss 0.5507	TrainAcc 0.9228	ValidAcc 0.9234	TestAcc 0.9199	BestValid 0.9254
	Epoch 925:	Loss 0.5537	TrainAcc 0.9242	ValidAcc 0.9253	TestAcc 0.9215	BestValid 0.9254
	Epoch 950:	Loss 0.5373	TrainAcc 0.9256	ValidAcc 0.9267	TestAcc 0.9226	BestValid 0.9267
	Epoch 975:	Loss 0.5424	TrainAcc 0.9258	ValidAcc 0.9259	TestAcc 0.9226	BestValid 0.9267
	Epoch 1000:	Loss 0.5400	TrainAcc 0.9257	ValidAcc 0.9263	TestAcc 0.9227	BestValid 0.9267
	Epoch 1025:	Loss 0.5336	TrainAcc 0.9265	ValidAcc 0.9267	TestAcc 0.9235	BestValid 0.9267
	Epoch 1050:	Loss 0.5311	TrainAcc 0.9278	ValidAcc 0.9272	TestAcc 0.9245	BestValid 0.9272
	Epoch 1075:	Loss 0.5231	TrainAcc 0.9280	ValidAcc 0.9281	TestAcc 0.9247	BestValid 0.9281
	Epoch 1100:	Loss 0.5218	TrainAcc 0.9283	ValidAcc 0.9278	TestAcc 0.9246	BestValid 0.9281
	Epoch 1125:	Loss 0.5192	TrainAcc 0.9288	ValidAcc 0.9286	TestAcc 0.9256	BestValid 0.9286
	Epoch 1150:	Loss 0.5130	TrainAcc 0.9286	ValidAcc 0.9283	TestAcc 0.9254	BestValid 0.9286
	Epoch 1175:	Loss 0.5143	TrainAcc 0.9303	ValidAcc 0.9301	TestAcc 0.9266	BestValid 0.9301
	Epoch 1200:	Loss 0.5140	TrainAcc 0.9294	ValidAcc 0.9290	TestAcc 0.9261	BestValid 0.9301
	Epoch 1225:	Loss 0.5080	TrainAcc 0.9302	ValidAcc 0.9296	TestAcc 0.9265	BestValid 0.9301
	Epoch 1250:	Loss 0.5096	TrainAcc 0.9301	ValidAcc 0.9294	TestAcc 0.9265	BestValid 0.9301
	Epoch 1275:	Loss 0.5044	TrainAcc 0.9312	ValidAcc 0.9309	TestAcc 0.9279	BestValid 0.9309
	Epoch 1300:	Loss 0.4994	TrainAcc 0.9320	ValidAcc 0.9316	TestAcc 0.9278	BestValid 0.9316
	Epoch 1325:	Loss 0.5000	TrainAcc 0.9320	ValidAcc 0.9310	TestAcc 0.9283	BestValid 0.9316
	Epoch 1350:	Loss 0.4988	TrainAcc 0.9324	ValidAcc 0.9314	TestAcc 0.9283	BestValid 0.9316
	Epoch 1375:	Loss 0.4930	TrainAcc 0.9328	ValidAcc 0.9325	TestAcc 0.9293	BestValid 0.9325
	Epoch 1400:	Loss 0.4928	TrainAcc 0.9324	ValidAcc 0.9311	TestAcc 0.9287	BestValid 0.9325
	Epoch 1425:	Loss 0.4907	TrainAcc 0.9326	ValidAcc 0.9319	TestAcc 0.9286	BestValid 0.9325
	Epoch 1450:	Loss 0.4909	TrainAcc 0.9334	ValidAcc 0.9322	TestAcc 0.9293	BestValid 0.9325
	Epoch 1475:	Loss 0.4898	TrainAcc 0.9343	ValidAcc 0.9331	TestAcc 0.9299	BestValid 0.9331
	Epoch 1500:	Loss 0.4851	TrainAcc 0.9334	ValidAcc 0.9325	TestAcc 0.9289	BestValid 0.9331
	Epoch 1525:	Loss 0.4811	TrainAcc 0.9342	ValidAcc 0.9334	TestAcc 0.9299	BestValid 0.9334
	Epoch 1550:	Loss 0.4794	TrainAcc 0.9349	ValidAcc 0.9338	TestAcc 0.9307	BestValid 0.9338
	Epoch 1575:	Loss 0.4798	TrainAcc 0.9347	ValidAcc 0.9341	TestAcc 0.9308	BestValid 0.9341
	Epoch 1600:	Loss 0.4819	TrainAcc 0.9347	ValidAcc 0.9337	TestAcc 0.9305	BestValid 0.9341
	Epoch 1625:	Loss 0.4787	TrainAcc 0.9351	ValidAcc 0.9340	TestAcc 0.9307	BestValid 0.9341
	Epoch 1650:	Loss 0.4738	TrainAcc 0.9357	ValidAcc 0.9339	TestAcc 0.9313	BestValid 0.9341
	Epoch 1675:	Loss 0.4722	TrainAcc 0.9367	ValidAcc 0.9358	TestAcc 0.9327	BestValid 0.9358
	Epoch 1700:	Loss 0.4684	TrainAcc 0.9361	ValidAcc 0.9342	TestAcc 0.9316	BestValid 0.9358
	Epoch 1725:	Loss 0.4679	TrainAcc 0.9354	ValidAcc 0.9339	TestAcc 0.9313	BestValid 0.9358
	Epoch 1750:	Loss 0.4672	TrainAcc 0.9372	ValidAcc 0.9360	TestAcc 0.9324	BestValid 0.9360
	Epoch 1775:	Loss 0.4650	TrainAcc 0.9382	ValidAcc 0.9364	TestAcc 0.9335	BestValid 0.9364
	Epoch 1800:	Loss 0.4648	TrainAcc 0.9362	ValidAcc 0.9346	TestAcc 0.9319	BestValid 0.9364
	Epoch 1825:	Loss 0.4618	TrainAcc 0.9372	ValidAcc 0.9353	TestAcc 0.9328	BestValid 0.9364
	Epoch 1850:	Loss 0.4634	TrainAcc 0.9367	ValidAcc 0.9351	TestAcc 0.9324	BestValid 0.9364
	Epoch 1875:	Loss 0.4619	TrainAcc 0.9368	ValidAcc 0.9353	TestAcc 0.9323	BestValid 0.9364
	Epoch 1900:	Loss 0.4594	TrainAcc 0.9358	ValidAcc 0.9337	TestAcc 0.9316	BestValid 0.9364
	Epoch 1925:	Loss 0.4605	TrainAcc 0.9382	ValidAcc 0.9359	TestAcc 0.9336	BestValid 0.9364
	Epoch 1950:	Loss 0.4538	TrainAcc 0.9372	ValidAcc 0.9353	TestAcc 0.9328	BestValid 0.9364
	Epoch 1975:	Loss 0.4547	TrainAcc 0.9390	ValidAcc 0.9370	TestAcc 0.9342	BestValid 0.9370
	Epoch 2000:	Loss 0.4541	TrainAcc 0.9370	ValidAcc 0.9343	TestAcc 0.9326	BestValid 0.9370
	Epoch 2025:	Loss 0.4487	TrainAcc 0.9392	ValidAcc 0.9366	TestAcc 0.9340	BestValid 0.9370
	Epoch 2050:	Loss 0.4505	TrainAcc 0.9403	ValidAcc 0.9377	TestAcc 0.9353	BestValid 0.9377
	Epoch 2075:	Loss 0.4538	TrainAcc 0.9395	ValidAcc 0.9370	TestAcc 0.9343	BestValid 0.9377
	Epoch 2100:	Loss 0.4494	TrainAcc 0.9391	ValidAcc 0.9364	TestAcc 0.9341	BestValid 0.9377
	Epoch 2125:	Loss 0.4475	TrainAcc 0.9396	ValidAcc 0.9364	TestAcc 0.9344	BestValid 0.9377
	Epoch 2150:	Loss 0.4479	TrainAcc 0.9404	ValidAcc 0.9374	TestAcc 0.9356	BestValid 0.9377
	Epoch 2175:	Loss 0.4473	TrainAcc 0.9387	ValidAcc 0.9363	TestAcc 0.9337	BestValid 0.9377
	Epoch 2200:	Loss 0.4444	TrainAcc 0.9389	ValidAcc 0.9356	TestAcc 0.9336	BestValid 0.9377
	Epoch 2225:	Loss 0.4446	TrainAcc 0.9383	ValidAcc 0.9348	TestAcc 0.9328	BestValid 0.9377
	Epoch 2250:	Loss 0.4458	TrainAcc 0.9422	ValidAcc 0.9391	TestAcc 0.9369	BestValid 0.9391
	Epoch 2275:	Loss 0.4391	TrainAcc 0.9391	ValidAcc 0.9359	TestAcc 0.9337	BestValid 0.9391
	Epoch 2300:	Loss 0.4372	TrainAcc 0.9389	ValidAcc 0.9360	TestAcc 0.9339	BestValid 0.9391
	Epoch 2325:	Loss 0.4412	TrainAcc 0.9385	ValidAcc 0.9355	TestAcc 0.9335	BestValid 0.9391
	Epoch 2350:	Loss 0.4374	TrainAcc 0.9412	ValidAcc 0.9384	TestAcc 0.9352	BestValid 0.9391
	Epoch 2375:	Loss 0.4326	TrainAcc 0.9415	ValidAcc 0.9384	TestAcc 0.9364	BestValid 0.9391
	Epoch 2400:	Loss 0.4399	TrainAcc 0.9396	ValidAcc 0.9367	TestAcc 0.9340	BestValid 0.9391
	Epoch 2425:	Loss 0.4352	TrainAcc 0.9381	ValidAcc 0.9357	TestAcc 0.9321	BestValid 0.9391
	Epoch 2450:	Loss 0.4328	TrainAcc 0.9420	ValidAcc 0.9387	TestAcc 0.9362	BestValid 0.9391
	Epoch 2475:	Loss 0.4374	TrainAcc 0.9427	ValidAcc 0.9397	TestAcc 0.9370	BestValid 0.9397
	Epoch 2500:	Loss 0.4323	TrainAcc 0.9373	ValidAcc 0.9345	TestAcc 0.9311	BestValid 0.9397
	Epoch 2525:	Loss 0.4313	TrainAcc 0.9402	ValidAcc 0.9369	TestAcc 0.9343	BestValid 0.9397
	Epoch 2550:	Loss 0.4327	TrainAcc 0.9424	ValidAcc 0.9392	TestAcc 0.9368	BestValid 0.9397
	Epoch 2575:	Loss 0.4324	TrainAcc 0.9420	ValidAcc 0.9383	TestAcc 0.9355	BestValid 0.9397
	Epoch 2600:	Loss 0.4283	TrainAcc 0.9398	ValidAcc 0.9366	TestAcc 0.9335	BestValid 0.9397
	Epoch 2625:	Loss 0.4331	TrainAcc 0.9392	ValidAcc 0.9363	TestAcc 0.9328	BestValid 0.9397
	Epoch 2650:	Loss 0.4271	TrainAcc 0.9445	ValidAcc 0.9414	TestAcc 0.9384	BestValid 0.9414
	Epoch 2675:	Loss 0.4276	TrainAcc 0.9422	ValidAcc 0.9390	TestAcc 0.9355	BestValid 0.9414
	Epoch 2700:	Loss 0.4223	TrainAcc 0.9394	ValidAcc 0.9363	TestAcc 0.9333	BestValid 0.9414
	Epoch 2725:	Loss 0.4262	TrainAcc 0.9426	ValidAcc 0.9392	TestAcc 0.9364	BestValid 0.9414
	Epoch 2750:	Loss 0.4323	TrainAcc 0.9444	ValidAcc 0.9413	TestAcc 0.9381	BestValid 0.9414
	Epoch 2775:	Loss 0.4212	TrainAcc 0.9431	ValidAcc 0.9393	TestAcc 0.9370	BestValid 0.9414
	Epoch 2800:	Loss 0.4262	TrainAcc 0.9408	ValidAcc 0.9379	TestAcc 0.9346	BestValid 0.9414
	Epoch 2825:	Loss 0.4222	TrainAcc 0.9394	ValidAcc 0.9363	TestAcc 0.9329	BestValid 0.9414
	Epoch 2850:	Loss 0.4258	TrainAcc 0.9406	ValidAcc 0.9378	TestAcc 0.9340	BestValid 0.9414
	Epoch 2875:	Loss 0.4228	TrainAcc 0.9450	ValidAcc 0.9415	TestAcc 0.9394	BestValid 0.9415
	Epoch 2900:	Loss 0.4232	TrainAcc 0.9431	ValidAcc 0.9393	TestAcc 0.9362	BestValid 0.9415
	Epoch 2925:	Loss 0.4209	TrainAcc 0.9435	ValidAcc 0.9400	TestAcc 0.9366	BestValid 0.9415
	Epoch 2950:	Loss 0.4188	TrainAcc 0.9428	ValidAcc 0.9395	TestAcc 0.9365	BestValid 0.9415
	Epoch 2975:	Loss 0.4229	TrainAcc 0.9387	ValidAcc 0.9349	TestAcc 0.9318	BestValid 0.9415
	Epoch 3000:	Loss 0.4224	TrainAcc 0.9380	ValidAcc 0.9337	TestAcc 0.9315	BestValid 0.9415
	Epoch 3025:	Loss 0.4191	TrainAcc 0.9413	ValidAcc 0.9376	TestAcc 0.9344	BestValid 0.9415
	Epoch 3050:	Loss 0.4169	TrainAcc 0.9440	ValidAcc 0.9404	TestAcc 0.9377	BestValid 0.9415
	Epoch 3075:	Loss 0.4164	TrainAcc 0.9429	ValidAcc 0.9395	TestAcc 0.9366	BestValid 0.9415
	Epoch 3100:	Loss 0.4145	TrainAcc 0.9370	ValidAcc 0.9333	TestAcc 0.9301	BestValid 0.9415
	Epoch 3125:	Loss 0.4193	TrainAcc 0.9416	ValidAcc 0.9381	TestAcc 0.9349	BestValid 0.9415
	Epoch 3150:	Loss 0.4182	TrainAcc 0.9397	ValidAcc 0.9361	TestAcc 0.9320	BestValid 0.9415
	Epoch 3175:	Loss 0.4163	TrainAcc 0.9451	ValidAcc 0.9412	TestAcc 0.9385	BestValid 0.9415
	Epoch 3200:	Loss 0.4145	TrainAcc 0.9412	ValidAcc 0.9379	TestAcc 0.9350	BestValid 0.9415
	Epoch 3225:	Loss 0.4176	TrainAcc 0.9443	ValidAcc 0.9407	TestAcc 0.9378	BestValid 0.9415
	Epoch 3250:	Loss 0.4155	TrainAcc 0.9461	ValidAcc 0.9421	TestAcc 0.9394	BestValid 0.9421
	Epoch 3275:	Loss 0.4126	TrainAcc 0.9447	ValidAcc 0.9406	TestAcc 0.9381	BestValid 0.9421
	Epoch 3300:	Loss 0.4113	TrainAcc 0.9439	ValidAcc 0.9398	TestAcc 0.9369	BestValid 0.9421
	Epoch 3325:	Loss 0.4073	TrainAcc 0.9426	ValidAcc 0.9383	TestAcc 0.9351	BestValid 0.9421
	Epoch 3350:	Loss 0.4112	TrainAcc 0.9445	ValidAcc 0.9405	TestAcc 0.9371	BestValid 0.9421
	Epoch 3375:	Loss 0.4069	TrainAcc 0.9443	ValidAcc 0.9406	TestAcc 0.9373	BestValid 0.9421
	Epoch 3400:	Loss 0.4065	TrainAcc 0.9380	ValidAcc 0.9327	TestAcc 0.9298	BestValid 0.9421
	Epoch 3425:	Loss 0.4135	TrainAcc 0.9439	ValidAcc 0.9397	TestAcc 0.9366	BestValid 0.9421
	Epoch 3450:	Loss 0.4091	TrainAcc 0.9468	ValidAcc 0.9428	TestAcc 0.9405	BestValid 0.9428
	Epoch 3475:	Loss 0.4073	TrainAcc 0.9454	ValidAcc 0.9418	TestAcc 0.9388	BestValid 0.9428
	Epoch 3500:	Loss 0.4114	TrainAcc 0.9450	ValidAcc 0.9405	TestAcc 0.9387	BestValid 0.9428
	Epoch 3525:	Loss 0.4050	TrainAcc 0.9435	ValidAcc 0.9385	TestAcc 0.9363	BestValid 0.9428
	Epoch 3550:	Loss 0.4063	TrainAcc 0.9453	ValidAcc 0.9413	TestAcc 0.9381	BestValid 0.9428
	Epoch 3575:	Loss 0.4072	TrainAcc 0.9465	ValidAcc 0.9419	TestAcc 0.9398	BestValid 0.9428
	Epoch 3600:	Loss 0.4049	TrainAcc 0.9430	ValidAcc 0.9385	TestAcc 0.9359	BestValid 0.9428
	Epoch 3625:	Loss 0.4060	TrainAcc 0.9422	ValidAcc 0.9372	TestAcc 0.9346	BestValid 0.9428
	Epoch 3650:	Loss 0.4066	TrainAcc 0.9453	ValidAcc 0.9417	TestAcc 0.9377	BestValid 0.9428
	Epoch 3675:	Loss 0.4115	TrainAcc 0.9450	ValidAcc 0.9399	TestAcc 0.9378	BestValid 0.9428
	Epoch 3700:	Loss 0.4030	TrainAcc 0.9417	ValidAcc 0.9364	TestAcc 0.9344	BestValid 0.9428
	Epoch 3725:	Loss 0.4026	TrainAcc 0.9450	ValidAcc 0.9405	TestAcc 0.9377	BestValid 0.9428
	Epoch 3750:	Loss 0.4039	TrainAcc 0.9475	ValidAcc 0.9431	TestAcc 0.9410	BestValid 0.9431
	Epoch 3775:	Loss 0.4077	TrainAcc 0.9464	ValidAcc 0.9424	TestAcc 0.9399	BestValid 0.9431
	Epoch 3800:	Loss 0.4046	TrainAcc 0.9452	ValidAcc 0.9404	TestAcc 0.9379	BestValid 0.9431
	Epoch 3825:	Loss 0.4042	TrainAcc 0.9459	ValidAcc 0.9412	TestAcc 0.9388	BestValid 0.9431
	Epoch 3850:	Loss 0.4052	TrainAcc 0.9472	ValidAcc 0.9424	TestAcc 0.9406	BestValid 0.9431
	Epoch 3875:	Loss 0.4032	TrainAcc 0.9465	ValidAcc 0.9420	TestAcc 0.9402	BestValid 0.9431
	Epoch 3900:	Loss 0.4048	TrainAcc 0.9431	ValidAcc 0.9378	TestAcc 0.9358	BestValid 0.9431
	Epoch 3925:	Loss 0.4029	TrainAcc 0.9427	ValidAcc 0.9368	TestAcc 0.9351	BestValid 0.9431
	Epoch 3950:	Loss 0.4009	TrainAcc 0.9468	ValidAcc 0.9432	TestAcc 0.9400	BestValid 0.9432
	Epoch 3975:	Loss 0.4040	TrainAcc 0.9453	ValidAcc 0.9402	TestAcc 0.9376	BestValid 0.9432
	Epoch 4000:	Loss 0.4014	TrainAcc 0.9455	ValidAcc 0.9398	TestAcc 0.9390	BestValid 0.9432
	Epoch 4025:	Loss 0.4016	TrainAcc 0.9468	ValidAcc 0.9413	TestAcc 0.9399	BestValid 0.9432
	Epoch 4050:	Loss 0.3974	TrainAcc 0.9477	ValidAcc 0.9431	TestAcc 0.9404	BestValid 0.9432
	Epoch 4075:	Loss 0.4013	TrainAcc 0.9469	ValidAcc 0.9420	TestAcc 0.9398	BestValid 0.9432
	Epoch 4100:	Loss 0.3932	TrainAcc 0.9459	ValidAcc 0.9404	TestAcc 0.9391	BestValid 0.9432
	Epoch 4125:	Loss 0.4002	TrainAcc 0.9460	ValidAcc 0.9402	TestAcc 0.9388	BestValid 0.9432
	Epoch 4150:	Loss 0.3942	TrainAcc 0.9476	ValidAcc 0.9425	TestAcc 0.9416	BestValid 0.9432
	Epoch 4175:	Loss 0.3959	TrainAcc 0.9451	ValidAcc 0.9396	TestAcc 0.9375	BestValid 0.9432
	Epoch 4200:	Loss 0.3950	TrainAcc 0.9455	ValidAcc 0.9394	TestAcc 0.9383	BestValid 0.9432
	Epoch 4225:	Loss 0.3998	TrainAcc 0.9439	ValidAcc 0.9381	TestAcc 0.9363	BestValid 0.9432
	Epoch 4250:	Loss 0.3992	TrainAcc 0.9472	ValidAcc 0.9420	TestAcc 0.9402	BestValid 0.9432
	Epoch 4275:	Loss 0.3997	TrainAcc 0.9489	ValidAcc 0.9440	TestAcc 0.9425	BestValid 0.9440
	Epoch 4300:	Loss 0.3930	TrainAcc 0.9417	ValidAcc 0.9355	TestAcc 0.9335	BestValid 0.9440
	Epoch 4325:	Loss 0.3921	TrainAcc 0.9472	ValidAcc 0.9421	TestAcc 0.9403	BestValid 0.9440
	Epoch 4350:	Loss 0.3931	TrainAcc 0.9460	ValidAcc 0.9409	TestAcc 0.9387	BestValid 0.9440
	Epoch 4375:	Loss 0.3968	TrainAcc 0.9481	ValidAcc 0.9421	TestAcc 0.9415	BestValid 0.9440
	Epoch 4400:	Loss 0.3940	TrainAcc 0.9441	ValidAcc 0.9390	TestAcc 0.9370	BestValid 0.9440
	Epoch 4425:	Loss 0.3932	TrainAcc 0.9496	ValidAcc 0.9450	TestAcc 0.9429	BestValid 0.9450
	Epoch 4450:	Loss 0.3972	TrainAcc 0.9483	ValidAcc 0.9431	TestAcc 0.9407	BestValid 0.9450
	Epoch 4475:	Loss 0.3924	TrainAcc 0.9466	ValidAcc 0.9412	TestAcc 0.9398	BestValid 0.9450
	Epoch 4500:	Loss 0.3922	TrainAcc 0.9447	ValidAcc 0.9387	TestAcc 0.9367	BestValid 0.9450
	Epoch 4525:	Loss 0.3968	TrainAcc 0.9466	ValidAcc 0.9403	TestAcc 0.9388	BestValid 0.9450
	Epoch 4550:	Loss 0.3934	TrainAcc 0.9497	ValidAcc 0.9438	TestAcc 0.9428	BestValid 0.9450
	Epoch 4575:	Loss 0.3904	TrainAcc 0.9493	ValidAcc 0.9439	TestAcc 0.9425	BestValid 0.9450
	Epoch 4600:	Loss 0.3936	TrainAcc 0.9451	ValidAcc 0.9397	TestAcc 0.9381	BestValid 0.9450
	Epoch 4625:	Loss 0.3960	TrainAcc 0.9491	ValidAcc 0.9441	TestAcc 0.9421	BestValid 0.9450
	Epoch 4650:	Loss 0.3956	TrainAcc 0.9477	ValidAcc 0.9428	TestAcc 0.9403	BestValid 0.9450
	Epoch 4675:	Loss 0.3940	TrainAcc 0.9492	ValidAcc 0.9437	TestAcc 0.9429	BestValid 0.9450
	Epoch 4700:	Loss 0.3891	TrainAcc 0.9475	ValidAcc 0.9406	TestAcc 0.9398	BestValid 0.9450
	Epoch 4725:	Loss 0.3932	TrainAcc 0.9474	ValidAcc 0.9410	TestAcc 0.9400	BestValid 0.9450
	Epoch 4750:	Loss 0.3897	TrainAcc 0.9502	ValidAcc 0.9444	TestAcc 0.9434	BestValid 0.9450
	Epoch 4775:	Loss 0.3944	TrainAcc 0.9495	ValidAcc 0.9443	TestAcc 0.9427	BestValid 0.9450
	Epoch 4800:	Loss 0.3901	TrainAcc 0.9459	ValidAcc 0.9399	TestAcc 0.9384	BestValid 0.9450
	Epoch 4825:	Loss 0.3887	TrainAcc 0.9497	ValidAcc 0.9444	TestAcc 0.9429	BestValid 0.9450
	Epoch 4850:	Loss 0.3887	TrainAcc 0.9476	ValidAcc 0.9422	TestAcc 0.9410	BestValid 0.9450
	Epoch 4875:	Loss 0.3925	TrainAcc 0.9483	ValidAcc 0.9423	TestAcc 0.9416	BestValid 0.9450
	Epoch 4900:	Loss 0.3889	TrainAcc 0.9465	ValidAcc 0.9407	TestAcc 0.9398	BestValid 0.9450
	Epoch 4925:	Loss 0.3863	TrainAcc 0.9506	ValidAcc 0.9452	TestAcc 0.9441	BestValid 0.9452
	Epoch 4950:	Loss 0.3848	TrainAcc 0.9513	ValidAcc 0.9459	TestAcc 0.9444	BestValid 0.9459
	Epoch 4975:	Loss 0.3899	TrainAcc 0.9460	ValidAcc 0.9400	TestAcc 0.9388	BestValid 0.9459
	Epoch 5000:	Loss 0.3870	TrainAcc 0.9487	ValidAcc 0.9423	TestAcc 0.9416	BestValid 0.9459
Node 2, Pre/Post-Pipelining: 1.093 / 0.869 ms, Bubble: 81.540 ms, Compute: 248.343 ms, Comm: 46.800 ms, Imbalance: 36.518 ms
Node 3, Pre/Post-Pipelining: 1.093 / 0.877 ms, Bubble: 80.373 ms, Compute: 255.170 ms, Comm: 46.470 ms, Imbalance: 30.882 ms
Node 0, Pre/Post-Pipelining: 1.092 / 0.966 ms, Bubble: 80.400 ms, Compute: 286.069 ms, Comm: 32.268 ms, Imbalance: 13.094 ms
Node 1, Pre/Post-Pipelining: 1.096 / 0.859 ms, Bubble: 81.162 ms, Compute: 254.341 ms, Comm: 42.201 ms, Imbalance: 34.968 ms
Node 5, Pre/Post-Pipelining: 1.095 / 0.864 ms, Bubble: 81.541 ms, Compute: 253.516 ms, Comm: 45.004 ms, Imbalance: 33.405 ms
Node 6, Pre/Post-Pipelining: 1.097 / 0.847 ms, Bubble: 82.936 ms, Compute: 253.870 ms, Comm: 41.147 ms, Imbalance: 36.062 ms
Node 4, Pre/Post-Pipelining: 1.092 / 0.868 ms, Bubble: 80.641 ms, Compute: 249.136 ms, Comm: 45.591 ms, Imbalance: 37.743 ms
Node 7, Pre/Post-Pipelining: 1.098 / 16.238 ms, Bubble: 67.639 ms, Compute: 281.082 ms, Comm: 32.036 ms, Imbalance: 16.677 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.092 ms
Cluster-Wide Average, Post-Pipelining Overhead: 0.966 ms
Cluster-Wide Average, Bubble: 80.400 ms
Cluster-Wide Average, Compute: 286.069 ms
Cluster-Wide Average, Communication: 32.268 ms
Cluster-Wide Average, Imbalance: 13.094 ms
Node 0, GPU memory consumption: 8.059 GB
Node 2, GPU memory consumption: 6.042 GB
Node 3, GPU memory consumption: 6.018 GB
Node 6, GPU memory consumption: 6.042 GB
Node 1, GPU memory consumption: 6.042 GB
Node 4, GPU memory consumption: 6.018 GB
Node 5, GPU memory consumption: 6.042 GB
Node 7, GPU memory consumption: 6.171 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.912705 s---------------
------------------------node id 1,  per-epoch time: 0.912705 s---------------
------------------------node id 4,  per-epoch time: 0.912705 s---------------
------------------------node id 2,  per-epoch time: 0.912705 s---------------
------------------------node id 5,  per-epoch time: 0.912705 s---------------
------------------------node id 3,  per-epoch time: 0.912705 s---------------
------------------------node id 6,  per-epoch time: 0.912705 s---------------
------------------------node id 7,  per-epoch time: 0.912705 s---------------
************ Profiling Results ************
	Bubble: 652.818503 (ms) (71.56 percentage)
	Compute: 256.016650 (ms) (28.06 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.481749 (ms) (0.38 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 2.431 GB
	Aggregated layer-level communication throughput: 503.710 Gbps
Highest valid_acc: 0.9459
Target test_acc: 0.9444
Epoch to reach the target acc: 4949
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
