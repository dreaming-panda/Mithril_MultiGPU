gnerv1
Tue Aug  1 00:00:53 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   27C    P8    19W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   27C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 22%] Built target context
[ 36%] Built target core
Scanning dependencies of target cudahelp
[ 38%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 41%] Linking CXX static library libcudahelp.a
[ 77%] Built target cudahelp
[ 80%] Linking CXX executable graphsage
[ 88%] Linking CXX executable gcnii
[ 88%] Linking CXX executable estimate_comm_volume
[ 88%] Linking CXX executable gcn
[ 91%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target OSDI2023_MULTI_NODES_graphsage
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.928 seconds.
Building the CSC structure...
        It takes 2.055 seconds.
Building the CSC structure...
        It takes 2.085 seconds.
Building the CSC structure...
        It takes 2.166 seconds.
Building the CSC structure...
        It takes 2.315 seconds.
Building the CSC structure...
        It takes 2.431 seconds.
Building the CSC structure...
        It takes 2.608 seconds.
Building the CSC structure...
        It takes 2.712 seconds.
Building the CSC structure...
        It takes 1.865 seconds.
        It takes 1.871 seconds.
        It takes 1.865 seconds.
        It takes 1.867 seconds.
        It takes 2.199 seconds.
Building the Feature Vector...
        It takes 2.366 seconds.
        It takes 2.340 seconds.
Building the Feature Vector...
        It takes 0.269 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 2.365 seconds.
        It takes 0.038 seconds.
Building the Feature Vector...
        It takes 0.314 seconds.
Building the Label Vector...
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.035 seconds.
        It takes 0.307 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.315 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Building the Feature Vector...
        It takes 0.292 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.287 seconds.
Building the Label Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.037 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.279 seconds.
Building the Label Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.604 Gbps (per GPU), 452.833 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.334 Gbps (per GPU), 450.670 Gbps (aggregated)
The layer-level communication performance: 56.326 Gbps (per GPU), 450.606 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.118 Gbps (per GPU), 448.945 Gbps (aggregated)
The layer-level communication performance: 56.095 Gbps (per GPU), 448.758 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.914 Gbps (per GPU), 447.315 Gbps (aggregated)
The layer-level communication performance: 55.866 Gbps (per GPU), 446.925 Gbps (aggregated)
The layer-level communication performance: 55.839 Gbps (per GPU), 446.715 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.825 Gbps (per GPU), 1286.596 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.821 Gbps (per GPU), 1286.570 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.704 Gbps (per GPU), 1285.634 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.803 Gbps (per GPU), 1286.424 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.711 Gbps (per GPU), 1285.684 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.803 Gbps (per GPU), 1286.424 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.797 Gbps (per GPU), 1286.374 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.806 Gbps (per GPU), 1286.448 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.532 Gbps (per GPU), 828.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.517 Gbps (per GPU), 828.136 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.531 Gbps (per GPU), 828.245 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.532 Gbps (per GPU), 828.259 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.529 Gbps (per GPU), 828.232 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.528 Gbps (per GPU), 828.225 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.529 Gbps (per GPU), 828.232 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.531 Gbps (per GPU), 828.245 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.874 Gbps (per GPU), 254.993 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.874 Gbps (per GPU), 254.990 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.874 Gbps (per GPU), 254.994 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.873 Gbps (per GPU), 254.988 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.874 Gbps (per GPU), 254.988 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.873 Gbps (per GPU), 254.983 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.874 Gbps (per GPU), 254.992 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.873 Gbps (per GPU), 254.985 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.88ms  2.40ms  2.69ms  3.06  8.38K  3.53M
 chk_1  0.74ms  2.74ms  2.89ms  3.87  6.74K  3.60M
 chk_2  0.79ms  2.60ms  2.76ms  3.52  7.27K  3.53M
 chk_3  0.79ms  2.63ms  2.80ms  3.52  7.92K  3.61M
 chk_4  0.62ms  2.56ms  2.70ms  4.36  5.33K  3.68M
 chk_5  0.99ms  2.56ms  2.91ms  2.93 10.07K  3.45M
 chk_6  0.95ms  2.72ms  2.89ms  3.05  9.41K  3.48M
 chk_7  0.81ms  2.55ms  2.72ms  3.36  8.12K  3.60M
 chk_8  0.66ms  2.66ms  2.82ms  4.25  6.09K  3.64M
 chk_9  1.09ms  2.48ms  2.67ms  2.46 11.10K  3.38M
chk_10  0.64ms  2.70ms  2.85ms  4.44  5.67K  3.63M
chk_11  0.81ms  2.57ms  2.74ms  3.37  8.16K  3.54M
chk_12  0.79ms  2.78ms  3.50ms  4.45  7.24K  3.55M
chk_13  0.62ms  2.61ms  2.69ms  4.33  5.41K  3.68M
chk_14  0.77ms  2.86ms  3.01ms  3.92  7.14K  3.53M
chk_15  0.94ms  2.70ms  2.88ms  3.06  9.25K  3.49M
chk_16  0.59ms  2.53ms  2.66ms  4.51  4.78K  3.77M
chk_17  0.76ms  2.65ms  2.81ms  3.72  6.85K  3.60M
chk_18  0.80ms  2.46ms  2.61ms  3.27  7.47K  3.57M
chk_19  0.60ms  2.51ms  3.71ms  6.19  4.88K  3.75M
chk_20  0.76ms  2.55ms  2.69ms  3.54  7.00K  3.63M
chk_21  0.62ms  2.51ms  2.64ms  4.24  5.41K  3.68M
chk_22  1.08ms  2.75ms  2.94ms  2.72 11.07K  3.39M
chk_23  0.78ms  2.62ms  2.77ms  3.54  7.23K  3.64M
chk_24  1.00ms  2.74ms  2.87ms  2.88 10.13K  3.43M
chk_25  0.72ms  2.52ms  2.66ms  3.71  6.40K  3.57M
chk_26  0.65ms  2.73ms  2.87ms  4.40  5.78K  3.55M
chk_27  0.95ms  2.58ms  2.77ms  2.93  9.34K  3.48M
chk_28  0.72ms  2.89ms  3.04ms  4.25  6.37K  3.57M
chk_29  0.62ms  2.67ms  2.82ms  4.55  5.16K  3.78M
chk_30  0.63ms  2.56ms  2.74ms  4.36  5.44K  3.67M
chk_31  0.71ms  2.71ms  2.83ms  3.97  6.33K  3.63M
   Avg  0.78  2.63  2.84
   Max  1.09  2.89  3.71
   Min  0.59  2.40  2.61
 Ratio  1.84  1.20  1.42
   Var  0.02  0.01  0.05
Profiling takes 2.406 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 361.230 ms
Partition 0 [0, 5) has cost: 361.230 ms
Partition 1 [5, 9) has cost: 336.347 ms
Partition 2 [9, 13) has cost: 336.347 ms
Partition 3 [13, 17) has cost: 336.347 ms
Partition 4 [17, 21) has cost: 336.347 ms
Partition 5 [21, 25) has cost: 336.347 ms
Partition 6 [25, 29) has cost: 336.347 ms
Partition 7 [29, 33) has cost: 343.234 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.342 ms
GPU 0, Compute+Comm Time: 131.636 ms, Bubble Time: 28.726 ms, Imbalance Overhead: 3.979 ms
GPU 1, Compute+Comm Time: 124.694 ms, Bubble Time: 28.405 ms, Imbalance Overhead: 11.244 ms
GPU 2, Compute+Comm Time: 124.694 ms, Bubble Time: 28.421 ms, Imbalance Overhead: 11.227 ms
GPU 3, Compute+Comm Time: 124.694 ms, Bubble Time: 28.298 ms, Imbalance Overhead: 11.350 ms
GPU 4, Compute+Comm Time: 124.694 ms, Bubble Time: 28.196 ms, Imbalance Overhead: 11.452 ms
GPU 5, Compute+Comm Time: 124.694 ms, Bubble Time: 28.241 ms, Imbalance Overhead: 11.407 ms
GPU 6, Compute+Comm Time: 124.694 ms, Bubble Time: 28.485 ms, Imbalance Overhead: 11.163 ms
GPU 7, Compute+Comm Time: 126.339 ms, Bubble Time: 28.870 ms, Imbalance Overhead: 9.132 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 315.911 ms
GPU 0, Compute+Comm Time: 243.235 ms, Bubble Time: 56.203 ms, Imbalance Overhead: 16.473 ms
GPU 1, Compute+Comm Time: 237.994 ms, Bubble Time: 55.386 ms, Imbalance Overhead: 22.531 ms
GPU 2, Compute+Comm Time: 237.994 ms, Bubble Time: 54.842 ms, Imbalance Overhead: 23.076 ms
GPU 3, Compute+Comm Time: 237.994 ms, Bubble Time: 54.726 ms, Imbalance Overhead: 23.191 ms
GPU 4, Compute+Comm Time: 237.994 ms, Bubble Time: 54.863 ms, Imbalance Overhead: 23.055 ms
GPU 5, Compute+Comm Time: 237.994 ms, Bubble Time: 55.022 ms, Imbalance Overhead: 22.895 ms
GPU 6, Compute+Comm Time: 237.994 ms, Bubble Time: 54.860 ms, Imbalance Overhead: 23.057 ms
GPU 7, Compute+Comm Time: 255.934 ms, Bubble Time: 55.391 ms, Imbalance Overhead: 4.586 ms
The estimated cost of the whole pipeline: 504.266 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 697.577 ms
Partition 0 [0, 9) has cost: 697.577 ms
Partition 1 [9, 17) has cost: 672.694 ms
Partition 2 [17, 25) has cost: 672.694 ms
Partition 3 [25, 33) has cost: 679.581 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.667 ms
GPU 0, Compute+Comm Time: 140.672 ms, Bubble Time: 26.620 ms, Imbalance Overhead: 3.374 ms
GPU 1, Compute+Comm Time: 136.909 ms, Bubble Time: 26.173 ms, Imbalance Overhead: 7.584 ms
GPU 2, Compute+Comm Time: 136.909 ms, Bubble Time: 26.045 ms, Imbalance Overhead: 7.712 ms
GPU 3, Compute+Comm Time: 137.948 ms, Bubble Time: 25.756 ms, Imbalance Overhead: 6.963 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 315.328 ms
GPU 0, Compute+Comm Time: 255.092 ms, Bubble Time: 48.039 ms, Imbalance Overhead: 12.197 ms
GPU 1, Compute+Comm Time: 251.919 ms, Bubble Time: 48.295 ms, Imbalance Overhead: 15.114 ms
GPU 2, Compute+Comm Time: 251.919 ms, Bubble Time: 48.461 ms, Imbalance Overhead: 14.947 ms
GPU 3, Compute+Comm Time: 261.837 ms, Bubble Time: 49.520 ms, Imbalance Overhead: 3.970 ms
    The estimated cost with 2 DP ways is 510.294 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1370.272 ms
Partition 0 [0, 17) has cost: 1370.272 ms
Partition 1 [17, 33) has cost: 1352.275 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 215.903 ms
GPU 0, Compute+Comm Time: 187.972 ms, Bubble Time: 23.039 ms, Imbalance Overhead: 4.892 ms
GPU 1, Compute+Comm Time: 186.676 ms, Bubble Time: 23.946 ms, Imbalance Overhead: 5.282 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 352.795 ms
GPU 0, Compute+Comm Time: 306.257 ms, Bubble Time: 39.197 ms, Imbalance Overhead: 7.341 ms
GPU 1, Compute+Comm Time: 309.767 ms, Bubble Time: 37.731 ms, Imbalance Overhead: 5.297 ms
    The estimated cost with 4 DP ways is 597.133 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2722.547 ms
Partition 0 [0, 33) has cost: 2722.547 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 588.973 ms
GPU 0, Compute+Comm Time: 588.973 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 712.541 ms
GPU 0, Compute+Comm Time: 712.541 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1366.590 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.4409	TrainAcc 0.5059	ValidAcc 0.5252	TestAcc 0.5221	BestValid 0.5252
	Epoch 50:	Loss 1.7892	TrainAcc 0.6894	ValidAcc 0.7108	TestAcc 0.7050	BestValid 0.7108
	Epoch 75:	Loss 1.4405	TrainAcc 0.7489	ValidAcc 0.7669	TestAcc 0.7605	BestValid 0.7669
	Epoch 100:	Loss 1.2431	TrainAcc 0.7898	ValidAcc 0.8033	TestAcc 0.7973	BestValid 0.8033
	Epoch 125:	Loss 1.1088	TrainAcc 0.8154	ValidAcc 0.8265	TestAcc 0.8223	BestValid 0.8265
	Epoch 150:	Loss 1.0188	TrainAcc 0.8341	ValidAcc 0.8442	TestAcc 0.8385	BestValid 0.8442
	Epoch 175:	Loss 0.9534	TrainAcc 0.8474	ValidAcc 0.8556	TestAcc 0.8499	BestValid 0.8556
	Epoch 200:	Loss 0.8988	TrainAcc 0.8580	ValidAcc 0.8659	TestAcc 0.8602	BestValid 0.8659
	Epoch 225:	Loss 0.8563	TrainAcc 0.8670	ValidAcc 0.8743	TestAcc 0.8695	BestValid 0.8743
	Epoch 250:	Loss 0.8257	TrainAcc 0.8731	ValidAcc 0.8804	TestAcc 0.8758	BestValid 0.8804
	Epoch 275:	Loss 0.7966	TrainAcc 0.8790	ValidAcc 0.8855	TestAcc 0.8811	BestValid 0.8855
	Epoch 300:	Loss 0.7734	TrainAcc 0.8844	ValidAcc 0.8891	TestAcc 0.8852	BestValid 0.8891
	Epoch 325:	Loss 0.7522	TrainAcc 0.8886	ValidAcc 0.8930	TestAcc 0.8898	BestValid 0.8930
	Epoch 350:	Loss 0.7336	TrainAcc 0.8914	ValidAcc 0.8953	TestAcc 0.8921	BestValid 0.8953
	Epoch 375:	Loss 0.7113	TrainAcc 0.8952	ValidAcc 0.8982	TestAcc 0.8954	BestValid 0.8982
	Epoch 400:	Loss 0.6995	TrainAcc 0.8974	ValidAcc 0.9006	TestAcc 0.8975	BestValid 0.9006
	Epoch 425:	Loss 0.6873	TrainAcc 0.9000	ValidAcc 0.9026	TestAcc 0.8997	BestValid 0.9026
	Epoch 450:	Loss 0.6760	TrainAcc 0.9019	ValidAcc 0.9041	TestAcc 0.9015	BestValid 0.9041
	Epoch 475:	Loss 0.6632	TrainAcc 0.9046	ValidAcc 0.9066	TestAcc 0.9036	BestValid 0.9066
	Epoch 500:	Loss 0.6555	TrainAcc 0.9066	ValidAcc 0.9085	TestAcc 0.9054	BestValid 0.9085
	Epoch 525:	Loss 0.6460	TrainAcc 0.9083	ValidAcc 0.9106	TestAcc 0.9069	BestValid 0.9106
	Epoch 550:	Loss 0.6379	TrainAcc 0.9096	ValidAcc 0.9108	TestAcc 0.9079	BestValid 0.9108
	Epoch 575:	Loss 0.6267	TrainAcc 0.9108	ValidAcc 0.9122	TestAcc 0.9091	BestValid 0.9122
	Epoch 600:	Loss 0.6166	TrainAcc 0.9124	ValidAcc 0.9137	TestAcc 0.9105	BestValid 0.9137
	Epoch 625:	Loss 0.6053	TrainAcc 0.9127	ValidAcc 0.9146	TestAcc 0.9106	BestValid 0.9146
	Epoch 650:	Loss 0.6023	TrainAcc 0.9146	ValidAcc 0.9162	TestAcc 0.9124	BestValid 0.9162
	Epoch 675:	Loss 0.5989	TrainAcc 0.9160	ValidAcc 0.9176	TestAcc 0.9138	BestValid 0.9176
	Epoch 700:	Loss 0.5916	TrainAcc 0.9172	ValidAcc 0.9180	TestAcc 0.9147	BestValid 0.9180
	Epoch 725:	Loss 0.5863	TrainAcc 0.9185	ValidAcc 0.9196	TestAcc 0.9159	BestValid 0.9196
	Epoch 750:	Loss 0.5806	TrainAcc 0.9196	ValidAcc 0.9204	TestAcc 0.9169	BestValid 0.9204
	Epoch 775:	Loss 0.5771	TrainAcc 0.9202	ValidAcc 0.9215	TestAcc 0.9177	BestValid 0.9215
	Epoch 800:	Loss 0.5753	TrainAcc 0.9211	ValidAcc 0.9220	TestAcc 0.9186	BestValid 0.9220
	Epoch 825:	Loss 0.5675	TrainAcc 0.9212	ValidAcc 0.9218	TestAcc 0.9185	BestValid 0.9220
	Epoch 850:	Loss 0.5591	TrainAcc 0.9222	ValidAcc 0.9228	TestAcc 0.9193	BestValid 0.9228
	Epoch 875:	Loss 0.5627	TrainAcc 0.9225	ValidAcc 0.9228	TestAcc 0.9196	BestValid 0.9228
	Epoch 900:	Loss 0.5517	TrainAcc 0.9230	ValidAcc 0.9232	TestAcc 0.9198	BestValid 0.9232
	Epoch 925:	Loss 0.5455	TrainAcc 0.9235	ValidAcc 0.9235	TestAcc 0.9206	BestValid 0.9235
	Epoch 950:	Loss 0.5419	TrainAcc 0.9245	ValidAcc 0.9246	TestAcc 0.9211	BestValid 0.9246
	Epoch 975:	Loss 0.5426	TrainAcc 0.9253	ValidAcc 0.9253	TestAcc 0.9217	BestValid 0.9253
	Epoch 1000:	Loss 0.5390	TrainAcc 0.9257	ValidAcc 0.9259	TestAcc 0.9225	BestValid 0.9259
	Epoch 1025:	Loss 0.5368	TrainAcc 0.9262	ValidAcc 0.9264	TestAcc 0.9229	BestValid 0.9264
	Epoch 1050:	Loss 0.5314	TrainAcc 0.9275	ValidAcc 0.9277	TestAcc 0.9239	BestValid 0.9277
	Epoch 1075:	Loss 0.5284	TrainAcc 0.9277	ValidAcc 0.9278	TestAcc 0.9245	BestValid 0.9278
	Epoch 1100:	Loss 0.5242	TrainAcc 0.9286	ValidAcc 0.9290	TestAcc 0.9249	BestValid 0.9290
	Epoch 1125:	Loss 0.5170	TrainAcc 0.9294	ValidAcc 0.9292	TestAcc 0.9256	BestValid 0.9292
	Epoch 1150:	Loss 0.5190	TrainAcc 0.9294	ValidAcc 0.9296	TestAcc 0.9258	BestValid 0.9296
	Epoch 1175:	Loss 0.5155	TrainAcc 0.9300	ValidAcc 0.9301	TestAcc 0.9263	BestValid 0.9301
	Epoch 1200:	Loss 0.5143	TrainAcc 0.9308	ValidAcc 0.9304	TestAcc 0.9268	BestValid 0.9304
	Epoch 1225:	Loss 0.5110	TrainAcc 0.9310	ValidAcc 0.9307	TestAcc 0.9274	BestValid 0.9307
	Epoch 1250:	Loss 0.5100	TrainAcc 0.9317	ValidAcc 0.9315	TestAcc 0.9278	BestValid 0.9315
	Epoch 1275:	Loss 0.5046	TrainAcc 0.9322	ValidAcc 0.9314	TestAcc 0.9284	BestValid 0.9315
	Epoch 1300:	Loss 0.5041	TrainAcc 0.9323	ValidAcc 0.9318	TestAcc 0.9284	BestValid 0.9318
	Epoch 1325:	Loss 0.5013	TrainAcc 0.9326	ValidAcc 0.9316	TestAcc 0.9283	BestValid 0.9318
	Epoch 1350:	Loss 0.4928	TrainAcc 0.9324	ValidAcc 0.9316	TestAcc 0.9287	BestValid 0.9318
	Epoch 1375:	Loss 0.4961	TrainAcc 0.9326	ValidAcc 0.9311	TestAcc 0.9285	BestValid 0.9318
	Epoch 1400:	Loss 0.4896	TrainAcc 0.9318	ValidAcc 0.9303	TestAcc 0.9275	BestValid 0.9318
	Epoch 1425:	Loss 0.4895	TrainAcc 0.9318	ValidAcc 0.9308	TestAcc 0.9276	BestValid 0.9318
	Epoch 1450:	Loss 0.4915	TrainAcc 0.9311	ValidAcc 0.9299	TestAcc 0.9269	BestValid 0.9318
	Epoch 1475:	Loss 0.4966	TrainAcc 0.9325	ValidAcc 0.9315	TestAcc 0.9283	BestValid 0.9318
	Epoch 1500:	Loss 0.4959	TrainAcc 0.9350	ValidAcc 0.9340	TestAcc 0.9306	BestValid 0.9340
	Epoch 1525:	Loss 0.5063	TrainAcc 0.9344	ValidAcc 0.9333	TestAcc 0.9296	BestValid 0.9340
	Epoch 1550:	Loss 0.4869	TrainAcc 0.9325	ValidAcc 0.9324	TestAcc 0.9278	BestValid 0.9340
	Epoch 1575:	Loss 0.4790	TrainAcc 0.9358	ValidAcc 0.9348	TestAcc 0.9318	BestValid 0.9348
	Epoch 1600:	Loss 0.4897	TrainAcc 0.9306	ValidAcc 0.9293	TestAcc 0.9256	BestValid 0.9348
	Epoch 1625:	Loss 0.4747	TrainAcc 0.9285	ValidAcc 0.9278	TestAcc 0.9244	BestValid 0.9348
	Epoch 1650:	Loss 0.4784	TrainAcc 0.9353	ValidAcc 0.9342	TestAcc 0.9307	BestValid 0.9348
	Epoch 1675:	Loss 0.4789	TrainAcc 0.9371	ValidAcc 0.9360	TestAcc 0.9330	BestValid 0.9360
	Epoch 1700:	Loss 0.4712	TrainAcc 0.9376	ValidAcc 0.9360	TestAcc 0.9335	BestValid 0.9360
	Epoch 1725:	Loss 0.4715	TrainAcc 0.9357	ValidAcc 0.9342	TestAcc 0.9309	BestValid 0.9360
	Epoch 1750:	Loss 0.4689	TrainAcc 0.9333	ValidAcc 0.9318	TestAcc 0.9286	BestValid 0.9360
	Epoch 1775:	Loss 0.4704	TrainAcc 0.9355	ValidAcc 0.9340	TestAcc 0.9307	BestValid 0.9360
	Epoch 1800:	Loss 0.4667	TrainAcc 0.9381	ValidAcc 0.9360	TestAcc 0.9333	BestValid 0.9360
	Epoch 1825:	Loss 0.4686	TrainAcc 0.9386	ValidAcc 0.9368	TestAcc 0.9341	BestValid 0.9368
	Epoch 1850:	Loss 0.4598	TrainAcc 0.9380	ValidAcc 0.9359	TestAcc 0.9329	BestValid 0.9368
	Epoch 1875:	Loss 0.4636	TrainAcc 0.9367	ValidAcc 0.9349	TestAcc 0.9317	BestValid 0.9368
	Epoch 1900:	Loss 0.4585	TrainAcc 0.9359	ValidAcc 0.9340	TestAcc 0.9311	BestValid 0.9368
	Epoch 1925:	Loss 0.4573	TrainAcc 0.9379	ValidAcc 0.9360	TestAcc 0.9328	BestValid 0.9368
	Epoch 1950:	Loss 0.4602	TrainAcc 0.9396	ValidAcc 0.9374	TestAcc 0.9347	BestValid 0.9374
	Epoch 1975:	Loss 0.4546	TrainAcc 0.9399	ValidAcc 0.9377	TestAcc 0.9351	BestValid 0.9377
	Epoch 2000:	Loss 0.4559	TrainAcc 0.9396	ValidAcc 0.9374	TestAcc 0.9344	BestValid 0.9377
	Epoch 2025:	Loss 0.4540	TrainAcc 0.9384	ValidAcc 0.9361	TestAcc 0.9334	BestValid 0.9377
	Epoch 2050:	Loss 0.4560	TrainAcc 0.9369	ValidAcc 0.9348	TestAcc 0.9318	BestValid 0.9377
	Epoch 2075:	Loss 0.4557	TrainAcc 0.9369	ValidAcc 0.9348	TestAcc 0.9317	BestValid 0.9377
	Epoch 2100:	Loss 0.4524	TrainAcc 0.9391	ValidAcc 0.9368	TestAcc 0.9341	BestValid 0.9377
	Epoch 2125:	Loss 0.4575	TrainAcc 0.9409	ValidAcc 0.9387	TestAcc 0.9361	BestValid 0.9387
	Epoch 2150:	Loss 0.4559	TrainAcc 0.9411	ValidAcc 0.9392	TestAcc 0.9360	BestValid 0.9392
	Epoch 2175:	Loss 0.4495	TrainAcc 0.9413	ValidAcc 0.9399	TestAcc 0.9362	BestValid 0.9399
	Epoch 2200:	Loss 0.4533	TrainAcc 0.9406	ValidAcc 0.9379	TestAcc 0.9353	BestValid 0.9399
	Epoch 2225:	Loss 0.4529	TrainAcc 0.9376	ValidAcc 0.9353	TestAcc 0.9325	BestValid 0.9399
	Epoch 2250:	Loss 0.4482	TrainAcc 0.9350	ValidAcc 0.9331	TestAcc 0.9297	BestValid 0.9399
	Epoch 2275:	Loss 0.4427	TrainAcc 0.9367	ValidAcc 0.9347	TestAcc 0.9314	BestValid 0.9399
	Epoch 2300:	Loss 0.4549	TrainAcc 0.9415	ValidAcc 0.9389	TestAcc 0.9362	BestValid 0.9399
	Epoch 2325:	Loss 0.4566	TrainAcc 0.9413	ValidAcc 0.9396	TestAcc 0.9356	BestValid 0.9399
	Epoch 2350:	Loss 0.4409	TrainAcc 0.9418	ValidAcc 0.9402	TestAcc 0.9361	BestValid 0.9402
	Epoch 2375:	Loss 0.4500	TrainAcc 0.9407	ValidAcc 0.9383	TestAcc 0.9353	BestValid 0.9402
	Epoch 2400:	Loss 0.4543	TrainAcc 0.9346	ValidAcc 0.9319	TestAcc 0.9290	BestValid 0.9402
	Epoch 2425:	Loss 0.4355	TrainAcc 0.9375	ValidAcc 0.9355	TestAcc 0.9321	BestValid 0.9402
	Epoch 2450:	Loss 0.4445	TrainAcc 0.9424	ValidAcc 0.9398	TestAcc 0.9372	BestValid 0.9402
	Epoch 2475:	Loss 0.4427	TrainAcc 0.9428	ValidAcc 0.9412	TestAcc 0.9372	BestValid 0.9412
	Epoch 2500:	Loss 0.4346	TrainAcc 0.9425	ValidAcc 0.9398	TestAcc 0.9368	BestValid 0.9412
	Epoch 2525:	Loss 0.4451	TrainAcc 0.9395	ValidAcc 0.9368	TestAcc 0.9338	BestValid 0.9412
	Epoch 2550:	Loss 0.4341	TrainAcc 0.9392	ValidAcc 0.9366	TestAcc 0.9337	BestValid 0.9412
	Epoch 2575:	Loss 0.4361	TrainAcc 0.9429	ValidAcc 0.9394	TestAcc 0.9373	BestValid 0.9412
	Epoch 2600:	Loss 0.4357	TrainAcc 0.9437	ValidAcc 0.9413	TestAcc 0.9379	BestValid 0.9413
	Epoch 2625:	Loss 0.4349	TrainAcc 0.9437	ValidAcc 0.9407	TestAcc 0.9384	BestValid 0.9413
	Epoch 2650:	Loss 0.4348	TrainAcc 0.9421	ValidAcc 0.9387	TestAcc 0.9362	BestValid 0.9413
	Epoch 2675:	Loss 0.4280	TrainAcc 0.9401	ValidAcc 0.9371	TestAcc 0.9342	BestValid 0.9413
	Epoch 2700:	Loss 0.4287	TrainAcc 0.9422	ValidAcc 0.9392	TestAcc 0.9362	BestValid 0.9413
	Epoch 2725:	Loss 0.4363	TrainAcc 0.9441	ValidAcc 0.9407	TestAcc 0.9384	BestValid 0.9413
	Epoch 2750:	Loss 0.4308	TrainAcc 0.9446	ValidAcc 0.9420	TestAcc 0.9387	BestValid 0.9420
	Epoch 2775:	Loss 0.4265	TrainAcc 0.9445	ValidAcc 0.9412	TestAcc 0.9386	BestValid 0.9420
	Epoch 2800:	Loss 0.4305	TrainAcc 0.9430	ValidAcc 0.9395	TestAcc 0.9369	BestValid 0.9420
	Epoch 2825:	Loss 0.4230	TrainAcc 0.9418	ValidAcc 0.9387	TestAcc 0.9359	BestValid 0.9420
	Epoch 2850:	Loss 0.4236	TrainAcc 0.9426	ValidAcc 0.9392	TestAcc 0.9368	BestValid 0.9420
	Epoch 2875:	Loss 0.4280	TrainAcc 0.9443	ValidAcc 0.9407	TestAcc 0.9387	BestValid 0.9420
	Epoch 2900:	Loss 0.4319	TrainAcc 0.9450	ValidAcc 0.9421	TestAcc 0.9389	BestValid 0.9421
	Epoch 2925:	Loss 0.4237	TrainAcc 0.9452	ValidAcc 0.9425	TestAcc 0.9388	BestValid 0.9425
	Epoch 2950:	Loss 0.4254	TrainAcc 0.9451	ValidAcc 0.9418	TestAcc 0.9391	BestValid 0.9425
	Epoch 2975:	Loss 0.4244	TrainAcc 0.9436	ValidAcc 0.9404	TestAcc 0.9373	BestValid 0.9425
	Epoch 3000:	Loss 0.4283	TrainAcc 0.9402	ValidAcc 0.9371	TestAcc 0.9341	BestValid 0.9425
	Epoch 3025:	Loss 0.4200	TrainAcc 0.9402	ValidAcc 0.9371	TestAcc 0.9342	BestValid 0.9425
	Epoch 3050:	Loss 0.4214	TrainAcc 0.9432	ValidAcc 0.9397	TestAcc 0.9372	BestValid 0.9425
	Epoch 3075:	Loss 0.4329	TrainAcc 0.9455	ValidAcc 0.9424	TestAcc 0.9394	BestValid 0.9425
	Epoch 3100:	Loss 0.4313	TrainAcc 0.9443	ValidAcc 0.9421	TestAcc 0.9377	BestValid 0.9425
	Epoch 3125:	Loss 0.4136	TrainAcc 0.9456	ValidAcc 0.9428	TestAcc 0.9396	BestValid 0.9428
	Epoch 3150:	Loss 0.4312	TrainAcc 0.9430	ValidAcc 0.9392	TestAcc 0.9364	BestValid 0.9428
	Epoch 3175:	Loss 0.4226	TrainAcc 0.9360	ValidAcc 0.9323	TestAcc 0.9294	BestValid 0.9428
	Epoch 3200:	Loss 0.4196	TrainAcc 0.9430	ValidAcc 0.9390	TestAcc 0.9367	BestValid 0.9428
	Epoch 3225:	Loss 0.4345	TrainAcc 0.9454	ValidAcc 0.9427	TestAcc 0.9391	BestValid 0.9428
	Epoch 3250:	Loss 0.4140	TrainAcc 0.9459	ValidAcc 0.9434	TestAcc 0.9393	BestValid 0.9434
	Epoch 3275:	Loss 0.4176	TrainAcc 0.9446	ValidAcc 0.9406	TestAcc 0.9376	BestValid 0.9434
	Epoch 3300:	Loss 0.4135	TrainAcc 0.9427	ValidAcc 0.9384	TestAcc 0.9360	BestValid 0.9434
	Epoch 3325:	Loss 0.4107	TrainAcc 0.9451	ValidAcc 0.9407	TestAcc 0.9384	BestValid 0.9434
	Epoch 3350:	Loss 0.4167	TrainAcc 0.9468	ValidAcc 0.9429	TestAcc 0.9401	BestValid 0.9434
	Epoch 3375:	Loss 0.4116	TrainAcc 0.9468	ValidAcc 0.9436	TestAcc 0.9404	BestValid 0.9436
	Epoch 3400:	Loss 0.4145	TrainAcc 0.9467	ValidAcc 0.9429	TestAcc 0.9396	BestValid 0.9436
	Epoch 3425:	Loss 0.4115	TrainAcc 0.9450	ValidAcc 0.9402	TestAcc 0.9380	BestValid 0.9436
	Epoch 3450:	Loss 0.4078	TrainAcc 0.9447	ValidAcc 0.9412	TestAcc 0.9383	BestValid 0.9436
	Epoch 3475:	Loss 0.4089	TrainAcc 0.9465	ValidAcc 0.9418	TestAcc 0.9400	BestValid 0.9436
	Epoch 3500:	Loss 0.4115	TrainAcc 0.9473	ValidAcc 0.9438	TestAcc 0.9405	BestValid 0.9438
	Epoch 3525:	Loss 0.4064	TrainAcc 0.9473	ValidAcc 0.9436	TestAcc 0.9405	BestValid 0.9438
	Epoch 3550:	Loss 0.4099	TrainAcc 0.9466	ValidAcc 0.9422	TestAcc 0.9396	BestValid 0.9438
	Epoch 3575:	Loss 0.4102	TrainAcc 0.9450	ValidAcc 0.9402	TestAcc 0.9378	BestValid 0.9438
	Epoch 3600:	Loss 0.4057	TrainAcc 0.9456	ValidAcc 0.9411	TestAcc 0.9383	BestValid 0.9438
	Epoch 3625:	Loss 0.4050	TrainAcc 0.9466	ValidAcc 0.9421	TestAcc 0.9400	BestValid 0.9438
	Epoch 3650:	Loss 0.4102	TrainAcc 0.9475	ValidAcc 0.9433	TestAcc 0.9408	BestValid 0.9438
	Epoch 3675:	Loss 0.4046	TrainAcc 0.9481	ValidAcc 0.9440	TestAcc 0.9412	BestValid 0.9440
	Epoch 3700:	Loss 0.4091	TrainAcc 0.9477	ValidAcc 0.9434	TestAcc 0.9407	BestValid 0.9440
	Epoch 3725:	Loss 0.4027	TrainAcc 0.9466	ValidAcc 0.9425	TestAcc 0.9394	BestValid 0.9440
	Epoch 3750:	Loss 0.4038	TrainAcc 0.9466	ValidAcc 0.9422	TestAcc 0.9396	BestValid 0.9440
	Epoch 3775:	Loss 0.4040	TrainAcc 0.9464	ValidAcc 0.9422	TestAcc 0.9394	BestValid 0.9440
	Epoch 3800:	Loss 0.4023	TrainAcc 0.9476	ValidAcc 0.9435	TestAcc 0.9408	BestValid 0.9440
	Epoch 3825:	Loss 0.4015	TrainAcc 0.9483	ValidAcc 0.9440	TestAcc 0.9415	BestValid 0.9440
	Epoch 3850:	Loss 0.3993	TrainAcc 0.9487	ValidAcc 0.9442	TestAcc 0.9415	BestValid 0.9442
	Epoch 3875:	Loss 0.4004	TrainAcc 0.9483	ValidAcc 0.9444	TestAcc 0.9413	BestValid 0.9444
	Epoch 3900:	Loss 0.4025	TrainAcc 0.9480	ValidAcc 0.9437	TestAcc 0.9410	BestValid 0.9444
	Epoch 3925:	Loss 0.4088	TrainAcc 0.9466	ValidAcc 0.9415	TestAcc 0.9395	BestValid 0.9444
	Epoch 3950:	Loss 0.4126	TrainAcc 0.9407	ValidAcc 0.9360	TestAcc 0.9340	BestValid 0.9444
	Epoch 3975:	Loss 0.4156	TrainAcc 0.9343	ValidAcc 0.9293	TestAcc 0.9279	BestValid 0.9444
	Epoch 4000:	Loss 0.4035	TrainAcc 0.9397	ValidAcc 0.9351	TestAcc 0.9338	BestValid 0.9444
	Epoch 4025:	Loss 0.4330	TrainAcc 0.9459	ValidAcc 0.9422	TestAcc 0.9392	BestValid 0.9444
	Epoch 4050:	Loss 0.4291	TrainAcc 0.9410	ValidAcc 0.9374	TestAcc 0.9336	BestValid 0.9444
	Epoch 4075:	Loss 0.4082	TrainAcc 0.9475	ValidAcc 0.9435	TestAcc 0.9404	BestValid 0.9444
	Epoch 4100:	Loss 0.4189	TrainAcc 0.9415	ValidAcc 0.9361	TestAcc 0.9341	BestValid 0.9444
	Epoch 4125:	Loss 0.3918	TrainAcc 0.9452	ValidAcc 0.9411	TestAcc 0.9384	BestValid 0.9444
	Epoch 4150:	Loss 0.4008	TrainAcc 0.9480	ValidAcc 0.9449	TestAcc 0.9412	BestValid 0.9449
	Epoch 4175:	Loss 0.3979	TrainAcc 0.9491	ValidAcc 0.9445	TestAcc 0.9419	BestValid 0.9449
	Epoch 4200:	Loss 0.3938	TrainAcc 0.9473	ValidAcc 0.9423	TestAcc 0.9399	BestValid 0.9449
	Epoch 4225:	Loss 0.3926	TrainAcc 0.9480	ValidAcc 0.9436	TestAcc 0.9408	BestValid 0.9449
	Epoch 4250:	Loss 0.3917	TrainAcc 0.9491	ValidAcc 0.9451	TestAcc 0.9420	BestValid 0.9451
	Epoch 4275:	Loss 0.3948	TrainAcc 0.9494	ValidAcc 0.9446	TestAcc 0.9422	BestValid 0.9451
	Epoch 4300:	Loss 0.3913	TrainAcc 0.9488	ValidAcc 0.9436	TestAcc 0.9415	BestValid 0.9451
	Epoch 4325:	Loss 0.3891	TrainAcc 0.9487	ValidAcc 0.9437	TestAcc 0.9416	BestValid 0.9451
	Epoch 4350:	Loss 0.3882	TrainAcc 0.9488	ValidAcc 0.9436	TestAcc 0.9415	BestValid 0.9451
	Epoch 4375:	Loss 0.3889	TrainAcc 0.9491	ValidAcc 0.9439	TestAcc 0.9420	BestValid 0.9451
	Epoch 4400:	Loss 0.3893	TrainAcc 0.9496	ValidAcc 0.9444	TestAcc 0.9425	BestValid 0.9451
	Epoch 4425:	Loss 0.3889	TrainAcc 0.9500	ValidAcc 0.9448	TestAcc 0.9427	BestValid 0.9451
	Epoch 4450:	Loss 0.3877	TrainAcc 0.9495	ValidAcc 0.9444	TestAcc 0.9422	BestValid 0.9451
	Epoch 4475:	Loss 0.3853	TrainAcc 0.9493	ValidAcc 0.9439	TestAcc 0.9417	BestValid 0.9451
	Epoch 4500:	Loss 0.3913	TrainAcc 0.9491	ValidAcc 0.9446	TestAcc 0.9419	BestValid 0.9451
	Epoch 4525:	Loss 0.3891	TrainAcc 0.9494	ValidAcc 0.9445	TestAcc 0.9420	BestValid 0.9451
	Epoch 4550:	Loss 0.3945	TrainAcc 0.9495	ValidAcc 0.9447	TestAcc 0.9424	BestValid 0.9451
	Epoch 4575:	Loss 0.3893	TrainAcc 0.9493	ValidAcc 0.9439	TestAcc 0.9418	BestValid 0.9451
	Epoch 4600:	Loss 0.3879	TrainAcc 0.9486	ValidAcc 0.9431	TestAcc 0.9411	BestValid 0.9451
	Epoch 4625:	Loss 0.3802	TrainAcc 0.9493	ValidAcc 0.9441	TestAcc 0.9420	BestValid 0.9451
	Epoch 4650:	Loss 0.3892	TrainAcc 0.9490	ValidAcc 0.9441	TestAcc 0.9416	BestValid 0.9451
	Epoch 4675:	Loss 0.3947	TrainAcc 0.9474	ValidAcc 0.9430	TestAcc 0.9409	BestValid 0.9451
	Epoch 4700:	Loss 0.3904	TrainAcc 0.9460	ValidAcc 0.9404	TestAcc 0.9395	BestValid 0.9451
	Epoch 4725:	Loss 0.3862	TrainAcc 0.9452	ValidAcc 0.9400	TestAcc 0.9385	BestValid 0.9451
	Epoch 4750:	Loss 0.3884	TrainAcc 0.9474	ValidAcc 0.9432	TestAcc 0.9409	BestValid 0.9451
	Epoch 4775:	Loss 0.4056	TrainAcc 0.9497	ValidAcc 0.9452	TestAcc 0.9426	BestValid 0.9452
	Epoch 4800:	Loss 0.4352	TrainAcc 0.9464	ValidAcc 0.9421	TestAcc 0.9386	BestValid 0.9452
	Epoch 4825:	Loss 0.3974	TrainAcc 0.9427	ValidAcc 0.9392	TestAcc 0.9351	BestValid 0.9452
	Epoch 4850:	Loss 0.4024	TrainAcc 0.9489	ValidAcc 0.9446	TestAcc 0.9428	BestValid 0.9452
	Epoch 4875:	Loss 0.4248	TrainAcc 0.9355	ValidAcc 0.9302	TestAcc 0.9281	BestValid 0.9452
	Epoch 4900:	Loss 0.3927	TrainAcc 0.9464	ValidAcc 0.9414	TestAcc 0.9390	BestValid 0.9452
	Epoch 4925:	Loss 0.4042	TrainAcc 0.9471	ValidAcc 0.9430	TestAcc 0.9391	BestValid 0.9452
	Epoch 4950:	Loss 0.3871	TrainAcc 0.9496	ValidAcc 0.9452	TestAcc 0.9426	BestValid 0.9452
	Epoch 4975:	Loss 0.3873	TrainAcc 0.9456	ValidAcc 0.9396	TestAcc 0.9382	BestValid 0.9452
	Epoch 5000:	Loss 0.3898	TrainAcc 0.9494	ValidAcc 0.9447	TestAcc 0.9425	BestValid 0.9452
Node 3, Pre/Post-Pipelining: 0.250 / 1.907 ms, Bubble: 80.937 ms, Compute: 252.982 ms, Comm: 46.992 ms, Imbalance: 30.907 ms
Node 4, Pre/Post-Pipelining: 0.207 / 1.923 ms, Bubble: 81.567 ms, Compute: 248.703 ms, Comm: 46.291 ms, Imbalance: 35.807 ms
Node 5, Pre/Post-Pipelining: 0.211 / 1.934 ms, Bubble: 81.803 ms, Compute: 254.664 ms, Comm: 45.071 ms, Imbalance: 30.469 ms
Node 7, Pre/Post-Pipelining: 0.170 / 17.319 ms, Bubble: 66.805 ms, Compute: 281.723 ms, Comm: 32.119 ms, Imbalance: 15.576 ms
Node 0, Pre/Post-Pipelining: 0.103 / 2.003 ms, Bubble: 81.134 ms, Compute: 285.396 ms, Comm: 32.640 ms, Imbalance: 11.715 ms
Node 1, Pre/Post-Pipelining: 0.206 / 1.917 ms, Bubble: 81.174 ms, Compute: 254.533 ms, Comm: 42.634 ms, Imbalance: 33.181 ms
Node 6, Pre/Post-Pipelining: 0.206 / 1.930 ms, Bubble: 81.938 ms, Compute: 256.847 ms, Comm: 40.657 ms, Imbalance: 32.643 ms
Node 2, Pre/Post-Pipelining: 0.184 / 1.937 ms, Bubble: 81.930 ms, Compute: 248.941 ms, Comm: 47.201 ms, Imbalance: 34.039 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.103 ms
Cluster-Wide Average, Post-Pipelining Overhead: 2.003 ms
Cluster-Wide Average, Bubble: 81.134 ms
Cluster-Wide Average, Compute: 285.396 ms
Cluster-Wide Average, Communication: 32.640 ms
Cluster-Wide Average, Imbalance: 11.715 ms
Node 0, GPU memory consumption: 8.059 GB
Node 3, GPU memory consumption: 6.018 GB
Node 1, GPU memory consumption: 6.042 GB
Node 5, GPU memory consumption: 6.042 GB
Node 2, GPU memory consumption: 6.042 GB
Node 4, GPU memory consumption: 6.018 GB
Node 7, GPU memory consumption: 6.171 GB
Node 6, GPU memory consumption: 6.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.896371 s---------------
------------------------node id 1,  per-epoch time: 0.896371 s---------------
------------------------node id 2,  per-epoch time: 0.896371 s---------------
------------------------node id 3,  per-epoch time: 0.896371 s---------------
------------------------node id 4,  per-epoch time: 0.896371 s---------------
------------------------node id 5,  per-epoch time: 0.896371 s---------------
------------------------node id 6,  per-epoch time: 0.896371 s---------------
------------------------node id 7,  per-epoch time: 0.896371 s---------------
************ Profiling Results ************
	Bubble: 636.228892 (ms) (71.01 percentage)
	Compute: 256.296360 (ms) (28.60 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.485663 (ms) (0.39 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 2.431 GB
	Aggregated layer-level communication throughput: 500.559 Gbps
Highest valid_acc: 0.9452
Target test_acc: 0.9426
Epoch to reach the target acc: 4774
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
