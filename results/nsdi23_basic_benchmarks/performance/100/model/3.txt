Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.855 seconds.
Building the CSC structure...
        It takes 2.027 seconds.
Building the CSC structure...
        It takes 2.232 seconds.
Building the CSC structure...
        It takes 2.351 seconds.
Building the CSC structure...
        It takes 2.443 seconds.
Building the CSC structure...
        It takes 2.613 seconds.
Building the CSC structure...
        It takes 2.628 seconds.
Building the CSC structure...
        It takes 2.695 seconds.
Building the CSC structure...
        It takes 1.822 seconds.
        It takes 1.873 seconds.
        It takes 2.234 seconds.
        It takes 2.282 seconds.
Building the Feature Vector...
        It takes 2.363 seconds.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 2.323 seconds.
        It takes 0.036 seconds.
        It takes 2.396 seconds.
        It takes 2.375 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.297 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.297 seconds.
Building the Label Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.033 seconds.
        It takes 0.290 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.281 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 62.622 Gbps (per GPU), 500.977 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.584 Gbps (per GPU), 500.672 Gbps (aggregated)
The layer-level communication performance: 62.583 Gbps (per GPU), 500.662 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.550 Gbps (per GPU), 500.396 Gbps (aggregated)
The layer-level communication performance: 62.546 Gbps (per GPU), 500.366 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 62.517 Gbps (per GPU), 500.138 Gbps (aggregated)
The layer-level communication performance: 62.510 Gbps (per GPU), 500.080 Gbps (aggregated)
The layer-level communication performance: 62.505 Gbps (per GPU), 500.044 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 166.086 Gbps (per GPU), 1328.688 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.084 Gbps (per GPU), 1328.672 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.086 Gbps (per GPU), 1328.684 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.074 Gbps (per GPU), 1328.596 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.082 Gbps (per GPU), 1328.658 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.083 Gbps (per GPU), 1328.665 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.083 Gbps (per GPU), 1328.661 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 166.082 Gbps (per GPU), 1328.656 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.663 Gbps (per GPU), 917.306 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.663 Gbps (per GPU), 917.308 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.663 Gbps (per GPU), 917.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.662 Gbps (per GPU), 917.294 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.661 Gbps (per GPU), 917.287 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.662 Gbps (per GPU), 917.296 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.662 Gbps (per GPU), 917.298 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.662 Gbps (per GPU), 917.299 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.003 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.001 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.003 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.000 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.001 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.374 Gbps (per GPU), 362.992 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.375 Gbps (per GPU), 363.000 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.374 Gbps (per GPU), 362.991 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.48ms  2.80ms  3.12  8.38K  3.53M
 chk_1  0.75ms  2.86ms  2.99ms  3.97  6.74K  3.60M
 chk_2  0.80ms  2.71ms  2.90ms  3.63  7.27K  3.53M
 chk_3  0.81ms  2.77ms  2.93ms  3.63  7.92K  3.61M
 chk_4  0.64ms  2.71ms  2.86ms  4.45  5.33K  3.68M
 chk_5  1.01ms  2.69ms  2.89ms  2.85 10.07K  3.45M
 chk_6  0.97ms  2.83ms  3.03ms  3.13  9.41K  3.48M
 chk_7  0.82ms  2.69ms  2.86ms  3.48  8.12K  3.60M
 chk_8  0.68ms  2.79ms  3.05ms  4.47  6.09K  3.64M
 chk_9  1.11ms  2.60ms  2.79ms  2.52 11.10K  3.38M
chk_10  0.65ms  2.84ms  2.98ms  4.56  5.67K  3.63M
chk_11  0.83ms  2.69ms  2.87ms  3.46  8.16K  3.54M
chk_12  0.80ms  2.89ms  3.06ms  3.81  7.24K  3.55M
chk_13  0.65ms  2.74ms  2.89ms  4.47  5.41K  3.68M
chk_14  0.79ms  2.94ms  3.12ms  3.97  7.14K  3.53M
chk_15  0.96ms  2.82ms  3.07ms  3.20  9.25K  3.49M
chk_16  0.60ms  2.67ms  2.82ms  4.70  4.78K  3.77M
chk_17  0.77ms  2.79ms  2.95ms  3.83  6.85K  3.60M
chk_18  0.82ms  2.61ms  2.75ms  3.37  7.47K  3.57M
chk_19  0.61ms  2.66ms  2.82ms  4.63  4.88K  3.75M
chk_20  0.78ms  2.71ms  2.83ms  3.65  7.00K  3.63M
chk_21  0.64ms  2.66ms  2.80ms  4.39  5.41K  3.68M
chk_22  1.10ms  2.87ms  3.04ms  2.75 11.07K  3.39M
chk_23  0.80ms  2.77ms  2.91ms  3.65  7.23K  3.64M
chk_24  1.02ms  2.83ms  3.00ms  2.94 10.13K  3.43M
chk_25  0.73ms  2.64ms  2.79ms  3.82  6.40K  3.57M
chk_26  0.66ms  2.82ms  3.00ms  4.53  5.78K  3.55M
chk_27  0.96ms  2.73ms  2.92ms  3.04  9.34K  3.48M
chk_28  0.73ms  2.97ms  3.15ms  4.33  6.37K  3.57M
chk_29  0.63ms  2.81ms  2.97ms  4.70  5.16K  3.78M
chk_30  0.64ms  2.69ms  2.86ms  4.48  5.44K  3.67M
chk_31  0.73ms  2.83ms  3.02ms  4.15  6.33K  3.63M
   Avg  0.79  2.75  2.93
   Max  1.11  2.97  3.15
   Min  0.60  2.48  2.75
 Ratio  1.84  1.20  1.15
   Var  0.02  0.01  0.01
Profiling takes 2.480 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 377.902 ms
Partition 0 [0, 5) has cost: 377.902 ms
Partition 1 [5, 9) has cost: 352.522 ms
Partition 2 [9, 13) has cost: 352.522 ms
Partition 3 [13, 17) has cost: 352.522 ms
Partition 4 [17, 21) has cost: 352.522 ms
Partition 5 [21, 25) has cost: 352.522 ms
Partition 6 [25, 29) has cost: 352.522 ms
Partition 7 [29, 33) has cost: 358.114 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 167.776 ms
GPU 0, Compute+Comm Time: 136.025 ms, Bubble Time: 29.401 ms, Imbalance Overhead: 2.350 ms
GPU 1, Compute+Comm Time: 128.922 ms, Bubble Time: 29.109 ms, Imbalance Overhead: 9.744 ms
GPU 2, Compute+Comm Time: 128.922 ms, Bubble Time: 29.187 ms, Imbalance Overhead: 9.666 ms
GPU 3, Compute+Comm Time: 128.922 ms, Bubble Time: 29.111 ms, Imbalance Overhead: 9.743 ms
GPU 4, Compute+Comm Time: 128.922 ms, Bubble Time: 29.089 ms, Imbalance Overhead: 9.764 ms
GPU 5, Compute+Comm Time: 128.922 ms, Bubble Time: 29.146 ms, Imbalance Overhead: 9.708 ms
GPU 6, Compute+Comm Time: 128.922 ms, Bubble Time: 29.394 ms, Imbalance Overhead: 9.460 ms
GPU 7, Compute+Comm Time: 130.116 ms, Bubble Time: 29.751 ms, Imbalance Overhead: 7.908 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 325.689 ms
GPU 0, Compute+Comm Time: 251.807 ms, Bubble Time: 58.038 ms, Imbalance Overhead: 15.844 ms
GPU 1, Compute+Comm Time: 247.409 ms, Bubble Time: 57.303 ms, Imbalance Overhead: 20.978 ms
GPU 2, Compute+Comm Time: 247.409 ms, Bubble Time: 56.723 ms, Imbalance Overhead: 21.558 ms
GPU 3, Compute+Comm Time: 247.409 ms, Bubble Time: 56.628 ms, Imbalance Overhead: 21.652 ms
GPU 4, Compute+Comm Time: 247.409 ms, Bubble Time: 56.654 ms, Imbalance Overhead: 21.627 ms
GPU 5, Compute+Comm Time: 247.409 ms, Bubble Time: 56.725 ms, Imbalance Overhead: 21.556 ms
GPU 6, Compute+Comm Time: 247.409 ms, Bubble Time: 56.480 ms, Imbalance Overhead: 21.801 ms
GPU 7, Compute+Comm Time: 265.686 ms, Bubble Time: 57.061 ms, Imbalance Overhead: 2.942 ms
The estimated cost of the whole pipeline: 518.138 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 730.423 ms
Partition 0 [0, 9) has cost: 730.423 ms
Partition 1 [9, 17) has cost: 705.043 ms
Partition 2 [17, 25) has cost: 705.043 ms
Partition 3 [25, 33) has cost: 710.636 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 175.906 ms
GPU 0, Compute+Comm Time: 146.168 ms, Bubble Time: 26.680 ms, Imbalance Overhead: 3.058 ms
GPU 1, Compute+Comm Time: 142.225 ms, Bubble Time: 26.905 ms, Imbalance Overhead: 6.776 ms
GPU 2, Compute+Comm Time: 142.225 ms, Bubble Time: 26.892 ms, Imbalance Overhead: 6.789 ms
GPU 3, Compute+Comm Time: 142.775 ms, Bubble Time: 27.319 ms, Imbalance Overhead: 5.812 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 327.745 ms
GPU 0, Compute+Comm Time: 265.732 ms, Bubble Time: 50.847 ms, Imbalance Overhead: 11.165 ms
GPU 1, Compute+Comm Time: 263.518 ms, Bubble Time: 50.473 ms, Imbalance Overhead: 13.754 ms
GPU 2, Compute+Comm Time: 263.518 ms, Bubble Time: 50.828 ms, Imbalance Overhead: 13.398 ms
GPU 3, Compute+Comm Time: 273.868 ms, Bubble Time: 50.473 ms, Imbalance Overhead: 3.404 ms
    The estimated cost with 2 DP ways is 528.833 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1435.467 ms
Partition 0 [0, 17) has cost: 1435.467 ms
Partition 1 [17, 33) has cost: 1415.679 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 211.669 ms
GPU 0, Compute+Comm Time: 185.121 ms, Bubble Time: 21.279 ms, Imbalance Overhead: 5.268 ms
GPU 1, Compute+Comm Time: 183.319 ms, Bubble Time: 22.575 ms, Imbalance Overhead: 5.775 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 353.842 ms
GPU 0, Compute+Comm Time: 307.922 ms, Bubble Time: 38.364 ms, Imbalance Overhead: 7.555 ms
GPU 1, Compute+Comm Time: 312.336 ms, Bubble Time: 37.408 ms, Imbalance Overhead: 4.098 ms
    The estimated cost with 4 DP ways is 593.787 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2851.146 ms
Partition 0 [0, 33) has cost: 2851.146 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 459.517 ms
GPU 0, Compute+Comm Time: 459.517 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 587.515 ms
GPU 0, Compute+Comm Time: 587.515 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1099.383 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
