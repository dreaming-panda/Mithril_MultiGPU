Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.923 seconds.
Building the CSC structure...
        It takes 2.354 seconds.
Building the CSC structure...
        It takes 2.354 seconds.
Building the CSC structure...
        It takes 2.403 seconds.
Building the CSC structure...
        It takes 2.418 seconds.
Building the CSC structure...
        It takes 2.488 seconds.
Building the CSC structure...
        It takes 2.650 seconds.
Building the CSC structure...
        It takes 2.701 seconds.
Building the CSC structure...
        It takes 1.837 seconds.
        It takes 2.240 seconds.
        It takes 2.309 seconds.
        It takes 2.330 seconds.
Building the Feature Vector...
        It takes 2.397 seconds.
        It takes 2.386 seconds.
        It takes 2.284 seconds.
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 2.355 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.293 seconds.
Building the Label Vector...
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
        It takes 0.040 seconds.
        It takes 0.283 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.044 seconds.
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
Building the Feature Vector...
        It takes 0.277 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.031 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.280 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 5)
GPU 1, layer [5, 9)
GPU 2, layer [9, 13)
GPU 3, layer [13, 17)
GPU 4, layer [17, 21)
GPU 5, layer [21, 25)
GPU 6, layer [25, 29)
GPU 7, layer [29, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 63.851 Gbps (per GPU), 510.805 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.809 Gbps (per GPU), 510.473 Gbps (aggregated)
The layer-level communication performance: 63.808 Gbps (per GPU), 510.465 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.774 Gbps (per GPU), 510.190 Gbps (aggregated)
The layer-level communication performance: 63.768 Gbps (per GPU), 510.148 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 63.737 Gbps (per GPU), 509.894 Gbps (aggregated)
The layer-level communication performance: 63.730 Gbps (per GPU), 509.838 Gbps (aggregated)
The layer-level communication performance: 63.725 Gbps (per GPU), 509.799 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 164.672 Gbps (per GPU), 1317.378 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.662 Gbps (per GPU), 1317.297 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.656 Gbps (per GPU), 1317.252 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.660 Gbps (per GPU), 1317.284 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.652 Gbps (per GPU), 1317.216 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.665 Gbps (per GPU), 1317.316 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.658 Gbps (per GPU), 1317.261 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 164.666 Gbps (per GPU), 1317.326 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 114.192 Gbps (per GPU), 913.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.193 Gbps (per GPU), 913.542 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.192 Gbps (per GPU), 913.539 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.191 Gbps (per GPU), 913.529 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.192 Gbps (per GPU), 913.538 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.189 Gbps (per GPU), 913.509 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.191 Gbps (per GPU), 913.530 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 114.191 Gbps (per GPU), 913.528 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 45.464 Gbps (per GPU), 363.710 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.464 Gbps (per GPU), 363.710 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.463 Gbps (per GPU), 363.708 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.463 Gbps (per GPU), 363.707 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.464 Gbps (per GPU), 363.710 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.463 Gbps (per GPU), 363.705 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.463 Gbps (per GPU), 363.707 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 45.464 Gbps (per GPU), 363.708 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  2.46ms  2.78ms  3.12  8.38K  3.53M
 chk_1  0.76ms  3.27ms  2.97ms  4.28  6.74K  3.60M
 chk_2  0.80ms  2.68ms  2.89ms  3.63  7.27K  3.53M
 chk_3  1.17ms  2.76ms  2.96ms  2.52  7.92K  3.61M
 chk_4  0.63ms  3.04ms  2.87ms  4.83  5.33K  3.68M
 chk_5  1.00ms  2.63ms  2.89ms  2.88 10.07K  3.45M
 chk_6  0.96ms  2.83ms  3.02ms  3.14  9.41K  3.48M
 chk_7  0.82ms  2.66ms  2.86ms  3.49  8.12K  3.60M
 chk_8  0.68ms  2.78ms  2.96ms  4.34  6.09K  3.64M
 chk_9  1.10ms  2.58ms  2.81ms  2.55 11.10K  3.38M
chk_10  0.65ms  2.82ms  2.99ms  4.58  5.67K  3.63M
chk_11  0.82ms  2.68ms  2.87ms  3.49  8.16K  3.54M
chk_12  0.80ms  2.86ms  3.06ms  3.83  7.24K  3.55M
chk_13  1.20ms  2.73ms  2.88ms  2.40  5.41K  3.68M
chk_14  0.78ms  2.97ms  3.11ms  3.99  7.14K  3.53M
chk_15  0.95ms  2.81ms  3.01ms  3.16  9.25K  3.49M
chk_16  0.59ms  2.71ms  2.80ms  4.73  4.78K  3.77M
chk_17  0.76ms  2.77ms  2.95ms  3.87  6.85K  3.60M
chk_18  0.81ms  2.61ms  2.75ms  3.42  7.47K  3.57M
chk_19  0.60ms  2.64ms  2.83ms  4.69  4.88K  3.75M
chk_20  0.77ms  2.68ms  2.85ms  3.69  7.00K  3.63M
chk_21  0.64ms  2.64ms  2.82ms  4.44  5.41K  3.68M
chk_22  1.10ms  2.87ms  3.08ms  2.79 11.07K  3.39M
chk_23  0.79ms  2.77ms  2.93ms  3.70  7.23K  3.64M
chk_24  1.02ms  2.81ms  3.02ms  2.97 10.13K  3.43M
chk_25  0.73ms  2.62ms  2.80ms  3.84  6.40K  3.57M
chk_26  0.66ms  2.80ms  3.01ms  4.54  5.78K  3.55M
chk_27  0.96ms  2.71ms  2.92ms  3.03  9.34K  3.48M
chk_28  0.73ms  2.96ms  3.16ms  4.34  6.37K  3.57M
chk_29  0.63ms  2.79ms  2.97ms  4.70  5.16K  3.78M
chk_30  0.64ms  2.69ms  2.85ms  4.46  5.44K  3.67M
chk_31  0.73ms  2.81ms  3.03ms  4.17  6.33K  3.63M
   Avg  0.82  2.76  2.93
   Max  1.20  3.27  3.16
   Min  0.59  2.46  2.75
 Ratio  2.03  1.33  1.15
   Var  0.03  0.02  0.01
Profiling takes 2.489 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 379.969 ms
Partition 0 [0, 5) has cost: 379.969 ms
Partition 1 [5, 9) has cost: 353.780 ms
Partition 2 [9, 13) has cost: 353.780 ms
Partition 3 [13, 17) has cost: 353.780 ms
Partition 4 [17, 21) has cost: 353.780 ms
Partition 5 [21, 25) has cost: 353.780 ms
Partition 6 [25, 29) has cost: 353.780 ms
Partition 7 [29, 33) has cost: 359.016 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 180.127 ms
GPU 0, Compute+Comm Time: 136.912 ms, Bubble Time: 29.244 ms, Imbalance Overhead: 13.971 ms
GPU 1, Compute+Comm Time: 129.858 ms, Bubble Time: 28.940 ms, Imbalance Overhead: 21.329 ms
GPU 2, Compute+Comm Time: 129.858 ms, Bubble Time: 30.849 ms, Imbalance Overhead: 19.420 ms
GPU 3, Compute+Comm Time: 129.858 ms, Bubble Time: 32.542 ms, Imbalance Overhead: 17.727 ms
GPU 4, Compute+Comm Time: 129.858 ms, Bubble Time: 34.190 ms, Imbalance Overhead: 16.079 ms
GPU 5, Compute+Comm Time: 129.858 ms, Bubble Time: 36.096 ms, Imbalance Overhead: 14.174 ms
GPU 6, Compute+Comm Time: 129.858 ms, Bubble Time: 38.001 ms, Imbalance Overhead: 12.268 ms
GPU 7, Compute+Comm Time: 130.735 ms, Bubble Time: 39.857 ms, Imbalance Overhead: 9.535 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 330.501 ms
GPU 0, Compute+Comm Time: 251.632 ms, Bubble Time: 59.845 ms, Imbalance Overhead: 19.024 ms
GPU 1, Compute+Comm Time: 247.273 ms, Bubble Time: 58.993 ms, Imbalance Overhead: 24.235 ms
GPU 2, Compute+Comm Time: 247.273 ms, Bubble Time: 57.972 ms, Imbalance Overhead: 25.256 ms
GPU 3, Compute+Comm Time: 247.273 ms, Bubble Time: 56.509 ms, Imbalance Overhead: 26.719 ms
GPU 4, Compute+Comm Time: 247.273 ms, Bubble Time: 56.153 ms, Imbalance Overhead: 27.075 ms
GPU 5, Compute+Comm Time: 247.273 ms, Bubble Time: 56.294 ms, Imbalance Overhead: 26.934 ms
GPU 6, Compute+Comm Time: 247.273 ms, Bubble Time: 56.164 ms, Imbalance Overhead: 27.064 ms
GPU 7, Compute+Comm Time: 266.408 ms, Bubble Time: 56.767 ms, Imbalance Overhead: 7.326 ms
The estimated cost of the whole pipeline: 536.159 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 733.749 ms
Partition 0 [0, 9) has cost: 733.749 ms
Partition 1 [9, 17) has cost: 707.560 ms
Partition 2 [17, 25) has cost: 707.560 ms
Partition 3 [25, 33) has cost: 712.796 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 188.951 ms
GPU 0, Compute+Comm Time: 148.908 ms, Bubble Time: 26.541 ms, Imbalance Overhead: 13.502 ms
GPU 1, Compute+Comm Time: 144.998 ms, Bubble Time: 30.451 ms, Imbalance Overhead: 13.502 ms
GPU 2, Compute+Comm Time: 144.998 ms, Bubble Time: 34.091 ms, Imbalance Overhead: 9.862 ms
GPU 3, Compute+Comm Time: 145.193 ms, Bubble Time: 38.139 ms, Imbalance Overhead: 5.619 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 335.211 ms
GPU 0, Compute+Comm Time: 266.858 ms, Bubble Time: 53.590 ms, Imbalance Overhead: 14.763 ms
GPU 1, Compute+Comm Time: 264.843 ms, Bubble Time: 50.409 ms, Imbalance Overhead: 19.958 ms
GPU 2, Compute+Comm Time: 264.843 ms, Bubble Time: 50.359 ms, Imbalance Overhead: 20.008 ms
GPU 3, Compute+Comm Time: 275.969 ms, Bubble Time: 50.254 ms, Imbalance Overhead: 8.988 ms
    The estimated cost with 2 DP ways is 550.370 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1441.308 ms
Partition 0 [0, 17) has cost: 1441.308 ms
Partition 1 [17, 33) has cost: 1420.355 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 223.927 ms
GPU 0, Compute+Comm Time: 191.888 ms, Bubble Time: 21.243 ms, Imbalance Overhead: 10.797 ms
GPU 1, Compute+Comm Time: 189.668 ms, Bubble Time: 29.846 ms, Imbalance Overhead: 4.414 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 361.111 ms
GPU 0, Compute+Comm Time: 310.703 ms, Bubble Time: 38.059 ms, Imbalance Overhead: 12.349 ms
GPU 1, Compute+Comm Time: 315.987 ms, Bubble Time: 37.295 ms, Imbalance Overhead: 7.829 ms
    The estimated cost with 4 DP ways is 614.290 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2861.664 ms
Partition 0 [0, 33) has cost: 2861.664 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 472.654 ms
GPU 0, Compute+Comm Time: 472.654 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 595.469 ms
GPU 0, Compute+Comm Time: 595.469 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1121.529 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [34, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 90)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [90, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [146, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [174, 202)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [202, 233)
*** Node 7, constructing the helper classes...
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 146)
*** Node 4, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[34, 62)...
+++++++++ Node 2 initializing the weights for op[62, 90)...
+++++++++ Node 4 initializing the weights for op[118, 146)...
+++++++++ Node 3 initializing the weights for op[90, 118)...
+++++++++ Node 7 initializing the weights for op[202, 233)...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 5 initializing the weights for op[146, 174)...
+++++++++ Node 6 initializing the weights for op[174, 202)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 7, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5756
	Epoch 50:	Loss 1.9190
	Epoch 75:	Loss 1.5452
	Epoch 100:	Loss 1.3218
	Epoch 125:	Loss 1.1337
	Epoch 150:	Loss 1.0415
	Epoch 175:	Loss 0.9707
	Epoch 200:	Loss 0.9153
	Epoch 225:	Loss 0.8706
	Epoch 250:	Loss 0.8333
	Epoch 275:	Loss 0.8036
	Epoch 300:	Loss 0.7825
	Epoch 325:	Loss 0.7592
	Epoch 350:	Loss 0.7458
	Epoch 375:	Loss 0.7239
	Epoch 400:	Loss 0.7086
	Epoch 425:	Loss 0.6948
	Epoch 450:	Loss 0.6822
	Epoch 475:	Loss 0.6687
	Epoch 500:	Loss 0.6649
	Epoch 525:	Loss 0.6513
	Epoch 550:	Loss 0.6423
	Epoch 575:	Loss 0.6366
	Epoch 600:	Loss 0.6297
	Epoch 625:	Loss 0.6255
	Epoch 650:	Loss 0.6135
	Epoch 675:	Loss 0.6059
	Epoch 700:	Loss 0.6062
	Epoch 725:	Loss 0.5979
	Epoch 750:	Loss 0.5953
	Epoch 775:	Loss 0.5897
	Epoch 800:	Loss 0.5859
	Epoch 825:	Loss 0.5797
	Epoch 850:	Loss 0.5738
	Epoch 875:	Loss 0.5723
	Epoch 900:	Loss 0.5706
	Epoch 925:	Loss 0.5720
	Epoch 950:	Loss 0.5599
	Epoch 975:	Loss 0.5587
	Epoch 1000:	Loss 0.5548
	Epoch 1025:	Loss 0.5531
	Epoch 1050:	Loss 0.5516
	Epoch 1075:	Loss 0.5472
	Epoch 1100:	Loss 0.5444
	Epoch 1125:	Loss 0.5480
	Epoch 1150:	Loss 0.5351
	Epoch 1175:	Loss 0.5368
	Epoch 1200:	Loss 0.5310
	Epoch 1225:	Loss 0.5339
	Epoch 1250:	Loss 0.5277
	Epoch 1275:	Loss 0.5326
	Epoch 1300:	Loss 0.5272
	Epoch 1325:	Loss 0.5202
	Epoch 1350:	Loss 0.5268
	Epoch 1375:	Loss 0.5194
	Epoch 1400:	Loss 0.5191
	Epoch 1425:	Loss 0.5265
	Epoch 1450:	Loss 0.5126
	Epoch 1475:	Loss 0.5063
	Epoch 1500:	Loss 0.5097
	Epoch 1525:	Loss 0.5052
	Epoch 1550:	Loss 0.5033
	Epoch 1575:	Loss 0.5042
	Epoch 1600:	Loss 0.5023
	Epoch 1625:	Loss 0.5034
	Epoch 1650:	Loss 0.5046
	Epoch 1675:	Loss 0.4992
	Epoch 1700:	Loss 0.4974
	Epoch 1725:	Loss 0.4979
	Epoch 1750:	Loss 0.4916
	Epoch 1775:	Loss 0.4963
	Epoch 1800:	Loss 0.4932
	Epoch 1825:	Loss 0.4901
	Epoch 1850:	Loss 0.4937
	Epoch 1875:	Loss 0.4891
	Epoch 1900:	Loss 0.4858
	Epoch 1925:	Loss 0.4867
	Epoch 1950:	Loss 0.4845
	Epoch 1975:	Loss 0.4820
	Epoch 2000:	Loss 0.4830
	Epoch 2025:	Loss 0.4798
	Epoch 2050:	Loss 0.4830
	Epoch 2075:	Loss 0.4828
	Epoch 2100:	Loss 0.4783
	Epoch 2125:	Loss 0.4790
	Epoch 2150:	Loss 0.4757
	Epoch 2175:	Loss 0.4748
	Epoch 2200:	Loss 0.4735
	Epoch 2225:	Loss 0.4748
	Epoch 2250:	Loss 0.4736
	Epoch 2275:	Loss 0.4763
	Epoch 2300:	Loss 0.4721
	Epoch 2325:	Loss 0.4753
	Epoch 2350:	Loss 0.4718
	Epoch 2375:	Loss 0.4701
	Epoch 2400:	Loss 0.4647
	Epoch 2425:	Loss 0.4670
	Epoch 2450:	Loss 0.4655
	Epoch 2475:	Loss 0.4665
	Epoch 2500:	Loss 0.4605
	Epoch 2525:	Loss 0.4658
	Epoch 2550:	Loss 0.4626
	Epoch 2575:	Loss 0.4623
	Epoch 2600:	Loss 0.4598
	Epoch 2625:	Loss 0.4628
	Epoch 2650:	Loss 0.4638
	Epoch 2675:	Loss 0.4593
	Epoch 2700:	Loss 0.4589
	Epoch 2725:	Loss 0.4613
	Epoch 2750:	Loss 0.4544
	Epoch 2775:	Loss 0.4578
	Epoch 2800:	Loss 0.4575
	Epoch 2825:	Loss 0.4568
	Epoch 2850:	Loss 0.4571
	Epoch 2875:	Loss 0.4551
	Epoch 2900:	Loss 0.4535
	Epoch 2925:	Loss 0.4533
	Epoch 2950:	Loss 0.4552
	Epoch 2975:	Loss 0.4531
	Epoch 3000:	Loss 0.4554
	Epoch 3025:	Loss 0.4536
	Epoch 3050:	Loss 0.4514
	Epoch 3075:	Loss 0.4453
	Epoch 3100:	Loss 0.4484
	Epoch 3125:	Loss 0.4499
	Epoch 3150:	Loss 0.4469
	Epoch 3175:	Loss 0.4459
	Epoch 3200:	Loss 0.4491
	Epoch 3225:	Loss 0.4500
	Epoch 3250:	Loss 0.4434
	Epoch 3275:	Loss 0.4439
	Epoch 3300:	Loss 0.4457
	Epoch 3325:	Loss 0.4461
	Epoch 3350:	Loss 0.4398
	Epoch 3375:	Loss 0.4429
	Epoch 3400:	Loss 0.4382
	Epoch 3425:	Loss 0.4441
	Epoch 3450:	Loss 0.4421
	Epoch 3475:	Loss 0.4409
	Epoch 3500:	Loss 0.4385
	Epoch 3525:	Loss 0.4386
	Epoch 3550:	Loss 0.4342
	Epoch 3575:	Loss 0.4356
	Epoch 3600:	Loss 0.4363
	Epoch 3625:	Loss 0.4404
	Epoch 3650:	Loss 0.4358
	Epoch 3675:	Loss 0.4372
	Epoch 3700:	Loss 0.4442
	Epoch 3725:	Loss 0.4360
	Epoch 3750:	Loss 0.4364
	Epoch 3775:	Loss 0.4343
	Epoch 3800:	Loss 0.4349
	Epoch 3825:	Loss 0.4322
	Epoch 3850:	Loss 0.4313
	Epoch 3875:	Loss 0.4338
	Epoch 3900:	Loss 0.4311
	Epoch 3925:	Loss 0.4326
	Epoch 3950:	Loss 0.4326
	Epoch 3975:	Loss 0.4298
	Epoch 4000:	Loss 0.4287
	Epoch 4025:	Loss 0.4303
	Epoch 4050:	Loss 0.4239
	Epoch 4075:	Loss 0.4273
	Epoch 4100:	Loss 0.4259
	Epoch 4125:	Loss 0.4278
	Epoch 4150:	Loss 0.4289
	Epoch 4175:	Loss 0.4289
	Epoch 4200:	Loss 0.4256
	Epoch 4225:	Loss 0.4264
	Epoch 4250:	Loss 0.4271
	Epoch 4275:	Loss 0.4250
	Epoch 4300:	Loss 0.4275
	Epoch 4325:	Loss 0.4266
	Epoch 4350:	Loss 0.4245
	Epoch 4375:	Loss 0.4271
	Epoch 4400:	Loss 0.4247
	Epoch 4425:	Loss 0.4227
	Epoch 4450:	Loss 0.4278
	Epoch 4475:	Loss 0.4219
	Epoch 4500:	Loss 0.4216
	Epoch 4525:	Loss 0.4187
	Epoch 4550:	Loss 0.4257
	Epoch 4575:	Loss 0.4213
	Epoch 4600:	Loss 0.4188
	Epoch 4625:	Loss 0.4204
	Epoch 4650:	Loss 0.4185
	Epoch 4675:	Loss 0.4210
	Epoch 4700:	Loss 0.4165
	Epoch 4725:	Loss 0.4204
	Epoch 4750:	Loss 0.4149
	Epoch 4775:	Loss 0.4142
	Epoch 4800:	Loss 0.4145
	Epoch 4825:	Loss 0.4149
	Epoch 4850:	Loss 0.4291
	Epoch 4875:	Loss 0.4179
	Epoch 4900:	Loss 0.4187
	Epoch 4925:	Loss 0.4171
	Epoch 4950:	Loss 0.4127
	Epoch 4975:	Loss 0.4231
	Epoch 5000:	Loss 0.4127
Node 0, Pre/Post-Pipelining: 1.099 / 12.660 ms, Bubble: 83.586 ms, Compute: 292.450 ms, Comm: 33.011 ms, Imbalance: 12.355 ms
Node 6, Pre/Post-Pipelining: 1.095 / 12.583 ms, Bubble: 83.507 ms, Compute: 261.344 ms, Comm: 41.379 ms, Imbalance: 36.481 ms
Node 1, Pre/Post-Pipelining: 1.094 / 12.605 ms, Bubble: 83.547 ms, Compute: 263.432 ms, Comm: 43.007 ms, Imbalance: 32.101 ms
Node 5, Pre/Post-Pipelining: 1.099 / 12.545 ms, Bubble: 83.585 ms, Compute: 260.611 ms, Comm: 45.692 ms, Imbalance: 32.793 ms
Node 2, Pre/Post-Pipelining: 1.094 / 12.595 ms, Bubble: 84.118 ms, Compute: 255.583 ms, Comm: 47.911 ms, Imbalance: 35.101 ms
Node 7, Pre/Post-Pipelining: 1.099 / 28.363 ms, Bubble: 67.807 ms, Compute: 291.846 ms, Comm: 32.688 ms, Imbalance: 14.065 ms
Node 4, Pre/Post-Pipelining: 1.092 / 12.596 ms, Bubble: 83.442 ms, Compute: 255.872 ms, Comm: 46.958 ms, Imbalance: 36.720 ms
Node 3, Pre/Post-Pipelining: 1.091 / 12.608 ms, Bubble: 82.910 ms, Compute: 260.404 ms, Comm: 47.915 ms, Imbalance: 31.157 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.099 ms
Cluster-Wide Average, Post-Pipelining Overhead: 12.660 ms
Cluster-Wide Average, Bubble: 83.586 ms
Cluster-Wide Average, Compute: 292.450 ms
Cluster-Wide Average, Communication: 33.011 ms
Cluster-Wide Average, Imbalance: 12.355 ms
Node 0, GPU memory consumption: 8.059 GB
Node 2, GPU memory consumption: 6.042 GB
Node 1, GPU memory consumption: 6.042 GB
Node 3, GPU memory consumption: 6.018 GB
Node 6, GPU memory consumption: 6.042 GB
Node 7, GPU memory consumption: 6.171 GB
Node 4, GPU memory consumption: 6.018 GB
Node 5, GPU memory consumption: 6.042 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.435832 s---------------
------------------------node id 1,  per-epoch time: 0.435832 s---------------
------------------------node id 2,  per-epoch time: 0.435832 s---------------
------------------------node id 3,  per-epoch time: 0.435832 s---------------
------------------------node id 4,  per-epoch time: 0.435832 s---------------
------------------------node id 5,  per-epoch time: 0.435832 s---------------
------------------------node id 6,  per-epoch time: 0.435832 s---------------
------------------------node id 7,  per-epoch time: 0.435832 s---------------
************ Profiling Results ************
	Bubble: 168.965903 (ms) (38.76 percentage)
	Compute: 263.384964 (ms) (60.42 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.590918 (ms) (0.82 percentage)
	Layer-level communication (cluster-wide, per-epoch): 2.430 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 2.441 GB
	Aggregated layer-level communication throughput: 493.232 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
