Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.914 seconds.
Building the CSC structure...
        It takes 2.060 seconds.
Building the CSC structure...
        It takes 2.080 seconds.
Building the CSC structure...
        It takes 2.307 seconds.
Building the CSC structure...
        It takes 2.341 seconds.
Building the CSC structure...
        It takes 2.353 seconds.
Building the CSC structure...
        It takes 2.397 seconds.
Building the CSC structure...
        It takes 2.411 seconds.
Building the CSC structure...
        It takes 1.828 seconds.
        It takes 1.879 seconds.
        It takes 1.887 seconds.
        It takes 2.241 seconds.
        It takes 2.241 seconds.
        It takes 2.186 seconds.
        It takes 2.300 seconds.
        It takes 2.311 seconds.
Building the Feature Vector...
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.305 seconds.
Building the Label Vector...
        It takes 0.044 seconds.
        It takes 0.287 seconds.
Building the Label Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.040 seconds.
        It takes 0.306 seconds.
Building the Label Vector...
        It takes 0.306 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.037 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.278 seconds.
Building the Label Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.297 seconds.
Building the Label Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.032 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 64.261 Gbps (per GPU), 514.087 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.218 Gbps (per GPU), 513.744 Gbps (aggregated)
The layer-level communication performance: 64.217 Gbps (per GPU), 513.736 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.182 Gbps (per GPU), 513.453 Gbps (aggregated)
The layer-level communication performance: 64.177 Gbps (per GPU), 513.413 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 64.147 Gbps (per GPU), 513.176 Gbps (aggregated)
The layer-level communication performance: 64.139 Gbps (per GPU), 513.110 Gbps (aggregated)
The layer-level communication performance: 64.134 Gbps (per GPU), 513.075 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 163.867 Gbps (per GPU), 1310.937 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.870 Gbps (per GPU), 1310.957 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.859 Gbps (per GPU), 1310.871 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.868 Gbps (per GPU), 1310.944 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.863 Gbps (per GPU), 1310.906 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.868 Gbps (per GPU), 1310.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.854 Gbps (per GPU), 1310.835 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 163.866 Gbps (per GPU), 1310.932 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 113.868 Gbps (per GPU), 910.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.868 Gbps (per GPU), 910.946 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.868 Gbps (per GPU), 910.942 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.868 Gbps (per GPU), 910.941 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.868 Gbps (per GPU), 910.943 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.867 Gbps (per GPU), 910.940 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.867 Gbps (per GPU), 910.940 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 113.868 Gbps (per GPU), 910.941 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 46.033 Gbps (per GPU), 368.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.033 Gbps (per GPU), 368.266 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.033 Gbps (per GPU), 368.266 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.033 Gbps (per GPU), 368.262 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.033 Gbps (per GPU), 368.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.033 Gbps (per GPU), 368.265 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.033 Gbps (per GPU), 368.263 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 46.033 Gbps (per GPU), 368.263 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.90ms  2.48ms  2.81ms  3.12  8.38K  3.53M
 chk_1  0.76ms  2.83ms  2.98ms  3.93  6.74K  3.60M
 chk_2  0.81ms  2.72ms  2.89ms  3.58  7.27K  3.53M
 chk_3  0.81ms  2.74ms  2.93ms  3.62  7.92K  3.61M
 chk_4  0.63ms  2.69ms  2.83ms  4.46  5.33K  3.68M
 chk_5  1.01ms  2.66ms  2.86ms  2.83 10.07K  3.45M
 chk_6  0.97ms  2.82ms  3.02ms  3.13  9.41K  3.48M
 chk_7  0.83ms  2.67ms  2.86ms  3.46  8.12K  3.60M
 chk_8  0.69ms  2.77ms  2.95ms  4.29  6.09K  3.64M
 chk_9  1.11ms  2.60ms  2.80ms  2.52 11.10K  3.38M
chk_10  0.66ms  2.81ms  2.97ms  4.51  5.67K  3.63M
chk_11  0.83ms  2.68ms  2.87ms  3.45  8.16K  3.54M
chk_12  0.80ms  2.86ms  3.05ms  3.80  7.24K  3.55M
chk_13  0.64ms  2.72ms  2.87ms  4.45  5.41K  3.68M
chk_14  0.79ms  2.93ms  3.11ms  3.94  7.14K  3.53M
chk_15  0.96ms  2.80ms  3.00ms  3.13  9.25K  3.49M
chk_16  0.60ms  2.67ms  2.81ms  4.65  4.78K  3.77M
chk_17  0.77ms  2.78ms  2.93ms  3.81  6.85K  3.60M
chk_18  0.82ms  2.61ms  2.74ms  3.36  7.47K  3.57M
chk_19  0.61ms  2.68ms  2.81ms  4.61  4.88K  3.75M
chk_20  0.78ms  2.66ms  2.84ms  3.65  7.00K  3.63M
chk_21  0.64ms  2.67ms  2.81ms  4.39  5.41K  3.68M
chk_22  1.11ms  2.86ms  3.06ms  2.76 11.07K  3.39M
chk_23  0.80ms  2.76ms  2.94ms  3.67  7.23K  3.64M
chk_24  1.02ms  2.79ms  3.00ms  2.94 10.13K  3.43M
chk_25  0.73ms  2.62ms  2.80ms  3.81  6.40K  3.57M
chk_26  0.67ms  2.81ms  3.00ms  4.50  5.78K  3.55M
chk_27  0.96ms  2.70ms  2.93ms  3.04  9.34K  3.48M
chk_28  0.73ms  2.98ms  3.18ms  4.34  6.37K  3.57M
chk_29  0.64ms  2.78ms  2.98ms  4.68  5.16K  3.78M
chk_30  0.64ms  2.70ms  2.84ms  4.41  5.44K  3.67M
chk_31  0.73ms  2.84ms  2.99ms  4.09  6.33K  3.63M
   Avg  0.79  2.74  2.92
   Max  1.11  2.98  3.18
   Min  0.60  2.48  2.74
 Ratio  1.84  1.20  1.16
   Var  0.02  0.01  0.01
Profiling takes 2.473 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 376.222 ms
Partition 0 [0, 5) has cost: 376.222 ms
Partition 1 [5, 9) has cost: 350.783 ms
Partition 2 [9, 13) has cost: 350.783 ms
Partition 3 [13, 17) has cost: 350.783 ms
Partition 4 [17, 21) has cost: 350.783 ms
Partition 5 [21, 25) has cost: 350.783 ms
Partition 6 [25, 29) has cost: 350.783 ms
Partition 7 [29, 33) has cost: 356.525 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 166.817 ms
GPU 0, Compute+Comm Time: 135.109 ms, Bubble Time: 29.435 ms, Imbalance Overhead: 2.273 ms
GPU 1, Compute+Comm Time: 127.971 ms, Bubble Time: 29.072 ms, Imbalance Overhead: 9.774 ms
GPU 2, Compute+Comm Time: 127.971 ms, Bubble Time: 29.072 ms, Imbalance Overhead: 9.774 ms
GPU 3, Compute+Comm Time: 127.971 ms, Bubble Time: 29.005 ms, Imbalance Overhead: 9.840 ms
GPU 4, Compute+Comm Time: 127.971 ms, Bubble Time: 28.919 ms, Imbalance Overhead: 9.927 ms
GPU 5, Compute+Comm Time: 127.971 ms, Bubble Time: 28.892 ms, Imbalance Overhead: 9.953 ms
GPU 6, Compute+Comm Time: 127.971 ms, Bubble Time: 29.079 ms, Imbalance Overhead: 9.766 ms
GPU 7, Compute+Comm Time: 129.252 ms, Bubble Time: 29.424 ms, Imbalance Overhead: 8.140 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 324.416 ms
GPU 0, Compute+Comm Time: 250.474 ms, Bubble Time: 57.786 ms, Imbalance Overhead: 16.155 ms
GPU 1, Compute+Comm Time: 246.014 ms, Bubble Time: 57.058 ms, Imbalance Overhead: 21.344 ms
GPU 2, Compute+Comm Time: 246.014 ms, Bubble Time: 56.565 ms, Imbalance Overhead: 21.837 ms
GPU 3, Compute+Comm Time: 246.014 ms, Bubble Time: 56.540 ms, Imbalance Overhead: 21.863 ms
GPU 4, Compute+Comm Time: 246.014 ms, Bubble Time: 56.624 ms, Imbalance Overhead: 21.778 ms
GPU 5, Compute+Comm Time: 246.014 ms, Bubble Time: 56.691 ms, Imbalance Overhead: 21.711 ms
GPU 6, Compute+Comm Time: 246.014 ms, Bubble Time: 56.509 ms, Imbalance Overhead: 21.893 ms
GPU 7, Compute+Comm Time: 264.315 ms, Bubble Time: 57.030 ms, Imbalance Overhead: 3.071 ms
The estimated cost of the whole pipeline: 515.794 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 727.005 ms
Partition 0 [0, 9) has cost: 727.005 ms
Partition 1 [9, 17) has cost: 701.566 ms
Partition 2 [17, 25) has cost: 701.566 ms
Partition 3 [25, 33) has cost: 707.308 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 174.828 ms
GPU 0, Compute+Comm Time: 145.305 ms, Bubble Time: 26.686 ms, Imbalance Overhead: 2.837 ms
GPU 1, Compute+Comm Time: 141.353 ms, Bubble Time: 26.791 ms, Imbalance Overhead: 6.683 ms
GPU 2, Compute+Comm Time: 141.353 ms, Bubble Time: 26.613 ms, Imbalance Overhead: 6.862 ms
GPU 3, Compute+Comm Time: 141.958 ms, Bubble Time: 26.959 ms, Imbalance Overhead: 5.912 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 326.323 ms
GPU 0, Compute+Comm Time: 264.198 ms, Bubble Time: 50.491 ms, Imbalance Overhead: 11.634 ms
GPU 1, Compute+Comm Time: 261.988 ms, Bubble Time: 50.231 ms, Imbalance Overhead: 14.104 ms
GPU 2, Compute+Comm Time: 261.988 ms, Bubble Time: 50.694 ms, Imbalance Overhead: 13.642 ms
GPU 3, Compute+Comm Time: 272.349 ms, Bubble Time: 50.467 ms, Imbalance Overhead: 3.507 ms
    The estimated cost with 2 DP ways is 526.208 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1428.571 ms
Partition 0 [0, 17) has cost: 1428.571 ms
Partition 1 [17, 33) has cost: 1408.874 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 210.964 ms
GPU 0, Compute+Comm Time: 184.595 ms, Bubble Time: 21.337 ms, Imbalance Overhead: 5.033 ms
GPU 1, Compute+Comm Time: 182.818 ms, Bubble Time: 22.307 ms, Imbalance Overhead: 5.840 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 352.628 ms
GPU 0, Compute+Comm Time: 306.660 ms, Bubble Time: 38.122 ms, Imbalance Overhead: 7.846 ms
GPU 1, Compute+Comm Time: 311.107 ms, Bubble Time: 37.437 ms, Imbalance Overhead: 4.085 ms
    The estimated cost with 4 DP ways is 591.772 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2837.445 ms
Partition 0 [0, 33) has cost: 2837.445 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 454.055 ms
GPU 0, Compute+Comm Time: 454.055 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 582.063 ms
GPU 0, Compute+Comm Time: 582.063 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1087.923 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5185
	Epoch 50:	Loss 1.8661
	Epoch 75:	Loss 1.4976
	Epoch 100:	Loss 1.2765
	Epoch 125:	Loss 1.1045
	Epoch 150:	Loss 1.0120
	Epoch 175:	Loss 0.9380
	Epoch 200:	Loss 0.8849
	Epoch 225:	Loss 0.8371
	Epoch 250:	Loss 0.8082
	Epoch 275:	Loss 0.7765
	Epoch 300:	Loss 0.7553
	Epoch 325:	Loss 0.7272
	Epoch 350:	Loss 0.7092
	Epoch 375:	Loss 0.6906
	Epoch 400:	Loss 0.6764
	Epoch 425:	Loss 0.6606
	Epoch 450:	Loss 0.6498
	Epoch 475:	Loss 0.6413
	Epoch 500:	Loss 0.6286
	Epoch 525:	Loss 0.6227
	Epoch 550:	Loss 0.6151
	Epoch 575:	Loss 0.6057
	Epoch 600:	Loss 0.5957
	Epoch 625:	Loss 0.5917
	Epoch 650:	Loss 0.5868
	Epoch 675:	Loss 0.5787
	Epoch 700:	Loss 0.5713
	Epoch 725:	Loss 0.5668
	Epoch 750:	Loss 0.5633
	Epoch 775:	Loss 0.5584
	Epoch 800:	Loss 0.5545
	Epoch 825:	Loss 0.5472
	Epoch 850:	Loss 0.5439
	Epoch 875:	Loss 0.5415
	Epoch 900:	Loss 0.5345
	Epoch 925:	Loss 0.5310
	Epoch 950:	Loss 0.5331
	Epoch 975:	Loss 0.5274
	Epoch 1000:	Loss 0.5239
	Epoch 1025:	Loss 0.5216
	Epoch 1050:	Loss 0.5204
	Epoch 1075:	Loss 0.5117
	Epoch 1100:	Loss 0.5100
	Epoch 1125:	Loss 0.5113
	Epoch 1150:	Loss 0.5052
	Epoch 1175:	Loss 0.5067
	Epoch 1200:	Loss 0.5000
	Epoch 1225:	Loss 0.5006
	Epoch 1250:	Loss 0.4990
	Epoch 1275:	Loss 0.4942
	Epoch 1300:	Loss 0.4936
	Epoch 1325:	Loss 0.4912
	Epoch 1350:	Loss 0.4891
	Epoch 1375:	Loss 0.4901
	Epoch 1400:	Loss 0.4854
	Epoch 1425:	Loss 0.4820
	Epoch 1450:	Loss 0.4818
	Epoch 1475:	Loss 0.4810
	Epoch 1500:	Loss 0.4811
	Epoch 1525:	Loss 0.4765
	Epoch 1550:	Loss 0.4766
	Epoch 1575:	Loss 0.4738
	Epoch 1600:	Loss 0.4713
	Epoch 1625:	Loss 0.4699
	Epoch 1650:	Loss 0.4706
	Epoch 1675:	Loss 0.4647
	Epoch 1700:	Loss 0.4673
	Epoch 1725:	Loss 0.4643
	Epoch 1750:	Loss 0.4629
	Epoch 1775:	Loss 0.4603
	Epoch 1800:	Loss 0.4612
	Epoch 1825:	Loss 0.4628
	Epoch 1850:	Loss 0.4587
	Epoch 1875:	Loss 0.4601
	Epoch 1900:	Loss 0.4581
	Epoch 1925:	Loss 0.4532
	Epoch 1950:	Loss 0.4534
	Epoch 1975:	Loss 0.4508
	Epoch 2000:	Loss 0.4508
	Epoch 2025:	Loss 0.4512
	Epoch 2050:	Loss 0.4519
	Epoch 2075:	Loss 0.4486
	Epoch 2100:	Loss 0.4467
	Epoch 2125:	Loss 0.4481
	Epoch 2150:	Loss 0.4471
	Epoch 2175:	Loss 0.4436
	Epoch 2200:	Loss 0.4433
	Epoch 2225:	Loss 0.4424
	Epoch 2250:	Loss 0.4435
	Epoch 2275:	Loss 0.4415
	Epoch 2300:	Loss 0.4398
	Epoch 2325:	Loss 0.4395
	Epoch 2350:	Loss 0.4389
	Epoch 2375:	Loss 0.4395
	Epoch 2400:	Loss 0.4390
	Epoch 2425:	Loss 0.4365
	Epoch 2450:	Loss 0.4393
	Epoch 2475:	Loss 0.4350
	Epoch 2500:	Loss 0.4331
	Epoch 2525:	Loss 0.4334
	Epoch 2550:	Loss 0.4336
	Epoch 2575:	Loss 0.4345
	Epoch 2600:	Loss 0.4322
	Epoch 2625:	Loss 0.4306
	Epoch 2650:	Loss 0.4314
	Epoch 2675:	Loss 0.4286
	Epoch 2700:	Loss 0.4296
	Epoch 2725:	Loss 0.4273
	Epoch 2750:	Loss 0.4317
	Epoch 2775:	Loss 0.4310
	Epoch 2800:	Loss 0.4240
	Epoch 2825:	Loss 0.4229
	Epoch 2850:	Loss 0.4272
	Epoch 2875:	Loss 0.4223
	Epoch 2900:	Loss 0.4215
	Epoch 2925:	Loss 0.4232
	Epoch 2950:	Loss 0.4208
	Epoch 2975:	Loss 0.4205
	Epoch 3000:	Loss 0.4252
	Epoch 3025:	Loss 0.4199
	Epoch 3050:	Loss 0.4198
	Epoch 3075:	Loss 0.4177
	Epoch 3100:	Loss 0.4161
	Epoch 3125:	Loss 0.4185
	Epoch 3150:	Loss 0.4165
	Epoch 3175:	Loss 0.4160
	Epoch 3200:	Loss 0.4177
	Epoch 3225:	Loss 0.4173
	Epoch 3250:	Loss 0.4128
	Epoch 3275:	Loss 0.4195
	Epoch 3300:	Loss 0.4119
	Epoch 3325:	Loss 0.4139
	Epoch 3350:	Loss 0.4161
	Epoch 3375:	Loss 0.4155
	Epoch 3400:	Loss 0.4108
	Epoch 3425:	Loss 0.4098
	Epoch 3450:	Loss 0.4115
	Epoch 3475:	Loss 0.4105
	Epoch 3500:	Loss 0.4092
	Epoch 3525:	Loss 0.4106
	Epoch 3550:	Loss 0.4088
	Epoch 3575:	Loss 0.4111
	Epoch 3600:	Loss 0.4044
	Epoch 3625:	Loss 0.4112
	Epoch 3650:	Loss 0.4084
	Epoch 3675:	Loss 0.4087
	Epoch 3700:	Loss 0.4061
	Epoch 3725:	Loss 0.4052
	Epoch 3750:	Loss 0.4066
	Epoch 3775:	Loss 0.4062
	Epoch 3800:	Loss 0.4011
	Epoch 3825:	Loss 0.4038
	Epoch 3850:	Loss 0.4056
	Epoch 3875:	Loss 0.4007
	Epoch 3900:	Loss 0.4037
	Epoch 3925:	Loss 0.4003
	Epoch 3950:	Loss 0.4039
	Epoch 3975:	Loss 0.4030
	Epoch 4000:	Loss 0.4018
	Epoch 4025:	Loss 0.4002
	Epoch 4050:	Loss 0.3988
	Epoch 4075:	Loss 0.3966
	Epoch 4100:	Loss 0.4009
	Epoch 4125:	Loss 0.3990
	Epoch 4150:	Loss 0.3972
	Epoch 4175:	Loss 0.3975
	Epoch 4200:	Loss 0.3987
	Epoch 4225:	Loss 0.3979
	Epoch 4250:	Loss 0.3961
	Epoch 4275:	Loss 0.3945
	Epoch 4300:	Loss 0.3969
	Epoch 4325:	Loss 0.3932
	Epoch 4350:	Loss 0.3919
	Epoch 4375:	Loss 0.3936
	Epoch 4400:	Loss 0.3921
	Epoch 4425:	Loss 0.3927
	Epoch 4450:	Loss 0.3912
	Epoch 4475:	Loss 0.3931
	Epoch 4500:	Loss 0.3946
	Epoch 4525:	Loss 0.3935
	Epoch 4550:	Loss 0.3895
	Epoch 4575:	Loss 0.3922
	Epoch 4600:	Loss 0.3899
	Epoch 4625:	Loss 0.3916
	Epoch 4650:	Loss 0.3919
	Epoch 4675:	Loss 0.3889
	Epoch 4700:	Loss 0.3886
	Epoch 4725:	Loss 0.3879
	Epoch 4750:	Loss 0.3874
	Epoch 4775:	Loss 0.3879
	Epoch 4800:	Loss 0.3877
	Epoch 4825:	Loss 0.3868
	Epoch 4850:	Loss 0.3874
	Epoch 4875:	Loss 0.3863
	Epoch 4900:	Loss 0.3839
	Epoch 4925:	Loss 0.3837
	Epoch 4950:	Loss 0.3849
	Epoch 4975:	Loss 0.3874
	Epoch 5000:	Loss 0.3843
Node 0, Pre/Post-Pipelining: 2.167 / 30.076 ms, Bubble: 69.339 ms, Compute: 345.470 ms, Comm: 30.316 ms, Imbalance: 6.610 ms
Node 4, Pre/Post-Pipelining: 2.164 / 30.027 ms, Bubble: 70.647 ms, Compute: 321.628 ms, Comm: 33.652 ms, Imbalance: 28.066 ms
Node 5, Pre/Post-Pipelining: 2.160 / 29.969 ms, Bubble: 70.730 ms, Compute: 321.840 ms, Comm: 33.721 ms, Imbalance: 27.809 ms
Node 2, Pre/Post-Pipelining: 2.165 / 30.010 ms, Bubble: 68.875 ms, Compute: 323.920 ms, Comm: 35.409 ms, Imbalance: 24.520 ms
Node 3, Pre/Post-Pipelining: 2.155 / 30.035 ms, Bubble: 68.845 ms, Compute: 323.565 ms, Comm: 36.181 ms, Imbalance: 24.109 ms
Node 7, Pre/Post-Pipelining: 2.161 / 38.073 ms, Bubble: 63.890 ms, Compute: 340.649 ms, Comm: 27.156 ms, Imbalance: 13.397 ms
Node 1, Pre/Post-Pipelining: 2.162 / 30.070 ms, Bubble: 69.343 ms, Compute: 345.058 ms, Comm: 30.966 ms, Imbalance: 6.378 ms
Node 6, Pre/Post-Pipelining: 2.168 / 38.110 ms, Bubble: 63.846 ms, Compute: 339.550 ms, Comm: 28.057 ms, Imbalance: 13.650 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 2.167 ms
Cluster-Wide Average, Post-Pipelining Overhead: 30.076 ms
Cluster-Wide Average, Bubble: 69.339 ms
Cluster-Wide Average, Compute: 345.470 ms
Cluster-Wide Average, Communication: 30.316 ms
Cluster-Wide Average, Imbalance: 6.610 ms
Node 0, GPU memory consumption: 10.143 GB
Node 1, GPU memory consumption: 8.962 GB
Node 3, GPU memory consumption: 7.893 GB
Node 2, GPU memory consumption: 7.915 GB
Node 4, GPU memory consumption: 7.891 GB
Node 5, GPU memory consumption: 7.917 GB
Node 7, GPU memory consumption: 8.046 GB
Node 6, GPU memory consumption: 8.067 GB
Node 0, Graph-Level Communication Throughput: 99.137 Gbps, Time: 56.522 ms
Node 1, Graph-Level Communication Throughput: 102.470 Gbps, Time: 56.449 ms
Node 2, Graph-Level Communication Throughput: 100.497 Gbps, Time: 55.757 ms
Node 3, Graph-Level Communication Throughput: 107.431 Gbps, Time: 53.842 ms
Node 4, Graph-Level Communication Throughput: 104.064 Gbps, Time: 53.846 ms
Node 5, Graph-Level Communication Throughput: 105.258 Gbps, Time: 54.954 ms
Node 6, Graph-Level Communication Throughput: 103.252 Gbps, Time: 54.269 ms
Node 7, Graph-Level Communication Throughput: 105.886 Gbps, Time: 54.628 ms
------------------------node id 0,  per-epoch time: 0.484329 s---------------
------------------------node id 1,  per-epoch time: 0.484329 s---------------
------------------------node id 2,  per-epoch time: 0.484329 s---------------
------------------------node id 3,  per-epoch time: 0.484329 s---------------
------------------------node id 4,  per-epoch time: 0.484329 s---------------
------------------------node id 5,  per-epoch time: 0.484329 s---------------
------------------------node id 6,  per-epoch time: 0.484329 s---------------
------------------------node id 7,  per-epoch time: 0.484329 s---------------
************ Profiling Results ************
	Bubble: 152.894102 (ms) (31.56 percentage)
	Compute: 259.981311 (ms) (53.67 percentage)
	GraphCommComputeOverhead: 10.015176 (ms) (2.07 percentage)
	GraphCommNetwork: 55.039003 (ms) (11.36 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.468425 (ms) (1.34 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.041 GB
	Graph-level communication (cluster-wide, per-epoch): 5.303 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 6.356 GB
	Aggregated layer-level communication throughput: 280.149 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
