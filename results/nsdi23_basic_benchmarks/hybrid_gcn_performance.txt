gnerv1
Tue Aug  1 16:36:26 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   46C    P2    71W / 230W |   3112MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   54C    P2    81W / 230W |   3978MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   50C    P2    66W / 230W |   4386MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   47C    P2    64W / 230W |   3814MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     45821      C   ...onda3/envs/gnn/bin/python     3109MiB |
|    1   N/A  N/A     45822      C   ...onda3/envs/gnn/bin/python     3975MiB |
|    2   N/A  N/A     45823      C   ...onda3/envs/gnn/bin/python     4383MiB |
|    3   N/A  N/A     45824      C   ...onda3/envs/gnn/bin/python     3811MiB |
+-----------------------------------------------------------------------------+
[ 22%] Built target context
[ 22%] Built target core
[ 77%] Built target cudahelp
[ 94%] Built target OSDI2023_MULTI_NODES_gcnii
[ 94%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_graphsage
[ 94%] Built target OSDI2023_MULTI_NODES_gcn
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 5 on machine gnerv3
Initialized node 2 on machine gnerv2
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.903 seconds.
Building the CSC structure...
        It takes 1.916 seconds.
Building the CSC structure...
        It takes 2.070 seconds.
Building the CSC structure...
        It takes 2.087 seconds.
Building the CSC structure...
        It takes 2.327 seconds.
Building the CSC structure...
        It takes 2.424 seconds.
Building the CSC structure...
        It takes 2.427 seconds.
Building the CSC structure...
        It takes 2.648 seconds.
Building the CSC structure...
        It takes 1.840 seconds.
        It takes 1.827 seconds.
        It takes 1.840 seconds.
        It takes 1.927 seconds.
        It takes 2.111 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.340 seconds.
        It takes 2.376 seconds.
        It takes 2.339 seconds.
        It takes 0.288 seconds.
Building the Label Vector...
        It takes 0.313 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 0.042 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.255 seconds.
Building the Label Vector...
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.256 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
        It takes 0.783 seconds.
        It takes 1.025 seconds.
        It takes 0.513 seconds.
        It takes 1.050 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
        It takes 0.101 seconds.
        It takes 0.101 seconds.
        It takes 0.101 seconds.
        It takes 0.102 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 54.620 Gbps (per GPU), 436.960 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.365 Gbps (per GPU), 434.922 Gbps (aggregated)
The layer-level communication performance: 54.353 Gbps (per GPU), 434.824 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.167 Gbps (per GPU), 433.338 Gbps (aggregated)
The layer-level communication performance: 54.130 Gbps (per GPU), 433.043 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 53.961 Gbps (per GPU), 431.691 Gbps (aggregated)
The layer-level communication performance: 53.934 Gbps (per GPU), 431.476 Gbps (aggregated)
The layer-level communication performance: 53.899 Gbps (per GPU), 431.193 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.920 Gbps (per GPU), 1255.357 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.926 Gbps (per GPU), 1255.406 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.929 Gbps (per GPU), 1255.430 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.923 Gbps (per GPU), 1255.380 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.919 Gbps (per GPU), 1255.355 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.931 Gbps (per GPU), 1255.451 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.923 Gbps (per GPU), 1255.380 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.861 Gbps (per GPU), 1254.887 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.749 Gbps (per GPU), 805.990 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.747 Gbps (per GPU), 805.977 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.748 Gbps (per GPU), 805.983 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.743 Gbps (per GPU), 805.945 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.749 Gbps (per GPU), 805.990 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.737 Gbps (per GPU), 805.893 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.749 Gbps (per GPU), 805.990 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.697 Gbps (per GPU), 805.577 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.620 Gbps (per GPU), 268.958 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.620 Gbps (per GPU), 268.956 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.619 Gbps (per GPU), 268.954 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.619 Gbps (per GPU), 268.952 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.619 Gbps (per GPU), 268.951 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.620 Gbps (per GPU), 268.960 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.619 Gbps (per GPU), 268.954 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.619 Gbps (per GPU), 268.955 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.39ms  2.44ms  2.21ms  1.10  8.38K  3.53M
 chk_1  2.71ms  2.65ms  2.48ms  1.09  6.74K  3.60M
 chk_2  2.61ms  2.52ms  2.37ms  1.10  7.27K  3.53M
 chk_3  2.62ms  2.54ms  2.38ms  1.10  7.92K  3.61M
 chk_4  2.53ms  2.51ms  2.38ms  1.06  5.33K  3.68M
 chk_5  2.52ms  2.44ms  2.31ms  1.09 10.07K  3.45M
 chk_6  2.70ms  2.62ms  2.41ms  1.12  9.41K  3.48M
 chk_7  2.45ms  2.46ms  2.31ms  1.07  8.12K  3.60M
 chk_8  2.62ms  2.60ms  2.48ms  1.06  6.09K  3.64M
 chk_9  2.49ms  2.35ms  2.11ms  1.18 11.10K  3.38M
chk_10  2.71ms  2.63ms  2.53ms  1.07  5.67K  3.63M
chk_11  2.56ms  2.49ms  2.32ms  1.10  8.16K  3.54M
chk_12  2.78ms  2.69ms  2.54ms  1.09  7.24K  3.55M
chk_13  2.59ms  2.54ms  2.41ms  1.08  5.41K  3.68M
chk_14  2.86ms  2.77ms  2.61ms  1.10  7.14K  3.53M
chk_15  2.69ms  2.59ms  2.37ms  1.13  9.25K  3.49M
chk_16  2.49ms  2.45ms  2.36ms  1.06  4.78K  3.77M
chk_17  2.67ms  2.59ms  2.43ms  1.10  6.85K  3.60M
chk_18  2.48ms  2.38ms  2.23ms  1.11  7.47K  3.57M
chk_19  2.53ms  2.47ms  2.36ms  1.07  4.88K  3.75M
chk_20  2.53ms  2.48ms  2.32ms  1.09  7.00K  3.63M
chk_21  2.52ms  2.47ms  2.35ms  1.07  5.41K  3.68M
chk_22  2.73ms  2.62ms  2.36ms  1.15 11.07K  3.39M
chk_23  2.63ms  2.54ms  2.39ms  1.10  7.23K  3.64M
chk_24  2.64ms  2.56ms  2.34ms  1.13 10.13K  3.43M
chk_25  2.49ms  2.42ms  2.28ms  1.09  6.40K  3.57M
chk_26  2.69ms  2.63ms  2.51ms  1.07  5.78K  3.55M
chk_27  2.56ms  2.48ms  2.26ms  1.13  9.34K  3.48M
chk_28  2.89ms  2.79ms  2.65ms  1.09  6.37K  3.57M
chk_29  2.68ms  2.60ms  2.49ms  1.07  5.16K  3.78M
chk_30  2.56ms  2.55ms  2.38ms  1.08  5.44K  3.67M
chk_31  2.72ms  2.70ms  2.50ms  1.09  6.33K  3.63M
   Avg  2.61  2.55  2.39
   Max  2.89  2.79  2.65
   Min  2.39  2.35  2.11
 Ratio  1.21  1.19  1.26
   Var  0.01  0.01  0.01
Profiling takes 2.816 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 328.282 ms
Partition 0 [0, 4) has cost: 328.282 ms
Partition 1 [4, 8) has cost: 326.236 ms
Partition 2 [8, 12) has cost: 326.236 ms
Partition 3 [12, 16) has cost: 326.236 ms
Partition 4 [16, 20) has cost: 326.236 ms
Partition 5 [20, 24) has cost: 326.236 ms
Partition 6 [24, 28) has cost: 326.236 ms
Partition 7 [28, 32) has cost: 321.108 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 157.926 ms
GPU 0, Compute+Comm Time: 123.550 ms, Bubble Time: 28.208 ms, Imbalance Overhead: 6.168 ms
GPU 1, Compute+Comm Time: 122.320 ms, Bubble Time: 27.909 ms, Imbalance Overhead: 7.698 ms
GPU 2, Compute+Comm Time: 122.320 ms, Bubble Time: 27.756 ms, Imbalance Overhead: 7.850 ms
GPU 3, Compute+Comm Time: 122.320 ms, Bubble Time: 27.569 ms, Imbalance Overhead: 8.038 ms
GPU 4, Compute+Comm Time: 122.320 ms, Bubble Time: 27.438 ms, Imbalance Overhead: 8.169 ms
GPU 5, Compute+Comm Time: 122.320 ms, Bubble Time: 27.387 ms, Imbalance Overhead: 8.219 ms
GPU 6, Compute+Comm Time: 122.320 ms, Bubble Time: 27.337 ms, Imbalance Overhead: 8.270 ms
GPU 7, Compute+Comm Time: 120.407 ms, Bubble Time: 27.501 ms, Imbalance Overhead: 10.018 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 298.342 ms
GPU 0, Compute+Comm Time: 227.998 ms, Bubble Time: 51.871 ms, Imbalance Overhead: 18.473 ms
GPU 1, Compute+Comm Time: 231.214 ms, Bubble Time: 51.710 ms, Imbalance Overhead: 15.418 ms
GPU 2, Compute+Comm Time: 231.214 ms, Bubble Time: 51.800 ms, Imbalance Overhead: 15.329 ms
GPU 3, Compute+Comm Time: 231.214 ms, Bubble Time: 51.890 ms, Imbalance Overhead: 15.239 ms
GPU 4, Compute+Comm Time: 231.214 ms, Bubble Time: 52.143 ms, Imbalance Overhead: 14.985 ms
GPU 5, Compute+Comm Time: 231.214 ms, Bubble Time: 52.487 ms, Imbalance Overhead: 14.641 ms
GPU 6, Compute+Comm Time: 231.214 ms, Bubble Time: 52.810 ms, Imbalance Overhead: 14.319 ms
GPU 7, Compute+Comm Time: 232.029 ms, Bubble Time: 53.749 ms, Imbalance Overhead: 12.563 ms
The estimated cost of the whole pipeline: 479.082 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 654.518 ms
Partition 0 [0, 8) has cost: 654.518 ms
Partition 1 [8, 16) has cost: 652.472 ms
Partition 2 [16, 24) has cost: 652.472 ms
Partition 3 [24, 32) has cost: 647.344 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.654 ms
GPU 0, Compute+Comm Time: 135.288 ms, Bubble Time: 26.521 ms, Imbalance Overhead: 3.846 ms
GPU 1, Compute+Comm Time: 134.661 ms, Bubble Time: 25.796 ms, Imbalance Overhead: 5.197 ms
GPU 2, Compute+Comm Time: 134.661 ms, Bubble Time: 25.371 ms, Imbalance Overhead: 5.622 ms
GPU 3, Compute+Comm Time: 133.772 ms, Bubble Time: 24.985 ms, Imbalance Overhead: 6.898 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 300.244 ms
GPU 0, Compute+Comm Time: 243.291 ms, Bubble Time: 45.588 ms, Imbalance Overhead: 11.366 ms
GPU 1, Compute+Comm Time: 244.727 ms, Bubble Time: 46.004 ms, Imbalance Overhead: 9.514 ms
GPU 2, Compute+Comm Time: 244.727 ms, Bubble Time: 46.521 ms, Imbalance Overhead: 8.996 ms
GPU 3, Compute+Comm Time: 245.185 ms, Bubble Time: 48.083 ms, Imbalance Overhead: 6.977 ms
    The estimated cost with 2 DP ways is 489.193 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1306.990 ms
Partition 0 [0, 16) has cost: 1306.990 ms
Partition 1 [16, 32) has cost: 1299.816 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 213.375 ms
GPU 0, Compute+Comm Time: 185.156 ms, Bubble Time: 23.182 ms, Imbalance Overhead: 5.036 ms
GPU 1, Compute+Comm Time: 184.410 ms, Bubble Time: 23.337 ms, Imbalance Overhead: 5.627 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 341.517 ms
GPU 0, Compute+Comm Time: 297.699 ms, Bubble Time: 37.166 ms, Imbalance Overhead: 6.652 ms
GPU 1, Compute+Comm Time: 298.641 ms, Bubble Time: 37.509 ms, Imbalance Overhead: 5.367 ms
    The estimated cost with 4 DP ways is 582.637 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2606.806 ms
Partition 0 [0, 32) has cost: 2606.806 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 560.361 ms
GPU 0, Compute+Comm Time: 560.361 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 676.578 ms
GPU 0, Compute+Comm Time: 676.578 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1298.786 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 48)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [96, 144)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 48)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [96, 144)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [48, 96)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [144, 190)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [48, 96)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [144, 190)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 48)...
+++++++++ Node 5 initializing the weights for op[96, 144)...
+++++++++ Node 2 initializing the weights for op[48, 96)...
+++++++++ Node 6 initializing the weights for op[144, 190)...
+++++++++ Node 3 initializing the weights for op[48, 96)...
+++++++++ Node 4 initializing the weights for op[96, 144)...
+++++++++ Node 0 initializing the weights for op[0, 48)...
+++++++++ Node 7 initializing the weights for op[144, 190)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.4067
	Epoch 50:	Loss 3.2769
	Epoch 75:	Loss 3.2067
	Epoch 100:	Loss 3.1219
	Epoch 125:	Loss 2.9283
	Epoch 150:	Loss 2.8164
	Epoch 175:	Loss 2.7415
	Epoch 200:	Loss 2.6117
	Epoch 225:	Loss 2.5635
	Epoch 250:	Loss 2.4867
	Epoch 275:	Loss 2.3883
	Epoch 300:	Loss 2.3356
	Epoch 325:	Loss 2.2684
	Epoch 350:	Loss 2.1655
	Epoch 375:	Loss 2.1243
	Epoch 400:	Loss 2.0291
	Epoch 425:	Loss 1.9260
	Epoch 450:	Loss 1.8623
	Epoch 475:	Loss 1.7657
	Epoch 500:	Loss 1.7167
	Epoch 525:	Loss 1.6331
	Epoch 550:	Loss 1.5403
	Epoch 575:	Loss 1.4928
	Epoch 600:	Loss 1.4006
	Epoch 625:	Loss 1.4283
	Epoch 650:	Loss 1.3400
	Epoch 675:	Loss 1.2511
	Epoch 700:	Loss 1.2341
	Epoch 725:	Loss 1.2355
	Epoch 750:	Loss 1.1903
	Epoch 775:	Loss 1.1729
	Epoch 800:	Loss 1.1387
	Epoch 825:	Loss 1.0968
	Epoch 850:	Loss 1.0609
	Epoch 875:	Loss 1.0428
	Epoch 900:	Loss 1.0438
	Epoch 925:	Loss 1.0545
	Epoch 950:	Loss 1.0480
	Epoch 975:	Loss 1.0043
	Epoch 1000:	Loss 1.0009
	Epoch 1025:	Loss 1.0839
	Epoch 1050:	Loss 1.0202
	Epoch 1075:	Loss 1.0064
	Epoch 1100:	Loss 0.9974
	Epoch 1125:	Loss 1.0385
	Epoch 1150:	Loss 1.0369
	Epoch 1175:	Loss 1.0166
	Epoch 1200:	Loss 0.9950
	Epoch 1225:	Loss 0.9763
	Epoch 1250:	Loss 0.9765
	Epoch 1275:	Loss 0.9763
	Epoch 1300:	Loss 0.9574
	Epoch 1325:	Loss 0.9864
	Epoch 1350:	Loss 0.9920
	Epoch 1375:	Loss 1.0007
	Epoch 1400:	Loss 0.9825
	Epoch 1425:	Loss 0.9794
	Epoch 1450:	Loss 0.9618
	Epoch 1475:	Loss 0.9597
	Epoch 1500:	Loss 0.9578
	Epoch 1525:	Loss 0.9959
	Epoch 1550:	Loss 0.9870
	Epoch 1575:	Loss 1.0011
	Epoch 1600:	Loss 0.9761
	Epoch 1625:	Loss 0.9284
	Epoch 1650:	Loss 0.8869
	Epoch 1675:	Loss 0.8863
	Epoch 1700:	Loss 0.9255
	Epoch 1725:	Loss 0.9580
	Epoch 1750:	Loss 0.9303
	Epoch 1775:	Loss 0.8959
	Epoch 1800:	Loss 0.8788
	Epoch 1825:	Loss 0.8356
	Epoch 1850:	Loss 0.8335
	Epoch 1875:	Loss 0.8223
	Epoch 1900:	Loss 0.8400
	Epoch 1925:	Loss 0.8660
	Epoch 1950:	Loss 0.8326
	Epoch 1975:	Loss 0.8177
	Epoch 2000:	Loss 0.8166
	Epoch 2025:	Loss 0.8134
	Epoch 2050:	Loss 0.7894
	Epoch 2075:	Loss 0.7801
	Epoch 2100:	Loss 0.7837
	Epoch 2125:	Loss 0.7891
	Epoch 2150:	Loss 0.7774
	Epoch 2175:	Loss 0.7745
	Epoch 2200:	Loss 0.7571
	Epoch 2225:	Loss 0.7792
	Epoch 2250:	Loss 0.7512
	Epoch 2275:	Loss 0.7463
	Epoch 2300:	Loss 0.7413
	Epoch 2325:	Loss 0.7360
	Epoch 2350:	Loss 0.7543
	Epoch 2375:	Loss 0.7475
	Epoch 2400:	Loss 0.7489
	Epoch 2425:	Loss 0.7456
	Epoch 2450:	Loss 0.7583
	Epoch 2475:	Loss 0.7400
	Epoch 2500:	Loss 0.7350
	Epoch 2525:	Loss 0.7259
	Epoch 2550:	Loss 0.7169
	Epoch 2575:	Loss 0.7199
	Epoch 2600:	Loss 0.7105
	Epoch 2625:	Loss 0.7159
	Epoch 2650:	Loss 0.7161
	Epoch 2675:	Loss 0.7074
	Epoch 2700:	Loss 0.7062
	Epoch 2725:	Loss 0.7281
	Epoch 2750:	Loss 0.6951
	Epoch 2775:	Loss 0.6950
	Epoch 2800:	Loss 0.6882
	Epoch 2825:	Loss 0.7190
	Epoch 2850:	Loss 0.6922
	Epoch 2875:	Loss 0.6879
	Epoch 2900:	Loss 0.6855
	Epoch 2925:	Loss 0.6838
	Epoch 2950:	Loss 0.6869
	Epoch 2975:	Loss 0.6761
	Epoch 3000:	Loss 0.6724
	Epoch 3025:	Loss 0.6756
	Epoch 3050:	Loss 0.6735
	Epoch 3075:	Loss 0.6697
	Epoch 3100:	Loss 0.6819
	Epoch 3125:	Loss 0.6986
	Epoch 3150:	Loss 0.6918
	Epoch 3175:	Loss 0.6824
	Epoch 3200:	Loss 0.6816
	Epoch 3225:	Loss 0.6964
	Epoch 3250:	Loss 0.6910
	Epoch 3275:	Loss 0.6946
	Epoch 3300:	Loss 0.6968
	Epoch 3325:	Loss 0.6892
	Epoch 3350:	Loss 0.6849
	Epoch 3375:	Loss 0.6950
	Epoch 3400:	Loss 0.6807
	Epoch 3425:	Loss 0.6987
	Epoch 3450:	Loss 0.6900
	Epoch 3475:	Loss 0.6759
	Epoch 3500:	Loss 0.6744
	Epoch 3525:	Loss 0.6537
	Epoch 3550:	Loss 0.6516
	Epoch 3575:	Loss 0.6575
	Epoch 3600:	Loss 0.6538
	Epoch 3625:	Loss 0.6715
	Epoch 3650:	Loss 0.6568
	Epoch 3675:	Loss 0.6607
	Epoch 3700:	Loss 0.6484
	Epoch 3725:	Loss 0.6527
	Epoch 3750:	Loss 0.6687
	Epoch 3775:	Loss 0.6553
	Epoch 3800:	Loss 0.6456
	Epoch 3825:	Loss 0.6536
	Epoch 3850:	Loss 0.6542
	Epoch 3875:	Loss 0.6526
	Epoch 3900:	Loss 0.6503
	Epoch 3925:	Loss 0.6375
	Epoch 3950:	Loss 0.6273
	Epoch 3975:	Loss 0.6297
	Epoch 4000:	Loss 0.6307
	Epoch 4025:	Loss 0.6287
	Epoch 4050:	Loss 0.6227
	Epoch 4075:	Loss 0.6141
	Epoch 4100:	Loss 0.6267
	Epoch 4125:	Loss 0.8274
	Epoch 4150:	Loss 0.6800
	Epoch 4175:	Loss 0.6549
	Epoch 4200:	Loss 0.6573
	Epoch 4225:	Loss 0.6434
	Epoch 4250:	Loss 0.6370
	Epoch 4275:	Loss 0.6328
	Epoch 4300:	Loss 0.6278
	Epoch 4325:	Loss 0.6291
	Epoch 4350:	Loss 0.6196
	Epoch 4375:	Loss 0.6225
	Epoch 4400:	Loss 0.6231
	Epoch 4425:	Loss 0.6189
	Epoch 4450:	Loss 0.6103
	Epoch 4475:	Loss 0.6108
	Epoch 4500:	Loss 0.6174
	Epoch 4525:	Loss 0.6279
	Epoch 4550:	Loss 0.6181
	Epoch 4575:	Loss 0.6181
	Epoch 4600:	Loss 0.6148
	Epoch 4625:	Loss 0.6149
	Epoch 4650:	Loss 0.6138
	Epoch 4675:	Loss 0.6007
	Epoch 4700:	Loss 0.6057
	Epoch 4725:	Loss 0.6284
	Epoch 4750:	Loss 0.6260
	Epoch 4775:	Loss 0.6126
	Epoch 4800:	Loss 0.6158
	Epoch 4825:	Loss 0.6144
	Epoch 4850:	Loss 0.6054
	Epoch 4875:	Loss 0.6039
	Epoch 4900:	Loss 0.6029
	Epoch 4925:	Loss 0.6137
	Epoch 4950:	Loss 0.5943
	Epoch 4975:	Loss 0.5976
Node 2, Pre/Post-Pipelining: 2.229 / 3.126 ms, Bubble: 62.876 ms, Compute: 306.419 ms, Comm: 18.634 ms, Imbalance: 16.504 ms
Node 6, Pre/Post-Pipelining: 2.635 / 11.028 ms, Bubble: 55.573 ms, Compute: 316.363 ms, Comm: 16.849 ms, Imbalance: 7.658 ms
Node 3, Pre/Post-Pipelining: 2.228 / 3.125 ms, Bubble: 62.897 ms, Compute: 307.880 ms, Comm: 16.903 ms, Imbalance: 16.704 ms
Node 7, Pre/Post-Pipelining: 2.635 / 11.155 ms, Bubble: 55.339 ms, Compute: 319.376 ms, Comm: 13.785 ms, Imbalance: 7.677 ms
	Epoch 5000:	Loss 0.5988
Node 0, Pre/Post-Pipelining: 2.179 / 3.750 ms, Bubble: 63.246 ms, Compute: 311.801 ms, Comm: 15.736 ms, Imbalance: 12.798 ms
Node 1, Pre/Post-Pipelining: 2.181 / 3.758 ms, Bubble: 63.205 ms, Compute: 314.031 ms, Comm: 13.672 ms, Imbalance: 12.664 ms
Node 5, Pre/Post-Pipelining: 2.612 / 3.156 ms, Bubble: 63.076 ms, Compute: 306.475 ms, Comm: 17.180 ms, Imbalance: 17.909 ms
Node 4, Pre/Post-Pipelining: 2.611 / 3.171 ms, Bubble: 62.982 ms, Compute: 304.686 ms, Comm: 19.720 ms, Imbalance: 17.208 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 2.179 ms
Cluster-Wide Average, Post-Pipelining Overhead: 3.750 ms
Cluster-Wide Average, Bubble: 63.246 ms
Cluster-Wide Average, Compute: 311.801 ms
Cluster-Wide Average, Communication: 15.736 ms
Cluster-Wide Average, Imbalance: 12.798 ms
Node 0, GPU memory consumption: 8.286 GB
Node 4, GPU memory consumption: 6.967 GB
Node 1, GPU memory consumption: 7.462 GB
Node 7, GPU memory consumption: 6.778 GB
Node 3, GPU memory consumption: 6.874 GB
Node 5, GPU memory consumption: 6.897 GB
Node 2, GPU memory consumption: 6.991 GB
Node 6, GPU memory consumption: 6.893 GB
Node 0, Graph-Level Communication Throughput: 115.199 Gbps, Time: 52.489 ms
Node 4, Graph-Level Communication Throughput: 128.214 Gbps, Time: 47.161 ms
Node 1, Graph-Level Communication Throughput: 106.826 Gbps, Time: 50.829 ms
Node 5, Graph-Level Communication Throughput: 97.631 Gbps, Time: 55.616 ms
Node 2, Graph-Level Communication Throughput: 124.123 Gbps, Time: 48.715 ms
Node 6, Graph-Level Communication Throughput: 121.360 Gbps, Time: 49.825 ms
Node 3, Graph-Level Communication Throughput: 106.112 Gbps, Time: 51.171 ms
Node 7, Graph-Level Communication Throughput: 106.900 Gbps, Time: 50.794 ms
------------------------node id 0,  per-epoch time: 0.409848 s---------------
------------------------node id 4,  per-epoch time: 0.409848 s---------------
------------------------node id 1,  per-epoch time: 0.409848 s---------------
------------------------node id 5,  per-epoch time: 0.409848 s---------------
------------------------node id 2,  per-epoch time: 0.409848 s---------------
------------------------node id 6,  per-epoch time: 0.409848 s---------------
------------------------node id 3,  per-epoch time: 0.409848 s---------------
------------------------node id 7,  per-epoch time: 0.409848 s---------------
************ Profiling Results ************
	Bubble: 100.226194 (ms) (24.45 percentage)
	Compute: 242.738371 (ms) (59.21 percentage)
	GraphCommComputeOverhead: 9.864350 (ms) (2.41 percentage)
	GraphCommNetwork: 50.829654 (ms) (12.40 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.285237 (ms) (1.53 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.521 GB
	Graph-level communication (cluster-wide, per-epoch): 5.344 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 5.868 GB
	Aggregated layer-level communication throughput: 270.109 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
