Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.871 seconds.
Building the CSC structure...
        It takes 2.073 seconds.
Building the CSC structure...
        It takes 2.075 seconds.
Building the CSC structure...
        It takes 2.282 seconds.
Building the CSC structure...
        It takes 2.427 seconds.
Building the CSC structure...
        It takes 2.461 seconds.
Building the CSC structure...
        It takes 2.685 seconds.
Building the CSC structure...
        It takes 2.689 seconds.
Building the CSC structure...
        It takes 1.805 seconds.
        It takes 1.839 seconds.
        It takes 1.958 seconds.
        It takes 2.281 seconds.
Building the Feature Vector...
        It takes 2.320 seconds.
        It takes 2.397 seconds.
        It takes 0.251 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 2.277 seconds.
        It takes 2.395 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.250 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.283 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.040 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.306 seconds.
Building the Label Vector...
        It takes 0.289 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
        It takes 0.040 seconds.
        It takes 0.284 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Building the Feature Vector...
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.283 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.032 Gbps (per GPU), 440.259 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.785 Gbps (per GPU), 438.278 Gbps (aggregated)
The layer-level communication performance: 54.774 Gbps (per GPU), 438.193 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.580 Gbps (per GPU), 436.636 Gbps (aggregated)
The layer-level communication performance: 54.555 Gbps (per GPU), 436.439 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.373 Gbps (per GPU), 434.984 Gbps (aggregated)
The layer-level communication performance: 54.334 Gbps (per GPU), 434.668 Gbps (aggregated)
The layer-level communication performance: 54.300 Gbps (per GPU), 434.397 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.519 Gbps (per GPU), 1276.149 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.519 Gbps (per GPU), 1276.149 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.516 Gbps (per GPU), 1276.125 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.516 Gbps (per GPU), 1276.125 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.513 Gbps (per GPU), 1276.101 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.446 Gbps (per GPU), 1275.566 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.516 Gbps (per GPU), 1276.125 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.516 Gbps (per GPU), 1276.125 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.724 Gbps (per GPU), 805.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.724 Gbps (per GPU), 805.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.725 Gbps (per GPU), 805.796 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.722 Gbps (per GPU), 805.777 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.725 Gbps (per GPU), 805.796 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.724 Gbps (per GPU), 805.790 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.723 Gbps (per GPU), 805.783 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.724 Gbps (per GPU), 805.790 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.144 Gbps (per GPU), 257.151 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.144 Gbps (per GPU), 257.151 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.144 Gbps (per GPU), 257.149 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.143 Gbps (per GPU), 257.147 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.143 Gbps (per GPU), 257.141 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.143 Gbps (per GPU), 257.145 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.143 Gbps (per GPU), 257.145 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.142 Gbps (per GPU), 257.133 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.43ms  2.54ms  2.33ms  1.09  8.38K  3.53M
 chk_1  2.79ms  2.74ms  2.58ms  1.08  6.74K  3.60M
 chk_2  2.75ms  2.64ms  2.48ms  1.11  7.27K  3.53M
 chk_3  2.74ms  2.67ms  2.50ms  1.10  7.92K  3.61M
 chk_4  2.68ms  2.62ms  2.52ms  1.06  5.33K  3.68M
 chk_5  2.64ms  2.56ms  2.33ms  1.14 10.07K  3.45M
 chk_6  2.83ms  2.73ms  2.52ms  1.12  9.41K  3.48M
 chk_7  2.70ms  2.60ms  2.46ms  1.10  8.12K  3.60M
 chk_8  2.77ms  2.72ms  2.59ms  1.07  6.09K  3.64M
 chk_9  2.62ms  2.47ms  2.22ms  1.18 11.10K  3.38M
chk_10  2.82ms  2.75ms  2.62ms  1.07  5.67K  3.63M
chk_11  2.69ms  2.59ms  2.41ms  1.12  8.16K  3.54M
chk_12  2.90ms  2.80ms  2.65ms  1.10  7.24K  3.55M
chk_13  2.71ms  2.66ms  2.52ms  1.08  5.41K  3.68M
chk_14  2.96ms  2.85ms  2.70ms  1.10  7.14K  3.53M
chk_15  2.82ms  2.70ms  2.50ms  1.13  9.25K  3.49M
chk_16  2.66ms  2.58ms  2.51ms  1.06  4.78K  3.77M
chk_17  2.78ms  2.68ms  2.55ms  1.09  6.85K  3.60M
chk_18  2.60ms  2.49ms  2.36ms  1.10  7.47K  3.57M
chk_19  2.67ms  2.74ms  2.50ms  1.10  4.88K  3.75M
chk_20  2.67ms  2.58ms  2.45ms  1.09  7.00K  3.63M
chk_21  2.64ms  2.58ms  2.45ms  1.07  5.41K  3.68M
chk_22  2.83ms  2.71ms  2.45ms  1.15 11.07K  3.39M
chk_23  2.77ms  2.65ms  2.52ms  1.10  7.23K  3.64M
chk_24  2.77ms  2.68ms  2.47ms  1.12 10.13K  3.43M
chk_25  2.60ms  2.53ms  2.40ms  1.09  6.40K  3.57M
chk_26  2.81ms  2.75ms  2.62ms  1.08  5.78K  3.55M
chk_27  2.78ms  2.59ms  2.40ms  1.16  9.34K  3.48M
chk_28  3.04ms  2.90ms  2.74ms  1.11  6.37K  3.57M
chk_29  2.83ms  2.80ms  2.64ms  1.07  5.16K  3.78M
chk_30  2.68ms  2.67ms  2.51ms  1.07  5.44K  3.67M
chk_31  2.83ms  2.78ms  2.65ms  1.07  6.33K  3.63M
   Avg  2.74  2.67  2.50
   Max  3.04  2.90  2.74
   Min  2.43  2.47  2.22
 Ratio  1.25  1.17  1.23
   Var  0.01  0.01  0.01
Profiling takes 2.939 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 343.915 ms
Partition 0 [0, 4) has cost: 343.915 ms
Partition 1 [4, 8) has cost: 341.456 ms
Partition 2 [8, 12) has cost: 341.456 ms
Partition 3 [12, 16) has cost: 341.456 ms
Partition 4 [16, 20) has cost: 341.456 ms
Partition 5 [20, 24) has cost: 341.456 ms
Partition 6 [24, 28) has cost: 341.456 ms
Partition 7 [28, 32) has cost: 336.242 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 162.616 ms
GPU 0, Compute+Comm Time: 128.508 ms, Bubble Time: 29.035 ms, Imbalance Overhead: 5.073 ms
GPU 1, Compute+Comm Time: 127.069 ms, Bubble Time: 28.669 ms, Imbalance Overhead: 6.878 ms
GPU 2, Compute+Comm Time: 127.069 ms, Bubble Time: 28.489 ms, Imbalance Overhead: 7.058 ms
GPU 3, Compute+Comm Time: 127.069 ms, Bubble Time: 28.290 ms, Imbalance Overhead: 7.258 ms
GPU 4, Compute+Comm Time: 127.069 ms, Bubble Time: 28.146 ms, Imbalance Overhead: 7.401 ms
GPU 5, Compute+Comm Time: 127.069 ms, Bubble Time: 28.107 ms, Imbalance Overhead: 7.440 ms
GPU 6, Compute+Comm Time: 127.069 ms, Bubble Time: 28.069 ms, Imbalance Overhead: 7.479 ms
GPU 7, Compute+Comm Time: 125.197 ms, Bubble Time: 28.260 ms, Imbalance Overhead: 9.159 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 308.474 ms
GPU 0, Compute+Comm Time: 238.137 ms, Bubble Time: 53.739 ms, Imbalance Overhead: 16.597 ms
GPU 1, Compute+Comm Time: 241.480 ms, Bubble Time: 53.549 ms, Imbalance Overhead: 13.444 ms
GPU 2, Compute+Comm Time: 241.480 ms, Bubble Time: 53.613 ms, Imbalance Overhead: 13.381 ms
GPU 3, Compute+Comm Time: 241.480 ms, Bubble Time: 53.676 ms, Imbalance Overhead: 13.317 ms
GPU 4, Compute+Comm Time: 241.480 ms, Bubble Time: 53.941 ms, Imbalance Overhead: 13.052 ms
GPU 5, Compute+Comm Time: 241.480 ms, Bubble Time: 54.301 ms, Imbalance Overhead: 12.692 ms
GPU 6, Compute+Comm Time: 241.480 ms, Bubble Time: 54.637 ms, Imbalance Overhead: 12.356 ms
GPU 7, Compute+Comm Time: 242.499 ms, Bubble Time: 55.575 ms, Imbalance Overhead: 10.399 ms
The estimated cost of the whole pipeline: 494.644 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 685.371 ms
Partition 0 [0, 8) has cost: 685.371 ms
Partition 1 [8, 16) has cost: 682.912 ms
Partition 2 [16, 24) has cost: 682.912 ms
Partition 3 [24, 32) has cost: 677.698 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.352 ms
GPU 0, Compute+Comm Time: 139.950 ms, Bubble Time: 27.112 ms, Imbalance Overhead: 3.290 ms
GPU 1, Compute+Comm Time: 139.187 ms, Bubble Time: 26.489 ms, Imbalance Overhead: 4.677 ms
GPU 2, Compute+Comm Time: 139.187 ms, Bubble Time: 26.096 ms, Imbalance Overhead: 5.069 ms
GPU 3, Compute+Comm Time: 138.318 ms, Bubble Time: 25.714 ms, Imbalance Overhead: 6.320 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 312.173 ms
GPU 0, Compute+Comm Time: 253.530 ms, Bubble Time: 47.214 ms, Imbalance Overhead: 11.430 ms
GPU 1, Compute+Comm Time: 255.061 ms, Bubble Time: 47.614 ms, Imbalance Overhead: 9.498 ms
GPU 2, Compute+Comm Time: 255.061 ms, Bubble Time: 48.072 ms, Imbalance Overhead: 9.040 ms
GPU 3, Compute+Comm Time: 255.609 ms, Bubble Time: 49.442 ms, Imbalance Overhead: 7.123 ms
    The estimated cost with 2 DP ways is 506.652 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1368.282 ms
Partition 0 [0, 16) has cost: 1368.282 ms
Partition 1 [16, 32) has cost: 1360.609 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 218.629 ms
GPU 0, Compute+Comm Time: 189.996 ms, Bubble Time: 23.610 ms, Imbalance Overhead: 5.023 ms
GPU 1, Compute+Comm Time: 189.193 ms, Bubble Time: 24.026 ms, Imbalance Overhead: 5.410 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 354.377 ms
GPU 0, Compute+Comm Time: 309.021 ms, Bubble Time: 38.478 ms, Imbalance Overhead: 6.878 ms
GPU 1, Compute+Comm Time: 310.035 ms, Bubble Time: 38.475 ms, Imbalance Overhead: 5.867 ms
    The estimated cost with 4 DP ways is 601.656 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2728.892 ms
Partition 0 [0, 32) has cost: 2728.892 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 585.184 ms
GPU 0, Compute+Comm Time: 585.184 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 706.639 ms
GPU 0, Compute+Comm Time: 706.639 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1356.414 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 24)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [24, 48)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [48, 72)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [72, 96)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [120, 144)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [168, 190)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [96, 120)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [144, 168)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 24)...
+++++++++ Node 1 initializing the weights for op[24, 48)...
+++++++++ Node 3 initializing the weights for op[72, 96)...
+++++++++ Node 2 initializing the weights for op[48, 72)...
+++++++++ Node 4 initializing the weights for op[96, 120)...
+++++++++ Node 6 initializing the weights for op[144, 168)...
+++++++++ Node 5 initializing the weights for op[120, 144)...
+++++++++ Node 7 initializing the weights for op[168, 190)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.3135	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 50:	Loss 3.1847	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.0584
	Epoch 75:	Loss 2.9771	TrainAcc 0.1583	ValidAcc 0.1747	TestAcc 0.1775	BestValid 0.1747
	Epoch 100:	Loss 2.7422	TrainAcc 0.0690	ValidAcc 0.0584	TestAcc 0.0574	BestValid 0.1747
	Epoch 125:	Loss 2.4197	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.1747
	Epoch 150:	Loss 2.1215	TrainAcc 0.1556	ValidAcc 0.1532	TestAcc 0.1513	BestValid 0.1747
	Epoch 175:	Loss 1.8561	TrainAcc 0.1081	ValidAcc 0.1124	TestAcc 0.1072	BestValid 0.1747
	Epoch 200:	Loss 1.6464	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.1747
	Epoch 225:	Loss 1.4922	TrainAcc 0.0399	ValidAcc 0.0339	TestAcc 0.0333	BestValid 0.1747
	Epoch 250:	Loss 1.3021	TrainAcc 0.0738	ValidAcc 0.0699	TestAcc 0.0668	BestValid 0.1747
	Epoch 275:	Loss 1.1372	TrainAcc 0.0915	ValidAcc 0.0859	TestAcc 0.0827	BestValid 0.1747
	Epoch 300:	Loss 1.0546	TrainAcc 0.0755	ValidAcc 0.0710	TestAcc 0.0677	BestValid 0.1747
	Epoch 325:	Loss 0.9815	TrainAcc 0.0772	ValidAcc 0.0750	TestAcc 0.0708	BestValid 0.1747
	Epoch 350:	Loss 0.9167	TrainAcc 0.0873	ValidAcc 0.0794	TestAcc 0.0767	BestValid 0.1747
	Epoch 375:	Loss 0.8477	TrainAcc 0.0798	ValidAcc 0.0720	TestAcc 0.0709	BestValid 0.1747
	Epoch 400:	Loss 0.8133	TrainAcc 0.0935	ValidAcc 0.0877	TestAcc 0.0848	BestValid 0.1747
	Epoch 425:	Loss 0.7752	TrainAcc 0.0424	ValidAcc 0.0334	TestAcc 0.0326	BestValid 0.1747
	Epoch 450:	Loss 0.7499	TrainAcc 0.0345	ValidAcc 0.0281	TestAcc 0.0283	BestValid 0.1747
	Epoch 475:	Loss 0.7397	TrainAcc 0.0362	ValidAcc 0.0316	TestAcc 0.0316	BestValid 0.1747
	Epoch 500:	Loss 0.7237	TrainAcc 0.0255	ValidAcc 0.0224	TestAcc 0.0230	BestValid 0.1747
	Epoch 525:	Loss 0.6945	TrainAcc 0.0280	ValidAcc 0.0249	TestAcc 0.0253	BestValid 0.1747
	Epoch 550:	Loss 0.7010	TrainAcc 0.0643	ValidAcc 0.0633	TestAcc 0.0599	BestValid 0.1747
	Epoch 575:	Loss 0.6836	TrainAcc 0.0622	ValidAcc 0.0538	TestAcc 0.0525	BestValid 0.1747
	Epoch 600:	Loss 0.6695	TrainAcc 0.0693	ValidAcc 0.0704	TestAcc 0.0671	BestValid 0.1747
	Epoch 625:	Loss 0.6811	TrainAcc 0.0723	ValidAcc 0.0739	TestAcc 0.0721	BestValid 0.1747
	Epoch 650:	Loss 0.6501	TrainAcc 0.1112	ValidAcc 0.1159	TestAcc 0.1123	BestValid 0.1747
	Epoch 675:	Loss 0.6453	TrainAcc 0.0468	ValidAcc 0.0427	TestAcc 0.0422	BestValid 0.1747
	Epoch 700:	Loss 0.6376	TrainAcc 0.0864	ValidAcc 0.0853	TestAcc 0.0860	BestValid 0.1747
	Epoch 725:	Loss 0.6440	TrainAcc 0.0239	ValidAcc 0.0230	TestAcc 0.0243	BestValid 0.1747
	Epoch 750:	Loss 0.6374	TrainAcc 0.0670	ValidAcc 0.0736	TestAcc 0.0671	BestValid 0.1747
	Epoch 775:	Loss 0.6152	TrainAcc 0.0841	ValidAcc 0.0812	TestAcc 0.0785	BestValid 0.1747
	Epoch 800:	Loss 0.6096	TrainAcc 0.0747	ValidAcc 0.0735	TestAcc 0.0693	BestValid 0.1747
	Epoch 825:	Loss 0.6042	TrainAcc 0.1181	ValidAcc 0.1178	TestAcc 0.1147	BestValid 0.1747
	Epoch 850:	Loss 0.5972	TrainAcc 0.1123	ValidAcc 0.1107	TestAcc 0.1073	BestValid 0.1747
	Epoch 875:	Loss 0.5994	TrainAcc 0.1045	ValidAcc 0.1041	TestAcc 0.1016	BestValid 0.1747
	Epoch 900:	Loss 0.5957	TrainAcc 0.0531	ValidAcc 0.0536	TestAcc 0.0512	BestValid 0.1747
	Epoch 925:	Loss 0.5893	TrainAcc 0.0298	ValidAcc 0.0319	TestAcc 0.0309	BestValid 0.1747
	Epoch 950:	Loss 0.5721	TrainAcc 0.1008	ValidAcc 0.1011	TestAcc 0.0989	BestValid 0.1747
	Epoch 975:	Loss 0.5603	TrainAcc 0.0923	ValidAcc 0.0921	TestAcc 0.0894	BestValid 0.1747
	Epoch 1000:	Loss 0.5596	TrainAcc 0.0699	ValidAcc 0.0673	TestAcc 0.0673	BestValid 0.1747
	Epoch 1025:	Loss 0.5605	TrainAcc 0.0349	ValidAcc 0.0392	TestAcc 0.0381	BestValid 0.1747
	Epoch 1050:	Loss 0.5722	TrainAcc 0.0109	ValidAcc 0.0088	TestAcc 0.0090	BestValid 0.1747
	Epoch 1075:	Loss 0.5583	TrainAcc 0.0152	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1747
	Epoch 1100:	Loss 0.5422	TrainAcc 0.0096	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1125:	Loss 0.5669	TrainAcc 0.0700	ValidAcc 0.0650	TestAcc 0.0656	BestValid 0.1747
	Epoch 1150:	Loss 0.5392	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1175:	Loss 0.5285	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0084	BestValid 0.1747
	Epoch 1200:	Loss 0.5247	TrainAcc 0.0098	ValidAcc 0.0081	TestAcc 0.0084	BestValid 0.1747
	Epoch 1225:	Loss 0.5279	TrainAcc 0.0243	ValidAcc 0.0157	TestAcc 0.0162	BestValid 0.1747
	Epoch 1250:	Loss 0.5232	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1747
	Epoch 1275:	Loss 0.5214	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0084	BestValid 0.1747
	Epoch 1300:	Loss 0.5211	TrainAcc 0.0248	ValidAcc 0.0158	TestAcc 0.0161	BestValid 0.1747
	Epoch 1325:	Loss 0.5362	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1747
	Epoch 1350:	Loss 0.5257	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1375:	Loss 0.5178	TrainAcc 0.0040	ValidAcc 0.0025	TestAcc 0.0026	BestValid 0.1747
	Epoch 1400:	Loss 0.5136	TrainAcc 0.0097	ValidAcc 0.0083	TestAcc 0.0084	BestValid 0.1747
	Epoch 1425:	Loss 0.5742	TrainAcc 0.0251	ValidAcc 0.0155	TestAcc 0.0157	BestValid 0.1747
	Epoch 1450:	Loss 0.5246	TrainAcc 0.0367	ValidAcc 0.0339	TestAcc 0.0332	BestValid 0.1747
	Epoch 1475:	Loss 0.5174	TrainAcc 0.0098	ValidAcc 0.0082	TestAcc 0.0085	BestValid 0.1747
	Epoch 1500:	Loss 0.5125	TrainAcc 0.0096	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1525:	Loss 0.4944	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1550:	Loss 0.4908	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1575:	Loss 0.4827	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1600:	Loss 0.4781	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1625:	Loss 0.4792	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1650:	Loss 0.4729	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1675:	Loss 0.4670	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1700:	Loss 0.4657	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1725:	Loss 0.4687	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1750:	Loss 0.4673	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1775:	Loss 0.4620	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1800:	Loss 0.4611	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1825:	Loss 0.4608	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1850:	Loss 0.4566	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1875:	Loss 0.4587	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1900:	Loss 0.4528	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1925:	Loss 0.4524	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1950:	Loss 0.4489	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 1975:	Loss 0.4501	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2000:	Loss 0.4473	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2025:	Loss 0.4562	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2050:	Loss 0.4469	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2075:	Loss 0.4451	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2100:	Loss 0.4424	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2125:	Loss 0.4569	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2150:	Loss 0.4497	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2175:	Loss 0.4560	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2200:	Loss 0.4481	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2225:	Loss 0.4397	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2250:	Loss 0.4469	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2275:	Loss 0.4392	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2300:	Loss 0.4395	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2325:	Loss 0.4416	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2350:	Loss 0.4387	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2375:	Loss 0.4348	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2400:	Loss 0.4319	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2425:	Loss 0.4349	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2450:	Loss 0.4353	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2475:	Loss 0.4328	TrainAcc 0.0096	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2500:	Loss 0.4305	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2525:	Loss 0.4330	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2550:	Loss 0.4270	TrainAcc 0.0257	ValidAcc 0.0213	TestAcc 0.0218	BestValid 0.1747
	Epoch 2575:	Loss 0.4269	TrainAcc 0.0096	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2600:	Loss 0.4249	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2625:	Loss 0.4297	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2650:	Loss 0.4244	TrainAcc 0.0098	ValidAcc 0.0081	TestAcc 0.0084	BestValid 0.1747
	Epoch 2675:	Loss 0.4223	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2700:	Loss 0.4226	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 2725:	Loss 0.4229	TrainAcc 0.0210	ValidAcc 0.0168	TestAcc 0.0170	BestValid 0.1747
	Epoch 2750:	Loss 0.4194	TrainAcc 0.0223	ValidAcc 0.0180	TestAcc 0.0185	BestValid 0.1747
	Epoch 2775:	Loss 0.4214	TrainAcc 0.0097	ValidAcc 0.0082	TestAcc 0.0083	BestValid 0.1747
	Epoch 2800:	Loss 0.4244	TrainAcc 0.0229	ValidAcc 0.0189	TestAcc 0.0192	BestValid 0.1747
	Epoch 2825:	Loss 0.4265	TrainAcc 0.0287	ValidAcc 0.0232	TestAcc 0.0238	BestValid 0.1747
	Epoch 2850:	Loss 0.4246	TrainAcc 0.0210	ValidAcc 0.0169	TestAcc 0.0170	BestValid 0.1747
	Epoch 2875:	Loss 0.4235	TrainAcc 0.0215	ValidAcc 0.0176	TestAcc 0.0177	BestValid 0.1747
	Epoch 2900:	Loss 0.4297	TrainAcc 0.0212	ValidAcc 0.0170	TestAcc 0.0171	BestValid 0.1747
	Epoch 2925:	Loss 0.4282	TrainAcc 0.0210	ValidAcc 0.0169	TestAcc 0.0170	BestValid 0.1747
	Epoch 2950:	Loss 0.4278	TrainAcc 0.0097	ValidAcc 0.0082	TestAcc 0.0083	BestValid 0.1747
	Epoch 2975:	Loss 0.4243	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 3000:	Loss 0.4219	TrainAcc 0.0215	ValidAcc 0.0175	TestAcc 0.0178	BestValid 0.1747
	Epoch 3025:	Loss 0.4238	TrainAcc 0.0097	ValidAcc 0.0081	TestAcc 0.0083	BestValid 0.1747
	Epoch 3050:	Loss 0.4167	TrainAcc 0.0103	ValidAcc 0.0084	TestAcc 0.0086	BestValid 0.1747
	Epoch 3075:	Loss 0.4146	TrainAcc 0.0261	ValidAcc 0.0230	TestAcc 0.0223	BestValid 0.1747
	Epoch 3100:	Loss 0.4149	TrainAcc 0.0106	ValidAcc 0.0084	TestAcc 0.0084	BestValid 0.1747
	Epoch 3125:	Loss 0.4180	TrainAcc 0.0097	ValidAcc 0.0082	TestAcc 0.0083	BestValid 0.1747
	Epoch 3150:	Loss 0.4132	TrainAcc 0.0210	ValidAcc 0.0169	TestAcc 0.0170	BestValid 0.1747
	Epoch 3175:	Loss 0.4100	TrainAcc 0.0210	ValidAcc 0.0169	TestAcc 0.0170	BestValid 0.1747
	Epoch 3200:	Loss 0.4096	TrainAcc 0.0220	ValidAcc 0.0179	TestAcc 0.0181	BestValid 0.1747
	Epoch 3225:	Loss 0.4082	TrainAcc 0.0236	ValidAcc 0.0197	TestAcc 0.0192	BestValid 0.1747
	Epoch 3250:	Loss 0.4043	TrainAcc 0.0252	ValidAcc 0.0210	TestAcc 0.0203	BestValid 0.1747
	Epoch 3275:	Loss 0.4074	TrainAcc 0.0408	ValidAcc 0.0333	TestAcc 0.0330	BestValid 0.1747
	Epoch 3300:	Loss 0.4062	TrainAcc 0.0210	ValidAcc 0.0170	TestAcc 0.0169	BestValid 0.1747
	Epoch 3325:	Loss 0.4032	TrainAcc 0.0215	ValidAcc 0.0176	TestAcc 0.0177	BestValid 0.1747
	Epoch 3350:	Loss 0.4019	TrainAcc 0.0210	ValidAcc 0.0169	TestAcc 0.0170	BestValid 0.1747
	Epoch 3375:	Loss 0.4035	TrainAcc 0.0224	ValidAcc 0.0172	TestAcc 0.0173	BestValid 0.1747
	Epoch 3400:	Loss 0.4006	TrainAcc 0.0214	ValidAcc 0.0175	TestAcc 0.0169	BestValid 0.1747
	Epoch 3425:	Loss 0.4014	TrainAcc 0.0348	ValidAcc 0.0282	TestAcc 0.0274	BestValid 0.1747
	Epoch 3450:	Loss 0.3970	TrainAcc 0.0344	ValidAcc 0.0270	TestAcc 0.0264	BestValid 0.1747
	Epoch 3475:	Loss 0.3971	TrainAcc 0.0220	ValidAcc 0.0180	TestAcc 0.0181	BestValid 0.1747
	Epoch 3500:	Loss 0.3960	TrainAcc 0.0351	ValidAcc 0.0282	TestAcc 0.0275	BestValid 0.1747
	Epoch 3525:	Loss 0.4013	TrainAcc 0.0361	ValidAcc 0.0299	TestAcc 0.0291	BestValid 0.1747
	Epoch 3550:	Loss 0.3984	TrainAcc 0.0379	ValidAcc 0.0322	TestAcc 0.0317	BestValid 0.1747
	Epoch 3575:	Loss 0.3943	TrainAcc 0.0331	ValidAcc 0.0269	TestAcc 0.0262	BestValid 0.1747
	Epoch 3600:	Loss 0.3945	TrainAcc 0.0351	ValidAcc 0.0280	TestAcc 0.0276	BestValid 0.1747
	Epoch 3625:	Loss 0.3950	TrainAcc 0.0246	ValidAcc 0.0183	TestAcc 0.0183	BestValid 0.1747
	Epoch 3650:	Loss 0.3919	TrainAcc 0.0370	ValidAcc 0.0313	TestAcc 0.0305	BestValid 0.1747
	Epoch 3675:	Loss 0.3924	TrainAcc 0.0378	ValidAcc 0.0322	TestAcc 0.0316	BestValid 0.1747
	Epoch 3700:	Loss 0.3887	TrainAcc 0.0350	ValidAcc 0.0284	TestAcc 0.0275	BestValid 0.1747
	Epoch 3725:	Loss 0.3910	TrainAcc 0.0347	ValidAcc 0.0276	TestAcc 0.0269	BestValid 0.1747
	Epoch 3750:	Loss 0.3862	TrainAcc 0.0258	ValidAcc 0.0185	TestAcc 0.0187	BestValid 0.1747
	Epoch 3775:	Loss 0.3882	TrainAcc 0.0454	ValidAcc 0.0397	TestAcc 0.0388	BestValid 0.1747
	Epoch 3800:	Loss 0.3876	TrainAcc 0.0406	ValidAcc 0.0347	TestAcc 0.0337	BestValid 0.1747
	Epoch 3825:	Loss 0.3875	TrainAcc 0.0470	ValidAcc 0.0408	TestAcc 0.0401	BestValid 0.1747
	Epoch 3850:	Loss 0.3859	TrainAcc 0.0463	ValidAcc 0.0401	TestAcc 0.0393	BestValid 0.1747
	Epoch 3875:	Loss 0.3848	TrainAcc 0.0229	ValidAcc 0.0182	TestAcc 0.0185	BestValid 0.1747
	Epoch 3900:	Loss 0.3842	TrainAcc 0.0174	ValidAcc 0.0156	TestAcc 0.0149	BestValid 0.1747
	Epoch 3925:	Loss 0.3855	TrainAcc 0.0395	ValidAcc 0.0316	TestAcc 0.0307	BestValid 0.1747
	Epoch 3950:	Loss 0.3825	TrainAcc 0.0401	ValidAcc 0.0322	TestAcc 0.0316	BestValid 0.1747
	Epoch 3975:	Loss 0.3854	TrainAcc 0.0385	ValidAcc 0.0382	TestAcc 0.0370	BestValid 0.1747
	Epoch 4000:	Loss 0.3816	TrainAcc 0.0211	ValidAcc 0.0170	TestAcc 0.0170	BestValid 0.1747
	Epoch 4025:	Loss 0.3826	TrainAcc 0.0375	ValidAcc 0.0317	TestAcc 0.0310	BestValid 0.1747
	Epoch 4050:	Loss 0.3796	TrainAcc 0.0508	ValidAcc 0.0443	TestAcc 0.0429	BestValid 0.1747
	Epoch 4075:	Loss 0.3806	TrainAcc 0.0420	ValidAcc 0.0366	TestAcc 0.0353	BestValid 0.1747
	Epoch 4100:	Loss 0.3789	TrainAcc 0.0382	ValidAcc 0.0380	TestAcc 0.0367	BestValid 0.1747
	Epoch 4125:	Loss 0.3794	TrainAcc 0.0273	ValidAcc 0.0261	TestAcc 0.0239	BestValid 0.1747
	Epoch 4150:	Loss 0.3812	TrainAcc 0.0386	ValidAcc 0.0383	TestAcc 0.0374	BestValid 0.1747
	Epoch 4175:	Loss 0.3797	TrainAcc 0.0358	ValidAcc 0.0279	TestAcc 0.0269	BestValid 0.1747
	Epoch 4200:	Loss 0.3791	TrainAcc 0.0461	ValidAcc 0.0402	TestAcc 0.0394	BestValid 0.1747
	Epoch 4225:	Loss 0.3772	TrainAcc 0.0489	ValidAcc 0.0419	TestAcc 0.0408	BestValid 0.1747
	Epoch 4250:	Loss 0.3773	TrainAcc 0.0490	ValidAcc 0.0418	TestAcc 0.0411	BestValid 0.1747
	Epoch 4275:	Loss 0.3791	TrainAcc 0.0511	ValidAcc 0.0450	TestAcc 0.0439	BestValid 0.1747
	Epoch 4300:	Loss 0.3850	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1747
	Epoch 4325:	Loss 0.4149	TrainAcc 0.0285	ValidAcc 0.0273	TestAcc 0.0282	BestValid 0.1747
	Epoch 4350:	Loss 0.4054	TrainAcc 0.0467	ValidAcc 0.0438	TestAcc 0.0440	BestValid 0.1747
	Epoch 4375:	Loss 0.4115	TrainAcc 0.0190	ValidAcc 0.0174	TestAcc 0.0167	BestValid 0.1747
	Epoch 4400:	Loss 0.4049	TrainAcc 0.0500	ValidAcc 0.0416	TestAcc 0.0452	BestValid 0.1747
	Epoch 4425:	Loss 0.3950	TrainAcc 0.0542	ValidAcc 0.0425	TestAcc 0.0461	BestValid 0.1747
	Epoch 4450:	Loss 0.3909	TrainAcc 0.0512	ValidAcc 0.0502	TestAcc 0.0486	BestValid 0.1747
	Epoch 4475:	Loss 0.3887	TrainAcc 0.0392	ValidAcc 0.0373	TestAcc 0.0358	BestValid 0.1747
	Epoch 4500:	Loss 0.3817	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1747
	Epoch 4525:	Loss 0.3845	TrainAcc 0.0294	ValidAcc 0.0271	TestAcc 0.0256	BestValid 0.1747
	Epoch 4550:	Loss 0.3810	TrainAcc 0.0764	ValidAcc 0.0660	TestAcc 0.0684	BestValid 0.1747
	Epoch 4575:	Loss 0.3786	TrainAcc 0.0797	ValidAcc 0.0689	TestAcc 0.0710	BestValid 0.1747
	Epoch 4600:	Loss 0.3769	TrainAcc 0.0807	ValidAcc 0.0702	TestAcc 0.0722	BestValid 0.1747
	Epoch 4625:	Loss 0.3743	TrainAcc 0.0464	ValidAcc 0.0439	TestAcc 0.0426	BestValid 0.1747
	Epoch 4650:	Loss 0.3708	TrainAcc 0.0807	ValidAcc 0.0701	TestAcc 0.0721	BestValid 0.1747
	Epoch 4675:	Loss 0.3705	TrainAcc 0.0807	ValidAcc 0.0702	TestAcc 0.0723	BestValid 0.1747
	Epoch 4700:	Loss 0.3696	TrainAcc 0.0913	ValidAcc 0.0826	TestAcc 0.0840	BestValid 0.1747
	Epoch 4725:	Loss 0.3704	TrainAcc 0.0462	ValidAcc 0.0436	TestAcc 0.0425	BestValid 0.1747
	Epoch 4750:	Loss 0.3690	TrainAcc 0.0807	ValidAcc 0.0702	TestAcc 0.0722	BestValid 0.1747
	Epoch 4775:	Loss 0.3675	TrainAcc 0.0462	ValidAcc 0.0436	TestAcc 0.0424	BestValid 0.1747
	Epoch 4800:	Loss 0.3675	TrainAcc 0.0512	ValidAcc 0.0505	TestAcc 0.0486	BestValid 0.1747
	Epoch 4825:	Loss 0.3674	TrainAcc 0.0462	ValidAcc 0.0436	TestAcc 0.0425	BestValid 0.1747
	Epoch 4850:	Loss 0.3645	TrainAcc 0.0743	ValidAcc 0.0644	TestAcc 0.0665	BestValid 0.1747
	Epoch 4875:	Loss 0.3652	TrainAcc 0.0680	ValidAcc 0.0583	TestAcc 0.0608	BestValid 0.1747
	Epoch 4900:	Loss 0.3647	TrainAcc 0.0462	ValidAcc 0.0436	TestAcc 0.0423	BestValid 0.1747
	Epoch 4925:	Loss 0.3656	TrainAcc 0.0508	ValidAcc 0.0441	TestAcc 0.0425	BestValid 0.1747
	Epoch 4950:	Loss 0.3652	TrainAcc 0.0461	ValidAcc 0.0436	TestAcc 0.0423	BestValid 0.1747
	Epoch 4975:	Loss 0.3651	TrainAcc 0.0400	ValidAcc 0.0395	TestAcc 0.0384	BestValid 0.1747
	Epoch 5000:	Loss 0.3664	TrainAcc 0.0337	ValidAcc 0.0307	TestAcc 0.0313	BestValid 0.1747
Node 0, Pre/Post-Pipelining: 1.089 / 0.945 ms, Bubble: 74.281 ms, Compute: 258.422 ms, Comm: 16.913 ms, Imbalance: 17.700 ms
Node 6, Pre/Post-Pipelining: 1.092 / 0.844 ms, Bubble: 73.693 ms, Compute: 245.372 ms, Comm: 29.103 ms, Imbalance: 19.781 ms
Node 3, Pre/Post-Pipelining: 1.089 / 0.897 ms, Bubble: 73.494 ms, Compute: 248.301 ms, Comm: 26.339 ms, Imbalance: 19.547 ms
Node 7, Pre/Post-Pipelining: 1.092 / 16.502 ms, Bubble: 58.312 ms, Compute: 258.261 ms, Comm: 16.855 ms, Imbalance: 18.684 ms
Node 2, Pre/Post-Pipelining: 1.091 / 0.871 ms, Bubble: 74.139 ms, Compute: 242.563 ms, Comm: 29.655 ms, Imbalance: 21.525 ms
Node 1, Pre/Post-Pipelining: 1.092 / 0.861 ms, Bubble: 73.838 ms, Compute: 251.496 ms, Comm: 27.698 ms, Imbalance: 14.521 ms
Node 4, Pre/Post-Pipelining: 1.089 / 0.877 ms, Bubble: 73.843 ms, Compute: 241.320 ms, Comm: 26.196 ms, Imbalance: 26.721 ms
Node 5, Pre/Post-Pipelining: 1.090 / 0.870 ms, Bubble: 73.686 ms, Compute: 245.292 ms, Comm: 30.634 ms, Imbalance: 18.274 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.089 ms
Cluster-Wide Average, Post-Pipelining Overhead: 0.945 ms
Cluster-Wide Average, Bubble: 74.281 ms
Cluster-Wide Average, Compute: 258.422 ms
Cluster-Wide Average, Communication: 16.913 ms
Cluster-Wide Average, Imbalance: 17.700 ms
Node 0, GPU memory consumption: 6.739 GB
Node 3, GPU memory consumption: 5.796 GB
Node 2, GPU memory consumption: 5.819 GB
Node 7, GPU memory consumption: 5.569 GB
Node 1, GPU memory consumption: 5.819 GB
Node 4, GPU memory consumption: 5.796 GB
Node 6, GPU memory consumption: 5.819 GB
Node 5, GPU memory consumption: 5.819 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.758955 s---------------
------------------------node id 1,  per-epoch time: 0.758955 s---------------
------------------------node id 4,  per-epoch time: 0.758955 s---------------
------------------------node id 2,  per-epoch time: 0.758955 s---------------
------------------------node id 5,  per-epoch time: 0.758955 s---------------
------------------------node id 3,  per-epoch time: 0.758955 s---------------
------------------------node id 6,  per-epoch time: 0.758955 s---------------
------------------------node id 7,  per-epoch time: 0.758955 s---------------
************ Profiling Results ************
	Bubble: 510.506053 (ms) (67.29 percentage)
	Compute: 244.659599 (ms) (32.25 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.508653 (ms) (0.46 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 1.216 GB
	Aggregated layer-level communication throughput: 410.509 Gbps
Highest valid_acc: 0.1747
Target test_acc: 0.1775
Epoch to reach the target acc: 74
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
