Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.911 seconds.
Building the CSC structure...
        It takes 2.043 seconds.
Building the CSC structure...
        It takes 2.291 seconds.
Building the CSC structure...
        It takes 2.358 seconds.
Building the CSC structure...
        It takes 2.395 seconds.
Building the CSC structure...
        It takes 2.414 seconds.
Building the CSC structure...
        It takes 2.631 seconds.
Building the CSC structure...
        It takes 2.653 seconds.
Building the CSC structure...
        It takes 1.837 seconds.
        It takes 1.876 seconds.
        It takes 2.188 seconds.
        It takes 2.180 seconds.
        It takes 2.257 seconds.
Building the Feature Vector...
        It takes 2.380 seconds.
        It takes 2.374 seconds.
        It takes 0.287 seconds.
Building the Label Vector...
        It takes 2.384 seconds.
        It takes 0.042 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.268 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.032 seconds.
Building the Feature Vector...
        It takes 0.303 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.282 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.036 seconds.
        It takes 0.031 seconds.
Building the Feature Vector...
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
        It takes 0.252 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.344 Gbps (per GPU), 450.756 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.100 Gbps (per GPU), 448.797 Gbps (aggregated)
The layer-level communication performance: 56.096 Gbps (per GPU), 448.769 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.878 Gbps (per GPU), 447.020 Gbps (aggregated)
The layer-level communication performance: 55.855 Gbps (per GPU), 446.843 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.660 Gbps (per GPU), 445.277 Gbps (aggregated)
The layer-level communication performance: 55.613 Gbps (per GPU), 444.900 Gbps (aggregated)
The layer-level communication performance: 55.582 Gbps (per GPU), 444.657 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.858 Gbps (per GPU), 1286.868 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.858 Gbps (per GPU), 1286.868 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.846 Gbps (per GPU), 1286.769 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.751 Gbps (per GPU), 1286.005 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.846 Gbps (per GPU), 1286.769 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.754 Gbps (per GPU), 1286.029 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.843 Gbps (per GPU), 1286.744 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.847 Gbps (per GPU), 1286.772 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.269 Gbps (per GPU), 834.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.270 Gbps (per GPU), 834.162 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.270 Gbps (per GPU), 834.162 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.270 Gbps (per GPU), 834.162 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.269 Gbps (per GPU), 834.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.269 Gbps (per GPU), 834.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.269 Gbps (per GPU), 834.149 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.270 Gbps (per GPU), 834.162 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.068 Gbps (per GPU), 264.542 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.067 Gbps (per GPU), 264.538 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.067 Gbps (per GPU), 264.532 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.067 Gbps (per GPU), 264.538 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.066 Gbps (per GPU), 264.528 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.066 Gbps (per GPU), 264.531 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.066 Gbps (per GPU), 264.527 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.067 Gbps (per GPU), 264.536 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.41ms  2.51ms  2.27ms  1.11  8.38K  3.53M
 chk_1  2.80ms  2.73ms  2.60ms  1.08  6.74K  3.60M
 chk_2  2.73ms  2.63ms  2.48ms  1.10  7.27K  3.53M
 chk_3  2.74ms  2.66ms  2.49ms  1.10  7.92K  3.61M
 chk_4  2.67ms  2.62ms  2.51ms  1.07  5.33K  3.68M
 chk_5  2.65ms  2.55ms  2.34ms  1.13 10.07K  3.45M
 chk_6  2.80ms  2.72ms  2.50ms  1.12  9.41K  3.48M
 chk_7  2.70ms  2.59ms  2.43ms  1.11  8.12K  3.60M
 chk_8  2.77ms  2.71ms  2.58ms  1.07  6.09K  3.64M
 chk_9  2.59ms  2.47ms  2.22ms  1.17 11.10K  3.38M
chk_10  2.79ms  2.74ms  2.63ms  1.06  5.67K  3.63M
chk_11  2.68ms  2.58ms  2.43ms  1.10  8.16K  3.54M
chk_12  2.88ms  2.79ms  2.64ms  1.09  7.24K  3.55M
chk_13  2.69ms  2.64ms  2.53ms  1.06  5.41K  3.68M
chk_14  2.95ms  2.85ms  2.69ms  1.10  7.14K  3.53M
chk_15  2.81ms  2.71ms  2.50ms  1.13  9.25K  3.49M
chk_16  2.62ms  2.60ms  2.48ms  1.06  4.78K  3.77M
chk_17  2.76ms  2.68ms  2.54ms  1.08  6.85K  3.60M
chk_18  2.57ms  2.48ms  2.34ms  1.10  7.47K  3.57M
chk_19  2.65ms  2.58ms  2.48ms  1.07  4.88K  3.75M
chk_20  2.63ms  2.57ms  2.42ms  1.09  7.00K  3.63M
chk_21  2.62ms  2.57ms  2.45ms  1.07  5.41K  3.68M
chk_22  2.81ms  2.70ms  2.44ms  1.15 11.07K  3.39M
chk_23  2.76ms  2.65ms  2.50ms  1.10  7.23K  3.64M
chk_24  2.77ms  2.68ms  2.44ms  1.14 10.13K  3.43M
chk_25  2.60ms  2.52ms  2.38ms  1.09  6.40K  3.57M
chk_26  2.80ms  2.73ms  2.60ms  1.08  5.78K  3.55M
chk_27  2.77ms  2.59ms  2.37ms  1.17  9.34K  3.48M
chk_28  3.05ms  2.91ms  2.72ms  1.12  6.37K  3.57M
chk_29  2.85ms  2.79ms  2.61ms  1.09  5.16K  3.78M
chk_30  2.71ms  2.68ms  2.51ms  1.08  5.44K  3.67M
chk_31  2.84ms  2.77ms  2.64ms  1.08  6.33K  3.63M
   Avg  2.73  2.66  2.49
   Max  3.05  2.91  2.72
   Min  2.41  2.47  2.22
 Ratio  1.27  1.18  1.23
   Var  0.01  0.01  0.01
Profiling takes 2.925 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 342.478 ms
Partition 0 [0, 4) has cost: 342.478 ms
Partition 1 [4, 8) has cost: 340.008 ms
Partition 2 [8, 12) has cost: 340.008 ms
Partition 3 [12, 16) has cost: 340.008 ms
Partition 4 [16, 20) has cost: 340.008 ms
Partition 5 [20, 24) has cost: 340.008 ms
Partition 6 [24, 28) has cost: 340.008 ms
Partition 7 [28, 32) has cost: 334.736 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 161.855 ms
GPU 0, Compute+Comm Time: 127.798 ms, Bubble Time: 29.023 ms, Imbalance Overhead: 5.034 ms
GPU 1, Compute+Comm Time: 126.375 ms, Bubble Time: 28.608 ms, Imbalance Overhead: 6.871 ms
GPU 2, Compute+Comm Time: 126.375 ms, Bubble Time: 28.395 ms, Imbalance Overhead: 7.085 ms
GPU 3, Compute+Comm Time: 126.375 ms, Bubble Time: 28.137 ms, Imbalance Overhead: 7.343 ms
GPU 4, Compute+Comm Time: 126.375 ms, Bubble Time: 27.971 ms, Imbalance Overhead: 7.509 ms
GPU 5, Compute+Comm Time: 126.375 ms, Bubble Time: 27.888 ms, Imbalance Overhead: 7.592 ms
GPU 6, Compute+Comm Time: 126.375 ms, Bubble Time: 27.816 ms, Imbalance Overhead: 7.664 ms
GPU 7, Compute+Comm Time: 124.479 ms, Bubble Time: 27.992 ms, Imbalance Overhead: 9.384 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 307.609 ms
GPU 0, Compute+Comm Time: 236.719 ms, Bubble Time: 53.399 ms, Imbalance Overhead: 17.491 ms
GPU 1, Compute+Comm Time: 240.095 ms, Bubble Time: 53.213 ms, Imbalance Overhead: 14.302 ms
GPU 2, Compute+Comm Time: 240.095 ms, Bubble Time: 53.274 ms, Imbalance Overhead: 14.240 ms
GPU 3, Compute+Comm Time: 240.095 ms, Bubble Time: 53.336 ms, Imbalance Overhead: 14.178 ms
GPU 4, Compute+Comm Time: 240.095 ms, Bubble Time: 53.650 ms, Imbalance Overhead: 13.864 ms
GPU 5, Compute+Comm Time: 240.095 ms, Bubble Time: 54.082 ms, Imbalance Overhead: 13.432 ms
GPU 6, Compute+Comm Time: 240.095 ms, Bubble Time: 54.484 ms, Imbalance Overhead: 13.030 ms
GPU 7, Compute+Comm Time: 241.142 ms, Bubble Time: 55.540 ms, Imbalance Overhead: 10.927 ms
The estimated cost of the whole pipeline: 492.937 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 682.486 ms
Partition 0 [0, 8) has cost: 682.486 ms
Partition 1 [8, 16) has cost: 680.016 ms
Partition 2 [16, 24) has cost: 680.016 ms
Partition 3 [24, 32) has cost: 674.744 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 169.408 ms
GPU 0, Compute+Comm Time: 139.134 ms, Bubble Time: 27.033 ms, Imbalance Overhead: 3.242 ms
GPU 1, Compute+Comm Time: 138.388 ms, Bubble Time: 26.388 ms, Imbalance Overhead: 4.632 ms
GPU 2, Compute+Comm Time: 138.388 ms, Bubble Time: 25.917 ms, Imbalance Overhead: 5.103 ms
GPU 3, Compute+Comm Time: 137.512 ms, Bubble Time: 25.462 ms, Imbalance Overhead: 6.435 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 309.842 ms
GPU 0, Compute+Comm Time: 252.034 ms, Bubble Time: 47.153 ms, Imbalance Overhead: 10.656 ms
GPU 1, Compute+Comm Time: 253.526 ms, Bubble Time: 47.550 ms, Imbalance Overhead: 8.767 ms
GPU 2, Compute+Comm Time: 253.526 ms, Bubble Time: 47.994 ms, Imbalance Overhead: 8.323 ms
GPU 3, Compute+Comm Time: 254.155 ms, Bubble Time: 49.214 ms, Imbalance Overhead: 6.474 ms
    The estimated cost with 2 DP ways is 503.213 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1362.501 ms
Partition 0 [0, 16) has cost: 1362.501 ms
Partition 1 [16, 32) has cost: 1354.760 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 215.151 ms
GPU 0, Compute+Comm Time: 187.304 ms, Bubble Time: 23.328 ms, Imbalance Overhead: 4.520 ms
GPU 1, Compute+Comm Time: 186.495 ms, Bubble Time: 23.669 ms, Imbalance Overhead: 4.987 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 349.053 ms
GPU 0, Compute+Comm Time: 304.856 ms, Bubble Time: 38.090 ms, Imbalance Overhead: 6.107 ms
GPU 1, Compute+Comm Time: 305.933 ms, Bubble Time: 38.055 ms, Imbalance Overhead: 5.065 ms
    The estimated cost with 4 DP ways is 592.415 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2717.261 ms
Partition 0 [0, 32) has cost: 2717.261 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 572.033 ms
GPU 0, Compute+Comm Time: 572.033 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 692.958 ms
GPU 0, Compute+Comm Time: 692.958 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1328.240 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 24)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [24, 48)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [48, 72)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [72, 96)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [96, 120)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [120, 144)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [144, 168)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [168, 190)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[24, 48)...
+++++++++ Node 4 initializing the weights for op[96, 120)...
+++++++++ Node 2 initializing the weights for op[48, 72)...
+++++++++ Node 5 initializing the weights for op[120, 144)...
+++++++++ Node 3 initializing the weights for op[72, 96)...
+++++++++ Node 7 initializing the weights for op[168, 190)...
+++++++++ Node 0 initializing the weights for op[0, 24)...
+++++++++ Node 6 initializing the weights for op[144, 168)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.3185	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.0183
	Epoch 50:	Loss 3.1048	TrainAcc 0.0691	ValidAcc 0.0584	TestAcc 0.0575	BestValid 0.0584
	Epoch 75:	Loss 2.7348	TrainAcc 0.1414	ValidAcc 0.1349	TestAcc 0.1300	BestValid 0.1349
	Epoch 100:	Loss 2.4161	TrainAcc 0.1167	ValidAcc 0.1025	TestAcc 0.0985	BestValid 0.1349
	Epoch 125:	Loss 2.0369	TrainAcc 0.0214	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.1349
	Epoch 150:	Loss 1.7214	TrainAcc 0.0873	ValidAcc 0.0822	TestAcc 0.0802	BestValid 0.1349
	Epoch 175:	Loss 1.4854	TrainAcc 0.1543	ValidAcc 0.1535	TestAcc 0.1491	BestValid 0.1535
	Epoch 200:	Loss 1.2822	TrainAcc 0.1432	ValidAcc 0.1426	TestAcc 0.1390	BestValid 0.1535
	Epoch 225:	Loss 1.1023	TrainAcc 0.1167	ValidAcc 0.1144	TestAcc 0.1121	BestValid 0.1535
	Epoch 250:	Loss 1.0064	TrainAcc 0.1118	ValidAcc 0.1097	TestAcc 0.1054	BestValid 0.1535
	Epoch 275:	Loss 0.9399	TrainAcc 0.1480	ValidAcc 0.1446	TestAcc 0.1391	BestValid 0.1535
	Epoch 300:	Loss 0.8897	TrainAcc 0.0949	ValidAcc 0.0951	TestAcc 0.0902	BestValid 0.1535
	Epoch 325:	Loss 0.8413	TrainAcc 0.0658	ValidAcc 0.0653	TestAcc 0.0598	BestValid 0.1535
	Epoch 350:	Loss 0.8158	TrainAcc 0.1064	ValidAcc 0.1029	TestAcc 0.1000	BestValid 0.1535
	Epoch 375:	Loss 0.7945	TrainAcc 0.0463	ValidAcc 0.0406	TestAcc 0.0396	BestValid 0.1535
	Epoch 400:	Loss 0.7753	TrainAcc 0.0613	ValidAcc 0.0534	TestAcc 0.0520	BestValid 0.1535
	Epoch 425:	Loss 0.7754	TrainAcc 0.1048	ValidAcc 0.1019	TestAcc 0.0991	BestValid 0.1535
	Epoch 450:	Loss 0.7750	TrainAcc 0.0806	ValidAcc 0.0953	TestAcc 0.0922	BestValid 0.1535
	Epoch 475:	Loss 0.7360	TrainAcc 0.0500	ValidAcc 0.0473	TestAcc 0.0437	BestValid 0.1535
	Epoch 500:	Loss 0.7228	TrainAcc 0.0480	ValidAcc 0.0446	TestAcc 0.0413	BestValid 0.1535
	Epoch 525:	Loss 0.7147	TrainAcc 0.0392	ValidAcc 0.0366	TestAcc 0.0330	BestValid 0.1535
	Epoch 550:	Loss 0.7127	TrainAcc 0.0382	ValidAcc 0.0358	TestAcc 0.0325	BestValid 0.1535
	Epoch 575:	Loss 0.6807	TrainAcc 0.0377	ValidAcc 0.0356	TestAcc 0.0323	BestValid 0.1535
	Epoch 600:	Loss 0.6725	TrainAcc 0.0381	ValidAcc 0.0358	TestAcc 0.0324	BestValid 0.1535
	Epoch 625:	Loss 0.6684	TrainAcc 0.0492	ValidAcc 0.0468	TestAcc 0.0417	BestValid 0.1535
	Epoch 650:	Loss 0.6598	TrainAcc 0.0241	ValidAcc 0.0270	TestAcc 0.0242	BestValid 0.1535
	Epoch 675:	Loss 0.6417	TrainAcc 0.0597	ValidAcc 0.0564	TestAcc 0.0527	BestValid 0.1535
	Epoch 700:	Loss 0.6354	TrainAcc 0.0343	ValidAcc 0.0340	TestAcc 0.0304	BestValid 0.1535
	Epoch 725:	Loss 0.6293	TrainAcc 0.0628	ValidAcc 0.0613	TestAcc 0.0578	BestValid 0.1535
	Epoch 750:	Loss 0.6192	TrainAcc 0.0218	ValidAcc 0.0234	TestAcc 0.0209	BestValid 0.1535
	Epoch 775:	Loss 0.6076	TrainAcc 0.0289	ValidAcc 0.0304	TestAcc 0.0268	BestValid 0.1535
	Epoch 800:	Loss 0.5995	TrainAcc 0.0227	ValidAcc 0.0252	TestAcc 0.0225	BestValid 0.1535
	Epoch 825:	Loss 0.6021	TrainAcc 0.0312	ValidAcc 0.0335	TestAcc 0.0311	BestValid 0.1535
	Epoch 850:	Loss 0.5962	TrainAcc 0.0228	ValidAcc 0.0251	TestAcc 0.0225	BestValid 0.1535
	Epoch 875:	Loss 0.5867	TrainAcc 0.0172	ValidAcc 0.0188	TestAcc 0.0182	BestValid 0.1535
	Epoch 900:	Loss 0.5836	TrainAcc 0.0194	ValidAcc 0.0193	TestAcc 0.0169	BestValid 0.1535
	Epoch 925:	Loss 0.6164	TrainAcc 0.0436	ValidAcc 0.0428	TestAcc 0.0393	BestValid 0.1535
	Epoch 950:	Loss 0.5769	TrainAcc 0.0486	ValidAcc 0.0506	TestAcc 0.0501	BestValid 0.1535
	Epoch 975:	Loss 0.5767	TrainAcc 0.0397	ValidAcc 0.0403	TestAcc 0.0409	BestValid 0.1535
	Epoch 1000:	Loss 0.5725	TrainAcc 0.0403	ValidAcc 0.0404	TestAcc 0.0411	BestValid 0.1535
	Epoch 1025:	Loss 0.5650	TrainAcc 0.0265	ValidAcc 0.0253	TestAcc 0.0235	BestValid 0.1535
	Epoch 1050:	Loss 0.5587	TrainAcc 0.0415	ValidAcc 0.0399	TestAcc 0.0363	BestValid 0.1535
	Epoch 1075:	Loss 0.5542	TrainAcc 0.0381	ValidAcc 0.0370	TestAcc 0.0359	BestValid 0.1535
	Epoch 1100:	Loss 0.5463	TrainAcc 0.0274	ValidAcc 0.0261	TestAcc 0.0249	BestValid 0.1535
	Epoch 1125:	Loss 0.5477	TrainAcc 0.0195	ValidAcc 0.0188	TestAcc 0.0168	BestValid 0.1535
	Epoch 1150:	Loss 0.5454	TrainAcc 0.0419	ValidAcc 0.0400	TestAcc 0.0378	BestValid 0.1535
	Epoch 1175:	Loss 0.5440	TrainAcc 0.0203	ValidAcc 0.0195	TestAcc 0.0176	BestValid 0.1535
	Epoch 1200:	Loss 0.5432	TrainAcc 0.0348	ValidAcc 0.0337	TestAcc 0.0307	BestValid 0.1535
	Epoch 1225:	Loss 0.5350	TrainAcc 0.0441	ValidAcc 0.0421	TestAcc 0.0385	BestValid 0.1535
	Epoch 1250:	Loss 0.5290	TrainAcc 0.0466	ValidAcc 0.0451	TestAcc 0.0420	BestValid 0.1535
	Epoch 1275:	Loss 0.5297	TrainAcc 0.0262	ValidAcc 0.0288	TestAcc 0.0258	BestValid 0.1535
	Epoch 1300:	Loss 0.5290	TrainAcc 0.0463	ValidAcc 0.0449	TestAcc 0.0418	BestValid 0.1535
	Epoch 1325:	Loss 0.5266	TrainAcc 0.0375	ValidAcc 0.0392	TestAcc 0.0410	BestValid 0.1535
	Epoch 1350:	Loss 0.5194	TrainAcc 0.0380	ValidAcc 0.0399	TestAcc 0.0411	BestValid 0.1535
	Epoch 1375:	Loss 0.5204	TrainAcc 0.0306	ValidAcc 0.0300	TestAcc 0.0285	BestValid 0.1535
	Epoch 1400:	Loss 0.5170	TrainAcc 0.0308	ValidAcc 0.0283	TestAcc 0.0265	BestValid 0.1535
	Epoch 1425:	Loss 0.5176	TrainAcc 0.0289	ValidAcc 0.0268	TestAcc 0.0253	BestValid 0.1535
	Epoch 1450:	Loss 0.5150	TrainAcc 0.0443	ValidAcc 0.0432	TestAcc 0.0402	BestValid 0.1535
	Epoch 1475:	Loss 0.5086	TrainAcc 0.0243	ValidAcc 0.0256	TestAcc 0.0236	BestValid 0.1535
	Epoch 1500:	Loss 0.5050	TrainAcc 0.0325	ValidAcc 0.0313	TestAcc 0.0298	BestValid 0.1535
	Epoch 1525:	Loss 0.5048	TrainAcc 0.0308	ValidAcc 0.0301	TestAcc 0.0289	BestValid 0.1535
	Epoch 1550:	Loss 0.5019	TrainAcc 0.0285	ValidAcc 0.0265	TestAcc 0.0252	BestValid 0.1535
	Epoch 1575:	Loss 0.5003	TrainAcc 0.0328	ValidAcc 0.0321	TestAcc 0.0309	BestValid 0.1535
	Epoch 1600:	Loss 0.5007	TrainAcc 0.0331	ValidAcc 0.0313	TestAcc 0.0294	BestValid 0.1535
	Epoch 1625:	Loss 0.4954	TrainAcc 0.0398	ValidAcc 0.0383	TestAcc 0.0344	BestValid 0.1535
	Epoch 1650:	Loss 0.5031	TrainAcc 0.0268	ValidAcc 0.0254	TestAcc 0.0236	BestValid 0.1535
	Epoch 1675:	Loss 0.4950	TrainAcc 0.0443	ValidAcc 0.0436	TestAcc 0.0448	BestValid 0.1535
	Epoch 1700:	Loss 0.4938	TrainAcc 0.0576	ValidAcc 0.0629	TestAcc 0.0641	BestValid 0.1535
	Epoch 1725:	Loss 0.4987	TrainAcc 0.0128	ValidAcc 0.0114	TestAcc 0.0120	BestValid 0.1535
	Epoch 1750:	Loss 0.4904	TrainAcc 0.0293	ValidAcc 0.0269	TestAcc 0.0254	BestValid 0.1535
	Epoch 1775:	Loss 0.4906	TrainAcc 0.0412	ValidAcc 0.0398	TestAcc 0.0365	BestValid 0.1535
	Epoch 1800:	Loss 0.4873	TrainAcc 0.0277	ValidAcc 0.0256	TestAcc 0.0241	BestValid 0.1535
	Epoch 1825:	Loss 0.4918	TrainAcc 0.0390	ValidAcc 0.0382	TestAcc 0.0344	BestValid 0.1535
	Epoch 1850:	Loss 0.4808	TrainAcc 0.0432	ValidAcc 0.0410	TestAcc 0.0379	BestValid 0.1535
	Epoch 1875:	Loss 0.4801	TrainAcc 0.0363	ValidAcc 0.0374	TestAcc 0.0388	BestValid 0.1535
	Epoch 1900:	Loss 0.4767	TrainAcc 0.0348	ValidAcc 0.0382	TestAcc 0.0365	BestValid 0.1535
	Epoch 1925:	Loss 0.4795	TrainAcc 0.0264	ValidAcc 0.0256	TestAcc 0.0230	BestValid 0.1535
	Epoch 1950:	Loss 0.4806	TrainAcc 0.0446	ValidAcc 0.0416	TestAcc 0.0385	BestValid 0.1535
	Epoch 1975:	Loss 0.4759	TrainAcc 0.0437	ValidAcc 0.0413	TestAcc 0.0378	BestValid 0.1535
	Epoch 2000:	Loss 0.4727	TrainAcc 0.0273	ValidAcc 0.0259	TestAcc 0.0243	BestValid 0.1535
	Epoch 2025:	Loss 0.4765	TrainAcc 0.0352	ValidAcc 0.0376	TestAcc 0.0387	BestValid 0.1535
	Epoch 2050:	Loss 0.4785	TrainAcc 0.0319	ValidAcc 0.0312	TestAcc 0.0268	BestValid 0.1535
	Epoch 2075:	Loss 0.4734	TrainAcc 0.0609	ValidAcc 0.0706	TestAcc 0.0706	BestValid 0.1535
	Epoch 2100:	Loss 0.4720	TrainAcc 0.0338	ValidAcc 0.0350	TestAcc 0.0364	BestValid 0.1535
	Epoch 2125:	Loss 0.4834	TrainAcc 0.0111	ValidAcc 0.0087	TestAcc 0.0093	BestValid 0.1535
	Epoch 2150:	Loss 0.4687	TrainAcc 0.0406	ValidAcc 0.0386	TestAcc 0.0356	BestValid 0.1535
	Epoch 2175:	Loss 0.4651	TrainAcc 0.0269	ValidAcc 0.0253	TestAcc 0.0238	BestValid 0.1535
	Epoch 2200:	Loss 0.4655	TrainAcc 0.0529	ValidAcc 0.0629	TestAcc 0.0592	BestValid 0.1535
	Epoch 2225:	Loss 0.4637	TrainAcc 0.0252	ValidAcc 0.0248	TestAcc 0.0217	BestValid 0.1535
	Epoch 2250:	Loss 0.4625	TrainAcc 0.0324	ValidAcc 0.0363	TestAcc 0.0350	BestValid 0.1535
	Epoch 2275:	Loss 0.4645	TrainAcc 0.0200	ValidAcc 0.0194	TestAcc 0.0173	BestValid 0.1535
	Epoch 2300:	Loss 0.4617	TrainAcc 0.0610	ValidAcc 0.0722	TestAcc 0.0717	BestValid 0.1535
	Epoch 2325:	Loss 0.4960	TrainAcc 0.0111	ValidAcc 0.0089	TestAcc 0.0093	BestValid 0.1535
	Epoch 2350:	Loss 0.4616	TrainAcc 0.0199	ValidAcc 0.0198	TestAcc 0.0176	BestValid 0.1535
	Epoch 2375:	Loss 0.4598	TrainAcc 0.0233	ValidAcc 0.0274	TestAcc 0.0257	BestValid 0.1535
	Epoch 2400:	Loss 0.4625	TrainAcc 0.0635	ValidAcc 0.0713	TestAcc 0.0676	BestValid 0.1535
	Epoch 2425:	Loss 0.4842	TrainAcc 0.0444	ValidAcc 0.0500	TestAcc 0.0446	BestValid 0.1535
	Epoch 2450:	Loss 0.5180	TrainAcc 0.0235	ValidAcc 0.0237	TestAcc 0.0215	BestValid 0.1535
	Epoch 2475:	Loss 0.5308	TrainAcc 0.0714	ValidAcc 0.0616	TestAcc 0.0595	BestValid 0.1535
	Epoch 2500:	Loss 0.5289	TrainAcc 0.0293	ValidAcc 0.0281	TestAcc 0.0254	BestValid 0.1535
	Epoch 2525:	Loss 0.5233	TrainAcc 0.0275	ValidAcc 0.0261	TestAcc 0.0247	BestValid 0.1535
	Epoch 2550:	Loss 0.5174	TrainAcc 0.0408	ValidAcc 0.0380	TestAcc 0.0352	BestValid 0.1535
	Epoch 2575:	Loss 0.4923	TrainAcc 0.0349	ValidAcc 0.0363	TestAcc 0.0376	BestValid 0.1535
	Epoch 2600:	Loss 0.4761	TrainAcc 0.0536	ValidAcc 0.0517	TestAcc 0.0520	BestValid 0.1535
	Epoch 2625:	Loss 0.4734	TrainAcc 0.0339	ValidAcc 0.0309	TestAcc 0.0317	BestValid 0.1535
	Epoch 2650:	Loss 0.4663	TrainAcc 0.0519	ValidAcc 0.0496	TestAcc 0.0518	BestValid 0.1535
	Epoch 2675:	Loss 0.4633	TrainAcc 0.0469	ValidAcc 0.0458	TestAcc 0.0463	BestValid 0.1535
	Epoch 2700:	Loss 0.4582	TrainAcc 0.0481	ValidAcc 0.0475	TestAcc 0.0479	BestValid 0.1535
	Epoch 2725:	Loss 0.4614	TrainAcc 0.0460	ValidAcc 0.0453	TestAcc 0.0460	BestValid 0.1535
	Epoch 2750:	Loss 0.4542	TrainAcc 0.0433	ValidAcc 0.0417	TestAcc 0.0390	BestValid 0.1535
	Epoch 2775:	Loss 0.4549	TrainAcc 0.0307	ValidAcc 0.0277	TestAcc 0.0260	BestValid 0.1535
	Epoch 2800:	Loss 0.4518	TrainAcc 0.0377	ValidAcc 0.0358	TestAcc 0.0325	BestValid 0.1535
	Epoch 2825:	Loss 0.4507	TrainAcc 0.0435	ValidAcc 0.0425	TestAcc 0.0384	BestValid 0.1535
	Epoch 2850:	Loss 0.4508	TrainAcc 0.0574	ValidAcc 0.0546	TestAcc 0.0524	BestValid 0.1535
	Epoch 2875:	Loss 0.4476	TrainAcc 0.0394	ValidAcc 0.0382	TestAcc 0.0395	BestValid 0.1535
	Epoch 2900:	Loss 0.4468	TrainAcc 0.0354	ValidAcc 0.0318	TestAcc 0.0299	BestValid 0.1535
	Epoch 2925:	Loss 0.4538	TrainAcc 0.0123	ValidAcc 0.0107	TestAcc 0.0108	BestValid 0.1535
	Epoch 2950:	Loss 0.4487	TrainAcc 0.0416	ValidAcc 0.0399	TestAcc 0.0365	BestValid 0.1535
	Epoch 2975:	Loss 0.4456	TrainAcc 0.0407	ValidAcc 0.0388	TestAcc 0.0358	BestValid 0.1535
	Epoch 3000:	Loss 0.4446	TrainAcc 0.0369	ValidAcc 0.0340	TestAcc 0.0312	BestValid 0.1535
	Epoch 3025:	Loss 0.4491	TrainAcc 0.0461	ValidAcc 0.0449	TestAcc 0.0462	BestValid 0.1535
	Epoch 3050:	Loss 0.4475	TrainAcc 0.0375	ValidAcc 0.0372	TestAcc 0.0382	BestValid 0.1535
	Epoch 3075:	Loss 0.4514	TrainAcc 0.0191	ValidAcc 0.0189	TestAcc 0.0163	BestValid 0.1535
	Epoch 3100:	Loss 0.4516	TrainAcc 0.0521	ValidAcc 0.0518	TestAcc 0.0513	BestValid 0.1535
	Epoch 3125:	Loss 0.4624	TrainAcc 0.0279	ValidAcc 0.0268	TestAcc 0.0253	BestValid 0.1535
	Epoch 3150:	Loss 0.4540	TrainAcc 0.0458	ValidAcc 0.0441	TestAcc 0.0451	BestValid 0.1535
	Epoch 3175:	Loss 0.4497	TrainAcc 0.0554	ValidAcc 0.0526	TestAcc 0.0510	BestValid 0.1535
	Epoch 3200:	Loss 0.4479	TrainAcc 0.0415	ValidAcc 0.0398	TestAcc 0.0364	BestValid 0.1535
	Epoch 3225:	Loss 0.4435	TrainAcc 0.0359	ValidAcc 0.0348	TestAcc 0.0305	BestValid 0.1535
	Epoch 3250:	Loss 0.4461	TrainAcc 0.0219	ValidAcc 0.0220	TestAcc 0.0197	BestValid 0.1535
	Epoch 3275:	Loss 0.4390	TrainAcc 0.0190	ValidAcc 0.0188	TestAcc 0.0162	BestValid 0.1535
	Epoch 3300:	Loss 0.4388	TrainAcc 0.0192	ValidAcc 0.0189	TestAcc 0.0165	BestValid 0.1535
	Epoch 3325:	Loss 0.4396	TrainAcc 0.0194	ValidAcc 0.0192	TestAcc 0.0166	BestValid 0.1535
	Epoch 3350:	Loss 0.4359	TrainAcc 0.0415	ValidAcc 0.0399	TestAcc 0.0361	BestValid 0.1535
	Epoch 3375:	Loss 0.4368	TrainAcc 0.0192	ValidAcc 0.0188	TestAcc 0.0164	BestValid 0.1535
	Epoch 3400:	Loss 0.4358	TrainAcc 0.0188	ValidAcc 0.0185	TestAcc 0.0161	BestValid 0.1535
	Epoch 3425:	Loss 0.4359	TrainAcc 0.0186	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 3450:	Loss 0.4326	TrainAcc 0.0187	ValidAcc 0.0183	TestAcc 0.0161	BestValid 0.1535
	Epoch 3475:	Loss 0.4344	TrainAcc 0.0188	ValidAcc 0.0185	TestAcc 0.0162	BestValid 0.1535
	Epoch 3500:	Loss 0.4331	TrainAcc 0.0186	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 3525:	Loss 0.4658	TrainAcc 0.0186	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 3550:	Loss 0.4377	TrainAcc 0.0186	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 3575:	Loss 0.4385	TrainAcc 0.0187	ValidAcc 0.0183	TestAcc 0.0161	BestValid 0.1535
	Epoch 3600:	Loss 0.4329	TrainAcc 0.0187	ValidAcc 0.0184	TestAcc 0.0161	BestValid 0.1535
	Epoch 3625:	Loss 0.4357	TrainAcc 0.0398	ValidAcc 0.0385	TestAcc 0.0400	BestValid 0.1535
	Epoch 3650:	Loss 0.4358	TrainAcc 0.0416	ValidAcc 0.0409	TestAcc 0.0422	BestValid 0.1535
	Epoch 3675:	Loss 0.4309	TrainAcc 0.0418	ValidAcc 0.0404	TestAcc 0.0369	BestValid 0.1535
	Epoch 3700:	Loss 0.4345	TrainAcc 0.0369	ValidAcc 0.0354	TestAcc 0.0325	BestValid 0.1535
	Epoch 3725:	Loss 0.4299	TrainAcc 0.0186	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 3750:	Loss 0.4296	TrainAcc 0.0223	ValidAcc 0.0223	TestAcc 0.0204	BestValid 0.1535
	Epoch 3775:	Loss 0.4271	TrainAcc 0.0370	ValidAcc 0.0371	TestAcc 0.0362	BestValid 0.1535
	Epoch 3800:	Loss 0.4264	TrainAcc 0.0468	ValidAcc 0.0456	TestAcc 0.0427	BestValid 0.1535
	Epoch 3825:	Loss 0.4328	TrainAcc 0.0341	ValidAcc 0.0348	TestAcc 0.0365	BestValid 0.1535
	Epoch 3850:	Loss 0.4251	TrainAcc 0.0423	ValidAcc 0.0404	TestAcc 0.0368	BestValid 0.1535
	Epoch 3875:	Loss 0.4255	TrainAcc 0.0206	ValidAcc 0.0201	TestAcc 0.0178	BestValid 0.1535
	Epoch 3900:	Loss 0.4234	TrainAcc 0.0187	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 3925:	Loss 0.4218	TrainAcc 0.0186	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 3950:	Loss 0.4241	TrainAcc 0.0186	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 3975:	Loss 0.4220	TrainAcc 0.0192	ValidAcc 0.0190	TestAcc 0.0166	BestValid 0.1535
	Epoch 4000:	Loss 0.4217	TrainAcc 0.0187	ValidAcc 0.0183	TestAcc 0.0161	BestValid 0.1535
	Epoch 4025:	Loss 0.4275	TrainAcc 0.0342	ValidAcc 0.0352	TestAcc 0.0367	BestValid 0.1535
	Epoch 4050:	Loss 0.4246	TrainAcc 0.0351	ValidAcc 0.0340	TestAcc 0.0336	BestValid 0.1535
	Epoch 4075:	Loss 0.4400	TrainAcc 0.0186	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 4100:	Loss 0.4248	TrainAcc 0.0199	ValidAcc 0.0189	TestAcc 0.0167	BestValid 0.1535
	Epoch 4125:	Loss 0.4383	TrainAcc 0.0511	ValidAcc 0.0514	TestAcc 0.0516	BestValid 0.1535
	Epoch 4150:	Loss 0.4290	TrainAcc 0.0206	ValidAcc 0.0195	TestAcc 0.0175	BestValid 0.1535
	Epoch 4175:	Loss 0.4262	TrainAcc 0.0349	ValidAcc 0.0351	TestAcc 0.0367	BestValid 0.1535
	Epoch 4200:	Loss 0.4245	TrainAcc 0.0200	ValidAcc 0.0191	TestAcc 0.0168	BestValid 0.1535
	Epoch 4225:	Loss 0.4280	TrainAcc 0.0197	ValidAcc 0.0188	TestAcc 0.0166	BestValid 0.1535
	Epoch 4250:	Loss 0.4268	TrainAcc 0.0198	ValidAcc 0.0190	TestAcc 0.0166	BestValid 0.1535
	Epoch 4275:	Loss 0.4276	TrainAcc 0.0197	ValidAcc 0.0189	TestAcc 0.0166	BestValid 0.1535
	Epoch 4300:	Loss 0.4332	TrainAcc 0.0350	ValidAcc 0.0357	TestAcc 0.0370	BestValid 0.1535
	Epoch 4325:	Loss 0.4434	TrainAcc 0.0123	ValidAcc 0.0106	TestAcc 0.0107	BestValid 0.1535
	Epoch 4350:	Loss 0.4359	TrainAcc 0.0124	ValidAcc 0.0107	TestAcc 0.0108	BestValid 0.1535
	Epoch 4375:	Loss 0.4282	TrainAcc 0.0186	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 4400:	Loss 0.4237	TrainAcc 0.0196	ValidAcc 0.0188	TestAcc 0.0165	BestValid 0.1535
	Epoch 4425:	Loss 0.4219	TrainAcc 0.0199	ValidAcc 0.0191	TestAcc 0.0167	BestValid 0.1535
	Epoch 4450:	Loss 0.4193	TrainAcc 0.0197	ValidAcc 0.0188	TestAcc 0.0166	BestValid 0.1535
	Epoch 4475:	Loss 0.4196	TrainAcc 0.0264	ValidAcc 0.0252	TestAcc 0.0234	BestValid 0.1535
	Epoch 4500:	Loss 0.4173	TrainAcc 0.0187	ValidAcc 0.0184	TestAcc 0.0162	BestValid 0.1535
	Epoch 4525:	Loss 0.4249	TrainAcc 0.0187	ValidAcc 0.0183	TestAcc 0.0160	BestValid 0.1535
	Epoch 4550:	Loss 0.4211	TrainAcc 0.0187	ValidAcc 0.0183	TestAcc 0.0161	BestValid 0.1535
	Epoch 4575:	Loss 0.4197	TrainAcc 0.0187	ValidAcc 0.0183	TestAcc 0.0161	BestValid 0.1535
	Epoch 4600:	Loss 0.4186	TrainAcc 0.0783	ValidAcc 0.0822	TestAcc 0.0781	BestValid 0.1535
	Epoch 4625:	Loss 0.4264	TrainAcc 0.0416	ValidAcc 0.0399	TestAcc 0.0363	BestValid 0.1535
	Epoch 4650:	Loss 0.4308	TrainAcc 0.0270	ValidAcc 0.0254	TestAcc 0.0240	BestValid 0.1535
	Epoch 4675:	Loss 0.4354	TrainAcc 0.0163	ValidAcc 0.0153	TestAcc 0.0157	BestValid 0.1535
	Epoch 4700:	Loss 0.4359	TrainAcc 0.0370	ValidAcc 0.0310	TestAcc 0.0295	BestValid 0.1535
	Epoch 4725:	Loss 0.4519	TrainAcc 0.0307	ValidAcc 0.0284	TestAcc 0.0276	BestValid 0.1535
	Epoch 4750:	Loss 0.4576	TrainAcc 0.0431	ValidAcc 0.0416	TestAcc 0.0373	BestValid 0.1535
	Epoch 4775:	Loss 0.4378	TrainAcc 0.0352	ValidAcc 0.0357	TestAcc 0.0371	BestValid 0.1535
	Epoch 4800:	Loss 0.4353	TrainAcc 0.0268	ValidAcc 0.0249	TestAcc 0.0235	BestValid 0.1535
	Epoch 4825:	Loss 0.4376	TrainAcc 0.0275	ValidAcc 0.0258	TestAcc 0.0240	BestValid 0.1535
	Epoch 4850:	Loss 0.4260	TrainAcc 0.0275	ValidAcc 0.0259	TestAcc 0.0240	BestValid 0.1535
	Epoch 4875:	Loss 0.4250	TrainAcc 0.0277	ValidAcc 0.0259	TestAcc 0.0241	BestValid 0.1535
	Epoch 4900:	Loss 0.4194	TrainAcc 0.0312	ValidAcc 0.0281	TestAcc 0.0271	BestValid 0.1535
	Epoch 4925:	Loss 0.4211	TrainAcc 0.0218	ValidAcc 0.0213	TestAcc 0.0191	BestValid 0.1535
	Epoch 4950:	Loss 0.4163	TrainAcc 0.0263	ValidAcc 0.0245	TestAcc 0.0231	BestValid 0.1535
	Epoch 4975:	Loss 0.4121	TrainAcc 0.0273	ValidAcc 0.0260	TestAcc 0.0234	BestValid 0.1535
	Epoch 5000:	Loss 0.4102	TrainAcc 0.0276	ValidAcc 0.0254	TestAcc 0.0239	BestValid 0.1535
Node 1, Pre/Post-Pipelining: 1.091 / 0.854 ms, Bubble: 73.784 ms, Compute: 251.143 ms, Comm: 27.659 ms, Imbalance: 14.678 ms
Node 2, Pre/Post-Pipelining: 1.088 / 0.877 ms, Bubble: 74.008 ms, Compute: 242.624 ms, Comm: 29.648 ms, Imbalance: 21.280 ms
Node 0, Pre/Post-Pipelining: 1.090 / 0.884 ms, Bubble: 74.369 ms, Compute: 256.533 ms, Comm: 16.825 ms, Imbalance: 19.369 ms
Node 3, Pre/Post-Pipelining: 1.087 / 0.885 ms, Bubble: 73.354 ms, Compute: 248.370 ms, Comm: 26.313 ms, Imbalance: 19.344 ms
Node 5, Pre/Post-Pipelining: 1.089 / 0.882 ms, Bubble: 73.507 ms, Compute: 245.550 ms, Comm: 30.680 ms, Imbalance: 17.850 ms
Node 6, Pre/Post-Pipelining: 1.087 / 0.881 ms, Bubble: 73.415 ms, Compute: 247.083 ms, Comm: 29.140 ms, Imbalance: 17.949 ms
Node 7, Pre/Post-Pipelining: 1.089 / 16.505 ms, Bubble: 58.119 ms, Compute: 258.400 ms, Comm: 16.927 ms, Imbalance: 18.368 ms
Node 4, Pre/Post-Pipelining: 1.086 / 0.891 ms, Bubble: 73.673 ms, Compute: 241.576 ms, Comm: 26.173 ms, Imbalance: 26.341 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.090 ms
Cluster-Wide Average, Post-Pipelining Overhead: 0.884 ms
Cluster-Wide Average, Bubble: 74.369 ms
Cluster-Wide Average, Compute: 256.533 ms
Cluster-Wide Average, Communication: 16.825 ms
Cluster-Wide Average, Imbalance: 19.369 ms
Node 0, GPU memory consumption: 6.739 GB
Node 3, GPU memory consumption: 5.796 GB
Node 1, GPU memory consumption: 5.819 GB
Node 2, GPU memory consumption: 5.819 GB
Node 7, GPU memory consumption: 5.569 GB
Node 5, GPU memory consumption: 5.819 GB
Node 4, GPU memory consumption: 5.796 GB
Node 6, GPU memory consumption: 5.819 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.745570 s---------------
------------------------node id 1,  per-epoch time: 0.745570 s---------------
------------------------node id 4,  per-epoch time: 0.745570 s---------------
------------------------node id 2,  per-epoch time: 0.745570 s---------------
------------------------node id 5,  per-epoch time: 0.745570 s---------------
------------------------node id 3,  per-epoch time: 0.745570 s---------------
------------------------node id 6,  per-epoch time: 0.745570 s---------------
------------------------node id 7,  per-epoch time: 0.745570 s---------------
************ Profiling Results ************
	Bubble: 497.108232 (ms) (66.70 percentage)
	Compute: 244.678820 (ms) (32.83 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.519275 (ms) (0.47 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 1.216 GB
	Aggregated layer-level communication throughput: 410.565 Gbps
Highest valid_acc: 0.1535
Target test_acc: 0.1491
Epoch to reach the target acc: 174
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
