Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.911 seconds.
Building the CSC structure...
        It takes 1.962 seconds.
Building the CSC structure...
        It takes 2.042 seconds.
Building the CSC structure...
        It takes 2.249 seconds.
Building the CSC structure...
        It takes 2.260 seconds.
Building the CSC structure...
        It takes 2.445 seconds.
Building the CSC structure...
        It takes 2.611 seconds.
Building the CSC structure...
        It takes 2.665 seconds.
Building the CSC structure...
        It takes 1.840 seconds.
        It takes 1.831 seconds.
        It takes 1.851 seconds.
        It takes 2.174 seconds.
        It takes 2.372 seconds.
Building the Feature Vector...
        It takes 2.311 seconds.
Building the Feature Vector...
        It takes 2.289 seconds.
Building the Feature Vector...
        It takes 0.296 seconds.
Building the Label Vector...
        It takes 2.378 seconds.
        It takes 0.282 seconds.
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.032 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.267 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.286 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.038 seconds.
Building the Feature Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.277 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
        It takes 0.272 seconds.
Building the Label Vector...
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 56.576 Gbps (per GPU), 452.607 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.320 Gbps (per GPU), 450.564 Gbps (aggregated)
The layer-level communication performance: 56.310 Gbps (per GPU), 450.480 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.082 Gbps (per GPU), 448.656 Gbps (aggregated)
The layer-level communication performance: 56.053 Gbps (per GPU), 448.427 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.883 Gbps (per GPU), 447.061 Gbps (aggregated)
The layer-level communication performance: 55.840 Gbps (per GPU), 446.723 Gbps (aggregated)
The layer-level communication performance: 55.811 Gbps (per GPU), 446.488 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 161.549 Gbps (per GPU), 1292.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.549 Gbps (per GPU), 1292.394 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.537 Gbps (per GPU), 1292.295 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.437 Gbps (per GPU), 1291.499 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.534 Gbps (per GPU), 1292.270 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.531 Gbps (per GPU), 1292.245 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.534 Gbps (per GPU), 1292.271 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 161.468 Gbps (per GPU), 1291.747 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.512 Gbps (per GPU), 836.095 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.514 Gbps (per GPU), 836.109 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.512 Gbps (per GPU), 836.095 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.511 Gbps (per GPU), 836.088 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.513 Gbps (per GPU), 836.102 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.513 Gbps (per GPU), 836.102 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.513 Gbps (per GPU), 836.102 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.512 Gbps (per GPU), 836.095 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.072 Gbps (per GPU), 272.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.072 Gbps (per GPU), 272.576 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.073 Gbps (per GPU), 272.582 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.072 Gbps (per GPU), 272.575 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.072 Gbps (per GPU), 272.577 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.072 Gbps (per GPU), 272.572 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.071 Gbps (per GPU), 272.570 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.060 Gbps (per GPU), 272.483 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.39ms  2.50ms  2.26ms  1.11  8.38K  3.53M
 chk_1  2.83ms  2.73ms  2.58ms  1.10  6.74K  3.60M
 chk_2  2.70ms  2.61ms  2.46ms  1.10  7.27K  3.53M
 chk_3  2.71ms  2.66ms  2.47ms  1.10  7.92K  3.61M
 chk_4  2.66ms  2.59ms  2.48ms  1.07  5.33K  3.68M
 chk_5  2.62ms  2.53ms  2.32ms  1.13 10.07K  3.45M
 chk_6  2.82ms  2.70ms  2.48ms  1.14  9.41K  3.48M
 chk_7  2.68ms  2.59ms  2.40ms  1.11  8.12K  3.60M
 chk_8  2.76ms  2.69ms  2.56ms  1.08  6.09K  3.64M
 chk_9  2.57ms  2.46ms  2.20ms  1.17 11.10K  3.38M
chk_10  2.79ms  2.73ms  2.61ms  1.07  5.67K  3.63M
chk_11  2.65ms  2.57ms  2.41ms  1.10  8.16K  3.54M
chk_12  2.86ms  2.78ms  2.62ms  1.09  7.24K  3.55M
chk_13  2.69ms  2.63ms  2.52ms  1.07  5.41K  3.68M
chk_14  2.92ms  2.84ms  2.69ms  1.09  7.14K  3.53M
chk_15  2.80ms  2.69ms  2.50ms  1.12  9.25K  3.49M
chk_16  2.62ms  2.57ms  2.48ms  1.05  4.78K  3.77M
chk_17  2.75ms  2.67ms  2.53ms  1.09  6.85K  3.60M
chk_18  2.58ms  2.46ms  2.34ms  1.10  7.47K  3.57M
chk_19  2.62ms  2.57ms  2.48ms  1.06  4.88K  3.75M
chk_20  2.67ms  2.56ms  2.42ms  1.11  7.00K  3.63M
chk_21  2.61ms  2.56ms  2.43ms  1.08  5.41K  3.68M
chk_22  2.82ms  2.70ms  2.43ms  1.16 11.07K  3.39M
chk_23  2.74ms  2.64ms  2.50ms  1.10  7.23K  3.64M
chk_24  2.78ms  2.68ms  2.44ms  1.14 10.13K  3.43M
chk_25  2.58ms  2.52ms  2.38ms  1.08  6.40K  3.57M
chk_26  2.78ms  2.73ms  2.60ms  1.07  5.78K  3.55M
chk_27  2.71ms  2.59ms  2.39ms  1.13  9.34K  3.48M
chk_28  3.02ms  2.89ms  2.86ms  1.06  6.37K  3.57M
chk_29  2.81ms  2.78ms  2.59ms  1.08  5.16K  3.78M
chk_30  2.68ms  2.67ms  2.49ms  1.07  5.44K  3.67M
chk_31  2.81ms  2.77ms  2.62ms  1.07  6.33K  3.63M
   Avg  2.72  2.65  2.49
   Max  3.02  2.89  2.86
   Min  2.39  2.46  2.20
 Ratio  1.26  1.17  1.30
   Var  0.01  0.01  0.02
Profiling takes 2.912 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 341.111 ms
Partition 0 [0, 4) has cost: 341.111 ms
Partition 1 [4, 8) has cost: 338.767 ms
Partition 2 [8, 12) has cost: 338.767 ms
Partition 3 [12, 16) has cost: 338.767 ms
Partition 4 [16, 20) has cost: 338.767 ms
Partition 5 [20, 24) has cost: 338.767 ms
Partition 6 [24, 28) has cost: 338.767 ms
Partition 7 [28, 32) has cost: 333.605 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 161.375 ms
GPU 0, Compute+Comm Time: 127.395 ms, Bubble Time: 28.867 ms, Imbalance Overhead: 5.113 ms
GPU 1, Compute+Comm Time: 126.023 ms, Bubble Time: 28.523 ms, Imbalance Overhead: 6.830 ms
GPU 2, Compute+Comm Time: 126.023 ms, Bubble Time: 28.375 ms, Imbalance Overhead: 6.977 ms
GPU 3, Compute+Comm Time: 126.023 ms, Bubble Time: 28.168 ms, Imbalance Overhead: 7.185 ms
GPU 4, Compute+Comm Time: 126.023 ms, Bubble Time: 28.018 ms, Imbalance Overhead: 7.334 ms
GPU 5, Compute+Comm Time: 126.023 ms, Bubble Time: 27.975 ms, Imbalance Overhead: 7.377 ms
GPU 6, Compute+Comm Time: 126.023 ms, Bubble Time: 27.933 ms, Imbalance Overhead: 7.419 ms
GPU 7, Compute+Comm Time: 124.083 ms, Bubble Time: 28.103 ms, Imbalance Overhead: 9.189 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 306.699 ms
GPU 0, Compute+Comm Time: 235.876 ms, Bubble Time: 53.242 ms, Imbalance Overhead: 17.581 ms
GPU 1, Compute+Comm Time: 239.098 ms, Bubble Time: 53.069 ms, Imbalance Overhead: 14.532 ms
GPU 2, Compute+Comm Time: 239.098 ms, Bubble Time: 53.148 ms, Imbalance Overhead: 14.453 ms
GPU 3, Compute+Comm Time: 239.098 ms, Bubble Time: 53.227 ms, Imbalance Overhead: 14.374 ms
GPU 4, Compute+Comm Time: 239.098 ms, Bubble Time: 53.651 ms, Imbalance Overhead: 13.950 ms
GPU 5, Compute+Comm Time: 239.098 ms, Bubble Time: 54.051 ms, Imbalance Overhead: 13.550 ms
GPU 6, Compute+Comm Time: 239.098 ms, Bubble Time: 54.406 ms, Imbalance Overhead: 13.195 ms
GPU 7, Compute+Comm Time: 240.069 ms, Bubble Time: 55.490 ms, Imbalance Overhead: 11.140 ms
The estimated cost of the whole pipeline: 491.477 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 679.877 ms
Partition 0 [0, 8) has cost: 679.877 ms
Partition 1 [8, 16) has cost: 677.534 ms
Partition 2 [16, 24) has cost: 677.534 ms
Partition 3 [24, 32) has cost: 672.372 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 168.773 ms
GPU 0, Compute+Comm Time: 138.665 ms, Bubble Time: 26.912 ms, Imbalance Overhead: 3.196 ms
GPU 1, Compute+Comm Time: 137.941 ms, Bubble Time: 26.228 ms, Imbalance Overhead: 4.605 ms
GPU 2, Compute+Comm Time: 137.941 ms, Bubble Time: 25.854 ms, Imbalance Overhead: 4.978 ms
GPU 3, Compute+Comm Time: 137.053 ms, Bubble Time: 25.483 ms, Imbalance Overhead: 6.237 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 308.677 ms
GPU 0, Compute+Comm Time: 251.121 ms, Bubble Time: 46.962 ms, Imbalance Overhead: 10.594 ms
GPU 1, Compute+Comm Time: 252.507 ms, Bubble Time: 47.383 ms, Imbalance Overhead: 8.787 ms
GPU 2, Compute+Comm Time: 252.507 ms, Bubble Time: 47.844 ms, Imbalance Overhead: 8.327 ms
GPU 3, Compute+Comm Time: 253.038 ms, Bubble Time: 49.155 ms, Imbalance Overhead: 6.485 ms
    The estimated cost with 2 DP ways is 501.323 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1357.411 ms
Partition 0 [0, 16) has cost: 1357.411 ms
Partition 1 [16, 32) has cost: 1349.906 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 214.983 ms
GPU 0, Compute+Comm Time: 186.952 ms, Bubble Time: 23.237 ms, Imbalance Overhead: 4.795 ms
GPU 1, Compute+Comm Time: 186.142 ms, Bubble Time: 23.538 ms, Imbalance Overhead: 5.304 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 347.906 ms
GPU 0, Compute+Comm Time: 303.973 ms, Bubble Time: 37.875 ms, Imbalance Overhead: 6.059 ms
GPU 1, Compute+Comm Time: 304.850 ms, Bubble Time: 38.035 ms, Imbalance Overhead: 5.021 ms
    The estimated cost with 4 DP ways is 591.034 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2707.317 ms
Partition 0 [0, 32) has cost: 2707.317 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 558.396 ms
GPU 0, Compute+Comm Time: 558.396 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 678.888 ms
GPU 0, Compute+Comm Time: 678.888 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1299.148 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 24)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [24, 48)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [48, 72)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [72, 96)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [96, 120)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [120, 144)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [144, 168)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [168, 190)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 24)...
+++++++++ Node 1 initializing the weights for op[24, 48)...
+++++++++ Node 2 initializing the weights for op[48, 72)...
+++++++++ Node 3 initializing the weights for op[72, 96)...
+++++++++ Node 4 initializing the weights for op[96, 120)...
+++++++++ Node 5 initializing the weights for op[120, 144)...
+++++++++ Node 7 initializing the weights for op[168, 190)...
+++++++++ Node 6 initializing the weights for op[144, 168)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.3245	TrainAcc 0.0213	ValidAcc 0.0183	TestAcc 0.0173	BestValid 0.0183
	Epoch 50:	Loss 3.0916	TrainAcc 0.1241	ValidAcc 0.1187	TestAcc 0.1150	BestValid 0.1187
	Epoch 75:	Loss 2.9066	TrainAcc 0.0891	ValidAcc 0.0744	TestAcc 0.0730	BestValid 0.1187
	Epoch 100:	Loss 2.6845	TrainAcc 0.0182	ValidAcc 0.0166	TestAcc 0.0157	BestValid 0.1187
	Epoch 125:	Loss 2.3416	TrainAcc 0.0380	ValidAcc 0.0368	TestAcc 0.0350	BestValid 0.1187
	Epoch 150:	Loss 2.0439	TrainAcc 0.0546	ValidAcc 0.0496	TestAcc 0.0520	BestValid 0.1187
	Epoch 175:	Loss 1.8047	TrainAcc 0.0826	ValidAcc 0.0760	TestAcc 0.0747	BestValid 0.1187
	Epoch 200:	Loss 1.6134	TrainAcc 0.0484	ValidAcc 0.0438	TestAcc 0.0449	BestValid 0.1187
	Epoch 225:	Loss 1.4236	TrainAcc 0.0462	ValidAcc 0.0399	TestAcc 0.0411	BestValid 0.1187
	Epoch 250:	Loss 1.3164	TrainAcc 0.0920	ValidAcc 0.0911	TestAcc 0.0899	BestValid 0.1187
	Epoch 275:	Loss 1.2002	TrainAcc 0.0173	ValidAcc 0.0334	TestAcc 0.0320	BestValid 0.1187
	Epoch 300:	Loss 1.1476	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 325:	Loss 1.1185	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 350:	Loss 1.0497	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 375:	Loss 1.0100	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 400:	Loss 0.9791	TrainAcc 0.0889	ValidAcc 0.0883	TestAcc 0.0878	BestValid 0.1187
	Epoch 425:	Loss 0.9519	TrainAcc 0.0555	ValidAcc 0.0606	TestAcc 0.0578	BestValid 0.1187
	Epoch 450:	Loss 0.9261	TrainAcc 0.0461	ValidAcc 0.0397	TestAcc 0.0411	BestValid 0.1187
	Epoch 475:	Loss 0.8842	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 500:	Loss 0.8541	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 525:	Loss 0.8385	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 550:	Loss 0.8401	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 575:	Loss 0.8073	TrainAcc 0.0472	ValidAcc 0.0402	TestAcc 0.0420	BestValid 0.1187
	Epoch 600:	Loss 0.7920	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 625:	Loss 0.7807	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 650:	Loss 0.7463	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 675:	Loss 0.7327	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 700:	Loss 0.7179	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 725:	Loss 0.7146	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 750:	Loss 0.6957	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 775:	Loss 0.6857	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 800:	Loss 0.6774	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 825:	Loss 0.6680	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 850:	Loss 0.6650	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 875:	Loss 0.6551	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 900:	Loss 0.6441	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 925:	Loss 0.6492	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 950:	Loss 0.6282	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 975:	Loss 0.6150	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1000:	Loss 0.6194	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1025:	Loss 0.6168	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1050:	Loss 0.6083	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1075:	Loss 0.5910	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1100:	Loss 0.5831	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1125:	Loss 0.6133	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1150:	Loss 0.6012	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1175:	Loss 0.5795	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1200:	Loss 0.5594	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1225:	Loss 0.5681	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1250:	Loss 0.5598	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1275:	Loss 0.5431	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1300:	Loss 0.5409	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1325:	Loss 0.5757	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1350:	Loss 0.5432	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1375:	Loss 0.5553	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1400:	Loss 0.5318	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1425:	Loss 0.5330	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1450:	Loss 0.5260	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1475:	Loss 0.5231	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1500:	Loss 0.5191	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1525:	Loss 0.5397	TrainAcc 0.0472	ValidAcc 0.0405	TestAcc 0.0420	BestValid 0.1187
	Epoch 1550:	Loss 0.5157	TrainAcc 0.0501	ValidAcc 0.0444	TestAcc 0.0459	BestValid 0.1187
	Epoch 1575:	Loss 0.5055	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1600:	Loss 0.5018	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1625:	Loss 0.5041	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1650:	Loss 0.5111	TrainAcc 0.0518	ValidAcc 0.0472	TestAcc 0.0485	BestValid 0.1187
	Epoch 1675:	Loss 0.5083	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1700:	Loss 0.4978	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1725:	Loss 0.5217	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1750:	Loss 0.5116	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1775:	Loss 0.5033	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1800:	Loss 0.5005	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1825:	Loss 0.4981	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1850:	Loss 0.4912	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1875:	Loss 0.4917	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1900:	Loss 0.4919	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1925:	Loss 0.4967	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1950:	Loss 0.4819	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 1975:	Loss 0.4785	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2000:	Loss 0.4796	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2025:	Loss 0.4986	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2050:	Loss 0.5646	TrainAcc 0.0470	ValidAcc 0.0403	TestAcc 0.0418	BestValid 0.1187
	Epoch 2075:	Loss 0.5912	TrainAcc 0.0981	ValidAcc 0.0976	TestAcc 0.0962	BestValid 0.1187
	Epoch 2100:	Loss 0.5731	TrainAcc 0.0980	ValidAcc 0.0976	TestAcc 0.0962	BestValid 0.1187
	Epoch 2125:	Loss 0.6297	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2150:	Loss 0.6042	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2175:	Loss 0.5889	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2200:	Loss 0.5742	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2225:	Loss 0.5389	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2250:	Loss 0.5410	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2275:	Loss 0.5454	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2300:	Loss 0.5328	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2325:	Loss 0.5294	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2350:	Loss 0.5012	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2375:	Loss 0.4914	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2400:	Loss 0.4880	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2425:	Loss 0.4853	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2450:	Loss 0.4915	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2475:	Loss 0.4963	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2500:	Loss 0.5157	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2525:	Loss 0.4911	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2550:	Loss 0.4844	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2575:	Loss 0.4938	TrainAcc 0.0411	ValidAcc 0.0361	TestAcc 0.0378	BestValid 0.1187
	Epoch 2600:	Loss 0.4689	TrainAcc 0.0473	ValidAcc 0.0405	TestAcc 0.0420	BestValid 0.1187
	Epoch 2625:	Loss 0.4735	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2650:	Loss 0.4647	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2675:	Loss 0.4648	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2700:	Loss 0.4579	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2725:	Loss 0.4552	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2750:	Loss 0.4502	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2775:	Loss 0.4467	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2800:	Loss 0.4449	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2825:	Loss 0.4486	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2850:	Loss 0.4447	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2875:	Loss 0.4485	TrainAcc 0.0971	ValidAcc 0.0971	TestAcc 0.0955	BestValid 0.1187
	Epoch 2900:	Loss 0.4495	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2925:	Loss 0.4936	TrainAcc 0.0074	ValidAcc 0.0069	TestAcc 0.0069	BestValid 0.1187
	Epoch 2950:	Loss 0.4960	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 2975:	Loss 0.5688	TrainAcc 0.0562	ValidAcc 0.0610	TestAcc 0.0587	BestValid 0.1187
	Epoch 3000:	Loss 0.4997	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3025:	Loss 0.5122	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3050:	Loss 0.4862	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3075:	Loss 0.4678	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3100:	Loss 0.4567	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3125:	Loss 0.4454	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3150:	Loss 0.4433	TrainAcc 0.0499	ValidAcc 0.0441	TestAcc 0.0457	BestValid 0.1187
	Epoch 3175:	Loss 0.4375	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3200:	Loss 0.4376	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3225:	Loss 0.4341	TrainAcc 0.0474	ValidAcc 0.0405	TestAcc 0.0420	BestValid 0.1187
	Epoch 3250:	Loss 0.4303	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3275:	Loss 0.4310	TrainAcc 0.0493	ValidAcc 0.0431	TestAcc 0.0447	BestValid 0.1187
	Epoch 3300:	Loss 0.4272	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3325:	Loss 0.4250	TrainAcc 0.0555	ValidAcc 0.0527	TestAcc 0.0530	BestValid 0.1187
	Epoch 3350:	Loss 0.4250	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3375:	Loss 0.4230	TrainAcc 0.0605	ValidAcc 0.0591	TestAcc 0.0592	BestValid 0.1187
	Epoch 3400:	Loss 0.4238	TrainAcc 0.0461	ValidAcc 0.0398	TestAcc 0.0411	BestValid 0.1187
	Epoch 3425:	Loss 0.4274	TrainAcc 0.0914	ValidAcc 0.0838	TestAcc 0.0825	BestValid 0.1187
	Epoch 3450:	Loss 0.4205	TrainAcc 0.0280	ValidAcc 0.0184	TestAcc 0.0190	BestValid 0.1187
	Epoch 3475:	Loss 0.4225	TrainAcc 0.0313	ValidAcc 0.0235	TestAcc 0.0238	BestValid 0.1187
	Epoch 3500:	Loss 0.4247	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3525:	Loss 0.4219	TrainAcc 0.0632	ValidAcc 0.0533	TestAcc 0.0538	BestValid 0.1187
	Epoch 3550:	Loss 0.4214	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3575:	Loss 0.4200	TrainAcc 0.0302	ValidAcc 0.0224	TestAcc 0.0223	BestValid 0.1187
	Epoch 3600:	Loss 0.4179	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3625:	Loss 0.4192	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3650:	Loss 0.4174	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3675:	Loss 0.4156	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3700:	Loss 0.4125	TrainAcc 0.0326	ValidAcc 0.0265	TestAcc 0.0257	BestValid 0.1187
	Epoch 3725:	Loss 0.4163	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3750:	Loss 0.4132	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3775:	Loss 0.4158	TrainAcc 0.0275	ValidAcc 0.0175	TestAcc 0.0183	BestValid 0.1187
	Epoch 3800:	Loss 0.4119	TrainAcc 0.0288	ValidAcc 0.0196	TestAcc 0.0202	BestValid 0.1187
	Epoch 3825:	Loss 0.4173	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3850:	Loss 0.4172	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3875:	Loss 0.4174	TrainAcc 0.0321	ValidAcc 0.0257	TestAcc 0.0252	BestValid 0.1187
	Epoch 3900:	Loss 0.4171	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3925:	Loss 0.4249	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3950:	Loss 0.4172	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 3975:	Loss 0.4202	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4000:	Loss 0.4176	TrainAcc 0.0254	ValidAcc 0.0157	TestAcc 0.0159	BestValid 0.1187
	Epoch 4025:	Loss 0.4134	TrainAcc 0.0264	ValidAcc 0.0162	TestAcc 0.0167	BestValid 0.1187
	Epoch 4050:	Loss 0.4112	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4075:	Loss 0.4101	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4100:	Loss 0.4069	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4125:	Loss 0.4093	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4150:	Loss 0.4073	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4175:	Loss 0.4053	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4200:	Loss 0.4021	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4225:	Loss 0.3994	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4250:	Loss 0.4009	TrainAcc 0.0278	ValidAcc 0.0181	TestAcc 0.0188	BestValid 0.1187
	Epoch 4275:	Loss 0.4005	TrainAcc 0.0292	ValidAcc 0.0202	TestAcc 0.0206	BestValid 0.1187
	Epoch 4300:	Loss 0.3971	TrainAcc 0.0264	ValidAcc 0.0162	TestAcc 0.0167	BestValid 0.1187
	Epoch 4325:	Loss 0.3972	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4350:	Loss 0.4069	TrainAcc 0.0265	ValidAcc 0.0162	TestAcc 0.0167	BestValid 0.1187
	Epoch 4375:	Loss 0.3983	TrainAcc 0.0281	ValidAcc 0.0184	TestAcc 0.0191	BestValid 0.1187
	Epoch 4400:	Loss 0.4046	TrainAcc 0.0352	ValidAcc 0.0361	TestAcc 0.0378	BestValid 0.1187
	Epoch 4425:	Loss 0.5782	TrainAcc 0.0346	ValidAcc 0.0355	TestAcc 0.0371	BestValid 0.1187
	Epoch 4450:	Loss 0.5837	TrainAcc 0.0812	ValidAcc 0.0829	TestAcc 0.0831	BestValid 0.1187
	Epoch 4475:	Loss 0.5733	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4500:	Loss 0.5974	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4525:	Loss 0.5368	TrainAcc 0.0240	ValidAcc 0.0188	TestAcc 0.0192	BestValid 0.1187
	Epoch 4550:	Loss 0.4920	TrainAcc 0.0250	ValidAcc 0.0156	TestAcc 0.0157	BestValid 0.1187
	Epoch 4575:	Loss 0.4744	TrainAcc 0.0252	ValidAcc 0.0157	TestAcc 0.0159	BestValid 0.1187
	Epoch 4600:	Loss 0.4556	TrainAcc 0.0586	ValidAcc 0.0585	TestAcc 0.0591	BestValid 0.1187
	Epoch 4625:	Loss 0.4587	TrainAcc 0.0593	ValidAcc 0.0616	TestAcc 0.0615	BestValid 0.1187
	Epoch 4650:	Loss 0.4512	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4675:	Loss 0.4427	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4700:	Loss 0.4318	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4725:	Loss 0.4255	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4750:	Loss 0.4170	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4775:	Loss 0.4098	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4800:	Loss 0.4065	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4825:	Loss 0.3994	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4850:	Loss 0.3964	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4875:	Loss 0.3961	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4900:	Loss 0.3962	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4925:	Loss 0.3929	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4950:	Loss 0.3898	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
	Epoch 4975:	Loss 0.3909	TrainAcc 0.0246	ValidAcc 0.0153	TestAcc 0.0155	BestValid 0.1187
	Epoch 5000:	Loss 0.3897	TrainAcc 0.0251	ValidAcc 0.0156	TestAcc 0.0158	BestValid 0.1187
Node 3, Pre/Post-Pipelining: 1.089 / 0.880 ms, Bubble: 73.360 ms, Compute: 248.213 ms, Comm: 26.302 ms, Imbalance: 19.210 ms
Node 6, Pre/Post-Pipelining: 1.091 / 0.845 ms, Bubble: 73.524 ms, Compute: 245.483 ms, Comm: 29.078 ms, Imbalance: 19.263 ms
Node 2, Pre/Post-Pipelining: 1.088 / 0.867 ms, Bubble: 74.002 ms, Compute: 242.062 ms, Comm: 29.629 ms, Imbalance: 21.563 ms
Node 7, Pre/Post-Pipelining: 1.090 / 16.512 ms, Bubble: 58.118 ms, Compute: 258.267 ms, Comm: 16.909 ms, Imbalance: 18.190 ms
Node 4, Pre/Post-Pipelining: 1.089 / 0.852 ms, Bubble: 73.752 ms, Compute: 240.730 ms, Comm: 25.874 ms, Imbalance: 27.161 ms
Node 5, Pre/Post-Pipelining: 1.089 / 0.876 ms, Bubble: 73.498 ms, Compute: 245.738 ms, Comm: 30.507 ms, Imbalance: 17.521 ms
Node 1, Pre/Post-Pipelining: 1.090 / 0.854 ms, Bubble: 73.719 ms, Compute: 250.806 ms, Comm: 27.602 ms, Imbalance: 14.798 ms
Node 0, Pre/Post-Pipelining: 1.091 / 0.887 ms, Bubble: 74.284 ms, Compute: 256.816 ms, Comm: 16.818 ms, Imbalance: 18.855 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.091 ms
Cluster-Wide Average, Post-Pipelining Overhead: 0.887 ms
Cluster-Wide Average, Bubble: 74.284 ms
Cluster-Wide Average, Compute: 256.816 ms
Cluster-Wide Average, Communication: 16.818 ms
Cluster-Wide Average, Imbalance: 18.855 ms
Node 0, GPU memory consumption: 6.739 GB
Node 1, GPU memory consumption: 5.819 GB
Node 2, GPU memory consumption: 5.819 GB
Node 7, GPU memory consumption: 5.569 GB
Node 3, GPU memory consumption: 5.796 GB
Node 4, GPU memory consumption: 5.796 GB
Node 6, GPU memory consumption: 5.819 GB
Node 5, GPU memory consumption: 5.819 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.744351 s---------------
------------------------node id 1,  per-epoch time: 0.744351 s---------------
------------------------node id 4,  per-epoch time: 0.744351 s---------------
------------------------node id 2,  per-epoch time: 0.744351 s---------------
------------------------node id 5,  per-epoch time: 0.744351 s---------------
------------------------node id 3,  per-epoch time: 0.744351 s---------------
------------------------node id 6,  per-epoch time: 0.744351 s---------------
------------------------node id 7,  per-epoch time: 0.744351 s---------------
************ Profiling Results ************
	Bubble: 496.269383 (ms) (66.70 percentage)
	Compute: 244.313225 (ms) (32.83 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.499791 (ms) (0.47 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 1.216 GB
	Aggregated layer-level communication throughput: 411.877 Gbps
Highest valid_acc: 0.1187
Target test_acc: 0.1150
Epoch to reach the target acc: 49
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
