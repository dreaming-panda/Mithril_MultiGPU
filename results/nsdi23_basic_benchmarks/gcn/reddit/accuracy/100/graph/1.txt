Initialized node 0 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.234 seconds.
Building the CSC structure...
        It takes 2.375 seconds.
Building the CSC structure...
        It takes 2.415 seconds.
Building the CSC structure...
        It takes 2.511 seconds.
Building the CSC structure...
        It takes 2.513 seconds.
Building the CSC structure...
        It takes 2.644 seconds.
Building the CSC structure...
        It takes 2.675 seconds.
Building the CSC structure...
        It takes 2.678 seconds.
Building the CSC structure...
        It takes 2.169 seconds.
        It takes 2.336 seconds.
        It takes 2.359 seconds.
        It takes 2.350 seconds.
        It takes 2.409 seconds.
        It takes 2.280 seconds.
        It takes 2.274 seconds.
        It takes 2.373 seconds.
Building the Feature Vector...
        It takes 0.250 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.248 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.033 seconds.
Building the Feature Vector...
        It takes 0.273 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.270 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 0.295 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.031 seconds.
Building the Feature Vector...
        It takes 0.039 seconds.
        It takes 0.262 seconds.
Building the Label Vector...
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
        It takes 0.032 seconds.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
Chunks (number of global chunks: 8): 0-[0, 29120) 1-[29120, 58241) 2-[58241, 87362) 3-[87362, 116483) 4-[116483, 145604) 5-[145604, 174724) 6-[174724, 203845) 7-[203845, 232965)
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
GPU 0, layer [0, 32)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 10 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 53.420 Gbps (per GPU), 427.358 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 53.180 Gbps (per GPU), 425.439 Gbps (aggregated)
The layer-level communication performance: 53.170 Gbps (per GPU), 425.357 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.031 Gbps (per GPU), 448.246 Gbps (aggregated)
The layer-level communication performance: 52.965 Gbps (per GPU), 423.718 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 52.811 Gbps (per GPU), 422.490 Gbps (aggregated)
The layer-level communication performance: 55.786 Gbps (per GPU), 446.286 Gbps (aggregated)
The layer-level communication performance: 52.746 Gbps (per GPU), 421.964 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.689 Gbps (per GPU), 1277.510 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.667 Gbps (per GPU), 1277.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.695 Gbps (per GPU), 1277.558 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.695 Gbps (per GPU), 1277.558 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.683 Gbps (per GPU), 1277.461 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.665 Gbps (per GPU), 1277.317 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.680 Gbps (per GPU), 1277.437 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.713 Gbps (per GPU), 1277.702 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.275 Gbps (per GPU), 834.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.275 Gbps (per GPU), 834.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.264 Gbps (per GPU), 834.113 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.275 Gbps (per GPU), 834.196 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.275 Gbps (per GPU), 834.203 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.273 Gbps (per GPU), 834.182 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.254 Gbps (per GPU), 834.030 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.275 Gbps (per GPU), 834.196 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.668 Gbps (per GPU), 253.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.667 Gbps (per GPU), 253.337 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.668 Gbps (per GPU), 253.340 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.667 Gbps (per GPU), 253.335 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.668 Gbps (per GPU), 253.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.667 Gbps (per GPU), 253.334 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.664 Gbps (per GPU), 253.314 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.665 Gbps (per GPU), 253.316 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.83ms  9.62ms  8.98ms  1.09 29.12K 14.23M
 chk_1  5.44ms  5.05ms  4.45ms  1.22 29.12K  6.56M
 chk_2 17.43ms 17.09ms 16.54ms  1.05 29.12K 24.68M
 chk_3 17.32ms 16.94ms 16.37ms  1.06 29.12K 22.95M
 chk_4  5.23ms  4.88ms  4.29ms  1.22 29.12K  6.33M
 chk_5  9.42ms  9.04ms  8.48ms  1.11 29.12K 12.05M
 chk_6 10.66ms 10.22ms  9.68ms  1.10 29.12K 14.60M
 chk_7  9.94ms  9.52ms  8.78ms  1.13 29.12K 13.21M
   Avg 10.66 10.29  9.70
   Max 17.43 17.09 16.54
   Min  5.23  4.88  4.29
 Ratio  3.33  3.50  3.86
   Var 18.71 18.74 18.87
Profiling takes 2.789 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 332.319 ms
Partition 0 [0, 4) has cost: 332.319 ms
Partition 1 [4, 8) has cost: 329.417 ms
Partition 2 [8, 12) has cost: 329.417 ms
Partition 3 [12, 16) has cost: 329.417 ms
Partition 4 [16, 20) has cost: 329.417 ms
Partition 5 [20, 24) has cost: 329.417 ms
Partition 6 [24, 28) has cost: 329.417 ms
Partition 7 [28, 32) has cost: 324.638 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 307.499 ms
GPU 0, Compute+Comm Time: 123.391 ms, Bubble Time: 132.438 ms, Imbalance Overhead: 51.669 ms
GPU 1, Compute+Comm Time: 122.331 ms, Bubble Time: 122.256 ms, Imbalance Overhead: 62.912 ms
GPU 2, Compute+Comm Time: 122.331 ms, Bubble Time: 112.321 ms, Imbalance Overhead: 72.847 ms
GPU 3, Compute+Comm Time: 122.331 ms, Bubble Time: 112.832 ms, Imbalance Overhead: 72.336 ms
GPU 4, Compute+Comm Time: 122.331 ms, Bubble Time: 121.871 ms, Imbalance Overhead: 63.297 ms
GPU 5, Compute+Comm Time: 122.331 ms, Bubble Time: 130.910 ms, Imbalance Overhead: 54.258 ms
GPU 6, Compute+Comm Time: 122.331 ms, Bubble Time: 140.108 ms, Imbalance Overhead: 45.059 ms
GPU 7, Compute+Comm Time: 120.942 ms, Bubble Time: 150.662 ms, Imbalance Overhead: 35.895 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 595.364 ms
GPU 0, Compute+Comm Time: 231.607 ms, Bubble Time: 293.070 ms, Imbalance Overhead: 70.687 ms
GPU 1, Compute+Comm Time: 234.996 ms, Bubble Time: 272.592 ms, Imbalance Overhead: 87.776 ms
GPU 2, Compute+Comm Time: 234.996 ms, Bubble Time: 253.781 ms, Imbalance Overhead: 106.586 ms
GPU 3, Compute+Comm Time: 234.996 ms, Bubble Time: 235.351 ms, Imbalance Overhead: 125.017 ms
GPU 4, Compute+Comm Time: 234.996 ms, Bubble Time: 216.920 ms, Imbalance Overhead: 143.447 ms
GPU 5, Compute+Comm Time: 234.996 ms, Bubble Time: 215.923 ms, Imbalance Overhead: 144.444 ms
GPU 6, Compute+Comm Time: 234.996 ms, Bubble Time: 235.335 ms, Imbalance Overhead: 125.033 ms
GPU 7, Compute+Comm Time: 236.838 ms, Bubble Time: 254.839 ms, Imbalance Overhead: 103.687 ms
The estimated cost of the whole pipeline: 948.006 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 661.735 ms
Partition 0 [0, 8) has cost: 661.735 ms
Partition 1 [8, 16) has cost: 658.833 ms
Partition 2 [16, 24) has cost: 658.833 ms
Partition 3 [24, 32) has cost: 654.055 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 316.884 ms
GPU 0, Compute+Comm Time: 162.534 ms, Bubble Time: 152.682 ms, Imbalance Overhead: 1.668 ms
GPU 1, Compute+Comm Time: 162.158 ms, Bubble Time: 132.402 ms, Imbalance Overhead: 22.325 ms
GPU 2, Compute+Comm Time: 162.158 ms, Bubble Time: 112.369 ms, Imbalance Overhead: 42.357 ms
GPU 3, Compute+Comm Time: 161.497 ms, Bubble Time: 113.070 ms, Imbalance Overhead: 42.317 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 600.398 ms
GPU 0, Compute+Comm Time: 304.289 ms, Bubble Time: 213.023 ms, Imbalance Overhead: 83.087 ms
GPU 1, Compute+Comm Time: 305.945 ms, Bubble Time: 211.621 ms, Imbalance Overhead: 82.832 ms
GPU 2, Compute+Comm Time: 305.945 ms, Bubble Time: 250.830 ms, Imbalance Overhead: 43.623 ms
GPU 3, Compute+Comm Time: 306.872 ms, Bubble Time: 290.132 ms, Imbalance Overhead: 3.394 ms
    The estimated cost with 2 DP ways is 963.147 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1320.569 ms
Partition 0 [0, 16) has cost: 1320.569 ms
Partition 1 [16, 32) has cost: 1312.888 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 356.186 ms
GPU 0, Compute+Comm Time: 237.641 ms, Bubble Time: 118.112 ms, Imbalance Overhead: 0.433 ms
GPU 1, Compute+Comm Time: 236.993 ms, Bubble Time: 119.193 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 631.581 ms
GPU 0, Compute+Comm Time: 420.126 ms, Bubble Time: 211.455 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 421.320 ms, Bubble Time: 209.245 ms, Imbalance Overhead: 1.016 ms
    The estimated cost with 4 DP ways is 1037.156 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2633.457 ms
Partition 0 [0, 32) has cost: 2633.457 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 510.802 ms
GPU 0, Compute+Comm Time: 510.802 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 695.064 ms
GPU 0, Compute+Comm Time: 695.064 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1266.159 ms

*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 190)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 29120
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 190)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 29120, Num Local Vertices: 29121
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 190)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 58241, Num Local Vertices: 29121
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 190)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 87362, Num Local Vertices: 29121
*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 190)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 116483, Num Local Vertices: 29121
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 190)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 145604, Num Local Vertices: 29120
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 190)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 174724, Num Local Vertices: 29121
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 190)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 203845, Num Local Vertices: 29120
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 4 initializing the weights for op[0, 190)...
+++++++++ Node 1 initializing the weights for op[0, 190)...
+++++++++ Node 3 initializing the weights for op[0, 190)...
+++++++++ Node 2 initializing the weights for op[0, 190)...
+++++++++ Node 0 initializing the weights for op[0, 190)...
+++++++++ Node 5 initializing the weights for op[0, 190)...
+++++++++ Node 7 initializing the weights for op[0, 190)...
+++++++++ Node 6 initializing the weights for op[0, 190)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 3, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 50:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 75:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 100:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 125:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 150:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 175:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 200:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 225:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 250:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 275:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 300:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 325:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 350:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 375:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 400:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 425:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 450:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 475:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 500:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 525:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 550:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 575:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 600:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 625:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 650:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 675:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 700:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 725:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 750:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 775:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 800:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 825:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 850:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 875:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 900:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 925:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 950:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 975:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1000:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1025:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1050:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1075:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1100:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1125:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1150:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1175:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1200:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1225:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1250:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1275:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1300:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1325:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1350:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1375:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1400:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1425:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1450:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1475:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1500:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1525:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1550:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1575:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1600:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1625:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1650:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1675:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1700:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1725:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1750:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1775:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1800:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1825:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1850:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1875:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1900:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1925:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1950:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 1975:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2000:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2025:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2050:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2075:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2100:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2125:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2150:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2175:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2200:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2225:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2250:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2275:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2300:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2325:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2350:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2375:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2400:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2425:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2450:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2475:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2500:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2525:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2550:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2575:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2600:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2625:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2650:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2675:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2700:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2725:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2750:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2775:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2800:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2825:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2850:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2875:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2900:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2925:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2950:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 2975:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3000:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3025:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3050:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3075:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3100:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3125:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3150:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3175:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3200:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3225:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3250:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3275:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3300:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3325:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3350:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3375:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3400:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3425:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3450:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3475:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3500:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3525:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3550:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3575:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3600:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3625:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3650:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3675:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3700:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3725:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3750:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3775:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3800:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3825:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3850:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3875:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3900:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3925:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3950:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 3975:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4000:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4025:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4050:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4075:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4100:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4125:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4150:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4175:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4200:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4225:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4250:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4275:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4300:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4325:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4350:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4375:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4400:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4425:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4450:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4475:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4500:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4525:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4550:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4575:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4600:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4625:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4650:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4675:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4700:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4725:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4750:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4775:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4800:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4825:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4850:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4875:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4900:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4925:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4950:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 4975:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
	Epoch 5000:	Loss 3.7136	TrainAcc 0.0551	ValidAcc 0.0603	TestAcc 0.0576	BestValid 0.0603
Node 0, Pre/Post-Pipelining: 8.582 / 18.728 ms, Bubble: 0.617 ms, Compute: 859.272 ms, Comm: 0.011 ms, Imbalance: 0.021 ms
Node 1, Pre/Post-Pipelining: 8.576 / 18.685 ms, Bubble: 0.568 ms, Compute: 859.370 ms, Comm: 0.007 ms, Imbalance: 0.017 ms
Node 2, Pre/Post-Pipelining: 8.582 / 18.715 ms, Bubble: 0.381 ms, Compute: 859.526 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 3, Pre/Post-Pipelining: 8.586 / 18.761 ms, Bubble: 0.040 ms, Compute: 859.816 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 4, Pre/Post-Pipelining: 8.577 / 18.705 ms, Bubble: 0.785 ms, Compute: 859.137 ms, Comm: 0.010 ms, Imbalance: 0.017 ms
Node 5, Pre/Post-Pipelining: 8.576 / 18.684 ms, Bubble: 0.743 ms, Compute: 859.202 ms, Comm: 0.008 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 8.580 / 18.723 ms, Bubble: 0.324 ms, Compute: 859.579 ms, Comm: 0.009 ms, Imbalance: 0.019 ms
Node 7, Pre/Post-Pipelining: 8.576 / 18.671 ms, Bubble: 0.801 ms, Compute: 859.155 ms, Comm: 0.008 ms, Imbalance: 0.017 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.582 ms
Cluster-Wide Average, Post-Pipelining Overhead: 18.728 ms
Cluster-Wide Average, Bubble: 0.617 ms
Cluster-Wide Average, Compute: 859.272 ms
Cluster-Wide Average, Communication: 0.011 ms
Cluster-Wide Average, Imbalance: 0.021 ms
Node 0, GPU memory consumption: 15.813 GB
Node 1, GPU memory consumption: 15.200 GB
Node 2, GPU memory consumption: 15.208 GB
Node 3, GPU memory consumption: 15.184 GB
Node 4, GPU memory consumption: 15.174 GB
Node 6, GPU memory consumption: 15.204 GB
Node 5, GPU memory consumption: 15.198 GB
Node 7, GPU memory consumption: 15.180 GB
Node 0, Graph-Level Communication Throughput: 25.516 Gbps, Time: 625.696 ms
Node 1, Graph-Level Communication Throughput: 23.948 Gbps, Time: 709.110 ms
Node 4, Graph-Level Communication Throughput: 11.633 Gbps, Time: 723.361 ms
Node 2, Graph-Level Communication Throughput: 35.973 Gbps, Time: 491.655 ms
Node 5, Graph-Level Communication Throughput: 19.085 Gbps, Time: 639.916 ms
Node 3, Graph-Level Communication Throughput: 49.709 Gbps, Time: 465.571 ms
Node 6, Graph-Level Communication Throughput: 26.578 Gbps, Time: 616.330 ms
Node 7, Graph-Level Communication Throughput: 21.401 Gbps, Time: 636.195 ms
------------------------node id 0,  per-epoch time: 1.270237 s---------------
------------------------node id 1,  per-epoch time: 1.270237 s---------------
------------------------node id 4,  per-epoch time: 1.270237 s---------------
------------------------node id 2,  per-epoch time: 1.270237 s---------------
------------------------node id 5,  per-epoch time: 1.270237 s---------------
------------------------node id 3,  per-epoch time: 1.270237 s---------------
------------------------node id 6,  per-epoch time: 1.270237 s---------------
------------------------node id 7,  per-epoch time: 1.270237 s---------------
************ Profiling Results ************
	Bubble: 410.748976 (ms) (32.35 percentage)
	Compute: 207.013666 (ms) (16.30 percentage)
	GraphCommComputeOverhead: 18.972846 (ms) (1.49 percentage)
	GraphCommNetwork: 613.478159 (ms) (48.31 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 19.666793 (ms) (1.55 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 14.482 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 14.493 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0603
Target test_acc: 0.0576
Epoch to reach the target acc: 24
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
